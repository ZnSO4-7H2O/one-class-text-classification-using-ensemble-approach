{"title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in  Recurrent Neural Networks", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.", "text": "fig. lstmvis user interface. user interactively selects range text specifying hypothesis model select view range used match similar hidden state patterns displayed match view selection made specifying start-stop range text activation threshold leads selection hidden states start-stop range constrained using pattern plot meta-tracks depict extra information word position like predictions tool match selection similar hidden state patterns data varying lengths providing insight representations learned model. match view additionally includes user-deﬁned meta-data encoded heatmaps color heatmap mapped word matrix allows user patterns lead reﬁnement selection hypothesis. navigation aids provide convenience abstract— recurrent neural networks particular long short-term memory networks remarkably effective tool sequence modeling learn dense black-box hidden representation sequential input. researchers interested better understanding models studied changes hidden state representations time noticed interpretable patterns also signiﬁcant noise. work present lstmvis visual analysis tool recurrent neural networks focus understanding hidden state dynamics. tool allows users select hypothesis input range focus local state changes match states changes similar patterns large data align results structural annotations domain. show several cases tool analyzing speciﬁc hidden state properties dataset containing nesting phrase structure chord progressions demonstrate tool used isolate patterns statistical analysis. characterize domain different stakeholders goals tasks. long-term usage data putting tool online revealed great interest machine learning community. introduction recent years deep neural networks become central modeling tool many artiﬁcial cognition tasks image recognition speech recognition text classiﬁcation. models share common property utilize hidden feature representation input pre-speciﬁed user learned task hand. hidden representations proven effective classiﬁcation. however black-box nature learned deep neural networks utilize hidden features different model structures shown effective different tasks. standard deep neural networks learn ﬁxed-size features whereas convolutional neural networks dominant image recognition learn task-speciﬁc ﬁlter-bank produce spatial feature maps. work focus deep neural network architectures known recurrent neural networks produce time-series hidden feature-state representations. throughout work assume given sequence words time might consist english words want translate sentence whose sentiment would like detect even symbolic input musical notes code. additionally assume mapping word vector representation representation either standard ﬁxed mapping wordvec learned rest model. formally rnns class neural networks sequentially input word vectors sequence hidden feature-state representations achieved learning weights neural network applied recursively time-step function takes input vector hidden state vector gives hidden state vector hidden state vector vectors particularly change time main focus work. interested particular change single hidden state varies. model learns hidden states represent features input words. such learned modeling tasks utilizing discrete sequential input. notable application language modeling core task natural language processing. language modeling time preﬁx words taken input goal model distribution next word used produce distribution applying multi-class classiﬁcation based hidden feature-state vector formally deﬁne softmax parameters. full computation language model shown figure widely observed hidden states able capture important information structure input sentence necessary perform prediction. however difﬁcult trace captured exactly learned. instance shown rnns count parentheses match quotes unclear whether rnns naturally discover aspects language phrases grammar topics. work focus particularly exploring question examining dynamics hidden states time. cases mainly focus long short-term memory networks lstms deﬁne variant modiﬁed hidden state update effectively learn long-term interactions. models widely used practice. addition lstms rnns stacked layers produce multiple hidden state vectors time step improves performance. results mainly stacked lstms visualization requires access time evolving abstract vector representation therefore used layer wide variety models. note lstms maintain cell state vector hidden state vector time step. system used analyze either vectors experiments found cell states easier work with. simplicity however refer vectors generically hidden states throughout paper. fig. recurrent neural network language model used compute time step word converted word vector used update hidden state rnn. hidden state vector used prediction. language modeling used deﬁne probability next word softmax. capturing representation sequence-modeling applications text processing. recent strong empirical results indicate internal representations learn capture complex relationships words within sentence document. improved representation directly applications machine translation speech recognition music generation text classiﬁcation among variety applications. rnns shown clear improvements sequence modeling proven difﬁcult interpret feature representation. such remains unclear exactly particular model representing long-distance relationships within sequence. typically rnns contain millions parameters utilize repeated transformations large hidden representations time-varying conditions. factors make model inter-dependencies challenging interpret without sophisticated mathematical tools. enable users explore complex network interactions directly connect abstract representations human understandable inputs? work focus visual analysis hidden features rnns. developed lstmvis tool allow advanced user groups explore form hypotheses hidden state dynamics. analyzed neural network users identiﬁed three major user roles different requirements architects develop novel deep learning structures trainers develop data sets train existing models users apply deep models data. lstmvis focused architects trainers performed goal task analysis develop effective visual encodings interactions. lstmvis combines time-series based select interface interactive match tool search similar hidden state patterns large dataset. live system accessed lstm.seas.harvard.edu source code provided. present cases applying technique identify explore patterns rnns trained large real-world datasets text speech recognition biological sequence analysis domains. discuss release-before-publish strategy used develop improve tool based user feedback. background recurrent neural networks rnns type deep neural network architecture proven effective sequence modeling tasks text processing. major challenge working variable-length text sequences producing features capture summarize long-distance relations text. relationships particularly important tasks require processing generating sequences machine translation. rnnbased models effectively learn hidden representations time-step used decision making. refer change understanding rnns visualization core contribution visualizing state dynamics rnns structured inspired previous work convolutional neural networks vision applications linguistic tasks visualizations shown useful tool understanding certain aspects rnns. karpathy static visualization techniques help understand hidden states language models. work demonstrates selected cells model clear events open parentheses start urls. fig. views neural network models different user roles. architect analyzes modiﬁes components system. trainer abstracts model main components parameters focuses training different data sets. user abstract view model considers whether output coherent given input. present additional techniques particularly gradient-based saliency important words. work also looks several different models datasets including text classiﬁcation auto-encoders. kadar show rnns speciﬁcally learn lexical categories grammatical functions carry semantic information partially modifying inputs model. inspired techniques approach tries extend beyond single examples provide general interactive visualization approach data exploratory analysis. extending models interpretability recent work also developed methods extending rnns certain problems make easier interpret popular technique neural attention mechanism allow model focus particular aspect input. bahdanau attention soft alignment machine translation. attention identify important aspects image captioning whereas hermann attention important aspects document extraction task. approaches side beneﬁt visualize aspect model using. approach differs work requires changing underlying model structure whereas attempt interpret hidden states ﬁxed model directly. interactive visualization neural networks work interactive visualization interpreting machine learning models. tzeng present visualization system feedforward neural networks goal interpretation kapoor give user-interface tuning learning itself. prospector system provides general-purpose tool practitioners better understand machine learning model predictions. recent work also describes systems focus analysis hidden states convolutional neural networks. utilize metaphor show neurons connections learned features. rauber projections explore relationships neurons learned observations. work focused user interfaces constructing models tensorboard related playground convolutional neural models playground. tensorflow.org/. work similar spirit work tzeng rauber concerned interpreting hidden states neural network models. however speciﬁc goals focus rnns needs speciﬁc users resulting visual design signiﬁcantly different. user analysis goals deep neural networks widely employed research industrial setting diverse users different needs. developing visual analysis goals ﬁrst laid prototypical stakeholders might beneﬁt improved analysis. meetings members natural language processing group computational biology group identiﬁed three user roles incentives view neural network models. figure summarizes following roles aspect model might hope master architects looking develop deep learning methodologies modify existing deep architectures domains. architect interested training many variant network structures comparing models capture features domain. assume architects deeply knowledgeable machine learning neural networks internal structure system. direct goal comparing performance variant models understanding learned properties system. trainers users interested applying known architectures tasks domain experts. trainers utilize rnns tool understand concepts network optimization. however main focus application domain utilizing effective methods solve known problems. goal known network architecture observe learns novel model. examples trainers include bioinformatician applied machine learning engineer. users make prevalent role network users. users utilize general-purpose pretrained networks various tasks. users need understand training process apply networks algorithm data. main desire explain results locate happening something goes wrong. examples end-users include data scientists product engineers using user roles general neural network domain believe help describe understand stakeholders space. analysis decided focus advanced spectrum particularly user role architects. aimed provide users greater visibility internals system. user feedback ﬁrst prototype motivated include trainer role users focus predictions model. future work want develop systems engage users deep neural networks more. roles mind aimed help trainers architects better understand high-level question what information capture hidden feature-states?. addressing question main goal project. based series dicussions deep learning experts identiﬁed following domain goals formulate hypothesis properties hidden states might learn capture speciﬁc model. hypothesis requires initial understanding hidden state values time close read original input. filter hidden states using discrete textual selection along continuous thresholding. selections methods allow user form hypotheses separate visual signal noise. match selections similar examples based hidden state activation pattern. matched phrase intuitively similar characteristics selection support reject hypothesis. align textual annotations visually matched phrases. annotations allow user compare learned representation alternative structural hypotheses partof-speech tags known grammars. annotation data easily extensible. provide general interface used model text-like dataset. make easy generate crowd knowledge trigger discussions similarities differences wide variety models. list tasks provided guideline design lstmvis. addition tasks deﬁne core interaction mechanisms discovery lstmvis visualize filter select match align ﬁrst describe implementation interactions later demonstrate application multiple cases section design lstmvis lstmvis composed major visual analysis components. select view supports formulation hypothesis using novel visual encoding hidden state changes match view allows reﬁnement hypothesis remaining agnostic underlying data model design decisions outcome long iterative process. first developed several interactive low-ﬁdelity prototypes varying complexity expert users highlight different aspects data developing complete system released broader audience online collect long-term feedback based feedback release developed current version several months later. visualization hidden state changes visualizing progression hidden state vectors along sequence words core lstmvis. following refer hidden state dimension ddimensional hidden state vector. related literature common encode hidden state vectors heatmap along time-axis style favored view complete hidden state vectors however approach several drawbacks interactive visualization. foremost heatmaps scale well increasing dimensionality hidden state vectors. non-effective encoding important information i.e. hidden state values color hue. additionally emphasize order hidden states vector fig. early-stage prototypes system. hidden state vectors encoded heatmaps time. style places emphasis relationships neighboring states particular meaning model. selection prototype utilizing parallel coordinates. prototype emphasized selections based small movements state values directly plot made difﬁcult specify connections hidden state values source text. decided consider hidden state data item timesteps dimensions data item parallel coordinates plot. encode hidden state value using effective visual variable position. figure shows ﬁrst iteration using parallel coordinates plot. number data points along plot additionally encoded heatmap background emphasize dense sparse regions. however cumbersome formulate hypothesis longer range adjusting many y-axis brush selectors granularity. allowing user perform selection directly manipulating hidden state values felt decoupled original source information—the text. idea facilitate selection process allow user easily discretize data based threshold select ranges directly words figure shows ﬁrst complete prototype online collect long-term user feedback several user comments believe redundant encoding hidden states select view well understood. ﬁnal design shown figure omitted redundant encoding sake clarity highlight wider regions text. xaxis labeled word inputs corresponding time-step. words ﬁxed width time steps distorted. figure shows movement hidden state vector example sequence. select view select view shown half figure centered around parallel coordinates plot hidden state dynamics. full plot difﬁcult comprehend directly. therefore lstmvis allows user formulate hypothesis semantics subset hidden states selecting range words express fig. hypothesis selection process. selection covers little prince threshold blue highlighted hidden states selected. threshold raised pattern plot bottom extended left eliminating hidden states values reading pattern plot additionally extended right removing hidden states threshold reading i.e. hidden states selected following pattern threshold word little prince word after. lstmvis also provides several convenience methods navigate speciﬁc time steps. buttons timeline used move forward backward lstmvis also offers search functionality speciﬁc phrases. selection panel used efﬁciently switch different layers word-width decreased increased provide support different average word lengths option turn heatmaps match view provides convenience working many mate tracks. interaction methods allow user deﬁne hypothesis word range results selection subset hidden states following speciﬁed pattern w.r.t. deﬁned threshold relies hidden state vectors reﬁne reject hypothesis user make match view. match view match view provides evidence selected hypothesis. view provides relevant matched phrases similar hidden state patterns phrase selected user. goal maintaining intuitive match interface deﬁne matches ranges data would lead similar hidden states selection criteria formally assume user selected threshold hidden states limited selection right left further shown figures rank possible candidate ranges dataset starting time ending time step process original selection limited either side modify step take account candidates. instance limit left include state indices also satisfy ha−c efﬁciency score possible candidate ranges limit candidate ﬁltering ranges minimum number fig. snippet ﬁrst complete prototype. list selected cells brushing method remained ﬁnal version modiﬁed pattern plot omitted redundant encodings interesting property. instance user select range within shared nesting levels tree-structured text representative noun phrase text corpus chord progression musical corpus. select user brushes range words form pattern interest process implicitly selects hidden states selected range. dashed line parallel coordinates plot indicates user-deﬁned threshold value partitions hidden states within range. addition selecting range user modify pattern plot deﬁne hidden states must also immediately selected range. figure shows different combinations pattern plot conﬁgurations corresponding hidden state selections. using selection criteria user creates selected hidden states follow speciﬁed on/off pattern w.r.t. deﬁned threshold. assist selection ranges user make heatmap underlying word sequence depicts many selected hidden states word indicates repetitions hidden state patterns close local neighborhood. additionally user aligned tracks textual annotations selection view. e.g. visualize part-ofspeech annotations named entity recognition results. feature meta tracks result feedback multiple online fig. plot phrase parenthesis synthetic language. full hidden states. note strong movement states parenthesis boundaries. selection made start fourth level nesting. even select view clear several hidden states represent four-level nesting count. plots meta-track indicates nesting level ground truth. result matching selected states indicates seem capture nesting level phrases variable length. matching algorithm retrieve results shown each word matrix located match view. cell word matrix linked cell corresponding heatmaps. heatmaps encode additional information word matching results. always available match count heatmap encodes number overlapping states timestep. user additional annotations similar meta tracks selection view heatmaps imagine annotations ground truth data e.g. part-of-speech tags text corpora information help calibrate hypotheses e.g. nesting depth tree-structured text. figure shows hovering little leads highlights match count heatmap indicating seven overlapping states match selection position. heatmap indicates word position acts adjective heatmap colors mapped directly background matches simple method reveal pattern across results based human identiﬁable patterns alignments matching mapping methods lead data analysis reﬁnement current hypothesis. design considerations limitations lstmvis operates around bottom-up approach starting seed hypothesis searching similarities across whole dataset. project experimented top-down approaches following shneiderman mantra found global overviews projections hidden state values summary statistics little reveal interpretable patterns large datasets. lstmvis differs related work instead working prediction output operates directly hidden states identify relationships. design decision addresses target group architects trainers familiar model internals. another goal tool representation invariant ordering hidden states hidden state vector remaining scalable w.r.t. size vector. addressed section conditions fulﬁlled plotting hidden states individually onto parallel coordinates. however even compact representation applications hidden states become challenging analyze whole. approaches address visual noise either ﬁlter hidden states using interactive methods algorithmic preprocessing like dimensionality reduction techniques pruning methods. testing latter automated ﬁltering approaches decided using within application. found technical design implementation lstmvis consists modules visualization system modeling component. source code documentation example models available lstm.seas.harvard.edu. visualization client-server system uses javascript client side python flask connexion numpy server side. timeseries data loaded dynamically ﬁles. optional annotation ﬁles speciﬁed categorical data labels data sets added easily declarative yaml conﬁguration ﬁle. modeling system completely separated visualization allow compatibility deep learning framework experiments torch framework. trained models separately exported results visualization. cases experimenting system trained explored many different models datasets tasks including word character language models neural machine translation systems auto-encoders summarization systems classiﬁers. additionally also experimented types real synthetic input data. proof-of-concept parenthesis language proof concept trained lstm language model synthetic data generated simple counting language parenthesis letter alphabet language constrained match parentheses nesting limited levels deep. opening parenthesis increases closing parenthesis decreases nesting level respectively. numbers generated randomly constrained indicate nesting level position. example string language might look like visualization hints model implicitly learned representation language modeling differentiate types phrases. course tool cannot conﬁrm deny type hypothesis provide clues analysis. check outside tool model clearly differentiating classes phrase dataset. compute every noun verb phrase shared task. vector representation set. results shown figure shows indeed on-off patterns enough partition noun phrases verb phrases. fig. projection hidden state patterns multi-word phrasal chunks penn treebank numerical follow-up phrase chunking hypothesis. points indicate noun phrases blue points indicate verb phrases colors indicate remaining phrase types. trained language modeling model separates phrase classes hidden states. fig. phrase selections match annotations wall street journal. user selects phrase marked improvement matches found entirely noun phrases start different words. note groundtruth noun phrases indicated sequence colors cyan blue violet select range starting invited. results various open verb phrases sequence colors orange blue note examples model return matches varying lengths. next consider case real-world natural language model perspective architect interested structure internal states relate underlying properties. experiment trained -layer lstm language model hidden states penn treebank following medium-sized model model trained language modeling interested seeing additionally learned properties underlying language structure. test this additionally include linguistic annotations visual analysis penn treebank. experimented including part-of-speech tags named entities parse structure. focus case phrase chunking. annotated dataset gold-standard phrase chunks provided conll shared task subset treebank include annotations noun phrases verb phrases along prepositions several less common phrase types. running experimental analysis found strong pattern selecting noun phrases hypotheses leads almost entirely noun phrase matches. additionally found selecting verb phrase preﬁxes would lead primarily verb phrase matches. figure show examples selections matches. biological sequence analysis models commonly used time-series analysis outside space text processing. area interest biological sequence analysis genomics deep neural networks directly applied sequences tasks classiﬁcation sequence labeling. consider models trained regulatory marker prediction neural networks used predict binding sites aligned sequences genome task recently explored models case study collaborated domain expert trainer approached becoming aware release tool. employed mixed rnn/cnn model trained genomic dataset made billion base pair long nucleotide sequence problem regulatory marker prediction. notably problem differs several crucial ways previous case granularity input much smaller training objective different problem known exhibit much longer term dependencies latent structure. conversations around issues introduce several iterative features tool. include ability zoom timeseries representation adjust granularity auto-scaling axes ranges allow switching data different activation ranges outputs notably ability add/remove arbitrarily many annotation label along word sequence timeline allow domain experts view predictions ground truth annotation track associated features time step. figure shows example last feature shows predict binding locations several different proteins along region dna. based mistakes model. instance early tool researcher noticed layer network using available states another signiﬁcant artifacts poorly aligned convolutional layer. observations helped provide feedback subsequent experimental design. furthermore recent work indicates increasing interest lstmvis biomedical domain genomics fig. biological sequence analysis. selected region genome seven different aligned annotation tracks. track indicates prediction protein binding sites time step. tracks activated/deactivated part debugging process compare predictions ground truth input properties user-speciﬁed annotations. fig. three examples single state patterns guitar chord dataset. several permutation common progression several patterns ending variant iviivv variants -iii chord progression patterns based http//openmusictheory.com/. musical chord progressions past work lstm structure emphasized cases single hidden states semantically interpretable. text biological data sets found exceptions rarely case. however datasets regular long-term structure single states could quite meaningful. simple example collected large songs annotated chords rock songs training data chords total. trained lstm language model predict next chord sequence conditioned previous chord symbols viewed results lstmvis found regular repeating structure chord progressions strongly reﬂected hidden states. certain states turn beginning standard progression remain though variant-length patterns resolution reached. figure examine three common general chord progressions rock music. select prototypical instance progression show single state captures pattern i.e. remains progression begins turns upon resolution. long-term case study shneiderman plaisant propose strategies multi-dimensional in-depth long-term case studies evaluate information visualization tools. observe scaling order magnitude terms number users duration observations clearly beneﬁcial. decided adopt core ideas report qualitative feedback quantitative success indicators open-source release lstmvis june created webpage video introduces core ideas lstmvis lstm.seas.harvard.edu. webpage provides opportunity users leave comments. advertise tool online followed social media strategy included collecting followers twitter using broadcast media reddit hackernews inviting editors popular machine learning blogs write guest articles. often reproduce scenarios explained online video. allow users share insights exploratory analysis ensured parameter string contains necessary information replay share scenario. collected logging information webpage using google analytics. within ﬁrst days page received unique users trafﬁc coming social media arriving directly. social media trafﬁc dominated channels used advertise tool surprise also observed substantial trafﬁc channels contact directly days recorded page views shrinking user trafﬁc social media point users used search accessed webpage directly small percentage users tried online demo. users used datasets shown explanation video explore further. open source code release stable simple enough ...ensure tool reasonable level reliability support basic features easy adoption provide user documentation explains data used tool. also provide convenience tools prepare data import asked students testers source code. based feedback made several improvements installation process. example prototype required nodejs resolve client-side dependencies. providing required libraries within repository removed cumbersome step installing nodejs target audience. observe around programmers liked project practitioners copied project make custom modiﬁcations. think demonstrate reasonable interest system days considering highly specialized target audience. furthermore observe adoption tool several documented cases. evermann et.al. describe application lstmvis understand trained model business process intelligence dataset. et.al. lstmvis experiments investigate language variants vagueness website privacy policies. increasing interest apply tool biomedical genomic data. indicated case described section e.g. ching besides quantitative observations also collected qualitative feedback comments webpage github tickets in-person presentations prototype. qualitative feedback make several changes system discussed section retrospective conducting long-term user study beneﬁted project multiple stages. preparing release prototype required focus strongly simplicity usability robustness tool. lead many small improvements internal prototype. design iterations inferred user feedback strengthen tool further. think planning successful long-term study requires four core ingredients reach target audience describing approach inform using social media email etc. allow users play tool setting demo server allow engagement experimentation tool providing sufﬁciently documented easily adoptable source code make simple possible users give feedback discussion forums reported issues person. study found crucial continuously engage users quickly take action based feedback. conclusion lstmvis provides interactive visualization facilitate data analysis hidden states. tool based direct inference user selects range text represent hypothesis tool matches selection examples data set. tool easily allows external annotations verify reject hypothesizes. requires time-series hidden states makes easy adopt wide range visual analyses different data sets models even different tasks demonstrate model presented several case studies applying tool different data sets. releasing tool source code online allows collect long-term user feedback already make several improvements. future work explore wide variety application cases adopted beyond imagined cases user groups. example case contacted highschool student using lstmvis learn methods. optimize learning scenario thinking datasets blog publication focus learning. another interesting future work support user role users simpliﬁed views internals help explaining speciﬁc model behavior. references abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos elsen engel fougner hannun legresley narang ozair prenger raiman satheesh seetapun sengupta wang wang wang xiao yogatama zhan zhu. deep speech end-to-end speech recognition english mandarin. corr abs/. boulanger-lewandowski bengio vincent. modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. proceedings international conference machine learning icml edinburgh scotland june july icml.cc omnipress lengerich israeli lanchantin woloszynek carpenter shrikumar cofer harris decaprio kundaje peng wiley segler gitter greene. opportunities obstacles deep learning biology medicine. biorxiv hermann kocisk´y grefenstette espeholt suleyman blunsom. teaching machines read comprehend. cortes lawrence sugiyama garnett eds. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada recurrent neural networks. k´ad´ar chrupała alishahi. representation linguistic form function recurrent neural networks. arxiv preprint arxiv. chen hovy jurafsky. visualizing understanding neural models nlp. proceedings conference north american chapter association computational linguistics human language technologies association computational linguistics diego california june mikolov sutskever chen corrado dean. distributed representations words phrases compositionality. advances neural information processing systems shneiderman plaisant. strategies evaluating information visualization tools multi-dimensional in-depth long-term case studies. proceedings workshop beyond time errors novel evaluation methods information visualization beliv venice italy introduction conll shared task language-independent named entity recognition. proceedings seventh conference natural language learning hlt-naacl -volume association computational linguistics wiles. recurrent neural networks learn implement symbolsensitive counting. advances neural information processing systems proceedings conference vol. press kiros courville salakhutdinov zemel bengio. show attend tell neural image caption generation visual attention. bach blei eds. proceedings international conference machine learning icml lille france july vol. jmlr proceedings jmlr.org", "year": 2016}