{"title": "Geometry and Expressive Power of Conditional Restricted Boltzmann  Machines", "tag": ["cs.NE", "cs.LG", "stat.ML", "60K99, 68T05, 68R05"], "abstract": "Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parametrized by interaction weights and biases. We address the representational power of these models, proving results their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models.", "text": "conditional restricted boltzmann machines undirected stochastic neural networks layer input output units connected bipartitely layer hidden units. networks deﬁne models conditional probability distributions states output units given states input units parametrized interaction weights biases. address representational power models proving results ability represent conditional markov random ﬁelds conditional distributions restricted supports minimal size universal approximators maximal model approximation errors dimension representable conditional distributions. contribute tools investigating conditional probability models allow improve results derived existing work restricted boltzmann machine probability models. keywords conditional restricted boltzmann machine universal approximation kullback-leibler approximation error expected dimension restricted boltzmann machines generative probability models deﬁned undirected stochastic networks bipartite interactions between visible hidden units. models well-known machine learning applications used infer distributed representations data train layers deep neural networks restricted connectivity networks allows train efﬁciently basis cheap inference ﬁnite gibbs sampling even deﬁned many units parameters. deﬁnes gibbs-boltzmann probability distributions observable states network depending interaction weights biases. introduction offered fischer igel expressive power probability models attracted much attention studied numerous papers treating particular universal approximation properties approximation errors efﬁciency representation dimension certain applications preferred work conditional probability distributions instead joint probability distributions. example classiﬁcation task conditional distribution used indicate belief class input without modeling probability observing input; sensorimotor control describe stochastic policy choosing actions based world observations; context information communication describe channel. rbms naturally deﬁne models conditional probability distributions called conditional restricted boltzmann machines models inherit many nice properties probability models cheap inference efﬁcient training. speciﬁcally crbm deﬁned clamping states input subset visible units rbm. input state obtains conditioned distribution states output visible units. figure illustration architecture. kind conditional models slight variants thereof seen success many applications; example classiﬁcation collaborative ﬁltering motion modeling reinforcement learning however much theoretical work addressing expressive power crbms. note relatively straightforward obtain results expressive power crbms existing theoretical work probability models. nevertheless accurate analysis requires take account speciﬁcities conditional case. formally crbm collection rbms possible input value. rbms differ biases hidden units inﬂuenced input values. however hidden biases independent different inputs moreover interaction weights biases visible units shared different inputs. sharing parameters draws substantial distinction crbm models independent tuples models. paper address representational power crbms contributing theoretical insights optimal number hidden units. focus lies classes conditional distributions possibly represented crbm ﬁxed number inputs outputs depending number hidden units. said this discuss problem ﬁnding optimal parameters give rise desired conditional distribution problems related incomplete knowledge target conditional distributions generalization errors. number training methods crbms discussed references listed above depending concrete applications. problems deal following distinct parameters model mapped distinct conditional distributions; smallest number hidden units sufﬁces obtaining model approximate target conditional distribution arbitrarily well approximate target conditional distribution without exceeding given error tolerance; approximate selected classes conditional distributions arbitrarily well? provide non-trivial solutions problems. focus case binary units main ideas extend case discrete non-binary units. paper organized follows. section contains formal deﬁnitions elementary properties crbms. section investigates geometry crbm models three subsections. section study dimension sets conditional distributions represented crbms show cases dimension expected counting parameters section address universal approximation problem deriving upper lower bounds minimal number hidden units sufﬁces purpose section analyze maximal approximation errors crbms derive upper-bound minimal number hidden units sufﬁces approximate every conditional distribution within given error tolerance section investigates expressive power crbms subsections. section describe crbms represent natural families conditional distributions arise markov random ﬁelds. section study ability crbms approximate conditional distributions restricted supports. section addresses especially approximation deterministic conditional distributions section offer discussion outlook. order present main results concise deferred proofs appendices. nonetheless think proofs interesting right prepared fair amount detail. denote probability distributions probability distribution y∈{}n -dimensional simplex denote conditional distributions variable given another variable ∆kn. conditional distribution row-stochastic matrix rows k-dimensional polytope rk×n. regarded k-fold cartesian product probability simplex possible input state abbreviation natural number. deﬁnition conditional restricted boltzmann machine input units output units hidden units denoted rbmk conditional distributions written collection restricted boltzmann machine probability models shared parameters. input output distribution probability distribution represented rbmnm parameters interaction weights biases visible units differ biases hidden units. joint behavior distributions shared parameters trivial. also regarded representing block-wise normalized versions joint probability distributions represented rbmn+km. namely joint distribution rbmn+km ∆k+n array entries conditioning section investigate three basic questions geometry crbm models. first dimension crbm model? second many hidden units crbm need order able approximate every conditional distribution arbitrarily well? third accurate approximations crbm depending number hidden units? model rbmk deﬁned marginalizing hidden units graphical model. implies several choices parameters represent conditional distributions. turn dimension representable conditional distributions smaller number model parameters principle. otherwise equal dimension ambient polytope conditional model said expected dimension. distributions dim. particular show holds practical cases number hidden units smaller exponential respect number visible units dimension parametric model given maximum rank jacobian parametrization computing rank jacobian easy general. resort compute rank limit large parameters corresponds considering piece-wise linearized version original model called tropical model. cueto used approach study dimension probability models. apply ideas order study dimension crbm conditional models. deﬁnition denote cardinality largest subset whose elements least hamming distance apart. denote smallest cardinality every element hamming distance apart set. identiﬁed joint distributions form strictly positive marginals particular ﬁxing marginal distribution obtain identiﬁcation subset ∆k+n. figure illustrates identiﬁcation case implies universal approximators joint probability distributions deﬁne universal approximators conditional distributions. know rbmn+km universal approximator whenever improves previous results younes maaten hand since conditional models need model input-state distribution principle possible rbmk universal approximator even rbmn+km universal approximator. fact obtain following improvement proposition follow corresponding results probability models fact model rbmk approximate every conditional distribution arbitrarily well whenever natural number satisfying functions tend approximately respectively tends inﬁnity. results signiﬁcant reduce bounds following universal approximation results probability models additive term order corresponds precisely order parameters needed order model input-state distributions. expected asymptotic behavior theorem’s bound exponential number input output units. lies nature universal approximation property. crude lower bound number hidden units sufﬁces universal approximation obtained comparing number parameters model dimension conditional polytope results presented highlight fact crbm universal approximation possible drastically smaller number hidden units universal approximation number visible units. however even reductions universal approximation property requires enormous number hidden units. order provide informative description approximation capabilities crbms next section investigate maximal approximation error decreases hidden units added model. practical perspective necessary approximate conditional distributions arbitrarily well fair approximations sufﬁce. especially important number required hidden units grows disproportionately quality approximation. section investigate maximal approximation errors crbms depending number hidden units. figure gives schematic illustration maximal approximation error conditional model. kullback-leibler divergence probability distributions ∆k+n given proposition implies universal approximation result proposition special case vanishing approximation error imply theorem way. taking speciﬁc properties conditional model account improve proposition obtain following theorem implies universal approximation result theorem special case vanishing approximation error. note following weaker practical version theorem plain terms results presented show worst case approximation errors crbms decrease least logarithm number hidden units. hand practice interested approximating possible conditional distributions special classes. expect crbms approximate certain classes conditional distributions better others. subject next section. section classes conditional distributions compactly represented crbms whether crbms approximate interesting conditional distributions using moderate number hidden units. ﬁrst part question familiar classes conditional distributions expressed terms crbms turn would allow compare crbms models develop intuitive picture deﬁnition second part question clearly depends speciﬁc problem hand. nonetheless classes conditional distributions considered generally interesting contain solutions instances certain classes problems. example class deterministic conditional distributions sufﬁces solve markov decision problem optimal way. section discuss ability crbms represent conditional markov random ﬁelds depending number hidden units have. main idea hidden unit used model pure interaction group visible units. idea appeared previous work younes context universal approximation. section continue discussion classes conditional distributions represented crbms depending number hidden units. focus hierarchy conditional distributions deﬁned total number input-output pairs positive probability. note ckn) ∆kn. vertices conditional distributions assign positive probability output given input called deterministic. carath´eodory’s theorem every element convex combination fewer deterministic conditional distributions. sets arise naturally context reinforcement learning partially observable markov decision processes namely every ﬁnite pomdp associated effective dimension dimension state processes generated stationary stochastic policies. mont´ufar showed policies represented conditional distributions sufﬁcient generate processes generated ∆kn. general effective dimension relative small much smaller policy search space ∆kn. result shows intuitive fact hidden unit used model probability input-output pair. since conditional distribution input-output probabilities completely determined probabilities interesting whether amount hidden units indicated proposition strictly necessary. below theorem show that indeed hidden units required modeling positions positive probability input-output pairs even speciﬁc values need modeled. note certain structures positive probability input-output pairs modeled fewer hidden units stated proposition simple example following direct generalization corollary following focus deterministic conditional distributions. particularly interesting simple class conditional distributions restricted supports. well known ﬁnite markov decision processes optimal policy deﬁned stationary deterministic conditional distribution furthermore showed always possible deﬁne simple two-dimensional manifolds approximate deterministic conditional distributions arbitrarily well. theorem model rbmk approximate every conditional distribution arbitrarily well represented feedforward network input units hidden layer linear threshold units output layer sigmoid units. approximate every deterministic conditional distribution arbitrarily well represented feedforward linear threshold network input hidden output units. representational power feedforward linear threshold networks studied intensively literature. example wenzel showed feedforward linear threshold network input hidden output units represent following parity function fparity indicator function union linearly separable subsets although crbms approximate rich class deterministic conditional distributions arbitrarily well next result shows number hidden units required universal approximation deterministic conditional distributions rather large theorem order approximate deterministic conditional distributions arbitrarily well crbm requires exponentially many hidden units respect number input units. paper gives theoretical description representational capabilities conditional restricted boltzmann machines relating model complexity model accuracy. crbms based well studied restricted boltzmann machine probability models. proved extensive series results generalize recent theoretical work representational power rbms non-trivial way. studied problem parameter identiﬁability. showed every crbm exponentially many hidden units represent conditional distributions dimension equal number model parameters. implies practical cases crbms waste parameters generically ﬁnitely many choices interaction weights biases produce conditional distribution. addressed classical problems universal approximation approximation quality. results show crbm hidden units approximate every conditional distribution output units given input units without surpassing kullback-leibler approximation error form thus model universal approximator whenever fact provided tighter bounds depending instance universal approximation property attained whenever proof based upper bound complexity algorithm packs boolean cubes sequences non-overlapping stars improvements possible. worth mentioning conditional distributions approximation error maximal small. largely open difﬁcult problem. note results plugged certain analytic integrals produce upper-bounds expectation value approximation error approximating conditional distributions drawn product dirichlet density polytope conditional distributions. future work would interesting extend considerations analysis crbm training complexity errors resulting non-optimal parameter choices. also studied speciﬁc classes conditional distributions represented crbms depending number hidden units. showed crbms represent conditional markov random ﬁelds using hidden unit model interaction group visible variables. furthermore showed crbms approximate binary functions input bits output bits arbitrarily well particular implies exponentially many deterministic conditional distributions approximated arbitrarily well crbm number hidden units exponential number input units. aligns well known examples functions cannot compactly represented shallow feedforward networks reveals intrinsic constraints crbm models prevent grossly over-ﬁtting. think developed techniques used studying conditional probability models well. particular future work would interesting compare representational power crbms combinations crbms feedforward nets also would interesting apply techniques study stacks crbms multilayer conditional models. finally although analysis focuses case binary units main ideas extended case discrete non-binary units. proof proposition joint distribution form marginals dimension shows ﬁrst statement. items follow directly corresponding statements probability model. proof theorem prove stronger statement condition appearing ﬁrst item relaxed following }k+n contains disjoint radius- hamming balls whose union contain form }k+n whose complement full afﬁne rank subset rk+n. proof based ideas developed studying probanm parametrization given deﬁnition dimennm maximum rank jacobian possible choices argmaxz∈{}m denote likely hidden state rbmk+nm given visible state depending parameter direct algebraic manipulations maximum rank jacobian bounded maximum dimension column-span matrix rows modulo vectors whose entries independent given kronecker product deﬁned ikjl. modulo operation effect disregarding input distribution joint distribution represented rbm. example ﬁrst block remove columns correspond without affecting mentioned column-span. summarizing maximal column-rank modulo vectors whose entries independent given lower bound dimension rbmk note depends discrete way; parameter space partitioned ﬁnitely many regions constant. piece-wise linear thus emerging linear pieces represented tropical crbm morphism image tropical crbm model. taking visible state vectors likely hidden state vectors. geometrically inference function corresponds slicings -dimensional unit hypercube. namely every hidden unit divides visible space }k+n rk+n halfspaces according consider linear classiﬁers select rows corresponding disjoint hamming balls radius rank equal number classiﬁers times aci) plus rank a{}k+n\\∪i∈ci column-rank modulo functions equal rank minus minus number cylinder sets contained ∪i∈ci. completes proof general statement ﬁrst item. example given ﬁrst item consequence following observations. cylinder contains points. given cylinder intersects radius- hamming ball contained also intersects radius- hamming sphere around choosing radius- hamming ball slicings centers least hamming distance apart ensure union contain cylinder proof corollary maximal cardinality distance- binary codes length largest integer varshamov l−log. furthermore minimal size radius covering codes length known l−log section contains proof theorem minimal size crbm universal approximators. proof constructive; given target conditional distribution proceeds adjusting weights hidden units successively obtaining desired approximation. idea proof hidden unit used model probability output vector several different input vectors. probability given output vector adjusted single hidden unit jointly several input vectors input vectors general position. comes cost generating dependent output probabilities inputs afﬁne space. main difﬁculty proof lies construction sequences successively conﬂict-free groups afﬁnely independent inputs estimating shortest possible length sequences exhausting possible inputs. proof composed several lemmas propositions. start deﬁnitions deﬁnition given probability distributions ﬁnite hadamard product renormalized entry-wise product probability distribution deﬁned p)q) building product assume supports disjoint normalization term vanish. probability distributions represented rbms described terms hadamard products. namely every probability distribution represented rbmnm model rbmnm+ additional hidden unit represent precisely probability distribution form mixture words additional hidden unit amounts hadamard-multiplying distributions representable distributions representable mixtures product distributions. result obtained considering hadamard products mixtures equal uniform distribution. case distributions form strictly positive product distribution λ+n)x weight deﬁnition radius- hamming ball consisting length-k binary vector immediate neighbors; zi}| denotes hamming distance deﬁnition r-dimensional cylinder length-k binary vectors arbitrary values coordinates ﬁxed values coordinates; geometric intuition simple cylinder corresponds vertices face unit cube radius- hamming ball corresponds vertices corner unit cube. vectors radius- hamming ball afﬁnely independent. figure illustration. order prove theorem want that given strictly positive conditional distribution exists rbmn+k probability sharing steps taking strictly positive joint distribution idea starting distribution represented hidden units sharing step realized adding hidden unit rbm. order obtain sequences sharing steps following technical lemma lemma radius- hamming ball cylinder subset containing center denote dirac delta assigning probability ∆k+n strictly positive probability distribution conditionals proof. deﬁne sharing step product distribution supported {˜y} }k+n. note given distribution radius- hamming ball whose center contained product distribution )x∈c∩b )x∈c∩b. words restriction product distribution radius- hamming ball made proportional non-negative vector length |b|. factor distributions hence restriction given vector where without loss generality chose centered choosing factor distributions appropriately vector proof. consider show probability distribution written transformation dirac delta sharing steps. claim follows lemma enumeration starting distribution t-th sharing step deﬁned sharing steps obtain transforming -rows according lemma -rows transformed well non-trivial dependent way. fortunately sharing step allows reset exactly certain rows desired point measure without introducing non-trivial dependencies observations made above construct algorithm generates arbitrarily accurate approximation given conditional distribution applying sequence sharing algorithm algorithmic illustration proof theorem algorithm performs sequential sharing steps strictly positive joint distribution ∆k+n resulting distribution denotes inputs readily processed current iteration. steps given strictly positive joint distribution. denote star intersection radius- hamming ball cylinder containing center ball. figure details algorithm given algorithm approximate given target conditional distribution arbitrarily well need evaluate number sharing steps algorithm purpose investigate combinatorics sharing step sequences evaluate worst case lengths. choose starting distribution small enough depending target conditional targeted approximation accuracy deﬁnition sequence stars packing property smallest cylinder containing stars sequence intersect previous star sequence called star packing sequence number sharing steps algorithm bounded times length star packing sequence inputs note choices stars lengths possible star packing sequences unique. figure gives example showing starting sequence large stars necessarily best strategy produce short sequence. next lemma states class star packing sequences certain length depending size input space. thereby lemma upper-bounds worst case complexity algorithm figure examples radius- hamming balls cylinder sets dimension cylinder sets shown bold vertices connected dashed edges nested hamming balls bold vertices connected solid edges. three examples star packing sequences illustration star packing sequence constructed lemma proof. star packing sequence constructed following procedure. step deﬁne cylinder sets packing sites covered stars include sub-star cylinder sets sequence. initialization step split s-dimensional cylinder sets denoted k−s}. ﬁrst step s-dimensional cylinder packed rdimensional cylinder sets deﬁne star radius- hamming ball within centered smallest element include sequence. point sites covered stars split s-dimensional cylinder sets denote note cylinder hence )-rows conditional distribution processed algorithm jointly reset single sharing step achieve ∪jd. second step cylinder packed -dimensional cylinder sets corresponding stars included sequence. procedure iterated r-th step. step -dimensional cylinder packed single -dimensional cylinder hence point exhausted procedure terminates. summarizing procedure initialized creating branches ﬁrst step branch produces stars splits branches generally i-th step branch produces stars splits branches total number stars given precisely times value iterative function fr)) whereby total number resets given i∈). figure offers illustration star packing sequences. ﬁgure shows case case initial branch stars shown solid blue dashed dotted green. clarity stars highlighted. stars resulting split branches similar translated versions highlighted ones. order make universal approximation bound comprehensible table evaluated sequence furthermore next proposition gives explicit expression coefﬁcients appearing bound. yields second part theorem general bound decreases increasing except possibly values small. pair sufﬁcient number hidden units obtaining universal approximator. remark want model restricted class conditional distributions adapting algorithm restrictions yield tighter bounds number hidden units sufﬁces represent restricted conditionals. example want model target conditionals inputs subset care algorithm need replace case cylinder packing understood collection disjoint cylinder sets ∪i∈ci furthermore cylinder corresponding star conditionals common support ci-rows reset distribution |t|− sharing steps needed transform distribution whose conditionals approximate desired accuracy. particular class target conditional distributions supp term complexity bound algorithm replaced proposition follows simple parameter counting arguments. order make rigorous ﬁrst make observation universal approximation probability distributions boltzmann machines models based exponential families without hidden variables requires number model parameters large dimension approximated. denote conditionals inputs form ﬁnite outputs ﬁnite accordingly denote probability distributions lemma ﬁnite sets. deﬁned conditionals marginal ∆x×y exponential family ∆x×y×z. universal approximator conditionals |x|. intuition lemma that models deﬁned marginals exponential families conditionals approximated arbitrarily well essentially equal conditionals represented exactly implying low-dimensional universal approximators type. proof lemma consider ﬁrst case probability distributions; case image exponential family differentiable closure consists distributions approximated arbitrarily well compact set. since continuous image also compact model universal approximator ﬁnite union exponential families; exponential family possible support distributions point critical point sard’s theorem mapped measure zero hence ﬁnite union measure zero general case note universal approximator joint model ∆x×y universal approximator. latter marginal exponential family ∆x×y×z. hence claim follows ﬁrst part. proof proposition universal approximator conditionals model consisting probability distributions form expw universal approximator probability distributions ∆k+n. latter marginal exponential family dimension thus lemma k+n−k−n approximate certain products partition models arbitrarily well proposition integer model rbmk arbitrarily well whenever k−sf proof. analogous proof proposition differences. element corresponds cylinder collection cylinder sets partition algorithm slightly different sharing steps deﬁned uniform distribution cylinder corresponding proof. proof follows closely arguments presented consider visible units hidden units. consider joint distribution exp) fully observable deﬁned follows. label hidden units subsets denote largest element maximal element cardinality one. involves choosing appropriate corresponding remaining polynomial consists terms strictly smaller monomials. apply lemma repeatedly polynomial monomials remain. terms canceled bsxs lemma consider function model rbmk approximate deterministic policy arbitrarily well whenever represented feedforward linear threshold network hidden units; generic choice proof. consider conditional distribution visible marginal expz consider weights large enough argmaxzz argmaxzz βb)y note generic choices argmaxzz consists single point argmaxz βb)y βb)y). here again generic choices argmaxyz∗ βb)y consists single point hsz∗ joint distribution parameters tends point measure case tends hsz∗ proof theorem second statement precisely lemma general statement arguments follows. note conditional distribution output units given hidden units crbm feedforward network version. proof theorem builds following lemma describes combinatorial property deterministic policies approximated arbitrarily well crbms. recall heaviside step function maps real number lemma consider function model rbmk approximate deterministic policy arbitrarily well choice model parameters proof. consider choice input state conditional represented mixture comporbmk nents expw b)y) mixture weights expw support mixture distribution equal union supports mixture components non-zero mixture weights. present case choosing small enough made arbitrarily small ﬁxed case every necessarily bits function form rm×n lemma shows deterministic policy approximated rbmk arbitrarily well corresponds y-coordinate ﬁxed points deﬁned composition linear threshold functions }k+n particular upper bound number deterministic policies approximated arbitrarily well rbmk total number compositions linear threshold functions; inputs outputs inputs outputs. number linear threshold functions inputs outputs. number deterministic policies approximated arbitrarily well rbmk thus bounded ltf· m+nm. actual number much smaller view ﬁxed-point shared parameter constraints. hand number claim follows comparing deterministic policies large numbers.", "year": 2014}