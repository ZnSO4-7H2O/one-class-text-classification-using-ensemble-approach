{"title": "Semantic Specialisation of Distributional Word Vector Spaces using  Monolingual and Cross-Lingual Constraints", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We present Attract-Repel, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialised cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialised vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual DST models, which brings further performance improvements.", "text": "present attract-repel algorithm improving semantic quality word vectors injecting constraints extracted lexical resources. attract-repel facilitates constraints monocrosslingual resources yielding semantically specialised cross-lingual vector spaces. evaluation shows method make existing cross-lingual lexicons construct highquality vector spaces plethora different languages facilitating semantic transfer highlower-resource ones. effectiveness approach demonstrated state-ofthe-art results semantic similarity datasets languages. next show attractrepel-specialised vectors boost performance downstream task dialogue state tracking across multiple languages. finally show cross-lingual vector spaces produced algorithm facilitate training multilingual models brings performance improvements. word representation learning become research area central importance modern natural language processing. common techniques inducing distributed word representations grounded distributional hypothesis relying co-occurrence information large textual corpora learn meaningful word representations recently methods beyond stand-alone unsupervised learning gained increased popularity. models typically build distributional ones using humanautomatically-constructed knowledge bases enrich semantic content existing word vector collections. often done postprocessing step distributional word vectors reﬁned satisfy constraints extracted lexical resource wordnet term approach semantic specialisation. paper advance semantic specialisation paradigm number ways. introduce algorithm attract-repel uses synonymy antonymy constraints drawn lexical resources tune word vector spaces using linguistic information difﬁcult capture conventional distributional training. evaluation shows attract-repel outperforms previous methods make similar lexical resources achieving state-of-the-art results word similarity datasets simlex- simverb- deploy attract-repel algorithm multilingual setting using semantic relations extracted babelnet cross-lingual lexical resource inject constraints words different languages word representations. allows embed vector spaces multiple languages single vector space exploiting information high-resource languages improve word representations lower-resource ones. table illustrates effects cross-lingual attract-repel specialisation showing nearest neighbours three english words across three cross-lingual spaces. considerable amount prior research joint learning cross-lingual vector spaces best knowledge ﬁrst apply semantic specialisation problem. demonstrate efﬁcacy state-of-theart results four languages multilingual simlex- dataset show approach yields semantically informative vectors lower-resource languages collect intrinsic evaluation datasets hebrew croatian show cross-lingual specialisation signiﬁcantly improves word vector quality low-resource languages. second part paper explore attract-repel-specialised vectors downstream application. important motivation training word vectors improve lexical coverage supervised models language understanding tasks e.g. question answering textual entailment some residual effects distributional hypothesis persist. example nl_krieken dutch cherries identiﬁed synonym en_morning song called morning wish’ emile krieken. approach suited languages lexical resources exist. however many languages coverage cross-lingual lexicons. instance babelnet automatically aligns wordnet wikipedia providing accurate cross-lingual mappings languages. evaluation demonstrate substantial gains hebrew croatian spoken less million people worldwide. work task dialogue state tracking extrinsic evaluation. task arises construction statistical dialogue systems involves understanding goals expressed user updating system’s distribution goals conversation progresses information becomes available. show incorporating specialised vectors state-of-the-art neural-network model improves performance english dialogues. multilingual spirit paper produce italian german datasets show using attract-repel-specialised vectors leads even stronger gains languages. finally show cross-lingual vectors used train single model performs three languages case outperforming monolingual model. best knowledge ﬁrst work multilingual training component statistical dialogue system. results indicate multilingual training holds great promise bootstrapping language understanding models languages especially dialogue domains data collection resource-intensive. available www.github.com/nmrksic/ attract-repel. include attractrepel source code; bilingual word vector collections combining english languages; hebrew croatian intrinsic evaluation datasets; italian german dialogue state tracking datasets collected work. related work semantic specialisation usefulness distributional word representations demonstrated across many application areas part-of-speech tagging machine translation dependency semantic parsing sentiment analysis named entity recognition many others. importance semantic specialisation downstream tasks relatively unexplored improvements performance observed dialogue state tracking spoken language understanding judging lexical entailment semantic specialisation methods fall categories train distributed representations ‘from scratch’ combining distributional knowledge lexical information; inject lexical information pre-trained collections word vectors. methods categories make similar lexical resources; common examples include wordnet framenet paraphrase databases learning scratch methods modify prior regularization original training procedure using linguistic constraints ones modify skip-gram objective function introducing semantic constraints train word vectors emphasise word similarity relatedness. osborne propose method incorporating prior knowledge canonical correlation analysis method used dhillon learn spectral word embeddings. methods introduce semantic similarity constraints extracted lexicons approaches proposed schwartz symmetric patterns push away antonymous words pattern-based vector space. combine approaches using thesauri distributional data train embeddings specialised capturing antonymy. faruqui dyer many different lexicons create interpretable sparse binary vectors achieve competitive performance across range intrinsic evaluation tasks. theory word representations produced models consider distributional lexical information jointly could good representations produced ﬁne-tuning distributional vectors. however performance surpassed ﬁne-tuning methods. fine-tuning pre-trained vectors rothe schütze ﬁne-tune word vector spaces improve representations synsets/lexemes found wordnet. faruqui jauhar synonymy constraints procedure termed retroﬁtting bring vectors semantically similar words close together wieting modify skip-gram objective function ﬁne-tune word vectors injecting paraphrasing constraints ppdb. mrkši´c build retroﬁtting approach jointly injecting synonymy antonymy constraints; idea reassessed nguyen expand line work incorporating semantic intensity information constraints recski ensembles rich concept dictionaries improve combined collection semantically specialised word vectors. attract-repel instance second family models providing portable light-weight approach incorporating external knowledge arbitrary vector spaces. experiments show attract-repel outperforms previously proposed post-processors setting state-of-art performance widely used simlex- word similarity dataset. moreover show starting distributional vectors allows method existing cross-lingual resources distributional vector spaces different languages uniﬁed vector space beneﬁts positive semantic transfer constituent languages. cross-lingual word representations existing models induce cross-lingual word representations rely cross-lingual distributional information models differ cross-lingual signal/supervision languages uniﬁed bilingual vector spaces models learn basis parallel word-aligned data sentencealigned data ones require documentaligned data learn basis available bilingual dictionaries upadhyay vuli´c korhonen overview cross-lingual word embedding work. inclusion cross-lingual information results shared cross-lingual vector spaces boost performance monolingual tasks word similarity support cross-lingual tasks bilingual lexicon induction cross-lingual information retrieval transfer learning resource-lean languages however prior work cross-lingual word embedding tended exploit pre-existing linguistic resources babelnet. work make cross-lingual constraints derived repositories induce high-quality cross-lingual vector spaces facilitating semantic transfer highlower-resource languages. experiments show cross-lingual vector spaces produced attract-repel consistently outperform representative selection strong cross-lingual word embedding models intrinsic extrinsic evaluation across several languages. jecting similarity antonymy constraints distributional vector spaces. procedure term attract-repel builds paragram counter-ﬁtting procedures inject linguistic constraints existing vector spaces improve ability capture semantic similarity. vocabulary synonymous word pairs antonymous word pairs ease notation word pair sets correspond vector pair optimisation procedure operates mini-batches consists synonymy pairs antonymy pairs pairs negative word vectors present synonymy pair negative example pair chosen remaining in-batch vectors closest closest antonymy pair negative example pair chosen remaining in-batch vectors furthest away furthest negative examples used force synonymous pairs closer respective negative examples; force antonymous pairs away negative examples. ﬁrst term cost function pulls synonymous words together hinge loss function δsyn similarity margin determines much closer synonymous vectors respective negative examples. second part cost function pushes antonyaddition terms include additional regularisation term aims preserve abundance high-quality semantic content present initial vector space long information contradict injected linguistic constraints. word vectors present given mini-batch then comparison prior work attract-repel draws inspiration three methods retroﬁtting paragram counter-ﬁtting whereas retroﬁtting paragram consider antonymy counter-ﬁtting models synonymy antonymy. attract-repel differs method important ways context-sensitive updates counter-ﬁtting uses attract repel terms pull synonyms together push antonyms apart withconsidering relation word vectors. example ‘attract term’ given synonymy constraints δsyn similarity enforced between synonyms. conversely attract-repel ﬁne-tunes vector spaces operating minibatches example pairs updating word vectors position negative example implies stronger semantic relation expressed position target example. importantly attract-repel makes ﬁnegrained updates example pair regularisation counter-ﬁtting preserves distances pairs word vectors initial vector space trying ‘pull’ words’ neighbourhoods move incorporate external knowledge. radius initial neighbourhood introduces opaque hyperparameter procedure. conversely attract-repel implements standard regularisation ‘pulls’ vector towards distributional vector representation. intrinsic evaluation perform exhaustive comparison models showing attract-repel signiﬁcantly outperforms counterﬁtting monocross-lingual setups. optimisation following wieting adagrad algorithm train word embeddings epochs sufﬁces magnitude parameter updates converge. similar faruqui wieting mrkši´c early stopping. relying language-speciﬁc validation sets attract-repel procedure induce semantically specialised word vectors languages intrinsic evaluation datasets. hyperparameter tuning spearman’s correlation ﬁnal word vectors multilingual wordsim- gold-standard association dataset attract-repel procedure hyperparameters regularization constant λreg similarity antonymy margins δsim δant mini-batch sizes size ppdb constraint used language grid search four simlex languages choosing hyperparameters achieved best wordsim- score. many languages present semi-automatically constructed lexicons babelnet ppdb however intrinsic evaluation datasets simlex- exist languages require expert translators skilled annotators. grid search λreg δsim δant table linguistic constraint counts language pair ﬁgures show number injected synonymy antonymy constraints. monolingual constraints underlined. experimental setup distributional vectors ﬁrst present sixteen experimental languages english german italian russian dutch swedish french spanish portuguese polish bulgarian croatian irish persian vietnamese ﬁrst four languages multilingual simlex- dataset. four simlex languages employ four well-known high-quality word vector collections common crawl glove english vectors pennington german vectors vuli´c korhonen italian vectors dinu russian vectors kutuzov andreev addition languages also train skip-gram negative sampling variant wordvec model latest wikipedia dump language induce -dimensional word vectors. monolingual similarity employ multilingual paraphrase database resource contains paraphrases automatically extracted parallel-aligned corpora ppdb sizes four simlex languages. λreg δsim δant consistently achieved best performance ppdb constraint size best english german italian achieved best performance russian. frequency cut-off words occurred less frequently removed vocabularies. wordvec parameters standard values epochs negative samples global learning rate subsampling rate cross-lingual similarity employ babelnet multilingual semantic network automatically constructed linking wikipedia wordnet babelnet groups words different languages babel synsets. consider words language pair synonymous belong synonymous babel synsets. made babelnet word senses tagged conceptual ignored ones tagged named entities. given large collection cross-lingual semantic constraints attract-repel bring vector spaces different languages together shared cross-lingual space. ideally sharing information across languages lead improved semantic content language especially limited monolingual resources. antonymy babelnet also used extract monolingual cross-lingual antonymy constraints. following faruqui found ppdb constraints beneﬁcial wordnet ones babelnet monolingual synonymy. availability resources ppdb babelnet created automatically. however ppdb relies large high-quality parallel corpora europarl total multilingual ppdb provides collections paraphrases languages. hand babelnet uses wikipedia’s interlanguage links statistical machine translation provide cross-lingual mappings languages. evaluation show ppdb babelnet used jointly improve word representations lower-resource languages tying bilingual spaces high-resource ones. validate claim hebrew croatian ‘lower-resource’ languages lack ppdb resource relatively small wikipedia sizes. datasets spearman’s rank correlation simlex- dataset used intrinsic evaluation metric throughout experiments. unlike gold standard resources wordsim- simlex- consists word pairs scored annotators instructed discern semantic similarity conceptual association related nonsimilar words rating. leviant reichart translated simlex german italian russian crowd-sourcing similarity scores native speakers languages. resource multilingual intrinsic evaluation. investigate portability approach lower-resource languages used experimental setup collect simlex datasets hebrew croatian. english vectors also report spearman’s correlation simverb- semantic similarity dataset focuses verb pair similarity. experiments monolingual cross-lingual specialisation start distributional vectors simlex languages english german italian russian. language ﬁrst perform semantic specialisation spaces using monolingual synonyms; monolingual antonyms; combination both. cross-lingual synonyms antonyms constraints train shared fourlingual vector space languages. comparison baseline methods monocross-lingual specialisation performed using attract-repel counter-ﬁtting order conclusively determine methods exhibited superior performance. retroﬁtting paragram methods inject synonymy cost functions expressed using sub-components counter-ﬁtting attract-repel cost functions. such performance investigated methods make similarity constraints illustrates performance range preceding models. importance initial vectors three different sets initial vectors well-known distributional word vector collections distributional vectors trained latest wikipedia dumps; word vectors randomly initialised using xavier initialisation specialisation lower-resource languages experiment ﬁrst construct bilingual spaces combine four simlex languages; twelve languages. since pair contains least simlex language analyse improvement monolingual specialisation understand robust performance gains across different language pairs. next newly collected simlex datasets hebrew croatian evaluate extent bilingual semantic specialisation using attract-repel babelnet constraints improve word representations lower-resource languages. comparison state-of-the-art bilingual spaces english-italian english-german bilingual spaces induced attract-repel compared state-of-the-art methods constructing bilingual vector spaces retrained using constraints used model; latter models various sources supervision means cannot trained using sets constraints. models competitive setups proposed goal experiment show vector spaces induced attract-repel exhibit better intrinsic extrinsic performance deployed language understanding tasks. hyperparameters used δsim δant λreg achieved best performance tuned original simlex languages. largest available ppdb size used languages available ppdb word vectors monolingual distributional vectors counter-fitting mono-syn counter-fitting mono-ant counter-fitting mono-syn mono-ant counter-fitting cross-syn counter-fitting mono-syn cross-syn counter-fitting mono-syn mono-ant cross-syn cross-ant attract-repel mono-syn attract-repel mono-ant attract-repel mono-syn mono-ant attract-repel cross-syn attract-repel mono-syn cross-syn attract-repel mono-syn mono-ant cross-syn cross-ant table multilingual simlex-. effect using counter-fitting attract-repel procedures inject monocross-lingual synonymy antonymy constraints four collections distributional word vectors. best results state-of-the-art performance four languages. results discussion table shows effects monolingual crosslingual semantic specialisation four well-known distributional vector spaces simlex languages. monolingual specialisation leads strong improvements simlex performance across languages. cross-lingual specialisation brings improvements languages beneﬁting sharing cross-lingual vector space. italian particular shows strong evidence effective transfer italian vectors’ performance coming close top-performing english ones. comparison baselines table gives exhaustive comparison attract-repel counterﬁtting attract-repel achieved substantially stronger performance experiments. believe results conclusively show ﬁnegrained updates regularisation employed attract-repel present better alternative context-insensitive attract/repel terms pair-wise regularisation employed counter-ﬁtting. state-of-the-art wieting note hyperparameters widely used paragram-sl vectors tuned simlex comparable methods holdout dataset. implies work uses vectors starting point yield meaningful high scores either. reported english score multilingual simlex- corresponds original simlex- outperforms score reported wieting sets high score dataset. similarly simverb- score vectors outperforming current state-of-the-art score reported gerz starting distributional spaces table repeats previous experiment different sets initial vector spaces randomly initialised word vectors; skip-gram negative sampling vectors trained latest wikipedia dumps. randomly initialised vectors serve decouple impact injecting external knowledge information embedded distributional vectors. random vectors beneﬁt monocrosslingual specialisation english performance surprisingly strong languages suffering lack initialisation. table simlex- performance. tying simlex languages bilingual vector spaces different languages. ﬁrst number represents monolingual specialisation. bilingual spaces improved baselines. en-fr vectors high score original simlex-. table bilingual semantic specialisation hebrew croatian; original simlex languages. shows simlex scores language improve distributional vectors tied bilingual vector spaces four high-resource languages. comparing distributional vectors trained wikipedia high-quality word vector collections used table italian russian vectors particular start substantially weaker simlex scores. difference performance largely mitigated semantic specialisation. however vector spaces still exhibit weaker performance compared table believe shows quality initial distributional vector spaces important large part compensated semantic specialisation. bilingual specialisation table shows effect combining four original simlex languages twelve languages bilingual specialisation substantially improves monolingual specialisation language pairs. indicates improvements language independent large extent. interestingly even though monolingual synonymy constraints right-most languages combining simlex languages still improved word vector quality four highresource languages. reason even resourcedeprived languages irish help improve vector space quality high-resource ones english italian provide implicit indicators lower-resource languages previous experiment indicates bilingual specialisation improves high-quality estimates highresource languages. however little show much word vectors lower-resource languages improve specialisation. table investigates proposition using newly collected simlex datasets hebrew croatian. tying distributional vectors languages crosslingual spaces high-resource ones leads substantial improvements. table also shows distributional vectors four simlex languages improve tied languages hebrew croatian exhibit similar trends original simlex languages tying english italian leads stronger gains tying morphologically sophisticated german russian. indeed tying english consistently lead strongest performance. believe shows bilingual attract-repel specialisation english promises produce highquality vector spaces many lower-resource languages coverage among babelnet languages existing bilingual spaces table compares intrinsic performance bilingual english-italian english-german vectors produced attract-repel previously proposed approaches constructing bilingual vector table comparison intrinsic quality bilingual spaces produced attract-repel method produced state-of-the-art methods constructing bilingual vector spaces. spaces. languages language pairs attract-repel achieves substantial gains methods. next section show differences intrinsic performance lead substantial gains downstream evaluation. task-oriented dialogue systems help users achieve goals making travel reservations ﬁnding restaurants. slot-based systems application domains deﬁned ontologies enumerate goals users express goals expressed slot-value pairs modular task-based systems dialogue state tracking component charge maintaining belief state system’s internal distribution possible states dialogue. figure shows correct dialogue state turn example dialogue. unseen data/labels dialogue ontologies large many possible class labels occur training set. overcome problem delexicalisation-based models replace occurrences ontology values generic tags facilitate transfer learning across different ontology values. done exact matching supplemented semantic lexicons encode rephrasings morphology linguistic variation. instance lexicons would required deal underlined non-exact matches figure user suggest something fancy. system sure where? user downtown. korean places? system sorry korean places centre. user japanese? system sticks’n’sushi meets criteria. figure annotated dialogue states sample dialogue. underlined words show rephrasings ontology values typically handled using semantic dictionaries. exact matching bottleneck semantic lexicons hand-crafted small dialogue domains. mrkši´c showed semantically specialised vector spaces used automatically induce lexicons simple dialogue domains. however domains grow sophisticated reliance semantic dictionaries list potential rephrasings ontology values becomes bottleneck deploying dialogue systems. ambiguous rephrasings problematic instance approach user asking iceland could referring country supermarket chain someone asking songs train interested train timetables. importantly english principal language dialogue systems research understates challenges complex linguistic phenomena present languages. work investigate extent semantic specialisation empower models rely dictionaries. neural belief tracker novel model operates purely distributed representations words learning compose utterance context representations uses decide potentially many ontologydeﬁned intents expressed user overcome data sparsity problem uses label embedding decompose multi-class classiﬁcation problem many binary classiﬁcation ones slot model iterates slot values deﬁned ontology deciding whether expressed current utterance surrounding context. ﬁrst layer consists neural networks produce distributed representations user utterance preceding system output embedded label candidate slot-value pair. representations passed downstream semantic decoding context modelling networks subsequently make binary decision regarding current slot-value candidate. contradicting goals detected model chooses probable one. training procedure keeps initial word vectors ﬁxed test time unseen words semantically related familiar slot values recognised purely position original vector space. thus essential deployed word vectors specialised semantic similarity distributional effects keep antonymous words’ vectors together detrimental performance multilingual dataset evaluation based dataset introduced mrkši´c dataset based ontology used challenge consists wizard-of-oz dialogues amazon mechanical turk users assumed role dialogue system caller looking restaurants cambridge since users typed instead using speech interacted intelligent assistants language used sophisticated case dstc users would quickly adapt system’s inability cope complex queries. experiments ontology dialogues translated italian german gengo.com web-based human translation platform. intrinsic downstream evaluation monocross-lingual semantic specialisation improves semantic content word vector collections according intrinsic evaluation would expect model perform higherquality belief tracking improved vectors deployed. investigate difference performance english german italian model employs following word vector collections distributional word vectors; monolingual semantically specialised vectors; monolingual subspaces cross-lingual semantically specialised en-de-it-ru vectors. language also compare performance achieved using state-of-the-art bilingual vector spaces compared sect. training multilingual model values expressed domain ontology language independent. assume common semantic grounding across languages decouple ontologies dialogue corpora single ontology across languages. since know high-performing english attainable ground italian german ontologies original english ontology. single ontology coupled cross-lingual vectors allows combine training data multiple languages train single model capable performing belief tracking across three languages once. given high-quality cross-lingual vector space combining languages effectively increases training size therefore lead improved performance across languages. vector spaces. subsequent three rows show performance distributional vector spaces; monolingual specialisation; ende-it-ru cross-lingual specialisation. last shows performance multilingual model trained using ontology grounding training data three languages combined used train improved model. figure investigates usefulness ontology grounding bootstrapping models languages less data ﬁgures display italian german performance models trained using different proportions in-language training dataset. topperforming dash-dotted curve shows performance model trained using language-speciﬁc dialogues english training data. results table show types specialisation improve performance achieved using distributional vectors baseline bilingual spaces. interestingly bilingual vectors vuli´c korhonen outperform despite weaker simlex performance showing intrinsic evaluation capture relevant aspects pertaining word vectors’ usability downstream tasks. en-it/en-de en-it/en-de en-it/en-de en-it/en-de en-it/en-de monolingual distributional vectors monolingual specialisation cross-lingual specialisation english ontology grounding table model accuracy across three languages. ﬁgure shows performance model trained using subspace given vector space corresponding target language. english baseline ﬁgures show stronger en-it en-de ﬁgures. provements particularly large gains lowdata scenario investigated figure ﬁgure also shows difference performance monocrosslingual vectors substantial. again large disparity simlex scores induced minor improvements performance. summary results show that semantically specialised vectors beneﬁt performance; large gains simlex scores always induce large downstream gains; high-quality crosslingual spaces facilitate transfer learning languages offer effective method bootstrapping models lower-resource languages. finally german performance substantially weaker english italian corroborating intuition linguistic phenomena cases compounding make german challenging. release datasets hope multilingual evaluation give community tool evaluating downstream performance vector spaces morphologically richer languages. presented novel attract-repel method injecting linguistic constraints word vector space representations. procedure semantically specialises word vectors jointly injecting monocross-lingual synonymy antonymy constraints creating uniﬁed cross-lingual vector spaces achieve state-of-the-art performance well-established simlex- dataset multilingual variants. next shown attractrepel induce high-quality vectors lowerresource languages tying bilingual vector spaces high-resource ones. also demonstrated substantial gains intrinsic evaluation translate gains downstream task dialogue state tracking release novel non-english datasets finally shown semantically rich cross-lingual vectors facilitate language transfer providing effective method bootstrapping belief tracking models languages. work results especially emphasise need improving vector space models morphologically rich languages. moreover intrinsic task-based experiments exposed discrepancies between conclusions drawn types evaluation. consider major directions future work. authors would like thank anders johannsen help extracting babelnet constraints. would also like thank action editor sebastian padó anonymous tacl reviewers constructive feedback. ivan vuli´c reichart anna korhonen supported consolida-", "year": 2017}