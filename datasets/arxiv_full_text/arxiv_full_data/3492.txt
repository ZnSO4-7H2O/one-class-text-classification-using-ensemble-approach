{"title": "Learning to Compose Domain-Specific Transformations for Data  Augmentation", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.", "text": "data augmentation ubiquitous technique increasing size labeled training sets leveraging task-speciﬁc data transformations preserve class labels. often easy domain experts specify individual transformations constructing tuning sophisticated compositions typically needed achieve state-of-the-art results time-consuming manual task practice. propose method automating process learning generative sequence model user-speciﬁed transformation functions using generative adversarial approach. method make arbitrary non-deterministic transformation functions robust misspeciﬁed user input trained unlabeled data. learned transformation model used perform data augmentation discriminative model. experiments show eﬃcacy approach image text datasets achieving improvements accuracy points cifar- points relation extraction task accuracy points using domain-speciﬁc transformation operations medical imaging dataset compared standard heuristic augmentation approaches. modern machine learning models deep neural networks billions free parameters accordingly require massive labeled data sets training. settings labeled data available suﬃcient quantities avoid overﬁtting training set. technique artiﬁcially expanding labeled training sets transforming data points ways preserve class labels known data augmentation quickly become critical eﬀective tool combatting labeled data scarcity problem. data augmentation seen form weak supervision providing practitioners leverage knowledge invariances task domain. indeed data augmentation cited essential nearly every state-of-the-art result image classiﬁcation becoming increasingly common modalities well even well studied benchmark tasks however choice data augmentation strategy known cause large variances performance diﬃcult select papers often reporting heuristically found parameter ranges practice often simple formulate large primitive transformation operations time-consuming diﬃcult parameterizations compositions needed state-of-the-art results. particular many transformation operations vastly diﬀerent eﬀects based parameterization transformations applied with even particular order composition. example brightness saturation enhancements might destructive applied together produce realistic images paired geometric transformations. given diﬃculty searching conﬁguration space facto norm practice consists applying transformations random order random parameterizations selected hand-tuned ranges. recent lines work attempt automate data augmentation entirely either rely figure three examples transformation functions diﬀerent domains example sequences incremental image applied cifar- images conditional word-swap using externally trained language model speciﬁcally targeting nouns entity mentions relation extraction task unsupervised segementation-based translation applied mass-containing mammography images large quantities labeled data restricted sets simple transformations consider local perturbations informed domain knowledge contrast directly ﬂexibly leverage domain experts’ knowledge invariances valuable form weak supervision real-world settings labeled training data limited. paper present method data augmentation directly leverages user domain knowledge form transformation operations automates diﬃcult process composing parameterizing them. formulate problem learning generative sequence model black-box transformation functions user-speciﬁed operators representing incremental transformations data points need diﬀerentiable deterministic. example could rotate image small degree swap word sentence translate segmented structure image design generative adversarial objective allows train sequence model produce transformed data points still within data distribution interest using unlabeled data. stochastic non-diﬀerentiable present reinforcement learning-based training strategy model. learned model used perform data augmentation labeled training data discriminative model. given ﬂexibility representation data augmentation process apply approach many diﬀerent domains diﬀerent modalities including text images. real-world mammography image task achieve accuracy point boost randomly composed augmentation learning appropriately combine standard image domain-speciﬁc derived collaboration radiology experts. using novel language model-based boost heuristic augmentation text relation extraction task corpus. %-subsample cifar- dataset achieve accuracy point gain standard heuristic augmentation approach competitive comparable semi-supervised approaches. additionally show empirical results suggesting proposed approach robust misspeciﬁed tfs. hope proposed method practical value practitioners interest researchers open-sourced code https//github.com/hazyresearch/tanda. standard data augmentation setting expand labeled training leveraging knowledge class-preserving transformations. practitioner domain expertise providing individual transformations straightforward. however high performance augmentation techniques compositions ﬁnely tuned transformations achieve state-of-the-art results heuristically searching space possible compositions parameterizations task often infeasible. goal automate task learning compose parameterize user-speciﬁed transformation operators ways diverse still preserve class labels. method transformations modeled sequences incremental user-speciﬁed operations called transformation functions rather making strong assumption provided preserve class labels existing approaches assume weaker form class invariance enables figure high-level diagram method. users input transformation functions unlabeled data. generative adversarial approach used train null class discriminator generator produces sequences hτl. finally trained generator used perform data augmentation discriminative model unlabeled data learn generative model transformation sequences. propose representative model classes handle modeling commutative non-commutative transformations. augmentation sequence modeling approach represent transformations sequences incremental operations. setting user provides performs incremental transformation example could rotate image degrees swap word sentence move segmented tumor mass around background mammography image order accommodate wide range user-deﬁned treat black-box functions need deterministic diﬀerentiable. formulation gives tractable tune parameterization composition discretized ﬁne-grained manner. representation thought implicit binning strategy tuning parameterizations e.g. degree rotation might represented three applications ﬁve-degree rotation also provides direct represent compositions multiple transformation operations. critical multitude state-of-the-art results literature show importance using compositions transformations image also conﬁrm experimentally section weakening class-invariance assumption data augmentation technique fundamentally relies assumption transformation operations’ relation class labels. previous approaches make unrealistic assumption provided transformation operations preserve class labels data points. assumption puts large burden precise speciﬁcation user based observations violated many real-world data augmentation strategies. instead consider weaker modeling assumption. assume transformation operations classes might destructively data points distribution interest entirely represents out-of-distribution null class. intuitively weaker assumption motivated categorical image classiﬁcation setting observe transformation operations provided user almost never turn example plane often turn plane indistinguishable garbage image ﬁrst consider weaker invariance assumption believe figure modeling assumption transformations natural distribution interest rarely classes. demonstration take images cifar- randomly search transformation sequence best maps diﬀerent class according trained discriminative model. matches rarely resemble target class often longer look like normal images all. note consider ﬁxed user-provided adversarially selected ones. closely matches various practical data augmentation settings interest. section also provide empirical evidence weaker assumption useful binary classiﬁcation settings modalities image data. critically also enables learn model sequences using unlabeled data alone. minimizing null class mappings using unlabeled data given assumption objective learn model generates sequences indices ﬁxed length resulting sequences likely data points crucially involve using class labels data points unlabeled data. goal minimize probability generated sequence mapping unlabeled data points null class respect generative adversarial objective order approximate jointly train generator discriminative model using generative adversarial network objective minimizing respect maximizing respect standard setup training procedure viewed minimax game discriminator’s goal assign values transformed out-of-distribution data points high values real in-distribution data points simultaneously generator’s goal generate transformation sequences produce data points indistinguishable real data points according discriminator. black-box nature user-speciﬁed seems infeasible hard-code sets inverse operations avoid. mitigate this instead consider second objective term distance function. evaluated using distance input space feature space learned ﬁnal pre-softmax layer discriminator combining eqns. ﬁnal objective hyperparameter. minimize respect maximize respect independent model ﬁrst consider mean ﬁeld model sequential chosen independently. reduces task learning parameters think representing task-speciﬁc accuracies frequencies example might want learn elastic deformations swirls rarely applied images cifar- small rotations applied frequently. particular mean ﬁeld model also provides simple eﬀectively learning stochastic discretized parameterizations tfs. example representing ﬁve-degree rotations rotatedeg marginal value could thought roughly equivalent learning rotate degrees average. state-based model important cases however independent representation learned mean ﬁeld model could overly limited. many settings certain diﬀerent eﬀects depending applied them. example certain similar pairs image transformations might overly lossy applied together blur zoom operation brighten saturate operation. mean ﬁeld model could represent disjunctions these. another scenario independent model fails non-commutative lossy operators cases modeling sequences transformations could important. therefore consider long short-term memory network representative sequence model. output cell network distribution tfs. next sequence sampled distribution one-hot vector next cell network. core challenge face learning generates sequences necessarily diﬀerentiable deterministic. constraint critical facet approach usability perspective allows users easily write black-box scripts language choosing leveraging arbitrary subfunctions libraries methods. order work around constraint describe model syntax reinforcement learning provides convenient framework approaches handling computation graphs non-diﬀerentiable stochastic nodes reinforcement learning formulation index applied resulting incrementally transformed data point. consider state applied incremental tfs. note include incrementally transformed data points since stochastic. model classes considered uses diﬀerent state representation mean ﬁeld model state representation used ˆsmf lstm model ˆslstm lstm state update operation performed standard lstm cell parameterized policy gradient incremental rewards log) cumulative loss data point step practice equivalent using loss baseline term. next stochastic transition policy implictly deﬁned compute recurrent policy gradient objective following standard practice approximate quantity sampling batches data points sampled action sequences data point. also standard techniques discounting factor considering future rewards appendix details. heuristic data augmentation state-of-the-art image classiﬁcation pipelines limited form data augmentation generally consists applying crops ﬂips small aﬃne transformations ﬁxed order random parameters drawn randomly hand-tuned ranges. addition various studies applied heuristic data augmentation techniques modalities audio text reported literature selection augmentation strategies large performance impacts thus require extensive selection tuning hand interpolation-based techniques techniques explored generating augmented training sets interpolating labeled data points. example well-known smote algorithm applies basic technique oversampling class-imbalanced settings recent work explores using similar interpolation approach learned feature space proposes learning class-conditional model diﬀeomorphisms interpolating nearest-neighbor labeled data points perform augmentation. view approaches complementary orthogonal goal directly exploit user domain knowledge class-invariant transformation operations. adversarial data augmentation several lines recent work explored techniques viewed forms data augmentation adversarial respect classiﬁcation model. approaches transformation operations selected adaptively given order maximize loss classiﬁcation model trained procedures make strong assumption provided transformations preserve class labels bespoke models restricted sets operations another line recent work showed augmentation small adversarial linear perturbations regularizer complimentary work consider taking advantage non-local transformations derived user knowledge task domain invariances. finally generative adversarial networks recently made great progress learning complete data generation models unlabeled data. used augment labeled training sets well. class-conditional gans generate artiﬁcial data points require large sets labeled training data learn from. standard unsupervised gans used generate additional out-of-class data points augment labeled training sets compare proposed approach methods empirically section experimentally validate proposed framework learning augmentation models several benchmark real-world data sets exploring image recognition natural language understanding tasks. focus performance classiﬁcation models trained labeled datasets augmented approach others used practice. also examine robustness user misspeciﬁcation sensitivity core hyperparameters. datasets transformation functions benchmark image datasets experiments mnist cifar- datasets using subset class labels train classiﬁcation models treating rest unlabeled data. used generic mnist cifar- small rotations shears central swirls elastic deformations. also used morphologic operations mnist adjustments saturation contrast brightness cifar-. benchmark text dataset applied approach employment relation extraction subtask nist automatic content extraction corpus goal identify mentions employer-employee relations news articles. given standard class imbalance information extraction tasks like this used data augmentation oversample minority positive class. ﬂexibility representation allowed take straightforward novel approach data augmentation setting. constructed trigram language model using corpus reuters corpus volume sample word conditioned preceding words. used model basis select words swap based part-of-speech location relative entities interest mammography tumor-classiﬁcation dataset demonstrate eﬀectiveness approach real-world applications also considered task classifying benign versus malignant tumors images digital database screening mammography dataset class-balanced dataset consisting labeled mammograms. collaboration domain experts radiology constructed basic sets. ﬁrst consisted standard image transformation operations subselected break class-invariance mammography setting. example brightness operations excluded reason. second consisted ﬁrst well several novel segmentationbased transplantation tfs. utilized output unsupervised segmentation algorithm isolate tumor mass perform transformation operation rotation shifting stitch randomly-sampled benign tissue image. fig. illustrative example appendix details. classiﬁer performance evaluated approach using augment labeled training sets tasks mentioned above show achieve strong gains heuristic baselines. particular given evaluate performance mean ﬁeld lstm generators trained using approach standard data augmentation techniques used practice. ﬁrst consists applying random crops images performing simple minority class duplication relation extraction task. second standard heuristic approach applying random compositions given transformation operations common technique used practice approaches heur. additionally random cropping technique basic approach. present results table report test accuracy random subsample available labeled training data. additionally include extra ddsm task highlighting impact adding domain-speciﬁc segmentation-based operations described performance. architectures trained unlabeled data generate out-of-class data points augment labeled training set. following protocol cifar- train generator full unlabeled data discriminator disjoint random folds labeled training including validation averaging results. table test performance models trained subsamples labeled training data including validation splits using various data augmentation approaches. none indicates performance augmentation. tasks measured accuracy except measured score. settings train sequence generator full unlabeled data. select ﬁxed sequence length task initial calibration experiment ddsm tasks. note ﬁndings mirrored literature namely compositions multiple lead higher model accuracies. selected hyperparameters generator performance validation set. used trained generator transform entire training epoch classiﬁcation model training. mnist ddsm four-layer all-convolutional cifar -layer resnet bi-directional lstm. additionally incorporate basic transformation regularization term train last epochs without applying transformations cases hyperparameters reported literature. details generator model training appendix across applications studied approach outperforms heuristic data augmentation approach commonly used practice. furthermore lstm generator outperforms simple mean ﬁeld settings indicating value modeling sequential structure data augmentation. particular realize signiﬁcant gains standard heuristic data augmentation cifar- competitive comparable semi-supervised approaches signiﬁcantly smaller variance. also train cifar- model using full labeled training dataset strong relative gains coming within points current state-of-the-art using much simpler model. ddsm tasks also achieve strong performance gains showing ability method productively incorporate complex transformation operations domain expert users. particular ddsm observe addition segmentation-based causes heuristic augmentation approach perform signiﬁcantly worse large number failure modes resulting combinations segmentation-based gradient-based blending standard zoom rotate. contrast lstm model learns avoid destructive subsequences achieves highest score resulting point boost comparable heuristic approach. robustness misspeciﬁcation high-level goals approach enable easier interface users requiring specify completely class-preserving. lack assumption well-speciﬁed transformation operations approach strong empirical performance realized evidence robustness. additionally illustrate robustness approach misspeciﬁed train mean ﬁeld generator mnist using standard figure learned frequency parameters misspeciﬁed normal mnist. mean ﬁeld model correctly learns avoid misspeciﬁed tfs. larger sequence lengths lead higher model accuracy cifar- random performs best shorter sequences according sequence length calibration experiment. presented method learning parameterize compose user-provided black-box transformation operations used data augmentation. approach able model arbitrary allowing practitioners leverage domain knowledge ﬂexible simple manner. training generative sequence model speciﬁed transformation functions using reinforcement learning gan-like framework able generate realistic transformed data points useful data augmentation. demonstrated method yields strong gains standard heuristic approaches data augmentation range applications modalities complex domain-speciﬁc transformation functions. many possible future directions research learning data augmentation strategies proposed model conditioning generator’s stochastic policy featurized version data point transformed generating sequences dynamic length. broadly excited formalizing data augmentation novel form weak supervision allowing users directly encode domain knowledge invariants machine learning models. acknowledgements would like thank daniel selsam ioannis mitliagkas christopher william hamilton daniel rubin valuable feedback conversations. gratefully acknowledge support defense advanced research projects agency simplex program n--c darpa program fa--- darpa programs fa--- fa--- national institute health oﬃce naval research awards moore foundation okawa research grant american family insurance accenture toshiba intel. research also supported part aﬃliate members supporters stanford dawn project intel microsoft teradata vmware. material based research sponsored darpa agreement number fa---. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views policies endorsements either expressed implied darpa afrl u.s. government.", "year": 2017}