{"title": "Fast and Effective Algorithms for Symmetric Nonnegative Matrix  Factorization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Symmetric Nonnegative Matrix Factorization (SNMF) models arise naturally as simple reformulations of many standard clustering algorithms including the popular spectral clustering method. Recent work has demonstrated that an elementary instance of SNMF provides superior clustering quality compared to many classic clustering algorithms on a variety of synthetic and real world data sets. In this work, we present novel reformulations of this instance of SNMF based on the notion of variable splitting and produce two fast and effective algorithms for its optimization using i) the provably convergent Accelerated Proximal Gradient (APG) procedure and ii) a heuristic version of the Alternating Direction Method of Multipliers (ADMM) framework. Our two algorithms present an interesting tradeoff between computational speed and mathematical convergence guarantee: while the former method is provably convergent it is considerably slower than the latter approach, for which we also provide significant but less stringent mathematical proof regarding its convergence. Through extensive experiments we show not only that the efficacy of these approaches is equal to that of the state of the art SNMF algorithm, but also that the latter of our algorithms is extremely fast being one to two orders of magnitude faster in terms of total computation time than the state of the art approach, outperforming even spectral clustering in terms of computation time on large data sets.", "text": "symmetric nonnegative matrix factorization models arise naturally simple reformulations many standard clustering algorithms including popular spectral clustering method. recent work demonstrated elementary instance snmf provides superior clustering quality compared many classic clustering algorithms variety synthetic real world data sets. work present novel reformulations instance snmf based notion variable splitting produce fast eﬀective algorithms optimization using provably convergent accelerated proximal gradient procedure heuristic version alternating direction method multipliers framework. algorithms present interesting tradeoﬀ computational speed mathematical convergence guarantee former method provably convergent considerably slower latter approach also provide signiﬁcant less stringent mathematical proof regarding convergence. extensive experiments show eﬃcacy approaches equal state snmf algorithm also latter algorithms extremely fast orders magnitude faster terms total computation time state approach outperforming even spectral clustering terms computation time large data sets. graph-based clustering approaches data points treated individual nodes graph whose edges weighted using similarity function. weights stored symmetric adjacency matrix whose entry denotes similarity data points common approach separating graph clusters approximate factorization nonnegative matrix. data point assigned cluster index largest entry factorization obtained solving symmetric nonnegative matrix factorization problem deﬁned denotes frobenius norm nonnegativity constraint taken element-wise. snmf shown superior clustering eﬃcacy compared number data-clustering graph-clustering algorithms moreover slight variations snmf formulation shown equivalent variety clustering algorithms including k-means nonnegative matrix factorization well several normalized spectral clustering approaches eﬃcacy myriad connections powerful clustering approaches eﬃcient algorithms solving snmf model particular value practicing data-miner. however array algorithms developed parent problem nonnegative matrix factorization nmf-related problems specialized algorithms developed solving fact snmf problem fewer variables solve standard formulation challenging solve forced equality matrix factors. paper develop novel algorithms snmf based notion variable splitting combined classic approaches constrained numerical optimization namely quadratic penalty method solution provably convergent accelerated proximal gradient method heuristic form alternating direction method multipliers framework algorithms typically outperform current state method developed terms clustering eﬃcacy latter algorithm additionally runs average orders magnitude faster terms computation time medium-sized data sets consisting several thousand data points even outperform spectral clustering terms time larger data sets. remainder paper organized follows next section brieﬂy review popular datagraph-based clustering approaches well current state algorithm solving standard snmf model section introduce derive proposed algorithms based notion variable splitting. discuss computation time complexity fast admm based algorithm compare state newton-like procedure section ﬁfth section contains experiments synthetic real data illustrate eﬃcacy extreme eﬃciency proposed approaches. conclude brief reﬂections section appendix work contains critical mathematical details regarding ﬁrst approach well strong mathematical proof regarding convergence second algorithm. section review state data graph-based clustering approaches. addition highlight many important connections exist wide array techniques illustrates snmf problem interest work relates methods framework matrix factorization. spectral clustering immensely popular graph-based clustering approach groups given data points spectral analysis graph laplacian matrix associated input adjacency matrix laplacian given matrix symmetric positive semi-deﬁnite hence diagonalized orthogonal basis eigenvectors stacked columnwise form closed form solution unconstrained symmetric matrix factorization problem spectral clustering framework data partitioned clusters eigenvectors stacked column-wise matrix. ﬁnal clustering assignments made performing k-means rows matrix popular normalized version spectral clustering replaces normalized version given denotes diagonal matrix whose entries square root corresponding entries inverse follows strategy assigning data points respective clusters. simple adjustment spectral clustering works signiﬁcantly better practice additionally mentioned introduction section normalized version close connections kernelized k-means snmf matrix matrix constrained nonnegative. employed clustering applications matrix typically contains data hypothesized number clusters data lies number columns matrices data point assigned cluster index largest entry recovered matrix. directly related problem dictionary learning popular signal processing machine learning communities references therein) factorization desired coeﬃcient matrix constrained sparse. furthermore dictionary learning problems thought variations basic k-means paradigm column corresponds individual centroid location point’s centroid assignment elementary snmf problem matrix adjacency matrix shown eﬀective graph-based clustering applications. using array synthetic real data sets several works shown algorithms solve snmf problem produce superior clustering quality compared standard algorithms including normalized spectral clustering discussed section k-means well steplength tuned iteration standard adaptive procedure ensure descent step positive part operator sets negative entries input zero. always case newton approximation schemes ﬁner approximates true hessian becomes accurate) rapid convergence scheme higher memory computation overhead step. work authors oﬀer several hessian approximations schemes rapid convergence minimal overhead. section propose approaches solving snmf problem based notion variable splitting extremely popular reformulation technique signal image processing ﬁrst instance propose solve quadratic-penalized relaxation original problem whereas second approach solving original problem itself. note explicitly constrained even though constraint seems redundant since already constrained nonnegative constrained equal however redundant relax problem squaring equality constraint bringing objective approach approximating constrained optimization problem known quadratic penalty method widely used numerparticular show formally ical optimization. solving problem equivalent solving constrained problem hence original snmf problem shown generally speaking however common moderate value practice typically provides solution form problem solves original problem well many applications experiments found case snmf problem well finally note relaxed form snmf problem also thought regularized form standard problem precisely problem solve alternatingly minimizing case convergence turn produces provably convergent approach order employ accelerated proximal gradient method direction standard proximal gradient step descent step projected onto nonnegative orthant takes form aimed solve relaxed form snmf problem solve reformulated version exact problem primal-dual method known alternating direction method multipliers developed close half century admm lagrange multiplier methods general seen explosion recent interest machine learning signal processing communities classically admm provably convergent convex problems recent work also proven convergence method particular families nonconvex problems also extensive successful admm heuristic method highly nonconvex problems spirit applied admm nonconvex problem like works provide excellent results empirically furthermore speciﬁc reformulation chosen used splitting variables allows prove signiﬁcant result regarding convergence admm applied reformulation i.e. ﬁxed point algorithm indeed point original problem type result fact shown hold applying admm matrix factorization problems well denotes inner-product input matrices parameter typically requires small amount tuning practice alternate minimizing primal variables gradient ascent step dual variables reduces simple constrained minimization form note practice rarely solve actually inverting matrix instead eﬃcient catch cholesky factorization matrix solve corresponding linear system using forward-backward substitution. finally quadratic minimization problem nonnegativity constraint section compute iteration complexity snmfadmm fastest proposed algorithms compare iteration cost state snmf approach mentioned section seen algorithm iteration snmfadmm includes updating primal variables dual variables assuming rn×k construction corresponding cholesky factorization ﬁrst step updating require approximately operations respectively. analysis account matrix assignment operations dealt memory pre-allocation. find cholesky factorization yk−t solve =ayk− ρlk− λk−t solve backward substitution find cholesky factorization solve =axk ρlk− γk−t solve backward substitution additionally whenever possible take advantage symmetry matrices involved example case computing sparse graph structure used work resulting adjacency matrix ⌊logn⌋ nonzero entries therefore computing requires approximately operations. considering operations needed forward backward substitutions total iteration cost updating adds ﬂops. updating primal variable done using basic operations. together operations needed updating dual variable total iteration cost snmfadmm given comparison symnmf algorithm projected newton-like algorithm solving comparatively high iteration computational cost authors propose method takes limited number subsampled hessian evaluations iteration. adjustment lowers iteration cost approach retaining something quadratic convergence standard newton’s method. however even inexpensive newton’s approach serious scaling issues terms memory computation time dealing large real world data sets size more. moreover typically large iteration cost symnmf greatly surpasses snmfadmm derived section present results applying proposed algorithms several commonly used benchmark data sets including synthetic real world data sets. order evaluate clustering eﬃcacy algorithm compare standard normalized spectral clustering algorithm nonnegative matrix factorization built matlab uses popular alternating least squares solution approach symmetric nonnegative matrix factorization algorithm discussed section stopping condition snmfapg snmfadmm algorithms stopping threshold used experiments. threshold achieved experiments reported algorithms keeping ﬁxed snmfapg snmfadmm respectively. choices made running algorithms real benchmark datasets times using clusters instance detailed subsection equally spaced values range chose value algorithm provided strongest average performance datasets. however note choice quite robust entire range tested values initial experiments algorithms. algorithm choice forces ﬁnal matrix respective surrogate variables extremely similar algorithms converge thus ﬁnal assignment datapoints experiment calculated original snmf problem experiments section matlab machine intel core processor ram. large-scale even medium-sized data sets desirable work sparse adjacency matrices signiﬁcantly lower computational time require less space store. therefore follow suggestion sparse graphs. ﬁrst step graph-based clustering given data address construct adjacency matrix following work construct adjusted q-nearest neighbors graph data point connected nearest neighbors denoted matrix contains weights assigned edges deﬁned here local scale parameter distance nearest neighbor throughout experiments kept ﬁxed suggested parameter chosen ⌊logn⌋ total number data points data sake comparability results adopt normalized objective ﬁrst evaluate algorithms synthetic data sets shown figure data comprised clusters two-dimensional points total number data points vary algorithms times data diﬀerent random initializations snmf factorization matrix initialization used snmf algorithms. finally number runs algorithms resulted perfect clustering data reported table measure clustering performance. best result data highlighted. based results table data sets considered least challenging data sets respectively. also seen algorithms particularly snmfadmm perform least well often outperform symnmf synthetic data sets. report results k-means algorithms since perform poorly types data sets shown figure recently nmf-based algorithms extensively used clustering tasks especially image document data sets. representative image data sets popular coil- data contains grayscale images consisting objects taken diﬀerent viewpoints. images object form cluster resulting equally-sized clusters total. second real data used reuters popular text categorization collection consisting documents manually indexed categories personnel reuters ltd. original reuters- data documents assigned label. remove documents data remaining document belongs cluster. moreover keep largest clusters avoid clusters data points disrupt relative balance clusters’ sizes upon completion cluster assignment process cluster mapped gold standard classes. depending size overlap cluster mapped class quantify eﬃcacy clustering algorithm. formally cluster label given data point clustering algorithm provided gold standard label. accuracy score deﬁned unit impulse sequence zero everywhere else. here kuhn-munkres algorithm best mapping. note higher values suggest better clustering performance whenever clustering gold standard identical accuracy reaches maximum i.e. tables show clustering results coil- reuters- data sets respectively. experiments randomly select clusters entire data three clustering algorithms times each selected clusters. instances feed random initialization algorithms well symnmf fairly compare algorithms. note spectral clustering require initialization matlab’s built-in random initialization. procedure repeated times data sets. last tables corresponds case entire data selected since select clusters report scores averaged random initializations computation time reported graph-based methods include time spent constructing neighborhood graph cases considered negligible compared runtime algorithms themselves. moreover since graph-based algorithms must several times ensure good solution found cost ameliorated even number runs graph-based algorithm. best results highlighted bold. tables show algorithms particularly snmfadmm highly competitive state method terms clustering quality experiments. graph-based approaches outperform surprising given ability capture wider array cluster conﬁguration well signiﬁcant engineering advantage i.e. employ carefully engineered graph transformation input data. terms total computation time spectral clustering demonstrates lowest computation time coil experiments snmfadmm signiﬁcantly faster symnmf especially larger values furthermore reuters- experiments snmfadmm orders magnitude work introduced novel algorithms solving snmf problem exceptionally strong model graph-based clustering applications. particular experimental evidence forth work indicates algorithms eﬀective state approach also case snmfadmm average orders magnitude faster state snmf approach terms computation time. make strong statements regarding mathematical convergence snmfadmm proving complete convergence algorithm remains open problem future work. thus algorithms present interesting tradeoﬀ computational speed mathematical convergence guarantee snmfapg provably convergent considerably slower snmfadmm less currently said regarding provable convergence. thus overall empirical evidence presented here combined iteration complexity analysis strong proof mathematical convergence snmfadmm suggests algorithms extend practical usability snmf framework general large-scale clustering problems research direction pursue future. convex function lipschitz continuous gradient constant inverse value used ﬁxed step length iterations proximal gradient directions independently suﬃces compute maximum eigenvalue hessian directions ﬁrst three equations given second three equations enforce primal feasibility next dual feasibility ﬁnal equation ensures complementary slackness holds. rearranging third equation allows simplify ﬁnal lines giving equivalent system indeed converges point reformulation fact converges point original snmf formulation follows immediately equivalence reformulation problem. shows convergent output admm algorithm point original snmf problem", "year": 2016}