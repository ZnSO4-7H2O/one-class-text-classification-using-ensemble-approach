{"title": "Thoughts on Massively Scalable Gaussian Processes", "tag": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "abstract": "We introduce a framework and early results for massively scalable Gaussian processes (MSGP), significantly extending the KISS-GP approach of Wilson and Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs) on billions of datapoints, without requiring distributed inference, or severe assumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP learning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test point prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as Kronecker products of Toeplitz matrices approximated by circulant matrices. This multi-level circulant approximation allows one to unify the orthogonal computational benefits of fast Kronecker and Toeplitz approaches, and is significantly faster than either approach in isolation; 2) local kernel interpolation and inducing points to allow for arbitrarily located data inputs, and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block structure (BTTB), which enables fast inference and learning when multidimensional Kronecker structure is not present; and 4) projections of the input space to flexibly model correlated inputs and high dimensional data. The ability to handle many ($m \\approx n$) inducing points allows for near-exact accuracy and large scale kernel learning.", "text": "introduce framework early results massively scalable gaussian processes signiﬁcantly extending kiss-gp approach wilson nickisch msgp framework enables gaussian processes billions datapoints without requiring distributed inference severe assumptions. parvolves decomposing covariance matrices kronecker products toeplitz matrices approximated circulant matrices. multi-level circulant approximation allows unify orthogonal computational beneﬁts fast kronecker toeplitz approaches signiﬁcantly faster either approach isolation; local kernel test time predictions; exploiting block-toeplitz toeplitz-block structure enables fast inference learning multidimensional kronecker structure present; projections input space ﬂexibly model correlated inputs every minute users share hundreds thousands pictures videos tweets reviews blog posts. ever before access massive datasets almost every area science engineering including genomics robotics climate science. wealth information provides unprecedented opportunity automatically learn rich representations data allows greatly improve performance predictive tasks also provides mechanism scientiﬁc discovery. expressive non-parametric methods gaussian processes great potential large-scale structure discovery; indeed methods highly ﬂexible information capacity grows amount available data. however large data problems mostly uncharted territory storage inducing points hard apply massive datasets containing examples. moreover computational tractability approaches require severely aﬀect predictive performance limit representational power ability kernel learning needed large datasets directions scalable gaussian processes involved mini-batches data stochastic variational inference distributed learning approaches promising inference undergo severe approximations small number inducing points still required. indeed stochastic tions require distributed learning severe assumptions. approach builds recently introduced kiss-gp framework several signiﬁcant advances enable massive datasets. particular provide begin brieﬂy reviewing gaussian processes structure exploiting inference kissgp sections introduce msgp approach section demonstrate scalability accuracy msgp experiments section conclude section mean vector covariance matrix deﬁned mean vector covariance function gaussian process covariance function parametrized assuming additive gaussian noise predictive distribution evaluated test points indexed given kx∗x represents matrix covariances evaluated covariance matrices follow notational conventions. mean vector covariance matrix evaluated training inputs computational bottleneck inference solving linear system kernel learning computing determinant |kxx σi|. standard procedure compute cholesky decomposition matrix requires operations storage. afterwards predictive mean variance cost respectively test point structure exploiting approaches make existing structure accelerate inference learning. approaches beneﬁt often exact predictive accuracy impressive scalability inapplicable problems severe grid restrictions data inputs brieﬂy review kronecker toeplitz structure. rectilinear grid product kernel across dimensions case compute eigendecomposition separately taking eigendecompositions much smaller inference learning proceed orthogonal matrix eigenvectors also decomposes kronecker product diagonal matrix eigenvalues thus simple invert. overall grid data points grid dimensions inference learning cost storage computations. computing predictive variance single test point requires operations thus limiting toeplitz methods points kernel learning required. since toeplitz methods limited problems inputs complement kronecker methods exploit multidimensional grid structure. recently wilson nickisch introduced fast gaussian process method called kiss-gp performs local kernel interpolation combination inducing point approximations structure exploiting algebra approximate matrix cross-covariances training inputs inducing inputs ˜kxu matrix interpolation weights. approximate points ˜kxu given user-speciﬁed kernel structured kernel interpolation procedure gives rise fast approximate kernel wilson nickisch propose perform local kernel interpolation method called kiss-gp extremely sparse interpolation matrices. example performing local cubic interpolation d-dimensional input data contain non-zero entries row. furthermore wilson nickisch show classical inducing point methods re-derived within framework global interpolation noise free non-sparse interpolation weights. example subset regression inducing moreover wilson nickisch show performing local kernel interpolation achieve close linear scaling number inducing points placing points rectilinear grid exploiting toeplitz kronecker structure without requiring data inputs grid. scaling compares favourably operations stochastic variational approaches alparticular inference solve performing linear conju iterations required convergence machine precision value practice depends conditioning kski rather mvms sparse cost mvms exploiting structure roughly linear moreover eﬃciently approximate eigenvalues kski evaluate |kski kernel learning using circulant determinant approximations unify toeplitz kronecker structure; enable extremely fast marginal likelihood evaluations extend kiss-gp toeplitz methods scalable kernel learning input dimensions cannot exploit multidimensional kronecker structure scalability. general bttb structure enables fast exact multidimensional inference wilson nickisch propose fast inference learning test time predictions standard namely predictive mean predictive variance single test point show obtain test time predictions eﬃciently approximating latent mean variance gaussian likelihood predictive distribution given relations note methodology fast test predictions rely performed inference learning particular applied trained gaussian process model including full inducing points methods fitc data ﬁnite basis models respectively n∗×m sparse interpolation matrices containing assume). term pre-computed training taking mvms addition computation required obtain thus computation test time multiplication sparse costs operations leading operations test point using interpolated covariance ˜kxx similar predictive mean precomputed require multiplication sparse interpolation weight matrix leading operations test point every requires solution linear system size computationally taxing. eﬃciently precompute instead employ stochastic estimator based observation variance projection gaussian random variable draw gaussian samples eigendecomposition covariance evaluated grid kronecker toeplitz methods greatly restricted requiring data inputs located grid. lift restriction creating structure unobserved inducing variables part structured kernel interpolation framework described section toeplitz methods apply problems kronecker methods require multidimensional structure eﬃciency gains. present circulant approximation unify complementary beneﬁts kronecker toeplitz methods greatly scale marginal likelihood evaluations toeplitz based methods requiring grid structure data inputs faster kronecker product arbitrary positive deﬁnite matrices nested toeplitz structure often present kronecker decompositions wasted. indeed possible eﬃciently solve linear systems toeplitz matrices particularly eﬃcient obtain full eigendecomposition. fast operations toeplitz matrix obtained relationship circulant matrix. symmetric circulant matrices toeplitz matrices ﬁrst column given circulant vector subsequent column shifted position next. words c|j−i| circulant matrices computationally attractive eigendecomposition given matrix ﬁrst column therefore ci=...mj=...m using zero padding truncation i=...m circulant computed eﬃciently ffts. inference achieved solving iterative procedure involves mvms asymptotic cost computations. determinant single predictive variance however require speed solving linear toeplitz systems circulant pre-conditioners approximate inverses. wishes minimise distance preconditioner toeplitz matrix mincd. three classical pre-conditioners include cstrang minc chan minc circulant whittle approximation circwhittle given truncating positive deﬁniteness toep complete guaranteed construction large lattices approach often used accuracy favourable asymptotic properties consistency eﬃciency figure benchmark diﬀerent circulant approximations illustrating consistent good quality whittle embedding. covse covmatern covrq deﬁned rasmussen williams asymptotic normality fact circulant approximation asymptotically equivalent initial covariance gray hence logdet approximation inevitably converges exact value. particular circulant approximation pursued dramatic practical eﬀect performance. empirical comparison veriﬁed whittle approximations yields consistently accurate approximation results several structure. using dimension-wise circulant embedding size fast mvms accomplished using multi-dimensional fourier transformation applying fourier transformations along dimension rendering fast inference using feasible. similarly whittle approximation log-determinant generalised truncated circwhittle terms instead result whittle approximation covariance matrix block-circulant circulant blocks fortunately bccb matrices eigendecomposition whittle approximation deﬁned before. hence computational approximation beneﬁts toeplitz case carry bttb case. result exploiting bttb structure allows eﬃciently deal multivariate data without requiring factorizing covariance function. even exploit structure framework section provides eﬃciency gains conventional inducing point approaches particularly test time predictions. however place onto multidimensional grid kronecker structure total number inducing points grows exponentially number grid dimensions limiting fewer grid dimensions practical applications. however need limit applicability approach data inputs input dimensions even wish exploit kronecker structure indeed many inducing point approaches suﬀer curse dimensionality input projections provided eﬀective countermeasure wish learn parameters mapping supervised manner gaussian process marginal likelihood. representation highly general deep learning architecture example ordinarily project hidden layer lives dimensional space. richer models context added ﬂexibility special meaning. kronecker methods typically require kernel separates product across input dimensions here capture sophisticated correlations diﬀerent dimensions data inputs projection matrix preserving kronecker structure moreover learning supervised manner e.g. gaussian porcess marginal likelihood immediate advantages unsupervised dimensionality reduction example subset data inputs used producing target values structure would detected unsupervised method learned thus addition critical beneﬁt allowing applications dimensional data inputs also enrich expressive power msgp model. entries become hyperparameters gaussian process marginal likelihood treated exactly standard kernel hyperparameters length-scale. learn parameters marginal likelihood optimisation. computing derivatives marginal likelihood respect projection matrix requires care structured kernel interpolation approximation covariance matrix provide mathematical details appendix preliminary experiments stress test msgp terms training prediction runtime well accuracy empirically verifying scalability predictive performance. also demonstrate consistency model able learn supervised projections higher dimensional input spaces. compare exact gaussian processes fitc sparse spectrum gaussian processes data msgp toeplitz methods msgp using scalable approach test predictions experiments executed workstation intel ram. used step length bdgp based values reported hensman batchsize also evaluated bdgp larger batchsize cannot exploit kronecker structure dimensional inputs scalability toeplitz methods apply inputs traditionally limited points requires many marginal likelihood evaluations kernel learning. thus stress test value circulant approximation transparently give greatest advantage alternative approaches terms scalability number inducing points initially stress test input space moving onto higher dimensional problems. figure show runtime marginal likelihood evaluation function training points number inducing points msgp quickly overtakes alternatives increases past point. moreover runtimes msgp diﬀerent numbers inducing points converge quickly increases msgp requires training time inducing points inducing points indeed msgp able accommodate unprecedented number inducing points overtaking alternatives using inducing points using inducing points. exceptionally large number inducing points allows near-exact accuracy ability retain model structure necessary large scale kernel learning. note inducing points practical upper limit alternative approaches example gives practical upper emphasize although stochastic optimization bdgp take time converge ability mini-batches jointly optimizing variational parameters hyper parameters part principled framework makes bdgp highly scalable. indeed methodology msgp bdgp complementary could combined particularly scalable gaussian process framework. figure show prediction runtime msgp practically independent ﬁxed much faster alternatives depend least quadratically also local interpolation strategy test predictions introduced section greatly improves upon msgp using exact predictive distributions exploiting kronecker toeplitz algebra. mean absolute error smae mae/mae true test targets mean true test targets test predictions made method. compare predictive mean variance fast predictions msgp using standard predictive equations kronecker algebra predictions made using exact gaussian process. increase number inducing points quality predictions improved expect. fast predictive variances based local kernel interpolation sensitive number inducing points alternatives nonetheless reasonable performance. also fast predictive variances improved increasing number samples stochastic estimator section noticeable larger values numbers inducing points notably fast predictive mean based local interpolation essentially indistinguishable predictive mean using standard predictive equations without interpolation. overall error fast predictions much less average variability data. figure prediction runtime comparison. ‘slow’ mean refer bdgp using standard kronecker toeplitz algebra predictive mean variance without fast local interpolation proposed section figure accuracy comparison. compare relative absolute diﬀerence predictive mean variance msgp using ‘fast’ local kernel interpolation ‘slow’ standard predictions exact inference. locations randomly sampled gaussian distribution input grid structure dimensional space sample data gaussian process kernel operating dimensional inputs shown figure orthogonal projection onto d-dimensional subspace spanned rows metric motivated one-to-one correspondence subspaces orthogonal projections. bounded maximum distance indicates subspaces orthogonal minimum achieved subspaces identical. information metric including compute dist available chapter golub loan also make predictions withheld test points compare method using exact high dimensional space iii) exact true subspace results shown figure average results times value show standard errors. extremely subspace smae errors validate consistency approach reconstructing ground truth projection. moreover approach competitive smae best possible method true moreover although subspace error moderate still able substantially outperform standard baseline exact applied observed inputs constraints prevent degeneracies kernel hyperparameters causing practical issues length-scales growing large values shrink marginal likelihood determinant complexity penalty re-scaling leave marginal likelihood model term unaﬀected. practice found unit scaling suﬃcient avoid issues thus preferable orthonormal constrained. circulant determinant approximations unify toeplitz kronecker structure; enable extremely fast marginal likelihood evaluations extend kiss-gp toeplitz methods scalable kernel learning input dimensions cannot exploit multidimensional kronecker structure scalability. general bttb structure enables fast exact multidimensional inference without requiring kronecker decompositions projections demonstrate advantages comparing state alternatives. particular show msgp exceptionally scalable terms training points inducing points number testing points. ability handle large prove important retaining accuracy scalable gaussian process methods enabling large scale kernel learning.", "year": 2015}