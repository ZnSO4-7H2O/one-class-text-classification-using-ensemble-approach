{"title": "Multilinear Low-Rank Tensors on Graphs & Applications", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We propose a new framework for the analysis of low-rank tensors which lies at the intersection of spectral graph theory and signal processing. As a first step, we present a new graph based low-rank decomposition which approximates the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then, building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low-rank tensor inverse problems, such as tensor Robust PCA. The whole framework is named as 'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis shows: 1) MLRTG stands on the notion of approximate stationarity of multi-dimensional signals on graphs and 2) the approximation error depends on the eigen gaps of the graphs. We demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance videos and hyperspectral images. Generalization of the tensor concepts to non-euclidean domain, orders of magnitude speed-up, low-memory requirement and significantly enhanced performance at low SNR are the key aspects of our framework.", "text": "propose framework analysis lowrank tensors lies intersection spectral graph theory signal processing. ﬁrst step present graph based low-rank decomposition approximates classical low-rank matrices multilinear tensors. then building novel decomposition construct general class convex optimization problems approximately solving low-rank tensor inverse problems tensor robust pca. whole framework named multilinear low-rank tensors graphs theoretical analysis shows mlrtg stands notion approximate stationarity multidimensional signals graphs approximation error depends eigen gaps graphs. demonstrate applications wide variety artiﬁcial real tensor datasets fmri surveillance videos hyperspectral images. generalization tensor concepts non-euclidean domain orders magnitude speed-up low-memory requirement signiﬁcantly enhanced performance aspects framework. introduction low-rank tensors span wide variety applications like tensor compression robust completion parameter approximation cnns however current literature lacks development main facets large scale processing generalization non-euclidean domain example tensor rn×n×n tensor robust nuclear norm minimization costs iteration unacceptable even small many application speciﬁc alternatives randomization sketching fast optimization methods proposed speed work answer following question possible generalize notion tensors graphs target applications scalable manner? best knowledge little effort made target former price higher cost however work done tackle problems simultaneously. therefore revisit tensors perspective develop entirely novel scalable approximate framework beneﬁts graphs. recently shown case time varying signals ﬁrst eigenvectors graph provide smooth basis data notion graph stationarity. generalize concept higher order tensors develop framework encodes tensors multilinear combination graph eigenvectors constructed rows different modes multilinear combination call graph core tensor highly structured like core tensor obtained multilinear used solve plethora tensor related inverse problems highly scalable manner. paper propose multilinear low-rank tensors graphs novel signal model low-rank tensors. using signal model develop entirely novel scalable approximate framework variety inverse problems involving tensors multilinear robust tensor pca. importantly theoretically link concept joint approximate graph stationarity characterize approximation error terms eigen gaps graphs. various experiments wide variety artiﬁcial real benchmark datasets videos face images hyperspectral spectral covariance given cµpµ. signal approximately stationary graph energy concentrated diagonal mlrtg additionally require energy concentrated corner call lowfrequency energy concentration. fig. illustrates properties terms arbitrary matrix. leftmost plot corresponds case approximate stationarity middle case non-noisy mlrtg rightmost plot case approximate mlrtg. note energy spilling left submatrix noise results approximate low-rank representation. examples many real world datasets satisfy approximate mlrtg assumption. fig. presents example hyperspectral face tensor. singular values modes show low-rankness property whereas graph spectral covariance energy concentration plot show energy mode expressed terms graph eigenvectors. examples fmri coil tensor also presented fig. appendix example tensor rn×n×n robust tensor graphs scales opposed notation represent tensors bold calligraphic letters matrices capital letters tensor rn×n×n multilinear rank rn×nn. simplicity work tengraphs speciﬁcally refer knn-nearest neighbors graph rows vertex edge weight matrix deﬁned constructed gaussian kernel combinatorial laplacian given degree matrix. eigenvalue decomposition pµλµp refer eigenvectors eigenvalues throughout flann construction costs parallelizable. also assume fast parallelizable framework proposed available computation costs denotes vectorization denotes kronecker product rn×k∀µ rk×k×k graph core tensor refer tensor possible mlrtg mlt. main idea illustrated ﬁgure ﬁrst page paper. call tuple graph multilinear rank sequel simplicity notation work matricized version denote pk⊗pk rn×k. simple words encode low-rank tensor terms low-frequency laplacian eigenvectors. multilinear combination called gct. highly structured like core tensor obtained standard multilinear used broad range tensor based applications. furthermore fact encodes interaction graph eigenvectors renders interpretation multi-dimensional graph fourier transform. real applications noise tensor approximately low-rank following lemma holds figure hyperspectral image tensor stanford database. singular values modes graph spectral covariance energy concentration plot clearly show tensor approximately mlrtg. applications mlrtg notice decomposition deﬁned quite similar standard mulitlinear possible deﬁne graph based mlsvd using tensor rn×n×n factors rn×r linked core rr×r×r. attained every mode stops improve. costs mlsvd gmlsvd case given terms pre-computed laplacian eigenvectors determined gctp raises question factors relate core tensor argue following mlsvd then pµkaµk give example below thoretical study presented theorem algorithm gmlsvd thus tensor compute gmlsvd following steps compute graph core tensor gctp perform mlsvd factors pµkaµk core tensor given laplacian eigenvectors gmlsvd scales iteration complexity solving gctp. example understand this imagine case tensor vectorized wedge images coil dataset columns. matrix corresponds left singular vectors obtained correspond ﬁrst eigenvectors laplacian rows fig. shows example wedge image singular vector obtained laplaponents laplacian eigenvectors eigenvalues modes tensor former pre-computed needs determined appropriate procedure. determined used directly low-dimensional feature tensor employed useful tasks. therefore possible propose general framework solving broad range tensor matrix inverse problems optimize general linear operator matricization xµ∗g minx norm depending application consideration denote kernelized laplacian eigenvalues weights nuclear norm minimization. assuming eigenvalues sorted ascending order corresponds higher penalization higher singular values correspond noise. thus goal determine graph core tensor whose rank minimized modes. nuclear norm minimization full tensor appeared earlier works however note case lift computational burden minimizing core tensor graph core tensor pursuit ﬁrst application corresponds case interested clean matricized tensor straight-forward determine matricized case noisy corrupted gaussian noise seeks robust possible without appropriate regularization hence propose solve problem frobenius norm simple words theorem states singular vectors values matrix tensor obtained gmlsvd equivalent obtained mlsvd general inverse problem equivalent solving graph regularized matrix tensor factorization problem factors belong span graph eigenvectors constructed modes tensor. bound shows recover mlrtg large eigen gaps λµk∗ /λµk∗+. occurs rows matricized clustered clusters. closer smaller case selects error mlt. characterized projection singular vectors complement graph eigenvectors ¯pµk∗. experiments show selecting always leads better recovery exact value known. although inverse problems form orders magnitude faster standard tensor based inverse problems introduce approximation. first note present procedure determine optimal furthermore noted proof theorem pµλµp choice depends eigen assumption λµk+) might exist knn-laplacians. finally noise data adds approximation well. figure performance comparison gsvd gmlsvd mlsvd artiﬁcial artiﬁcial real tensors different scenarios. artiﬁcial tensors size rank along mode tensor size core tensor size along mode used artiﬁcial real tensors respectively. leftmost plots show reconstruction errors middle plots show reconstruction error singular values right plots show subspace angles subspace vectors. clearly gsvd gmlsvd outperform mlsvd terms computation time error noisy datasets. experimental results datasets study performance gmlsvd trpcag perform extensive experimentation artiﬁcial real tensors. types low-rank artiﬁcial datasets generated ﬁltering randomly generated tensor combinatorial laplacians constructed ﬂattened modes then different levels gaussian sparse noise added tensor. real datasets include hyperspectral images fmri image video tensors point cloud datasets. methods gmlsvd trpcag low-rank tensor factorization methods programmed using gspbox unlocbox tensorlab toolboxes. note gmlsvd robust gaussian trpcag sparse noise therefore methods tested varying levels types noise. avoid confusion call tensor version gmlsvd graph tensors gaussian noise compare gmlsvd performance mlsvd. tensor sparse noise compare trpcag tensor robust gmlsvd. tensors gaussian noise compare gsvd simple svd. finally matrix sparse noise compare trpcag robust robust graphs fast robust graphs compressive methods tested datasets computational reasons. parameters experiments involving trpcag gmlsvd graphs constructed rows ﬂattened modes tensor using gaussian kernel weighting edges. methods graphs constructed required using parameters above. method several hyper-parameters require tuning. fair comparison methods properly tuned hyper-parameters best results reported. details datasets methods parameter tuning please refer appendix evaluation metrics metrics used evaluation divided types quantitative qualitative. three different types quantitative measures used normalized reconstruction error tensor normalized reconstruction error ﬁrst singular values along mode clean tensor arccos|vu| alignment singular vectors diagu|) denote mode singular vectors determined proposed method clean tensor. qualitative measure involves visual quality low-rank components tensors. experiments gmlsvd performance study artiﬁcial datasets ﬁrst rows fig. show performance gsvd gmlsvd artiﬁcial tensors size rank mode varying levels gaussian noise ranging three plots show reconstruction error recovered tensor ﬁrst singular values subspace angle mode subspace w.r.t clean tensor. results compared standard tensor standard mlsvd tensor. interesting note leftmost plot reconstruction error gsvd tends lower compared higher noise levels middle plot explains phenomena error singular values signiﬁcantly lower gsvd higher noise levels. observation logical higher levels noise lower singular values also affected. simple singular value thresholding method eliminate effect noise lower singular values whereas gsvd smart weighted nuclear norm method thresholds lower singular values function graph eigenvalues. effect shown detail fig. contrary subspace angle gsvd higher levels noise. means subspaces gsvd well aligned ones clean tensor. however shown right plot fig. strong ﬁrst show individual subspace vectors gsvd well aligned clean tensor. primary reason reconstruction error gsvd less compared snr. higher approximation error gsvd dominates error noise. thus gsvd reveals true power scenarios. similar observations made gmlsvd fig. time memory performance real datasets fig. shows results gmlsvd compared mlsvd real dataset size dimensions core tensor size used experiment. interesting note scenarios reconstruction error methods approximately same. gmlsvd mlsvd show signiﬁcantly lower erfigure singular values gsvd clean data singular vector alignment gsvd clean data matrix corrupted gaussian noise clearly gsvd eliminates effect noise lower singular values aligns ﬁrst singular vectors appropriately. singular values whereas gmlsvd’s subspaces less aligned compared mlsvd. rightmost plot ﬁgure compares computation time methods. clearly gmlsvd wins computation time signiﬁcantly compared mlsvd dataset gmlsvd requires memory whereas mlsvd requires shown detailed results fig. appendices. fig. also presents results hyperspectral fmri tensors reveals gmlsvd performs better compared mlsvd requiring less computation time memory datasets regime. visualization clean noisy gmlsvd recovered tensors singular values alignments subspace vectors fmri hyperspectral tensors please refer fig. appendices. compression obvious goal mlsvd compression low-rank tensors therefore gmlsvd also used purpose. fig. shows results face hyperspectral tensor. using core size attained times compression maintaining fig. appendices shows results three datasets. rightmost plots fig. appendices also show compression results fmri datasets. figure performance analysis trpcag artiﬁcial artiﬁcial varying levels sparse noise airport lobby video tensor artiﬁcial tensors size rank along mode. core tensor size along mode used artiﬁcial tensors real tensor. leftmost plots show reconstruction errors middle plots show reconstruction error singular values right plots show subspace angles subspace vectors. real video dataset rightmost frame shows result obtained trpcag. computation time different methods written frames. clearly trpcag performs quite well signiﬁcantly reducing computation time. figure robust recovery subspace structures trpcag. leftmost plots show clean sparsely corrupted sample wedge image coil dataset. plots show singular vector recovered various low-rank recovery methods frpcag rpca rpcag trpcag shows alignment singular vecors recovered methods clean tensor. experiments trpcag performance study artiﬁcial datasets ﬁrst rows fig. show experiments artiﬁcial datasets varying levels sparse noise. version trpcag compared state-of-the-art robust based methods frpcag rpca also based methods like mlsvd gsvd. conclusions similar gaussian noise experiments drawn matrices thus trpcag better state-of-the-art methods presence large fraction sparse noise. tensor trpcag compared gmlsvd trpca interestingly performance trpca always better case even presence high levels noise. trpca minutes whereas trpca converge even hours. result obtained trpcag visualized fig. complete videos actual frames low-rank sparse components experiments provided supplementary material paper. robust subspace recovery imperative point inverse problems form implicitly determine subspace structures grossly corrupted tensors. examples gmlsvd section correspond case clean tensor. section show singular vectors recovered trpcag sparse grossly corrupted tensors also align closely clean tensor. fig. shows example wedge image coil dataset used section leftmost plots show clean sparsely corrupted sample wedge image. plots show singular vector recovered various low-rank recovery methods frpcag rpca rpcag trpcag shows alignment singular vecors recovered methods clean tensor. rightmost plots correspond case trpcag clearly show recovered subspace robust sparse noise. examples yale coil datasets also shown fig. appendices. effect parameters space constraints study effect parameters multilinear rank power fig. appendices. summarize parameters tuned becomes insensitive parameter conclusion inspired fact ﬁrst eigenvectors knn-graph provide smooth basis data present graph based low-rank tensor decomposition model. low-rank tensor decomposed multilinear combination lowest eigenvectors graphs constructed rows ﬂattened modes tensor propose general tensor based convex optimization framework overcomes computational memory burden standard tensor problems enhances performance regime. speciﬁcally demonstrate applications mlrtg graph based mlsvd tensor robust graphs artiﬁcial real datasets different noise levels. theoretically prove link mlrtg joint stationarity signals graphs. also study performance guarantee proposed general optimization framework connecting factorized graph regularized problem. time performance real datasets fig. present experiments real video dataset obtained airport lobby goal separate static low-rank component sparse part video. results trpcag compared rpca rpcag frpcag cpca downsampling factor along frames. clearly trpcag recovers low-rank qualitatively equivalent methods time times less rpca rpcag order magnitude less compared frpcag. furthermore trpcag requires time sampling based cpca method recovers better quality low-rank structure seen row. performance quality trpcag also evident point cloud experiment fig. recover low-rank point cloud dancing person adding sparse noise experiments videos point cloud datasets presented figs. appendices. scalability trpcag video show scalability trpcag compared trpca made video snowfall campus tried separate snow-fall low-rank background methods. video dimension trpcag took less", "year": 2016}