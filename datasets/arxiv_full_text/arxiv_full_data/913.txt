{"title": "Predicting Parameters in Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.", "text": "demonstrate signiﬁcant redundancy parameterization several deep learning models. given weight values feature possible accurately predict remaining values. moreover show parameter values predicted many need learned all. train several different architectures learning small number weights predicting rest. best case able predict weights network without drop accuracy. recent work scaling deep networks construction largest artiﬁcial neural networks date. possible train networks tens millions even billion parameters largest networks trained using asynchronous sgd. framework many copies model parameters distributed many machines updated independently. additional synchronization mechanism coordinates machines ensure different copies parameters drift other. major drawback technique training inefﬁcient makes parallel resources largest networks dean gains distribution largest distributing model machines reduces training time mini-batch factor increasing machines achieves speedup factor roughly speedups signiﬁcant clear trend diminishing returns overhead coordinating machines grows. approaches distributed learning neural networks involve training batch mode methods scaled nearly online counterparts. seems clear distributed architectures always required extremely large networks; however efﬁciency decreases greater distribution also makes sense study techniques learning larger networks single machine. reduce number parameters must learned communicated network ﬁxed size reduce number machines required train hence also reduce overhead coordination distributed framework. work study techniques reducing number free parameters neural networks exploiting fact weights learned networks tend structured. technique present extremely general applied broad range models. technique also completely orthogonal choice activation function well learning optimizations; work alongside recent advances neural network training dropout rectiﬁed units maxout without modiﬁcation. figure ﬁrst column block shows four learned features second column shows parameters chosen random original ﬁrst column. third column shows random used predict remaining parameters. left right blocks convnet trained stl- trained mnist convnet trained cifar- reconstruction trained hyv¨arinen’s natural image dataset reconstruction trained stl-. intuition motivating techniques paper well known observation ﬁrst layer features neural network trained natural image patches tend globally smooth local edge features similar local gabor features given structure representing value pixel feature separately redundant since highly likely value pixel equal weighted average neighbours. taking advantage type structure means need store weights every input feature. intuition illustrated figures remainder paper dedicated elaborating observation. describe general purpose technique reducing number free parameters neural networks. core technique based representing weight matrix rank product smaller matrices. factoring weight matrix able directly control size parameterization controlling rank weight matrix. na¨ıve application technique straightforward tends reduce performance networks. show carefully constructing factors learning factor train networks vastly fewer parameters achieve performance full networks structure. constructing good ﬁrst factor exploiting smoothness structure inputs. prior knowledge smoothness structure expect impose structure directly choice factor. prior knowledge available show still possible make good data driven choice. demonstrate experimentally parameter prediction technique extremely effective. best cases able predict parameters network without drop predictive accuracy. throughout paper make distinction dynamic static parameters. dynamic parameters updated frequently learning potentially observation mini-batch. contrast static parameters whose values computed altered. although values parameters depend data expensive compute computation need done entire learning process. reason distinction static parameters much easier handle distributed system even values must shared machines. since values static parameters change access need synchronized. copies parameters safely distributed across machines without synchronization overhead incurred distributing dynamic parameters. figure rica different amounts parameter prediction. leftcolumn parameters learned l-bfgs. rightcolumn parameters learned remaining values predicted iteration. intermediate columns interpolate extremes increments deep networks composed several layers transformations form nv-dimensional input nh-dimensional output matrix parameters. column contains weights connecting unit visible layer single unit hidden layer. reduce number free parameters representing product matrices size size making much smaller achieve substantial reduction number parameters. principle learning factored weight matrices straightforward. simply replace objective function compute derivatives respect instead practice na¨ıve approach preform well learning full rank weight matrix directly. moreover factored representation redundancy. invertible matrix size remove redundancy value learn question remains reasonable choice following section provides answer question. function mapping weight space real numbers estimate values function using regression. case image patches coordinates pixel structures possible. simple regression model appropriate linear combination basis functions. view columns form dictionary basis functions features network linear combinations features parameterized problem thus becomes choosing good base dictionary representing network features. base dictionary feature prediction constructed several ways. obvious choice train single layer unsupervised model features model dictionary. approach advantage extremely ﬂexible—no assumptions structure feature space required—but drawback requiring additional training phase. prior knowledge structure feature space exploit construct appropriate dictionary. example learning features images could choose selection fourier wavelet bases encode expectation smoothness. also build using kernels encode prior knowledge. achieve kernel ridge regression denote observed values weight vector restricted subset domain introduce kernel matrix entries model covariance locations parameters locations kernel enables make smooth predictions parameter vector entire domain using standard kernel ridge predictor section describe feature prediction process applies features derived image patches using kernel ridge regression since intuition strongest case. defer discussion select kernel deep layers well non-image data visible layer later section. settings prediction process formally identical intuition less clear. vectorized image patch corresponding visible layer standard neural network hidden activity induced patch given network nonlinearity weight matrix whose columns correspond features matched visible layer. consider single column weight matrix whose elements indexed case image patch indices multidimensional indicating spatial location colour channel index select locations represent ﬁlter explicitly denote vector weights locations. wide variety options selected. found choosing uniformly random works well; however possible performance could improved carefully designing process selecting −wα. notice values predict full feature predict entire feature matrix parallel using image patches expect smoothness pixel space appropriate kernel squared exponential kernel length scale parameter controls degree smoothness. convenient interpretation pixel locations image corresponding basis function dictionary deﬁned kernel. generically index collection dictionary elements remainder paper even dictionary element correspond directly pixel location example. motivated technique method predicting features neural network; however approach also interpreted linear pooling process. recall hidden activations standard neural network applying nonlinearity given motivation proceeded along lines replacing uαwα discussing relationship predicted counterpart. alternatively write vαwα linear transformation data. interpretation think predicted layer composed layers internally. ﬁrst linear layer applies ﬁxed pooling operator given second ordinary fully connected layer visible units. columnar architecture prediction process described assumes features; however restrictive. continuing intuition ﬁlters smooth local edge detectors might want choose give high resolution local area pixel space using sparser representation remainder space. naturally case would want choose several different concentrates high resolution information different regions. straightforward extend feature prediction setting. suppose several different index sets corresponding elements dictionary form sub-dictionary predicted feature matrix full predicted feature matrix formed concatenating matrices blockwise block full predicted feature matrix treated completely independently. blocks share parameters—even corresponding dictionaries different. thought deﬁning column representation inside layer. input column shared representations computed column independent. output layer obtained concatenating output column. represented graphically figure figure left columnar architecture fully connected network path column highlighted. column corresponds different right columnar architecture convolutional network. setting wα’s take linear combinations feature maps obtained convolving input dictionary. make abuse notation main text—the vectorized ﬁlter banks must reshaped convolution takes place. introducing additional columns network increases number static parameters number dynamic parameters remains ﬁxed. increase static parameters comes fact column dictionary. reason corresponding increase number dynamic parameters ﬁxed size hidden layer hidden units divided columns. number dynamic parameters depends number hidden units size dictionary. convolutional network interpretation similar. setting appropriately sized ﬁlter bank. using denote result vectorizing ﬁlters write uαwα using slight abuse notation write uαwα. above re-order operations obtain vαwα resulting structure similar layer ordinary mlp. structure illustrated figure note ﬁrst convolved produce preprocessing column comes convolution ﬁxed ﬁlters deﬁned dictionary. next form linear combinations ﬁxed convolutions coefﬁcients given particular order operations result computational improvements number hidden channels larger elements separable turn attention selecting appropriate dictionary different layers network. appropriate choice dictionary inevitably depends structure weight space. weight space topological structure expect smoothness example weights correspond pixels image patch choose kernel-based dictionary enforce type smoothness expect. topological structure exploit propose data driven dictionaries. obvious choice shallow unsupervised feature learning autoencoder build dictionary layer. another option construct data-driven kernels ridge regression. easy choices using empirical covariance empirical squared covariance hidden units averaged data. since correlations hidden activities depend weights lower layers cannot initialize kernels deep layers without training previous layers. handle pre-training layer autoencoder. construct kernel using empirical covariance hidden units data using pre-trained weights. layer pre-trained figure left comparing performance different dictionaries predicting weights ﬁrst layers network mnist. legend shows dictionary type layer– layer right performance timit core test using hidden layers. ﬁne-tune entire network backpropagation phase kernel parameters ﬁxed. also experiment choices dictionary random projections random connections perform initial experiments using mlps order demonstrate effectiveness technique. train several models mnist using different strategies constructing dictionary different numbers columns different degrees reduction number dynamic parameters used feature. chose explore permutations mnist since small enough allow broad coverage. networks experiment hidden layers architecture sigmoid activation function. ﬁnal layer softmax classiﬁer. cases preform parameter prediction ﬁrst second layers only; ﬁnal softmax layer never predicted. layer contains approximately total network parameters substantial savings possible even features layer predicted. figure shows performance using several different strategies constructing dictionary using columns ﬁrst second layers. divide hidden units layer equally columns different dictionaries follows nokernel ordinary model feature prediction lowrank optimized. randcon random connections randfixu random projections using matrix gaussian entries. ridge regression squared exponential kernel length scale ridge regression covariance kernel. ridge regression squared covariance kernel. dictionary pre-trained autoencoder. se–emp se-emp architectures preform substantially better alternatives especially dynamic parameters. consistency pre-trained models except lowrank autoencoders. pretrain lowrank model found autoencoder pretraining extremely unstable model. figure shows results similar experiment timit. speech data analyzed using hamming window ﬁxed frame rate. experiments represented speech using th-order frequency cepstral coefcients energy along ﬁrst second temporal derivatives. networks used experiment hidden layers units. phone error rate measured performing viterbi decoding figure shows performance convnet cifar-. ﬁrst convolutional layer ﬁlters input image using ﬁlters size second convolutional layer applies ﬁlters size output ﬁrst layer. third convolutional layer transforms output second layer applying ﬁlters size output third layer input fully connected layer hidden units ﬁnally softmax layer outputs. reduce parameters ﬁnal softmax layer. convolutional layers column fully connected layer columns. convolutional layers natural topological structure exploit dictionary constructed squared exponential kernel convolutional layer. input fully connected layer network comes convolutional layer ridge regression squared exponential kernel predict parameters layer well. reconstruction method learning overcomplete models similar linear autoencoder network. demonstrate effectively predict parameters rica cifar- stl-. order rica classiﬁer follow procedure coates figure shows results parameter prediction rica cifar- stl-. rica single layer architecture predict parameters squared exponential kernel dictionary length scale nokernel line shows performance rica feature prediction task. cases able predict half dynamic parameters without substantial drop accuracy. figure compares performance rica models number dynamic parameters. models ordinary rica parameter prediction parameters feature predicted using squared exponential kernel dictionary length scale since parameters feature predicted second model twice many features number dynamic parameters. several methods limiting number parameters neural network explored literature. early approach technique optimal brain damage uses approximate second derivative information remove parameters already trained network. technique apply setting since limit number parameters training rather after. common approach limiting number parameters locally connected features size parameterization locally connected networks reduced using tiled convolutional networks groups feature weights tile input figure left comparison performance rica without parameter prediction cifar- stl-. right comparison rica rica parameter prediction using number dynamic parameters substantial gain accuracy number dynamic parameters using technique. error bars stl- show conﬁdence intervals recommended testing protocol. space tied. convolutional neural networks even restrictive force feature tied weights receptive ﬁelds. techniques similar paper appeared shallow models computer vision literature. double sparsity method rubinstein involves approximating linear dictionaries dictionaries similar manner approximate network features. rigamonti study approximating convolutional ﬁlter banks linear combinations separable ﬁlters. works focus shallow single layer models contrast focus deep networks. techniques described paper orthogonal parameter reduction achieved tying weights tiled convolutional pattern. tying weights effectively reduces number feature maps constraining features different locations share parameters. approach reduces number parameters required represent feature straightforward incorporate tiled convolutional network. cires¸an control number parameters removing connections layers convolutional network random. achieve state-of-the-art results using randomly connected layers part network. technique subsumes idea random connections described section idea regularizing networks prior knowledge smoothness delicate process. lang hinton tried imposing explicit smoothness constraints regularization found universally reduce performance. na¨ıvely factoring weight matrix learning factors tends reduce performance well. although idea simple conceptually execution difﬁcult. g¨ulc¸ehre demonstrated prior knowledge extremely important learning highlights importance introducing effectively. recent work shown state results several benchmark tasks computer vision achieved training neural networks several columns representation different preprocessing different columns representation particular relevance approach interpretation similar described section unlike work consider deep columns paper; however collimation attractive increasing parallelism within network columns operate completely independently. reason could incorporate deeper columns networks would make potentially interesting avenue future work. approach superﬁcially similar factored whose parameters form tensor. since total number parameters model prohibitively large tensor represented outer product three matrices. major differences technique factored include fact factored speciﬁc model whereas technique applied broadly—even factored rbms. addition factored factors learned whereas approach dictionary ﬁxed judiciously. paper always choose indices uniformly random. wide variety options could considered here. works focused learning receptive ﬁelds directly would interesting incorporate technique. similar vein careful attention selection kernel functions appropriate. considered simple examples shown preform well study hardly exhaustive. using different types kernels encode different types prior knowledge weight space even learning kernel functions directly part optimization procedure possibilities deserve exploration. natural topology weight space available infer topology dictionary empirical statistics; however possible instead construct dictionary induce desired topology weight space directly. parallels work inducing topology representations well work learning pooling structures deep networks shown achieve signiﬁcant reductions number dynamic parameters deep models. idea orthogonal complementary recent advances deep learning dropout rectiﬁed units maxout. creates many avenues future work improving large scale industrial implementations deep networks also brings question whether right parameterizations deep learning.", "year": 2013}