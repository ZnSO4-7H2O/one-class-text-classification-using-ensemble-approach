{"title": "Interpretable Semantic Textual Similarity: Finding and explaining  differences between sentences", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "User acceptance of artificial intelligence agents might depend on their ability to explain their reasoning, which requires adding an interpretability layer that fa- cilitates users to understand their behavior. This paper focuses on adding an in- terpretable layer on top of Semantic Textual Similarity (STS), which measures the degree of semantic equivalence between two sentences. The interpretability layer is formalized as the alignment between pairs of segments across the two sentences, where the relation between the segments is labeled with a relation type and a similarity score. We present a publicly available dataset of sentence pairs annotated following the formalization. We then develop a system trained on this dataset which, given a sentence pair, explains what is similar and different, in the form of graded and typed segment alignments. When evaluated on the dataset, the system performs better than an informed baseline, showing that the dataset and task are well-defined and feasible. Most importantly, two user studies show how the system output can be used to automatically produce explanations in natural language. Users performed better when having access to the explanations, pro- viding preliminary evidence that our dataset and method to automatically produce explanations is useful in real applications.", "text": "abstract user acceptance artiﬁcial intelligence agents might depend ability explain reasoning requires adding interpretability layer facilitates users understand behavior. paper focuses adding interpretable layer semantic textual similarity measures degree semantic equivalence sentences. interpretability layer formalized alignment pairs segments across sentences relation segments labeled relation type similarity score. present publicly available dataset sentence pairs annotated following formalization. develop system trained dataset which given sentence pair explains similar different form graded typed segment alignments. evaluated dataset system performs better informed baseline showing dataset task well-deﬁned feasible. importantly user studies show system output used automatically produce explanations natural language. users performed better access explanations providing preliminary evidence dataset method automatically produce explanations useful real applications. since early days expert systems acknowledged factor users domain experts accept expert systems real-world domains ability expert systems explain reasoning also think user acceptance artiﬁcial intelligence agents depend ability explain reasoning requires adding interpretability layer facilitate users understand behavior. work explores interpretability context semantic textual similarity measures semantic equivalence text snippets using graded similarity capturing notion sentences similar others ranging complete unrelatedness semantic equivalence. systems attaining high correlations gold truth scores routinely reported example task given following sentences drawn corpus news headlines annotators judged similarity roughly equivalent minor information differs sentences talk accidents casualties pakistan differ number people killed level detail ﬁrst speciﬁes accident second speciﬁes location pakistan. giving explanations comes naturally people constructing algorithms computational models mimic human level performance represents difﬁcult natural language understanding problem. article deﬁne ﬁrst step ambitious goal. build evaluate system that given sentences returns textual explanation commonalities differences sentences. system based formalization interpretability layer explicit alignment segments sentences alignments annotated relation type similarity score. core part system trained evaluated dataset sentence pairs annotated alignments. trained system thus able return figure graphical representation interpretability layer. sentences split four segments each aligned follows similar similarity score killed equivalent killed score accident speciﬁc road accident score pakistan general pakistan score section details annotation procedure. reasons similarity sentences form typed segment alignments. evaluation annotated dataset shows system performs better informed baseline showing task well-deﬁned feasible. figure shows formalization interpretability layer sample sentences including segments alignments types scores alignments. types include relations like equivalence opposition specialization similarity relatedness. similarity scores aligned segments range addition dataset core system also build verbalization system system takes input alignments produces humanreadable explanation based templates. system returns following text alignment figure order measure quality usefulness explanations direct comparison human-elicited text problematic would tell usefulness. instead measure whether automatically produced explanations useful user studies. ﬁrst study english native speakers scored similarity sentence pairs without automatically produced explanations. second study simulated tutoring scenario students graded respect reference sentence. users simulating students state whether agreed grade without access automatically produced explanations. studies show users read explanations agreed system scores often users access explanations. describes system that given sentence pairs able return alignments segments sentences annotated relation type graded similarity score. system trained evaluated annotated dataset good results well informed baseline state-of-the-art. shows user-studies automatically produced explanations help users better attain tasks providing preliminary evidence formalization speciﬁc system useful real applications. paper organized follows. ﬁrst introduce related work. section deals creation dataset followed section presents comparison related dataset. section explains system section evaluation. section presents user study ﬁnally section draws conclusions. explanations important teaching domain intelligent tutoring systems strive provide feedback beyond correct/incorrect judgments. cases systems rely costly domain-dependent question-dependent knowledge scalable alternatives based generic natural language processing techniques also available approach related spirit last paper formalize interpretability layer differently below. area interpretability representation models learned data also widespread concern. ritter show able infer classes easily interpretable humans fyshe argue dimensions word representations correspond easily interpretable concepts. knowledge article ﬁrst research work area addressing explicit human-readable explanations. work situated area natural language understanding related enabling tasks extensively used evaluate quality semantic representations semantic textual similarity textual entailment. semantic textual similarity focus several semeval tasks staring ongoing time writing paper. given pair sentences systems compute similar return similarity score bounded grade used. related paraphrasing textual entailment instead binary reﬂects graded notion. also differs textual entailment directional. enabling technology application machine translation evaluation information extraction question answering text summarization. work reuses existing datasets adds interpretable layer form typed alignments sentence segments. formalization interpretable layers proposes explicit alignment segments alignment labeled relation type similarity score. previous work alignment text segments language usually focused word level exceptions. instance brockett released pascal corpora composed sentence pairs semantically equivalent words phrases text hypothesis sentences aligned. word either linked words left unlinked links marked either sure possible depending degree conﬁdence alignment. annotators dataset viewed sentence pairs corpora pairs parallel strings words lines association them limited coverage phrases like multiword expressions. work step further focus text segments beyond words well adding alignment types similarity scores. similar effort aligned tokens dataset although short phrases also aligned chunks semantically equivalent non-compositional. case formalization covers kind segments including non-equivalent equivalent segments compositional not. recent work performed parallel ours pavlick annotated automatically derived database paraphrases short phrases entailment relations natural logic used crowdsourcing annotate hand around thousand phrase pairs database. section includes head-tohead comparison annotation schemes showing work complementary formalization annotation. different strand work coming educational domain close textual entailment nielsen deﬁned so-called facets facet pair words non-explicit semantic relation words. facet hypothesis text usually sentence annotated information whether entailed reference text. context tutoring systems dataset comprises student responses reference answers. reference answer decomposed hand constituent facets. student answers annotated label entailed facets corresponding reference answer contrary proposal explicit alignment facets facets necessarily correspond text segments rather represent pairs words unknown semantic relation text. initial motivation interpretable similar nielsen think interpretability especially useful ﬁeld tutoring systems depart work explicitly aligning segments sentences well providing labels relation similarity scores. idea facets later followed levy call partial textual entailment. approach complementary ours could also align facets characterize semantic relations well alignment relations. another perspective enrich textual entailment datasets partial entailment annotations also enrich datasets explicit alignments types related entailment relations. back relation section present formalization full. related work includes semeval task related tutoring systems automatically score student answers joint student response analysis recognizing textual entailment challenge task ﬁrst large-scale non-commercial automatic short answer grading competition goal mentioned task assess student responses questions science domain focusing correctness completeness response content. typical scenario expected correct student answer would entail reference answer. goal mentioned task label student answers according different categories task includes pilot subtask participants annotate facets. opinion effective feedback needs identify speciﬁc text segments student answers differ reference answer alignments. section presents interpretable dataset. ﬁrst introduce annotation procedure followed source sentence pairs evaluation method inter-tagger annotation data. text segments. segments annotated according deﬁnition chunks non-recursive core intra-clausal constituent extending beginning head. typical chunk consists content word surrounded constellation function words matching ﬁxed template. marking chunks sentence annotator follows conll task guidelines adapted slightly purpose main clause split smaller chunks consisting noun phrases verb chains prepositional phrases adverbs expressions. figure shows examples chunks. alignment. alignment marked using interface. aligning meaning chunks context taken account. annotators must align many chunks possible. given limitations interface decided focus one-to-one alignments chunk aligned chunk. reason options align strongest corresponding chunk aligned. chunk left unaligned labeled alic. chunks also left unaligned corresponding chunk found punctuation marks ignored left unaligned. http//www.clips.ua.ac.be/conll/chunking/ https//github.com/ixa-ehu/ixa-pipe-chunk modiﬁed tool developed align words https//www.ldc.upenn.edu/ language-resources/tools/ldc-word-aligner. reused xml-based annotation format well. score. chunks aligned annotator provides similarity score alignment score ranges note aligned pair would never score would mean chunks aligned. restrictions concerning possible score values speciﬁc labels. label. assigning labels aligned chunks interpretation whole sentence including common sense inference taken account. possible labels following figure graphical representation interpretability layer. sentences split four segments each aligned follows moped general scooter similarity score parked similar sitting score sidewalk speciﬁc street score front grafﬁti related front building score section details annotation procedure. note alic noali also fact even aligned meaning respective chunk adds factuality polarity nuance sentence. labels scores independent. annotating scores labels dataset comprises pairs sentences news headlines image descriptions already mentioned sample pair headlines figure shows sample pair images together alignment. headlines corpus composed naturally occurring news headlines gathered europe media monitor engine several different news sources described best images dataset subset pascalvoc- dataset described rashtchian consists images around descriptions each. dataset comprised sentence pairs headlines images respectively split evenly training addition table shows statistics datasets. headlines contain slightly less chunks less tokens chunk image captions. half aligned pairs datasets score decreasing number aligned pairs score range. regarding labels equi used label followed simi oppo. breakdown scores types similar datasets. alic used times often headlines dataset. large number unaligned chunks specially images dataset. finally fact seldom used news dataset never images dataset. annotation pairs took hours minutes pair. annotation faster towards project around minutes pair. used in-house adaptation interface designed cross-lingual word alignment helped enter annotations faster. order evaluate systems perform interpretable decided adopt word alignment evaluation methods machine translation community. particular evaluation method based melamed uses precision recall token alignments. note fraser marcu argued better measure alignment error rate. idea segment alignment mapped token alignment token pairs aligned pairs aligned weight. weight token-token alignment inverse number alignments token so-called factor precision measured ratio token-token alignments exist system gold standard ﬁles divided number alignments system. recall measured similarly ratio token-token alignments exist system gold-standard divided number alignments gold standard. precision recall evaluated alignments pairs table headlines images dataset statistics across splits. ﬁrst three rows report respectively number sentence pairs chunks sentence tokens chunk. rows report number aligned chunk pairs break-down according similarity score followed breakdown according label aligned pairs. last four rows report number unaligned chunks many times additional fact labels used. evaluation done four difference levels segment alignment alone segment alignment require labels agree segment alignment differences score penalized ﬁnally segment alignment score labels scores taken account. evaluation script freely available together dataset. subset sentence pairs dataset. annotators previously read guidelines agreed mutual uncertainties. agreement computed using evaluation script tagger taken system gold standard. overall results agreement shown table notice metrics used compute metrics used evaluate system performance task. segment alignment done high agreement headlines images dataset. agreement type also high well agreement scores considering agreement type score scores also highest score simpler images dataset. high results show annotation task well-deﬁned replicable high agreement scores. labels closely related used natural logic later adapted purpose annotating database paraphrases compare annotations latter closer work. mutually exclusive entailment relations labels created independently theirs overlap between annotation schemes remarkable. table shows mapping between respective labels one-to-one mapping exception distinguish similar related several researchers argued convenience separate phenomena similarity refers conceptually similar concepts relatedness refers concepts closely related note relatedness also encompasses similarity similar concepts also tend related. hill present recent review phenomena review word relatedness word similarity datasets. annotated resource thus complementary pavlick annotated crowdsourcing subset phrase pairs automatically derived paraphrase database case annotate pairs manually identiﬁed chunks text pairs. note source pairs different label pairs phrases automatically induced paraphrastic label pairs chunks occur pairs naturally occurring sentences different similarity ranges. addition distinguish similar related pairs label explicitly factuality polarity phenomena. system interpretable needs perform chunking align chunks label score alignments. ﬁrst describe baseline system performs steps turn present improvements. baseline performs steps following publicly available algorithms. ﬁrst runs ixa-pipes chunker lower-case tokens align identical tokens. chunks aligned based number aligned tokens greedy manner starting pair chunks highest relative number aligned tokens. chunks aligned tokens left unaligned. finally baseline uses rule-based algorithm directly assign labels scores follows aligned chunk pairs assigned equi label rest either assigned alic noali procedure assign scores follows alignment guidelines equi pairs scored maximum score rest scored given chunker perfect analyzed output chunker respect gold chunks available training data used regular expressions improve chunks. rules concern conjunctions punctuation prepositions rules used join adjacent chunks. rules mainly join preposition noun phrase single chunk well noun phrases separated punctuation conjunctions combination those. addition chunker stanford parser producing part speech lemma dependency analysis. freely available state-of-the-art monolingual word aligner producing token alignments. order produce chunk alignment possible chunk alignment weighted according number aligned tokens chunks. hungarian-munkres algorithm used chunk alignments optimize overall alignment weight. feature description jaccard overlap jaccard overlap stopwords jaccard overlap stopwords difference length betweeen chunks difference length betweeen chunks wordnet path similarity among sense pairs wordnet similarity among sense pairs wordnet similarity among sense pairs simulating root maximum common subsumer simulating root maximum common subsumer simulating root maximum common subsumer whether chunk senses speciﬁc chunk senses difference wordnet depth segment head minimum value pairwise difference wordnet depth maximum value pairwise difference wordnet depth lemmatized lowercased tokens chunk lemmatized lowercased tokens chunk maximum similarity value using ﬁrst resource section maximum similarity value using second resource section maximum similarity value using third resource section used support vector machines training data reduced indistinctly joined available datasets performed grid search optimize cost gamma parameters using randomly shufﬂed -fold cross validation. development experiments found classiﬁer failing detect fact removed labels training ﬁnal system. development experiments also showed performance classiﬁer sensitive quality chunker. classiﬁer ﬁrst trained tested using cross-validation data contained gold chunks gold alignments classiﬁer test folds contained system chunking performance suffered. tried several variations gold automated versions train data obtained best cross-validation results using following versions training folds concatenation gold version version mixing automatic chunking gold-standard alignments labels. thus trained ﬁnal classiﬁers semi-automatically produced version. euclidean distance collobert weston word vector distances converted similarity range using following formula d/max contains distances observed dataset. ppdb paraphrase database values used xxxl version. resource yields conditional probabilities. scores undirected case database contains values directions average. given pair aligned chunks compute similarity word pair chunks maximum similarities according three resources above. compute similarity chunks mean similarities addition similarities word ﬁrst chunk addition similarities word second chunk follows table results three systems datasets. columns show results evaluation criteria stands type score. best results bold. last shows results full system using gold standard chunks instead automatically produced chunks. developed three systems baseline improved baseline better chunking alignment models baseline labeling scoring modules full system supervised labeling similarity-based scoring systems developed using training subset dataset alone access test. evaluated three systems according evaluation measures section table shows results headlines images datasets. better chunking alignment improves alignment score points datasets respect base. poor performance alignment causes baseline system also attain scores type score well overall score comparison base+ full shows classiﬁer able better assign types specially images. method produce score also stronger full thus produces best overall note performance four available metrics decreases metrics bounded bounded type score. score easier task. fact labeling perfect type score would score drop around absolute points observed datasets. regarding datasets headlines challenging lower scores across four evaluations. performed analysis errors performed full system level processing starting chunking. last table reports results full system running gold standard chunks. results improve datasets high alignment results show chunking quality good performance. results using gold chunks comparable datasets indicates difference performance headlines images running system data caused automatic chunker. thus conclude headlines difﬁcult chunk images causes worse performance dataset. errors chunking seem related verbs shown example below could caused particular syntactic structures used news headlines different expected automatic chunker. regarding quality alignments found aligner tended miss alignments access semantic relations words numbers following pairs include bold chunks aligned system order check type-labeling errors built confusion matrix full system gold standard headlines dataset confusion matrix built keeping correct alignments incorrectly aligned tokens cannot analyzed type errors. errors system caused figure label score confusion matrices heatmaps full system gold standard scores labels headlines dataset. gold standard labels scores rows system labels scores columns. cases system able label equivalent chunks mistakes recognizing identical entities synonyms. next examples chunks sentence labeled speciﬁc chunks sentence however sentence pairs alignments labeled equi instead spe. matt smith leave doctor years matt smith quits doctor blasio sworn york mayor succeeding bloomberg bill blasio sworn york mayor succeeding michael bloomberg regarding errors scores fig. shows confusion matrix scores rounded nearest integer. errors contiguous scores exceptions like system returning instead instead shows bias system towards high scores would like future. table shows results best system respect stateof-the-art semeval task competition included subtask interpretable based dataset. system outperforms best system datasets except score results headlines. chunker post processing rules could explain better performance headlines. alignment software system. labeling module supervised system based support vector machines participants could send three runs. note task also included track gold chunks made available participants. sake space focus natural track systems need chunk sentences own. similar ours. better results explained larger number features include similarity scores scoring module wordnet similarity measures. unlike ours scoring module based labels. good results participating system improvement baselines show interpretable feasible task steps alignment labeling relations scoring similarity. also indirect evidence task well designed annotation consistent. note participated semeval task previous version system. difference strategy train classiﬁer alignment labels based gold standard chunks uses mixture gold chunks system-produced chunks order judge whether information returned interpretable system used clarify explain semantic judgments humans performed user studies. ﬁrst devised verbalization algorithm which given sentences similarity score typed scored alignment chunks returns english text verbalizing differences commonalities sentences. contrasted activities users without interpretable verbalizations trying show verbalizations helped users case studies. given output interpretable system devised simple templatebased algorithm verbalize alignment information natural language. label alignment used select template score used qualify strength relation summarized table example verbalization sentence pair shown bottom figure aware verbalization algorithm could improved specially avoid repetitions make text ﬂuent easier read. currently produces sentence alignment resulting much text. information several alignments could synthetized summarized shorter messages. case show simple verbalization algorithm effective enough user case studies. table templates employed producing verbalizations summarized label. refer aligned chunks sentence respectively. score used select qualiﬁers simi rel. professor smith asked students write headlines reading texts. graded students using headlines reference. grades used professor smith following ones insufficient used annotate datasets figure corresponds case verbalization shown volunteer. measured agreement volunteers gold standard score. order contrast whether verbalizations impact performance users task three scenarios verbalization automatic verbalization based interpretable gold standard automatic verbalization based interpretable system output. second user study consider english second language education scenario volunteers played role inspector overseeing grades given lecturer student. student summarize piece news single headline. re-use pairs sentences headlines dataset together similarity score. volunteer given sentences ﬁrst reference headline used professor asses student second headline produced student. similarity score used grade given student. task given user thus assess extent agree grading. users given following information reference headline professor headline done student grade given optionally feedback form automatically produced verbalization. collect feedback form integer figure shows instructions example pair alongside grade. three scenarios verbalization automatic verbalization based interpretable gold standard automatic verbalization based interpretable system output. conduct user studies randomly selected sentence pairs headlines dataset sentence pairs accompanied gold standard similarity score ranges thus sampled pairs uniformly according score. pairs used user studies. ﬁrst user study involved native english speakers. second user study related english second language setting involved non-native english speakers veriﬁed level english. sketch helps organize ﬁles distributed without verbalizations ones distributed verbalizations based gold standard annotations semeval data ones distributed verbalizations produced system described section sketch distributes items across users verbalizations uniform order reduce biases across users verbalizations item sets. sketch used distribute ﬁles scenarios. table sketch used distribute item sets among participants three possible verbalizations option rows. number underscore refers order presentation user e.g. shown user measure results ﬁrst user study correlation scores given participants gold standard score. follow tradition open evaluation tasks pearson coefﬁcient correlation main measure also report spearman rank correlation. table shows correlation non-verbalized pairs gold standard verbalized pairs system verbalized pairs. correlation measures seem output similar values higher correlation values verbalized scenarios showing explanations indeed helpful task. verbalizations obtained system output comparable gold standard showing approximate performance might enough helpful task. even amount data points small performed signiﬁcance tests verbalization options using fisher’s z-transformation relatedness difference system verbalization verbalizations statistically signiﬁcant pearson spearman p-values gold standard verbalizations verbalsecond user study results correspond agreement level scenario table reports mean agreement level binary agreement user study effect system verbalizations clear previous case binary agreement better mean agreement level similar automatic verbalizations produced using gold standard annotations clear impact task users tend agree scores assigned lecturer. difference system verbalization verbalizations statistically signiﬁcant case difference gold verbalizations verbalization signiﬁcant results show simple method produce verbalizations based interpretable annotations effective user studies users could accomplish better task hand. strong indication annotation task well-deﬁned leads verbalizations intelligible help users understand semantic similarity target texts. results obtained interpretable systems promising clear positive effect ﬁrst user study. paper presents interpretable semantic textual similarity formalize interpretability layer sts. describe publicly available dataset sentence pairs relations segments sentence labeled relation type similarity score. labels represent relations segments equivalence opposition speciﬁcity similarity relatedness together factuality polarity differences. interpretable labels closely related available natural logic textual entailment thus dataset complementary resources presented pavlick also built system interpretable based pipeline ﬁrst identiﬁes chunks input sentence aligns chunks sentences ﬁnally uses supervised system label alignments mixture several similarity measures score alignments. good results improvement baselines show interpretable feasible task steps alignment labeling relations scoring similarity. also indirect evidence task well designed annotation consistent also supported high inter-annotator agreement. beyond low-level annotation also studied whether annotations could useful ﬁnal applications. constructed simple verbalization algorithm given sentences interpretable annotations produces textual explanation differences/similarities sentences. carried succesfull small-scale user studies show evidence users access explanations perform task better. take preliminary indication automatically produced explanations effective understand texts. near future would like improve performance interpretable system. current system performs step independently enforce consistency. instance produce weak relation type like strong similarity score vice versa. fact alignment score could feed typing type alignment could useful assigning score. thus currently exploring joint algorithms would perform steps together using neural networks error analysis shows system bias towards equivalence high scores future versions system remedy. would also like improve simple naive verbalization algorithm effectiveness real tasks also depends producing natural-looking text point contain superﬂuous information. finally plan perform extensive user study real task. tutoring systems english second language look like promising direction building systems automatically grade students produce explanations grading. aitor gonzalez-agirre inigo lopez-gazpio supported doctoral grants mineco. work described project partially funded mineco projects muster tuner well basque government also want thank volunteers participated user studies.", "year": 2016}