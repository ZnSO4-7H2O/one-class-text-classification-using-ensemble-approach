{"title": "Reinforcement Learning in POMDPs with Memoryless Options and  Option-Observation Initiation Sets", "tag": ["cs.AI", "cs.LG"], "abstract": "Many real-world reinforcement learning problems have a hierarchical nature, and often exhibit some degree of partial observability. While hierarchy and partial observability are usually tackled separately (for instance by combining recurrent neural networks and options), we show that addressing both problems simultaneously is simpler and more efficient in many cases. More specifically, we make the initiation set of options conditional on the previously-executed option, and show that options with such Option-Observation Initiation Sets (OOIs) are at least as expressive as Finite State Controllers (FSCs), a state-of-the-art approach for learning in POMDPs. OOIs are easy to design based on an intuitive description of the task, lead to explainable policies and keep the top-level and option policies memoryless. Our experiments show that OOIs allow agents to learn optimal policies in challenging POMDPs, while being much more sample-efficient than a recurrent neural network over options.", "text": "cyclic tasks solved. using recurrent neural networks options top-level policy addresses challenges brings design complexity rnns rnns also limitations regarding long time horizons memory decays time thesis precup suggests options already close addressing partial observability thus removing need complicated solutions. paper prove intuition correct showing standard options sufﬁce pomdps; introducing option-observation initiation sets make initiation sets options conditional previously-executed option; contrast existing algorithms pomdps oois handle repetitive tasks restrict action available sub-tasks keep top-level option policies memoryless. wide range robotic simulated experiments section conﬁrm oois allow partially observable tasks solved optimally demonstrate oois much sample-efﬁcient recurrent neural network options illustrate ﬂexibility oois regarding amount domain knowledge available design time. section demonstrate robustness oois sub-optimal option sets. generally accepted designer provides options initiation sets show section random initiation sets combined learned option policies termination functions allow oois used without domain knowledge. motivating example oois designed solve complex partially-observable tasks decomposed fully-observable sub-tasks. instance robot ﬁrst-person sensors able avoid obstacles open doors manipulate objects even precise location building obmany real-world reinforcement learning problems hierarchical nature often exhibit degree partial observability. hierarchy partial observability usually tackled separately show addressing problems simultaneously simpler efﬁcient many cases. speciﬁcally make initiation options conditional previously-executed option show options option-observation initiation sets least expressive finite state controllers state-of-the-art approach learning pomdps. oois easy design based intuitive description task lead explainable policies keep top-level option policies memoryless. experiments show oois allow agents learn optimal policies challenging pomdps much sample-efﬁcient recurrent neural network options. real-world applications reinforcement learning face main challenges complex long-running tasks partial observability. options particular instance hierarchical focus addresses ﬁrst challenge factoring complex task simpler sub-tasks instead learning action perform depending observation agent learns top-level policy repeatedly selects options turn execute sequences actions returning second challenge partial observability addressed maintaining belief agent thinks full state reasoning possible future observations storing information external memory later reuse using recurrent neural networks allow information time-steps combined solutions challenges recently designed planning solutions learning algorithms ideal. hq-learning decomposes task sequence fully-observable subtasks precludes khepera robothas gather objects terminals separated wall bring root objects gathered terminal becomes empty requires many journeys root terminal. terminal emptied automatically reﬁlled. robot therefore alternatively gather objects terminals episode ﬁnishes terminals emptied random number times. root colored marked paper qr-code encoding terminal screen displaying color dynamic qrcode robot cannot read qr-codes away state terminal cannot observed root agent decide terminal makes environment partially observable requires robot remember terminal last visited whether full empty. robot able control speed wheels. wireless camera mounted robot detects bright color blobs ﬁeld view read nearby qrcodes. low-level actions observations combined complicated task motivate hierarchical reinforcement learning. fixed options allow robot move towards largest green blue blob ﬁeld view. options terminate soon qr-code front camera close enough read. robot learn policy options solves task. robot gather large number objects alternating terminals several times. repetitive nature task incompatible hq-learning options standard initiation sets able solve task top-level policy memoryless cannot remember terminal robot arrives root whether terminal full empty. terminals dozen feet away root almost hundred primitive actions executed complete root/terminal journey. without options represents figure observations khepera robot. color image camera. color blobs detected vision system observed robot. qr-codes decoded robot couple inches away them. oois allow option selected conditionally previously executed much simpler combining options recurrent neural networks ability oois solve complex pomdps builds time abstraction capabilities expressiveness options. section shows oois allow policy robotic task learned expert level. additional experiments demonstrate top-level option policies learned agent oois lead substantial gains standard initiation sets even option reduced unsuited task section formally introduces markov decision processes options partially observable mdps finite state controllers presenting main contribution section markov decision processes discrete-time markov decision process discrete actions deﬁned possibly-inﬁnite states ﬁnite actions reward function provides scalar reward state transition transition function outputs probability distribution states given state-action pair discount factor deﬁnes sensitive agent future rewards. stochastic memoryless policy maps state probability distribution actions. goal agent policy maximizes expected γtrt] obtainable options options framework deﬁned context mdps consists options option tuple memoryless option policy termination function gives probability option terminate state initiation deﬁnes states started memoryless top-level policy maps states distribution options allows choose option start given state. option started executes termination point selects option based current state. partially observable mdps real-world problems completely captured mdps exhibit least degree partial observability. partially observable extended components possibly-inﬁnite observations function produces observations based unobservable state process. different states requiring different optimal actions produce observation. makes pomdps remarkably challenging reinforcement learning algorithms memoryless policies select actions options based current observation typically longer sufﬁce. finite state controllers finite state controllers commonly used pomdps. deﬁned ﬁnite nodes action function maps nodes probability distribution actions successor function maps nodes observations probability distribution next nodes initial function maps initial observations nodes ﬁrst time-step agent observes activates node sampling action performed sampling time-step node sampled action sampled fscs allow agent select actions according entire history past observations shown best approaches pomdps oois main contribution make options least expressive relevant pomdps fscs able leverage hierarchical structure problem. main contribution option-observation initiation sets make initiation sets options conditional option terminated. prove oois make options least expressive fscs even top-level option policies memoryless options without oois strictly less expressive fscs section show robotic simulated tasks oois allow challenging pomdps solved optimally. conditioning previous option descriptions partially observable tasks natural language often contain allusions sub-tasks must sequenced cycled through possibly branches. good memory-based policy motivating example agent bring objects terminals root described green terminal root back green terminal full blue terminal otherwise symmetrically blue terminal. sequence sub-tasks contains condition easily translated options. options sharing single policy green terminal root executed terminal full empty. root option goes back green terminal follow ωge. green terminal empty going back therefore forbidden forces agent switch blue terminal green empty. formally deﬁne main contribution optionobservation initiation sets allow describe options follow ones. deﬁne initiation option options available time depends observation previouslyexecuted option observations options. allows agent condition option selected time terminated even toplevel policy observe ωt−. top-level option policies remain memoryless. observe keeps observation space top-level policy small instead extending without impairing representational power oois shown next sub-section. oois make options expressive fscs finite state controllers state-of-the-art policies applicable pomdps proving options oois expressive fscs provide lower bound expressiveness oois ensure applicable wide range pomdps. theorem oois allow options represent policy expressed using finite state controller. figure two-nodes finite state controller emits inﬁnite sequence abab... based uninformative observation cannot expressed using options without oois. option corresponds edge fsc. equation ensures every option stops emitted single action takes transition every timestep. equation maps current option action emitted destination node corresponding edge. t−nt implement show reduction uses options trivial policies execute single time-step leads large amount options compensate. practice expect able express policies real-world pomdps much less options number states would require shown simulated robotic experiments addition sufﬁcient next sub-section proves oois necessary options expressive fscs. options regular initiation sets able express memory-based policies tiny valid finite state controller presented figure cannot mapped options policy options proves options without oois strictly less expressive fscs. theorem options without oois expressive finite state controllers. proof. figure shows finite state controller emits sequence alternating based constant uninformative observation task requires memory observation provide information last letter emitted must emitted. options memoryless policies options executing multiple time-steps unable represent exactly. combination options execute single time-step cannot represent either options framework unable represent memorybased policies single-time-step options experiments section illustrate oois allow agents perform optimally environments options without oois fail. section shows oois allow agent learn expert-level policy motivating example section shows top-level option policies required repetitive task learned learning option policies allow agent leverage random oois thereby removing need designing them. section progressively reduce amount options available agent demonstrate oois still allow good memory-based policies emerge sub-optimal amount options used. results averaged runs standard deviation represented light regions ﬁgures. source code experimental data scripts plotting scripts experiments along detailed description robotic setup available supplementary material. video detailing robotic experiment available http//steckdenis.be/oois_demo.mp. learning algorithm agents learn top-level option policies using single feed-forward neural network hidden layer neurons trained using policy gradient adam optimizer neural network takes three inputs produces output. inputs problem-speciﬁc observation features one-hot encoded current option mask mask. output joint probability distribution selecting actions options terminating continuing current option trainable weights biases layer sigmoid function element-wise product vectors. fraction ensures valid probability distribution produced network. initiation sets options implemented using mask input neural network vector integers dimension output. executing top-level policy mask forces probability primitive actions zero preserves option according prevents top-level policy terminating. executing option policy mask allows primitive actions executed. instance options three actions mask executing options. executing top-level policy mask option ﬁnished initiation ﬁrst option according rule second option. neural network trained using policy gradient following loss action executed time simple discounted future rewards ignores changes current option. gives agent information complete outcome action option directly evaluating ﬂattened policy. baseline used reduce variance estimate predicts expected cumulative reward obtainable option using separate neural network trained monte-carlo return obtained comparison lstm options order provide complete evaluation oois variant networks section hidden layer replaced layer lstm units also evaluated every task. units leads best results experiments ensures fair comparison lstm oois. experiments lstm agents provided options agent oois. providing option less options leads worse results. options allow lstm network focus important observations reduces time horizon considered. shorter time horizons shown beneﬁcial lstm despite efforts lstm options manages learn good policies robotic experiment requires twice amount episodes oois repetitive task dozens repetitions seem confuse network quickly diverges good policy learn treemaze much complex version t-maze task originally used benchmark reinforcement learning lstm agents lstm agent learns optimal policy episodes results illustrate learning recurrent neural networks sometimes difﬁcult oois allow reliably obtain good results minimal engineering effort. object gathering ﬁrst experiment illustrates oois allow expertlevel policy learned complex robotic partiallyobservable repetitive task. experiment takes place figure cumulative reward episode obtained object gathering task oois without oois using lstm options. oois learns expert-level policy much quicker lstm options. lstm curve ﬂattens-out episodes. environment described section robot gather objects terminals green blue bring back root location. actual robot effector navigates root terminals pretends move objects. agent receives reward reaches full terminal terminal empty. beginning episode terminal contains objects amount selected randomly terminal. agent goes empty terminal re-ﬁlled objects. episode ends emptyings whether terminal full empty observed agent terminal. agent therefore remember information acquired terminals order properly choose root terminal agent access memoryless options green blue objects terminate agent close enough read qr-code displayed them. initiation ωg..g ωb..b ωgibi description options oois purposefully uninformative illustrates little information agent task. option used experiment also richer simple example section solution problem going back empty terminal encoded oois must learned agent. agents without oois learn top-level policies options. compare ﬁxed agent using expert interprets options follows ωr..r root full/empty green/blue terminal ωg..gb..b green/blue terminal root previous terminal full/empty green/blue. root oois ensure option amongst green full green green empty blue blue full blue blue empty green selected top-level policy corresponds top-level policy learned oois allow task solved shown figure standard initiation sets allow task learned. experiments robot slow developed small simulator task used produce figure successfully asserted accuracy using -episodes runs actual robot. agent learns properly select options terminals depending qr-code output proper distribution options root thereby matching expert policy. lstm agent learns policy requires twice amount episodes high variance displayed figure comes varying amounts objects terminals random selection many times emptied. modiﬁed duplicatedinput cases hierarchical reinforcement learning agent provided policies several options. case oois allow agent learn toplevel policy option policies termination functions. experiment agent learn top-level option policies copy characters input tape output tape removing duplicate agent observes single input character time write character output tape time-step. input tape sequence symbols random number agent observes single symbol read i-th position input sequence observe actions representing symbol whether must pushed onto output tape whether incremented decremented reward given correct symbol written output tape. episode ﬁnishes reward incorrect symbol written. agent access options oois designed cannot follow itself restriction reward shaping hint option provided. agent automatically discovers must copy current character output must skip character without copying also learns top-level policy selects observing allowed otherwise figure cumulative reward episode obtained modiﬁed duplicatedinput random designed oois withoois using lstm options. despite efforts lstm options repeatedly learns forgets optimal policies shown high variance line. options standard initiation sets fails agent without oois learns copy characters never skips shows oois necessary learning task allow learn top-level option policies suited repetitive partially observable task. option policies learned agent becomes able adapt random oois thereby removing need designing oois. agent options option randomly-selected options initiation initiation sets re-sampled run. agents learn leverage option achieve good results average looking individual runs random oois allow optimal policies learned several runs require time others explains high variance noticeable steps shown figure next section shows improperly-deﬁned human-provided options happen design phase still allows agent perform reasonably well. combined results random oois shows oois tailored exact amount domain knowledge available particular task. treemaze optimal options oois difﬁcult design. agent learns option policies previous section demonstrates random oois sufﬁce. experiment focuses human-provided option policies shows sub-optimal options arising misspeciﬁcation environment normal trial-and-error design phase prevent agents oois learning reasonably good policies. treemaze generalization t-maze environment arbitrary heights. agent starts root tree-like maze depicted figure reach extremity leaves. leaf reached chosen uniformly randomly episode indicated agent using bits observed time ﬁrst time-steps. agent receives afterwards remember order navigate goal. agent observes position current corridor number junctions already crossed reward given time-step reaching goal. episode ﬁnishes agent reaches leaves. optimal reward consider options predeﬁned memoryless policies several sharing policy encoding distinct states -bit memory bits unknown. partial-knowledge options right terminate. fullknowledge options corresponding leaf. oois deﬁned option followed itself represents memory state single ﬂipped five agents learn top-level policy requires learn available options remember leaf agents know name meaning options. three agents access options agent oois access full-knowledge options therefore cannot disambiguate unknown bits. agent oois restricted options therefore cannot reach odd-numbered goals. options agents terminate ﬁrst cells ﬁrst corridor allow top-level policy observe second third bits. figure shows agent oois consistently learns optimal policy task. number options reduced quality resulting policies decreases still remaining agent without oois. even agent options cannot reach half goals performs better agent without oois options. experiment demonstrates oois provide measurable beneﬁts standard initiation sets even option largely reduced. combined three experiments demonstrate oois lead optimal policies challenging pomdps consistently outperform lstm options allow option policies learned still used reduced domain knowledge available. figure cumulative reward episode obtained treemaze using options. even insufﬁcient amount options oois lead better performance oois options. lstm options learns task episodes. paper proposes oois extension initiation sets options restrict options allowed executed terminates. makes options expressive finite state controllers. experimental results conﬁrm challenging partially observable tasks simulated physical robots requiring exact information storage hundreds time-steps solved using options. experiments also illustrate oois lead reasonably good policies option improperly deﬁned learning option policies allow random oois used thereby providing turnkey solution partial observability. options oois also perform surprisingly well compared lstm network options. lstm options require design oois ability learn without a-priori knowledge comes cost sample efﬁciency explainability. furthermore random oois easy lstm lead superior results oois therefore provide compelling alternative recurrent neural networks options applicable wide range problems. finally compatibility oois large variety reinforcement learning algorithms leads many future research opportunities. instance obtained encouraging results continuous action spaces using cacla implement parametric options take continuous arguments executed continuous-action hierarchical pomdps. thanks finn lattimore gave computer ﬁrst author could ﬁnish paper attending conference sydney computer unexpectedly fried. thanks joris scharpff helpful input paper. volodymyr mnih adri`a puigdom`enech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning volume pages leonid peshkin nicolas meuleau leslie kaelbling. learning policies external memory. proceedings international conference machine learning pages richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems volume chen tessler shahar givony zahavy daniel mankowitz shie mannor. deep hierarchical approach lifelong learning minecraft. european workshop reinforcement learning hado hasselt marco wiering. reinforcement learning continuous action spaces. proceedings ieee symposium approximate dynamic programming reinforcement learning pages j¨urgen schmidhuber. hq-learning. adaptive behavior", "year": 2017}