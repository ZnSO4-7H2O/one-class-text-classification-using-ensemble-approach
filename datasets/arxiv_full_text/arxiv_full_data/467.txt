{"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "text": "figure schematic view neural machine translation. source words ﬁrst mapped word vectors recurrent neural network upon seeing symbol ﬁnal time step initializes target blue rnn. target time step attention applied source combined current hidden state produce prediction next word. prediction back target rnn. currently several existing implementations. many systems developed industry google microsoft baidu closed source unlikely released unrestricted licenses. many systems groundhog blocks tensorﬂowseqseq lamtram seqseq-attn exist mostly research code. libraries provide important functionality minimal support production users. perhaps promising university edinburgh’s nematus system originally based nyu’s system. nematus provides high-accuracy translation many options clear documentation used several successful research projects. development project aimed build upon strengths system providing additional documentation functionality provide useful open-source framework describe open-source toolkit neural machine translation toolkit prioritizes efﬁciency modularity extensibility goal supporting research model architectures feature representations source modalities maintaining competitive performance reasonable training requirements. toolkit consists modeling translation support well detailed pedagogical documentation underlying techniques. neural machine translation methodology machine translation remarkable improvements particularly terms human evaluation compared rule-based statistical machine translation systems originally developed using pure sequence-to-sequence models improved upon using attention-based variants become widely-applied technique machine translation well effective approach related tasks dialogue parsing summarization. approaches standardized becomes important machine translation community develop open implementations researchers benchmark against learn from extend upon. community beneﬁted greatly toolkits like moses phrase-based cdec syntax-based toolkits provide foundation build upon. toolkit provide shared mind introduce opennmt opensource framework neural machine translation. opennmt complete implementation. addition providing code core translation tasks opennmt designed three aims prioritize ﬁrst training test efﬁciency maintain model modularity readability support signiﬁcant research extensibility. engineering report describes system targets criteria. begin brieﬂy surveying background describing high-level implementation details describing speciﬁc case studies three criteria. showing benchmarks system terms accuracy speed memory usage several translation translation-like tasks. takes conditional language modeling view translation modeling probability target sentence given source sentence distribution estimated using attention-based encoder-decoder architecture source encoder recurrent neural network maps source word word vector processes sequence hidden vectors target decoder combines hidden representation previously generated words source hidden vectors predict scores possible next word. softmax layer used produce next-word distribution source hidden vectors inﬂuence distribution attention pooling layer weights source word relative expected contribution target prediction. complete model trained end-to-end minimize negative loglikelihood training corpus. unfolded network diagram shown figure lstm help model learn long-term features. translation requires relatively large stacked rnns consist several vertical layers rnns time step input feeding previous attention vector back input well predicted word shown quite helpful machine translation test-time decoding done beam search multiple hypothesis target predictions considered time step. implementing correctly difﬁcult motivates inclusion framework. opennmt complete library training deploying neural machine translation models. system successor seqseq-attn developed harvard completely rewritten ease efﬁciency readability generalizability. includes vanilla models along support attention gating stacking input feeding regularization beam search options necessary state-of-the-art performance. implemented lua/torch mathematical framework easily extended using torch’s internal standard neural network components. also extended adam lerer facebook research support python/pytorch framework api. system developed completely open github licensed. ﬁrst version primarily contributions systran paris harvard group. since ofﬁcial beta release project starred users active development outside organizations. project active forum community feedback hundred posts last months. also live demonstration available system nice aspect model relative compactness. opennmt system including preprocessing roughly lines code python version less lines comparison moses framework including language modeling lines. makes system easy completely understand newcomers. project fully self-contained depending minimal number external libraries including also simple language independent reversible tokenization detokenization tools. system efﬁciency systems take days weeks train training efﬁciency paramount concern. slightly faster training make difference plausible impossible experiments. memory sharing training gpu-based models memory size restrictions common limiter batch size thus directly impact training time. neural network toolkits torch often designed trade-off extra memory allocations speed declarative simplicity. opennmt wanted ways implemented external memory sharing system exploits known time-series control systems aggressively shares internal buffers between clones. potential shared buffers dynamically calculated exploration network graph starting training. practical aggressive memory reuse provides saving memory default model size. process independent batches training phase. modes available synchronous asynchronous training. synchronous training batches parallel simultaneously gradients aggregated update master parameters resynchronization following batch. asynchronous training batches independent independent gradients accumulated master copy parameters. asynchronous known provide faster convergence experiments gpus show speed epoch slight loss training efﬁciency. training similar loss gives total speed-up training. c/mobile/gpu translation training systems requires signiﬁcant code complexity facilitate fast back-propagation-through-time. deployment system much less complex requires forwarding values network running beam search much simpliﬁed compared smt. opennmt includes several different translation deployments specialized different run-time environments batched cpu/gpu implementation quickly translating large sentences simple single-instance implementation mobile devices specialized implementation. ﬁrst implementation suited research instance allowing user easily include constraints feasible sentences ideas pointer networks copy mechanisms. last implementation particularly suited industrial standard production environments; reads structure network uses eigen package implement basic linear algebra necessary decoding. table compares different implementations based batch size beam size. secondary goal desire code readability non-experts. targeted goal explicitly separating many optimizations core model including tutorial documentation within code. test whether approach would allow novel feature development experimented case studies. case study factored neural translation feature-based factored neural translation instead generating word time step model generates word associated features. instance system might include words separate case features. extension requires modifying inputs output decoder generate multiple symbols. opennmt aspects abstracted core translation code therefore factored translation simply modiﬁes input network instead process featurebased representation output generator network instead produce multiple conditionally independent predictions. case study attention networks attention encoder step translation crucial model perform well. default method utilize global attention mechanism. however many types attention recently proposed including local attention sparse-max attention hierarchical attention among others. simply module opennmt easily substituted. recently harvard group developed structured attention approach utilizes graphical model inference compute attention. method quite computationally complex; however modularized torch interface used opennmt substitute standard attention. deep learning quickly evolving ﬁeld. recently work variational seqseq auto-encoders memory networks propose interesting extensions basic seqseq models. next discuss case study demonstrate opennmt extensible future variants. multiple modalities recent work shown nmt-like systems effective imageto-text generation tasks task quite different standard machine translation source sentence image. translation require style modal inputs case study adapted systems non-textual inputs opennmt. ﬁrst image-to-text system developed mathematical model replaces source deep convolution source input. excepting preprocessing entire adaptation requires less lines additional code also open-sourced github.com/opennmt/imtext. second speech-to-text recognition system based work chan system implemented directly opennmt replacing source encoder pyrimidal source model. tokenization aimed opennmt standalone project depend commonly used tools. instance moses tokenizer language speciﬁc heuristics necessary nmt. therefore include simple reversible tokenizer includes markers seen model allow simple deterministic detokenization extremely simple languageindependent tokenization rules. tokenizer also perform byte pair encoding become popular method sub-word tokenizaword embeddings opennmt includes tools simplifying process using pretrained word embeddings even allowing automatic download embeddings many languages. allows training languages domain relatively little aligned data. additionally opennmt export word embeddings trained models standard formats. allows analysis external tools tensorboard shown figure document runs model. expect performance memory usage improve development. public benchmarks available http//opennmt. net/models/ also includes publicly available pre-trained models tasks tutorial instructions tasks. benchmarks intel core .ghz trained geforce cuda cudnn additionally trained multilingual translation model following johnson model translates french spanish portuguese italian romanian. training data sentences selected open parallel corpus speciﬁcally europarl globalvoices ted. corpus selected multisource multi-target sentence translation languages. corpus tokenized using shared byte pair encoding comparative results multi-way translation independent training presented table systematically large improvement shows language pair beneﬁts training jointly language pairs. additionally found interest community using opennmt non-standard tasks like sentence document summarization dialogue response generation among others. using opennmt able replicate sentence summarization results chopra reaching rouge- score gigaword data. also trained model million sentences opensubtitles data based work vinyals achieving comparable perplexity. introduce opennmt research toolkit prioritizes efﬁciency modularity. hope develop opennmt maintain strong results research frontier providing stable framework production use. samuel bowman luke vilnis oriol vinyals andrew rafal j´ozefowicz samy bengio. generating sentences continuous space. signll conference computational natural language learning conll berlin germany august pages http//aclweb.org/anthology/k/k/k-.pdf. kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoderdecoder statistical machine translation. proc emnlp. sumit chopra michael auli alexander rush seas harvard. abstractive sentence summarization attentive recurrent neural networks. proceedings naacl-hlt pages junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. jeffrey dean greg corrado rajat monga chen matthieu devin mark andrew senior paul tucker yang quoc large advances scale distributed deep networks. neural information processing systems. pages chris dyer jonathan weese hendra setiawan adam lopez ferhan ture vladimir eidelman juri ganitkevitch phil blunsom philip resnik. cdec decoder alignment learning framework ﬁnite-state context-free translation models. proceedings system demonstrations. association computational linguistics pages mike schuster quoc maxim krikun yonghui zhifeng chen nikhil thorat fernanda vigas martin wattenberg greg corrado macduff hughes jeffrey dean johnson. google’s multilingual neural machine translation system enabling zeroshot translation philipp koehn hieu hoang alexandra birch chris callison-burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens moses open source proc toolkit statistical machine translation. acl. association computational linguistics pages yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging arxiv preprint human machine translation. arxiv. jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. corr abs/.. http//arxiv.org/abs/.. zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy. hierarchical attention networks document classiﬁcation. proceedings conference north american chapter association computational linguistics human language technologies.", "year": 2017}