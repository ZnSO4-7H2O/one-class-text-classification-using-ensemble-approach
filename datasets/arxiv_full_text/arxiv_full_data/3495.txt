{"title": "Adaptive PCA for Time-Varying Data", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "In this paper, we present an online adaptive PCA algorithm that is able to compute the full dimensional eigenspace per new time-step of sequential data. The algorithm is based on a one-step update rule that considers all second order correlations between previous samples and the new time-step. Our algorithm has O(n) complexity per new time-step in its deterministic mode and O(1) complexity per new time-step in its stochastic mode. We test our algorithm on a number of time-varying datasets of different physical phenomena. Explained variance curves indicate that our technique provides an excellent approximation to the original eigenspace computed using standard PCA in batch mode. In addition, our experiments show that the stochastic mode, despite its much lower computational complexity, converges to the same eigenspace computed using the deterministic mode.", "text": "paper present online adaptive algorithm able compute full dimensional eigenspace time-step sequential data. algorithm based one-step update rule considers second order correlations previous samples time-step. algorithm complexity time-step deterministic mode complexity time-step stochastic mode. test algorithm number time-varying datasets diﬀerent physical phenomena. explained variance curves indicate technique provides excellent approximation original eigenspace computed using standard batch mode. addition experiments show stochastic mode despite much lower computational complexity converges eigenspace computed using deterministic mode. principal component analysis important machine learning techniques many reasons. firstly unsupervised learning algorithm theoretically proven capture maximal variability input data given ﬁxed-size low-dimensional space. another main reason directly deals eigenspace problem hand. real world endless amount problems physical phenomena modelled eigenvalue equations. important example dirac equation assumes variables physical object obey eigenvalue problem quantum physics quantum states electron atom take actually time-dependent eigenfunctions called quantum eigenstates despite elegance widely used last four decades. reason that basic form quadratic space time complexity requires large memory processing speed. nowadays machines shown capable handling complexity thanks larger available memory faster capabilities. however wide range problems dimensionality data massive extracting principal components standard becomes infeasible. many algorithms developed signiﬁcant principal components linear complexity dependence data size. however approaches stochastic limited extracting certain number eigenvectors paper consider time-dependent systems require regular monitoring analysis time-step. particularly important instance equilibria stability analysis system. many physical phenomena electron eigenstates example mentioned above time-dependent behavior signiﬁcant eigenvectors converges equilibrium eigenstate. propose adaptive algorithm able capture eigenvectors data complexity time-step deterministic mode complexity time-step stochastic mode number previous time-steps. test algorithm time-varying datasets diﬀerent physical phenomena. compare performance algorithm standard applied batch-mode. literature main directions research taken. ﬁrst concerning applications employ solving real-world problems second direction pca-optimization concerned optimization computational complexity pca. link directions clear since studies application direction assume pre-computed eigenspace focus mainly distribution test data eigenspace. hand optimization direction target use-case obvious. addition optimization-direction algorithms stochastic nature usually tested rather simple datasets data global eigenspase easily derived. case always consider pre-computed eigenspace matter computational complexity required ﬁnding fact many online datasets provide list signiﬁcant eigenvectors studied samples. regard applications research well reported ﬁelds computer vision computer graphics. instance facial recognition kirby sirovich proposed holistic representation human face images extracting orthogonal dimensions form face-space called eigenfaces gong ﬁrst relationship distribution samples eigenspace called manifolds actual pose image human face. extended using reproducing kernel hilbert spaces non-linearly face-space much higher dimensional space knittel paris employed pca-based technique initial seeds vector quantization image compression. number previous reported uses pca-related methods computer graphics visualization literature. instance nishino proposed method called eigen-texture creates image sample range images using pca. found partitioning samples smaller cell-images improved rendering surface-based data. grabner proposed hardware accelerated technique uses multiple eigenspaces method image-based reconstruction polygon mesh model. employed dynamic projections visualization multivariate data. broersen discussed techniques generation transfer functions used assign optical properties color opacity attributes volume data. takemoto used feature space reduction support transfer funtion design exploration volumetric microscopy data. fout presented volume compression method based transform coding using karhunen-loève transform closely related pca. pca-optimization research power iteration remains popular techniques ﬁnding eigenvectors recent leterature shamir proposed stochastic algorithm proven converge faster power precision convergence. addition techniques experimentally tested extract limited number signiﬁcant eigenvectors. arora proposed stochastic techniques based gradient-descent learning rule. slow convergence rate gradient-descent rule main limitation techniques. many algorithms developed eigenvectors incrementally number time-steps. techniques referred incremental algorithms. update schemes proposed krasulina popular incremental techniques. given time-step signiﬁcant eigenvector previous samples general update rule according oja’s method learning rate. process keep updating converging stable state. speed convergence technique matter ongoing research. balsubramani found speed convergence depends learning rate another problem technique consider change weightings previous time-steps. mitiagkas proposed incremental algorithm streaming data computational complexity important point highlight studies directions focus mainly signiﬁcant eigenvectors little attention paid least signiﬁcant ones. fact ﬁnding eigenvectors shown play role detecting outliers non-belonging samples since perpendicular best ﬁtting hyperplane. jollife pointed book principal components corresponding smallest eigenvalues unstructured left -overs extracting higher useful detecting outliers. ﬁrst smallest literature done gnanadesikan wilk based work gnanadesikan stated with p-dimensional data projection onto smallest principal component would relevant studying deviation observation hyperplane closest recently izenman shen used smallest kernel principal components outlier detection generalization linear case alakkari found least signiﬁcant eigenface used basis discriminating face non-face images partial diﬀerential equations many systems solved seeking hyperplane constituted entire solution. known method characteristics. sample mean. sequel paper assume samples centered hence need subtract sample mean explicitly. computing covariance matrix optimal low-dimensional bases cover variability samples extracting signiﬁcant eigenvectors covariance matrix eigenvectors extracted solving following eigenvalue equation eigenvector corresponding eigenvalue. eigenvalues describe variance maintained corresponding eigenvectors. hence interested subset eigenvectors highest eigenvalues encode given sample using p-dimensional projection values follows duality since case rank hence eigenvectors extracted since size solving becomes computationally expensive. eigenvectors dual eigenspace computing matrix solving eigenvalue problem similar oja’s update scheme mentioned background section. problem formula assumes weightings previous samples ﬁxed. eigenvector updated time-step weights previous samples also adjusted according projections updated eigenvector. change weights proportional correlations previous samples time-step. algorithm used following update rule unlike oja’s method online scheme adapts weightings previous samples based squared product time-step. addition timestep weighted based second order products xn+}n+ full pseudo-code algorithm shown algorithm parameters used algorithm space_limit speciﬁes maximal number signiﬁcant eigenvectors compute processing_limit speciﬁes maximal number products compute time-step eigenvector. mentioned earlier algorithm capable ﬁnding eigenvectors data. order compute full dimensional eigenspace deterministically space_limit processing_limit total number dimensions sample current number samples. full-dimensional mode algorithm starts time-steps initial eigenvector ends full-dimensional eigenspace data. line algorithm includes general update rule. line used particularly limited processing mode stress shared information learned vt+. line performs gram-schmidt process ensure following update terms orthogonal updated eigenvector. ﬁnishing loop constitute eigenvector since perpendicular updated components. limited dimensional mode algorithm maximal number eigenvectors update/compute time-step using space_limit parameter. since parameter value constant throughout execution algorithm bound time complexity products time-step. stochastic mode specify maximal number products computed time-step eigenvector. happens processing_limit. case choose processing_limit uniformly distributed random samples compute products time-step. bound time complexity products time-step. considering algorithm require computation covariance matrix full time complexity stochastic mode products applied algorithm time-varying datasets diﬀerent physical phenomena. ﬁrst dataset studies stages supernova period less second star’s core collapses second dataset studies ﬂuid dynamics turbulent vortex area ﬂuid third dataset shows evolution splash caused drop impacts liquid surface fourth dataset generated analyze chaotic path drop silicone bouncing surface vibrating bath material ﬁfth experimental data shows unusual behaviour particles self-organized spirals rotational ﬂuid finally sixth dataset shows behaviour nitrogen bubbles cascading side glass guinness well-investigated number papers table summarizes properties dataset. ﬁrst datasets form scalar ﬁeld remaining four datasets video format adapted original sources converting greyscale video frames cropping frames segment interest adapted datasets obtained emailing authors. compare performance algorithm standard terms explained variance curves. standard results generated using pcacov function matlab since dealing cases typical dual covariance matrix standard pca. adaptive incrementally update eigenvectors reaching last time-step algorithm technique. stress capability approach ﬁnding eigenvectors given datasets. figure shows explained variance curves dataset. clear algorithm provides excellent approximation original full-dimensional eigenspace. datasets curves exceed also interesting note well techniques able learn guinness cascade phenomenon variability covered ﬁrst eigenvectors. next compute limited-dimensional eigenspace dataset mainly compare performance deterministic stochastic modes algorithm. figure shows performance -dimensional adaptive pca. test stochastic mode performance applied runs algorithm processing_limit much lower number computations stochastic runs achieve almost performance deterministic mode. diﬀerence note self organized particles experiment stochastic runs provide mean explained variance deterministic mode covers variability. paper presented deterministic scheme ﬁnds eigenspace sequential data incrementally linear time complexity growth. model generalization oja’s method following main advantages. approach eigenvectors updated online manner unlike oja’s method applied iterative manner eigenvector. secondly model considers previous samples update formula whereas oja’s method considers recent time-step update rule. since algorithm considers second order correlations samples provides intensive learning scheme better resembles quadratic nature standard pca. limited-computations mode algorithm eigenvectors adapted according pattern learned limited population ensembles. experiments shown stochastic mode provides performance deterministic mode much lower number computations. technique serves robust modeling tool complex time-dependent systems decomposes systems temporal behaviour using orthogonal time-dependent functions correspond dual eigenspace. expressed follows figures shows time-dependent functions ﬁrst ﬁfth tenth eigenvectors supernova vortex datasets. note higher signiﬁcance eigenfunctions lower frequency higher amplitude. interpolating functions analyze system behavior continuous time. terms future work would interesting know performance algorithm using diﬀerent distributions previous samples stochastic mode. many systems recent samples higher priority older ones cctv surveillance applications records saved limited period time.", "year": 2017}