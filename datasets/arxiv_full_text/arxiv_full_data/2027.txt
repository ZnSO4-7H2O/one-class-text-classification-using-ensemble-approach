{"title": "Non-linear motor control by local learning in spiking neural networks", "tag": ["q-bio.NC", "cs.LG", "cs.NE", "cs.SY", "stat.ML"], "abstract": "Learning weights in a spiking neural network with hidden neurons, using local, stable and online rules, to control non-linear body dynamics is an open problem. Here, we employ a supervised scheme, Feedback-based Online Local Learning Of Weights (FOLLOW), to train a network of heterogeneous spiking neurons with hidden layers, to control a two-link arm so as to reproduce a desired state trajectory. The network first learns an inverse model of the non-linear dynamics, i.e. from state trajectory as input to the network, it learns to infer the continuous-time command that produced the trajectory. Connection weights are adjusted via a local plasticity rule that involves pre-synaptic firing and post-synaptic feedback of the error in the inferred command. We choose a network architecture, termed differential feedforward, that gives the lowest test error from different feedforward and recurrent architectures. The learned inverse model is then used to generate a continuous-time motor command to control the arm, given a desired trajectory.", "text": "learning weights spiking neural network hidden neurons using local stable online rules control non-linear body dynamics open problem. here employ supervised scheme feedback-based online local learning weights train network heterogeneous spiking neurons hidden layers control two-link reproduce desired state trajectory. network ﬁrst learns inverse model non-linear dynamics i.e. state trajectory input network learns infer continuous-time command produced trajectory. connection weights adjusted local plasticity rule involves pre-synaptic ﬁring post-synaptic feedback error inferred command. choose network architecture termed differential feedforward gives lowest test error different feedforward recurrent architectures. learned inverse model used generate continuous-time motor command control given desired trajectory. motor control requires building internal models muscles-body system brain possibly uses random movements pre-natal post-natal development termed motor babbling learn internal models associate body movements neural motor commands motor control given desired state trajectory dynamical system formed muscles body netschool computer communication sciences brain-mind institute school life sciences ´ecole polytechnique f´ed´erale lausanne lausanne epfl switzerland. correspondence aditya gilra <aditya.gilraepﬂ.ch>. works spiking neurons brain must learn produce time-dependent neural control input activates muscles produce desired movement. abstracting muscles-body dynamics require network generate control input given desired state trajectory makes state evolve train network spiking neurons hidden layers learn inverse model muscle-body dynamics namely infer control input given state trajectory inverse model used control body closed loop mode. networks continous-valued spiking neurons training weights hidden neurons local learning rule considered difﬁcult credit assignment problem supervised learning networks typically accomplished backpropagation non-local reservoir computing trains output weights force learning requires weight changes faster requisite dynamics schemes example here employ recent learning scheme called feedback-based online local learning weights draws upon function dynamics approximation theory adaptive control theory follow synaptically local provably stable convergent alternative training feedforward recurrent weights hidden spiking neurons network employed figure network conﬁguration learning inverse model. learning random motor commands cause arbitrary movements given resulting state network must infer continuous-time motor commands caused observed -dimensional state variables provided input network ﬁxed random weights -dimensional motor command linearly decoded ﬁltered output spike trains network ﬁxed weights dγi. copy random motor command used delay compute error inferred motor command i.e. error ˆuγ. deviation predicted command reference command back network ﬁxed random encoding weights keiγ. error signal used update network weights infer motor commands causing observed state. twin lines connection arrows denote multi-dimensional signals number representative dimensionality. learning inverse model differential feedforward network. learning paradigm conﬁguration panel input sent undelayed layer neurons encoding weights sent delayed another layer neurons encoding weights layers feed next ‘computation’ layer weights plastic error fedback ‘computation’ layer weights keiγ. error projected neuron along ﬁltered pre-synaptic spike train used update feedforward weights red. predict non-linear dynamics authors follow scheme learned forward model dynamics i.e. predicting state given neural control input train network perform full motor control ﬁrst learning inverse model infer control input given current trajectory employing closed loop make replicate desired trajectory. network heterogeneous leaky-integrate-andﬁre neurons different biases learn inverse model depicted figure adapted modelled two-link pendulum moving vertical plane gravity friction joints equilibrium downwards position soft bounds motion beyond shown figure random -dimensional torque input provided generate random state trajectories analogous motor babbling. -dimensional state denoted input network ﬁxed random weights. network must learn infer generated state trajectory chose network figure termed differential feedforward network variety network architectures learn inverse model. neurons feedforward hidden layers indexed followed neurons ‘computation’ hidden layer indexed state vector differential feedforward layers undelayed delayed interval ﬁxed random weights respectively. current neuron undelayed hidden layer given training network inferring command input given state trajectory motor babbling expect remove error feedback loop network still able infer command input given current trajectory. show various stages learning testing figure differential-feedforward network figure figure show different stages follow learning differential feedforward network learning without feedback network output remained zero. feedback even weights learned network output followed desired output i.e. time-delayed reference command. feedback network trained follow learning rule. follow learning feedforward weights learning rate froze weights removed feedback order test network learned inverse model. also tested network structured task trajectory addition random motor babbling. network output inferred command given state trajectory even without feedback indicating network learned inverse model. used causal delay supplying target commands since command past caused current state input needs inferred. differential delay undelayed delayed feedforward layers possibly controls accuracy temporal derivative. swept delays typical network time scales shown figure found gave lowest test error. consistent delay synaptic ﬁltering time constants input state inferred command. command delayed inferred differential delay allows accurate computation derivative perhaps closest time reference command delay seen figure thus remaining simulations including figure used asked whether seemingly low-pass ﬁltering inferred torque testing figure command varied faster network could approximate. used low-pass ﬁltered commands ﬁltering kernel decaying exponential time constant readout weights dγi. readout network compared time-delayed version command input delay causally infer past command. borrowing adaptive control theory error uγ−ˆuγ back neural currents computation layer neurons ﬁxed random feedback weights multiplied gain total current neuron ‘computation’ layer ﬁxed random bias feedforward feedforward error currents trick follow scheme pre-learn readout weights auto-encoder respect error feedback weights eiγ. learning auto-encoder accomplished existing schemes autoencoder pre-learned algorithmically here. auto-encoder error feedback high gain acts negative feedback serves make network output follow true command torque even withˆ hidden weights learned. system dynamics command torque required vary slower synaptic timescale. explained even hidden neurons entrained ideally should enables feedforward hidden weights learned using synaptically available quantities follow learning rule applied plastic weights current state network network output following time-delayed command input torque error feedback loop. follow learning shown reasonable approximations uniformly stable using lyapunov approach squared error learned output tending asymptotically zero details parameters generating random input dynamics ﬁxed random network parameters figure stages follow learning inverse model using differential feedforward network vertical lines divide ﬁgure three time stages learning without feedback left learning feedback middle testing without feedback right. input network namely -dimensional state trajectories shown different colours. component reference torque shown along network readout learning without feedback network output doesn’t infer command torque learning feedback network output follows command torque negative feedback error decreases time used train hidden weights. learning i.e. freezing weights removing feedback network infers command torque even without feedback indicating learned inverse model. figure mean squared test error versus different values differential delay causal torque delay learned inverse model using differential feedforward network neurons undelayed delayed layers neurons computation layer. plot mean squared test error without feedback approximately learning averaged state dimension. star marks mean squared test error figure mean squared test error versus torque ﬁltering increasing values torque ﬁltering time constant learned inverse model using differential feedforward network figure plot mean squared test error without feedback figure range comparison. star marks mean squared error simulation star figure diamond marks mean squared error simulation larger number neurons figure figure recurrent network cannot learn inverse model follow learning. learning conﬁguration figure except spiking network recurrently connected. feedforward weights recurrent weights plastic follow rule. trained recurrent network inverse model neurons recurrent layer increasing values learning rate recurrent weights feedforward weights learned constant rate. plot mean squared test error without feedback approximately learning averaged state dimension. effectively learning reasonable learning rate worked well learning forward model using recurrent network shown figure found test performance dropped sharply learning recurrent weights. near zero learning recurrent weights gave lowest mean squared test error. even included feedforward layer state input recurrent network learnable feedforward recurrent weights learning forward model still test performance degraded learning recurrent weights. note recurrent network follow learning scheme ﬁxed output weights cannot transformed differential feedforward architecture even part recurrent network learn delay hold earlier state memory always remains connected output ﬁxed network increasing ﬁltering time constants using decaying exponential kernel test error remained almost same shown figure larger number neurons test error came without ﬁltering torque approximation error ﬁnite number neurons seemed larger fast timescales torque. thus simulations ﬁltering torque. network approximates inverse dynamics using tuning curves heterogeneous neurons overcomplete basis functions studied adaptive control theory error approximation causes drift weights lead error increasing training time. literature also suggests ameliorative techniques include weight leakage term switched slowly whenever weight crosses value dead zone policy updating weights error falls threshold however need implement techniques learning inverse model networks suggesting approximations ﬁnite number neurons reasonable. however inverse model state differentiated function inverted infer command principle could recurrent network turing complete approximate inverse model provided input recurrent output weights wired correctly. however follow learning scheme output weights ﬁxed match error encoding weights enable local learning. indeed tried learn inverse model recurrent network figure total current neuron recurrent layer ﬁxed random bias feedforward recurrent error currents further even among feedforward network architectures tried follow learning shown figure found best performace ﬁxed number neurons obtained differential feedward network compared feedforward architectures imagine differential feedforward network could learn compute temporal difference undelayed delayed states approximating time derivative. learned inverse model used control reproduce desired state trajectory control input known generated network. shown figure desired state network already trained inverse model output requisite control command command generated network torques joints produce desired motion. simple feed-forward open loop mode small errors generated control command integrate time larger errors trajectory causing large deviations desired trajectory time. ameliorate this closed loop injected difference desired state true state multiplied gain back network input computing error desired state delayed since network also trained output control command lag. feedback loop error state variables network input weights control different feedback loop error control command back error feedback weights learning. latter feedback loop fact used learning present control. tested motor control scheme drawing diamond star wall open loop i.e. feeding back error state variables command torque generated network enabled draw pattern similar desired however parts pattern required large bending elbow joint difﬁcult reproduce possibly poorer learning limits motion divergence state desired state open loop time. without inverse model linear negative feedback loop state variables canmake follow desired trajectory even tuning feedback gains complicated non-linear dynamics two-link arm. control scheme intuition inverse model learned weights network produces open-loop command brings close enough desired state linear negative feedback around momentary operating point bring even closer. used synaptically local online stable learning scheme follow train network heterogeneous spiking neurons hidden layers infer continuous-time command needed drive non-linear dynamical system two-link produce desired trajectory. found differential feedforward network architecture performed best learning inverse model among variety feedforward recurrent architectures follow learning scheme. closed-loop trained inverse model used control arm. expect proportional-integral-derivative feedback control improve further. previous methods training networks hidden units incorporate desirable features follow learning. backpropagation variants backpropagation time real-time recurrent learning non-local time space synaptically local learning rules recurrent spiking neural networks typically demonstrated learning inverse models non-linear motor control reservoir computing methods initially learn weights within hidden units newer force learning methods require weights change faster time scale dynamics require different components multidimensional error different sub-parts network. reward-modulated hebbian rules used continuous-time control inferred torque component given desired state plotted versus time testing without feedback. mean squared test error without feedback unit time state dimension written plot. mean squared error test without feedback typically larger training corrective negative feedback. differential feedforward network default network architecture neurons undelayed delayed layers neurons computation layer direct differential feedforward network doesn’t intermediate undelayed delayed hidden layers. rather undelayed delayed states directly feed computation layer neurons. purely feedforward network neurons hidden layer. figure schematic control using inverse model. desired state trajectory differential feedforward network already trained inverse model. control input read network. read control input torques delivered produces true state already close desired inverse model performs well. improve long-time performance true state compared desired state delayed back gain inverse model learned infer control input delay) error non-linear feedforward control inverse model brings state close desired state linear negative feedback control brings state even closer. twin lines connection arrows denote multi-dimensional signals number representative dimensionality. interesting directions explore could learn control complex dynamical systems like robotic arms real-time implement neuromorphic hardware make follow scheme biologically plausible learn generate desired trajectory given higher-level goal reinforcement learning extend motor control hierarchy levels brain. references abbott depasquale brian memmesheimer raoul-martin. building functional networks spiking model neurons. nature neuroscience march issn ./nn.. alemi alireza machens christian den`eve sophie slotine jean-jacques. learning arbitrary dynamics efﬁcient balanced spiking networks using local plasticity rules. arxiv. bourdoukan ralph den`eve sophie. enforcing balance allows local supervised learning spiking recurrent networks. cortes lawrence sugiyama garnett garnett advances neural information processing systems curran associates inc. burbank kendra mirrored stdp implements autoencoder learning network spiking neurons. plos computational biology december issn ./journal.pcbi.. figure control two-link inverse model. elbow shoulder control torques generated learned differential feedforward network closed loop control architecture reponse desired star-drawing trajectory. star-drawing trajectories elbow angle angular velocity desired produced response torques closed loop control inverse model. network controls draw diamond star left blue desired trajectory coordinate space end-point required trace starting direction green arrow. middle trace produced open-loop control i.e. feedback error state back network. right trace produced closedloop feedback error state variables. since follow-based learning motor control uses spiking neurons local online biologically plausible aspects candidate learning scheme motor control brain. however violates dale’s outgoing synaptic weights single neuron learned network positive negative. however even without obeying dale’s implemented directly neuromorphic hardware incorporated neuro-robotics motor control spike-based coding provides power efﬁciency localility eases hardware implementation speeds compudadarlat maria o’doherty joseph sabes philip learning-based approach artiﬁcial sensory feedback leads optimal integration. nature neuroscience january issn ./nn.. dewolf travis stewart terrence slotine jeanjacques eliasmith chris. spiking neural model adaptive control. proc. soc. november issn ./rspb... gilra aditya gerstner wulfram. predicting non-linear dynamics stable local learning recurrent spiking neural network. elife november issn ./elife.. hennequin guillaume vogels gerstner wulfram. optimal control transient dynamics balanced networks supports generation complex movements. neuron june issn ./j.neuron.... hoerzer gregor legenstein robert maass wolfgang. emergence complex computational structures chaotic neural networks rewardmodulated hebbian learning. cerebral cortex january issn ./cercor/bhs. jaeger herbert haas harald. harnessing nonlinearity predicting chaotic systems saving energy wireless communication. science april ./science.. kappel david legenstein robert habenschuss stefan hsieh michael maass wolfgang. rewardbased stochastic self-conﬁguration neural circuits. arxiv. april khazipov rustem sirota anton leinekugel xavier holmes gregory ben-ari yehezkel buzs´aki gy¨orgy. early motor activity drives spindle bursts developing somatosensory cortex. nature december ./nature. lalazar hagai vaadia eilon. neural basis sensorimotor learning modifying internal models. current opinion neurobiology december issn ./j.conb.. legenstein robert maass wolfgang. edge chaos prediction computational performance neural circuit models. neural networks april issn ./j.neunet... legenstein robert markram henry maass wolfgang. input prediction autonomous movement analysis recurrent circuits spiking neurons. reviews neurosciences issn legenstein robert chase steven schwartz andrew maass wolfgang. reward-modulated hebbian learning rule explain experimentally observed network reorganization brain control task. journal neuroscience june jneurosci.-.. maass wolfgang natschl¨ager thomas markram henry. real-time computing without stable states framework neural computation based perturbations. neural computation november petersson waldenstr¨om alexandra f˚ahraeus christer schouenborg jens. spontaneous muscle twitches sleep guide spinal self-organization. nature july issn ./nature. rumelhart hinton williams learning internal representations error propagation. parallel distributed processing explorations microstructure cognition volume press cambridge williams ronald zipser david. learning algorithm continually running fully recurrent neural networks. neural computation june issn ./neco.... wong jeremy kistemaker dinant chin alvin gribble paul proprioceptive training improve motor learning? journal neurophysiology december issn /jn... zerkaoui salem druaux fabrice leclercq edouard lefebvre dimitri. stable adaptive control recurrent neural networks square mimo non-linear systems. engineering applications artiﬁcial intelligence june issn ./j.engappai.... song francis yang guangyu wang xiaojing. training excitatory-inhibitory recurrent neural networks cognitive tasks simple flexible framework. plos comput biol february issn ./journal. pcbi.. thalmeier dominik uhlmann marvin kappen hilbert memmesheimer raoul-martin. learning universal computations spikes. plos comput biol june issn ./journal.pcbi..", "year": 2017}