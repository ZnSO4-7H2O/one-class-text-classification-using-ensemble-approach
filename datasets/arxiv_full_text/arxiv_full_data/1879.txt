{"title": "Multinomial Adversarial Networks for Multi-Domain Text Classification", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Many text classification tasks are known to be highly domain-dependent. Unfortunately, the availability of training data can vary drastically across domains. Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multinomial adversarial network (MAN) to tackle the text classification problem in this real-world multidomain setting (MDTC). We provide theoretical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multiple probability distributions. MANs are thus a theoretically sound generalization of traditional adversarial networks that discriminate over two distributions. More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resorting to its ability to reduce the divergence among the feature distributions of each domain. We present experimental results showing that MANs significantly outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art performance for domains with no labeled data.", "text": "many text classiﬁcation tasks known highly domain-dependent. unfortunately availability training data vary drastically across domains. worse still domains annotated data all. work propose multinomial adversarial network tackle text classiﬁcation problem real-world multidomain setting provide theoretical justiﬁcations framework proving different instances mans essentially minimizers various f-divergence metrics among multiple probability distributions. mans thus theoretically sound generalization traditional adversarial networks discriminate distributions. speciﬁcally mdtc task learns features invariant across multiple domains resorting ability reduce divergence among feature distributions domain. present experimental results showing mans signiﬁcantly outperform prior mdtc task. also show mans achieve state-of-the-art performance domains labeled data. text classiﬁcation fundamental tasks natural language processing found wide spectrum applications ranging email spam detection social media analytics sentiment analysis data mining. past couple decades supervised statistical learning methods become dominant approach text classiﬁcation iyyer unfortunately many text classiﬁcation tasks highly domain-dependent text classiﬁer trained using labeled data domain likely perform poorly another. task sentiment classiﬁcation example phrase runs fast usually associated positive sentiment sports domain; user reviewing battery electronic device. real applications therefore adequate amount training data domain interest typically required expensive obtain. tackle domain adaptation multi-domain text classiﬁcation domain adaptation assumption domain abundant training data goal utilize knowledge learned source domain help perform classiﬁcations another lower-resourced target domain. focus work mdtc instead simulates arguably realistic scenario labeled data exist multiple domains insufﬁcient amounts train effective classiﬁer domains. worse still domains labeled data all. objective mdtc leverage available resources order improve system performance domains simultaneously. cmsc huang combines classiﬁer shared across domains classiﬁers domain captures domain-speciﬁc text classiﬁcation knowledge. paradigm sometimes known shared-private model cmsc however lacks explicit mechanism ensure shared classiﬁer captures domain-independent knowledge shared classiﬁer well also acquire domain-speciﬁc features useful subset domains. hypothesize better performance obtained constraint explicitly enforced. paper thus propose multinomial adversarial networks task multi-domain text classiﬁcation. contrast standard adversarial networks serve tool minimizing divergence distributions mans represent family theoretically sound adversarial networks that contrast leverage multinomial discriminator directly minimize divergence among multiple probability distributions. binomial adversarial networks applied numerous tasks domain adaptation crosslingual sentiment analysis anticipate mans make versatile machine learning framework applications beyond mdtc task studied work. introduce architecture prove directly minimizes f-divergence among multiple distributions indistinguishable upon successful training. speciﬁcally mdtc used overcome aforementioned limitation prior domain-speciﬁc features sneak shared model. done relying man’s power minimizing divergence among feature distributions domain. highlevel idea make extracted feature distributions domain indistinguishable another thus learning general features invariant across domains. validate effectiveness experiments mdtc data sets. ﬁrst signiﬁcantly outperforms stateof-the-art cmsc method widely used multi-domain amazon review dataset without relying external resources sentiment lexica applied fdu-mtl dataset obtain similar results achieves substantially higher accuracy previous top-performing method asp-mtl asp-mtl ﬁrst empirical attempt multinomial adversarial network proposed multi-task learning setting restricted viewed special case man. addition ﬁrst time provide theoretical guarantees absent asp-mtl. finally many mdtc methods cmsc require labeled data domain mans applied cases labeled data exists subset domains. evaluate semisupervised setting compare method accommodate unlabeled data domain show achieves performance comparable state model paper strive tackle text classiﬁcation problem real-world setting texts come variety domains varying amount labeled data. speciﬁcally assume total domains labeled domains labeled data unlabeled domains annotated training instances available. denote collection domains total number domains faced with. goal work mdtc general improve overall classiﬁcation performance across domains measured paper average classiﬁcation accuracy across domains shown figure multinomial adversarial network adopts shared-private paradigm bousmalis consists four components shared feature extractor domain feature extractor labeled domain text classiﬁer ﬁnally domain discriminator main idea explicitly model domain-invariant features beneﬁcial main classiﬁcation task across domains well domain-speciﬁc features mainly contribute classiﬁcation domain here adversarial domain discriminator multinomial output takes shared feature vector predicts likelihood sample coming domain. seen figure training aims confuse minimizing anticorrelated domain prediction respectively. example mlps softmax layer top. provide alternative architectures mathematical implications. present detailed descriptions training well theoretical grounds training figure arrows illustrate training ﬂows various components. adversarial nature domain discriminator trained separate optimizer rest networks updated main optimizer trained labeled domains takes input concatenation shared domain feature vectors. test time unlabeled domains domain features vector input. contrary takes shared features input labeled unlabeled domains. training described algorithm figure mdtc. ﬁgure demonstrates training mini-batch data domain. training iteration consists mini-batch training domain. parameters fsfdc updated together training ﬂows illustrated green arrows. parameters updated separately shown arrows. solid lines indicate forward passes dotted lines backward passes. domain loss anticorrelated recurrent neural nets multi-layer perceptron depending input data input also dependent feature extractor choice. output feature extractor ﬁxed-length vector considered hidden features given input text. hand outputs label probabilities class adopts loss domain loss simply −jd. loss intuitively translates pushing make random predictions. theoretical justiﬁcations. theories multinomial adversarial binomial adversarial nets known theoretical connections minimization various f-divergences distributions however adversarial training among multiple distributions despite similar idea empirically experimented theoretical justiﬁcations provided best knowledge. section present theoretical analysis showing validity man. particular show man’s objective equivalent minimizing total f-divergence shared feature distributions domains centroid distributions. choice loss function determine speciﬁc fdivergence minimized. furthermore adequate model capacity achieves optimum either loss function shared feature distributions identical hence learning invariant feature space across domains. i-th dimension output vector conceptually corresponds probability predicting domain ﬁrst derive optimal ﬁxed lemma ﬁxed either loss optimum domain discriminator algorithm loss functions text classiﬁer domain discriminator respectively. mentioned tmax layer classiﬁcation. hence adopt canonical negative loglikelihood loss true label tmax predictions. consider variants man. ﬁrst loss suits classiﬁcation task; anoption least-square loss shown able alleviate gradient vanishing problem using loss adversarial setting feature extractors training domain feature extractors straightforward sole objective help perform better within domain. hence domain finally shared feature extractor objectives help achieve higher accuracy make feature distribution invariant across domains. thus leads following bipartite loss table mdtc results amazon dataset. models bold performance rest taken huang numbers parentheses indicate standard errors calculated based runs. bold numbers indicate highest performance domain shows statistical signiﬁcance cmsc one-sample t-test. widely used mdtc datasets. note dataset already preprocessed features losing word order information. prohibits usage cnns rnns feature extractors limiting potential performance system. nonetheless adopt dataset fair comparison employ feature extractor. particular take frequent features represent review feature vector feature values counts features. feature extractor would input size order process reviews. amazon dataset contains samples four domains book electronics kitchen binary labels following huang conduct -way cross validation. three folds treated training serves validation remaining test set. -fold average test accuracy reported. proof involves application lagrangian multiplier solve minimum value details found appendix. following main theorems domain loss theorem optimality adopts loss therefore loss interpreted simultaneously minimizing classiﬁcation loss well divergence among feature distributions domains. thus learn shared feature mapping invariant across domains upon successful training beneﬁcial main classiﬁcation task. models shown domain-speciﬁc models only in-domain models trained; shared model only single model trained data; shared-private models combination previous two. within category various architectures examined least square logistic regression explained before feature extractors models among models ones preﬁx adversarial training man-l man-nll indicate loss loss respectively. table adopting modern deep neural networks methods achieve superior performance within ﬁrst model categories even without adversarial training. corroborated fact sp-mlp model performs comparably cmsc latter relies external resources sentiment lexica. moreover multinomial adversarial nets introduced improvement observed. loss functions outperforms shared-private baseline systems domain achieves statistically signiﬁcantly higher overall performance. mansp models provide mean accuracy well standard errors runs illustrate performance variance conduct signiﬁcance test. seen man’s performance relatively stable consistently outperforms cmsc. cmsc requires labeled data domain experiments naturally designed way. reality however many domains annotated corpora available. therefore also important look performance unlabeled domains mdtc system. fortunately depicted before man’s adversarial training utilizes unlabeled data domain learn domain-invariant features thus used unlabeled domains well. testing shared feature vector domain feature vector order validate man’s effectiveness compare state-of-the-art multi-source domain adaptation methods compared standard domain adaptation methods source target domain ms-da allows adaptation multiple source domains single target domain. analogically mdtc viewed multi-source multi-target domain adaptation superior multiple target domains exist. multiple target domains msda need treat independent task expensive cannot utilize unlabeled data target domains. work compare recent ms-da method mdan experiments target domain suit approach follow setting fair comparison. however worth noting designed mdtc setting deal multiple target domains time potentially improve performance taking advantage unlabeled data multiple target domains adversarial training. adopt setting zhao based multidomain amazon review dataset. four domains dataset treated target domain four separate experiments remaining three used source domains. table target domain shown test accuracy reported various systems. shows outperforms several baseline systems trained source-domains well single-source domain adaptation methods msda dann training data multiple source domains combined viewed single domain. finally compared mdan mdan achieves higher accuracy four target domains average accuracy similar mdan. therefore achieves table results fdu-mtl dataset. bolded models ours rest highest performance domain highlighted. full models standard errors shown parenthese statistical signiﬁcance asp-mtl indicated experiments dataset make fair comparisons previous experiments follow standard settings literature widely adopted amazon review dataset used. however dataset limitations first four domains. addition reviews already tokenized converted features consisting unigrams bigrams. review texts hence available dataset making impossible certain modern neural architectures cnns rnns. provide insights well work feature extractor architectures provide third experiments fdu-mtl dataset dataset created multi-task learning dataset tasks task essentially different domain reviews. amazon domains books electronics kitchen apparel camera health music toys video baby magazine software sports addition movies review domains imdb dataset. domain development samples test samples. amount training unlabeled data vary across domains roughly respectively. compare asp-mtl fdu-mtl dataset. asp-mtl also adopts adversarial training learning shared feature space viewed special case adopting loss furthermore provide theoretically justiﬁcations prove validity loss additional loss. besides theoretical superiority section show also substantially outperforms asp-mtl practice feature extractor choice. particular choose lstm feature extractor found achieve much better accuracy beindeed shown taing times faster. without adversarial training models outperform lstm ones large margin. introduced attain state-of-the-art performance every domain overall accuracy surpassing asp-mtl signiﬁcant margin hypothesize reason lstm performs much inferior attributed lack attention mechanism. asp-mtl last hidden unit taken extracted features. lstm effective representing context token might powerful enough directly encoding entire document therefore various attention mechanisms introduced vanilla lstm select words relevant making predictions. preliminary experiments bi-directional lstm dot-product attention yields better performance vanilla lstm asp-mtl. however still outperform much slower. result conclude that text classiﬁcation tasks effective efﬁcient extracting local higher-level features making single categorization. slightly higher overall performance compared man-l providing evidence claim recent study original loss inherently inferior. moreover variants excel different domains suggesting possibility performance gain using ensemble. multi-domain text classiﬁcation mdtc task ﬁrst examined zong proposed fusion training data multiple domains either feature level classiﬁer level. prior mdtc decomposes text classiﬁer general domain-speciﬁc ones. however general classiﬁer learned parameter sharing domain-speciﬁc knowledge sneak also require external resources help improve accuracy compute domain similarities. domain adaptation domain adaptation attempts transfer knowledge source domain target traditional form single-source single-target adaptation another variant ssmt adaptation tries simultaneously transfer knowledge multiple target domains single source. however cannot fully take advantage training data comes multiple source domains. msst adaptation deal multiple source domains transfers single target domain. therefore multiple target domains exist need treat independent problems expensive cannot utilize additional unlabeled data domains. finally mdtc viewed msmt adaptation arguably general realistic. adversarial networks idea adversarial networks proposed goodfellow image generation applied various tasks well ganin ﬁrst used ssst domain adaptation followed many others. bousmalis utilized adversarial training shared-private model domain adaptation learn domain-invariant features still focused ssst setting. finally idea using adversarial nets discriminate multiple distributions empirically explored recent work multi-task learning setting considered special case framework domain loss. nevertheless propose general framework alternative architectures adversarial component ﬁrst time provide theoretical justiﬁcations multinomial adversarial nets. moreover used lstm without attention feature extractor found perform sub-optimal experiments. instead chose convolutional neural nets feature extractor achieves higher accuracy running order magnitude faster work propose family multinomial adversarial networks generalize traditional binomial adversarial nets sense simultaneously minimize difference among multiple probability distributions instead two. provide theoretical justiﬁcations instances man-nll manl showing minimizers different f-divergence metrics among multiple distributions respectively. indicates used make multiple distributions indistinguishable another. hence applied variety tasks similar versatile binomial adversarial nets used many areas making distributions alike. paper design model mdtc task following shared-private paradigm shared feature extractor learn domain-invariant features domain feature extractors learn domain-speciﬁc ones. used enforce shared feature extractor learn domain-invariant knowledge resorting man’s power making indistinguishable shared feature distributions samples domain. conduct extensive experiments demonstrating model outperforms prior systems mdtc achieves state-ofthe-art performance domains without labeled data compared multi-source domain adaptation methods. references silvey. general class coefﬁcients divergence distribution journal royal statistical society. another. series http //www.jstor.org/stable/. query hardness estimation using jensen-shannon divergence among multiple scoring functions springer berlin heidelberg berlin heidelberg pages https//doi.org/./ ----_. dzmitry bahdanau kyunghyun yoshua neural machine translation bengio. jointly learning align translate. international conference learning representations http//arxiv.org/abs/ john blitzer mark dredze fernando pereira. biographies bollywood boom-boxes sentiblenders proceedings ment classiﬁcation. annual meeting association computational linguistics. association computational linguistics pages http //aclanthology.coli.uni-saarland. de/pdf/p/p/p-.pdf. trigeorgis dunathan silberman dilip krishnan domain separation netmitru erhan. sugiyama works. guyon editors luxburg advances information processing neural systems curran associates inc. pages http//papers.nips.cc/paper/ -domain-separation-networks. pdf. minmin chen zhixiang kilian weinberger sha. marginalized denoising autoencoders domain adaptation. john langford joelle pineau editors proceedings international conference machine learning york icml pages xilun chen athiwaratkun claire cardie kilian weinberger. adversarial deep averaging networks cross-lingual sentiment classiﬁcation. arxiv e-prints https//arxiv. org/abs/.. theodoros evgeniou massimiliano pontil. proceedregularized multi–task learning. ings tenth sigkdd international conference knowledge discovery data mining. york pages https//doi.org/. yaroslav ganin evgeniya ustinova hana ajakan pascal germain hugo larochelle franc¸ois laviolette mario marchand victor lempitsky. domain-adversarial training neural netjournal machine learning research works. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. ghahramani welling cortes lawrence weinberger editors advances neural information processing systems curran associates inc. pages http//papers.nips.cc/paper/ -generative-adversarial-nets. pdf. sergey ioffe christian szegedy. batch normalization accelerating deep network training proceedings reducing internal covariate shift. international conference machine learning. pages http//jmlr.org/ proceedings/papers/v/ioffe.pdf. mohit iyyer varun manjunatha jordan boyd-graber daum´e iii. deep unordered composition rivals syntactic methods text classiﬁproceedings annual meetcation. association computational linguistics international joint conference natural language processing association computational linguistics pages https//doi.org/ ./v/p-. yoon kim. convolutional neural networks sentence classiﬁcation. proceedings conference empirical methods natural language processing association computational linguistics pages https //doi.org/./v/d-. jiwei monroe tianlin s˙ebastien jean alan ritter jurafsky. adversarial learning neural dialogue generation. proceedings conference empirical methods natural language processing. association computational linguistics pages http//www.aclweb.org/ anthology/d-. shoushan chengqing zong. multiproceedings domain sentiment classiﬁcation. acl- short papers. association computational linguistics pages http //aclanthology.coli.uni-saarland. de/pdf/p/p/p-.pdf. pengfei xipeng xuanjing huang. adversarial multi-task learning text classiﬁcation. proceedings annual meeting association computational linguistics association computational linguistics pages https//doi. org/./v/p-. thang luong hieu pham christopher manning. effective approaches attention-based proceedings neural machine translation. conference empirical methods natural language processing. association computational linguistics pages https //doi.org/./v/d-. ryota f-gan training generative neural tomioka. samplers using variational divergence minimization. advances neural information processing systems. pages adam paszke gross soumith chintala gregory chanan edward yang zachary devito zeming alban desmaison luca antiga adam lerer. automatic differentiation pytorch. nips autodiff workshop yang jacob eisenstein. unsupervised multi-domain adaptation feature embeddings. proceedings conference north american chapter association computational linguistics human language technologies. association computational linguistics pages https//doi.org/./v/ zhao shanghang zhang guanhang jo˜ao costeira jos´e moura geoffrey gordon. multiple source domain adaptation adversarial training neural networks. corr abs/.. http//arxiv.org/abs/ afshin domain adaptation rostamizadeh. koller schumultiple sources. bottou editors urmans bengio advances information processing systems curran associates inc. pages http//papers.nips.cc/paper/ -domain-adaptation-with-multiple-sources. pdf. xudong qing haoran raymond y.k. zhen wang stephen paul smolley. least squares generative adversarial networks. ieee international conference computer vision andrew mccallum kamal nigam comparison event models naive bayes text classiﬁaaai- workshop learning text cation. categorization. madison volume pages equivalent deﬁnitions generalized jensen-shannon divergence original definition based shannon entropy reshaped expressed average kullbackleibler divergence centroid adopt latter here three experiments optimizers adam used learning rate size shared feature vector domain feature vector dropout used components. hidden layer size input relu used activation function. batch normalization used batch size ﬁrst experiments amazon review dataset feature extractor used. described paper input size hidden layers used size respectively. feature extractor used fdu-mtl experiment single convolution layer used. kernel sizes number kernels convolution layers take input word embeddings word input sequence. wordvec word embeddings trained bunch unlabeled amazon reviews convolution outputs relu layer pooling layer. pooled output single fully connected layer converted feature vector size either details using text classiﬁcation found original paper implemented using pytorch", "year": 2018}