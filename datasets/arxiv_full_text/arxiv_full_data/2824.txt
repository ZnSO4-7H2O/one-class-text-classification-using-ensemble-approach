{"title": "Path Consistency Learning in Tsallis Entropy Regularized MDPs", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i.e.,~at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation, in which the optimal policy is softmax, and thus, may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called {\\em sparse PCL}, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a {\\em sparse consistency} equation that specifies a relationship between the optimal value function and policy of the sparse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-optimality of a policy which satisfies sparse consistency, and show that as we increase the number of actions, this sub-optimality is better than that of the soft ERL optimal policy. We then use this result to derive the sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with a large number of actions.", "text": "model known optimal policy solution non-linear bellman optimality equations system large model unknown greedily solving bellman equations often results policies optimal. principled dealing issue regularization. among different forms regularization entropy regularization among studied value-based policy-based formulations. particular popular deep algorithms trpo based entropy-regularized policy search. refer interested readers insightful discussion entropy-regularized algorithms connection online learning. entropy-regularized entropy term added bellman equation. formulation four main advantages softens non-linearity bellman equations makes possible solve easily solution softened problem quantiﬁably much worse optimal solution terms accumulated return addition entropy term brings nice properties encouraging exploration maintaining close distance baseline policy unlike original problem deterministic solution solution softened problem stochastic preferable problems exploration dealing unexpected situations important. however common form shannon entropy term added bellman equations optimal policy form softmax. despite advantages softmax policy terms exploration main drawback step assigns non-negligible probability mass non-optimal actions problem aggravated number actions increased. result policies safe execute. address issue proposed special form general notion entropy study sparse entropy-regularized reinforcement learning problem entropy term special form tsallis entropy. optimal policy formulation sparse i.e. state non-zero probability small number actions. addresses main drawback standard shannon entropy-regularized formulation optimal policy softmax thus assign non-negligible probability mass non-optimal actions. problem aggravated number actions increased. paper follow work nachum soft setting propose class novel path consistency learning algorithms called sparse sparse problem work on-policy off-policy data. ﬁrst derive sparse consistency equation speciﬁes relationship between optimal value function policy sparse along system trajectory. crucially weak form converse also true quantify sub-optimality policy satisﬁes sparse consistency show increase number actions suboptimality better soft optimal policy. result derive sparse algorithms. empirically compare sparse soft counterpart show advantage especially problems large number actions. introduction reinforcement learning goal policy maximum long-term performance deﬁned discounted rewards generated following policy case number states actions small solution optimization problem called optimal policy denoted note several optimal policies single optimal value function proven solution space deterministic policies i.e. obtained greedy action w.r.t. optimal action-value function i.e. maxa optimal action-value function unique solution non-linear bellman optimality equations i.e. entropy regularized mdps discussed section ﬁnding optimal policy involves solving non-linear system equations often complicated. moreover optimal policy deterministic always selecting optimal action state even several optimal actions state. undesirable important explore deal unexpected situations. cases might interested multimodal policies still good performance. many researchers proposed regularizer form entropy term objective function solve following entropy-regularized optimization problem entropy-related term regularization parameter. entropy term smoothens objective function resulting problem often easier solve original another reason popularity entropy-regularized mdps. called tsallis entropy bellman equations. formulation property solution sparse distributions i.e. state small number actions non-zero probability. studied properties formulation proposed value-based algorithms solve showed although harder solve soft counterpart potentially solution closer original problem. paper propose novel path consistency learning algorithms tsallis problem called sparse pcl. class actor-critic type algorithms developed nachum soft problem. uses nice property soft namely equivalence consistency optimality learns parameterized policy value functions minimizing loss based consistency equation soft erl. notable feature soft work on-policy off-policy data. ﬁrst derive multi-step consistency equation tsallis problem called sparse consistency. prove setting optimality implies consistency unlike soft case consistency implies sub-optimality. sparse consistency equation derive algorithms on-policy off-policy data solve tsallis problem. empirically compare sparse soft counterpart. expected gain using sparse formulation number actions large algorithmic tasks discretized continuous control problems. markov decision processes consider reinforcement learning problem agent’s interaction system modeled mdp. tuple state action spaces; reward function transition probability distribution reward next state probability taking action state initial state distribution; discounting factor. paper assume action space ﬁnite large. goal stationary markovian policy i.e. mapping state action spaces simplex actions maximizes expected discounted rewards i.e. address issues softmax policy proposed entropy-regularized mdps. tropy called tsallis entropy i.e. similar soft problem optimization problem unique optimal value function satisfy following equations indicates action largest value note equations derived conditions case optimal policy zero probability several actions called sparse problem. regularization parameter controls sparsity resulted policy. policy would sparse smaller values sparse mdps optimal value function unique ﬁxedpoint sparse bellman optimality operator function deﬁned spmax similar spmax operator smoother function inputs thus solving sparse problem easier original cost optimal policy performs worse optimal policy original difference quantiﬁed entropy regularized shannon entropy common entropy-regularized mdps note entropy. problem seen problem reward function original reward function term encourages exploration. unlike optimization problem unique optimal value function satisfy following equations note equations derived conditions case optimal policy soft-max regularization parameter playing role temperature called soft problem. soft mdps optimal value function unique solution soft bellman optimality equations i.e. note sfmax operator smoother function inputs operator associated bellman optimality equation means solving soft problem easier original cost optimal policy performs worse optimal policy original difference quantiﬁed discriminate value function policy soft original mdps. note unbounded sub-optimality main drawback using softmax policies; large action space problems step policy assigns nonnegligible probability mass non-optimal actions aggregate detrimental reward performance. sub-trajectory on-policy off-policy data i.e. generated policy different current including dlength sub-trajectory replay buffer. note since optimal policy value function written based optimal action-value function write objective function based optimize parameters instead separate section begins main contributions work. ﬁrst identify consistency equation sparse mdps deﬁned prove relationship sparse consistency equation optimal policy value function sparse highlight similarities differences soft mdps discussed section extend sparse consistency equation multiple steps prove results allow multi-step sparse consistency equation derive on-policy off-policy algorithms solve sparse mdps fully describe section signiﬁcance sparse consistency equation providing efﬁcient tool computing near-optimal policy sparse mdps involves solving linear equations linear complementary constraints opposed solving ﬁxed-point non-linear sparse bellman operator report proofs theorems section appendix policy value function deﬁne consistency equation sparse mdps state actions present theorem states that similar soft mdps optimality sparse mdps necessary condition consistency i.e. optimality implies consistency. theorem optimal policy value function sparse satisfy consistency equation hand spmax operator complex sfmax thus slightly complicated solve sparse problem soft counterpart. however seen eqs. optimal policy sparse better performance soft counterpart difference becomes apparent number actions grows. large action size term turns constant grows unbounded. shown nachum consistency equation easily extended multi-step i.e. policy function state sequence actions satisfy multi-step consistency equation optimal i.e. property single multiple step consistency equations imply optimality motivation algorithm nachum path consistency learning main idea learn parameterized policy value function minimizing following objective function proof. consider multi-step consistency equation since true initial state sequence actions unrolling another steps starting state using action sequence add− yields result indicates ﬁxed decreases policy satisfying one-step consistency equations approaches true optimal connect performance original goal maximizing expected return present following corollary direct consequence theorem results reported section performance corollary policy satisﬁes consistency -optimal original equation i.e. state followed banach ﬁxed-point theorem show solution pair also solution one-step consistency dition i.e. thus α/-optimality performance guarantee implied theorem similar algorithm soft mdps sparse mdps multi-step consistency equation naturally leads path-wise algorithm training policy value function parameterized well lagrange multipliers parameterized auxiliary parameter characterize objective function algorithm ﬁrst deﬁne soft consistency error d-step sub-trajectory function γdvφ goal algorithm learn expectation initial state action sequence close possibles. sparse algorithm minimizes empirical objective function converges theorem immediately show multi-step sparse consistency necessary condition optimality. corollary optimal policy value function sparse satisfy multi-step consistency equation proof. proof follows directly theorem repeatedly applying expression trajectory taking expectation trajectory using telescopic cancellation value function intermediate states. conversely followed theorem prove following result performance policy satisfying mutli-step consistency equation. novel result showing solving multi-step consistency equation indeed sufﬁcient guarantee near-optimality. corollary policy satisﬁes multi-step consistency equation α/-optimal sparse error multi-step consistency equation. relationship justiﬁes solution policy sparse algorithm near-optimal moreover gradient w.r.t. parameters follows relate sparse algorithm standard actor-critic algorithm ∂j/∂θ ∂j/∂φ correspond actor critic updates respectively. advantage sparse standard need multi-time-scale update required convergence. optimizing minimizes mean square soft consistency error order satisfy multistep consistency still needs impose following constraints lagrange multipliers timization problem standard approach replace constraints adding penalty functions original objective function note penalty function associated penalty parameter |x|·|a| constraints. large tuning parameters becomes computationally expensive. another approach update penalty parameters using gradient ascent methods equivalent ﬁnding saddle point lagrangian function constrained optimization problem. however challenge balance primal dual updates practice. hereby describe alternative much simpler methodology parameterize lagrange multipliers aforementioned constraints immediately satisﬁed. although method impose extra restrictions representations function approximations avoids difﬁculties directly solving constrained optimization problem. specifically satisfy constraint parameterize multilayer perceptron network either activation function last layer. satisfy constraint consider case written form function approximator parameterization justiﬁed closed-form solution policy tsallis uniﬁed sparse note closed-form optimal policy functions optimal state-action value function soft based observation also parameterize policy value function sparse single function approximator although consistency imply optimality sparse mdps justiﬁcation parameterization comes corollary unique optimal value function optimal policy satisfy consistency equation actor-critic perspective signiﬁcance policy value function updated simultaneously without affecting convergence. accordingly update rule model parameter takes form compare performance algorithms following standard algorithmic tasks copy duplicatedinput repeatcopy reverse reversedaddition task viewed grid environment cell stores single character ﬁnite vocabulary agent moves grid environment writes output. time step agent observes character single cell located. observing character agent must take action form determines agent’s move adjacent cell determines whether agent writes output determines character agent writes based problem setting action figure results average reward sparse standard soft training. corresponds speciﬁc algorithmic task. particular task action space increased left right across rows corresponding increase difﬁculty. observe soft returns better solution action space small performance degrades quickly size action space grows. hand sparse able learn good policies tasks small action spaces unlike soft also successfully learns high-reward policies higher-dimension variants. appendix additional results. space size accordingly difﬁculty tasks grows size vocabulary. illustrate effectiveness tsallis entropy-regularized mdps problems large action space evaluate algorithms different choices |v|. task agent different goal. copy environment sequence characters agent aims copy sequence output. duplicatedinput environment sequence duplicated characters agent needs write de-duplicated sequence output. repeatcopy environment sequence characters agent must copy forward order reverse order ﬁnally forward order again. reverse environment sequence characters agent must copy output reverse order. finally reversedaddition environment grid digits representing numbers base-|v| agent needs sum. task agent receives reward correctly output character. episode terminated either task completed agent outputs incorrect character. follow similar experimental procedure nachum functions consistency equations parameterized recurrent neural network multiple heads. task algorithm perform hyper-parameter search optimal regularization weight corresponding training curves average reward shown figure increase statistical signiﬁcance experiments also train policies different monte carlo trials details experimental setup extra numerical results included appendix. task evaluated sparse compared original soft suite variants successively increase vocabulary size. vocabulary sizes soft achieves better results. suggests shannon entropy encourages better exploration small action spaces. indeed regimes greater proportion total actions useful explore exploration costly. therefore decreased exploration tsallis entropy figure results sparse soft halfcheetah discretized actions. ﬁgure shows average reward random runs training best hyper-parameters. bottom plot average probability most-likely actions training. bottom ﬁgure illustrates fast convergence sparse near-deterministic policy. sparse sub-optimal. mainly coarse discretization action space. main purpose demonstrate fast improved convergence deterministic policies sparse compared soft pcl. evaluation sparse left future work. conclusions work studied sparse entropy-regularized problem whose optimal policy non-zero probability small number actions. similar work nachum derived relationship optimality consistency problem. furthermore leveraging properties consistency equation proposed class sparse path consistency learning algorithms applicable on-policy off-policy data learn multi-step trajectories. found theoretical advantages sparse correspond empirical advantages well. tasks large number actions signiﬁcant improvement ﬁnal performance amount time needed reach performance using sparse compared original soft pcl. future work includes extending sparse algorithm general class tsallis entropy investigating possibility combining sparse path following algorithms trpo comparing performance sparse deterministic policy gradient algorithms continuous domain. outweigh asymptotic beneﬁts. sub-optimality bounds presented paper support behavior small αsoft αsparse pcl/. increase vocabulary size picture changes. advantage soft sparse decreases eventually order reversed sparse begins show significant improvement standard soft pcl. supports original hypothesis. large action spaces tendency soft assign non-zero probability many sub-optimal actions over-emphasizes exploration detrimental ﬁnal reward performance. hand sparse able handle exploration large action spaces properly. empirical results provide evidence unique advantage sparse pcl. evaluate algorithms halfcheetah continuous control problem openai gym. environment consists −dimensional action space dimension corresponds torque discretize continuous action either following grids {−−. even though resolution discretization grids coarse corresponding action spaces quite large sizes respectively. present results sparse soft discretized problems figure similar observations algorithmic tasks policy learned sparse performs much better soft pcl. speciﬁcally sparse achieves higher average reward able learn much faster. better visualize learning progress algorithms problems training step also compare average probability most-likely actions across time-steps on-policy trajectory. clearly sparse quickly converges near-deterministic policy policy generated soft still allocates signiﬁcant probability masses non-optimal actions environments like halfcheetah trajectory long horizon soft-max policy general suffer chooses large number sub-optimal actions episode exploration. farahmand ghavamzadeh szepesv´ari mannor regularized ﬁtted q-iteration planning continuous-space markovian decision problems. proceedings american control conference pakman tishby g-learning taming noise reinforcement learning soft update. proceedings international conference uncertainty artiﬁcial intelligence johns painter-wakeﬁeld parr linear complementarity regularized policy evaluation improceedings advances neural inprovement. formation processing systems press kolter regularization feature selection proleast-squares temporal difference learning. ceedings twenty-sixth international conference machine learning mnih badia mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning. proceedings international conference machine learning nachum norouzi mohammad kelvin schuurmans dale. trust-pcl off-policy trust region method continuous control. proceedings international conference learning representations sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation. proceedings advances neural information processing systems ﬁrst following technical result properties. proposition sparse-max bellman operator following properties translation γ-contraction monotonicity value functions detailed proof proposition found using results banach ﬁxed point theorem shows exists unique solution following ﬁxed point equation solution equal optimal value function analogous arguments standard mdps case optimal value function also computed using dynamic programming methods value iteration. proving main results notice using analogous arguments complementary-slackness property conditions second third consistency equation equivalent following condition implies maximizer inner optimization problem. third equality follows arithmetic manipulations ﬁrst inequality follows consistency equation i.e. exists bellman operator banach ﬁxedrecall γ−contraction property point theorem property implies exists unique ﬁxed point solution limit point converging iterative sequence limn→∞ initial value function also recall translation property bellman operator i.e. constant therefore repeatedly applying bellman operator sides inequality using properties bellman operator show input environment learning rate discount factor regularization rollout number steps replay buffer capacity prioritized replay hyper-parameter parameterizations follow descriptions section function gradients algorithmic tasks follow similar experimental setup described nachum parameterize values single lstm recurrent neural network internal dimension multiple heads training step sample batch episodes using current policy acting environment. perform gradient step based batch. experience replay buffer perform gradient step based off-policy batch sampled replay buffer. rollout nachum replay buffer prioritized episode rewards probability sampling episode replay buffer exp{αr}/z total reward episode normalizing factor replay buffer capacity episodes. experiments learning rate discount halfcheetah parameterized policy value networks feed forward networks hidden layers dimension tanh non-linearities. training step sampled steps environment input replay buffer. sample batch sub-episodes steps replay buffer prioritized exponentiated recency perform single training step. rollout discount performed hyperparameter search learning rate standard soft policy determined logits output neural network. figure average reward training sparse compared standard soft reversedaddition. task environment grid digits representing numbers base-|v| agent needs sum. tasks main paper sparse becomes advantageous compared soft action space increases size.", "year": 2018}