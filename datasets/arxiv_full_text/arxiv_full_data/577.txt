{"title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text  Networks", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.2.6"], "abstract": "Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \\textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.", "text": "unsupervised text embedding methods skip-gram paragraph vector attracting increasing attention simplicity scalability eﬀectiveness. however comparing sophisticated deep learning architectures convolutional neural networks methods usually yield inferior results applied particular machine learning tasks. possible reason text embedding methods learn representation text fully unsupervised without leveraging labeled information available task. although dimensional representations learned applicable many diﬀerent tasks particularly tuned task. paper proposing semi-supervised representation learning method text data call predictive text embedding predictive text embedding utilizes labeled unlabeled data learn embedding text. labeled information diﬀerent levels word co-occurrence information ﬁrst represented large-scale heterogeneous text network embedded dimensional space principled eﬃcient algorithm. dimensional embedding preserves semantic closeness words documents also strong predictive power particular task. compared recent supervised approaches based convolutional neural networks predictive text embedding comparable eﬀective much eﬃcient fewer parameters tune. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. kdd’ august sydney australia. http//dx.doi.org/./.. learning meaningful eﬀective representation text e.g. words documents critical prerequisite many machine learning tasks text classiﬁcation clustering retrieval. traditionally every word represented independently other document represented bag-of-words. however representations suﬀer problems data sparsity polysemy synonymy semantic relatedness diﬀerent words commonly ignored. distributed representations words documents eﬀectively address problem representing words documents low-dimensional spaces similar words documents embedded closely other. essential idea approaches comes distributional hypothesis shall know word company keeps mikilov proposed simple elegant word embedding model called skip-gram uses embedding target word predict embedding individual context word local window. mikolov extended idea proposed paragraph vectors order embed arbitrary pieces text e.g. sentences documents. basic idea embeddings sentences/documents predict embeddings words sentences/documents. comparing classical approaches also utilize distributional similarity word context brown clustering nearest neighbors text embedding approaches proved quite eﬃcient scaling millions documents single machine unsupervised learning process representations learned text embedding models general enough applied variety tasks classiﬁcation clustering ranking. however compared end-to-end sophisticated deep learning approaches convolutional neural networks performance text embeddings usually falls short speciﬁc tasks perhaps surprising deep neural networks fully leverage labeled information available task learn representations data. text embedding methods able consider labeled information learning representations; labels kick later classiﬁer trained using representations features. words unsuperfigure illustration converting partially labeled text corpora heterogeneous text network. word-word cooccurrence network word-document network encode unsupervised information capturing local context-level document-level word co-occurrences respectively; word-label network encodes supervised information capturing class-level word co-occurrences. despite deﬁciency still considerable advantages text embedding approaches comparing deep neural networks. first training deep neural networks especially convolutional neural networks computational intensive usually requires multiple gpus clusters cpus processing large amount data; second convolutional neural networks usually assume availability large amount labeled examples unrealistic many tasks. easily obtainable unlabeled data usually used indirect pre-training; third training cnns requires exhaustive tuning many parameters time consuming even experts infeasible non-experts. hand text embedding methods like skip-gram much eﬃcient much easier tune naturally accommodate unlabeled data. paper proposing predictive text embedding adapts advantages unsupervised text embeddings naturally utilizes labeled information representation learning. predictive text embedding eﬀective dimensional representation learned jointly limited labeled examples large amount unlabeled examples. comparing unsupervised embeddings representation optimized particular tasks like convolutional neural networks proposed method naturally extends previous work unsupervised information network embedding ﬁrst learns dimensional embedding words heterogeneous text network. network encodes diﬀerent levels co-occurrence information words words words documents words labels. network embedded dimensional vector space preserves second-order proximity vertices network. representation arbitrary piece text simply inferred average word representations turns quite eﬀective. whole optimization process remains eﬃcient scales millions documents billions tokens single machine. conduct extensive experiments real-world text corpora including long short documents. experimental results show predictive text embeddings signiﬁcantly outperform state-of-the-art unsupervised embeddings various text classiﬁcation tasks. compared endto-end convolutional neural networks text classiﬁcation predictive text embedding outperforms long documents generates comparable results short documents. enjoys various advantages convolutional neural networks much eﬃcient accommodates large-scale unlabeled data eﬀectively less sensitive model parameters. believe exploration points direction learning text embeddings could compete head-to-head deep neural networks particular tasks. semi-supervised manner. unlabeled data labeled information integrated heterogeneous text network incorporates diﬀerent levels cooccurrence information text. rest paper organized follows. ﬁrst introduce related work section section formally deﬁnes problem predictive text embedding heterogeneous text networks. section introduces proposed algorithm details. section presents results empirical experiments. conclude section distributed representation text proved quite eﬀective many natural language processing tasks word analogy tagging parsing language modeling sentiment analysis existing approaches generally classiﬁed categories unsupervised supervised. recent developed unsupervised approaches normally learn embeddings words and/or documents utilizing word co-occurrences local context document level approaches quite eﬃcient scaling millions documents. supervised approaches usually based deep neural network architectures recursive neural tensor networks convolutional neural networks rntns word embedded dimensional vector embeddings phrases recursively learned applying tensor-based composition function sub-phrases words parse tree. cnns word also represented vector convolutional kernel applied context windows diﬀerent positions sentences followed max-pooling fully connected layer. major diﬀerence categories approaches utilize labeled unlabeled information representation learning phase. unsupervised methods include labeled information learning representations labels train classiﬁer data transformed learned representation. rntns cnns incorporate labels directly representation learning learned representations particularly tuned classiﬁcation task. incorporate unlabeled examples however neural nets usually indirect approach pretrain word embeddings unsupervised approaches. comparing lines work learns text vectors semi-supervised representation learning algorithm directly utilizes labeled information large-scale unlabeled data. another piece work similar predictive word embedding learns word vectors particularly tuned sentiment analysis. however approach scale millions documents generalize classiﬁcation tasks. work also related problem network/graph embedding word representations learned heterogeneous text network. embedding networks/graphs dimensional spaces useful variety applications e.g. node classiﬁcation link prediction classical graph embedding algorithms isomap laplacian eigenmap applicable embedding large-scale networks contain millions vertices billions edges. recent work attempting embed large realworld networks. perozzi proposed network embedding model called deepwalk uses truncated random walks networks applicable networks binary edges. previous work proposed novel large-scale network embedding model called line suitable arbitrary types information networks undirected directed binary weighted line model optimizes objective function aims preserve local global network structures. deepwalk line unsupervised handle homogeneous networks. network embedding algorithm used extends line deal heterogeneous networks multiple types vertices edges exist. begin formally deﬁning problem predictive text embedding heterogeneous text networks. comparing unsupervised text embedding approaches including skip-gram paragraph vectors learn general semantic representations text goal learn representation text optimized given text classiﬁcation task. words anticipate text embedding strong predictive power performance given task. basic idea incorporate labeled unlabeled information learning text embeddings. achieve this desirable ﬁrst uniﬁed representation encode types information. paper propose diﬀerent types networks achieve this including word-word co-occurrence networks word-document networks word-label networks. word-word network captures word co-occurrences local contexts essential information used existing word embedding approaches skip-gram. beyond local contexts word co-occurrence document level also widely explored classical text representations statistical topic models e.g. latent dirichlet allocation capture document-level word cooccurrences introduce another network word-document network deﬁned below document network denoted bipartite network documents words. edges words documents. weight word document simply deﬁned number times appears document word-word word-document networks encode unlabeled information large-scale corpora capturing word co-occurrences local context level document level. encode labeled information introduce word-label network captures word co-occurrences category-level objective optimized stochastic gradient descent using techniques edge sampling negative sampling step binary edge sampled probability proportional weight meanwhile multiple negative edges sampled noise distribution sampling procedures address signiﬁcant deﬁciency stochastic gradient descent learning network embeddings. detailed optimization process readers refer embeddings word-word word-document word-label network learned model. note word-word network essentially bipartitenetwork treating undirected edge directed heterogeneous text network composed three bipartite networks word-word word-document word-label networks word vertices shared across three networks. learn embeddings heterogeneous text network intuitive approach collectively embed three bipartite networks achieved minimizing following objective function objective function optimized diﬀerent ways depending labeled information i.e. word-label network used. solution train model unlabeled data labeled data simultaneously. word-document word-label networks constructed unlabeled labeled text data. captures diﬀerent levels word co-occurrences contains labeled unlabeled information. note deﬁnition heterogeneous text network generalized integrate types networks word-sentence word-paragraph document-label networks. work using three types networks illustrative example. particularly focus word networks order ﬁrst represent words dimensional spaces. representation text units computed aggregating word representations. definition given large collection text data unlabeled labeled information problem predictive text embedding aims learn dimensional representations words embedding heterogeneous text network constructed collection dimensional vector space. section introduce proposed method learns predictive text embedding heterogeneous text networks. method ﬁrst learns vector representations words embedding heterogeneous text networks constructed free text dimensional space infer text embeddings based learned word vectors. heterogeneous text network composed three bipartite networks ﬁrst introduce approach embedding individual bipartite networks. previous work introduced line model learn embedding large-scale information networks line mainly designed homogeneous networks i.e. networks types nodes. line cannot directly applied heterogeneous networks weights diﬀerent types edges comparable. here ﬁrst adapt line model embedding bipartite networks. essential idea make second-order proximity vertices assumes vertices similar neighbors similar thus represented closely dimensional space. embedding vector vertex embedding vector vertex vertex deﬁnes conditional distribution vertices pair vertices conditional distributions preserve loss function word embedding text embedding speciﬁed euclidean distance. related inference process paragraph vectors minimizes objective diﬀerent loss however lead section move forward evaluate eﬀectiveness proposed algorithm predictive text embedding. variety text classiﬁcation tasks data sets selected purpose. experiments following. data sets select types text corpora consist either long short documents. long document corpora widely used text classiﬁcation data newsgroup containing categories; wiki snapshot wikipedia corpus april containing around million english articles. common words appeared vocabulary wiki kept. choose seven diverse categories classiﬁcation task including arts history human mathematicsnaturetechnology sports dbpedia ontology. category randomly select articles labeled documents training; imdb data sentiment classiﬁcation avoid distribution bias training test data sets randomly shuﬄe training test data sets; large benchmark corpus text classiﬁcation four subsets including corporate economics government market extracted corpus. data sets documents already represented bag-of-words orders words lost. short document corpora dblp contains titles papers computer science bibliography. choose diverse research ﬁelds classiﬁcation including databaseartiﬁcial intelligencehardwaresystem programming languages theory. ﬁeld select representative conferences collect papers published selected conferences labeled documents; movie review data review contains sentence twitter corpus call approach joint training. alternative solution learn embeddings unlabeled data ﬁrst ﬁne-tune embeddings word-label network. inspired idea pre-training ﬁne-tuning literature deep learning joint training three types networks used together. straightforward solution optimize objective merge edges three sets deploy edge sampling samples edge model updating step sampling probability proportional weight. however network heterogeneous weights edges diﬀerent types vertices comparable other. reasonable solution alternatively sample three sets edges. summarize detailed training algorithm alg. diﬀerent levels extracted unlabeled data labeled information speciﬁc classiﬁcation task. therefore word representations learned embedding heterogeneous text network robust also optimized task. word vectors learned representation arbitrary piece text obtained simply averaging vectors words piece text. vector representation piece compared algorithms compare algorithm representation learning algorithms text data including classical bag-of-words representation state-of-the-art approaches unsupervised supervised text embedding. convolutional neural network though proposed modeling sentences adapt general word sequences including long documents. although typically works fully labeled documents also utilize unlabeled data pre-training model unsupervised word embeddings marked cnn. text embedding. diﬀerent variants diﬀerent combinations word-word worddocument word-label networks. denote version uses word-label network only; learns unsupervised embedding word-word word-document networks ﬁne-tune word embeddings word-label network; jointly trains heterogeneous text network composed three networks. classiﬁcation parameter settings vector representations documents constructed learned apply classiﬁcation process using training data set. particular documents training used representation learning phase classiﬁer learning phase. class labels documents used representation learning phase unsupervised embedding method used; kick classiﬁer learning phase. class label used representation learning phase classiﬁer learning phase predictive embedding method used. test data held-out phases. classiﬁcation phase one-vs-rest logistic regression model liblinear package. classiﬁcation performance measured micro-f macro-f metrics. skip-gram pvdbow pvdm mini-batch size stochastic gradient descent learning mini-batches edge samples number negative samples window size skip-gram pvdbow pvdm constructing word-word co-occurrence network. structure uses convolution layer followed max-pooling layer fully-connected layer. following window size convolution layer number feature maps training data randomly selected validation data early stopping. dimensionality word vectors default embedding models. table compare performance text classiﬁcation long documents. start table wiki imdb data sets. ﬁrst compare performance unsupervised embedding methods either local word co-occurrences document level word co-occurrences combination line document-level word co-occurrences performs best among unsupervised embeddings. performance pvdm inferior pvdbow diﬀerent reported unfortunately able replicate results. similar results also reported results pvdbow imdb data results diﬀerent reported embeddings trained mixture training test data sets embeddings trained training data believe reasonable experiment setup. next compare performance predictive embeddings including diﬀerent variants pte. jointly trained using heterogeneous text network combination word-document word-label networks performs best among approaches. approaches jointly trained word-label network outperform corresponding unsupervised embedding approaches shows power learning predictive text embeddings supervision. consistently outperforms demonstrating incorporating unlabeled information i.e. word-word word-document networks also improves quality embeddings. also signiﬁcantly outperforms pte. shows jointly training interesting observe consistently outperforms cnn. promising sophisticated neural network architecture. also attempt pre-train word embeddings learned line tune labels. surprisingly performance pre-training signiﬁcantly improves imdb data sets remains almost wiki data set. implies pre-training well learned unsupervised embeddings useful. however even pre-training performance still inferior pte. probably model jointly train unlabeled labeled data utilize separately pre-training ﬁne-tuning. also outperforms classical bag-of-words representation even though dimensionality embeddings learned smaller bag-of-words. table reports results data sets. order words lost embedding methods require word order information applicable. similar results observed. predictive text embeddings outperform unsupervised embeddings. also much eﬀective pte. embedding approaches trained asynchronous stochastic gradient descent algorithm using threads single machine memory cores .ghz. compare running time imdb data set. method typically times faster models. pre-trained preexisting word embeddings converges much faster still times slower pte. table compares performance short documents. among unsupervised embeddings line combines document-level local context-level word co-occurrences performs best. line utilizing local context-level word co-occurrences outperforms line using document-level word co-occurrences opposite observations long documents. document-level word co-occurrences suﬀer sparsity short documents similar results observed statistical topic models performance pvdm still inferior pvdbow consistent results long documents. predictive embeddings best performance obtained among approaches predictive embeddings learned incorporating word-label network outperform corresponding unsupervised embeddings consistent results long documents. outperforms line showing usefulness incorporating unlabeled information. also signiﬁcantly outperforms showing advantage jointly training labeled unlabeled data. short documents observe consistently outperform cnn. reason probably problem word sense ambiguity becomes serious short documents. reduces problem word ambiguity using word orders local context convolutional kernels leverage orders. believe considerable room improve predictive text embedding utilizing word orders leave future work. compare head-to-head varying percentages labeled data. consider cases without unlabeled data mimicking scenarios supervised semi-supervised learning. setting semisupervised learning also compare classical semisupervised approaches naive bayes label propagation fig. reports performance long short documents. overall cnns ptes improve size labeled data increases. supervised settings i.e. outperforms comparable long short documents. semi-supervised settings i.e. consistently outperforms pre-trained best performing unsupervised word embeddings. also outperforms state-of-the-art semi-supervised approaches naive bayes label propagation. also notice size labeled data scarce pre-training unsupervised embeddings quite helpful especially short documents. even outperforms ptes training examples few. however size labeled data increases pre-training always improve performance note skip-gram increasing number labeled data training increase performance. also notice labeled documents performance inferior skip-gram dblp data set. reason number labeled examples scarce word-label network noisy treats word-label network equally robust word-word/word-document networks. adjust sampling probability word-label also analyze performance cnns ptes w.r.t. size unlabeled data. unlabeled data used pre-training unlabeled data used either pre-training jointly training. fig. reports results dblp data sets. space limitation omit results data sets similar. documents labeled rest used unlabeled; dblp randomly sample titles papers published conferences unlabeled data. unsupervised embeddings pre-training cnn. performance improves size unlabeled data increases. jointly training unlabeled labeled data much eﬀective separating pre-training ﬁne-tuning. proposed models mentioned previously parameters except number edge samples sensitive diﬀerent data sets default. analyze performance sensitivity w.r.t number samples fig. reports results dblp data sets. data sets number samples becomes large enough performance converges. therefore practice number samples suﬃciently large. reasonable estimation practice several times number edges heterogeneous text networks. embeddings. used line unsupervised embedding predictive embedding. training test documents visualized. fig. shows visualization results. training test data sets predictive embedding much better distinguishes diﬀerent classes learned unsupervised embedding intuitively shows power predictive embeddings task text classiﬁcation. unsupervised embeddings. essential information used unsupervised approaches local contextlevel document-level word co-occurrences. long documents document-level word co-occurrences useful local context-level word co-occurrences combination improve result; short documents local context-level word cooccurrences useful document-level word co-occurrences combination improve embedding. document-level word co-occurrences suﬀers short lengths documents. predictive embeddings. comparing model seems handle labeled information eﬀectively especially short documents. because uses much complicated structure particular utilizes word orders local context addresses word sense ambiguity. therefore case labeled data sparse outperform especially short documents. however advantage expense intensive computation exhaustive parameter tuning. hand much faster much easier conﬁgure labeled data becomes abundant performance least comparable usually superior cnn. compared model obvious advantage model capability able jointly train unlabeled labeled data. make unlabeled data indirect i.e. pre-training unsupervised word embeddings learned methods. pre-training always help size labeled data becomes abundant. labeled data available short documents suggest learn unsupervised embedding ﬁrst pre-train unsupervised word embedding; long documents suggest using pte. labeled data abundant long documents strongly suggest using model jointly train embedding unlabeled labeled data; short documents selection basically trades performance eﬃciency. believe study provides interesting direction eﬃciently learn predictive distributed text embeddings. feasible desirable develop similar methods compete deep neural networks end-to-end speciﬁc tasks avoid complex model architectures intensive computation. considerable room improve example considering orders words.", "year": 2015}