{"title": "Inference in Probabilistic Graphical Models by Graph Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A useful computation when acting in a complex environment is to infer the marginal probabilities or most probable states of task-relevant variables. Probabilistic graphical models can efficiently represent the structure of such complex data, but performing these inferences is generally difficult. Message-passing algorithms, such as belief propagation, are a natural way to disseminate evidence amongst correlated variables while exploiting the graph structure, but these algorithms can struggle when the conditional dependency graphs contain loops. Here we use Graph Neural Networks (GNNs) to learn a message-passing algorithm that solves these inference tasks. We first show that the architecture of GNNs is well-matched to inference tasks. We then demonstrate the efficacy of this inference approach by training GNNs on an ensemble of graphical models and showing that they substantially outperform belief propagation on loopy graphs. Our message-passing algorithms generalize out of the training set to larger graphs and graphs with different structure.", "text": "useful computation acting complex environment infer marginal probabilities probable states task-relevant variables. probabilistic graphical models efﬁciently represent structure complex data performing inferences generally difﬁcult. message-passing algorithms belief propagation natural disseminate evidence amongst correlated variables exploiting graph structure algorithms struggle conditional dependency graphs contain loops. graph neural networks learn message-passing algorithm solves inference tasks. ﬁrst show architecture gnns well-matched inference tasks. demonstrate efﬁcacy inference approach training gnns ensemble graphical models showing substantially outperform belief propagation loopy graphs. message-passing algorithms generalize training larger graphs graphs different structure. probabilistic graphical models provide statistical framework modelling conditional dependencies random variables widely used represent complex real-world phenomena. given graphical model distribution major goal compute marginal probability distributions task-relevant variables node graph given loss function distributions determine optimal estimator. another major goal compute probable state maxx inference. department neuroscience baylor college medicine department electrical computer engineering rice university department computer science university toronto uber toronto vector institute canadian institute advanced research. correspondence kijung yoon <kijung.yoongmail.com> pitkow <xaqrice.edu>. complex models loopy graphs exact inferences sorts often computationally intractable therefore generally relies approximate methods. important method computing approximate marginals belief propagation algorithm exchanges statistical information among neighboring nodes algorithm performs exact inference tree graphs graphs cycles. furthermore basic update steps belief propagation efﬁcient even closed-form solutions leading researchers construct variants generalizations work introduce end-to-end trainable inference systems based graph neural networks recurrent networks allow complex transformations nodes. show network architecture well-suited message-passing inference algorithms ﬂexibility gives wide applicability even cases closed-form algorithms unavailable. gnns vector-valued nodes encode probabilistic information variables graphical model. nodes send receive messages probabilities messages determined canonical learned nonlinear transformations information sources statistical interactions them. dynamics reﬂects probabilistic information throughout graphical model model reaches equilibrium nonlinear decoder extract approximate marginal probabilities states node. demonstrate value gnns inference probabilistic graphical models create ensemble graphical models train networks perform marginal inference test well inferences generalize beyond training graphs. results compare quite favorably belief propagation loopy graphs. goal compute marginal probabilities states graphical models. general graphs computations require exponentially large resources summing maximizing posp neighbors i.e. factors involve neighbors i.e. variables directly coupled recursive graph-based structure message equations leads naturally idea could describe messages nonlinear updates using graph neural network nodes correspond messages described next section. interestingly belief propagation also reformulated entirely without messages operations equivalent successively reparameterizing factors subgraphs original graphical model suggests could construct different mapping gnns graphical models nodes correspond factor nodes rather messages. interestingly reparameterization accomplished adjusts univariate potentials since updates lead multivariate coupling potentials unchanged inference algorithm converges estimated marginal joint probability factor namely given observe messages depend variable time term depends variable time factor itself therefore invariant time. since change interactions imitate action gnns need represent single variable nodes explicitly nonlinear functions nodes account interactions. experiments evaluate architectures gnns constructed latent states represent either message nodes single variable nodes. message inputs message outputs message operation needed expectation propagation inference suggests learning cnns estimating factorto-variable messages message-passing procedure. another related line work inference machines trains series logistic regressors handcrafted features estimate messages. applied idea pose estimation using convolutional layers introduces sequential inference recurrent neural networks application domain. similar line work approach present gnn-based models. gnns essentially extension recurrent neural networks operate graphstructured inputs central idea iteratively update hidden states node aggregating incoming messages propagated graph. here expressive neural networks model messagenode-update functions. recently provide good review several variants unify model called message-passing neural networks. gnns indeed similar structure message passing algorithms used probabilistic inference. reason gnns powerful architectures capturing statistical dependencies variables interest section brieﬂy review probabilistic graphical models describe architecture present network applied problem estimating marginal probabilities probable states variable discrete undirected graphical models. probabilistic graphical models simplify joint probability distribution many variables factorizing distribution according conditional independence relationships. factor graphs convenient general representation structured probability distributions. undirected bipartite graphs whose edges connect variable nodes encode individual variables factor nodes encode direct statistical interactions groups variables probability distribution normalized product factors mathematically node graph associated d-dimensional hidden state vector time step initialize hidden state zeros results depend initial values. every successive time step node sends message neighboring nodes. deﬁne -dimensional vectori→j node time step valued message message function speciﬁed multilayer perceptron rectiﬁed linear units note message function depends properties edge aggregate incoming messages single message destination node node update function case speciﬁed another neural network gated recurrent unit whose parameters shared across nodes. described equations sending messages updating node states deﬁne single time step. evaluate graph neural network iterating equations ﬁxed number time steps obtain ﬁnal state vectors feeding ﬁnal node states readout function given another ﬁnal sigmoidal nonlinearity next apply general architecture task probabilistic inference probabilistic graphical models. investigate mappings graphical models experiments show perform similarly much better belief propagation. experiments focus binary graphical models variables {+−}|v|. probability determined singleton factors ebixi biasing individual variables according vector pairwise factors ejij xixj couple different variables according symmetric matrix together factors produce joint distribution experiments graphical model’s parameters speciﬁed randomly provided input features inference. allow variety graph structures ranging complexity tree graphs grid graphs fully connected graphs. target marginals states given maxx experiments small graphs true values targets computed exactly exhaustive enumeration states. goal construct recurrent neural network canonical operations whose dynamics converge targets manner generalizes immediately graphical models. graph neural networks recurrent networks vectorvalued nodes whose states iteratively updated trainable nonlinear functions depend states neighbor nodes speciﬁed graph. form functions canonical i.e. shared graph edges function also depend properties edge. function parameterized neural network whose weights shared across edges. eventually states nodes interpreted another trainable ‘readout’ network. trained entire reused different graphs without alteration simply running different graph different inputs. figure mappings probabilistic graphical model graph neural network. example graphical model. mapping belief propagation messages nodes since different messages direction messages pairwise factor. message node connected message nodes share variable. mapping variable nodes onto nodes node connected others share factor graphical model. node corresponds message nodes graphical model. nodes connected corresponding message nodes connected message computed eij). update hidden state i→j).the readout extract node marginals states ﬁrst aggregates nodes target summation applies j→i). representation grows size number factors graphical model. second mapping uses nodes represent variable nodes probabilistic graphical model provide hidden states update factor nodes factors still inﬂuence inference since parameters passed message function iteration however avoids spending representational power properties change invariances tree-based reparameterization. mapping readout generated directly hidden state corresponding node mappings optimize networks minimize exact target marginals δxix∗ message functions mappings receive external inputs couplings edges necessary gnns infer correct marginals state. importantly message function depends hidden states source destination nodes previous time step. added ﬂexibility suggested expectation propagation algorithm where iteration inference proceeds ﬁrst removing previous estimate destination node updating based source distribution. experiments test well graph neural networks trained diverse small graph structures perform inference tasks. experiment test types gnns representing variable nodes representing message nodes examine generalization four conditions unseen graphs structure completely contrasting random graphs graphs size larger condition examine performance estimating marginal probabilities state. table experimental design training structured graphs nodes evaluated performance four classes graphical models i-iv different sizes graph topologies indicated table. figure performance gnn-based marginal inference training graphs. example graph structures used training testing shown adjacency matrices graphs estimated marginals shown true marginals msg-gnn node-gnn. individual dots reﬂect marginals single node graph. dots diagonal inference optimal. strengths normal distribution sample biases simulated data comprise training models validation models test models. graphical models small enough ground truth marginals states computed exactly enumeration. train gnns using adam learning rate validation error saturates early stopping window size nodes’ hidden states messages dimensions. experiments messages propagate time steps. mlps message function readout function hidden layers units each relu nonlinearities. understand properties learned evaluate different graph datasets ones trained condition test graphs size structure training graphs values singleton edge potentials differed. compared inferences ground truth well inferences drawn tested acyclic graphs exact gnns show impressive accuracy well however test graphs became loopier worsened substantially inference maintained strong performance condition increased graph size variables retaining graph structures training set. scenario scatter plots estimated versus true marginals show still outperforms loopy graphs except case graphs single loop quantify performance gnns average kullbackleibler divergence dklˆpi] across entire test graphs small large number nodes. performance gnns degrades graphs grow. however except msg-gnn tested nearly fully-connected graphs gnns perform better improvements order magnitude better graphs many loops investigate gnns generalize networks different size structure constructed connected random graphs also known erd˝os-r´enyi graphs systematically changed connectivity increasing edge probability smaller larger graphs gnns clearly ourperform irrespective size structure random graphs although inference methods show sizeconnectivity-dependent decline accuracy figure generalization performance gnns novel graphs. novel test graphs scatter plots estimated versus true marginals different inference algorithms plotted figure accuracy marginal inference measured negative kl-divergence scale graph structures shown smaller variants line colors indicate type inference method graphs scatter plots random graphs increasing edge probability nodes nodes generalization performance random graphs plotted past work provides insight dynamics convergence properties comparison examine node hidden states change time collecting distances successive node states despite variability mean distance decreases time independently graph topologies size suggests reasonable convergence inferences although rate ﬁnal precision convergence vary depending graph structures. mizing cross entropy loss delta function true target sigmoidal outputs gnns. marginalization experiments node-gnn slightly outperformed msg-gnn computing state signiﬁcantly outperform generalization tasks experiments demonstrated graph neural networks provide ﬂexible method learning perform inference probabilistic graphical models. showed learned representations nonlinear transformations operating edges graphical model generalize somewhat larger graphs even different structure. results support gnns excellent framework figure convergence inference measured mean standard deviation distances successive hidden node states time. displays dynamics four experimental conditions i-iv. reported experiments demonstrated successes small binary graphical models. future experiments consider training testing larger diverse graphs well broader classes graphical models nonbinary variables interesting sufﬁcient statistics nodes factors. expect training grows larger generalization abilities correspondingly increase resultant algorithm evaluated useful regularities. ﬂexibility nonlinearities implemented within graph neural networks allow apply learned inference techniques much wider range problems binary-valued pairwise markov random ﬁelds used here. continuous-valued distributions beneﬁt especially approach since exact marginals available even many simple cases. moreover exponential family even closed marginalization. nonetheless useful continually approximate iteratively improving marginals target exponential family. approach taken expectation propagation uses optimization best approximation. however likely computationally burdensome inference time learning canonical neural network perform task range inputs. examined possible representations graphical models within graph neural networks using variable nodes message nodes. interestingly experiments reveal beneﬁt expensive representations factor node. expected based theoretical arguments examining invariances belief propagation invariances direct consequence bp’s assumption tree graphs richer structure could principle perform better. possible structure could nodes factor nodes similar message graph fewer constraints information ﬂow. learn parameters. also computationally expensive procedure ﬁnding recurrent backpropagation efﬁcient method training gnns avoids onerous memory computation requirements. claimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/ibc u.s. government. belief propagation assumes incoming messages independent correct assumption tree graph. assumption still true iteratively nodes graphical model even though update schedule information could loops length messages pass backwards forwards tree graph. algorithm compensates loops aggregating messages excluding messages whose source current target node. reason loopy belief propagation incorrect graphs cycles information ﬂowing around longer loops wrongly integrated independent evidence. loopy messages independent correlated loop thus evidence misweighted. hypothesize major reason gnns outperform loopy graphs latent states store outbound information memories properly discount contributions incoming messages originated current target node. function enabled gated recurrent units synthesize incoming messages gnn. theory predicts longer memories therefore higher-dimensional latent states required graphs longer loops equal total strength. future experiments test hypothesis thereby guide novel neural network architectures improved probabilistic inference. three main threads artiﬁcial intelligence offer complementary advantages probabilistic statistical inference neural networks symbolic reasoning. combining strengths three provide best route forward general proposed combined probabilistic inference neural networks using neural networks’ ﬂexibility approximating functions canonical nonlinear structure inference problems sparsity direct interactions graphical models provide better performance example problems. successes encourage exploration. authors supported part intelligence advanced research projects activity department interior/interior business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disreferences almeida luis learning rule asynchronous perceptrons feedback combinatorial environment. proceedings first international conference neural networks volume ieee kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. deng zhiwei vahdat arash hexiang mori greg. structure inference machines recurrent neural networks analyzing relations group activity recognition. proceedings ieee conference computer vision pattern recognition duvenaud david maclaurin dougal iparraguirre jorge bombarell rafael hirzel timothy aspuru-guzik al´an adams ryan convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems gori marco monfardini gabriele scarselli franco. model learning graph domains. neural networks ijcnn’. proceedings. ieee international joint conference volume ieee tatikonda sekhar jordan michael loopy belief propagation gibbs measures. proceedings eighteenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. wainwright martin jaakkola tommi willsky alan tree-based reparameterization framework analysis sum-product related algorithms. ieee transactions information theory shih-en ramakrishna varun kanade takeo sheikh yaser. convolutional pose machines. proceedings ieee conference computer vision pattern recognition weiss yair freeman william correctness belief propagation gaussian graphical models arbitrary topology. advances neural information processing systems minka thomas expectation propagation approximate bayesian inference. proceedings seventeenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. noorshams nima wainwright martin stochastic belief propagation low-complexity alternative sum-product algorithm. ieee transactions information theory xiaojuan liao renjie jiaya fidler sanja urtasun raquel. graph neural networks rgbd semantic segmentation. proceedings ieee conference computer vision pattern recognition ross stephane munoz daniel hebert martial bagnell andrew. learning message-passing inference machines structured prediction. computer vision pattern recognition ieee conference ieee", "year": 2018}