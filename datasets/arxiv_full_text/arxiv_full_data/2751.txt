{"title": "Transfer Learning to Learn with Multitask Neural Model Search", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks. In this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.", "text": "deep learning models require extensive architecture design exploration hyperparameter optimization perform well given task. exploration model design space often made human expert optimized using combination grid search search heuristics large space possible choices. neural architecture search reinforcement learning approach proposed automate architecture design. successfully applied generate neural networks rival best human-designed architectures. however requires sampling constructing training hundreds thousands models achieve well-performing architectures. procedure needs executed scratch task. application wide tasks currently lacks transfer generalizable knowledge across tasks. paper present multitask neural model search controller. goal learn generalizable framework condition model construction successful model searches previously seen tasks thus signiﬁcantly speeding search tasks. demonstrate mnms conduct automated architecture search multiple tasks simultaneously still learning well-performing specialized models task. show pre-trained mnms controllers transfer learning tasks. leveraging knowledge previous searches pre-trained mnms models start better location search space reduce search time unseen tasks still discovering models outperform published human-designed models. designing deep learning models work well task requires extensive process iterative architecture engineering tuning. design decisions largely made human experts guided combination intuition grid search search heuristics. meta-learning aims automate model design using machine learning discover good architecture hyperparameter choices. recent advances meta-learning using reinforcement learning made promising strides towards accelerating even eliminating manual parameter search. example neural architecture search successfully discovered novel network architectures rival surpass best human-designed architectures challenging benchmark image recognition tasks however naively applying reinforcement learning task automated model construction requires sampling constructing training hundreds thousands networks relearn generate models scratch. human experts hand design tune networks based knowledge underlying dependencies search space experience prior tasks. therefore automatically learn leverage information. paper present multitask neural model search automated model construction framework ﬁnds best performing models search space multiple tasks simultaneneural architecture search method introduced applied construct convolutional neural networks cifar- task recurrent neural networks penn treebank tasks. later work authors attempted address computational cost using neural architecture search challenging tasks engineer convolutional architecture imagenet classiﬁcation paper demonstrated possible train controller simpler proxy cifar- task transfer architecture imagenet classiﬁcation stacking however work attempt transfer learn controller across multiple tasks relying instead human expert intuition additional network depth necessary challenging classiﬁcation task. additionally ﬁnal generated architectures required additional tuning choose hyperparameters learning rate evaluation test set. complexity model engineering machine learning widely recognized. optimization methods proposed ranging random search space possible architectures parameter modeling recent publications apply automate architecture generation. include metaqnn q-learning algorithm sequentially chooses layers metaqnn uses aggressive exploration reduce search time though cause resulting architectures underperform. separately propose agent transforms existing architectures incrementally avoid generating entire networks scratch. work also draws prior research transfer learning simultaneous multitask training. transfer learning shown achieve excellent results initialization method deep networks including models trained using simultaneous multitask training also facilitate learning tasks common structure though effectively retaining knowledge across tasks still active area research kirkpatrick samples architectures sequence actions. every action discretized design choice ﬁlter heights widths strides. child networks constructed architectures trained convergence. performance metric child network used reward update controller policy gradient algorithm. controller learns distribution architecture search space updated increase probability best performing architectures allowing sample better architectures time. original neural architecture search framework sampled models search space strictly architectural parameters later work shown framework extended automatically search model design parameters domains update rules network optimizers section describe multitask neural model search controller allows simultaneous model search multiple different tasks. many deep learning models require common design decisions choice network depth learning rate number training iterations; using generally deﬁned search space widely applicable architecture hyperparameter choices controller therefore engineer wide range models applicable many common machine learning tasks. multitask training space allow controller learn broadly applicable relationships search space actions leveraging shared behavior across tasks. mnms controller trained synchronously tasks. controller learns build differentiated architectures task. achieved sampling task uniformly beginning controller training iteration. task mapped unique embedding vector. tasks embeddings randomly initialized trained jointly controller. task embedding used condition model construction task. achieved concatenating task embedding every input controller rnn. speciﬁcally single-task controller generates output timestep determines distribution current actions. action sampled according action distribution embedded. action embedding passed back input next timestep. here multi-task training mnms multi-task training task embedding concatenated action embedding form input allowing condition action speciﬁc task previous works train controller using reinforce policy gradient algorithm recently algorithm here train using off-policy actor-critic algorithm actor controller generates sampled models critic controller trains replay bank sampled models rewards preliminary experiments on-policy training found controller shows reduced ability learn differentiated model task. speciﬁcally controller prone premature convergence single model design works generally well tasks best model tasks. on-policy sampling biased toward recent predictions optimal parameter distribution. hypothesis multitask training on-policy sampling prematurely reduce exploration better parameters individual task off-policy training allows actor controller continue explore separate parameter choices task better learn differentiated distribution parameter search space maximize expected performance each. figure overview multitask controller rnn. task embedding table maintained updated controller gradients learn differentiated task embeddings time. iteration multitask training task randomly sampled. task embedding passed controller along sampled action embedding timestep. full sequence outputted actions deﬁnes child architecture trained chosen task. task deﬁne different performance metric used reward. rewards affect amplitude updates controller need make sure distributions task rewards aligned mean similar variance. mean tasks reward distribution aligned scaling gradients advantage instead reward. advantage given model applied task deﬁned difference reward expected reward given task often referred baseline. standard technique usually applied increase training stability. multitask training baseline conditioned sampled task. keep track separate baseline task computed exponential moving average rewards recorded task. range tasks reward distribution normalized dividing advantage baseline refer normalized advantage. notice division baseline compromise convergence criteria seen using distinct adaptive learning rate task. using normalized advantage scale gradients instead reward allows mnms performance metric reward even training multiple tasks. using multitask framework transfer learn pretrained controllers simply reusing weights pretrained controller adding randomly initialized task embedding task. controller weights task embedding updated standard policy gradient steps. experiments also restart experience replay bank used off-policy critic rewards obtained task sampled. however future work could retain continue sample previously seen tasks order better retain controller memory former tasks. apply mnms setting demonstrating framework trained simultaneously design models separate text classiﬁcation tasks. transfer learn mnms model text classiﬁcation tasks demonstrate pre-trained framework achieves signiﬁcant speedups model search. binary spanish language identiﬁcation dataset consisting highest frequency wikipedia tokens english spanish german japanese. example label binary label denoting whether token spanish spanish. tasks chosen speciﬁcally differences task complexity language potential overﬁtting. would require controller capable true multitask model search differentiate tasks choosing optimal model parameters task. tasks chosen effectively transfer learned framework could conceivably leverage knowledge previous searches. baseline compare search convergence rates also trained mnms models scratch transfer learning tasks. four tasks deﬁne single general search space consisting common model parameters discrete parameter choices speciﬁed naive grid search parameters would therefore need parameter combinations search possible models. parameters represent general architectural training design choices applicable text classiﬁcation task. child networks constructed feed-forward neural networks using sampled parameters. speciﬁcally sampled parameter sequence consisting word embedding word embedding trainability number neural network layers nlayers number nodes layer nnodes learning rate number training iterations regularization weight construct feedforward network nlayers relu-activated layers nnodes layer. task network receives tokens embedded using continue gradient update entire word embedding table true. child model trained iterations using learning rate regularization weight child models ﬁnal fully-connected softmax layer trained using proximal adagrad optimizer batches training examples iteration. actor critic controller rnns used off-policy training -layer lstms hidden layer size timestep action task embeddings size resulting input size concatenation. controller embedding weights initialized uniformly random word embedding tables word embedding trainability number neural network layers number nodes hidden layer learning rate number training iterations regularization weight training controller receives gradient updates trained batches size learning rate updated gradient steps weights controllers averaged polyak average weight reward used updating controller cubed accuracy validation set. figure smoothed sampled model accuracy curves multitask training simultaneously spanish language identiﬁcation tasks best validation accuracy achieved task. curves shown savitzky-golay ﬁltering clarity. train mnms models simultaneously spanish language identiﬁcation tasks. accuracies achieved sampled child models time shown figure well validation accuracy achieved best sampled models task. model accuracy additionally best discovered model design outperforms hand-tuned state-ofthe-art model within subset models similar approach best performance task obtained complex architectures within scope search space also mnms framework differentiate tasks choose optimal parameters each. figure show mnms learns differentiated distributions parameter search space separate tasks. example mnms learns choose word embedding pre-trained spanish documents spanish language identiﬁcation task choosing word embeddings pre-trained english dataset stanford sentiment treebank task. finally mnms learns trivial language identiﬁcation task significant difference continuing train word embedding vectors simply using ﬁxed pre-trained word embeddings. task contains longer complex examples model learns must continue training word embeddings achieve better performance. similarly search converges favor higher hidden layer dimensions training iterations complex task. figure compares smoothed validation accuracy curves baseline mnms models trained scratch imdb corpus cine tasks mnms models pre-trained spanish language identiﬁcation. observe transfer learning allows mnms start better initial location parameter search space train consistently stably converge much quickly ﬁnding good parameters tasks. additionally best learned models discovered mnms perform essentially identically regardless whether search started scratch transfer learned pre-trained model demonstrating search biased towards pre-training converges prematurely local optima. compared hand-tuned state-of-the-art benchmarks also using averaged word vector inputs mnms discovers models outperform documented benchmarks tasks figure smoothed sampled model accuracy curves mnms models trained imdb corpus cine comparing models trained scratch without transfer learning models transfer learned pre-training. curves smoothed using savitzky-golay ﬁltering clarity. also mnms learns task embeddings encode expected relationships tasks example strong learned correlation imdb task embeddings separately spanish language identiﬁcation corpus cine task embeddings. figure heatmap showing correlations learned task embeddings pre-trained mnms controller transfer learned corpus cine imdb sentiment classiﬁcation tasks. machine learning model design choices exist vacuum. human experts design good models leveraging signiﬁcant prior knowledge intuitive relationships model parameters performance obtained different model designs similar tasks. automated model design algorithms learn models discovered prior tasks. paper demonstrates multitask neural model search discover good differentiated model designs multiple tasks simultaneously learning task embeddings encode meaningful relationships tasks. show multitask training provides good baseline transfer learning future tasks allowing mnms framework start better location search space converge quickly high-performing designs. james bergstra daniel yamins david cox. making science model search hyperparameter optimization hundreds dimensions vision architectures. international conference machine learning fermın cruz jose troyano fernando enriquez javier ortega. clasiﬁcaci´on documentos basada opini´on experimentos corpus crıticas cine espanol. procesamiento lenguaje natural james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabska-barwinska overcoming catastrophic forgetting neural networks. proceedings national academy sciences andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume association computational linguistics richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing whye victor bapst wojciech marian czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu. distral robust multitask reinforcement learning. arxiv preprint arxiv. barret zoph vijay vasudevan jonathon shlens quoc learning transferable architectures scalable image recognition. corr abs/. http//arxiv.org/ abs/..", "year": 2017}