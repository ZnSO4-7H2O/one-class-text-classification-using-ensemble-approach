{"title": "Benefits of depth in neural networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "For any positive integer $k$, there exist neural networks with $\\Theta(k^3)$ layers, $\\Theta(1)$ nodes per layer, and $\\Theta(1)$ distinct parameters which can not be approximated by networks with $\\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed \"semi-algebraic gates\" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $\\Omega(2^{k^3})$ total tree nodes are required).", "text": "positive integer exist neural networks layers nodes layer distinct parameters approximated networks layers unless exponentially large must possess nodes. result proved class nodes termed semi-algebraic gates includes common choices relu maximum indicator piecewise polynomial functions therefore establishing beneﬁts depth standard networks relu gates also convolutional networks relu maximization gates sum-product networks boosted decision trees maxi standard relu gate max{ called relu parameters vary node node. graphs present work acyclic exactly node outgoing edges whose computation output network. neural networks distinguish many function classes used machine learning possessing multiple layers meaning output result composing together arbitrary number nonlinear operations; contrast functions computed boosted decision stumps svms written neural networks constant number layers. purpose present work show standard types networks always gain representation power addition layers. concretely shown every positive integer exist neural networks layers nodes layer distinct parameters approximated networks layers nodes. stating main result choices pieces notation deserve explanation. first target many-layered function uses standard relu gates; means necessary general statement found theorem secondly notion approximation distance given functions pointwise disagreement averaged cube well proofs allow ﬂexibility lastly shallower networks used approximation semi-algebraic gates generalize earlier maximization standard relu gates allow analysis standard networks relu gates convolutional networks relu maximization gates sum-product networks boosted decision trees; full deﬁnition semi-algebraic gates appears section theorem integer dimension given. exists computed neural network standard relu gates layers total nodes distinct parameters analogs theorem boolean circuits boolean inputs routed {and not} gates studied extensively circuit complexity community called depth hierarchy theorems. seminal result h˚astad establishes inapproximability parity function shallow circuits standard neural networks appear received less study; closest present work investigation eldan shamir analyzing case dimension large showing exponential separation -layer networks regime handled theorem bibliographic notes open problems found section proof theorem occupies section idea function compositions sufﬁce construct highly oscillatory function whereas function addition gives function oscillations. thereafter elementary counting argument sufﬁces show low-oscillation functions approximate high-oscillation functions. theorem provides existence network approximated network many fewer layers. natural wonder many special functions. following bound indicates population fact quite modest. speciﬁcally construction behind theorem elaborated theorem seen exhibiting points ﬁxed labeling points upon shallow network hardly improves upon random guessing. forthcoming theorem similarly shows even simpler task ﬁtting points earlier class networks useless random labellings. order state result deﬁnitions order. firstly result notion neural network restrictive. neural graph denote graph structure also assignment gate functions nodes edges inputs gates assignment free parameters parameters gates. denote class functions obtained varying free parameters; deﬁnition fairly standard discussed detail section ﬁnal piece notation given function denote corresponding classiﬁer theorem neural graph given parameters layers total -semi-algebraic nodes. points proof direct corollary dimension semi-algebraic networks turn proved small modiﬁcation dimension proof piecewise polynomial networks moreover core methodology dimension bounds neural networks warren whose goal analog theorem polynomials lemma neural graph given parameters layers total nodes -semi-algebraic. deﬁnition semi-algebraic gate unfortunately complicated; designed capture standard nodes single abstraction without degrading bounds. note name semialgebraic standard refers deﬁned unions intersections polynomial inequalities deﬁnition function -semi-algebraic) exist polynomials subsets polynomial degree distinguished special cases semi-algebraic gates follows lemma standard piecewise polynomial gates generalize relu received fair attention theoretical community function -poly partitioned intervals polynomial degree within piece. maximization minimization gates become popular convolutional networks discussed section lastly decision trees boosted decision trees practically successful classes usually viewed competitors neural networks following structure. deﬁnition k-dt deﬁned recursively follows. constant function. ﬁrst evaluates thereafter conditionally evaluates either left l-dt right r-dt -bdt notation neural networks semi-algebraic gate simply function domain role neural network complicated domain function must partitioned arguments three types input network parameter vector vector real numbers coming parent nodes. convention input accessed root nodes convenience layer denote input itself nodes node parameter vector made available nodes layers though might subset speciﬁcally internal node computes function using parents semi-algebraic gate meaning fk). another common practice nodes apply univariate activation function afﬁne mapping parents weights afﬁne combination parameters permitted network additionally correspond edges graph. parameter appear multiple times network explains number parameters theorem less number edges nodes. entire network computes function equivalent function computed single node outgoing edges. stated previously denote graph underlying network also assignment gates nodes parameters parent outputs plugged gates functions obtained varying thus function deﬁned above corresponding computation performed results related dimension meaning theorem lemma class results instance theorem generalization theorem parameters also network graph vary. denote network layer nodes input dimension simpliﬁcation denote networks gates layers nodes. various empirical prescriptions vary number nodes layer; instance convolutional networks typically increase layer layer followed exponential decrease layers ﬁnally layers number nodes idea behind ﬁrst step depicted right. given functions denote partitions intervals classiﬁers constant within interval. formally count oscillations deﬁne crossing number |if| much larger piecewise constant regions exhibit many oscillations thus poorly approximates lemma given take denote partition given pieces arguably strange form left hand side bound lemma accommodate different notions distance. distance lebesgue measure theorem sufﬁce cross must regular meaning must cross appreciable distance crossings must evenly spaced. however merely show give different classiﬁers arbitrary measure additional regularity needed. proof respectively denote sets intervals corresponding |if| |ig|. every fixing since constant whereas alternates number elements disagrees everywhere |xj|/ |xj| even least |xj| thus least general. such control expression note every disjoint however ∪j∈ij smaller particular misses intervals whose interior intersects boundary interval since boundaries |if| preceding section oscillations function counted crossing number since handles univariate functions multivariate case handled ﬁrst choosing afﬁne considering giving central upper bounds sketching proofs notice analogy polynomials compositions additions vary impact upon oscillations. adding together polynomials resulting polynomial twice many terms exceed maximum degree either polynomial. hand composing polynomials result product degrees product terms. impact number roots crossings composition wins race higher oscillations. lemma afﬁne. proof somewhat painful owing fact convention structure intervals partitions namely ends closed open thus deferred appendix principle proof elementary depicted right given collection partitions intersection constituent intervals must share endpoints intervals intersection thus total number intervals bounds total number possible intersections. arguably failure increase complexity face arbitrary intersections semi-algebraic gates care number terms deﬁnition. recall means polynomials degree form regions deﬁning function intersecting simpler regions such order analyze semi-algebraic gates composed piecewise polynomial gates consider ﬁrst behavior predicate polynomials. lemma suppose polynomial degree -poly. -poly partition deﬁning reﬁnement partitions within pieces deﬁning proof lemma exists partition intervals reﬁnes partitions deﬁning since polynomial degree within intervals composition gives polynomial degree proof deﬁnition polynomial regions deﬁned intersections predicates lemma -poly thus together deﬁne partition gk)) pieces lemma cardinality reﬁnes partitions lemma partitions across predicate polynomials reﬁned single partition size thus also reﬁnes partitions deﬁned thanks reﬁnements element ﬁnal partition ﬁxed polynomial degree meaning βγ)-poly. many layers many oscillations idea behind construction follows. consider continuous function generalization triangle wave single peak additionally strictly increases along strictly decreases along consider effect composition along stretched copy since moreover bijection reasoning applies along meaning function peaks. iterating argument implies function peaks; following deﬁnition lemmas formalize reasoning. deﬁnition )-triangle continuous along divided intervals whenever strictly increasing along odd-numbered intervals strictly decreasing along even-numbered intervals. lemma )-triangle )-triangle )-triangle. proof since continuous along continuous along remaining analysis respectively denote interval boundaries consider interval meaning restriction strictly increasing. shown )-triangle analogous proof holds strictly decreasing restriction whereby follows considering choices lemma following functions )-triangle. min{σr schmitt lastly consider ﬁrst example min{σr whose graph linearly interpolates consequently along linear interpolates analogous meaning produced copies shrunken horizontally factor process repeats meaning copies grants regularity needed lebesgue measure theorem lemma real positive integer given choose unique nonnegative integer real proof theorem follows lemma shows many-layered relu network give rise highly oscillatory regular function lemma shows few-layered networks decision trees give rise functions oscillations lastly lemma shows combine inapproximability result. last piece proof averages possible offsets considers univariate problems composing networks afﬁne result carries resemblance random projection technique used depth hierarchy theorems boolean functions well earlier techniques complexities multivariate sets albeit extremely primitive form proof deﬁne denote pieces meaning |if| corollary grants moreover lemma triangle height base either handle distances ﬂexible target functions. theorem integer function given )-triangle deﬁne every deﬁne afﬁne function exist borel probability measures discrete uniform points continuous positive exactly every every satisﬁes theorem taken exists labeling points realizable network depth size approximated networks depth size hand section sketch proof theorem implies depth networks realize relatively different labellings. proof quick consequence dimension semi-algebraic networks following fact used denote growth function proof preceding result similar proofs gilbert-varshamov packing bound hoeffding’s inequality note similar result used warren prove rates approximation continuous functions polynomials without invoking hoeffding’s inequality remaining task control dimension semi-algebraic networks. note following generalization lemma provides semi-algebraic networks compute functions polynomial restricted certain polynomial regions. lemma neural network graph given parameters layers total nodes suppose every gate -sa. proof follows basic structure bound networks piecewise polynomial activation functions slightly modiﬁed proof also similar proof lemma performing induction layers network arguing node computes polynomial restricting attention range parameters. proof lemma manages multivariate though requires arguments warren signiﬁcantly complicated lemma minor departure dimension proof piecewise polynomial networks following lemma used track number regions complicated semi-algebraic networks. despite generalization dimension bound basically piecewise polynomial networks. lemma polynomials given degree arguably ﬁrst approximation theorem class smaller weierstrass approximation theorem states polynomials uniformly approximate continuous functions compact sets reﬁning this kolmogorov gave bound well subspaces functions approximate continuous functions vitushkin showed similar bound approximation polynomials terms polynomial degrees dimension modulus continuity target function. warren gave alternate proof generalization result process effectively proving dimension polynomials producing analog theorem polynomials. preceding results however focused separating large classes small classes aiming reﬁne this depth hierarchy theorems circuit complexity separated circuits certain depth circuits slightly smaller depth. mentioned section seminal result h˚astad architectures closer neural networks sum-product networks analyzed bengio delalleau recently martens medabalimi networks linear threshold functions layers kane williams note polynomial gates linear threshold gates semi-algebraic gates. closely present work vastly simpliﬁed account) eldan shamir analyze -layer networks general activation functions composed afﬁne mappings showing separations exponential input dimension. result also recent advances circuit complexity natural suppose theorem strengthened separating layer networks dimension large; however none earlier works give tight sense behavior triangle wave target functions considered appeared various forms throughout literature. general properties piecewise afﬁne highly oscillating functions investigated szymanski mccane mont´ufar also schmitt investigated show sigmoidal networks approximate high degree polynomials analysis similar here however looseness bounds sigmoidal networks prevented exponential separations depth hierarchies. tantalizing direction future work characterize difﬁcult function many even functions well-approximated smaller depths. arguably direction could value machine learning discovery underlying structure could lead algorithms recover trivial example sort structure could arise considering following proposition stating symmetric signal repeated pre-composing relu triangle function. proposition σr−σr) given satisﬁes every real integer words repetitions graph scaled horizontally uniformly within author indebted joshua zahl help navigating semi-algebraic geometry simpliﬁcation multivariate case theorem rastislav telg´arsky introduction general topic kolmogorov’s superposition theorem author thanks jacob abernethy peter bartlett s´ebastien bubeck alex kulesza valuable discussions.", "year": 2016}