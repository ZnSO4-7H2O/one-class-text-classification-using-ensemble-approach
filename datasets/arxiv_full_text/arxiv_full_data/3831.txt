{"title": "Attention-over-Attention Neural Networks for Reading Comprehension", "tag": ["cs.CL", "cs.NE"], "abstract": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.", "text": "cloze-style reading comprehension representative problem mining relationship document query. paper present simple novel model called attention-over-attention reader better solving cloze-style reading comprehension task. proposed model aims place another attention mechanism document-level attention induces attended attention ﬁnal answer predictions. advantage model simpler related works giving excellent performance. addition primary model also propose n-best re-ranking strategy double check validity candidates improve performance. experimental results show proposed methods signiﬁcantly outperform various state-ofthe-art systems large margin public datasets children’s book test. read comprehend human languages challenging tasks machines requires understanding natural languages ability reasoning various clues. reading comprehension general problem real world aims read comprehend given article context answer questions based recently cloze-style reading comprehension problem become popular task community. cloze-style query problem appropriate word given sentences taking context information account. comprehensions large-scale training data necessary learning relationships given document query. create large-scale training data neural networks hermann released cnn/daily mail news dataset document formed news articles queries extracted summary news. hill released children’s book test dataset afterwards training samples generated consecutive sentences books query formed sentence. following datasets vast variety neural network approaches proposed stem attention-based neural network become stereotype tasks well-known capability learning importance distribution inputs. paper present novel neural network architecture called attention-over-attention model. understand meaning literally model aims place another attention mechanism existing document-level attention. unlike previous works using heuristic merging functions setting various pre-deﬁned non-trainable terms model could automatically generate attended attention various document-level attentions make mutual look query-to-document also document-to-query beneﬁt interactive information. unlike previous works introducing complex architectures many non-trainable hyper-parameters model model much simple outperforms various state-of-the-art systems large margin. following paper organized follows. section give brief introduction cloze-style reading comprehension task well related public datasets. proposed attention-over-attention reader presented detail section n-best reranking strategy section experimental results analysis given section section related work discussed section finally give conclusion paper envisions future work. triple consists document query answer query note answer usually single word document requires human exploit context information document query. type answer word varies predicting preposition given ﬁxed collocation identifying named entity factual illustration. existing public datasets large-scale training data essential training neural networks. several public datasets cloze-style reading comprehension released. here introduce representative widely-used datasets. daily mail hermann ﬁrstly published datasets daily mail news data construct datasets web-crawled daily mail news data. characteristics datasets news article often associated summary. ﬁrst regard main body news article document query formed summary article entity word replaced special placeholder indicate missing word. replaced entity word answer query. apart releasing dataset also proposed methodology anonymizes named entity tokens data tokens also re-shufﬂe sample. motivation news articles containing limited named entities usually celebrities world knowledge learned dataset. methodology aims exploit general relationships anonymized named entities within single document rather common knowledge. following research datasets showed entity word anonymization effective expected children’s book test also dataset called children’s book test released hill built children’s book story project gutenberg different cnn/daily mail datasets summary available children’s book. proposed another extract query original data. document composed consecutive sentences story sentence regarded query word blanked special placeholder. cbtest datasets four types sub-datasets available classiﬁed part-of-speech named entity answer word containing named entities common nouns verbs prepositions. studies found answering verbs prepositions relatively less dependent content document humans even preposition blank-ﬁlling without presence document. studies shown hill answering verbs prepositions less dependent presence document. thus related works focusing solving types. section give detailed introduction proposed attention-over-attention reader model primarily motivated kadlec aims directly estimate answer document-level attention instead calculating blended representations document. previous studies showed investigation query representation necessary paid attention utilizing information query. paper propose novel work placing another attention primary attentions indicate importance attentions. give formal description proposed model. cloze-style training triple given proposed model constructed following steps. contextual embedding ﬁrst transform every word document query one-hot representations convert continuous representations shared embedding matrix sharing word embedding document query participate learning embedding beneﬁt mechanism. that bi-directional rnns contextual representations document query individually representation word formed concatenating forward backward hidden states. making trade-off between model performance training complexity choose gated recurrent unit recurrent unit implementation. take hdoc r|d|∗d hquery r|q|∗d denote contextual representations document query dimension pair-wise matching score obtaining contextual embeddings document hdoc query hquery calculate pair-wise matching matrix indicates pair-wise matching degree document word query word. formally given word document word query compute matching score product. calculate every pair-wise matching score document query word forming matrix r|d|∗|q| value column ﬁlled individual attentions getting pair-wise matching matrix apply column-wise softmax function probability distributions column column individual document-level attention considering single query word. denote r|d| document-level attention regarding query word time seen query-to-document attention. attention-over-attention different instead using naive heuristics combine individual attentions ﬁnal attention introduce another attention mechanism automatically decide importance individual attention. first calculate reversed attention every document word time calculate importance distribution query indicate query words important given single document word. apply row-wise softmax function pair-wise matching matrix query-level attentions. denote r|q| query-level attention regarding document word time seen final predictions following kadlec attention mechanism aggregated results. note ﬁnal output reﬂected vocabulary space rather document-level attention make signiﬁcant difference performance though kadlec illustrate clearly. obtained query-todocument attention document-to-query attention motivation exploit mutual information document query. however previous works relying query-to-document attention calculate document-level attention considering whole query. finally calculate product attended document-level attention r|d| i.e. attention-over-attention mechanism. intuitively operation calculating weighted individual document-level attention looking query word time proposed neural network architecture depicted figure note that model mainly adds limited steps calculations reader employ additional weights computational complexity similar reader. intuitively cloze-style reading comprehensions often reﬁll candidate blank query double-check appropriateness ﬂuency grammar candidate choose suitable one. problems candidate choose choose second possible candidate checking again. mimic process double-checking propose n-best re-ranking strategy generating answers neural networks. procedure illustrated follows. n-best decoding instead picking candidate highest possibility answer also extract follow-up candidates decoding process forms n-best list. reﬁll candidate query characteristic cloze-style problem candidate reﬁlled blank query form complete sentence. allows check candidate according context. feature scoring candidate sentences scored many aspects. paper exploit three features score n-best list. local n-gram different global local aims explore information given document statistics obtained test-time document. noted local trained sample-by-sample trained entire test legal real test case. model useful many unknown words test sample. word-class similar global word-class also trained document part training data words converted word class word class obtained using clustering methods. paper simply utilized mkcls tool generating word classes weight tuning tune weights among features adopt k-best mira algorithm automatically optimize weights validation widely used statistical machine translation tuning procedure. re-scoring re-ranking getting weights feature calculate weighted feature nbest sentences choose candidate lowest cost ﬁnal answer. deep lstm reader attentive reader human memnn reader reader stanford reader iterative attention epireader reader reader reranking memnn reader reader epireader iterative attention reader reader optimization adopted adam optimizer weight updating initial learning rate units still suffer gradient exploding issues gradient clipping threshold used batched training strategy samples. dimensions embedding hidden layer task listed table re-ranking step generate -best list baseline neural network model observe signiﬁcant variance changing n-best list size. language model features trained training proportion dataset -gram wordbased setting kneser-ney smoothing trained srilm toolkit results reported best model selected performance validation set. ensemble model made four best models trained using different random seed. implementation done theano keras models trained tesla gpu. overall results experiments carried public datasets news datasets cbtest ne/cn datasets statistics datasets listed table experimental results given table that reader outperforms state-of-the-art systems large margin absolute improvements epireader cbtest test sets demonstrate effectiveness model. also adding additional features re-ranking step another signiﬁcant boost reader cbtest ne/cn test sets. also found single model could stay previous best ensemble system even absolute improvement beyond best ensemble model cbtest validation set. comes ensemble model reader also shows signiﬁcant improvements previous best ensemble models large margin state-of-the-art system. investigate effectiveness employing attention-over-attention mechanism also compared model reader used predeﬁned merging heuristics etc. instead using pre-deﬁned merging heuristics letting model explicitly learn weights individual attentions results significant boost performance improvements made validation test reader. effectiveness re-ranking strategy seen re-ranking approach effective cloze-style reading comprehension task give detailed ablations section show contributions feature. thorough investigation re-ranking step listed detailed improvements adding feature mentioned section results table found category beneﬁt re-ranking features proportions quite different. generally speaking category performance mainly boosted lmlocal feature. however contrary category beneﬁts lmglobal lmwc rather lmlocal. also listed weights feature table lmglobal lmwc trained training seen global feature. however lmlocal trained within respective document part test sample seen local feature. table detailed results -best re-ranking cbtest ne/cn datasets. includes features previous rows. lmglobal denotes global lmlocal denotes local lmwc denotes word-class calculated ratio global local features found category much dependent local features category. much likely meet named entity common noun test phase adding local provides much information common noun. however contrary answering common noun requires less local information learned training data relatively. section give quantitative analysis reader. following analyses carried cbtest dataset. first investigate relations length document corresponding accuracy. result depicted figure reader shows consistent improvements reader different length document. especially length document exceeds improvements become larger indicating reader capable handling long documents. hermann proposed method obtaining large quantities triples news articles summary. along release cloze-style reading comprehension dataset also proposed attention-based neural network handle task. experimental results showed proposed neural network effective traditional baselines. hill released another dataset stems children’s books. different hermann work document query generated story without summary much general previous work. handle reading comprehension task proposed window-based memory network self-supervision heuristics also applied learn hard-attention. unlike previous works using blended representations document query estimate answer kadlec proposed simple model directly pick answer document motivated pointer network restriction model answer single word appear document. results various public datasets showed proposed model effective previous works. proposed exploit reading comprehension models tasks. ﬁrst applied reading comprehension model chinese zero pronoun resolution task automatically generated large-scale pseudo training data. experimental results ontonotes data showed method signiﬁcantly outperforms various state-of-the-art systems. work primarily inspired kadlec latter model widely applied many follow-up works unlike reader assume heuristics model using merge functions etc. used mechanism called attentionfurthermore also investigate model tends choose high-frequency candidate lower shown figure surprisingly found models good correct answer appears frequent document candidates. correct answer highest frequency among candidates takes test interestingly also found that frequency rank correct answer exceeds models also give relatively high performance. empirically think models tend choose extreme cases terms candidate frequency possible reason over-attention explicitly calculate weights different individual document-level attentions ﬁnal attention computing weighted them. also model typically general simple recently proposed model brings signiﬁcant improvements cutting edge systems. present novel neural architecture called attention-over-attention reader tackle clozestyle reading comprehension task. proposed reader aims compute attentions document also query side beneﬁt mutual information. weighted attention carried attended attention document ﬁnal predictions. among several public datasets model could give consistent signiﬁcant improvements various state-of-theart systems large margin. future work carried following aspects. believe model general apply tasks well ﬁrstly going fully investigate usage architecture tasks. also interested machine really comprehend language utilizing neural networks approaches serve document-level language model. context planning investigate problems need comprehensive reasoning several sentences. would like thank three anonymous reviewers thorough reviewing providing thoughtful comments improve paper. work supported national leading technology research project grant danqi chen jason bolton christopher manning. cnn/daily mail reading comprehension task. proceedings annual meeting association computational linguistics association computational linguistics pages http//aclweb.org/anthology/d-. yiming ting zhipeng chen shijin wang guoping consensus attentionbased neural networks chinese reading comproceedings coling prehension. international conference computational linguistics technical papers. coling organizing committee pages http//aclweb.org/anthology/c-. karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. advances neural information processing systems. pages felix hill antoine bordes sumit chopra jason weston. goldilocks principle reading children’s books explicit memory representations. arxiv preprint arxiv. efﬁcient method determining bilingual word classes. ninth conference european chapter association computational linguistics. http//aclweb.org/anthology/e-. rudolf kadlec martin schmid ondˇrej bajgar kleindienst. text understanding attention reader network. proceedings annual meeting association computational linguistics association computational linguistics pages https//doi.org/./v/p-. reinhard kneser hermann ney. improved backing-off m-gram language modeling. international conference acoustics speech signal processing. pages vol.. ting yiming qingyu shijin wang weinan zhang guoping generating exploiting large-scale pseudo training data zero pronoun resolution. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research adam trischler zheng xingdi yuan philip bachman alessandro sordoni kaheer suleman. natural language comprehension proceedings conference epireader. empirical methods natural language processing. association computational linguistics pages http//aclweb.org/anthology/d.", "year": 2016}