{"title": "A Deep Architecture for Semantic Matching with Multiple Positional  Sentence Representations", "tag": ["cs.AI", "cs.CL", "cs.NE"], "abstract": "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.", "text": "cntn lstm-rnn general paradigm quite straightforward easy implement however main disadvantage lies important local information lost compressing complicated sentence single vector. taking question answers example keywords three world important determine answer better attending three obviously better attending world opposite conclusion. however single sentence representation methods cannot well capture important local information directly representing complicated sentence single compact vector works focus taking multiple granularity e.g. word phrase sentence level representations consideration matching process. examples include arc-ii deepmatch bi-cnn-mi multigrancnn alleviate problem still completely solving matching problem. limited well capture contextualized local information directly involving word phrase level representations. taking following answer example obviously better respect although important keywords three. terms three different meanings whole sentence perspective. three focuses talking three football teams indicating three attendees different countries. however existing multiple granularity deep models cannot well distinguish threes. mainly word/phrase level representations local thus limited reﬂect true meanings words/phrases perspective matching natural language sentences central many applications information retrieval question answering. existing deep models rely single sentence representation multiple granularity representations matching. however methods cannot well capture contextualized local information matching process. tackle problem present deep architecture match sentences multiple positional sentence representations. speciﬁcally positional sentence representation sentence representation position generated bidirectional long short term memory matching score ﬁnally produced aggregating interactions different positional sentence representations k-max pooling multi-layer perceptron. model several advantages using bi-lstm rich context whole sentence leveraged capture contextualized local information positional sentence representation; matching multiple positional sentence representations ﬂexible aggregate different important contextualized local information sentence support matching; experiments different tasks question answering sentence completion demonstrate superiority model. semantic matching critical task many applications natural language processing information retrieval question answering paraphrase identiﬁcation taking question answering example given pair question answer matching function required determine matching degree sentences. recently deep neural network based models applied area achieved important progresses. deep models follow paradigm ﬁrst represent whole sentence single distributed representation compute similarities vectors output matching score. examples include dssm cdsmm arc-i copyright association advancement artiﬁcial intelligence rights reserved. mv-lstm consists three parts firstly positional sentence representation sentence representation position generated bidirectional long short term memory secondly interactions different positional sentence representations form similarity matrix/tensor different similarity functions; lastly ﬁnal matching score produced aggregating interactions k-max pooling multilayer perceptron. step positional sentence representation positional sentence representation requires reﬂect representation whole sentence attending position. therefore natural bi-lstm generate representation lstm capture long short term dependencies sentences. besides nice property emphasize nearby words representation process firstly give introduction lstm bi-lstm. long short term memory advanced type recurrent neural network using memory cells gates learn long term dependencies within sequence lstm several variants adopt common implementation used without peephole connections given input sentence word embedding position lstm outputs representation position follows. denote input forget output gates respectively. information stored memory cells representation. compared single directional lstm bidirectional lstm utilizes previous future context processing data directions separate lstms lstm processes input sequence forward direction processes input reverse direction. therefore obtain vectors position. reﬂect meaning whole sentence directions attending position therefore reasonable deﬁne positional sentence representation combination them. speciﬁcally position t-th positional sentence rep← resentation generated concatenating stands transpoi.e. generate positional sentence representations paper. speciﬁcally position bi-lstm obtain hidden vectors reﬂect meaning whole sentence directions attending position. positional sentence representation generated concatenating directly. second step model interactions positional sentence representations. paper three different operations adopted model interactions cosine bilinear tensor layer. finally adopt k-max pooling strategy automatically select strongest interaction signals aggregate produce ﬁnal matching score multi-layer perceptron model parameters learned automatically training data backpropagation stochastic gradient descent. model well capture contextualized local information matching process. compared single sentence representation methods mv-lstm well capture important local information introducing multiple positional sentence representations. compared multiple granularity deep models mv-lstm leveraged rich context determine importance local information using bi-lstm generate positional sentence representation. finally conduct extensive experiments tasks i.e. question answering sentence completion validate arguments. experimental results show mv-lstm outperform several existing baselines tasks including arc-i arc-ii cntn deepmatch multigrancnn lstm-rnn. deep architecture aggregate interactions positional sentence representations semantic matching positional sentence representation generated bi-lstm; step interactions sentences basis positional sentence representations model interactions pair sentences different positions. many kinds similarity functions slice tensor parameters parameters linear part. non-linear function rectiﬁer paper since always outputs positive value compatible similarity. step interaction aggregation introduce third step architecture i.e. integrate interactions different positional sentence representations output matching score sentences. k-max pooling matching sentences usually determined strong interaction signals. therefore k-max pooling automatically extract strongest interactions matrix/tensor similar speciﬁcally interaction matrix scan whole matrix values directly returned form vector according descending order. interaction tensor values slice tensor returned form vector. finally vectors concatenated single vector k-max pooling meaningful suppose cosine similarity directly outputs largest interaction means best matching position considered model; larger means utilize matching positions conduct semantic matching. therefore easy detect best matching position lies whether need aggregate multiple interactions different positions matching. experiments show best matching position usually ﬁrst last better results obtained leveraging matchings multiple positions. multilayer perception finally output matching score aggregating strong interaction signals ﬁltered k-max pooling. speciﬁcally feature vector obtained k-max pooling ﬁrst feed full connection hidden layer obtain higher level representation matching score obtained linear transformation figure illustration mv-lstm. input sentences. positional sentence representations ﬁrst obtained bi-lstm. k-max pooling selects interactions interaction matrix matching score ﬁnally computed multilayer perceptron. used modeling interactions stand j-th positional sentence representations sentences respectively. paper three similarity functions including cosine bilinear tensor layer. given vectors three functions output similarity score follows. bilinear considers interactions different dimensions thus capture complicated interactions compared cosine. speciﬁcally similarity score computed follows matrix reweight interactions different dimensions bias. applying bilinear compute interaction corresponding positional sentence representations sentence obviously bilinear well capture interleaving interactions cosine cannot. therefore bilinear capture meaningful interactions positional sentence representations compared cosine. tensor layer powerful functions roll back similarity metrics bilinear product. also shown great superiority modeling interactions vectors that’s multigrancnn ﬁrst uses cnns obtain word phrase sentence level representations computes matching score based interactions among representations arc-i cntn lstm-rnn single sentence representation models arc-ii deepmatch multigrancnn represent sentence multiple granularity. parameter settings word embeddings required model baseline deep models initialized skipgram wordvec word embedding trained wiki corpus directly comparing previous works. word embeddings trained whole dataset. dimensions besides hidden representation dimensions lstms also batchsize tasks. trainable parameters initialized randomly uniform distribution scale selected according performance validation tasks). initial learning rates adagrad also selected validation evaluation metrics tasks formalized ranking problem. speciﬁcally output ranking list sentences according descending order matching scores. goal rank positive higher negative ones. therefore precision mean reciprocal rank evaluation metrics. since positive example list formalized follows question answering question answering typical task semantic matching. paper dataset collected yahoo answers community question answering system users propose questions system users submit answers. user parameters model including parameters word embedding bi-lstm interaction function trained jointly backpropagation stochastic gradient descent. speciﬁcally adagrad parameters training. discussions mv-lstm cover lstm-rnn special case. speciﬁcally consider last positional sentence representation sentence generated single directional lstm mv-lstm directly reduces lstm-rnn. therefore mv-lstm general ability leverages positional sentence representations matching compared lstm-rnn. mv-lstm implicitly taken multiple granularity consideration. using bi-lstm ability involve long short term dependencies representing sentence mv-lstm potential capture important n-gram matching patterns. furthermore mv-lstm ﬂexible involve important granularity adaptively compared based models using ﬁxed window sizes. experimental settings firstly introduce experimental settings including baselines parameter settings evaluation metrics. baselines experiments tasks baselines listed follows. random guess outputs random ranking list testing. popular strong baseline information memory stick error sony cyber shot? might want format memory stick error message receiving. never heard stack underﬂow error overﬂow overﬂow running virtual memory. proposes question decide best answer. whole dataset contains pairs question accompanied best answer. select pairs questions best answers length that pairs form positive pairs. negative sampling adopted construct negative pairs. speciﬁcally question ﬁrst best answer query retrieval results whole answer lucene. randomly select answers construct negative pairs. last separate whole dataset training validation testing data proportion table gives example data. analysis different pooling parameters introduced approach pooling parameter meaningful model. obtain best matching position sentences. larger leveraging multiple matching positions determine ﬁnal score. therefore conduct experiments demonstrate inﬂuences different pooling parameters. here interaction function ﬁxed cosine order directly compare baseline model lstm-rnn similar results obtained interaction functions bilinear tensor layer. experiment report different results shown table performance better larger used k-max pooling mvlstm. means multiple sentence representations help matching. also observe larger improvement quite limited. therefore following experiments. compare model lstm-rnn bilstm-rnn lstm directions generate sentence representations respectively. lstm-rnn views matching problems matching last position bi-lstm-rnn leverage matchings ﬁrst last position. results table mv-lstms beat consistently. results indicate best matching position always ﬁrst last one. therefore consideration multiple positional sentence representations necessary. conduct case study show detailed analysis. considering positive pair table interaction model pools happened position respectively. corresponding words positions memory memory. means matching sentences best modeled attending words. clearly best matching position case last implicitly assumed lstm-rnn. matching positions number stands interaction produced similarity function. model focuses keyword correctly matching largely inﬂuenced positional representations keywords. addition also observe interactions stick memory play important role ﬁnal matching. therefore model capture important n-gram matching patterns involving rich context represent local information. performance comparison compare model baselines task since three different interaction functions model three versions denoted mv-lstm-cosine mv-lstm-bilinear mv-lstm-tensor respectively. experimental results listed table results several experimental ﬁndings. firstly deep models outperform mainly deep models learn better representations deal mismatch problem effectively. secondly comparing model single sentence representation deep models arc-i could learn russian internet free? good websites? sure free unforgettable languages however give basic vocabulary great system remembering yahoo home page whole list sites offering game free visit www.iwin.com free download. cntn lstm-rnn three models better them. speciﬁcally mv-lstm-tensor obtains relative improvement lstm-rnn mainly multiple positional sentence representations capture detailed local information them. thirdly comparing model multiple granularity models deepmatch arcii multigrancnn model also outperforms them. speciﬁcally mv-lstm-tensor obtains relative improvement multigrancnn reason lies local information obtained representing whole sentence therefore rich context information leveraged determine importance different local information. finally among three models mv-lstmtensor performs best. mainly tensor layer capture complicated interactions consistent observation cntn outperforms arc-i signiﬁcantly. give example data illustrated table show model outperforms best model considers multiple granularity i.e. multigrancnn. experiments show multigrancnn largely inﬂuenced word level matching free free thus wrong answer. frees different meanings ﬁrst focusing free language resources second talking free games. therefore word/phrase level matching requires consider whole context. model tackle problem considering multiple positional sentence representations. speciﬁcally positional interactions large matching small matching consistent intuitive understanding matching. sentence completion section show experimental results sentence completion tries match ﬁrst second clauses sentence. using exactly dataset constructed reuters speciﬁcally sentences balanced clauses divided comma extracted original reuters dataset clauses form positive matching pair. negative examples ﬁrst clause kept second clauses sampled clauses similar cosine similarity. positive example negative examples experimental results listed table considering using data baseline results directly cited acr-i arcii deepmatch. since used evaluation paper results baselines missing table results deep models gain larger improvements compared mismatch problem serious dataset usually ﬁrst clause keyword second one. results mainly consistent mvlstm still performs better baseline methods significantly relative improvement strongest baseline. paper propose novel deep architecture matching sentences multiple positional sentence representations namely mv-lstm. advantage model lies capture local information leverage rich context information determine importance local keywords whole sentence view. experimental results case studies show valuable insights assumption ﬁnal matching solely determined interaction mv-lstm achieve better results single sentence representation methods including lstm-rnn. means best matching position always last therefore consideration multiple positional sentence representations necessary. allow aggregation multiple interactions mv-lstm achieve even better results. means matching degree usually determined combination matchings different positions. therefore much effective considering multiple sentence representations. model also better multiple granularity methods deepmatch multigrancnn. means consideration multi-granularity need rely rich context whole sentence. work funded program china grants program china grant national natural science foundation china under grants research program chinese academy sciences grant kgzd-ew-t- youth innovation promotion association grant also would like thank prof. chengxiang zhai constructive comments.", "year": 2015}