{"title": "Don't Decay the Learning Rate, Increase the Batch Size", "tag": ["cs.LG", "cs.CV", "cs.DC", "stat.ML"], "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\\epsilon$ and scaling the batch size $B \\propto \\epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under 30 minutes.", "text": "common practice decay learning rate. show usually obtain learning curve training test sets instead increasing batch size training. procedure successful stochastic gradient descent momentum nesterov momentum adam. reaches equivalent test accuracies number training epochs fewer parameter updates leading greater parallelism shorter training times. reduce number parameter updates increasing learning rate scaling batch size finally increase momentum coefﬁcient scale although tends slightly reduce test accuracy. crucially techniques allow repurpose existing training schedules large batch training hyper-parameter tuning. train resnet- imagenet validation accuracy minutes. stochastic gradient descent remains dominant optimization algorithm deep learning. however ﬁnds minima generalize well parameter update takes small step towards objective. increasing interest focused large batch training attempt increase step size reduce number parameter updates required train model. large batches parallelized across many machines reducing training time. unfortunately increase batch size test accuracy often falls understand surprising observation smith argued interpret integrating stochastic differential equation. showed scale random ﬂuctuations dynamics introduces optimal batch size proportional learning rate goyal already observed scaling rule empirically exploited train resnet- imagenet validation accuracy hour. show decays learning rate simultaneously decays scale random ﬂuctuations dynamics. decaying learning rate simulated annealing. propose alternative procedure; instead decaying learning rate increase batch size training. strategy achieves near-identical model performance test number training epochs signiﬁcantly fewer parameter updates. proposal require ﬁne-tuning follow pre-existing training schedules; learning rate drops factor instead increase batch size shown previously reduce number parameter updates increasing learning rate scaling also increase momentum coefﬁcient scale although slightly reduces test accuracy. train inceptionresnet-v imagenet parameter updates using batches images reach validation accuracy also replicate setup goyal train resnet- imagenet accuracy minutes. ∗both authors contributed equally. work performed members google brain residency program. note number recent works discussed increasing batch size training knowledge paper shown empirically increasing batch size decaying learning rate quantitatively equivalent. contribution work demonstrate decaying learning rate schedules directly converted increasing batch size schedules vice versa; providing straightforward pathway towards large batch training. section discuss convergence criteria strongly convex minima section interpret decaying learning rates simulated annealing section discuss difﬁculties training large momentum coefﬁcients. finally section present conclusive experimental evidence empirical beneﬁts decaying learning rates deep learning obtained instead increasing batch size training. exploit observation tricks achieve efﬁcient large batch training cifar- imagenet. computationally-efﬁcient alternative full-batch training introduces noise gradient obstruct optimization. often stated reach minimum strongly convex function decay learning rate denotes learning rate gradient update. intuitively equation ensures reach minimum matter away parameters initialized equation ensures learning rate decays sufﬁciently quickly converge minimum rather bouncing around gradient noise however although equations appear imply learning rate must decay training equation holds batch size constant. consider proceed batch size vary follow recent work smith interpret integrating stochastic differential equation below represents cost function represents parameters evolve continuous time towards ﬁnal values. meanwhile represents gaussian random noise models consequences estimating gradient mini-batch. showed mean variance describes covariances gradient ﬂuctuations different parameters. also proved learning rate training size batch noise scale approximate \u0001n/b. decay learning rate noise scale falls enabling converge minimum cost function however achieve reduction noise scale constant learning rate increasing batch size. main contribution work show possible make efﬁcient vast training batches increases batch size training constant learning rate point revert decaying learning rates. surprise many researchers increasingly accepted small batch training often generalizes better test large batch training. generalization explored extensively keskar smith observed optimal batch size bopt maximized test accuracy constant learning rate. argued optimal batch size arises noise scale \u0001n/b also optimal supported claim demonstrating empirically bopt earlier goyal exploited linear scaling rule batch size learning rate train resnet- imagenet hour batches images. results indicate gradient noise beneﬁcial especially non-convex optimization. proposed noise helps escape sharp minima generalize poorly given results unclear present authors whether equations relevant deep learning. supporting view note researchers employ early stopping whereby intentionally prevent network reaching minimum. nonetheless decaying learning rates empirically successful. understand this note introducing random ﬂuctuations whose scale falls training also well established technique non-convex optimization; simulated annealing. initial noisy optimization phase allows explore larger fraction parameter space without becoming trapped local minima. located promising region parameter space reduce noise ﬁne-tune parameters. finally note interpretation explain conventional learning rate decay schedules like square roots exponential decay become less popular deep learning recent years. increasingly researchers favor sharper decay schedules like cosine decay step-function drops interpret shift note well known physical sciences slowly annealing temperature helps system converge global minimum sharp. meanwhile annealing temperature series discrete steps trap system robust minimum whose cost higher whose curvature lower. suspect similar intuition hold deep learning. reduces noise scale vanilla momentum coefﬁcient intuitively effective learning rate. proposed reduce number parameter updates required train model increasing learning rate momentum coefﬁcient simultaneously scaling increasing learning rate scaling performs well. however increasing momentum coefﬁcient scaling slightly reduces test accuracy. analyze observation consider momentum update equations accumulation mean gradient training example estimated batch size appendix analyze growth accumulation start training. variable tracks exponentially decaying average gradient estimates initially initialized zero. accumulation grows exponentially towards steady state value timescale approximately training epochs. time magnitude parameter updates suppressed reducing rate convergence. consequently training high momentum must introduce additional epochs allow dynamics catch furthermore increase momentum coefﬁcient increase timescale required accumulation forget gradients timescale becomes several epochs long accumulation cannot adapt changes loss landscape impeding training. likely particularly problematic points noise scale decays. kingma proposed initialization bias correction whereby learning rate increased early times compensate suppressed initial value accumulation. however batch size large found often causes instabilities early stages training. note goyal recommended reduced learning rate ﬁrst epochs. section demonstrate decreasing learning rate increasing batch size training equivalent. section show reduce number parameter updates increasing effective learning rate scaling batch size. section apply insights train inception-resnet-v imagenet using vast batches images. finally section train resnet- imagenet validation accuracy within minutes. ﬁrst experiments performed cifar- using wide resnet architecture following implementation zagoruyko komodakis ghost batch norm ghost batch size ensures mean gradient independent batch size required analysis smith demonstrate equivalence decreasing learning rate increasing batch size consider three different training schedules shown ﬁgure decaying learning rate follows original implementation; batch size constant learning rate repeatedly decays factor sequence steps. hybrid holds learning rate constant ﬁrst step instead increasing batch size factor however ﬁrst step batch size constant learning rate decays factor subsequent step. schedule mimics might proceed hardware imposes limit maximum achievable batch size. increasing batch size hold learning rate constant throughout training increase batch size factor every step. figure wide resnet cifar. training cross-entropy evaluated function number training epochs number parameter updates three learning curves identical increasing batch size reduces number parameter updates required. learning rate must decay training schedules show different learning curves reach different ﬁnal test accuracies. meanwhile noise scale decay three schedules indistinguishable. plot evolution training cross entropy ﬁgure train using momentum momentum parameter three training curves almost identical despite showing marked drops pass ﬁrst steps results suggest noise scale relevant learning rate. emphasize potential beneﬁts increasing batch size replot training cross-entropy ﬁgure function number parameter updates rather number epochs. three schedules match ﬁrst step point increasing batch size dramatically reduces number parameter updates required train model. finally conﬁrm alternative learning schedules generalize equally well test ﬁgure exhibit test accuracy function number epochs again three schedules almost identical. conclude achieve beneﬁts decaying learning rate experiments instead increasing batch size. present additional results establish proposal holds range optimizers using schedules presented ﬁgure ﬁgure present test accuracy training nesterov momentum momentum parameter observing three nearidentical curves. ﬁgure repeat experiment vanilla obtaining three highly similar curves finally ﬁgure repeat experiment adam figure wide resnet cifar. test accuracy function number parameter updates. increasing batch size replaces learning rate decay batch size increases. increased initial learning rate additionally increases initial learning rate finally increased momentum coefﬁcient also increases momentum coefﬁcient default parameter settings tensorflow initial base learning rate thus learning rate schedule obtained dividing ﬁgure remarkably even three curves closely track other. focus secondary objective; minimizing number parameter updates required train model. shown above ﬁrst step replace decaying learning rates increasing batch sizes. show also increase effective learning rate start training scaling initial batch size experiments conducted using momentum. images cifar- training since scaling rules hold decided maximum batch size bmax consider four training schedules decay noise scale factor series three steps. original training schedule follows implementation zagoruyko komodakis using initial learning rate decays factor step momentum coefﬁcient batch size increasing batch size also uses learning rate initial batch size momentum coefﬁcient batch size increases factor step. schedules identical decaying learning rate increasing batch size section above. increased initial learning rate also uses increasing batch sizes training additionally uses initial learning rate initial batch size finally increased momentum coefﬁcient combines increasing batch sizes training increased initial learning rate increased momentum coefﬁcient initial batch size note increase batch size reaches bmax point achieve subsequent decays noise scale decreasing learning rate. emphasize that previous section four schedules require number training epochs. plot evolution test accuracy ﬁgure function number parameter updates. implementation original training schedule requires updates reaches ﬁnal test accuracy increasing batch size requires updates reaching ﬁnal accuracy increased initial learning rate requires updates reaching ﬁnal accuracy finally increased momentum coefﬁcient requires less parameter updates reaches lower test accuracy across additional training runs schedule median accuracies respectively. discussed potential explanation performance drop training large momentum coefﬁcients section provide additional results appendix varying initial learning rate holding batch size constant. test accuracy falls initial learning rates larger figure inception-resnet-v imagenet. increasing batch size training achieves similar results decaying learning rate reduces number parameter updates experiment twice illustrate variance. apply insights reduce number parameter updates required train imagenet. goyal trained resnet- imagenet hour reaching validation accuracy. achieve this used batches initial learning rate momentum coefﬁcient completed training epochs decaying learning rate factor epoch. imagenet contains around million images corresponds parameter updates. also introduced warm-up phase start training learning rate batch size gradually increased. also train epochs follow schedule decaying noise scale factor epoch. however include warm-up phase. stronger baseline replaced resnet- inception-resnet-v initially used ghost batch size ﬁgure train learning rate momentum coefﬁcient initial batch size decaying learning rate hold batch size ﬁxed decay learning rate increasing batch size increase batch size ﬁrst step decay learning rate following steps. repeat schedule twice four runs exhibit similar evolution test accuracy training. ﬁnal accuracies decaying learning rate runs ﬁnal accuracy increasing batch size runs although slight drop difference ﬁnal test accuracies similar variance training runs. increasing batch size reduces number parameter updates training note training curves appear unusually noisy reduced number test evaluations reduce model training time. goyal already increased learning rate close maximum stable value. reduce number parameter updates must increase momentum coefﬁcient. introduce maximum batch size bmax ensures also improved stability distributed training. also increased ghost batch size matching batch size gpus reducing training time. compare three different schedules base schedule decaying noise scale factor epoch. initial learning rate throughout. momentum uses initial batch size momentum uses initial batch size momentum uses initial batch size schedules decay noise scale increasing batch size reaching bmax decay learning rate. plot test accuracy ﬁgure momentum achieves ﬁnal accuracy updates. performed runs momentum achieving ﬁnal accuracies updates. finally momentum achieves ﬁnal accuracies updates. conﬁrm increasing batch size training reduce model training times replicated set-up described goyal half comprising tensorcores using tensorflow ﬁrst train resnet- epochs validation accuracy minutes utilising batches images. utilise full increase batch size ﬁrst epochs images achieve validation accuracy minutes. last epochs ﬁrst epochs take minutes demonstrating near-perfect scaling efﬁciency across number parameter updates provides meaningful measure training time. knowledge ﬁrst procedure reduced training time goyal without sacriﬁcing ﬁnal validation accuracy contrast doubling initial learning rate using batches images throughout training achieves lower validation accuracy minutes demonstrating increasing batch size training crucial performance gains above. results show ideas presented paper become increasingly important hardware large-batch training becomes available. paper extends analysis smith include decaying learning rates. mandt also interpreted stochastic differential equation order discuss could modiﬁed perform approximate bayesian posterior sampling. however state analysis holds neighborhood minimum keskar showed beneﬁcial effects noise pronounced start training. proposed control theory learning rate momentum coefﬁcient. goyal observed linear scaling rule batch size learning rate used rule reduce time required train resnet- imagenet hour. knowledge scaling rule adopted krizhevsky bottou demonstrated converges strongly convex minima similar numbers training proposed layer-wise adaptive rate scaling applies different learning rates different parameters network used train imagenet minutes albeit lower ﬁnal accuracy k-fac also gaining popularity efﬁcient alternative sgd. wilson argued adaptive optimization methods tend generalize less well momentum work reduces convergence speed. asynchronous-sgd another popular strategy enables multiple gpus even batch sizes small consider asynchronous-sgd work since scaling rules enabled batch sizes order training size. often achieve beneﬁts decaying learning rate instead increasing batch size training. support claim experiments cifar- imagenet range optimizers including momentum adam. ﬁndings enable efﬁcient vast batch sizes signiﬁcantly reducing number parameter updates required train model. potential dramatically reduce model training times. increase batch size increasing learning rate momentum parameter scaling combining strategies train inception-resnet-v imagenet validation accuracy parameter updates using batches images. also exploit increasing batch sizes train resnet- imagenet validation accuracy minutes. strikingly achieve without hyper-parameter tuning since scaling rules enable directly convert existing hyper-parameter choices literature large batch training. thank prajit ramachandran gabriel bender matthew johnson martin abadi helpful discussions. also thank vijay vasudevan brennan saeta jonathan hseu bjarke roune rest team technical support. jeffrey dean greg corrado rajat monga chen matthieu devin mark andrew senior paul tucker yang quoc large scale distributed deep networks. advances neural information processing systems priya goyal piotr doll´ar ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. norman jouppi cliff young nishant patil david patterson gaurav agrawal raminder bajwa sarah bates suresh bhatia boden borchers in-datacenter performance analysis tensor processing unit. proceedings annual international symposium computer architecture nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. benjamin recht christopher stephen wright feng niu. hogwild lock-free approach parallelizing stochastic gradient descent. advances neural information processing systems accumulation variable mean gradient training example estimated batch size initialize accumulation zero takes number updates magnitude accumulation grow time size parameter updates suppressed reducing effective learning rate. model growth accumulation assuming gradient start training approximately constant consequently accumulation integrates underlying differential equation nepochs denotes number training epochs performed. accumulation variable grows exponentially consequently estimate effective number lost training epochs since batch size nlost \u0001/). must either introduce additional training epochs compensate ensure number lost training epochs negligible compared total number training epochs performed decaying noise scale. note nlost rises rapidly increases momentum coefﬁcient. exhibit test accuracy wide resnet implementation cifar ﬁgure function initial learning rate. learning rate batch size constant throughout training. matches original training schedule section main text. increase learning rate scale perform number training epochs.", "year": 2017}