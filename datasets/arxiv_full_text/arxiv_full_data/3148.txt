{"title": "Solving OSCAR regularization problems by proximal splitting algorithms", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The OSCAR (octagonal selection and clustering algorithm for regression) regularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for its grouping behavior) and was proposed to encourage group sparsity in scenarios where the groups are a priori unknown. The OSCAR regularizer has a non-trivial proximity operator, which limits its applicability. We reformulate this regularizer as a weighted sorted L_1 norm, and propose its grouping proximity operator (GPO) and approximate proximity operator (APO), thus making state-of-the-art proximal splitting algorithms (PSAs) available to solve inverse problems with OSCAR regularization. The GPO is in fact the APO followed by additional grouping and averaging operations, which are costly in time and storage, explaining the reason why algorithms with APO are much faster than that with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an exact proximity operator. Although convergence of PSAs with APO is may not be guaranteed, we have experimentally found that APO behaves similarly to GPO when the regularization parameter of the pair-wise L_inf norm is set to an appropriately small value. Experiments on recovery of group-sparse signals (with unknown groups) show that PSAs with APO are very fast and accurate.", "text": "oscar regularizer consists norm plus pair-wise norm proposed encourage group sparsity scenarios groups priori unknown. oscar regularizer nontrivial proximity operator limits applicability. reformulate regularizer weighted sorted norm propose grouping proximity operator approximate proximity operator thus making state-of-the-art proximal splitting algorithms available solve inverse problems oscar regularization. fact followed additional grouping averaging operations costly time storage explaining reason algorithms much faster gpo. convergences psas guaranteed since exact proximity operator. although convergence psas guaranteed experimentally found behaves similarly regularization parameter pair-wise norm appropriately small value. experiments recovery group-sparse signals show psas fast accurate. keywords proximal splitting algorithms alternating direction method multipliers iterative thresholding group sparsity proximity operator signal recovery split bregman past decades linear inverse problems attracted attention wide range areas statistics machine learning signal processing compressive sensing name few. typical forward model cases interest invertible making ill-posed problem addressed using form regularization prior knowledge unknown classical regularization formulations seek solutions problems form type prior knowledge focus much recent attention sparsity i.e. large fraction components zero ideal regularizer encouraging solutions smallest possible number nonarguably popular sparsity-encouraging regularizer. norm seen tightest convex approximation norm conditions object study yields solution. many variants regularizers proposed recent years much attention paid sparsity solutions also structure sparsity relevant problems provides another avenue inserting prior knowledge problem. particular considerable interest attracted group sparsity block sparsity general structured sparsity classic model group sparsity group lasso regularizer so-called norm sparse glasso regularizer proposed rsglasso λrlasso λrglasso non-negative parameters comparison glasso sglasso selects groups also individual variables within group. note costs possible advantages glasso sglasso standard lasso need deﬁne priori structure groups. problems components known similar value neighbors encourage type solution several proposals appeared elastic fused lasso grouping pursuit octagonal shrinkage clustering algorithm regression elastic regularizer relast-net encouraging sparsity grouping flasso regularizer xi+| total variation given rflasso i-th j-th predictors non-negative weight. finally recent graph-guided flasso regularizer based graph variable nodes edges rggflasso ∈ei<j wij|xi sign represents weight edge rggflasso reduces rflasso former group variables diﬀerent signs assignment latter cannot. graphguided group-sparsity-inducing regularizers proposed kind regularizers needs strong requirement prior information undirected graph. sake comparison several mentioned regularizers illustrated figure corresponding level curves depicted; also plot level curve classical ridge regularizer rridge roscar relast-net rflasso promote sparsity grouping grouping behaviors clearly diﬀerent oscar encourages equality pair variables discussed detail section elastic strictly convex doesn’t promote strict equality like oscar; total variation term flasso seen encourage sparsity diﬀerences pair successive variables thus recipe grouping guide variables seen above flasso elastic oscar potential used regularizers known components unknown signal exhibit structured sparsity group structure priori known. however pointed oscar outperforms models feature grouping. moreover flasso suitable grouping according magnitude since cannot group positive negative variables together even magnitudes similar; flasso also relies particular ordering variables always natural choice. consequently focus oscar regularizer paper. costly quadratic programming approach adopted solve optimization problem corresponding oscar. recently solved oscar generalized linear model context; algorithm therein proposed solves complicated constrained maximum likelihood problem iteration also costly. eﬃcient algorithm proposed reformulating oscar quadratic problem applying fista best knowledge currently fastest algorithm oscar. paper propose reformulating roscar weighted sorted norm present exact grouping proximity operator roscar based projection step proposed element-wise approximate proximal step show consists additional grouping averaging operation. furthermore alternative state-ofthe-art proximal splitting algorithms solve problems involved oscar regularization. past decade several special purpose algorithms proposed solve optimization problems form context linear inverse problems sparsity-inducing regularization. example homotopy methods lars stomp deal problems bound-constrained optimization. arguably standard socalled iterative shrinkage/thresholding algorithm forward-backward splitting fact tends slow particular matrix poorly conditioned stimulated much research aiming obtaining faster variants. two-step fast algorithm iterate depends previous ones rather previous twist fista shown considerably faster standard ist. another strategy obtain faster variants consists using aggressive step sizes; case sparsa also shown clearly split augmented lagrangian shrinkage algorithm addresses unconstrained optimization problems based variable splitting idea transform unconstrained problem constrained variable splitting tackle constrained problem using alternating direction method multipliers recently preconditioned version admm primal-dual scheme proposed admm close relationship bregman split-bregman methods recently applied imaging inverse problems state-of-the-art methods mentioned fista twist sparsa admm padmm considered paper context numerical experiments. contributions paper two-fold reformulate oscar regularizer weighted sorted norm propose gpo; main contribution makes solving corresponding optimization problems convenient; study performance state-of-the-art algorithms fista twist sparsa admm padmm solving problems oscar regularization. rest paper organized follows. section describes oscar apo. limitation space section details three algorithms fista sparsa padmm. section reports experimental results section concludes paper. sparsity grouping. parameters nonnegative constants controlling relative weights terms. becomes lasso roscar behaves like pairwise regularizer. note that choice roscar convex ball octagonal vertices octagon divided categories four sparsity-inducing vertices four vertices equality-inducing. figure depicts data-ﬁdelity term roscar illustrating possible eﬀects. discussed section compared glasso oscar doesn’t require pre-speciﬁcation group structure; compared flasso doesn’t depend certain ordering variables; compared elastic equality-inducing capability. features make oscar convenient regularizer many applications. worth noting representation unique sequence values elements strictly decreasing. notice also choice always correct regardless existing repeated absolute values. described previous subsection exact proximity operator roscar subsection propose approximate version obtained skipping function groupandaverage vein roscar illustrated figure pseudo code computing termed oscar follows. also viewed group-sparsity-promoting variant soft thresholding obtained removing exact computation proximity operator oscar regularizer. alteration longer guaranteed satisﬁed reason yield result gpo. however next show able obtain results often good produced although simpler faster. simple examples contain random samples uniformly distributed magnitude-sorted version i.e. |˜zi| |˜zi+| figure plot |˜z| |˜z| diﬀerent values plots show that naturally condition shown figure that smaller value result obtained similar obtained fewer zeros obtained shrinkage. figure conclude output respects decreasing value exhibits grouping values results obtained input signals shown figure results ﬁgure show that increases becomes clearly faster gpo. figure shows increases results obtained approach gpo. function) smooth lipschitz continuous gradient constant possibly nonsmooth. oscar special case roscar. paper assume solvable i.e. minimizers emply minx solve investigate state-of-the-art psas fista twist sparsa admm padmm algorithms apply gpo. worth recalling give exact solution cannot algorithms exact ones inexact ones. space limitation next detail fista sparsa padmm applied oscar; detailes algorithms obtained corresponding publications. experiments conclude sparsa fastest algorithm padmm yields best solutions. fista fast version iterative shrinkage-thresholding algorithm based nesterov’s acceleration scheme fista algorithmic framework oscar follows. algorithm function proxstep either deﬁned above; notation also adopted algorithms described below. fista oscar termed fista-apo oscar termed fista-gpo padmm preconditioned version admm also eﬃcient ﬁrstorder primal-dual algorithm convex optimization problems known saddle-point structure. oscar using padmm algorithm shown following turn convergence algorithms apo. since exact proximity operator convergences algorithms guaranteed convergence results. however approximate thus convergence algorithms mathematically clear leave open problem here spite practically found behaves similarly regularization parameter small enough value. report results experiments group-sparse signal recovery aiming showing diﬀerences among aforementioned psas apo.. experiments performed using matlab -bit windows intel core processor ram. order measure performance diﬀerent algorithms employ following four metrics deﬁned estimate original vector disturbance group somehow reﬂect real applications possesses positive negative groups. sensing matrix matrix whose components sampled standard normal represents estimate k-th iteration. oscar model original recovered signals shown figure results time iterations shown table figure show evolution objective function time respectively. conclude figures table psas much faster gpo. among psas sparsa-apo fastest time fista-apo uses fewer iterations. results obtained padmm accurate ones. algorithms. signal length matrix size keep setup subsection unchanged. results shown figure horizontal axis represents signal size. figure conclude that along increasing signal size speed psas much faster former suitable solving large-scale problems. proposed approaches eﬃciently solve problems involving oscar regularizer outperforms group-sparsity-inducing ones. exact approximate versions proximity operator oscar regularizer considered diﬀerences analyzed mathematically numerically. naturally approximate faster exact certain range parameters regularizer results similar. proximity operators provide useful building bock applications proximal spitting algorithms. considered state-of-the-art algorithms fista twist sparsa admm padmm. experiments group-sparse signal recovery shown algorithms working approximate proximity operator able obtain accurate results fast. however mathematical convergence proof left open problem whereas algorithms operating exact proximity operator inherit corresponding convergence results.", "year": 2013}