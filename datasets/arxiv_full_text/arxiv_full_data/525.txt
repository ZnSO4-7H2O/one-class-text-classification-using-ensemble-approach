{"title": "Character-Level Incremental Speech Recognition with Recurrent Neural  Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90% on the Wall Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284 training set.", "text": "real-time speech recognition applications latency important issue. developed character-level incremental speech recognition system responds quickly even speech hypotheses gradually improved speaking proceeds. algorithm employs speech-to-character unidirectional recurrent neural network end-to-end trained connectionist temporal classiﬁcation rnn-based character-level language model output values ctc-trained character-level probabilities processed beam search decoding. augments decoding providing long-term dependency information. propose tree-based online beam search additional depth-pruning enables system process inﬁnitely long input speech latency. system responds quickly speech also dictate out-of-vocabulary words according pronunciation. proposed model achieves word error rate wall street journal nov’ evaluation trained training set. incremental speech recognition allows speech-based interaction system react quickly utterance spoken. unlike ofﬂine sentence-wise automatic speech recognition decoding result available user ﬁnishes speaking returns n-best decoding results small latency speech. n-best results hypotheses gradually improve system receives speech data. since usually employed immediate reaction speech word stability incremental lattice generation important topics. paper introduce end-to-end character-level system unidirectional recurrent neural networks acoustic roughly dictates input speech rnnbased language model employed augment dictation result decoding. compared conventional word-level backend speech recognition system character-level capable dictating vocabulary words based pronunciaefforts deal words conventional based systems. graphemes employed basic units instead phonemes. also sub-lexical language model proposed detecting previously unseen words. rnn-based character-level end-to-end systems studied however lack capability dictating words since decoding performed word-level lms. recently lexicon-free end-to-end system introduced character-level employed. improve approach employing preﬁx tree based online beam search additional depth-pruning isr. character-level system proposed paper composed acoustic acoustic end-to-end trained connectionist temporal classiﬁcation using wall street journal speech corpus output acoustic probability characters decoded character-level beam search generate n-best hypotheses. improve performance character-level employed augment beam search performance. also propose depth-pruning efﬁcient tree-based beam search. separately trained large text corpus also included corpus. unlike word-level language modeling conventional statistical n-gram back-off models cannot used because much longer history window required character-level prediction. acoustic deep unidirectional long short-term memory network structures continuous inﬁnitely long input speech trained virtually inﬁnite training data streams generated randomly concatenating training sequences. proposed model evaluated single test sequence generated concatenating test utterances eval without external reset states utterance boundaries. performance examined varying beam width depth. generally wider beam increases accuracy. beam width trade-off between accuracy stability balance between adjusted beam depth. input -dimensional vector current label one-hot encoded. output also -dimensional vector represents probabilities next labels. although trained predict next characters given current character past character histories internally stored inside used prediction. well known remember contexts long time steps. acoustic trained long text stream generated attaching randomly picked sentences inserting labels sentences. trained adadelta based method accelerated training better annealing. training text nonverbalized punctuation contains characters used training randomly selected corpus reserved evaluation ﬁnal bits-per-character random sentences generated following method described brieﬂy next label randomly picked following probabilities current output back next step. iterating steps texts sequentially generated shown figure example clear learned linguistic structures well spellings words frequently appear. network total trainable parameters. model similar previous work end-to-end speech recognition rnns except major differences. case trained online long training sequences generated randomly concatenating several utterances. need reset states utterance boundary. necessary systems runs continuously inﬁnite input audio stream. also model unidirectional structure since bidirectional networks usually employed endto-end speech recognition suitable low-latency speech recognition. backward layers bidirectional networks cannot computed input utterance ﬁnished. input network -dimensional mel-frequency ﬁlterbank feature vector energy delta double-delta values resulting -dimensional vector. feature vectors extracted every hamming window. input vectors element-wisely standardized based statistics obtained training set. output -dimensional vector consists probabilities upper case alphabets special characters end-of-sentence symbol blank label. networks trained stochastic gradient descent parallel input streams networks unrolled times weight updates performed every forward steps. network performances evaluated every training frames. evaluation performed total frames development set. learning rate starts reduced factor whenever development improved consecutive evaluations. training ends learning rate drops trained networks training sets. ﬁrst standard second si-all speaker independent training utterances corpus. note utterances verbalized punctuations removed training sets. also transcriptions ﬁltered makes ﬁnal si-all sets contain roughly hours speech respectively. eval sets used development evaluation respectively. language model employed proposed system since conventional statistical n-gram backmodels suitable character-level prediction since cannot make long history windows. speciﬁcally fig. proposed online decoding evaluation respect beam depth. experiments conducted acoustic rnns trained si-all beam search performed beam width weight insertion bonus. modiﬁcation applied adding additional terms probability destination state state transition different label nodes occurs. probability next label computed using active label node added beam search tree. this context copied parent node child node processes label child node copied context. therefore active node context. pruning search tree performed standard beam search approach. frame active nodes hypotheses ancestor nodes remain alive pruning beam width however standard pruning width-pruning cannot prevent tree growing indeﬁnitely especially input speech long. gradually degrades efﬁciency beam search recent nodes since hypotheses would wasted maintain part lattice already context range lms. remedy issue propose additional pruning method called depth-pruning. procedure follows. first m-th ancestor node best hypothesis beam depth. then ancestor node becomes root node. pruning performed removing nodes descendants root node. beam better utilized recent hypotheses rather older ones. figure shows example depth-pruning beam depth note depth nodes larger beam depth. following experiments depth-pruning performed every frames. fig. example depth-pruning beam depth pruning performed selecting root node depth best hypothesis node becomes beam depth. shaded nodes indicate original active nodes. also path best hypothesis drawn thick strokes. sequence less equal number input frames. objective beam search decoding label sequence maximum posterior probability given input features time generated acoustic rnns however ctc-trained output blank label. labels additional blank label path sequence labels time length path deﬁnition every reduced corresponding example aab-c–a corresponds abca blank label. beam search ﬁrst represent lattice treebased structure node labels depicted figure then backtracking node generates unique label sequence deal state transitions need statebased network represented states shown figure easily done expanding tree node label states corresponding label followed blank label. since label-level search network based tree structure different statelevel paths different label sequences never meet other. simpliﬁes problem since interaction different sequence labelings equation concern. he’s he’s he’s could he’s could show ...in plaza ...in plaza rock ...in plaza draw rate seve ...in plaza draw rate seventy five ...in plaza draw crowd seventy five thousand ...in plaza draw crowd seventy five thousand people ...in plaza draw crowd seventy five thousand people says ...in plaza draw crowd seventy five thousand people says latin diplom ...in plaza draw crowd seventy five thousand people says latin diplomat ground truth he’s could show plaza draw table comparison wers end-to-end speech recognizers literature. reference wers phoneme based gmm/dnn-hmm systems also reported. systems trained evaluated eval. roughly wer. however little difference beam width increases preliminary experiments. best performing beam depths si-all systems respectively. means si-all system recognize speech immediately system. consider acoustic model si-all system embed stronger language model increased training data make decision precisely without relying external language model much. character error rate reported table optimal beam depths. comparison also report sentence-wise ofﬂine decoding results without depth-pruning. proposed system compared end-to-end word-level speech recognition systems table systems perform sentence-wise ofﬂine decoding bidirectional rnns. best result achieved miao ctc-trained deep bidirectional lstm network retrained trigram extended vocabulary. systems original trigram model provided corpus perform worse system character-level hand system beaten ones extended trigram models. however precise comparison decoding stages done employing model. figure shows incremental speech recognition result proposed system. best hypothesis reported every frames shown past best result corrected making additional speech input. example word rock changed draw frame listening word rate. moreover correction draw rate draw crowd hearing word people frame good evidence long term context also considered. character-level incremental speech recognizer proposed analyzed throughout paper. proposed system combines ctctrained character-level tree-based beam search decoding. online decoding long input speech depth-pruning proposed prevent indeﬁnite growth search tree. proposed model trained achieved long speech formed concatenating utterances eval evaluation set. incremental recognition result shows evidence character-level learn dependencies words even words apart hard caught using conventional n-gram back-off language models. note proposed system requires speech text corpus training. external lexicon senone modeling needed training huge advantage. moreover expected words infrequent words names places people dictated pronounced. alex graves navdeep jaitly abdel-rahman mohamed hybrid speech recognition deep bidirectional lstm automatic speech recognition understanding ieee workshop ieee kyuyeon hwang wonyong sung single stream parallelization generalized lstm-like rnns acoustics speech signal processing ieee international conference ieee tom´aˇs mikolov stefan kombrink luk´aˇs burget honza cernock`y sanjeev khudanpur extensions recurrent neural network language model acoustics speech signal processing ieee international conference ieee phillip woodland julian odell valtcho valtchev steve young large vocabulary continuous speech recognition using acoustics speech signal processing icassp-. ieee international conference ieee vol. ii–. ethan selfridge iker arizmendi peter heeman jason williams stability accuracy incremental speech recognition proceedings sigdial conference. association computational linguistics alex graves navdeep jaitly towards end-to-end speech recognition recurrent neural networks proceedings international conference machine learning awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates deepspeech scaling endto-end speech recognition arxiv preprint arxiv. awni hannun andrew maas daniel jurafsky andrew first-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns arxiv preprint arxiv. dzmitry bahdanau chorowski dmitriy serdyuk philemon brakel yoshua bengio end-to-end attentionbased large vocabulary speech recognition arxiv preprint arxiv. andrew maas ziang jurafsky andrew lexicon-free conversational speech recognition neural networks naacl conference north american chapter association computational linguistics human language technologies denver colorado june alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning. douglas paul janet baker design wall street journal-based corpus proceedings workshop speech natural language. association computational linguistics", "year": 2016}