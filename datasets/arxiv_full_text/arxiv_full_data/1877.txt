{"title": "Semi-Amortized Variational Autoencoders", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.", "text": "stochastic variational inference variational parameters data point randomly initialized optimized maximize evidence lower bound with example gradient ascent. updates based subset data making possible scale approach. amortized variational inference local variational parameters instead predicted inference network shared across dataset. variational autoencoders deep generative models utilize inference jointly train generative model alongside inference network. gives good local distributions within variational family requires performing optimization data point. fast inference having variational parameters parametric function input strict restriction. secondary affect militate learning good generative model since parameters updated based suboptimal variational parameters. cremer observe amortization signiﬁcant vaes especially complex datasets. recent work targeted amortization combining amortized inference iterative reﬁnement training methods encoder initialize local variational parameters subsequently iterative procedure reﬁne them. train hybrid approach utilize separate training time objective. example hjelm train inference network minimize kl-divergence initial ﬁnal variational distributions krishnan train inference network usual elbo objective based initial variational distribution. work address train/test objective mismatch consider methods training semi-amortized variational autoencoders fully end-to-end manner. propose approach leverages differentiable optimization differentiates training inference network/generative model. method able improve estimation variational parameters produce better generative models. amortized variational inference replaces instance-speciﬁc local inference global inference network. enabled efﬁcient training deep generative models variational autoencoders recent empirical work suggests inference networks produce suboptimal variational parameters. propose hybrid approach initialize variational parameters stochastic variational inference reﬁne them. crucially local procedure differentiable inference network generative model trained end-to-end gradient-based optimization. semi-amortized approach enables rich generative models without experiencing posterior-collapse phenomenon common training vaes problems like text generation. experiments show approach outperforms strong autoregressive variational baselines standard text image datasets. variational inference framework approximating intractable distribution optimizing family tractable surrogates. traditional algorithms iterate observed data update variational parameters closed-form coordinate ascent updates exploit conditional conjugacy style optimization challenging extend large datasets non-conjugate models. however recent advances stochastic black-box amortized variational inference made possible scale large datasets rich non-conjugate models zhang review modern methods). school engineering applied sciences harvard university cambridge csail imes massachusetts institute technology cambridge usa. correspondence yoon <yoonkimseas.harvard.edu>. apply approach train deep generative models text images observe outperform autoregressive/vae/svi baselines addition direct baselines combine perform end-to-end training. also framework able utilize powerful generative model without experiencing posterior-collapse phenomenon often observed vaes wherein variational posterior collapses prior generative model ignores latent variable problem particularly made difﬁcult utilize vaes text important open issue ﬁeld. sa-vae able outperform lstm language model utilizing lstm generative model maintains non-trivial latent representations. code available https//github.com/harvardnlp/sa-vae. background notation scalar valued function partitioned inputs slight abuse notation deﬁne denote ∇uif rdim i-th block gradient evaluated further denote total derivative respect exists differentiable function note general ∇uif since components could function also huiuj rdim×dim matrix formed taking i-th group rows j-th group columns hessian evaluated deﬁnitions generalize straightforwardly vector-valued function function optimizes directly instance-speciﬁc variational distributions require running iterative inference large number steps. further block coordinate ascent approach variational parameters optimized separately potentially making difﬁcult adapt local optima. uses global parametric model predict local variational parameters data point. particularly popular application training variational autoencoder runs inference network parameterized input obtain variational parameters this indeed case approach calculate elbo function data point generative model inference network total derivatives/jacobians usually denoted vectors denote column vectors clearer notation. coadapt. additionally inference involves running inference network input usually much faster running iterative optimization elbo. despite beneﬁts requiring variational parameters parametric function input strict restriction lead amortization gap. propagate forward hinder learning generative model updated based suboptimal semi-amortized variational autoencoders utilize inference network input give initial variational parameters subsequently reﬁne them. might appeal universal approximation theorem question necessity additional steps given rich-enough inference network. however practice variational parameters found usually optimal even powerful inference network amortization significant especially complex datasets note training need compute total derivative ﬁnal elbo respect unlike order update encoder generative model parameters total derivative requires backpropagating updates. speciﬁcally requires backpropagating gradient ascent following past work backpropagation step done efﬁciently fast hessian-vector products particular consider case perform step reﬁnement elbo brevity elbo. backpropagate this receive derivative backpropagate inference network calculate total derivative i.e. similar rules used derive full forward/backward step uses gradient descent momentum negative elbo shown algorithm implementation calculate hessian-vector products ﬁnite differences found memory efﬁcient automatic differentiation speciﬁcally estimate huiuj since case elbo non-deterministic function sampling care must taken calculating hessian-vector product ﬁnite differences ensure source randomness calculating gradient expressions. table variational upper bounds various models synthetic dataset svi/sa-vae trained/tested steps. true estimate true negative loglikelihood estimated samples prior biased consistent estimator oracle uses oracle generative model learned learns generative network. learned different methods. svi/savae iterative optimization steps. finally also show optimal variational parameters found grid search. seen figure variational parameters running sa-vae closest optimum obtained slightly away. table show variational upper bounds negative log-likelihood training various models oracle/learned generative model sa-vae outperforms vae/svi cases. text next experiments focused text modeling yahoo questions corpus yang text modeling deep generative models challenging problem approaches shown produce rich generative models collapse standard language models. ideally deep generative model trained variational inference would make latent space accurately modeling underlying distribution. architecture hyperparameters identical lstm-vae baselines considered yang except train instead adam found perform better training lstms. speciﬁcally inference network generative model onelayer lstms hidden units -dimensional word embeddings. last hidden state encoder used predict vector variational posterior means/log variances. sample variational posterior used predict initial hidden state generative lstm additionally input time step. latent variable -dimensional. following previous works variational models utilize kl-cost annealing strategy whereby multiplier term increased linearly batch epochs. appendix full architecture/hyperparameters. figure elbo landscape oracle generative model function variational posterior means randomly chosen test point. variational parameters obtained shown µvae µsvi initial/ﬁnal parameters savae shown svi/sa-vae iterations. optimal point found grid search shown experiments apply approach train generative models synthetic dataset addition text/images. experiments utilize stochastic gradient descent momentum negative elbo. prior spherical gaussian variational posterior diagonal gaussian variational parameters given mean vector diagonal variance vector i.e. preliminary experiments also experimented natural gradients optimization algorithms learning learning rates found signiﬁcantly improve results. full details regarding hyperparameters/model architectures experiments appendix results synthetic data ﬁrst apply approach synthetic dataset access true underlying generative model discrete sequences. generate synthetic data according following oracle generative process -dimensional latent variables one-hot indicator vectors initialize lstm/mlp randomly lstm single layer hidden state/input dimension equal generate time steps vocabulary size training consists points. appendix exact setup. oracle generative model learn inference network sa-vae. randomly selected test point plot elbo landscape figure function variational posterior means table results text modeling yahoo dataset. results yang bottom results work nll/kl numbers averaged across examples refers perplexity. refers number inference steps used training/testing. addition autoregressive/vae/svi baselines consider approaches also combine amortized inference iterative reﬁnement. ﬁrst approach krishnan generative model takes gradient step based based ﬁnal variational parameters inference network takes gradient step based based initial variational parameters i.e. update based elbo update based elbo. forward step identical sa-vae. refer baseline svi. second approach based salakhutdinov larochelle hjelm train inference network minimize kl-divergence initial ﬁnal variational distributions keeping latter ﬁxed. letting update based elbo update based ∇νg. note inference network updated based would take account fact functions found perform better reverse direction refer setup figure perplexity upper bound various models trained steps tested varying number steps random initialization. left except initialized variational parameters obtained inference network. however models trained vae/svi make negligible latent variable practically collapse language model negating beneﬁts using latent variables. contrast models combine amortized inference iterative reﬁnement make latent space term signiﬁcantly zero. outperform language model sa-vae modestly outperforms knowledge ﬁrst instances able train lstm generative model ignore latent code outperforms language model. might wonder improvements coming simply ﬂexible inference scheme test time rather learning better generative model. test this various models discard inference network test time perform variable number steps random initialization. results shown figure clear learned generative model quite different—it possible train perform test time obtain performance sa-vae figure results similar experiment reﬁne variational parameters initialized inference network variable number steps test time. inference network provides better initial parameters random initialization thus requires fewer iterations reach optimum. observe improvements running reﬁnement steps used training test time. interestingly sa-vae without reﬁnement steps test time substantially nonzero term indicates posterior-collapse phenomenon training lstm-based vaes text partially optimization issues. finally yang found initializing encoder pretrained language model improved performance observe baseline model trained hence pursue further. images next apply approach model images omniglot dataset posterior collapse less issue vaes trained images still expect improving amortization would result generative models better model underlying data make latent space. three-layer resnet inference network. generative model ﬁrst transforms -dimensional latent vector image spatial resolution concatenated original image -layer gated pixelcnn varying ﬁlter sizes followed ﬁnal sigmoid layer. employ kl-cost annealing schedule text experiments. appendix exact architecture/hyperparameters. results various models shown table ﬁndings largely consistent results text semi-amortized approaches outperform vae/svi baselines learn generative models make latent representations even steps unable perform well sa-vae trained reﬁnement steps indicating importance good initial parameters provided inference network. appendix further investigate performance sa-vae vary training size capacity inference network/generative model. note outperform state-of-the-art models employ hierarchical latent variables and/or sophisticated priors however additions largely orthogonal approach hypothesize also beneﬁt combining amortized inference iterative reﬁnement. table results image modeling omniglot dataset. results prior works bottom results work. gated pixelcnn autoregressive baseline refers number inference steps training/testing. variational models portion elbo shown parentheses. visualize saliency examples test figure example consists question followed answer yahoo corpus. qualitative analysis several things apparent latent variable seems encode question type therefore saliency high ﬁrst word; content words much higher saliency function words saliency </s> token quite high indicating length information also encoded latent space. third example observe left parenthesis higher saliency right parenthesis latter predicted conditioning former rather latent representation figure saliency visualization examples test set. saliency values rescaled within example easier visualization. indicates higher saliency values. input saliency ﬁrst test example addition sample outputs generated variational posterior middle except made-up example. best viewed color. encoder word embedding visualize input saliency test example made-up example input example also visualize samples variational posterior generated examples often meaningfully related input example. norm rather crude measure better measure would obtained analyzing spectra jacobian however computationally expensive calculate token corpus. ﬁrst sample sampling sample temperature i.e. softmax vector scores words. found generated examples related original roughly half cases. figure nouns adjectives verbs numbers </s> token higher saliency conjunctions determiners prepositions token—the latter relatively easier predict conditioning previous tokens; similarly average tokens occurring earlier much higher saliency occurring later latent variable used much predicting rare tokens; negative correlation saliency log-likelihood though relationship always hold—e.g. </s> high saliency relatively easy predict average log-likelihood appendix corresponding analysis input saliency qualitatively similar. figure output saliency part-of-speech position frequency log-likelihood. section deﬁnitions output saliency. dotted gray line plot shows average saliency across words. drawback approach training step requires backpropagating generative model multiple times costly especially generative model expensive compute potentially mitigated sophisticated meta learning approaches efﬁcient past gradient information averaging importance sampling could also consider employing synthetic gradients limit number backpropagation steps during training. krishnan observe important train iterative reﬁnement earlier stages therefore annealing number reﬁnement steps training progresses could also speed training. approach applicable variational families avail differentiable optimization respect elbo. contrast generally applicable optimization algorithms. work closely related line work uses separate model initialize variational parameters subsequently updates iterative procedure marino recently utilized meta-learning train inference network learns perform itmodel fully conditions entire history given always therefore powerfulenough model latent variable ignored advantage autoregressive model directly generate text latent representation. differentiating inference/optimization initially explored various researchers primarily outside area deep learning recently explored context hyperparameter optimization differentiable layer deep model initial work vae-based approaches image modeling focused simple generative models assumed independence among pixels conditioned latent variable recent works obtained substantial improvements loglikelihood sample quality utilizing powerful autoregressive models generative model contrast modeling text vaes remained challenging. bowman found using lstm generative model resulted degenerate case whereby variational posterior collapsed prior generative model ignored latent code. many works vaes text thus made simplifying conditional independence assumptions used less powerful generative models combined recurrent generative model topic model note contrast sequential vaes employ different latent variables time step work focus modeling entire sequence global latent variable. finally since work addresses amortization approximation combined existing work employing richer posterior/prior distributions within framework andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. proceedings nips chen kingma diederik salimans duan dhariwal prafulla schulman john sutskever ilya abbeel pieter. variational lossy autoencoder. proceedings iclr chung junyoung kastner kyle dinh laurent goel kratarth courville aaron bengio yoshua. recurrent latent variable model sequential data. proceedings nips goyal anirudh sordoni alessandro cote marc-alexandre rosemary bengio yoshua. z-forcing training stochastic recurrent networks. proceedings nips goyal prasoon zhiting liang xiaodan wang chenyu xing eric. nonparametric variational auto-encoders hierarchical representation learning. proceedings iccv ishaan kumar kundan ahmed faruk taiga adrien visin francesco vazquez david courville aaron. pixelvae latent variable model natural images. proceedings iclr hjelm devon kyunghyun chung junyoung salakhutdinov russ calhoun vince jojic nebojsa. iterative reﬁnement approximate posterior directed belief networks. proceedings nips jaderberg czarnecki wojciech marian osindero simon vinyals oriol graves alex silver david kavukcuoglu koray. decoupled neural interfaces using synthetic gradients. proceedings icml johnson matthew duvenaud david wiltschko alex adams ryan datta sandeep composing graphical models neural networks structured representations fast inference. proceedings nips serban iulian vlad sordoni alessandro ryan lowe laurent charlin pineau joelle courville aaron bengio yoshua. hierarchical latent variable encoder-decoder proceedings aaai model generating dialogues. stoyanov veselin ropson alexander eisner jason. empirical risk minimization graphical model parameters given approximate inference decoding model structure. proceedings aistats oord aaron kalchbrenner vinyals oriol espeholt lasse graves alex kavukcuoglu koray. conditional image generation pixelcnn decoders. proceedings nips wang wenlin wang wenqi shen dinghan huang jiaji ping satheesh sanjeev carin lawrence. topic proceedings compositional neural language model. aistats yang zichao zhiting salakhutdinov ruslan bergkirkpatrick taylor. improved variational autoencoders text modeling using dilated convolutions. proceedings icml rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. proceedings icml variational models spherical gaussian prior. variational family diagonal gaussian parameterized vector means variances. models trained initial variational parameters randomly initialized gaussian standard deviation equal lstm one-layer lstm hidden units input embedding also -dimensional. initial hidden/cell states zero generate time steps example consists single afﬁne transformation project vocabulary space tokens. lstm/mlp parameters randomly initialized except part directly connects latent variables initialized done make sure latent variables inﬂuence predicting generate training/validation/test examples. learn generative model lstm initialized inference network also one-layer lstm -dimensional hidden units/input embeddings variational parameters predicted afﬁne transformation ﬁnal hidden state encoder. models trained stochastic gradient descent batch size learning rate gradient clipping learning rate starts decaying factor epoch ﬁrst epoch validation performance improve. learning rate decay triggered ﬁrst epochs. train epochs enough convergence models. svi/sa-vae perform steps iterative inference stochastic gradient descent learning rate gradient clipping model architecture used yang inference network generative model one-layer lstms -dimensional hidden states input word embedding dimensional. ﬁnal hidden state encoder predict vector variational means variances. latent space dimensional. sample variational posterior used initialize initial hidden state generative lstm afﬁne transformation additionally input time step. dropout layers probability input-to-hidden layer hidden-to-output layer generative lstm only. inference network model data size data size data size data size -layer pixelcnn -layer pixelcnn -layer pixelcnn -layer pixelcnn -layer pixelcnn table upper bounds negative log-likelihood vae/sa-vae trained omniglot vary capacity inference network portion loss shown parentheses. vary training size -layer pixelcnn generative model. training size vary capacity generative model. data contains train/validation/test examples words vocabulary. models trained stochastic gradient descent batch size learning rate learning rate starts decaying factor epoch ﬁrst epoch validation performance improve. learning rate decay triggered ﬁrst epochs ensure adequate training. train epochs learning rate decayed times enough convergence models. model parameters initialized gradients clipped employ kl-cost annealing schedule whereby multiplier kl-cost term increased linearly batch epochs. models trained iterative inference perform stochastic gradient descent momentum learning rate gradients clipped step preprocessed omniglot dataset standard validation split randomly pick images training validation. previous works pixel value scaled interpreted probabilities images dynamically binarized during training. inference network consists residual blocks block made standard residual layer followed downsampling convolutional layer ﬁlter size stride equal layers feature maps. output residual network ﬂattened used obtain variational means/log variances afﬁne transformation. lution feature maps linear transformation concatenated original image ﬁnally input -layer gated pixelcnn pixelcnn three layers followed three layers three layers ﬁnally three layers. layers feature maps ﬁnal convolutional layer followed sigmoid nonlinearity produce distribution binary output. layers appropriately masked ensure distribution pixel conditioned pixels left/top train adam learning rate epochs batch size gradients clipped table investigate performance vae/sa-vae vary capacity inference network size training capacity generative model. inference network relu layers hidden units. varying pixelcnn generative model sequentially remove layers baseline -layer model starting bottom intuitively expect iterative inference help inference network generative model less powerful indeed table further might expect sa-vae helpful small-data regimes harder inference network amortize inference generalize well unseen data. however sa-vae outperforms similar margin across training sizes. finally observe across scenarios portion loss much higher models trained sa-vae indicating models learning generative models make latent representations.", "year": 2018}