{"title": "On Learning High Dimensional Structured Single Index Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Single Index Models (SIMs) are simple yet flexible semi-parametric models for machine learning, where the response variable is modeled as a monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights and the nonlinear function that relates features to observations. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions, and under general structural assumptions, has not been forthcoming. In this paper, we propose computationally efficient algorithms for SIM inference in high dimensions with structural constraints. Our general approach specializes to sparsity, group sparsity, and low-rank assumptions among others. Experiments show that the proposed method enjoys superior predictive performance when compared to generalized linear models, and achieves results comparable to or better than single layer feedforward neural networks with significantly less computational cost.", "text": "single index models simple ﬂexible semiparametric models machine learning response variable modeled monotonic function linear combination features. estimation context requires learning feature weights nonlinear function relates features observations. methods described learn sims dimensional regime method efﬁciently learn sims high dimensions general structural assumptions forthcoming. paper propose computationally efﬁcient algorithms inference high dimensions structural constraints. general approach specializes sparsity group sparsity low-rank assumptions among others. experiments show proposed method enjoys superior predictive performance compared generalized linear models achieves results comparable better single layer feedforward neural networks signiﬁcantly less computational cost. unknown weight vector smooth transfer function typical examples logit probit functions classiﬁcation linear function regression. high dimensional parameter estimation glms widely studied theoretical algorithmic point view classical work generalized linear models assumes known function often unknown real-world datasets hence need methods simultaneously learn ﬁrst introduced econometrics statistics literature ichimura horowitz since become popular statistical machine learning applications well. recently computationally statistically efﬁcient algorithms provided learning sims low-dimensional settings number samples/observations much larger ambient dimension however many problems modern machine learning signal processing computational biology high dimensional i.e. number parameters learn exceeds number data points example genetics infer activation weights thousands genes hundreds measurements. paper motivated high-dimensional data analysis problems consider learning high dimensions. hard learning problem statistical inference ill-posed indeed impossible high-dimensional setup without making additional structural assumptions unlike glms transfer function unknown also needs learned data. handle problems impose additional structure unknown weight vector elegantly captured concept small atomic cardinality make smoothness assumptions transfer function concept small atomic cardinality generalizes commonly imposed structure high-dimensional statistics sparsity group sparsity low-rank allows design single algorithm learn various structural assumptions. provide efﬁcient algorithm called used learn sims high dimensions. algorithm optimization procedure minimizes loss function calibrated unknown alternates projected gradient descent step update estimate function learning procedure called lpav learn monotonic lipschitz function. provide extensive experimental evidence demonstrates effectiveness variety high dimensional machine learning scenarios. moreover also show able obtain competitive often better results compared single layer neural network signiﬁcantly less computational cost. related work contributions alquier biau consider learning high dimensional single index models. provide estimators using pac-bayesian analysis relies reversible jump mcmc slow converge even moderately sized problems. learns high dimensional single index models simple sparsity assumptions weight vectors provide methods learn sim’s matrix factorization setting. ﬁrst steps towards learning high dimensional sim’s method handle several types structural assumptions generalizing approaches several structures elegant manner. restricted versions estimation problem sparsity constraints considered authors interested accurate parameter estimation prediction. hence works proposed algorithms learn transfer function. ﬁnally comment also related literature focused query points order learn index modsemi-parametric models called multiple learning involves corresponding multiple semi-parametric models model linear combination functions form also popular restrictions transfer function allow simple optimization methods learn finally neural networks emerged powerful alternative learn nonlinear transfer functions basically thought deﬁned compositions nonlinear functions. high dimensional setting hard estimate parameters accurately multilayer network thorough comparison beyond scope paper. nonetheless show method enjoys comparable often superior performance single-layer feed forward signiﬁcantly cheaper train. positive results indicate could perhaps method much cheaper alternative practical data analysis problems motivates consider deep variants method future. best knowledge simple practical algorithms good empirical performance learning single index models high dimensions available. notations sequel problem interested solve. assume provided i.i.d. data label generated according model unknown parameter vector additionally assume rn×d matrix corresponding corresponding vector observations. note case matrix estimation problems data matrices sake notational simplicity assume matrices vectorized. case problem recovering measurements ill-posed even known. overcome this usually makes additional structural assumptions parameters speciﬁcally assume parameters satisfy notion structural simplicity elaborate suppose given atoms written pa∈a caa. although number atoms uncountably inﬁnite notation implies expressed linear combination ﬁnite number atoms. denotes indicator function unity condition inside satisﬁed inﬁnity otherwise. vector structurally simple respect atomic kwka small. notion structural simplicity plays central role several high dimensional machine learning signal processing applications problems neuroubiquitous science compressed sensing atoms case merely signed canonical basis vectors atomic cardinality vector simply sparsity idea group sparsity plays central role multitask learning computational biology among applications. atoms dimensional unit disks atomic cardinality vector simply group sparsity notice optimization problem simultaneously learning function well weight vector. additional layer complication explains learning sims considerably harder problem learning glms typical optimization problem similar equation later show experimental results additional complexity optimization justiﬁed excellent results achieved algorithm compared based algorithms linear/logistic regression. algorithm solve optimizaion problem equation called calibrated single index algorithm sketched algorithm interleaves parameter learning iterative projected gradient descent monotonic function learning lpav algorithm. function learning using lpav lpav method update function learn monotonic function would model function multi-layer neural network learn weights newtwork using gradient based algorithm. lpav computationally simpler. furthermore learning several parameters typically option datapoor settings ones interested another alternative cast learning dictionary learning problem requires good dictionary hand turn relies domain expertise. given vector order function minimizes objective look ﬁrst order optimality condition. differentiating objective w.r.t. assuming i.e. assume features uncorrelated features similar variance elementary algebra need function optimize expression t−xi) yi). lpav solves exact optimization problem. precisely given data t−xi lpav outputs best univariate monotonic lipschitz function minimizes squared error yi). lpav using following known atomic positive integer loss function appropraitely designed elaborate next. notice formulation added squared norm penalty make objective function strongly convex. case dealing matrix problems frobenius norm constraint atomic cardinality ensures learning structurally simple parameters indeed makes problem well posed. since monotonically increasing function convex stochastic optimization problem convex. taking ﬁrst derivative verify opti⋆ solution satisﬁes relation constrained squared loss minimization problem. logit problem equation constrained logistic loss minimization problem. examples atomic projections component algorithm projection operator entirely depends atomic suppose given vector atomic positive integer also pa∈a achieve sense elements arranged descending order magnitude. deﬁne element vector denotes corresponding atom original representation. performing projections computationally efﬁcient cases atomic signed canonical basis vectors projection standard hard thresholding operation retain magnitude coefﬁcients reduces retaining best rank-s approximation since typically small done efﬁciently using power iterations. atoms dimensional unit disks projection step reduces computing norm restricted group retaining groups. computational complexity analyze computational complexity iterate algorithm need analyze time complexity gradient step projection step lpav steps used gradient step takes time. projection step low-rank sparse group sparse cases naively implemented using time max-heaps time. lpav algorithm quadratic program immense structure inequality constraints quadratic objective. using clever algorithmic techniques solve optimization problem time total runtime complexity iterations log)) making algorithm fairly efﬁcient. large scale problems data sparse case term replaced nnz. compare contrast method several algorithms various high dimensional structural settings several datasets. start case standard sparse parameter recovery proceeding display effectiveness method multitask/multilabel learning scenarios also structured matrix factorization setting. izing projection operator outputs best atomic-sparse representation argument. provided parameter update estimate using lpav algorithm data projected onto vector wt−. using updated estimate update weight vector single gradient step objective function optimization problem equation related equation gradient step followed atomic projection step while convergence checks stopping conditions decide stop noticed tens iterations sufﬁcient experiments structural constraint interest algorithm learn appropriate high dimensional sim. show next projection step indeed tractable whole cases interest high dimensional statistics. projection replaced soft thresholding-type operator well algorithmic performance largely unaffected. however performing hard thresholding typically efﬁcient shown enjoy comparable performance soft thresholding operators several cases. single layer feedforward trained using tensorﬂow adam optimizer used minimize cross-entropy used early stopping method validated results multiple epochs number hidden units varied since slnn constrained ﬁtting monotonic function would expect slnns smaller bias sims. however since slnns parameters larger variance sims. always perform train-validation-test split data report results test set. tested algorithms several datasets link page datasets machine learning repository. also four datasets newsgroups corpus atheism-religion autos-motorcycle cryptographyelectronics mac-windows. compared table since several datasets unbalanced methods. following summary outperforms simple widely popular learning algorithms sls. often difference between algorithms quite substantial. example measuring accuracy difference between either datasets least many cases large group sparsity multilabel multitask learning next consider problem multi-label learning using group sparsity structure. consider datasets. multilabel learning ﬂags dataset contains measurement possible labels data split measurements training test respectively. training randomly aside measurements validation. multitask learning atpd dataset consists simultaneous regression tasks dimensional data measurements. perform random split data training validation testing. compared method group sparse logistic regression least squares using malsar package logistic regression least squares range parameter values −··· varied step size scale method setting group sparsity parameter datasets. table shows method performs better compared methods. classiﬁcation score performance measure since multilabel problems highly unbalanced measures accuracy indicative performance. multitask learning report mse. structured matrix factorization visit problem matrix completion presence graph side information. consider datasets epinions flixster datasets social network among users. process data follows ﬁrst retain users items ratings. sparsify data randomly retain observations training figure applied high-dimensional dataset labels generated sparse weight vector sparsity level three lines show error decreases number iterations algorithm. covariates sampled {+−} using shown equation experiments kept hidden algorithm. figure show distance iterates changes number iterations increases. result tells distance monotonically decreases number iterations moreover problem harder dimensionality increases. combining results simulation result shown figure make following conjecture. conjecture suppose given i.i.d. labeled data satisﬁes lipschitz monotonic function kw⋆k restricted strong convex restricted strong smooth deﬁned given data distribution. appropriate choice aside observations cross validation. furthermore binarize observations corresponding likes dislikes among users items. showed problem structured matrix factorization cast following atomic norm constrained program. least squares approach solves following program singular vectors singular values graph laplacian graph among rows graph laplacian corresponding graph among columns atoms case except replace loss function calibrated loss. report table table dataset details performance different algorithms structured matrix factorization. stands least squares method method unknown nonlinearity. empirical discussion convergence known algorithm basically iterative gradient descent based algorithm convex likelhood function combined hard thresholding. analyzed exponential rates convergence established results assume likelihood loss function satisﬁes certain restricted strong convexity restricted strong smoothness assumptions. leads natural question establish exponential rates convergence algorithm single index model i.e. unknown? while unable establish formal analysis rates convergence case believe fast rates might achievable case best achieve much slower sub-linear rates convergence iterates. support claim experiment study quickly iterates generated converge synthetic dataset generated using sim. synthetic experiment setup follows generate covariates standard normal distribution", "year": 2016}