{"title": "What Does a TextCNN Learn?", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "TextCNN, the convolutional neural network for text, is a useful deep learning algorithm for sentence classification tasks such as sentiment analysis and question classification. However, neural networks have long been known as black boxes because interpreting them is a challenging task. Researchers have developed several tools to understand a CNN for image classification by deep visualization, but research about deep TextCNNs is still insufficient. In this paper, we are trying to understand what a TextCNN learns on two classical NLP datasets. Our work focuses on functions of different convolutional kernels and correlations between convolutional kernels.", "text": "textcnn convolutional neural network text useful deep learning algorithm sentence classiﬁcation tasks sentiment analysis question classiﬁcation. however neural networks long known black boxes interpreting challenging task. researchers developed several tools understand image classiﬁcation deep visualization research deep textcnns still insufﬁcient. paper trying understand textcnn learns classical datasets. work focuses functions different convolutional kernels correlations convolutional kernels. fig. network structure textcnn model; conv convolutional layers ﬁlter window feature maps each; batch normalization; relu rectiﬁed linear units; max-pool turns matrix vector keeping maximum values column; dropout keep rate extract n-grams dataset feed convolutional kernel appropriate ﬁlter window -grams second layer). record activation values convolutional kernel statistical analysis. kernel n-grams generate top- activation values. come sentences label classify kernel label. top- n-grams come mixed sentences classify kernel type other. experiment results sets convolutional kernels trec dataset kernels other class ﬁrst layer second layer. implies kernels ﬁrst layer learn generic features kernels second layer learn label-speciﬁc features. kernels other class learn generic features shared multiple labels. gain intuition kernels observed top- sensitive n-grams kernels models found typical examples other kernels focus overall grammar patterns n-grams others focus general topics related n-grams. however boundary types vague still many other kernels whose functions remain unknown humans. suppose activation value i-th kernel ﬁlled k-th n-gram aik. compute correlation coefﬁcient vector vector larger closer relationship kernels every either positive nearly zero omit symbols absolute value. found ﬁrst pair kernels recognizes similar ngrams second pair kernels recognizes completely different n-grams. example illustrates measurements reﬂect correlations kernels. data implies correlations kernels second layer stronger ﬁrst layer. discrepancy redundancy decrease number kernels second layer compress model. therefore correlation analysis potential help optimize networks. trained textcnn classifying texts used quantitative approaches analyze relationship kernels. method restricted convolution kernels though help analyze structures fully connected layers. using method results textcnn kernels learn features labels; kernels analogous; kernels learn common features different classes; depth layer inﬂuences learned features.", "year": 2018}