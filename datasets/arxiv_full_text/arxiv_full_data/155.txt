{"title": "Capacity and Trainability in Recurrent Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.", "text": "potential bottlenecks expressiveness recurrent neural networks ability store information task parameters store information input history units. show experimentally common architectures achieve nearly per-task per-unit capacity bounds careful training variety tasks stacking depths. store amount task information linear number parameters approximately bits parameter. additionally store approximately real number input history hidden unit. several tasks per-task parameter capacity bound determines performance. results suggest many previous results comparing architectures driven primarily differences training effectiveness rather differences capacity. supporting observation compare training difﬁculty several architectures show vanilla rnns difﬁcult train slightly higher capacity. finally propose novel architectures easier train lstm deeply stacked architectures. research application recurrent neural networks seen explosive growth last years rnns become central component successful model classes application domains deep learning seqseq neural machine translation draw model educational applications scientiﬁc discovery despite recent successes widely acknowledged designing training components complex models extremely tricky. painfully acquired expertise still crucial success projects. main strategies involved deployment models long short term memory networks recently gated recurrent unit proposed chung resulting models perceived easily trained achieving lower error. widely appreciated rnns universal approximators unresolved question degree gated models computationally powerful practice opposed simply easier train. provide evidence observed superiority gated models vanilla models almost exclusively driven trainability. first describe types capacity bottlenecks various architectures might expected suffer from parameter efﬁciency related learning task ability remember input history. next describe experimental setup disentangle effects bottlenecks including training extremely thorough hyperparameter optimization. finally describe capacity experiment results several potential bottlenecks rnns example much information task store parameters? much information input history store units? ﬁrst bottlenecks seen memory capacities different types memory. another different kind capacity stems computational primitives able perform. example maybe wants multiply numbers. terms number units time steps task straight-forward using speciﬁc computational primitives dynamics others extremely resource heavy. might expect differences computational capacity different computational primitives would play large role performance. however despite fact gated architectures outﬁtted multiplicative primitive hidden units vanilla found evidence computational bottleneck experiments. therefore focus per-parameter capacity learn task training per-unit memory capacity remember inputs. rnns many scalings matrices biases functional form certain nonlinearities. additionally many involved training choice optimizer learning rate schedule. order train models employed tuner uses gaussian process model similar spearmint snoek related work). basic idea requests values tuner runs optimization completion using values returns validation loss. loss used tuner combination previously reported losses choose values many experiments validation loss minimized respect hps. experiments report evaluation loss tuner highly optimized task studies used variety well-known architectures standard rnns vanilla newer irnn well gated architectures lstm. rounded models innovating novel architectures call update gate intersection ugrnn ‘minimally gated’ architecture coupled gate recurrent hidden state update hidden state. +rnn uses coupled gates gate recurrent depth dimensions straightforward way. explore various strengths weaknesses architecture also used variety network depths experiments. experiments held number parameters ﬁxed across different architectures different depths. precisely given experiment maximum number parameters along input output dimension. number hidden units layer chosen number parameters summed across layers network large possible without exceeding allowed maximum. tasks variants depths model sizes tuner order optimize relevant loss function. typically resulted many hundreds several thousands evaluations full training millions training steps. taken together amounted cpu-millennia worth computation. dimension rnns provides upper bound task-capacity upper bounds close match experimental results. instance performance saturates rapidly terms number unrolling steps relevant bound increases linearly number unrolling steps. \"unrolling\" refers recurrent computation time. empirically karpathy studied lstms encode information character-based text modeling tasks. further sussillo barak reverse-engineered vanilla trained simple tasks using tools language nonlinear dynamical systems theory. foerster behavior switched afﬁne recurrent networks carefully examined. ability rnns store information input better studied context machine learning theoretical neuroscience. previous work short term memory traces explores tradeoffs memory ﬁdelity duration case input presented every time step simpler capacity measure consisting ability store single input vector. results suggest that contrary common belief capacity rnns remember input history practical limiting factor performance. precise details makes architecture perform well extremely active research ﬁeld highly related article greff authors used random search along systematic removal pieces lstm architecture determine pieces lstm important others. ugrnn architecture directly inspired large impact removing forget gate lstm zhou introduced architecture minimal gating similar ugrnn directly inspired gru. in-depth comparison rnns grus context end-to-end speech recognition limited computational budget conducted amodei further ideas architectures improve ease training forget gates copying recurrent state time step another making deep feed-forward networks highway networks residual connections respectively. indeed +rnn inspired part coupled depth gate srivastava brieﬂy deﬁne architectures used study. unless otherwise stated denotes matrix denotes vector biases. symbol input time hidden state time remaining vector variables represent intermediate values. function denotes logistic sigmoid function either tanh relu initial conditions networks learned bias. finally well-known trick trade initialize gates lstm large bias induce better gradient ﬂow. included parameter denoted tuned along hps. based greff noticed forget gate crucial lstm performance tried variant began vanilla added single gate. gate determines whether hidden state carried previous time step updated hence update gate. alternative view ugrnn highway layer gated time success ugrnn shallower architectures study well observed trainability problems lstm deeper architectures developed intersection architecture coupled depth gate addition coupled recurrent gate. additional inﬂuences architecture recurrent gating lstm depth gating highway network architecture recurrent input depth input also recurrent output depth output note architecture applies layers dimension appropriate networks depth foundational result machine learning single-layer perceptron parameters store least bits information parameter precisely perceptron implement mapping n-dimensional input vectors arbitrary n-dimensional binary output vectors subject extremely weak restriction input vectors general position. rnns provide complex input-output mapping hidden units recurrent dynamics diversity nonlinearities. nonetheless wondered analogous capacity results rnns might able observe empirically. show section tasks complex temporal dynamics language modeling exhibit per-parameter capacity bottleneck explains performance rnns better per-unit bottleneck. make experimental design simple possible remove potential confounds stemming choice temporal dynamics study per-parameter capacity using task inspired gardner speciﬁcally measure much task-related information stored parameters memorization task random static input injected random static output read number time steps later. emphasize per-parameter bottleneck simpliﬁed task also arises temporally complex tasks language modeling. high level draw ﬁxed random inputs random labels train random inputs randomly chosen labels cross-entropy error. however rather returning cross-entropy error tuner instead return mutual information outputs true labels. treat number input-output mappings tuner select correct number mappings maximize mutual information outputs labels. mutual information compute bits parameter provides normalized measurement much learned task. precisely draw datasets binary inputs target binary labels uniform binary datasets }nin×b number samples dimensionality inputs. number samples treated practice optimal dataset size close bits mutual information true predicted labels. trend demonstrated figure app. appendix. value trained minimize cross entropy network output true labels. write output inputs corresponding random variable interested mutual information true class labels class labels predicted rnn. amount information stored task. setting calculated parameters. plot best value number parameters figure captures amount information stored parameters mapping estimate bits parameter divide number parameters shown figure five bits parameter examining results figure capacity architectures roughly linear number parameters across several orders magnitude parameter count. capacity bits parameter across architectures depths across several orders magnitude terms number parameters. given possibility small size effects larger portion weights used biases small number parameters believe estimates larger networks reliable. leads bits parameter estimate approximately averaging architectures depths. finally note per-parameter task capacity increases function number unrollings though diminishing gains ﬁnding results consistent across diverse architectures scales even surprising since prior experiments clear capacity would even scale linearly number parameters. instance previous results model compression reducing number parameters reducing depth parameters might lead predict different architectures parameters vastly different efﬁciencies task capacity increases sublinearly parameter count. gating slightly reduces capacity overall different architectures performed similarly capacity differences architectures appear hold across depths parameter counts. quantify differences constructed table showing change number parameters would need switch architecture another maintaining equivalent capacity trend emerged capacity experiments slightly reduced capacity function \"gatedness\". putting aside irnn performed worst discussed below noticed across depths model sizes performance average ugrnn lstm +rnn. vanilla gates ugrnn remaining three more. figure neural network architectures store approximately bits parameter task small variations across architectures. stored bits function network size. numbers represent maximum stored bits across optimizations time steps unrolled network size levels depth. level depth shown separately. showing bits parameter function network size. value cell multiplier number parameters needed give architecture x-axis capacity architecture y-axis. capacities measured averaging maximum stored bits parameter architecture across sizes levels depth. relus reduce capacity capacity tasks irnn performed noticeably worse architectures reaching maximum bits parameter roughly determine performance drop relu nonlinearity irnn identity initialization sorted ugrnn results looked maximum bits parameter optimizations using relu figure additional capacity analysis. effect relu nonlinearity capacity. solid lines indicate bits parameter -layer architectures tanh relu nonlinearity choices tuner. dashed lines show maximum bits parameter architecture results achieved relu nonlinearity considered. bits parameter function number time steps unrolled. error curve architectures depths memory throughput task. curve shows error plotted function number units random input dimension networks less units error reconstruction networks number units greater nearly perfectly reconstruct random input. considered. indeed ugrnn bits parameter dropped dramatically range architectures exclusively used relu providing strong evidence relu activation function problematic capacity task. additional capacity bottleneck rnns ability store information inputs time. plainly obvious irnn essentially integrator achieve perfect memory inputs number inputs less equal number hidden units clear complex architectures. measured per-unit input memory empirically. figure shows intuitive result every architecture studied reconstruct random dimensional input time future number hidden units layer network greater equal moreover regardless architecture error reconstructing input follows curve function number hidden units variants corresponding reconstructing dimensional subspace dimensional input. highlight per-unit capacity make point per-parameter task capacity appears limiting factor experiments per-unit capacity per-unit capacity remember previous inputs. thus comparing results architectures normalize different architectures number parameters number units frequently done literature makes sense common architectures computational cost processing single sample linear number parameters quadratic number units layer. show figure plotting capacity results numbers units gives misleading results. studied additional tasks believed easy enough train evaluation loss different architectures would reveal variations capacity rather trainability. critical aspect tasks could learned perfectly model sizes experiments. change model size therefore expect performance task also change. tasks random continuous functions task similar per-parameter capacity task above except target outputs real numbers number training samples held ﬁxed. performance tasks shown figure evaluation loss function number parameters plotted panels text task task respectively. tasks section number parameters rather number units provided bottleneck performance architectures performed extremely closely number parameters. close performance mean that model achieve loss another model number parameters would adjusted small factor figure architectures achieved near identical performance given number parameters language modeling random function ﬁtting task. text wikipedia number parameters bits character architectures. left right layer layer layer models. text number hidden units bits character layer architectures. note almost always misleading compare architectures heavily gated architectures appear better compared per-unit. except showing square error different model sizes trained rcfs. practice widely appreciated often signiﬁcant performance between example lstm vanilla lstm nearly always outperforming vanilla rnn. per-parameter capacity results provide evidence rough equivalence among variety architectures slightly higher capacity vanilla reconcile per-parameter capacity results widely held experience provide evidence gated architectures lstm easier train vanilla study tasks difﬁcult learn parallel parentheses counting independent input streams mathematical addition integers encoded character string parentheses task moderately difﬁcult learn arithmetic task quite hard. results optimizations shown figure parentheses task figure arithmetic task. tasks show that possible vanilla learn tasks reasonably well difﬁcult gated architecture. note best achieved loss arithmetic task still signiﬁcantly decreasing even evaluations irnn. three noteworthy trends trainability experiments. first across tasks depths irnn performed poorly took longest learn task. note however irnn always solved tasks eventually least depth second stacking depth increased gated architectures became architectures figure architectures easier train others. results searches extremely difﬁcult tasks. median evaluation error function optimization iteration layer architectures parentheses task. dots indicate evaluation loss achieved iteration. layer architectures. minimum evaluation error function optimization iteration parentheses task. depth order except arithmetic task. note best loss vanilla still decreasing evaluations. achieve results capacity trainability relied heavily tuner. practitioners time resources make tuner typically adjusting times themselves. wondered various architectures would perform randomly within ranges speciﬁed tried times parentheses task parameter architectures depths noticeable trends irnn returned infeasible error nearly half time lstm infeasible least number times infeasibility means training loss diverged. depth gave smallest error smallest median error depth +rnn delivered smallest error smallest median error. figure randomly generated hyperparameters +rnn easily trainable architectures. evaluation losses iterations randomly chosen sets layer parameter models parentheses task. statistics welch’s t-test equality means pairs architectures presented table app.. whisker plot evaluation losses layer model. layers. report number variants hold bits parameter task variants remember number random inputs nearly equal number hidden units rnn. quantiﬁcation number bits parameter store task particularly important previously known whether amount information task could stored even linear number parameters. results point empirical capacity limits task memorization input memorization apparently requirement remember features input time practical bottleneck. were vanilla irnn would perform better gated architectures proportion ratio number units not. based widespread results literature results difﬁcult tasks loss memory capacity improved trainability seems worthwhile trade off. indeed input memory capacity obviously impact task explicitly designed measure error curves instance language modeling task overlapped across architectures number parameters number units. result per-parameter task capacity bits parameter averaged architectures surprising agreement recently published results capacity synapses biological neurons. number recently calculated bits synapse based biological synapses hippocampus roughly measurable discrete sizes capacity results implications compressed networks employ quantization techniques. particular provide estimate number bits weight compressed without loss task performance. coincidentally authors used bits weight fully connected layers. additional observation per-parameter task capacity experiments increases time steps beyond appears saturate. interpret suggest recurrence endows additional capacity network shared parameters diminishing returns total capacity remains bounded even number time steps increases. also note performance nearly constant across architectures number parameters held ﬁxed. motivate design architectures small compute parameter ratios mixture experts rnns rnns large embedding dictionaries input output despite best efforts cannot claim perfectly trained models. potential problems optimization could local minima well stochastic behavior optimization result stochasticity batching random draws weight matrices. tried uncover effects running best performing times observe serious deviations best results another form validation comes fact capacity task essentially independent experiments yielded clustering architecture results yield framework choosing recurrent architecture? total believe yes. explored amodei practical concern recurrent models speed execution production environment. results suggest large resource budget training conﬁned resource budget inference choose vanilla rnn. conversely training resource budget small inference budget large choose gated model. another serious concern relates task complexity. task easy learn vanilla yield good results. however task even moderately difﬁcult learn gated architecture right choice. results point learnable gated rnns shallow architectures followed ugrnn. +rnn typically performed best deeper architectures. results trainability conﬁrm widely held view lstm extremely reliable architecture almost never best performer experiments. course experiments required fully ugrnn +rnn. things considered uncertain training environment results suggest using +rnn. references dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos erich elsen jesse engel linxi christopher fougner tony awni hannun billy patrick legresley libby sharan narang andrew sherjil ozair ryan prenger jonathan raiman sanjeev satheesh david seetapun shubho sengupta wang zhiqian wang chong wang xiao dani yogatama zhan zhenyao zhu. deep speech end-to-end speech recognition english mandarin. corr abs/. http//arxiv.org/abs/.. thomas bartol cailey bromer justin kinney michael chirillo jennifer bourne kristen harris terrence sejnowski. nanoconnectomic upper bound variability synaptic plasticity. elife kyunghyun bart merriënboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. thomas desautels andreas krause joel burdick. parallelizing exploration-exploitation tradeoffs gaussian process bandit optimization. journal machine learning research felix gers jurgen schmidhuber fred cummins. learning forget continual prediction lstm. artiﬁcial neural networks icann ninth international conference alex graves marcus liwicki santiago fernández roman bertolami horst bunke jürgen schmidhuber. novel connectionist system unconstrained handwriting recognition. pattern analysis machine intelligence ieee transactions rafal jozefowicz wojciech zaremba ilya sutskever. empirical exploration recurrent network architectures. proceedings international conference machine learning james martens ilya sutskever. learning recurrent neural networks hessian-free optimization. proceedings international conference machine learning noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. corr abs/. http//arxiv.org/abs/.. zichao yang marcin moczulski misha denil nando freitas alex smola song ziyu wang. deep fried convnets. proceedings ieee international conference computer vision guo-bing zhou jianxin chen-lin zhang zhi-hua zhou. minimal gated unit recurrent neural networks. international journal automation computing issn ./s---. http//dx.doi.org/./s---. used tuner uses gaussian process bandits approach optimization. setting tuner’s internal parameters uses batched bandits expected improvement acquisition function matern kernel feature scaling automatic relevance determination performed optimizing kernel hps. please desautels snoek closely related work. tasks requested tuner reported loss validation dataset. per-parameter capacity task evaluation validation training datasets identical. text validation evaluation sets consisted different sections held data. tasks evaluation validation training sets randomly drawn distribution. performance plot cases evaluation dataset. matrix inherently square e.g. three possible initializations identity orthogonal random normal distribution scaled number recurrent units. sole exception limited either orthogonal random normal initializations differentiate irnn. matrix inherently rectangular e.g. initialized random normal distribution scaled matrix initializations except identity initialization multiplicative scalar used scale matrix. scalar exponentially distributed recurrent matrices rectangular matrices. optimization algorithms additional parameters adam’s second order decay rate epsilon parameter. default values optimized. batch size individually hand experiments. seed used initialize random number generator task parameters whereas generator randomly seeded network parameters note network initial condition learned vector. figure app. capacity task optimal dataset size found tuner slightly larger mutual information bits reported figure architectures sizes depths. high-level perceptron capacity task wanted optimize amount information carried true random labels practice training objective standard cross-entropy. however returning validation loss tuner returned mutual information conceptually nested optimization inside another. inner loop optimizes training cross entropy returning mutual information. outer loop chooses particular number samples equation maximize amount mutual information. implementation necessitated straightforward differentiate mutual information respect number samples. training cross entropy error evaluated beginning time steps. memory capacity task wanted know much information reconstruct inputs later time point. picked input dimension varied number parameters networks number hidden units roughly centered around time steps target network exact reconstruction input square dataset constructed consisting random unit norm gaussian input vectors size target scalar outputs generated input vector also drawn unit norm gaussian. sample assigned power weighting normalization constant weightings summed characteristic time constant loss function training calculated time steps weighted square error acting weighting terms. text task task predict character ahead text dataset input hot-one encoded sequence output. loss cross-entropy loss softmax output layer. rather partial unrolling common language modeling generated random pointers text. ﬁrst time steps used initialize normal operating mode remaining steps used training inference. parentheses counting task independently counts number opened ‘parens’ e.g. parens used mean parens type pairs e.g. ‘<>’ additionally noise characters ‘j’. paren type hot-one encoding paren noise symbols total inputs. output paren type hot-one encoding digits represented count opened parens type. count exceeded network kept count paren closed count decreased. loss cross-entropy losses paren type. finally paren input stream random noise characters drawn random paren characters drawn e.g. streams like parens types treated noise current type e.g. string paren type ‘<>’ answer end. loss deﬁned ﬁnal time point arithmetic task hot-one encoded character sequence addition problem presented input network e.g. ‘-+= output hot-one encoded answer including correct amount left padded spaces ‘-’. additional task number compute steps input ﬁrst non-space character target output sequence. numbers input randomly uniformly selected time steps cross-entropy loss calculated. found task extremely difﬁcult networks learn task learned certain network architectures could perform task nearly perfectly. wondered robust variability random batching data random initialization parameters. identiﬁed best parentheses experiments parameter layer architectures reran parameter optimization times. measured number infeasible experiments well number statistics loss reruns results show best yielded distribution losses close originally reported loss value. table app. results runs parentheses task using best architecture depth chosen achieved minimum loss. table shows original loss achieved tuner amount infeasible trials minimum loss running iterations mean loss maximum loss standard deviation standard deviation divided mean. table app. results welch’s t-test equality means evaluation losses architecture pairs trained parentheses task randomly sampled hps. layer ugrnn irnn lstm pairs loss distributions different statistical signiﬁcance negative t-statistic indicates mean second architecture pair larger ﬁrst.", "year": 2016}