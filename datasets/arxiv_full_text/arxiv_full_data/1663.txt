{"title": "Question Asking as Program Generation", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.", "text": "hallmark human intelligence ability rich creative revealing questions. introduce cognitive model capable constructing humanlike questions. approach treats questions formal programs that executed state world output answer. model speciﬁes probability distribution complex compositional space programs favoring concise programs help agent learn current context. evaluate approach modeling types open-ended questions generated humans attempting learn ambiguous situation game. model predicts questions people creatively produce novel questions present training set. addition compare number model variants ﬁnding question informativeness complexity important producing human-like questions. active machine learning learner able query oracle order obtain information expected improve performance. theoretical empirical results show active learning speed acquisition variety learning tasks although impressive work active machine learning focused relatively simple types information requests contrast humans often learn asking richer questions directly target critical parameters learning task. human child might dogs long tails? what difference cats dogs? long term goal artiﬁcial intelligence develop algorithms similar capacity learn asking rich questions. premise make progress toward goal better understanding human question asking abilities computational terms paper propose computational framework explains people construct rich interesting queries within particular domain. insight model questions programs that executed state possible world output answer. example program corresponding does john prefer coffee tea? would return true possible world states correct answer false others. questions return different types answers. example many sugars john take coffee? would return number etc. depending world state. thinking questions syntactically well-formed programs recasts problem question asking program synthesis. show powerful formalism offers approach modeling question asking humans eventually enable human-like question asking machines. evaluate model using data containing natural language questions asked human participants information-search game given ambiguous situation context model predict questions human learners capturing constraints humans construct semantically meaningful questions. method successfully predicts frequencies contemporary active learning algorithms query labels causal interventions lack representational capacity consider richer range queries including expressed natural language. dialog systems designed questions systems still achieving human-like question asking. goal-directed dialog systems applied tasks booking table restaurant typically choose relatively small canned questions little genuine ﬂexibility creativity. deep learning systems also developed visual questions style tasks although models produce questions questions typically take stereotyped form open-ended question asking achieved non-goal-driven systems trained large amounts natural language dialog recent progress demonstrated however approaches cannot capture intentional goal-directed forms human question asking. recent work probed aspects question asking. visual question generation data contains images paired interesting human-generated questions. instance image wreck might paired question what caused accident? deep neural networks similar used image captioning capable producing types questions extensive training however require large datasets images paired questions whereas people intelligent questions novel scenario practice shown task below. moreover human question asking robust changes task goals state-of-the-art neural networks generalize ﬂexibly ways. goal develop model context-sensitive goal-directed question asking humans falls outside capabilities systems described above. focused analysis data collected consists natural language questions asked human players resolve ambiguous game situation players individually presented game board consisting grid tiles. tiles initially turned could ﬂipped reveal underlying color. player’s goal identify quickly possible size orientation position ships every board exactly three ships placed nonoverlapping otherwise random locations. ships identiﬁed color {blue purple}. ships width length orientation {horizontal vertical}. tile overlap ship displayed null water color ﬂipped. extensive instructions rules purpose game number practice rounds target contexts players presented partly revealed game board provided ambiguous information actual shape location ships. given chance natural-language question conﬁguration. player’s goal question asking opportunity gain much information possible hidden game board conﬁguration. rules given players questions must answerable using word combination questions allowed. questions recorded html text people typed wanted ask. good question context figure purple ship touch? what color tile helpful inferred revealed game board rules game answer water player completed contexts presented different underlying game board partially revealed pattern. since usefulness asking question depends context data figure battleship game used obtain question data rothe hidden positions three ships {blue purple} game board players sought identify. observing partly revealed board players allowed natural language question. partly revealed board context consists question-context pairs questions context. basic challenge active learning method predict question human given context overall rules game. particularly challenging data model subtle differences contexts determine question potentially useful along open-ended nature human question asking. describe components probabilistic model question generation. section describes elements approach compositionality computability reﬂected choice model questions programs. section describes grammar deﬁnes space allowable questions/programs. section speciﬁes probabilistic generative model sampling context-sensitive relevant programs space. remaining sections cover optimization program features alternative models analysis data revealed many questions data share similar concepts organized different ways. example concept ship size appeared various ways across questions result ﬁrst element modeling question generation recognize compositionality questions. words conceptual building blocks plus) together create meaning questions size)). combining meaningful parts give meaning larger expressions prominent approach linguistics compositionality generally inﬂuential idea cognitive science second element computability questions. propose human questions like programs executed state world output answer. example program executed looks number blue tiles hypothesized imagined battleship game board returns said number corresponds question long blue ship?. programs used evaluate potential useful information question executing program possible likely worlds preferring questions informative identifying true world state. approach modeling questions closely related formalizing question meaning partition possible worlds notion used previous studies linguistics psychology machine systems question answering also fruitfully modeled questions programs computational work cognitive science modeled various kinds concepts programs important contribution work tackles question asking provides method generating meaningful questions/programs scratch. capture compositionality computability represent questions simple programming language based lambda calculus lisp. every unit computation language surrounded parentheses ﬁrst element function following elements arguments function instance question long blue ship? would represented small program examples discussed below. step abstracted question representation exact choice words maintaining meaning. questions thought represented language thought programs language combined example asking whether ship larger blue ship. compute answer ﬁrst inner parentheses evaluated returning number corresponding number blue tiles game board respectively. numbers used arguments function returns either true false. ﬁnal property interest generativity questions ability construct novel expressions useful given context. system generate expressions language designed grammar context-free exceptions inspired grammar consists rewrite rules recursively applied grow expressions. expression cannot grown guaranteed interpretable program language. create question grammar begins expression contains start symbol rewrites symbols expression applying appropriate grammatical rules symbol rewritten. example applying rules arrive expression table shows core rewrite rules grammar. rules sufﬁcient represent questions human data set. enrich expressiveness conciseness language added lambda expressions mapping operators seen question ships size? conveniently represented evaluation sequentially assigns element λ-part ultimately returns vector three ship sizes. three ship sizes compared function. course question could also represented artiﬁcial agent using grammar able express wide range questions. decide question agent needs measure question usefulness. syntactically well-formed programs informative useful. instance program representing question blue ship larger itself? syntactically coherent. however useful question answer always false matter true size blue ship. propose probabilistic generative model aims predict questions people not. parameters model predict frequency humans particular questions particular context data formally ﬁtting generative model problem density estimation space question-like programs space deﬁned grammar. deﬁne probability question vector feature weights highlighting fact probability dependent parameterization weights normalizing constant possible questions generated grammar tables normalizing constant needs approximated since large enumerate. expected feature values given empirx∈x expected feature values given model. thus gradient zero model perfectly matched data terms average values features. computing exact expected feature values model intractable since large number possible questions importance sampling approximate expectation. create proposal distribution denoted question grammar probabilistic context free grammar uniform distributions choosing re-write rules. details optimization follows. first large questions sampled order approximate gradient step importance sampling. second procedure given model training iterations gradient ascent learning rate last purpose evaluating model importance sampler also used approximate normalizing constant estimator ex∼q turn describe question features considered namely features informativeness length four answer type. informativeness. perhaps important feature question’s informativeness model combination bayesian belief updating expected information gain compute informativeness agent needs represent several components belief current world state update belief receives answer sense possible deﬁne questions fewer functions. remove rule grammar corresponding questions data asked demonstration colored tile. although straightforward represent questions rule probabilistic nature draw exponentially complex computations possible-world answers. figure out-of-sample model predictions regarding frequency asking particular question. y-axis shows empirical question frequency x-axis shows model’s energy question rank correlation shown context. answers question. battleship game agent must identify single hypothesis space possible conﬁgurations agent question receive answer updating hypothesis space applying bayes’ rule prior speciﬁed ﬁrst uniform choice ship sizes second uniform choice possible conﬁgurations given sizes. likelihood valid output question program executed zero otherwise. expected information gain value question expected reduction uncertainty true hypothesis averaged across possible answers question shannon entropy. complete details bayesian ideal observer follow approach used figure shows scores human questions selected contexts. addition feature feig added second feature feig= zero otherwise provide offset linear feature. note value question always depends game context. remaining features described independent context. complexity. purely maximizing often favors long complicated programs +*size+*size+...). although machine would problem answering questions poses problem human answerer. generally speaking people prefer concise questions rather short questions data reﬂect this. probabilistic context free grammar provides measure complexity favors shorter programs probability grammar fcomp complexity feature. answer type. added four features answer types boolean number color location. question program belongs exactly answer types type orientation subsumed boolean horizontal true vertical false. allows model capture differences base rates question types relevance. finally added auxiliary feature deal fact grammar produce syntactically coherent programs reference game board ﬁlter feature marks questions assume agent’s goal accurately identify current world state. general setting agent would require cost function deﬁnes helpfulness answer reduced distance goal. evaluate features important human-like question generation tested full model uses features well variants respectively lesioned property. information-agnostic model feig feig= thus ignored informativeness questions. complexity-agnostic model ignored complexity feature. type-agnostic model ignored answer type features. probabilistic model question generation evaluated main ways. first tasked predicting distribution questions people asked novel scenarios evaluate quantitatively. second tasked generating genuinely novel questions present data evaluate qualitatively. make predictions different candidate models contexts asked predict remaining results different model first verify compositionality essential ingredient account human question asking. given context human questions appear contexts. model attempts simply reuse/reweight past questions unable account productivity least without much larger training questions. grammar programs provides account productivity human behavior. second compared different models ability quantitatively predict distribution human questions. table summarizes model predictions based log-likelihood questions asked held-out contexts. full model learned features informativeness complexity answer type relevance provides best account data. case lesioning components resulted lower quality predictions. complexity-agnostic model performed worse others highlighting important role complexity understanding questions people choose ask. full model also outperformed information-agnostic type-agnostic models suggesting people also optimize information gain prefer certain question types log-likelihood values approximate bootstrapped estimate normalizing constant compared full model alternative. full model’s loglikelihood advantage complexity-agnostic model held bootstrap samples information-agnostic model samples type-agnostic model third considered overall match best-ﬁt model human question frequencies. figure shows correlations energy values according held-out predictions full model frequencies human questions results show strong agreement contexts along modest alignment others average spearman’s rank correlation coefﬁcient comparison information-agnostic model achieved complexity-agnostic model achieved type-agnostic model achieved limitation human data sparse thus correlations limited measure however surprisingly correlation question generation frequency alone suggesting role question complexity features. last model tasked generating novel human-like questions part human data set. figure shows novel questions sampled model across four different game contexts. questions produced taking weighted samples programs produced section approximate inference weights determined energy ensure novelty samples rejected equivalent human question training data already sampled question. equivalence questions determined mutual information answer distributions programs differed arguments equivalent generated questions figure demonstrate model capable asking novel human-like questions useful respective contexts. interesting questions observed human data include ships horizontal? what left ship tiles? blue purple ships touching purple touching what column left tiles color bottom right corner board? four contexts selected illustrate creative range model complete contexts shown supplementary materials. people question asking cognitive tool gain information world. although people rich interesting questions active learning algorithms make focused requests supervised labels. formalize computational aspects rich productive people inquire world. central hypothesis active machine learning concepts generalized operate complex compositional space programs evaluated possible worlds. project represents step toward capable active learning machines. also number limitations current approach. first system operates semantic representations rather natural language text directly although possible system interface recent tools computational linguistics bridge second aspects grammar speciﬁc battleship domain. often said knowledge needed good question critics approach point model begins substantial domain knowledge special purpose structures. hand many aspects grammar domain general rather domain speciﬁc including general functions programming constructs logical connectives operations arithmetic mapping. extend approach domains unclear exactly much knowledge engineering needed much preserved current architecture. future work bring additional clarity extend approach different domains. perspective computational cognitive science results show people balance informativeness complexity producing semantically coherent questions. formulating question asking program generation provide ﬁrst predictive model date open-ended human question asking. thank chris barker bowman noah goodman doug markant feedback advice. research supported grant bcs- john templeton foundation varieties understanding project john mcdonnell foundation scholar award mooresloan data science environment nyu. figure novel questions generated probabilistic model. across four contexts model questions displayed next informative human questions comparison. model questions sampled equivalent training set. natural language translations question programs provided interpretation. questions lower energy likely according model. references bordes weston. learning end-to-end goal-oriented dialog. arxiv preprint chouinard. children’s questions mechanism cognitive development. mono gureckis markant. active learning strategies spatial concept learning game. proceedings annual conference cognitive science society gureckis markant. self-directed learning cognitive computational johnson hariharan maaten hoffman fei-fei zitnick girshick. inferring executing programs visual reasoning. international conference computer vision mostafazadeh misra devlin mitchell vanderwende. generating natural questions image. proceedings annual meeting association computational linguistics pages roberts. information structure discourse towards integrated formal theory pragmatics. working papers linguistics-ohio state university department linguistics pages rothe lake gureckis. asking evaluating natural language questions. papafragou grodner mirman trueswell editors proceedings annual conference cognitive science society austin serban sordoni bengio courville pineau. building end-to-end dialogue systems using generative hierarchical neural network models. proceedings thirtieth aaai conference artiﬁcial intelligence settles. active learning. morgan claypool publishers strub harm mary piot courville pietquin. end-to-end optimization goal-driven visually grounded dialogue systems. international joint conference artiﬁcial intelligence wang berant liang. building semantic parser overnight. proceedings annual meeting association computational linguistics international joint conference natural language processing pages supplementary material contains following game boards served contexts human question data full grammatical rules used simulations novel questions context produced computational model table part grammatical rules deﬁning possible questions. based rewrite rules represent questions human question data set. text details. rules marked reference battleship game board rules domain-general table part novel question programs generated probabilistic model. model questions sampled ﬁltered novelty meaning never appeared training set. please main text details sampling process. context refers contexts figure si-. energy scores reﬂect human-like quality assigned model.", "year": 2017}