{"title": "Energy Clustering", "tag": ["stat.ML", "cs.CV", "cs.DS", "cs.LG", "math.ST", "stat.TH"], "abstract": "Energy statistics was proposed by Sz\\'{e}kely in the 80's inspired by the Newtonian gravitational potential from classical mechanics, and it provides a hypothesis test for equality of distributions. It was further generalized from Euclidean spaces to metric spaces of strong negative type, and more recently, a connection with reproducing kernel Hilbert spaces (RKHS) was established. Here we consider the clustering problem from an energy statistics theory perspective, providing a precise mathematical formulation yielding a quadratically constrained quadratic program (QCQP) in the associated RKHS, thus establishing the connection with kernel methods. We show that this QCQP is equivalent to kernel $k$-means optimization problem once the kernel is fixed. These results imply a first principles derivation of kernel $k$-means from energy statistics. However, energy statistics fixes a family of standard kernels. Furthermore, we also consider a weighted version of energy statistics, making connection to graph partitioning problems. To find local optimizers of such QCQP we propose an iterative algorithm based on Hartigan's method, which in this case has the same computational cost as kernel $k$-means algorithm, based on Lloyd's heuristic, but usually with better clustering quality. We provide carefully designed numerical experiments showing the superiority of the proposed method compared to kernel $k$-means, spectral clustering, standard $k$-means, and Gaussian mixture models in a variety of settings.", "text": "called energy distance rotationally invariant nonnegative equality zero holds above denotes euclidean norm energy distance provides characterization equality distributions metric spaces negative type exists hilbert space allows compute quantities related probability distributions associated hilbert space even though semimetric satisfy triangle inequality since shown proper metric. equivalence established ﬁrst recall deﬁnition rkhs. hilbert space real-valued functions function reproducing kernel satisﬁes following conditions words function unique reproduces inner product kernel function exists called rkhs. properties immediately imply symmetric indeed notice deﬁnition since inner product real statistic equality distributions assume data space negative type. consider disjoint partition expectation generalized energy distance computed large enough null hypothesis alternative hypothesis least note test make assumptions distributions thus said non-parametric distribution-free. deﬁnition remains show last constraint comes last constraint matrix form reads replacing d/e. multiplying last equation left noticing ﬁnally obtain therefore optimization problem well-known closed form solution columns rn×k contain eigenvectors corresponding largest eigenvalues rk×k arbitrary orthogonal matrix. resulting optimal objective function assumes value vertices second equation assume graph self-loops i.e. using energy statistics formulation allows make inference graphs. above weight node instance degree complexity analysis h-clustering following. computing gram matrix requires operations dimension data point data size. however algorithms h-clustering assume given. operations need performed single time. point need compute once need compute cost computing thus cost step algorithm entire dataset gives time complexity note cost l-clustering kernel k-means algorithm. again sparse reduced number nonzero entries main goal section threefold. first want compare h-clustering euclidean space k-means gmm. second want compare h-clustering based hartigan’s method l-clustering kernel k-means based lloyd’s method following experimental setup holds unless speciﬁed otherwise. consider hclustering l-clustering spectral clustering following semimetrics correincreases yielding optimal accuracy sample points trial. results shown fig. spectral clustering practically performance higher l-clustering moreover optimal accuracy based bayes classiﬁcation error increase sample size show accuracy versus diﬀerent kernels within h-clustering algorithm compared k-means gmm. results", "year": 2017}