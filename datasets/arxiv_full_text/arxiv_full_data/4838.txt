{"title": "Backprop KF: Learning Discriminative Deterministic State Estimators", "tag": ["cs.LG", "cs.AI"], "abstract": "Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on synthetic tracking task with raw image inputs and on the visual odometry task in the KITTI dataset. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.", "text": "generative state estimators based probabilistic ﬁlters smoothers popular classes state estimators robots autonomous vehicles. however generative models limited capacity handle rich sensory observations camera images since must model entire distribution sensor readings. discriminative models suffer limitation typically complex train latent variable models state estimation. present alternative approach parameters latent state distribution directly optimized deterministic computation graph resulting simple effective gradient descent algorithm training discriminative state estimators. show procedure used train state estimators complex input camera images must processed using expressive nonlinear function approximators convolutional neural networks. model viewed type recurrent neural network connection probabilistic ﬁltering allows design network architecture particularly well suited state estimation. evaluate approach synthetic tracking task image inputs visual odometry task kitti dataset. results show signiﬁcant improvement standard generative approaches regular recurrent neural networks. state estimation important component mobile robotic applications including autonomous driving ﬂight generative state estimators based probabilistic ﬁlters smoothers popular classes state estimators. however generative models limited ability handle rich observations camera images since must model full distribution sensor readings. makes difﬁcult directly incorporate images depth maps high-dimensional observations. instead popular methods vision-based state estimation based domain knowledge geometric principles. discriminative models need model distribution sensor readings complex train state estimation. discriminative models crfs typically latent variables means training data must contain full state observations. real-world state estimation problem settings provide partial labels. example might observe noisy position readings sensor need infer corresponding velocities. discriminative models augmented latent state typically makes harder train. propose efﬁcient scalable method discriminative training state estimators. instead performing inference probabilistic latent variable model instead construct deterministic computation graph equivalent representational power. computation graph optimized end-to-end simple backpropagation gradient descent methods. corresponds type recurrent neural network model architecture network informed structure probabilistic state estimator. aside simplicity training procedure advantages approach ability incorporate arbitrary nonlinear components observation transition functions. example condition transitions camera images processed multiple convolutional layers shown remarkably effective interpreting camera images. entire network including observation transition functions trained end-to-end optimize performance state estimation task. main contribution work draw connection discriminative probabilistic state estimators recurrent computation graphs thereby derive discriminative deterministic state estimation method. point view probabilistic models propose method training expressive discriminative state estimators reframing representationally equivalent deterministic models. point view recurrent neural networks propose approach designing neural network architectures well suited state estimation informed successful probabilistic state estimation models. evaluate approach visual tracking problem requires processing images handling severe occlusion estimating vehicle pose images kitti dataset results show signiﬁcant improvement standard generative methods standard recurrent neural networks. successful methods state estimation probabilistic generative state space models based ﬁltering smoothing kalman ﬁlters perhaps best known state estimators extended case nonlinear dynamics linearization unscented transform. nonparametric ﬁltering methods particle ﬁltering also often used tasks multimodal posteriors. complete review state estimation refer reader standard references topic generative models estimate distribution state observation sequences originating underlying hidden state typically taken state space system. becomes impractical observation space extremely high dimensional observation complex highly nonlinear function state case vision-based state estimation corresponds image viewed robot’s on-board camera. challenges generative state space estimation mitigated using complex observation models approximate inference building effective generative models images remains challenging open problem. alternative generative models discriminative models conditional random ﬁelds directly estimate number crfs conditional state space models applied state estimation typically using log-linear representation. recently discriminative ﬁne-tuning generative models nonlinear neural network observations well direct training crfs neural network factors allowed training nonlinear discriminative models. however models extensively applied state estimation. training crfs cssms typically requires access true state labels generative models require observations often makes convenient physical systems true underlying state unknown. although crfs also combined latent states difﬁculty inference makes latent state models difﬁcult train. prior work also proposed optimize parameters respect discriminative loss contrast work approach incorporates rich sensory observations including images allows training highly expressive discriminative models. method optimizes state estimator deterministic computation graph analogous recurrent neural network training. recurrent neural networks state estimation explored several prior works generally limited simple tasks without complex sensory inputs images. part reason difﬁculty training general-purpose rnns. recently innovative architectures successful mitigating problem models long short-term memory figure standard two-step engineering approach ﬁltering high-dimensional observations. generative part hidden state observations latter observation actually output second deterministic model denoted dashed lines trained explicitly predict computation graph jointly optimizes models consisting deterministic deterministic ﬁlter infers hidden state given viewing entire model single deterministic computation graph trained end-to-end using backpropagation explained section gated recurrent unit lstms combined vision perception tasks activity recognition however domain state estimation black-box models ignore considerable domain knowledge available. drawing connection ﬁltering recurrent networks design recurrent computation graphs particularly well suited state estimation shown evaluation achieve improved performance standard lstm models. performing state estimation generative model directly using high-dimensional observations camera images difﬁcult observations typically produced complex highly nonlinear process. however practice low-dimensional vector extracted fully capture dependence observation underlying state system. denote state denote labeling states wish able infer example might correspond pairs images camera automobile velocity location vehicle. case ﬁrst train discriminative model predict feedforward manner ﬁlter predictions output desired state labels example kalman ﬁlter hidden state could trained predicted observations perform inference test time. standard approach state estimation high-dimensional observations illustrated figure method viewed engineering solution without probabilistic interpretation advantage trained discriminatively entire model conditioned acting internal latent variable. model need represent distribution observations explicitly. however function maps observations low-dimensional predictions trained optimal state estimation. instead trained predict intermediate variable readily integrated generative ﬁlter. contribution based generalized view state estimation subsumes naïve piecewisetrained models discussed previous section allows trained end-to-end using simple scalable stochastic gradient descent methods. naïve approach observation function trained directly predict since standard generative ﬁlter model provide straightforward optimize respect accuracy ﬁlter labels however ﬁlter viewed computation graph unrolled time shown figure graph ﬁlter internal state deﬁned posterior example kalman ﬁlter gaussian posteriors represent internal state tuple general refer state ﬁlter. also augment graph output function outputs parameters distribution labels case kalman ﬁlter would simply matrix deﬁnes linear observation function viewing ﬁlter computation graph trained discriminatively entire sequence rather individually single time steps. loss function output distribution computation graph might example given pφyt distribution induced parameters loss entire sequence respect furthermore denote operation performed ﬁlter compute based zt+. compute gradient respect parameters ﬁrst recursively computing gradient loss respect ﬁlter state back front according following recursion parameters optimized gradient descent using gradients. instance backpropagation time well known algorithm training recurrent neural networks. recognizing connection state-space models recurrent neural networks allows extend generic ﬁltering architecture explore continuum models ﬁlters discriminatively trained observation model fully general recurrent neural networks. experimental evaluation standard kalman ﬁlter update nonlinear convolutional neural network observation function found provides good trade-off incorporating domain knowledge end-to-end learning task visual tracking odometry variants model could explored future work. section compare deterministic discriminatively trained state estimator alternative methods including simple feedforward convolutional networks piecewise-trained kalman ﬁlter fully general lstm models. evaluate models tasks require processing image input synthetic task tracking disk presence clutter severe occlusion; kitti visual odometry task proposed model call backprop kalman ﬁlter computation graph made kalman ﬁlter feedforward convolutional neural network distills observation low-dimensional signal serves observation neural network outputs point observation observation covariance matrix since network trained together ﬁlter learn covariance matrix communicate desired degree uncertainty observation maximize accuracy ﬁnal ﬁlter prediction. figure illustration computation graph bkf. graph composed feedforward part processes images outputs intermediate observations matrix used form positive deﬁnite observation covariance matrix recurrent part integrates time produce ﬁltered state estimates. appendix details. compare backprop three alternative state estimators feedforward model piecewise lstm model. simplest models feedforward model consider temporal structure task consists feedforward convolutional network takes observations outputs point estimate label approach viable label information directly inferred tracking object. hand tasks require long term memory visual odometry cannot solved plain feedforward network. piecewise model corresponds simple generative approach described section combines feedforward network kalman ﬁlter ﬁlters network predictions produce distribution state estimate ˆxt. piecewise model based computation graph optimize ﬁlter network together end-to-end instead training pieces separately. difference graphs piecewise implement additional pathway propagating uncertainty feedforward network ﬁlter instead ﬁlter needs learn handle uncertainty independently. example instantiation depicted figure detailed overview computational blocks shown ﬁgure deferred appendix finally compare recurrent neural network based lstm hidden units model resembles backprop except ﬁlter portion graph replaced generic lstm layer. lstm model learns dynamics data without incorporating domain knowledge present special aspect network design novel response normalization layer applied convolutional activations applying nonlinearity. response normalization transforms activations activations layer always mean variance regardless input layer. parameters learned along parameters. normalization used convolutional networks evaluation resembles batch normalization behavior. however found approach substantially effective recurrent models require backpropagation time compared standard batch normalization approach known require additional care applied recurrent networks. since proposed independently work gives in-depth analysis method. normalization followed rectiﬁed linear unit pooling layer. state estimation task meant reﬂect typical challenges visual state estimation need long-term tracking handle occlusions presence noise need process pixel data. task requires tracking disk image observations shown figure distractor disks random colors radii added scene occlude disk trajectories disks follow linear-gaussian dynamics linear spring force pulls disks toward center frame drag force prevents high velocities. disks temporally leave frame since contacts modeled. gaussian noise added perturb motion. model parameters assumed known design ﬁlter straightforward learn also model parameters. difﬁculty task adjusted increasing decreasing number distractor disks affects frequency occlusions. figure illustration consecutive frames training sequences. objective track disk throughout -frame sequence. distractor disks sampled sequence random overlaid target disk. upper illustrates easy sequence bottom sequence high difﬁculty note target rarely visible hardest sequences. easiest variants task solvable feedforward estimator hardest variants require long-term tracking occlusion. emphasize sample efﬁciency models trained using randomly sampled sequences. results table show outperforms standard probabilistic kf-based estimators powerful expressive lstm estimators. tracking error simple feedforward model signiﬁcantly larger occlusions model tends predict mean coordinates target occluded. piecewise model performs better observation covariance conditioned kalman ﬁlter learns large observation covariance forces rely almost entirely dynamics model predictions. hand since learns output observation covariances conditioned optimize performance ﬁlter able compromise observations dynamics model. finally although lstm model general performs worse since incorporate prior knowledge structure state estimation problem. test robustness estimator occlusions trained model training sequences varying amounts clutter occlusions. evaluated models several test sets corresponding different level occlusion clutter. tracking error test difﬁculty varied shown figure note even absence distractors lstm models outperform feedforward model since target occasionally leaves ﬁeld view. performance piecewise change signiﬁcantly difﬁculty increases high amount clutter training piecewise learns large observation covariance rely primarily feedforward estimates prediction. achieves lowest error nearly cases. time also dramatically fewer parameters lstm models since transitions correspond simple kalman ﬁlter updates. figure error various models trained single training contained sequences varying difﬁculty. models evaluated several test sets ﬁxed difﬁculty. figure example image sequence kitti dataset corresponding difference image obtained subtracting values previous image current image observation formed concatenating images six-channel feature treated input convolutional neural network. ﬁgure shows every ﬁfth sample original sequence illustrative purpose. next evaluated state estimation models visual odometry task kitti dataset publicly available training contains trajectories ego-centric video sequences passenger driving suburban scenes along ground truth position orientation. dataset challenging since relatively small learning-based algorithms trajectories visually diverse. training kalman ﬁlter variants used simpliﬁed state-space model three state variables corresponding vehicle’s pose forward angular velocities. dynamics model non-linear equipped model-based state estimators extended kalman ﬁlters straightforward addition framework. objective task estimate relative change pose ﬁxed-length subsequences. however inferring pose requires integration past observations simple feedforward model cannot used directly. instead trained feedforward network consisting four convolutional fully connected layers approximately half million parameters estimate velocities pairs images consecutive time steps. practice found better difference image corresponding change pixel intensities images along current image input feedforward network ground truth velocities used train piecewise well pretrain models computed ﬁnite differencing ground truth positions. recurrent models–piecewise lstm model–were ﬁne-tuned predict vehicle’s pose. additionally lstm model found crucial pretrain recurrent layer predict pose velocities ﬁne-tuning. evaluated model using -fold cross-validation report average errors held-out trajectories folds. trained models randomly sampling subsequences time steps. fold constructed test sets using held-out trajectory ﬁrst contains possible subsequences time steps second subsequences lengths repeated experiment using sequences training fold evaluate resilience method overﬁtting. second test aims mimic ofﬁcial test protocol. note however methods tested sequences ofﬁcial test directly comparable results ofﬁcial kitti benchmark. table lists cross-validation results. expected error decreases consistently number training sequences becomes larger. case outperforms variants predicting position heading vehicle. piecewise incorporate domain knowledge data-efﬁcient. indeed performance lstm degrades faster number training sequences decreased. although models trained subsequences time steps also tested containing mixture different sequence lengths. lstm model generally failed generalize longer sequences kalman ﬁlter variants perform slightly better mixed sequence lengths. paper proposed discriminative approach state estimation consists reformulating probabilistic generative state estimation deterministic computation graph. makes possible train method end-to-end using simple backpropagation time methods analogously recurrent neural network. evaluation present instance approach refer backprop corresponds kalman ﬁlter combined feedforward convolutional neural network processes image observations. approach state estimation beneﬁts. first avoid need construct generative state space models complex high-dimensional observation spaces images. second reformulating probabilistic state-estimator deterministic computation graph apply simple effective backpropagation stochastic gradient descent optimization methods learn model parameters. avoids usual challenges associated inference continuous nonlinear conditional probabilistic models still preserving representational power corresponding approximate probabilistic inference method experiments corresponds approximate gaussian posteriors kalman ﬁlter. approach also viewed application ideas probabilistic state-space models design recurrent neural networks. since optimize state estimator deterministic computation graph corresponds particular type deterministic neural network model. however architecture neural network informed principled well-motivated probabilistic ﬁltering models provides natural avenue incorporating domain knowledge system. experimental results indicate end-to-end training discriminative state estimators improve performance substantially compared standard piecewise approach discriminative model trained process observations produce intermediate lowdimensional observations integrated standard generative ﬁlter. results also indicate that although accuracy matched recurrent lstm network large number hidden units outperforms general-purpose lstm dataset limited size. fact incorporates domain knowledge structure probabilistic ﬁlters network architecture providing better inductive bias training data limited case many real-world robotic applications. experiments primarily focused models based kalman ﬁlter. however approach state estimation equally well applied probabilistic ﬁlters update equations written closed form including information ﬁlter unscented kalman ﬁlter particle ﬁlter well deterministic ﬁlters state observers moving average processes. long ﬁlter expressed differentiable mapping observation previous state state construct differentiate corresponding computation graph. interesting direction future work extend discriminative state-estimators complex nonlinear dynamics larger latent state. example could explore continuum models span space simple kf-style state estimators fully general recurrent networks. trade-off extremes generality domain knowledge striking right balance given problem could produce substantially improved results even relative modest amounts training data.", "year": 2016}