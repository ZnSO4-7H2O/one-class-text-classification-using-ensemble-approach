{"title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "tag": ["cs.CV", "cs.CL", "cs.LG", "cs.NE"], "abstract": "We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.", "text": "present neural encoder-decoder model convert images presentational markup based scalable coarse-to-ﬁne attention mechanism. method evaluated context imageto-latex generation introduce dataset real-world rendered mathematical expressions paired latex markup. show unlike neural techniques using ctcbased models attention-based approaches tackle non-standard task. approach outperforms classical mathematical systems large margin in-domain rendered data pretraining also performs well out-of-domain handwritten data. reduce inference complexity associated attention-based approaches introduce coarse-to-ﬁne attention layer selects support region applying attention. optical character recognition commonly used recognize natural language image; however early work anderson research interest converting images structured language markup deﬁnes text presentational semantics. primary target research mathematical expressions handle presentational aspects superscript notation special symbols nested fractions effective systems combine specialized character segmentation grammars underlying mathematical layout language prime example approach infty system used convert printed mathematical expressions latex markup formats other mostly proprietary sysproblems like require joint processing image text data recently seen increased research interest reﬁnement deep neural models domains. instance advances made areas handwriting recognition natural scenes image caption generation high-level systems learn abstract encoded representation input image decoded generate textual output. addition performing quite well standard tasks models entirely data driven makes adaptable wide range datasets without requiring heavy preprocessing domain speciﬁc engineering. however note tasks image captioning differ traditional mathematical task respects ﬁrst unlike image captioning traditional task assumes left-to-right ordering neural systems addressing problem primarily relied connectionist temporal classiﬁcation stroke-based approaches. second image captioning task theoretically allows systems focus attention anywhere thus directly test system’s ability maintain consistent tracking attention. work explore attention-based imageto-text models problem generating structured markup. consider whether supervised model learn produce correct presentational markup image without requiring textual visual grammar underlying markup language. model incorporates multi-layer convolutional network image attention-based recurrent neural network decoder. adapt model problem capture document’s layout also incorporate source encoder layer form multi-row recurrent model part encoder. modeling contributions twofold. first show assumptions like left-to-right ordering inherent ctc-based models required neural since general-purpose encoders provide necessary trackfigure example model generating mathematical markup. model generates latex symbol time based input image gray lines highlight grid features formed encoder cnn’s output. dotted lines indicate center mass token blue cell indicates support selected coarse-level attention symbol cells indicate ﬁne-level attention. white space around image trimmed visualization. actual size blue mask http//lstm.seas.harvard.edu/latex/ complete interactive version visualization test set. accurate attention second order reduce attention computation overhead introduce novel two-layer hard-soft approach attention call coarse-to-ﬁne attention inspired coarse-to-ﬁne inference graphical models. sparse memory conditional computation neural networks also explored various levels success several previous works demonstrate coarse-toﬁne method trained reinforce signiﬁcantly reduces overhead attention leads small drop accuracy. make experiments possible also construct public dataset imlatex-k consists large collection rendered real-world mathematical expressions collected published articles. dataset provides challenging test-bed image-to-markup task based reconstructing mathematical markup rendered images originally written scientists. model trained generate latex markup goal rendering exact source image. experiments compare output model several research commercial baselines well ablations models. full system mathematical expression generation able reproduce image real-world test examples. additionally multi-row encoder leads signiﬁcant increase performance. also experiment training simulated handwritten version dataset recognize handwritten textual expressions. even small in-domain training model able deﬁne image-to-markup problem converting rendered source image target presentational markup fully describes content layout. source consists image. target consists sequence tokens y··· length output token markup language. rendering deﬁned possibly unknown many-to-one compile function compile. practice function quite complicated browser ill-speciﬁed e.g. latex language. supervised task learn approximately invert compile function using supervised examples behavior. assume given instances possibly differing dimensions that compile training pairs test time system given input rendered ground-truth generates hypothesis rendered black-box function compile. evaluation done i.e. produce similar rendered images similar ground-truth markup contrary past work neural model uses full grid encoder input image support left-to-right order generated markup. base model adapted encoder developed image captioning. notably though model also includes encoder helps performance system. encoder image captioning features used however important encoder localize relative positions within source image. past work localization handled effect partitions source regions. instead implicitly allow encoder localize input running rnns rows features. extension turns crucial performance. formally recurrent neural network parameterized function recursively maps input vector hidden state hidden state. time hidden state updated input following manner initial state. practice many different variants rnn; however long short-term memory networks shown effective tasks. simplicity describe model experiments lstm networks. model feature grid created running across input. recursively rows columns features deﬁned rnn. order capture sequential order information vertical direction trainable initial hidden state refer positional embeddings. decoder target markup tokens {yt} generated decoder based grid decoder trained conditional language model give probability next token given history annotations. language model deﬁned decoder tanh) wout learned linear transformations. vector used summarize decoding history rnn). context vector used capture context information annotation grid. describe compute next section. accuracy model dependent able track next current position image generating markup conveyed attentive context vector formally deﬁne latent categorical variable {··· {··· denote cell model attending assume access attention distribution context deﬁned figure network structure. given input image applied extract feature feature employ encode spatial layout information. encoded features used decoder visual attention mechanism produce ﬁnal outputs. clarity show encoding ﬁrst decoding step. section consider variants model another encoder applied feature extract coarse features used select support region ﬁne-grained features indicated blue masks. model ﬁrst extracts image features using convolutional neural network arranges features grid. encoded using recurrent neural network encoded features used decoder visual attention mechanism. decoder implements conditional language model vocabulary whole model trained maximize likelihood observed markup. full structure illustrated figure convolutional network visual features image extracted multi-layer convolutional neural network interleaved max-pooling layers. network architecture standard; model speciﬁcally after network used images unlike recent work ﬁnal fully-connected layers since want preserve locality features order visual attention. takes input produces feature grid size denotes number channels resulted feature height width. coarse-to-fine attention ideally could consider reduced possible coarse cells hierarchical attention reduce time complexity. borrowing name coarse-toﬁne inference experiment methods construct coarse attention sparse support reduce number attention cells consider. different approaches training sparse coarse distribution. probability simplex denotes number classes. sparsemax function computed efﬁciently projection shown produce sparser output standard softmax. nonzero entries returned sparsemax attention time complexity step practice suitably small. second approach hard attention approach shown work several image tasks take hard sample opposed considering full distribution. stochasticity objective longer differentiable. however stochastic networks trained using reinforce algorithm pose problem framework reinforcement learning treating agent’s stochastic action time log-likelihood symbol produced reward maximize total expected reward equivalently minimize negative expected reward loss. parameters precede nondifferentiable stochastic computation graph backpropagate gradient form gives unbiased estimate loss function gradient since decoder takes previous context vectors input time step action inﬂuences later rewards hence assume multiplicative discount rate future rewards reward practice gradient estimator noisy slow converge. following include moving average reward baseline timestep update tunable learning rate. subtract baselines rewards reduce figure shows example attention distribution step model. note several properties attention distribution image-to-text problem. important grid relatively small attention localize around current symbol. reason grid large practice support distribution quite small single markup symbol single region. noted above attention every time step requires expectation cells. therefore decoding complexity attention mechanism prohibitive applied large images. hierarchical attention producing target symbol image infer rough region likely appear last generated symbol high probability. addition grid therefore also impose grid image cell belongs larger region. producing markup ﬁrst attend coarse grid relevant coarse cell attend inside cells context vector method known hierarchical attention. problem deﬁne coarse grid size construct running additional convolution pooling layers encoders also introduce latent attention variable indicates parent level cell attended cell write pt)pt) ﬁrst generate coarse-level cell followed ﬁne-level cell within parameterize part model. employ standard attention mechanism approximate probability time conditional also employ standard attention mechanism before except consider ﬁne-level cells within coarse-level cell note table speciﬁcation. ‘conv‘ convolution layer ‘pool max-pooling layer. number ﬁlters kernel size stride size padding size ‘po’ ‘bn’ batch normalization. sizes order train time sample update network stochastic gradients. test time take argmax coarse-level attentions choose attention time complexity single time step thus take attention complexity decoding step. experiment task constructed public dataset imlatex-k collects large-corpus real-world mathematical expressions written latex. dataset provides difﬁcult test-bed learning reproduce naturally occurring rendered latex markup. corpus imlatex-k dataset provides different latex math equations along rendered pictures. extract formulas parsing latex sources papers tasks contain papers. extract formulas latex sources regular expressions keep matches whose number characters fall range avoid single symbols text sentences. settings extract different formulas around rendered vanilla latex environment. rendering done pdﬂatex formulas fail compile excluded. rendered ﬁles converted format. ﬁnal dataset provide contains images resolution corresponding latex formulas. tokenization training model requires settling token set. option purely character-based model. method requires fewer assumptions character-based models would signiﬁcantly memory intensive word-based models longer target sequences. therefore original markup simply split minimal meaningful latex tokens e.g. observed characters symbols \\sigma modiﬁer characters functions accents environments brackets miscellaneous commands. finally note naturally occurring latex contains many different expressions produce identical output. therefore experiment optional normalization step eliminate spurious ambiguity normalization wrote latex parser convert markup abstract syntax tree. apply safe normalizing tree transformation eliminate common spurious ambiguity ﬁxing order sub-superscripts transforming matrices arrays. surprisingly additional step gives small accuracy gain necessary strong results. synthetic data handwriting recognition main results focus rendered markup also considered problem recognizing handwritten math. little labeled data task also synthetized handwritten corpus imlatex-k dataset. created data replacing individual symbols handwritten symbols taken detexify’s training data. formulas original dataset rendering symbol randomly pick corresponding handwritten symbol detexify. example synthesized handwriting shown figure note although images dataset look like handwritten formulas capture certain aspects varying baselines dataset pretraining step handwritten formulas recognition small labeled dataset. experiments compare proposed model refer imtex classical baselines neural models model ablations image-to-latex task. also compare proposed model commercial ocr-based mathematical expression recognition system inftyreader. inftyreader implementation infty system combining symbol recognition structural analysis phases. neural models natural comparison standard image captioning approaches ctcbased approaches simulate image captioning setup model caption removes encoder i.e. replacing increases number ﬁlters number parameters same. implementation designed natural image ocr. better understand role attention model several baseline experiments different attention styles. examine ﬁne-level features necessary experiment standard attention system coarse feature maps also twolayer hierarchical model. additionally experiment different coarse-to-ﬁne mechanisms hard reinforcement learning sparsemax. finally additional experiments comparing approach models handwritten mathematical expressions crohme shared tasks. training years consisting training expressions dataset different domain rendered images designed stroke-based ocr. handle differences employ extensions convert data images rendering strokes also augment data randomly resizing rotating symbols also employ simulated imlatexk handwriting dataset pretrain large out-of-domain model ﬁne-tune crohme dataset. core evaluation method check accuracy rendered markup output image compared true image main evaluation reports exact match rendering gold predicted images additionally check exact match accuracy original image well value eliminating whitespace columns. also include standard intrinsic text generation metrics conditional language model perplexity bleu score tokenized normalized gold data. practice found latex renderer often misaligns identical expressions several pixels. correct this misalignments pixels wide exact match errors. implementation details speciﬁcations summarized table note model uses single-layer lstms rnns. bi-directional encoder. hidden state encoder size decoder token embeddings size model standard attention million parameters models hierarchical coarse-to-ﬁne attention million parameters additional convolution layers encoders. mini-batch stochastic gradient descent learn parameters. standard attention models batch size initial learning rate halve validation perplexity decrease. train model epochs validation perplexity choose best model. hierarchical coarseto-ﬁne attention models batch size hard attention pretrained weights hierarchical initialize parameters. initial learning rate average reward baseline learning rate reward discount rate complete model trained end-to-end maximize likelihood training data. beyond training data model given information markup language generating process. generate markup unseen images beam search beam size test time. hard constraints employed. original images cropped formula area padded pixels left right bottom. efﬁciency downsample images half original sizes. facilitate batching group images similar sizes whitespace. images larger sizes latex formulas tokens cannot parsed ignored training validation included testing. main experimental results shown table compare different systems image-to-markup task. infty system able quite well terms text accuracy performs poorly exact match image metrics. poor results neural system validate expectation strict left-to-right order assumption unsuitable case. reimplementation imattention standard standard standard coarse-only hierarchical hard sparsemax standard hierarchical hard sparsemax standard hierarchical hard sparsemax table main experimental results imlatex-k dataset. reports bleu score compared tokenized formulas bleu score compared normalized formulas exact match accuracy exact match accuracy deleting whitespace columns. systems except imtex-tok trained normalized data. results crohme handwriting datasets. list best systems competition myscript valencia tuat myscript nates tuat. imtex systems out-of-domain synthetic data well small given training set. *note proprietary myscript system uses large corpus private in-domain handwritten training data. captioning caption better pushing number standard attention system imtex encoder increases value achieving high accuracy task. latex normalizer provides points accuracy gain achieves high normalized bleu. indicates decoder able learn well despite ambiguities real-world latex. indicating attention crucial performance. hand high performance hierarchical indicates layers soft-attention hurt performance model. table shows average number cells attended coarse layers models. hard reinforce system sparsemax reduce lookups small cost accuracy. hard aggressive selecting single coarse cell. sparsemax achieves higher accuracy cost selecting multiple coarse cells. depending application reasonable alternatives reduce number lookups standard attention. ﬁnal experiments look crohme datasets designed stroke recognition task closest existing dataset task. dataset ﬁrst train synthetic handwriting dataset ﬁne-tune crohme training set. models achieve comparable performance best systems excepting myscript commercial system access additional in-domain data. note synthetic dataset contain variation baselines font sizes table average number coarse attention computations models throughout test set. standard hierarchical provide upper-bound coarse-only lowerboard whereas hard always minimal lookups. test accuracy shown ease comparison. noise common real data. expect increased performance system trained well-engineered data. datasets also hierarchical coarse-to-ﬁne models similarly effective. interestingly contrary full data problems hard performs better sparsemax. analysis better understand contribution part standard imtex model ablation experiments removing different features model shown table simplest model basic ngram latex achieves perplexity around simply switching lstm-lm reduces value likely ability count parentheses nesting-levels. values quite indicating strong regularity latex alone. adding back image data reduces perplexity adding encoder lstm adds small gain makes large difference ﬁnal accuracy. adding positional embeddings provides tiny gain. hard attention leads small increase perplexity. also consider effect training data performance. figure shows accuracy system different training size using standard attention. many neural systems model quite data hungry. order model reach accuracy least training examples needed. finally figure illustrates several common errors. qualitatively system quite accurate difﬁcult latex constructs. typically structure expression preserved symbol recognition errors. common presentation-affecting errors come font sizing issues using small parentheses instead large ones using standard math font instead escaping using mathcal. order reduce attention complexity propose coarse-to-ﬁne attention layer selects region using coarse view image ﬁne-grained cells within. contributions provide view task structured text show data-driven models effective without knowledge language. coarse-to-ﬁne attention mechanism general directly applicable domains including applying proposed coarse-to-ﬁne attention layer tasks document summarization combining proposed model neural inference machines memory networks. would like thank daniel kirsch providing detexify data wiseman yoon helpful feedback paper. research supported bloomberg data science research award. references anderson robert syntax-directed recognition handprinted two-dimensional mathematics. symposium interactive systems experimental applied mathematics proceedings association computing machinery inc. symposium jimmy mnih volodymyr kavukcuoglu koray. multiple object recognition visual attention. proceedings international conference learning representations belaid abdelwaheb haton jean-paul. syntactic approach handwritten mathematical formula recogniieee transactions pattern analysis mation. chine intelligence bengio emmanuel bacon pierre-luc pineau joelle precup doina. conditional computation neural networks faster models. corr abs/. http//arxiv.org/abs/.. ciresan claudiu meier ueli gambardella luca maria schmidhuber j¨urgen. deep simple neural nets handwritten digit recognition. neural computation graves alex fern´andez santiago gomez faustino schmidhuber j¨urgen. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning jaderberg simonyan karen vedaldi andrea zisserman andrew. reading text wild convolutional neural networks. international journal computer vision karpathy andrej fei-fei deep visual-semantic proalignments generating image descriptions. ceedings ieee conference computer vision pattern recognition klein guillaume yoon deng yuntian senellart jean rush alexander opennmt open-source toolkit neural machine translation. arxiv preprint arxiv. mnih volodymyr heess nicolas graves alex koray kavukcuoglu. recurrent models visual attention. advances neural information processing systems mouchere harold viard-gaudin christian zanibbi richard garain utpal hwan hyung. icdar crohme third international competition recognition online handwritten mathematical expressions. international conference suzuki masakazu tamari fumikazu fukuda ryoji uchida seiichi kanahori toshihiro. infty integrated system mathematical documents. proceedings symposium document engineering vinyals oriol toshev alexander bengio samy erhan dumitru. show tell neural image caption proceedings ieee conference generator. computer vision pattern recognition kelvin jimmy kiros ryan kyunghyun courville aaron salakhudinov ruslan zemel rich bengio yoshua. show attend tell neural image caption generation visual attention. proceedings international conference machine learning mouchere harold viard-gaudin christian zanibbi richard garain utpal. icfhr competition recognition on-line handwritten mathematical expressions frontiers handwriting recognition international conference ieee nagabhushan alaei alireza. tracing straightening baseline handwritten persian/arabic text-line approach based painting-technique. international journal computer science engineering papineni kishore roukos salim ward todd wei-jing. bleu method automatic evaluation proceedings anmachine translation. nual meeting association computational linguistics association computational linguistics jack hunt jonathan danihelka harley timothy senior andrew wayne gregory graves alex lillicrap tim. scaling memory-augmented neural networks sparse reads writes. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. schulman john heess nicolas weber theophane abbeel pieter. gradient estimation using stochastic computation graphs. advances neural information processing systems shazeer noam mirhoseini azalia maziarz krzysztof davis andy quoc hinton geoffrey dean jeff. sparsely-gated mixture-of-experts layer. proceedings international conference learning representations baoguang xiang cong. end-toend trainable neural network image-based sequence recognition application scene text recognition. arxiv preprint arxiv. baoguang xiang cong. end-toend trainable neural network image-based sequence recognition application scene text recognition. ieee transactions pattern analysis machine intelligence", "year": 2016}