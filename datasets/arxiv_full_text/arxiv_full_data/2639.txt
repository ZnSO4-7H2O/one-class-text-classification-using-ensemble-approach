{"title": "Bayesian Optimization with Gradients", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.OC"], "abstract": "Bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.", "text": "bayesian optimization successful global optimization expensiveto-evaluate multimodal objective functions. however unlike optimization methods bayesian optimization typically derivative information. paper show bayesian optimization exploit derivative information good solutions fewer objective function evaluations. particular develop novel bayesian optimization algorithm derivative-enabled knowledgegradient one-step bayes-optimal asymptotically consistent provides greater one-step value information derivative-free setting. d-kg accommodates noisy incomplete derivative information comes sequential batch forms optionally reduce computational cost inference automatically selected retention single directional derivative. also compute d-kg acquisition function gradient using novel fast discretization-free technique. show d-kg provides state-of-the-art performance compared wide range optimization procedures without gradients benchmarks including logistic regression deep learning kernel learning k-nearest neighbors. bayesian optimization able global optima remarkably small number potentially noisy objective function evaluations. bayesian optimization thus particularly successful automatic hyperparameter tuning machine learning algorithms objectives extremely expensive evaluate noisy multimodal. bayesian optimization supposes objective function drawn prior distribution functions typically gaussian process maintaining posterior observe objective places. acquisition functions expected improvement upper conﬁdence bound predictive entropy search knowledge gradient determine balance exploration exploitation decide query objective next. choosing points largest acquisition function values seeks identify global optimum using objective function evaluations possible. bayesian optimization procedures generally leverage derivative information beyond exceptions described sect. contrast types continuous optimization methods gradient information extensively. broader gradients optimization suggests gradients also quite useful bayesian optimization gradients inform objective’s relative value function location well-aligned optimization. d-dimensional problems gradients provide distinct pieces information objective’s relative value direction constituting values query together objective value itself. advantage particularly signiﬁcant high-dimensional problems. derivative information available many applications little additional cost. recent work makes gradient information available hyperparameter tuning. moreover optimization engineering systems modeled partial differential equations pre-dates hyperparameter tuning applications adjoint methods provide gradients cheaply even derivative information readily available compute approximative derivatives parallel ﬁnite differences. paper explore what when bayesian optimization derivative information. also develop bayesian optimization algorithm effectively leverages gradients hyperparameter tuning outperform state art. algorithm accommodates incomplete noisy gradient observations used sequential batch settings optionally reduce computational overhead inference selecting single valuable directional derivatives retain. purpose develop acquisition function called derivative-enabled knowledge-gradient d-kg generalizes previously proposed batch knowledge gradient method frazier derivative setting replaces approximate discretization-based method calculating knowledge-gradient acquisition function novel faster exact discretization-free method. note discretization-free method also interest beyond derivative setting used improve knowledge-gradient methods problem settings. also provide theoretical analysis d-kg algorithm showing one-step bayes-optimal construction derivatives available; provides one-step value greater derivative-free setting mild conditions; estimator global optimum asymptotically consistent. numerical experiments compare state-of-the-art batch bayesian optimization algorithms without derivative information gradient-based optimizer bfgs full gradients. assume familiarity bayesian optimization recommend rasmussen williams shahriari review. section begin describing related work. sect. describe bayesian optimization algorithm exploiting derivative information. sect. compare performance algorithm several competing methods collection synthetic real problems. code paper available https//github.com/wujian/cornell-moe. osborne proposes fully bayesian optimization procedures derivative observations improve conditioning covariance matrix. samples taken near previously observed points derivative information update covariance matrix. unlike current work derivative information affect acquisition function. directly compare osborne within benchmark sect. lizotte incorporates derivatives bayesian optimization modeling derivatives rasmussen williams lizotte shows bayesian optimization expected improvement acquisition function complete gradient information sample outperform bfgs. approach differences allow noisy incomplete derivative information; develop novel acquisition function outperforms derivatives; enable batch evaluations; implement compare batch bayesian optimization derivatives across several acquisition functions benchmarks applications kernel learning logistic regression deep learning k-nearest neighbors revealing empirically gradient information valuable; provide theoretical analysis bayesian optimization derivatives; develop scalable implementation. recently koistinen uses derivative observations minimum energy path calculations atomic rearrangements ahmed studies expected improvement gradient observations. ahmed randomly selected directional derivative retained iteration computational reasons similar approach retaining single directional derivative though differs random selection contrast value-of-informationbased selection. approach complementary works. batch bayesian optimization several recent algorithms proposed choose points evaluate iteration within area approach handling batch observations closely related batch knowledge gradient frazier generalize approach derivative setting provide novel exact method computing knowledge-gradient acquisition function avoids discretization used frazier generalization improves speed accuracy also applicable knowledge gradient methods continuous search spaces. recent advances improving access derivatives computational tractability make bayesian optimization gradients increasingly practical timely discussion. sect. reviews general approach incorporating derivative information bayesian optimization. sect. introduces novel acquisition function d-kg based knowledge gradient approach utilizes derivative information. sect. computes acquisition function gradient efﬁciently using novel fast discretization-free approach. sect. shows algorithm provides greater value information derivative-free setting one-step bayes-optimal asymptotically consistent used discretized feasible space. derivative information given expensive-to-evaluate function wish argminx∈af domain optimization. place prior speciﬁed mean function kernel function ﬁrst suppose sample observe function value partial derivatives possibly independent normally distributed noise later discuss relaxation observing single directional derivative. since gradient linear operator gradient also function gradient follow multi-output mean function kernel function deﬁned below rd+≥ gives variance observational noise. known estimate data. posterior distribution refer mean function posterior samples kernel function suppose sampled points x··· observed observation consists function value gradient given observations incomplete remove rows columns corresponding partial derivatives observed. observe directional derivatives rows columns corresponding observations entries obtained noting directional derivative linear transformation gradient. propose novel bayesian optimization algorithm exploit available derivative information based knowledge gradient approach call algorithm derivative-enabled knowledge gradient algorithm proceeds iteratively selecting iteration batch points maximum value information suppose observed points recall section -dimensional vector giving posterior mean partial derivatives sect. discusses remove assumption values provided. expected value posterior distribution samples make irrevocable decision solution overarching optimization problem receive loss equal value chosen point would choose argminx∈a similarly made decision samples conditional expected loss would minx∈a therefore deﬁne d-kg factor given candidate points refer outer optimization problem. d-kg solves outer optimization problem using method described section d-kg acquisition function differs batch knowledge gradient acquisition function time depends ∇y). frazier posterior mean turn requires calculating distribution gradient observations time-n posterior marginalizing them. thus d-kg algorithm differs gradient observations change posterior also prospect future gradient observations changes acquisition function. additional major distinction frazier d-kg employs novel discretization-free method computing acquisition function fig. illustrates behavior d-kg d-ei example. d-ei generalizes expected improvement batch acquisition derivative information d-kg clearly chooses better point evaluate d-ei. including partial derivatives computationally prohibitive since inference scales overcome challenge retaining value derivative observations include directional derivative iteration inference. d-kg naturally decide derivative include adjust choice best sample given observe limited information. deﬁne d-kg acquisition function observing function value derivative direction figure refer acquisition functions without gradients. d-kg d-ei refer counterparts gradients. topmost plots show posterior surfaces function sampled dimensional without incorporating observations gradients. posterior variance smaller gradients incorporated; utility sampling point value information criteria settings. derivatives observed query point high potential gain hand gradients observed d-kg makes considerably better sampling decision whereas d-ei samples essentially location plots bottom depict posterior surface respective sample. interestingly beneﬁts observing gradients d-kg’s observation yields accurate knowledge optimum’s location d-ei’s observation leaves substantial uncertainty. efﬁcient exact computation d-kg calculating maximizing d-kg difﬁcult continuous term minx∈a requires optimizing continuous domain must integrate optimal value dependence θt∇y). previous work knowledge gradient continuous domains approaches computation taking minima within expectations full domain discretized ﬁnite approximation. approach supports analytic integration scott poloczek sampling-based scheme frazier however discretization approach introduces error scales poorly dimension propose novel method calculating unbiased estimator gradient d-kg within stochastic gradient ascent maximize d-kg. method avoids discretization thus exact. also improves speed signiﬁcantly discretization-based scheme. section supplement show d-kg factor expressed mean function θt∇f evaluations dimensional ﬁrst dimensional standard normal random column vector matrix related kernel function θt∇f evaluations exact form speciﬁed supplement. sufﬁcient regularity conditions interchange gradient expectation operators unbiased estimator ∇d-kg regularity conditions assumes hold. unbiased gradient estimator within stochastic gradient ascent optionally multiple starts solve outer optimization problem argmaxzθd-kg similar approach observing full gradients solve outer optimization problem learning rate louter bayesian treatment hyperparameters. adopt fully bayesian treatment hyperparameters similar snoek draw samples hyperparameters emcee package average acquisition function across obtain additional argument d-kg indicates computation performed conditioning hyperparameters experiments found method computationally efﬁcient robust although principled treatment unknown hyperparameters within knowledge gradient framework would instead marginalize computing present three theoretical results giving insight properties d-kg proofs supplementary material. sake simplicity suppose partial derivatives provided d-kg. similar results hold d-kg relevant directional derivative detection. begin stating value information obtained d-kg exceeds achieved derivative-free setting. proposition given identical posteriors next show d-kg one-step bayes-optimal construction. proposition iteration left observe function values partial derivatives d-kg bayes-optimal among feasible policies. complement one-step optimality show d-kg asymptotically consistent feasible ﬁnite. asymptotic consistency means d-kg choose correct solution number samples goes inﬁnity. theorem function sampled known hyperparameters d-kg algorithm asymptotically consistent i.e. evaluate performance proposed algorithm d-kg relevant directional derivative detection standard synthetic benchmarks moreover examine ability tune hyperparameters weighted k-nearest neighbor metric logistic regression deep learning spectral mixture kernel provide easy-to-use python package core written available https// github.com/wujian/cornell-moe. compare d-kg several state-of-the-art methods batch expected improvement method wang utilize derivative information extension incorporates derivative information denoted d-ei. d-ei similar lizotte handles incomplete gradients supports batches. batch gp-ucb-pe method contal utilize derivative information extension does. batch knowledge gradient algorithm without derivative information frazier moreover generalize method osborne batches evaluate benchmark. algorithms allow incomplete gradient observations. benchmarks provide full gradient additionally compare gradient-based method l-bfgs-b provided scipy. suppose objective function drawn gaussian process constant mean function squared exponential kernel. sample sets hyperparameters emcee package recall immediate regret deﬁned loss respect global optimum. plots synthetic benchmark functions shown fig. report immediate regret solution algorithm would pick function number function evaluations. plots experiments report objective value solution instead immediate regret. error bars give mean value plus minus standard deviation. number replications stated benchmark’s description. evaluate methods test functions chosen bingham demonstrate ability beneﬁt noisy derivative information sample additive normally distributed noise zero mean standard deviation objective function partial derivatives. unknown algorithms must estimated observations. also investigate incomplete gradient observations affect algorithm performance. also experiment different batch sizes batch size branin rosenbrock ackley functions; otherwise batch size fig. summarizes experimental results. functions full gradient information. branin domain ackley hartmann function assume full gradient available. looking results branin function fig. d-kg outperforms competitors function evaluations obtains best solution overall bfgs makes faster progress bayesian optimization methods ﬁrst evaluations subsequently stalls fails obtain competitive solution. ackley function d-ei makes fast progress ﬁrst evaluations also fails make subsequent progress. conversely d-kg requires evaluations improve performance d-ei d-kg achieves best overall performance again. hartmann function d-kg clearly dominates competitors function evaluations. functions incomplete derivative information. rosenbrock function provide noisy observation third partial derivative. d-ei stuck early. d-kg hand ﬁnds near-optimal solution function evaluations; without derivatives catches evaluations performs comparably afterwards. levy benchmark fourth partial derivative observable noise shows different ordering algorithms best performance beating even formulation uses derivative information. explanation could smoothness regular shape function surface beneﬁts acquisition criteria. cosine mixture function provide noisy partial derivatives. d-kg derivatives perform better ei-type criterion achieve best performances d-kg beating derivatives slightly. general d-kg successfully exploits noisy derivative information best overall performance. figure average performance replications d-kg performs signiﬁcantly better competitors benchmarks except levy funcion. branin hartmann also plot black lines performance bfgs. benchmark tune weighted k-nearest neighbor metric optimize predictions durations based historical data. trip described pick-up time pick-up location drop-off point estimate duration obtained weighted average trips database happened time interval durationi weight). weight trip prediction given respective parameter values trip tunable hyperparameters. thus hyperparameters tune choose yellow public data june sampling records june training data trip records june validation data. test criterion root mean squared error compute partial derivatives validation dataset respect hyperparameters hyperparameter differentiable. fig. d-kg overtakes alternatives acquisition functions also beneﬁt exploiting derivative information. kernel learning. spectral mixture kernels used ﬂexible kernel learning enable long-range extrapolation. kernels obtained modeling spectral density mixture gaussians. stationary kernel described spectral mixture kernel particular setting hyperparameters initializing learning parameters difﬁcult. although access analytic closed form objective function expensive evaluate highly multimodal. moreover derivative information available. thus learning ﬂexible kernel functions perfect candidate approach. task train -component spectral mixture kernel airline data must determine mixture weights means variances gaussians. fig. summarizes performance batch size bfgs sensitive initialization human intervention often trapped local optima. d-kg hand consistently ﬁnds good solution obtains best solution algorithms overall observe gradient information highly valuable performing kernel learning task. logistic regression deep learning. tune logistic regression feedforward neural network hidden layers mnist dataset standard classiﬁcation task handwritten digits. training contains images test tune hyperparameters logistic regression regularization parameter learning rate mini batch size training epochs ﬁrst derivatives ﬁrst parameters obtained technique maclaurin neural network additionally tune number hidden units fig. reports mean standard deviation mean cross-entropy loss test replications. d-kg outperforms approaches suggests derivative information helpful. algorithm proves value tuning deep neural network harmonizes research computing gradients hyperparameters figure results weighted benchmark spectral mixture kernel benchmark logistic regression deep neural network batch size averaged replications. bayesian optimization successfully applied dimensional problems wish good solution small number objective function evaluations. considered several benchmarks well logistic regression deep learning kernel learning k-nearest neighbor applications. shown context derivative information extremely useful greatly decrease number objective function evaluations especially building upon knowledge gradient acquisition function even derivative information noisy available variables. bayesian optimization increasingly used automate parameter tuning machine learning objective functions extremely expensive evaluate. example parameters learn bayesian optimization could even hyperparameters deep neural network. expect derivative information bayesian optimization help enable promising applications moving towards fully automatic principled approaches statistical machine learning. future could combine derivative information ﬂexible deep projections recent advances scalable gaussian processes training test time predictions steps would help make bayesian optimization applicable much wider range problems wherever standard gradient based optimizers used even analytic objective functions expensive evaluate retaining faster convergence robustness multimodality.", "year": 2017}