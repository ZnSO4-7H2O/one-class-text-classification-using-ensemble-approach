{"title": "DiGrad: Multi-Task Reinforcement Learning with Shared Actions", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "Most reinforcement learning algorithms are inefficient for learning multiple tasks in complex robotic systems, where different tasks share a set of actions. In such environments a compound policy may be learnt with shared neural network parameters, which performs multiple tasks concurrently. However such compound policy may get biased towards a task or the gradients from different tasks negate each other, making the learning unstable and sometimes less data efficient. In this paper, we propose a new approach for simultaneous training of multiple tasks sharing a set of common actions in continuous action spaces, which we call as DiGrad (Differential Policy Gradient). The proposed framework is based on differential policy gradients and can accommodate multi-task learning in a single actor-critic network. We also propose a simple heuristic in the differential policy gradient update to further improve the learning. The proposed architecture was tested on 8 link planar manipulator and 27 degrees of freedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2 end effectors respectively. We show that our approach supports efficient multi-task learning in complex robotic systems, outperforming related methods in continuous action spaces.", "text": "reinforcement learning scenario. however recent advancements successfully combined deep neural networks stabilized learning process. deep networks used convolutional neural networks fully connected layers make agents learn play atari games. following success several improvements architecture like double prioritized replay duelling network proposed propelled deep multi-agents. proposed deep deterministic policy gradient continuous control tasks extended scope deep applications robotics. robotic systems like open closed kinematic chains applied ddpg framework learn manipulation tasks complex environments. following this applied ddpg learn reachability tasks humanoid robot. even though multi-agent systems posed multi-tasking systems shared actions shown fig. spine/torso common chain contributes reachability tasks hands humanoid robot. paper propose novel framework called digrad based differential policy gradients learn multi-tasking robotic systems different tasks share common actions. reinforcement learning algorithms inefﬁcient learning multiple tasks complex robotic systems different tasks share actions. environments compound policy learnt shared neural network parameters performs multiple tasks concurrently. however compound policy biased towards task gradients different tasks negate other making learning unstable sometimes less data efﬁcient. paper propose approach simultaneous training multiple tasks sharing common actions continuous action spaces call digrad proposed framework based differential policy gradients accommodate multi-task learning single actor-critic network. also propose simple heuristic differential policy gradient update improve learning. proposed architecture tested link planar manipulator degrees freedom humanoid learning multi-goal reachability tasks effectors respectively. show approach supports efﬁcient multi-task learning complex robotic systems outperforming related methods continuous action spaces. introduction increasing demand reinforcement learning ﬁelds robotics intelligent systems. reinforcement learning deals learning actions given environment achieve goal. classic reinforcement learning techniques make linear approximation tabular methods learn correlation. advancements deep neural networks recent times learning non-linear approximations feature extraction becomes much simpler. believed non-linear approximators like neural network hard train however classical methods based jacobian limited capability branched manipulators. methods like augmented jacobian constrained solution spaces methods based optimization often doesn’t provide real time control. hence based solvers great domain sample entire solution space learn real time controller complex robotic systems. direction learning multiple tasks scenarios ddpg learn compound policy taking care tasks. however found ddpg unstable multi-task scenarios. digrad addresses problems using differential policy gradient updates. test framework branched manipulators shown fig. learning reachability tasks multiple effectors simultaneously. proposed framework shows substantial improvement ddpg considerably robust experiments conducted. rest paper organised follows. section discusses related works background. section explains mathematical background behind proposed framework provides detailed algorithm. finally section contain experimental results discussion respectively. related works multi-task reinforcement learning algorithms rely shows transfer learning approaches. good collection methods. recent works based approach works explored learning universal abstractions state-action pairs feature successors. apart transfer learning works like investigated joint training multiple value functions polideep neural network setting cies. provided framework simultaneous training multiple stochastic policies distilled master policy. unlike work uses multiple networks policy network distilled policy. work show single network learn multiple deterministic policies simultaneously. mentioned methods assume multi-agent scenario whereas paper concentrate learning multiple tasks robotic system. recent works scenario works talk actions shared among different tasks thus limiting applicability. unlike frameworks explore case multi-task learning branched manipulator shared actionspaces. background consider standard reinforcement learning setup consisting agent interacting environment discrete time steps. time step agent takes state input performs action according policy receives reward assume fully observable environment model markov decision process state space action space initial state distribution state transition dynamics reward function goal reinforcement learning learn policy discount factor. since return depends action taken policy stochastic work consider deterministic policies. hence discounted state visitation distribution policy denoted action-value function used many reinforcement learning algorithms described expected return taking action state thereafter following given policy ddpg policy learning algorithm uses actorcritic based framework continuous control tasks. ddpg actor critic approximated using neural networks problem instability training addressed using target networks experience replay using replay buffer. setting critic network weights optimized minimizing following loss proposed framework basic concepts ddpg theorem derive policy gradient update support robust multi-task learning. finally explain learning improved using simple heuristic case shared actions. digrad differential policy gradients propose framework simultaneous reinforcement learning multiple tasks shared actions continuous domain. method based differential action-value updates actor-critic based framework using theorem. framework learns compound policy optimally performs shared tasks simultaneously. fig. shows higher level description method. section describe mathematical framework behind work. penultimate layer weights updated based reward obtained performing corresponding action remaining shared network captures correlation different actions hence kind parametrization useful case shared actions. apart this number parameters significantly reduced single network. critic update consider single actor-critic based framework critic given function approximators parametrized actor parametrized corresponding target networks parametrized since multiple critic outputs optimize minimizing loss given differential policy gradient task corresponding reward hence learn tasks need maximize expected reward task respect corresponding action therefore performance objective maximized environment setting consider tasks standard setting corresponding action spaces assume state space across tasks inﬁnite horizon discount factor compound action space performs given tasks simultaneously. denote compound actions denote actions denote states. therefore relation compound actions actions given reward functions task depend corresponding actions therefore denote reward function task corresponding action value function. compound deterministic policy task-speciﬁc deterministic policies; therefore proposed framework actor-critic based framework proposed multi-task learning given environment setting. compound policy parametrized instead multiple policy networks policy simple parametrization action-value functions would separate network outputs action-value corresponding task. another approach modelling action-value function single network parametrized outputs action-values tasks. here action value corresponding task. showed single critic network sufﬁcient multi-policy learning. setting easily extended cases shared tasks. framework accommodate heterogeneous dependent action spaces compared related multi-task algorithms assume action spaces homogeneous independent both. demonstrates wider applicability framework. algorithm section explain algorithm learn multiple tasks using digrad. algorithm similar standard ddpg signiﬁcant differences terms critic actor updates shown previous subsections. digrad compound action executed environment returns vector rewards corresponding task instead single reward. replay buffer stores current state compound action observed state executing action vector rewards entire algorithm shown algorithm experiments results proposed framework tested different settings order analyse advantages setting. considered four different network settings digrad follows single critic network heuristics single critic network without heuristics multi critic network heuristics multi critic network without heuristics. second term r.h.s zero action spaces disjoint hence framework used even shared actions. since update actor gradients different action values call differential gradient update. different standard gradient update actor updated based single action value heuristic direction policy gradient update policy shared action taken gradients action values corresponding tasks affects whereas policy execute action corresponding reward vector updated state st+. store transition st+) replay buffer randomly sample mini-batch replay buffer update critic according eqs. update actor policy according update target networks compare standard ddpg setting. hyper parameters settings. critic network architecture single multiple critic case aspects except number outputs. actor network parameters also cases. show comparison average reward well mean scores task plots. note average reward curves ddpg shown reward function settings ddpg different digrad. order test proposed multi-task learning framework considered different environments. environments training involved learning reachability tasks effectors simultaneously i.e. learning policy joint space trajectories reach point workspace. experiments deﬁne error score particular task experiments agents implemented using tensorflow code base consisting fully connected layers. rmsprop optimizer used train actor critic networks learning rates respectively. used crelu activation hidden layers. training normally distributed decaying noise function used exploration. whereas testing noise omitted. discount factor settings. tasks low-dimensional state description include joint angles joint positions goal positions. actor output angular velocities hence policy learns mapping conﬁguration space joint velocity space. reward function reward function digrad settings modelled keeping mind multi-task application. deﬁned before reward corresponding action task. give small positive reward task ﬁnished. also effectors reach respective goals positive reward given tasks. cases negative reward proportional error given. ddpg setting single reward unlike digrad. positive reward given effectors reach goals simultaneously. else negative reward given proportional error tasks distances respective goal corresponding effector. environments -link manipulator ﬁrst environment planar manipulator effectors shown fig. observe shared sub-chain joints common tasks. also action dimension non-shared chains kept different order check robustness framework. dimensions action given fig. shows performance curves different settings. mean score curves digrad based settings converge faster achieve better ﬁnal performance ddpg. even though action dimension task different network settings digrad framework worked equally well tasks. whereas ddpg showed comparable performance task-. single critic framework consistently better average reward episode multi critic frameworks. thus modelling action value functions critic network doesn’t affect performance. fact shared parameters single critic framework could help network capture correlation among actions much better multi-critic case. note that single critic framework achieving performances lesser parameters multi-critic framework. digrad frameworks heuristics perform non-heuristic frameworks. applying aforementioned heuristic signiﬁcant improvement average reward curve observed. mean score curves specially task- task- curve application heuristics helps network stable compared respective non-heuristic curves. thus normalising gradient action values shared action could help network deliver robust multifigure performance curves reachability task experiments link manipulator humanoid. bold line shows average runs coloured areas show average standard deviation tasks. note that average reward curve plotted ddpg reward function different digrad frameworks. task training. humanoid robot secondly test framework humanoid robot experiment involved reachability tasks hands humanoid robot using upper body consisting articulated torso. articulated torso shared chain affecting tasks. noteworthy articulated torso whereas arms each. thus contribution shared action task shared actions environment training developed matlab trained models tested dynamic simulation environment mujoco. fig.b summarizes results case. found dppg generally unstable solving multi-tasks problems. runs learn initially suffers degradation later. observe digrad algorithm yields better ﬁnal results greater stability. mean scores tasks single critic frameworks converge faster stable throughout experiment compared multi-critic frameworks. best setting single critic heuristic outperforming others cases. note that reward function ddpg kept different digrad framework. also tried different reward setting taking individual rewards reward signal ddpg framework deﬁned digrad reward setting. observed conclusion paper propose deep reinforcement learning algorithm called digrad multi-task learning single agent. framework based ddpg algorithm derived theorem. framework dedicated action value task whose update depends action reward corresponding task. introduced differential policy gradient update compound policy. tested proposed framework learning reachability tasks environments namely -link manipulator humanoid. experiments show framework gave better performance ddpg terms stability robustness accuracy. performances achieved keeping number parameters comparable ddpg framework. also algorithm learns multiple tasks without decrease performance task. work focuses learning coordinated multi-actions ﬁeld robotics single agent performs multiple tasks simultaneously. framework able capture relation among tasks tasks overlapped action space. future work focus extending framework multi-agent domain. introduction inverse kinematics jacobian transpose pseudoinverse damped least squares methods. ieee journal robotics automation stefano chiaverini. christos dimitrakakis constantin rothkopf. bayesian multitask inverse european workshop reinforcement reinforcement learning pages springer shixiang ethan holly timothy lillicrap sergey levine. deep reinforcement learning robotic manipulation asynchronous off-policy updates. robotics automation ieee international conference pages ieee charles klein caroline chu-jenq shamim ahmed. formulation extended jacobian method mapping algorithmic singuieee larities kinematically redundant manipulators. transactions robotics automation karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement integrating adtemporal abstraction intrinsic motivation. vances neural information processing systems pages alessandro lazaric mohammad ghavamzadeh. bayesian multi-task reinforcement learning. icml-th international conference machine learning pages omnipress timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous conarxiv preprint trol deep reinforcement learning. arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control guhan phaniteja dewangan madhava sarkar. deep reinforcement learning approach dynamically stable inverse kinematics humanoid robots. arxiv preprint arxiv. andrei rusu sergio gomez colmenarejo caglar gulcehre guillaume desjardins james kirkpatrick razvan pascanu volodymyr mnih koray kavukcuoglu raia hadsell. policy distillation. arxiv preprint arxiv. schaul david silver lever nicolas heess thomas degris daan wierstra martin riedmiller. deterministic policy gradient algorithms. icml richard sutton andrew barto. reinforcement learning introduction volume press cambridge victor bapst wojciech czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu. distral robust multitask reinforcement learning. advances neural information processing systems pages ziyu wang schaul matteo hessel hado hasselt marc lanctot nando freitas. dueling network architectures deep reinforcement learning. arxiv preprint arxiv. zhaoyang yang kathryn merrick hussein abbass lianwen jin. multi-task deep reinforcement learning continuous action control. proceedings twenty-sixth international joint conference artiﬁcial intelligence ijcai- pages haiyan sinno jialin pan. knowledge transfer deep reinforcement learning hierarchical experience replay. aaai pages jingwei zhang jost tobias springenberg joschka boedecker wolfram burgard. deep reinforcement learning successor features navarxiv preprint igation across similar environments. arxiv.", "year": 2018}