{"title": "Nested Dictionary Learning for Hierarchical Organization of Imagery and  Text", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "A tree-based dictionary learning model is developed for joint analysis of imagery and associated text. The dictionary learning may be applied directly to the imagery from patches, or to general feature vectors extracted from patches or superpixels (using any existing method for image feature extraction). Each image is associated with a path through the tree (from root to a leaf), and each of the multiple patches in a given image is associated with one node in that path. Nodes near the tree root are shared between multiple paths, representing image characteristics that are common among different types of images. Moving toward the leaves, nodes become specialized, representing details in image classes. If available, words (text) are also jointly modeled, with a path-dependent probability over words. The tree structure is inferred via a nested Dirichlet process, and a retrospective stick-breaking sampler is used to infer the tree depth and width.", "text": "tree-based dictionary learning model developed joint analysis imagery associated text. dictionary learning applied directly imagery patches general feature vectors extracted patches superpixels image associated path tree multiple patches given image associated node path. nodes near tree root shared multiple paths representing image characteristiccommon among diﬀerent types images. moving toward leaves nodes become specialized representing details image classes. available words also jointly modeled path-dependent probability words. tree structure inferred nested dirichlet process retrospective stick-breaking sampler used infer tree depth width. statistical topic models latent dirichlet allocation originally developed text analysis recently transitioned successfully analysis imagery. topic models image associated distribution topics topic characterized distribution observed features image. setting researchers typically represent image visual words methods applied perform unsupervised clustering classiﬁcation annotation images using image features work feature extraction performed pre-processing step local image descriptors e.g. scale-invariant feature transform types features commonly used extract features local patches segments superpixels diﬀerent images related another corresponding distributions topics. several limitations previous work. first vector quantization typically applied image features codes play role words traditional topic modeling. loss information quantization step must tune number codes secondly feature design typically performed separately subsequent image topic modeling. finally image-based topic modeling performed single scale level thereby accounting hierarchical characteristics natural imagery. recent work papers addressed particular aspects limitations none addressed all. example authors employed dictionarylearning framework eliminating need perform dictionary learning could applied traditional features pre-computed image could applied directly patches imagery thereby ameliorating requirement separating feature-design topic-modeling steps. however consider hierarchical characxmi represented sparse linear combination learned dictionary atoms. further patch assumed associated topic; probability dictionary atoms employed given patch dictated topic associated with. indicator variable deﬁnes topic associated xmi. k-dimensional vector deﬁnes probability columns employed represent topic component πhk. probability vectors drawn represents probability using object type introduction discussions below. representation correspondapproximation beta-bernoulli process also yields approximation indian buﬀet process imagery. recently employed nested chinese restaurant process infer hierarchical tree representation corpus images accompanying text; however work step still employed therefore precomputation features well. further tree width inferred depth set. finally ncrp construction disadvantage updating parent-child-transition parameters node tree time sampler yielding poor mixing relative stick-breaking dirichlet process implementation related distinct dictionary learning ncrp considered zhang hierarchical tree structure corpus imagery text stick-breaking construction employed; infer tree depth width using retrospective stick-breaking construction adapted hierarchical model removing step allowing perform topic modeling directly image patches thereby integrating feature design topic modeling. however desired dictionary learning also applied features pre-computed image using existing method feature design removing limitations wish build hierarchical model arrange images associated annotations vocabulary annotations assumed dimension vector represents pixels features associated patch image represents vector word counts associated image available tree construction developed alternative means constituting type tree manifested nested chinese restaurant process emphasize construction stick-breaking implementation employ probability words therefore associated path argue used control node usage image patches diﬀerent used represent words; irrelevant ﬁnal model path-dependent drawn directly dirichlet distribution therefore illustrative/motivating. discussion indicated width depth tree inﬁnite principle ﬁnite tree manifested given ﬁnite data motivates adaptive inference tree size. retrospective implementation stick-breaking process constitute truncated stick-breaking process deallows block updates therefore often manifestbetter mixing ncrp-type implementation related work considered wang blei inference employed tree size therefore inferred retrospective sampler developed allows inference tree depth width node layer i.e. node path node denote children level constitute distribution children nodes draw yielding ν\u0001\u0001j beta φ\u0001\u0001i deﬁned denoted drawn stick. probability measure constitutes principle inﬁnite children nodes λ\u0001\u0001i deﬁning probability transiting node child φ\u0001\u0001i constitutes topic-dependent probability dictionary usage child node. process continues principle inﬁnite number levels child node spawning inﬁnite subsequent children nodes manifesting tree inﬁnite depth width. however note draw typically relatively small number components appreciable amplitude. means constitutes principle inﬁnite number children nodes small fraction visited appreciable probability. contribution paper concerns retrospective sampling infer tree width depth. save space extensive experimental results discuss updates associated inferring tree depth. complete update equations provided supplementary material also summary notation. prior distribution speciﬁed stickbreaking draw image although sampled closed form posterior ﬁxed learn adaptively instead metropolis-hastings step proposal distribution deﬁned thereby constituting -dimensional stick representation; associated node-dependent statistics constituted discussed section model therefore infers retrospect l-level truncation small expands adaptively. model also ability shrink number sticks used component model less associated truncated level needed deﬁne number children/levels actually utilized. step nodes added retrospect discussed previous subsection completing step tree size constituted allows step imposition distribution words path. test proposed model datasets simulated illustrative example examines ability learn tree structure; subset mnist digits data face data microsoft image database; labelme data. case images supplemented annotations. process patches image. mnist face data randomly select partially overlapexamined diﬀerent methods initializing dictionary including random draws privarious ﬁxed redundant bases over-complete dct. alternatively existing dictionary-learning methods purpose covariatedependent hierarchical beta process learn initial dictionary atoms additionally examples initialize tree levels. initialization four nodes present beneath root node subsequent node children four levels; nested k-means clustering used initialize data among clusters respective levels. ﬁrst illustrate proposed model able infer depth width tree using synthesized data tree generates data known; data like considered figure blei simple example wish isolate component figure example synthesized images. left example images. middle ground truth underlying model. right inferred model maximumlikelihood collection sample. topic truth probabilities generative process image corresponds ﬁrst drawing path tree times node patch drawn ﬁnally alphabet members drawn bernoulli associated topic-dependent probabilities ﬁnal data consists counnumber times elements used across draws arranged posteriori align truth clarify presentation. example tree initialized paths three layers also initialized tree paths experimental setting similar recovery achieved. initialized less paths recovered tree still reasonable good. however inference topic-dependent usage probabilities robust numerous diﬀerent settings. results based samples discarded burn-in. mnist data maximum-likelihood collection sample paths path typically layers deep; face data paths inferred typically layers. quantitatively compare ability hierarchical dictionary construction data consider reconstruction error data comparing single-layer model results summarized table averaged across images patches addition results mnist faces data show results msrc labelme data sets also performed experiments investigate initialization aﬀects performance. table instead initializing dictionary hierarchical beta process initialized random. slight degradation performance random initialization marked results still better produced similar improvements initialization observed classiﬁcation task discussed below; helps random initialization still good. deviation) mnist face msrc labelme datasets. ‘ndp+hbp’ ‘ndp+random’ correspond proposed model dictionary initialized randomly respectively model corresponds proposed model data better model gains evident considering real sophisticated imagery important note proposed model eﬀectively complicated model speciﬁcally proposed model image cluster single-layer dirichlet process used perform clustering paths tree deﬁne clusters. cluster/path characterized distribution topics models topic characterized probabilities atom usage diﬀerence probabilities topic usage cluster drawn independently tree structure shared nodes different paths manifest statistical dependencies probability topic usage diﬀerent paths shared nodes. examples total gibbs samples discarded burn-in. results model correspond averaging across collection samples. examples useful results found relatively small number gibbs samples. organizing msrc data settings images annotationmsrc data considered allow direct comparison. choose images categories images manual annotations available. categories tree building face sheep ﬂower sign book chair. numbers images sheep classes respectively classes. image size full tree structure inferred shown figure maximum depth inferred second level clearly observed images clustered several main subgenres e.g. images containing grass ﬂowers another urban construction including cars buildings etc. path depict most-probable images assigned demonstrate form model pairs example images shown figure pairs images assigned distinct paths shared nodes near root node three example patches assigned selected images. pair example images sheep classes patches grass legs shared nodes distinct patches manifesting color texture separately assigned nodes bottom levels. images building class show diversity category. anticipated building category diverse common patches shared nodes near root specialized patches near leaves figure pairs example images paths assigned images shown splitting paths. example images image depict three example patches assigned respective node. typical examples illustrate ubiquitous patches shared nodes near root nodes toward leaves describing details associated specialized classes images. hierarchical structure missed model also apparently manifests better model summarized table labelme data contain image classes coast forest highway inside city mountain open country street tall building. settings images annotations wang randomly select images class thus total number images image remove terms occur less times obtain vocabulary unique words thus terms annotation labelme data average. figure visualizes sub-trees inferred tree structure; nodes inferred second level node represents subtree. class street class insidecity share root node labeled figure based learned posterior word distribution image class infer words probable path. figure shows example paths largestprobability words displayed; capital letters associated histogram figure associated paths tree indicated figure good connection manifested words paths randomly selected images held testing images classes class testing images. image represented estimated distribution nodes entire hierarchy. nodes associated image nonzero values distribution. calculate χ-distances node distribution testing images training images. algorithm applied obtain class label. figure shows confusion matrix classiﬁcation average classiﬁcation accuracy compared examples dictionary learning applied directly observed pixel values within given patch priori feature extraction. alternatively patch-dependent data correspond features extracted using image feature extraction algorithm. illustrate this correspond sift features patches; experiment dictionary learning replaces step models like wang figure show confusion matrix model based sift features average accuracy slightly better results reported wang also demonstrates performing dictionary learning directly patches rather state-of-the-art feature extraction method yields highly competitive results. compare proposed hierarchical model hierarchical model ofﬂine sift feature extraction employed. based related work wang used codebook size achieved average classiﬁcation accuracy compared reported algorithm. note however found model sensitive codebook size serious degradation performance manifested codes example. test proposed model considered classiﬁcation experiment msrc data characterized classes. five images class randomly chosen testing data remaining images treated training data learn hierarchical structure. average accuracy obtained proposed model compared using codebook size experiments indicate proposed model typically better classiﬁcation task even optimize latter respect number codes. experiments performed matlab machine gbyte ram. mcmc sample proposed model takes approximately minutes respectively mnist face msrc labelme experiments note model learning times relatively expensive model testing fast employed aforementioned classiﬁcation task. scale model larger numbers training images perform variational bayesian inference rather sampling employ online-learning methods conclusions nested dirichlet process integrated dictionary learning constitute hierarchical topic model imagery. dictionary learning employed original image pixels features image feature extractor. words available utilized well worddependent usage probabilities inferred path tree. model infers tree depth width. encouraging qualitative quantitative results demonstrated analysis many traditional datasets ﬁeld comparisons provided related published methods. figure structure sub-trees inferred labelme data. root sub-tree child node root entire tree structure. path images assigned listed order importance. letters refer paths distributions words depicted figure", "year": 2012}