{"title": "Using Synthetic Data to Train Neural Networks is Model-Based Reasoning", "tag": ["cs.LG", "cs.CV", "stat.ML", "68T05, 68T10", "I.2.6; I.7.5"], "abstract": "We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.", "text": "contribution paper point kind synthetic data train neural network standard loss fact equivalent training artifact amortized approximate inference sense gershman goodman generative model corresponding synthetic data generator. relationship forms basis recent work inference compilation probabilistic programming also noted paige wood papamakarios murray approximate inference guided neural proposals goal rather training neural networks using synthetic data. consequence need ever reuse training data inﬁnite labeled training data generated training time generative model. another contribution make suggestion take advantage framework running neural network test time compute task-speciﬁc uncertainties interest. contributions also seen reminder guidance neural network community continues move towards tackling unsupervised inference problems labeled training data difﬁcult impossible obtain. towards examine experimental ﬁndings highlight problems likely arise using synthetic data train neural networks. discuss problems terms brittleness demonstrated exist deep neural networks example szegedy showed perceptually indistinguishable variations neural network input lead profound changes output. also discuss model misspeciﬁcation bayesian sense paper structure follows. section develop probabilistic synthetic data generative model suggest single ﬂexible neural network architecture captcha-breaking. section train model independently using training data derived running synthetic data generator parameters produce corresponding style. neural networks shown produce extremely good breaking performance terms accuracy speed well beyond standard computer vision pipeline results comparable recent deep learning results. abstract—we draw formal connection using synthetic training data optimize neural network parameters approximate bayesian model-based reasoning. particular training neural network using synthetic data viewed learning proposal distribution generator approximate inference synthetic-data generative model. demonstrate connection recognition task develop novel captcha-breaking architecture train using synthetic data demonstrating state-of-the-art performance computing task-speciﬁc posterior uncertainty. using neural network trained also demonstrate successful breaking real-world captchas currently used facebook wikipedia. reasoning empirical results drawing connections bayesian modeling discuss robustness synthetic data results suggest important considerations ensuring good neural network generalization training synthetic data. neural networks powerful regressors training neural network regression means ﬁnding values free parameters using supervised learning techniques. generally requires large amount labeled training data. generally harder task larger neural network training data required. labeled training data scarce must either generate synthetic data train resort unsupervised generative modeling generally slow test-time inference since must afresh data. deep learning community reported remarkable results taking former approach either limited form data augmentation dataset artiﬁcially enlarged using label-preserving transformations training models solely synthetic data groundbreaking work text recognition wild achieved training neural network recognize text using synthetically generated realistic renders. goodfellow addressed recognition house numbers google street view images supervised fashion also solving recaptcha images using synthetic data train recognition network image latent text. authors google employees meant access true recaptcha generative model thus could generate millions labeled instances standard supervised-learning pipeline. recently stark discuss demonstrate brittleness regressors. demonstrate improved robustness focusing improving generative model. section illustrate connection demonstrated brittleness bayesian model mismatch. explaining learned neural network used perform sample-based approximate inference. assuming access true captcha generating system paucity labeled training data breaking captchas? hint appears probabilistic programming community’s approach procedural graphics generative model captchas proposed general purpose markov chain monte carlo bayesian inference used computationally inefﬁciently invert said model. make argument effectively generating synthetic training data manner jaderberg train neural network regresses latent captcha variables. either case developing ﬂexible well-calibrated synthetic training data generator ﬁrst concern. synthetic data generative model captcha speciﬁes joint densities parameterized style describe generate latent random variable corresponding captcha image referring ﬁrst table style pertains different schemes ranges fonts kerning deformations noise. note following equations omit style subscript keeping mind separate unique model style. latent structured random variable includes number letters multidimensional structured parameter controlling captcha-rendering parameters kerning various style-speciﬁc deformations letter identities. given these custom stochastic captcha renderer generate captcha image renderer ﬁdelity primary component synthetic data generation effort. corresponding per-style synthetic data generator corresponds model style-speciﬁc prior distribution latent variables including character identities. different style shown table different settings prior parameters drive captcha renderer. particular model places style-speciﬁc uniform distributions different intervals mechanism generating synthetic training data y)}. note cannot evaluated given rather sampled. captcha-breaking neural network designed taking account architectures shown perform well image inputs variable-length output sequences speciﬁcally choose combination convolutional neural networks recurrent neural networks. core neural architecture long shortterm memory network output time step passed output layers corresponding one-to-one components latent variable generative model constitute inputs captcha renderer. since latent variable components stylespeciﬁc instance-speciﬁc lstm time steps represent components latent time step. output layers fullyconnected layers followed softmax function distinct latent variable parameterize discrete probability distribution. since lstm ﬁxed-dimensional output output layers allow match dimensions discrete distributions corresponding latent variables. used embed captcha image ﬁxeddimensional embedding vector cnn. time step lstm input constructed concatenation image embedding value latent variable previous time step label vector corresponding training provided network similar used reed freitas using actual values generated synthetic image test time values sampled corresponding discrete probability distribution. denote combined parameters overall architecture forward propagation function given input output softmax layer time step corresponding ηθt. running example figure networks implemented torch trained adam optimization initial learning rate hyperparameters using minibatches size generative models implemented anglican probabilistic programming language coupled inference compilation framework. seen table architecture method training using synthetic data outperforms nearly state-of-the-art captcha breakers terms accuracy recognition times exception goodfellow used data drawn true recaptcha generator. labeled method shows breaking results speeds neural network trained using synthetic data decode unlabeled captchas captcha generator. goodfellow stark rows show directly comparable results namely using deep neural networks break unlabeled captchas training synthetic data. additional rows show breaking results traditional segment-and-classify computer vision image processing pipelines. these contrast others access true captcha generator instead report test results real-world captchas gathered wild. robust accuracies would seem conﬁrm captcha computer security perspective indeed broken. capabilities deep neural networks impressive noted kinds results occasion somewhat misleading particular note assumption that point paper referenced results deep learning literature training procedure captcha-breaking network access design softmax outputs determine parameters discrete probability distributions captcha generator parameters. loss minimize training negative softmax outputs notation denote element standard loss used training neural networks classiﬁcation. connection bayesian modeling interpret softmax outputs probabilities discrete random variables joint importance sampling proposal distribution explored detail section iv-b. wrote synthetic data generative models seven different captcha styles covering types frequently found captcha breaking literature these trained neural architecture consisting convolutions max-pooling second ﬁfth sixth convolutions ﬁnal fully-connected layers units; stack lstms hidden units each; fully-connected layers appropriate dimension mapping lstm output corresponding softmax dimension latent variable. relu activations used convolutions fully-connected layers overall. empirically veriﬁed supplying image embedding lstm every time step makes training progress faster setup train scratch together rest components compared alternative using pretraining data true generative process. indeed samples true generative process superior even hand-labeled training instances gathered wild. simulated data required access true generative model must come approximation true generative process model whether networks trained using approximate data robust sense working well real data wild becomes real question. another captcha really broken access true generative model—or legion human labelers pile cash? robustness results happens state-of-the-art models test data subtly different generated synthetic data? happens attempt transfer learning captcha style another? exploration questions forms inspiration basis rest paper. start tried trained models real captchas wikipedia facebook identiﬁed major services still make textual captchas collecting hand-labeling test sets images each. found trained wikipedia facebook models achieving recognition synthetic data yielded practically zero breaking rates real data. tried using model trained captcha style break another style found nearly always failed well. found partially caused non-overlapping latent variable domains renderers different styles. instance might expect recaptcha breaker work visually similar yahoo captchas found case. investigate performed experiments constructed test captchas trained networks cannot recognize despite perceptually indistinguishable captchas original generative model. found could more-or-less arbitrarily degrade test performance shifting test data either ways away original synthetic data ﬁrst corrupted image subtle additive noise shifts captcha small imperceptible euclidean distance original position. causes captcha breaking networks exhibit kind brittleness well known problem deep neural network classiﬁers second changing generative model test data relative training data even ways arguably human ability perceive also able cause test performance degrade. kind model misspeciﬁcation discussed bayesian inference literature facebook captchas appear measure preventing ﬂood-posting links particular facebook pages followed. wikipedia captchas appear account creation page. note textual recaptchas version replaced tasks image recognition making unlikely encounter collect. fig. synthetic data wikipedia generative model recognized correctly whereas even perceptually subtle changes adding per-pixel white noise \u0001kerning modiﬁed pixel result severely degraded recognition rates. overall recognition rates test groups samples taken note middle right columns recognized correctly robust wikipedia model. data generation. particular developed substantially ﬂexible generative model using elastic displacement ﬁelds introduced simard effectively forcing neural network generalize greater variation exhibited ground-truth labeled test data wild. improved generative models observed robust subtle modiﬁcations report figure results obtained encouraging achieving recognition rates real wikipedia facebook captchas respectively. cases robust results arrived improving quality synthetic data generator performance comparable traditional vision pipelines signiﬁcantly higher recognition threshold suggested deem deployed captcha system broken order explore factors cause brittleness neural network performance reported draw connection bayesian model mismatch out-of-sample generalization failure neural network regressors tested data different used training. prerequisite this review importance sampling approximate probabilistic inference algorithm naturally corresponds kind inference trained neural networks allow given joint distribution user-speciﬁed proposal distribution importance sampling allows approximate posterior distribution expectations arbitrary functions fig. illustration model mismatch. left model encompasses true data distribution; middle model partially matches true data distribution; right model completely mismatched true data distribution. using procedure similar described section generate training data model train neural network mapping importance sampling proposal parameters resulting proposals generated proposal distribution shown figure magenta dash-dotted ellipses. remember functions computed trained neural network regressor. draw samples proposal distribution repeatedly running trained neural network forward weight resulting samples according importance sampling scheme beginning section arrive approximations model-based posterior mean covariance note importance sampling generally inefﬁcient unless proposal distribution well-matched target distribution sense overlaps target extremely efﬁcient matches exactly. illustrate effects mismatch synthetic real data terms bayesian model misspeciﬁcation using simpler experiment highlighting conceptually believe happening. true data generating distribution model loss viewed monte carlo approximation expectation function joint distribution synthetic data which following paige wood shown kullback-leibler divergence proposal posterior averaged possible datasets running neural network trained using synthetic data common loss input actually produces efﬁcient proposal distribution parameters running neural network times given input subsequently weighting sampled values according obtain approximate posterior distribution note that case captchas must likelihood based approximate bayesian computation instead intractable order calculate weight remarkable natural scene text recognition results jaderberg show generalization synthetic data degree could argue result actually generative modeling triumph. results showing improved robustness wikipediafacebook-style captcha-breaking stem likewise focusing synthetic data generative model. addition training neural usefully prescriptive point networks using synthetic data equivalent performing proposal adaptation importance sampling inference consider three scenarios figure difference true data generating distribution illustrated marginal model progressively increased left right. true data generating distribution moves away model ﬁxed computational budget samples progressively worse estimates happening neural network training time learns invert model samples drawn figure model overlaps true data generative process neural network sees examples pairs representative true data generating mechanism then given sufﬁcient capacity terms neural architecture training time almost certainly learn mapping solves task predicting given model slightly misspeciﬁed number training examples domain true model might small might expect good generalization performance. high model misspeciﬁcation neural network simply never training examples look like true data such produce mostly spurious regression results leading unhelpful proposal distributions. experiment graphically illustrates kinds problems arise model misspeciﬁcation. indicates going synthetic data train neural network regressor ensure synthetic data generator ideally close possible true data generation process mismatch true data terms broadness match not) tolerable fact preferable perceptually indistinguishably miscalibrated model conjecture latter caused brittleness discovered trained neural networks illustrate figure intuition guided decision broaden synthetic data generator adding displacement ﬁelds simard section iii-b leading signiﬁcant improvements robustness evidenced improved real-data results obtained. this believe accounts fact captcha generator likely capture details true generative model subtle font differences. corollary bayesian inference interpretation training neural network synthetic data resulting neural network used approximate inference probabilistic model corresponding synthetic training data generator. importance sampling proposal distribution fact= consider individual time-dependent softmax layers captchasolving neural network probabilities proposal fig. posteriors real facebook wikipedia captchas. conditioning captcha show approximate posterior produced weighted importance sampling particles {)}m synthetic data generative model sets empirical cornerstone future theory quantiﬁes bounds impact model mismatch neural network approximate inference performance. tuan supported epsrc google studentships. atılım g¨unes¸ baydin frank wood supported darpa ppaml u.s. afrl cooperative agreement fa-- award number robert zinkov supported darpa grant fa---. visual document analysis proceedings seventh international conference document analysis recognition volume ser. icdar washington ieee computer society goodfellow bulatov ibarz arnoud shet multi-digit number recognition street view imagery using deep convolutional neural networks arxiv preprint arxiv. baydin wood inference compilation universal probabilistic programming international conference artiﬁcial intelligence statistics april fort lauderdale paige wood inference networks sequential monte carlo graphical models proceedings international conference machine learning ser. jmlr vol. wang wang robustness hollow captchas proceedings sigsac conference computer communications security. blum hopper langford captcha using hard problems security international conference theory applications cryptographic techniques. springer mansinghka kulkarni perov tenenbaum approximate bayesian image interpretation using generative probabilistic graphics programs advances neural information processing systems karpathy fei-fei deep visual-semantic alignments generating image descriptions proceedings ieee conference computer vision pattern recognition wood meent mansinghka approach probabilistic programming inference proceedings seventeenth international conference artiﬁcial intelligence statistics doucet johansen tutorial particle ﬁltering smoothing fifteen years later handbook nonlinear filtering vol. wilkinson approximate bayesian computation gives exact results assumption model error statistical applications genetics molecular biology vol.", "year": 2017}