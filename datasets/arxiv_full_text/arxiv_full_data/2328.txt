{"title": "Learning High-Dimensional Mixtures of Graphical Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider unsupervised estimation of mixtures of discrete graphical models, where the class variable corresponding to the mixture components is hidden and each mixture component over the observed variables can have a potentially different Markov graph structure and parameters. We propose a novel approach for estimating the mixture components, and our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. Our method is efficient when the union graph, which is the union of the Markov graphs of the mixture components, has sparse vertex separators between any pair of observed variables. This includes tree mixtures and mixtures of bounded degree graphs. For such models, we prove that our method correctly recovers the union graph structure and the tree structures corresponding to maximum-likelihood tree approximations of the mixture components. The sample and computational complexities of our method scale as $\\poly(p, r)$, for an $r$-component mixture of $p$-variate graphical models. We further extend our results to the case when the union graph has sparse local separators between any pair of observed variables, such as mixtures of locally tree-like graphs, and the mixture components are in the regime of correlation decay.", "text": "consider unsupervised estimation mixtures discrete graphical models class variable corresponding mixture components hidden mixture component observed variables potentially diﬀerent markov graph structure parameters. propose novel approach estimating mixture components output tree-mixture model serves good approximation underlying graphical model mixture. method eﬃcient union graph union markov graphs mixture components sparse vertex separators pair observed variables. includes tree mixtures mixtures bounded degree graphs. models prove method correctly recovers union graph structure tree structures corresponding maximum-likelihood tree approximations mixture components. sample computational complexities method scale poly r-component mixture p-variate graphical models. extend results case union graph sparse local separators pair observed variables mixtures locally tree-like graphs mixture components regime correlation decay. graphical models oﬀer graph-based framework representing multivariate distributions structural qualitative relationships variables represented graph structure parametric quantitative relationships represented values assigned diﬀerent groups nodes graph decoupling natural variety contexts including computer vision ﬁnancial modeling phylogenetics. moreover graphical models amenable eﬃcient inference distributed algorithms belief propagation recent innovations made feasible train models computational sample requirements high dimensions depending realization so-called choice variable latent hidden. mixture models widespread applicability since account changes observed data based hidden inﬂuences. recent works provided provable guarantees learning high-dimensional mixtures variety settings paper consider mixtures graphical models combines modeling power formulations. models incorporate context-speciﬁc dependencies structural relationships among observed variables change depending hidden context. models allow parsimonious representation high-dimensional data retaining computational advantage performing inference belief propagation variants. current practice learning mixtures graphical models based local-search heuristics expectation maximization however scales poorly number dimensions suﬀers convergence issues lacks theoretical guarantees. paper propose novel approach learning graphical model mixtures oﬀers powerful alternative time establish theoretical guarantees method wide class models includes tree mixtures mixtures bounded degree graphs. previous theoretical guarantees mostly limited mixtures product distributions models restrictive since posit manifest variables related another latent choice variable direct dependence otherwise. work signiﬁcant generalization models incorporates models tree mixtures mixtures bounded degree graphs. approach aims approximate underlying graphical model mixture tree-mixture model. view tree-mixture approximation oﬀers good tradeoﬀ data ﬁtting inferential complexity model. tree mixtures attractive since inference reduces belief propagation component trees tree mixtures thus present middle ground tree graphical models simplistic general graphical model mixtures inference tractable goal eﬃciently observed data tree mixture model. propose novel method learning discrete graphical mixture models. combines techniques used graphical model selection based conditional independence tests spectral decomposition methods employed estimating parameters mixtures product distributions. method proceeds three main stages graph structure estimation parameter estimation tree approximation. ﬁrst stage algorithm estimates union graph structure corresponding union markov graphs mixture components. propose rank criterion classifying node pair neighbors non-neighbors union graph mixture model viewed generalization conditional-independence tests graphical model selection method eﬃcient union graph sparse separators node pair holds tree mixtures mixtures bounded degree graphs. sample complexity algorithm logarithmic number nodes. thus method learns union graph structure graphical model mixture similar guarantees graphical model selection mixture components. since choice variable hidden involves decomposition observed statistics component models. leverage spectral decomposition method developed learning mixtures product distributions mixture product distributions observed variables conditionally independent given hidden class variable. adapt method setting follows consider diﬀerent triplets observed nodes condition suitable separator obtain estimates pairwise marginals mixture component natural non-degeneracy conditions. ﬁnal stage best tree approximation estimated component marginals standard chow-liu algorithm chow-liu algorithm produces max-weight spanning tree using estimated pairwise mutual information edge weights. establish algorithm recovers correct tree structure corresponding maximum-likelihood tree approximation mixture component. special case underlying distribution tree mixture implies recover tree structures corresponding mixture components. computational sample complexities method scale poly number nodes number mixture components. recall success method relies presence sparse vertex separators node pairs union graph i.e. union markov graphs mixture components. includes tree mixtures mixtures bounded degree graphs. extend methods analysis larger family models union graph sparse local separators weaker criterion. family includes locally tree-like graphs augmented graphs criterion sparse local separation signiﬁcantly widens scope prove methods succeed models mixture components regime correlation decay sample computational complexities signiﬁcantly improved class since depends size local separators proof techniques involve establishing correctness algorithm sample analysis involves careful spectral perturbation bounds guarantee success ﬁnding mixture components. addition setting sparse local separators incorporate correlation decay rate functions component models quantify additional distortion introduced local separators opposed exact separators. caveat method require dimension node variables larger number mixture components principle limitation overcome consider larger groups nodes implement method. another limitation require full rank views latent factor method succeed. however also requirement imposed learning mixtures product distributions. moreover known learning singular models i.e. satisfy rank condition least hard learning parity noise conjectured computationally hard another restriction require presence observed variable conditionally independent variables given latent choice variable. however note signiﬁcantly weaker case product mixture models observed variables required conditionally independent given latent factor. best knowledge work ﬁrst provide provable guarantees learning non-trivial graphical mixture models employed variety applications. recently focus learning mixture models high dimensions. number recent works dealing estimation high-dimensional gaussian mixtures starting work dasgupta learning wellseparated components recently long line works. works provide guarantees recovery various separation constraints mixture components and/or computational sample complexities growing exponentially number mixture components contrast so-called spectral methods computational sample complexities scaling polynomially number components impose stringent separation constraints outline below. spectral methods mixtures product distributions classical mixture model product distributions consists multivariate distributions single latent variable observed variables conditionally independent state latent variable hierarchical latent class models generalize models allowing multiple latent variables. spectral methods ﬁrst employed learning discrete mixtures product distributions recently extended learning general multiview mixtures method based triplet pairwise statistics observed variables build methods work. note setting mixture product distributions thus methods directly applicable. graphical model selection graphical model selection well studied problem starting seminal work chow ﬁnding best tree approximation graphical model. established maximum likelihood estimation reduces maximum weight spanning tree problem edge weights given empirical mutual information. however problem becomes challenging either nodes hidden interested estimating loopy graphs. learning structure latent tree models studied extensively mainly context phylogenetics eﬃcient algorithms provable performance guarantees available e.g. works high-dimensional loopy graphical model selection recent. approaches classiﬁed mainly groups non-convex local approaches based convex optimization also recent work learning conditional models e.g. however works directly applicable learning mixtures graphical models. mixtures graphical models works learning mixtures graphical models fewer mostly focus tree mixtures. works meila jordan kumar koller consider em-based approaches learning tree mixtures thiesson extend approach learn mixtures graphical models directed acyclic graphs termed bayesian multinets geiger heckerman using cheeseman-stutz asymptotic approximation armstrong consider bayesian approach assigning prior decomposable graphs. however approaches theoretical guarantees. recently mossel roch consider structure learning latent tree mixtures provide conditions successfully recovered. note model thought hierarchical mixture product distributions hierarchy changes according realization choice variable. setting diﬀers substantially work. mossel roch require component latent trees mixture diﬀerent order quartet tests distinguish establish uniform selection trees ensure condition. hand impose restriction allow graphs diﬀerent components same/diﬀerent moreover allow loopy graphs mossel roch restrict learning latent tree mixtures. however mossel roch allow latent variables tree assume variables observed mossel roch consider structure learning consider structure parameter estimations. mossel roch limit ﬁnite number mixtures allow scale number variables such methods operate signiﬁcantly diﬀerent settings. ﬁrst introduce concept graphical model discuss mixture models. graphical model family multivariate distributions markov given undirected graph paper consider mixtures discrete graphical models. denote discrete hidden choice variable corresponding selection diﬀerent components mixture taking priori know mixture component sample drawn. implies cannot directly apply previous methods designed graphical model selection. major challenge thus able decompose observed statistics mixture components. propose method learning mixture components given i.i.d. samples drawn graphical mixture model method proceeds three main stages. first estimate graph h=gh union markov graphs mixture. accomplished series rank tests. note special case gives intuitions provide intuitions properties union graph h=gh markov graph corresponding component note diﬀerent markov graph corresponding marginalized model represents natural markov properties respect observed statistics. ﬁrst establish simple result union graph satisﬁes markov property mixture component. recall denotes vertex separator nodes i.e. removal disconnects fact nodes thus observation implies conditional independence relationships mixture component satisﬁed union graph conditioned latent factor result exploited obtain union graph estimate follows nodes neighbors separator found results conditional independence main challenge indeed variable observed thus conditional independence cannot directly inferred observed statistics. however eﬀect observed statistics quantiﬁed follows establish sequel computational sample complexities learning method scale exponentially thus algorithm suitable graphs small section relax requirement exact separation local separation. larger class graphs satisfy local separation property including mixtures locally tree-like graphs. rank test propose ranktest algorithm structure estimation h=gh union markov graph r-component mixture. method based search potential separators given nodes based eﬀective method involves searching separators node pair considering sets satisfying computational complexity procedure dimension node variable number nodes. number rank tests performed node pairs conditioning sets; rank tests complexity since involves performing singular value decomposition matrix. section relax strict separation constraint local separation constraint regime correlation decay refers bound size local separators non-neighbor nodes graph. assumption relates number components dimension sample space variables. note allow number components grow number nodes long cardinality sample space variable also large enough. principle assumption removed considering grouping nodes together performing rank tests groups. assumption imposes constraints graph structure formed union component graphs. bound separator sets node pairs crucial parameter complexity learning depends relax assumption separator bound criterion local separation section assumption required success rank tests distinguish neighbors non-neighbors graph rules presence spurious rank matrices neighboring nodes assumption provides natural threshold singular values rank test. section modify threshold also account distortion approximate vertex separation contrast setting exact separation considered section. provides ﬁnite sample complexity bound. provide result success recovering graph theorem ranktest outputs correct graph h=gh union component markov graphs assumptions probability least remarks thus rank test also applicable graphical model selection. previous works proposed tests based conditional independence using either conditional mutual information conditional variation distances anandkumar bresler rank test thus alternative test conditional independence. addition extends naturally estimation union graph structure mixture components. spectral decomposition methods ﬁrst proposed chang later generalized mossel roch recently anandkumar applicable mixtures product distributions. illustrate method simple example. pairwise triplet statistics denote mu|h similarly mv|h mw|h assume full rank. denote probability matrices muv{w;k} ]ij. parameters estimated lemma latent variable model yw|h conditional probability matrices mu|h mv|h mw|h rank thus procedure recovering conditional probabilities observed variables conditioned latent factor. using parameters also recover mixing weights parameters performing spectral decomposition diﬀerent triplets however obstacle remains spectral decomposition diﬀerent triplets results diﬀerent permutations labels hidden variable overcome this note triplets share eigenvectors left node same. thus consider ﬁxed node left node ﬁxed matrix diagonalize triplets obtain consistent ordering hidden labels triplet decompositions. thus learn general product distribution mixture using third-order statistics. adapt method learning general graphical model mixtures. ﬁrst make simple observation obtain mixtures product distributions considering separators thus ﬁxing conﬁguration nodes suvw obtain product distribution mixture previously proposed rank test successful estimating possess correct knowledge separators suvw. case obtain estimates ﬁxing nodes suvw using spectral decomposition described lemma procedure repeated diﬀerent triplets fig.. diﬀerent triplets. however additional complication arises consider graphical model mixtures conditioning separators required. require permutation condition required hold identiﬁability operate statistics diﬀerent triplets words resort operations order statistics require additional conditions identiﬁability. however setting signiﬁcant generalization mixtures product distributions required hold nodes. algorithm allows variables triplet diﬀerent dimensions anandkumar details. thus obtain estimates pairwise marginals mixture components. computational complexity procedure scales number nodes cardinality node variable bound separator sets. details implementation spectral method appendix learning parity noise conjectured computationally hard condition indeed additional constraint graph required ensure alignment hidden labels spectral decompositions diﬀerent groups variables discussed before condition assumes various spectral bounds characterizes sample complexity. remarks recall denotes number variables denotes number mixture components denotes dimension node variable denotes bound separator sets node pair union graph. quantity appendix consider ﬁnal stage approach viz. learning tree approximations using estimates pairwise marginals mixture components spectral decomposition method. impose standard condition non-degeneracy mixture component guarantee existence unique tree structure corresponding maximum-likelihood tree approximation mixture component. graph least three connected components choose reference node components estimate marginals components. instance three connected components choose node reference node estimate marginals similarly choose node reference node estimate marginals align diﬀerent estimates obtain marginals. replace edge note max-weight spanning tree property intuitively denotes bottleneck errors likely occur tree structure estimation. similar observations made error exponent analysis chow-liu algorithm. sample complexity correctly estimating using samples based given ensures mutual information quantities estimated within separation bound thus approach succeeds recovering correct tree structures corresponding remarks ml-tree approximations mixture components computational sample complexities scaling polynomially number variables number components dimension variable paper considered learning tree approximations graphical model mixtures. proposed novel methods combined techniques used previously graphical model selection learning mixtures product distributions. provided provable guarantees method established polynomial sample computational complexities number nodes number mixture components cardinality node variable guarantees applicable wide family models. future plan investigate learning mixtures continuous models gaussian mixture models. isolated union graph estimate obtained using algorithm spectral decomposition graph bg∪. separates bg∪. fig.. distribution samples upon spectral decomposition obtain mixture components spect employ estimated pairwise marginals chow-liu tree approximation {bth}h mixture component. algorithm findmixturecomponents ﬁnding tree-approximations components r-component mixture using samples graph separator separating graph bg∪. isolated nodes graph otherwise declare fail. bg). spect}h specdecom; hbth{bp tree}∈ bthi chowliutree{bp spect}ab∈v \\{u∗}. outputhbπspect procedure specdecom ﬁnding comgraph bgn. empirical distribution computed using samples similarly ]ij. choose left orthonormal singular vectors mlcm uwzl simpliﬁcation tree mixtures simplify method limiting tree approximations subgraphs graph procedure coincides original method component markov graphs {gh}h trees i.e. case chow-liu tree coincides implies need compute pairwise marginals edges using specdecom routine instead node pairs chowliutree procedure computes maximum weighted spanning tree instead complete graph. leads slight improvement sample complexity note remarks theorem statistics spect {u∗} estimate ∪h∈gh. estimate separated bg∪. employ procedure specdecom conﬁguration using suitable separator threshold test estimated component statistics spect manner. similar test used graphical model selection note obtain sample complexity results test lines analysis section method eﬃcient maximum degree small. extend analysis setting graphical model mixture union graph sparse local separators weaker criterion sparse exact separators. provide deﬁnition local separator. detailed discussion refer wide family graphs possess property sparse local separation i.e. small addition graphs considered previous section additionally includes family locally tree-like graphs bounded degree graphs augmented graphs formed union bounded degree graph locally tree-like graph detailed discussion refer consider learning mixtures graphical models markov graphs sparse local separators. assume models regime correlation decay makes learning feasible proposed methods. technically correlation decay deﬁned multiple ways notion strong spatial mixing weaker notion known weak spatial mixing. graphical model said satisfy weak spatial mixing conditional distribution node asymptotically independent conﬁguration growing boundary said satisfy strong spatial mixing total variation distance conditional distributions node conditioning diﬀerent conﬁgurations depends graph distance node conﬁgurations diﬀer. formally deﬁne below incorporate provide learning guarantees. details. setting potentials edges zero. sets dist minu∈av∈a dist denote minimum graph distance. denote nodes within graph distance node denote boundary nodes i.e. exactly node denote induced subgraph consider marginal distribution node instead conditional distribution sets weaker criterion typically referred weak spatial mixing. however order provide learning guarantees require notion strong mixing. class ising models regime correlation decay explicitly characterized terms maximum edge potential model. maximum edge potential certain threshold model said regime correlation decay. threshold explicitly characterized certain graph families. derivations. slightly modify deﬁnition correlation decay compared usual notion considering models diﬀerent graphs induced subgraph neighborhood graph instead models diﬀerent boundary conditions. provide suﬃcient conditions success ranktest algorithm note crucial diﬀerence compared previous section refers bound local separators contrast bound exact separators. lead signiﬁcant reduction computational complexity running rank test many graph families since complexity scales number nodes cardinality node variable. assumptions comparable assumptions section conditions identical. conditions comparable diﬀerence assumes bound exact separators assumes bound local separators weaker criterion. again conditions rank matrices neighboring nodes identical. condition additional condition regarding presence correlation decay mixture components. assumption required approximate conditional independence conditioning local separator sets mixture component. words threshold path lengths considered local separation large enough small). provides modiﬁed threshold account distortion local separators provides modiﬁed sample complexity. opposed exact separators nodes consideration. prove method succeeds estimating pairwise marginals component model following conditions. additional distortion introduced local separators findmixturecomponents opposed exact separators. assumptions identical bound number samples slightly worse compared depending correlation decay rate function moreover perturbation lower bound local separators contrast exact vertex separators. before below impose additional conditions order result implies recover matrix ma|h{s;k} using suitable reference node witness separator set. isolated node reference node since focus recovering edge marginals mixture components consider node pair {u∗} node ∪h∈gh described findmixturecomponents. thus able recover mab|h{s;k} exact statistics. since observed knowledge thus recover mab|h desired. spectral decompositions diﬀerent groups aligned since node since isolated ﬁxing variables eﬀect conditional distribution i.e. since recover edge marginals mab|h correctly recover correct tree approximation marginals edges estimate ∪h∈gh union component graph well constructing chow-liu trees subgraphs bg∪. thus instead considering node pair {u∗} need choose bg∪. changes deﬁnition αmin αmax ρ′min ρ′min relate perturbation probability vector perturbation corresponding mutual information recall discrete random variables mutual information related entropies maxh∈ correlation decay rate function corresponding model path threshold local vertex separators. notation convenience node denote original component model markov graph denote corresponding marginal distribution mixture. denote component denote p-norm vector k~vkp corresponding induced norm matrix kakp sup~v=~ ka~vkp/k~vkp. frobenius norm matrix denoted kakf. matrix rm×n σ/σmin ka−k invertible). kbubα lemma consider setting deﬁnitions lemma rm×n rm×n ⊤bxbv ⊤xbv invertible rk×k matrix whose column ~ξi. rk×k matrix. deﬁne distinct real eigenvaluesbλbλ |bλτ corresponding eigenvectors bξbξ normalized kbξik satisfy kbξτ ~ξik kr−k matrix rk×k whose column satisﬁes matrix distinct real eigenvalues bλbλ |bλj exists matrix rk×k whose column right eigenvector corresponding tobλj scaled kbr~ejk matrix invertible inverse satisﬁes kbr− element br−baibr denoted bλij", "year": 2012}