{"title": "Learning Using 1-Local Membership Queries", "tag": ["cs.LG", "cs.AI"], "abstract": "Classic machine learning algorithms learn from labelled examples. For example, to design a machine translation system, a typical training set will consist of English sentences and their translation. There is a stronger model, in which the algorithm can also query for labels of new examples it creates. E.g, in the translation task, the algorithm can create a new English sentence, and request its translation from the user during training. This combination of examples and queries has been widely studied. Yet, despite many theoretical results, query algorithms are almost never used. One of the main causes for this is a report (Baum and Lang, 1992) on very disappointing empirical performance of a query algorithm. These poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial, and impossible to interpret by humans.  In this work we study a new model of local membership queries (Awasthi et al., 2012), which tries to resolve the problem of artificial queries. In this model, the algorithm is only allowed to query the labels of examples which are close to examples from the training set. E.g., in translation, the algorithm can change individual words in a sentence it has already seen, and then ask for the translation. In this model, the examples queried by the algorithm will be close to natural examples and hence, hopefully, will not appear as artificial or random. We focus on 1-local queries (i.e., queries of distance 1 from an example in the training sample). We show that 1-local membership queries are already stronger than the standard learning model. We also present an experiment on a well known NLP task of sentiment analysis. In this experiment, the users were asked to provide more information than merely indicating the label. We present results that illustrate that this extra information is beneficial in practice.", "text": "classic machine learning algorithms learn labelled examples. example design machine translation system typical training consist english sentences translation french. stronger model algorithm also query labels examples creates. translation task algorithm create english sentence request translation user training. combination examples queries resembles human learning patterns widely studied. despite many theoretical results query algorithms almost never used. main causes report disappointing empirical performance query algorithm. poor results mainly attributed fact algorithm queried labels examples artiﬁcial impossible interpret humans. work study model local membership queries model algorithm tries resolve problem artiﬁcial queries. allowed query labels examples close examples training set. e.g. translation algorithm change individual words sentence already seen translation. model examples queried algorithm close natural examples hence hopefully appear artiﬁcial random. work focus -local membership queries show -local membership queries already stronger standard learning model. also present experiment well known task sentiment analysis. experiment users asked provide resembles -local queries information merely indicating label. present results illustrate extra information beneﬁcial practice. would like thank advisor prof. shai shalev-shwartz outstanding team support inspiration. would also like thank amit daniely guidance mentorship. extensive knowledge patience invaluable. privilege work him. alon gonen rosenﬂed yoav wald yossi arjevani nomi vinokurov avishai wagner remarkable friendship counsel. especially levi assistance empirical work oﬃcemates zahi ajami dikla cohn wonderful companionship. would like thank parents love support throughout years weisberg family help especially susan editorial comments. last least would like thank husband enduring support expressed many levels encouraging diﬃcult times editing drafts providing home cooked meals. additional data useful? experimental setup sentiment analysis dataset pre-processing language model scoring algorithm results precision recall over-ﬁtting comparing feature selection methods humans learn? look process child learning recognize cat. focus types input. ﬁrst type input child’s parent points states look cat. second type input answer child’s frequent question what that? child pose seeing also seeing mouse rabbit small animal. types input basis learning model originally suggested celebrated paper theory learnable valiant’s learning model learning algorithm access sources information examples oracle. learning algorithm call examples receive example label additionally learning algorithm oracle provides label example presented input types look models learning learning using calls examples learning using calls examples oracle. ﬁrst standard probably approximately correct model. second called pac+mq model. theoretical work searching limits additional strength membership queries. membership queries addition examples proven stronger standard model many cases despite model seems much stronger intuitively formally rarely used practice. commonly believed result fact many cases easy implement algorithms create artiﬁcial examples labeled part training phase. problem labeling artiﬁcial examples highlighted experiment baum lang baum lang implemented membership query algorithm proposed baum learning halfspaces algorithm poor results attributed fact algorithm created artiﬁcial unnatural examples resulted noisy labeling. elaborate experiment criticize conclusions section suggested solution problem unnatural examples proposed awasthi suggested mid-way model learning queries restricted ones. queries model allows algorithm local queries i.e. queries close sense examples sample set. hopefully examples similar natural examples also appear natural least close natural case appearing random artiﬁcial. work awasti started investigate power limitations model local queries. proved positive results learning sparse polynomials oplogpnqqlocal queries deﬁned locally smooth distributions sense generalize uniform product distributions. also proposed algorithm learns formulas uniform distribution quasi-polynomial time using work follows awasthi focused -local queries closest original model. formulate arguably natural distributional assumption present algorithm uses -local membership queries learn formulas assumption. also provide matching lower bound namely prove learning dnfs assumption hard without queries assuming learning decision trees hard. ﬁrst example natural problem -local queries stronger vanilla model finally provide empirical evidence using local queries helpful practice importantly implementation queries easy straightforward acquired crowdsourcing without expert. present method using local queries perform user-induced feature selection process present results protocol task sentiment analysis tweets. results show acquiring expressive data using -local queries achieve better results fewer examples. based fact smaller data suﬃcient gain twice need less manpower labeling process less computing power training process. note similar experiments also present encouraging results along line supplies evidence query-based methods useful practice. valiant’s probably approximately correct model learning formulates problem learning concept examples. examples chosen according ﬁxed unknown arbitrary distribution instance space. learner’s task prediction rule. requirement high probability prediction rule correct small fraction instances. positive results known model i.e. concept classes proven pac-learnable. maybe signiﬁcant example class halfspaces. examples include relatively weak classes dnfs cnfs constantly many terms rank decision trees constant despite positive results learning problems probably intractable. fact beyond results mentioned above almost positive results known. furthermore several negative results known. example learning automatons logarithmic depth circuits intersections polynomially many halfspaces intractable assuming security various cryptographic schemes model passive model learner receives random data examples labels outputs classiﬁer. stronger version would active model learner gathers information world asking questions receiving responses. several types active models proposed membership query synthesis stream-based selective sampling pool-based sampling work area membership queries model presented model learner allowed query label particular example chooses model shown stronger several scenarios. examples concept classes proven pac-learnable membership queries available include class deterministic finite automatons class k-term logplogpnqq class decision trees k-almost monotone-dnf formulas class intersections k-halfspaces class formulas uniform distribution last results built upon freund’s boosting algorithm fourier-based technique learning using membership queries discussed above widespread signiﬁcant theoretical work model. hand almost practical work implementing ideas done. well-known exception work baum lang applied variation algorithm learning linear classiﬁer proposed baum algorithm uses idea given examples positive negative query oracle possible approximately accurate separating halfspace using binary search line positive negative examples. experiment attempts evaluate idea practice. task chose task binary digit classiﬁcation. algorithm would receive examples positive negative would return weights halfspace. generalization error halfspace would tested examples data. query technique used experiment diﬀerent original algorithm direct implementation algorithm would repeatedly ﬂash images screen binary search would require test subject type correct label image. process seemed likely error prone instead provided interface permitted test subject scan input space using mouse click image seemed right edge recognizability compared performance algorithm variants three classic algorithms backpropogation perceptron simplex baselines ﬁrst returns perpendicular bisection line segments connecting examples second returns randomly oriented hyperplane midpoint line. query learning algorithm uses additional information obtained users described above three algorithms additional examples drawn data set. three algorithms outperformed query-based algorithm. surprisingly even baseline choosing perpendicular bisection line signiﬁcantly better results halfspace created query algorithm. method worse query based method random bisector method. suggest reason poor results question users answer boundary pattern outside range human competence. work many conclusion membership queries useful practice balcan dasgupta more). argue several problems conclusion. first foremost task users asked perform intuitive task easy think variants queries would suitable. therefore surprising labeling turned noisy considering nature question hand. second algorithm abilities; used queries additional option sample extra points data. several suggestions made ways solve problem algorithm’s generation unnatural examples. common drop whole framework membership queries focus types active learning stream-based pool-based. idea ﬁlter existing examples taken large unlabeled data drawn distribution rather creating artiﬁcial examples. another suggestion give human annotator option answering don’t know tolerant incorrect answers. theoretical framework model incomplete membership oracle answers random subset queries missing. notion ﬁrst presented angluin slonim followed notion limited malicious blum sloan tur´an bisht third method restrict examples learning algorithm query examples similar examples drawn distribution. formalized work awasthi present concept learning using local membership queries. framework deals problem raised questioning examples close examples distribution escape problem generating random non-classiﬁable examples. focused n-dimensional boolean hyper-cube oplogpnqq-local queries i.e. learning algorithm given option hamming distance lower oplogpnqq. model suggested mid-way model using plogpnq logptqq-local queries. another interesting result presented time using oplogpnqq-local queries. also presented results regarding using q-local queries powerful using r-local queries also showed local queries always help. showed section give experimental evidence extra information user helpful. works along line. druck propose pool-based active learning approach user provides labels input features rather instances. users asked provide label input features labeled input feature denotes particular feature highly indicative particular label. following that settles presented active learning annotation interface users label instances features simultaneously. point time instance list features label presented screen. user choose either label instance choose feature list indicative feature choice. another similar work raghavan allan raghavan studied problem tandem learning combine uncertainty sampling instances along co-occurrence-based interactive feature selection. experiments conducted text domain features always unigrams. experiments presented encouraging results using human annotators either reaching better results showing excessive annotators reduce size data sometimes both. xi’s sampled i.i.d. unknown distribution unknown hypothesis. focus so-called realizable case assumed learner returns hypothesis goal approximate namely loss small possible loss deﬁned ldh‹pˆhq px„d return hypothesis loss time polynomial deﬁnition learning algorithm learns approximately correct distribution hypothesis work guarantees respect restricted families. learns w.r.t family pairs distributions hypotheses following holds algorithm satisﬁes requirements learning algorithm whenever pair deﬁnition learning algorithm belongs similar remark look succinctly described hypotheses small non-negligible probabilities. simplicity take convention small negligible least results easily generalized case small non-negligible deﬁned deﬁnition denote hdnf hypothesis class functions realized intuitively evaluating formula given example check conditions deem example positive conditions holds. consider case conditions chance prototype example. namely example satisﬁes condition strong way. assumptions deﬁnition target function realized formula every example satisﬁes term. function realized decision tree always holds. sense assumption holds functions realized stable decision tree. deﬁnition makes strong assumption namely every positive example evidence term. next deﬁnition relaxes assumption assumes every term non-negligible probability evident example. size algorithm asks -local membership queries. running time ﬁrst loop opnmq loop terms running time second loop opmmq. running time polynomial also hypothesis algorithm returns formula logpnq logpdq probability seeing example satisﬁes evidently less union bound probability sample contain evident example every term least every ﬁrst loop. second loop remove terms remove terms contradicts examples since examples sample labeled never remove term part therefore probability least note done algorithm might create wrong term reason second loop. sample test every term added ﬁrst loop. example tipxq h‹pxq remove continue next term. denote probability sample satisfy event section provide evidence queries upper bounds crucial. show problem learning poly-sized decision trees reduced problem learning dnfs w.r.t. distributions realized small evident examples. learning decision trees widely believed intractable section present empirical results algorithm takes advantage extensive knowledge order perform smart feature selection. standard supervised classiﬁcation tasks user asked give label example. task additional information. speciﬁcally faced situation large number features features interpretation easily understood. every example sample asked user label addition asked features indicate instance labeled such. ﬁnished iterating entire sample used information relevant features narrow feature space. concretely trained linear classiﬁers features chosen indicative users. arguably algorithm gathers additional information manner similar using -local membership queries. -local query tests whether changing value single feature changes label. seen asking whether feature relevant prediction not. algorithm presented here relevant features broader way. namely explicitly words relevant corresponding label. humans make decisions often complex thought processes know whether access speciﬁc considerations used decision making process. ﬁrst goal experiment show least tasks important parts thought process easily accessible. i.e. annotators’ knowledge retrieved asking simple questions. second goal show using extra knowledge help signiﬁcantly decrease number tagged examples required. formulate goals using notion error decomposition. classiﬁer returned algorithm. decompose ldpˆhq approximation ldpˆhq approximation error) approximation error \u0001app measures good class linear classiﬁers restrict words since class linear informative features use. estimation error measures extent algorithm overﬁts data. formulate goals claims approximation estimation error. applying user induced feature selection mentioned increase approximation error reduce hypothesis class smaller one. want show feature space chosen users still expressive enough increase approximation error minor. addition show feature selection eﬀective sense estimation error decreases signiﬁcantly. sentiment analysis natural language processing task identifying attitude given text task studied community many years diﬀerent scale levels. started document level classiﬁcation task focus shifted handling sentence level newest focus sentiment analysis microblog data like twitter. working informal text genres users post opnions emotions recations practically everything presents challenges natural language processing beyond encountered working traditional text genres news-wire product reviews. indeed classical approaches sentiment analysis directly applicable tweets. focus relatively large texts e.g. movie product reviews tweets short ﬁne-grained. nevertheless great prominence social media last years encouraged focus sentiment detection microblogging domain. recent work sentiment analysis twitter data. examples chose task demonstrate method since example constructed limited number features making features important classiﬁcation. therefore seems information supplied users useful focusing attention important features. secondly fact claims hold enable smaller data important kind tasks since require large labeled data often costly. worked data semeval shared task sentiment analysis tweets dataset constructed tweets collected one-year period spanning january january tweets labeled using crowd sourcing tool amazon mechanical turk labels ﬁltered spammers. sentence users asked indicate overall sentiment sentence positive negative neutral also mark subjective words/phrases sentence. learning task worked classifying sentiment entire sentence. although want predict sentiment tweet labellings richer labelled data-set. i.e. instance training holds additional information sentiment words/phrases sentence indicate positive negative sentiment. results evaluated averaged scores. scoring function used semeval shared task overall common scoring function tasks. score harmonic mean precision recall. every label it’s score. positive label precision number tweets correctly labeled positive divided total number tweets labeled positive this labelling procedure originally intended used separate tasks. ﬁrst given tweet containing marked instance word phrase identify sentiment instance second identifying sentiment whole tweet compare variants feature space using entire feature space using query acquired feature space contains features selected users positive negative example. information data number features given table used simple naive bayes classiﬁer small smoothing parameter. also checked classiﬁcation algorithmsrandom forests logistic regression multiclass results present results unigram model. test scores language models almost identical feature spaces training scores gets higher model complexity expected. since training contains approximately instances chose present results simplest model number features would comparable number instances. results variants presented ﬁgure seen test scores algorithm outperforms variant uses additional information. diﬀerence test performance approximately constant across diﬀerent training sizes. getting back claims regarding approximation error looking ﬁnal training scores seen variants figure train test -scores naive bayes classiﬁer using entire feature space compared using queries-acquired feature space positive samples negative samples average positive negative scores. almost identical measurements. fact indicates increase approximation error. regarding improvement estimation error seen clearly looking test scores train scores. query acquired model smaller model. additional interesting properties seen precision recall graphs example looking results positive samples improvement results using query model almost improvement precision scores. data query model reaches test precision non-query model reaches test precision score even using whole data set. another interesting property seen small training used diﬀerence test scores query non-query methods twice large diﬀerence largest possible training used. figure train test precision recall scores naive bayes classiﬁer using entire feature space compared using queries-acquired feature space positive samples bottom negative samples left precision right recall. label term measures much appearance contributes fact correct label using terms sort features order conveys informativeness. since features words interesting insights looking informative features variant uses. look list chosen features variants almost identical. look algorithm uses entire feature space chooses signiﬁcant features clearly over-ﬁt training data. example nick lloyd justin unigram model saturday over-ﬁtting obviously decrease increase training size already stated generally natural language processing much harder acquire large labeled data set. therefore method avoids signiﬁcantly decrease kind over-ﬁtting high value. question raised whether improvement results eﬀect feature selection itself fact features selected query process important part. order answer this compared algorithm using automatic feature selection techniques. checked feature selection methods ﬁlter backward elimination. training number features method instructed select number features chosen users set. results presented ﬁgure training scores automatic feature selection techniques much lower training score using entire feature space fact reasonable much smaller hypothesis class. look test scores seen using feature selection techniques improve test score little compared feature selection still lies well score query acquired features method. figure train test averaged f-scores method compared automatic feature selection techniques ﬁlter method backward elimination well feature selection presented theoretical empirical evidence local-membership queries useful beneﬁcial. theoretical setup shown even -local queries stronger vanilla model arguably natural problem. empirical setup demonstrated getting additional information users significantly better results achieved. moreover data experiment created using crowdsourcing asking simple questions. shows getting extra knowledge easy task. today model practice almost non-existent. even popular models active learning pool-based stream-based fairly rare. e.g. recent survey annotation projects natural language processing tasks respondents stated ever decided active learning seems plenty room incorporating profound human knowledge ﬁeld machine learning especially since today knowledge collected quite easily. limitations general opq-local queries model. examples open questions limitations model uses opq-local queries comparison model uses logpnq-local queries?", "year": 2015}