{"title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "text": "relation detection core component many applications including knowledge base question answering paper propose hierarchical recurrent neural network enhanced residual learning detects relations given input question. method uses deep residual bidirectional lstms compare questions relation names different levels abstraction. additionally propose simple kbqa system integrates entity linking proposed relation detector make components enhance other. experimental results show approach achieves outstanding relation detection performance importantly helps kbqa system achieve state-of-the-art accuracy single-relation multi-relation benchmarks. knowledge base question answering systems answer questions obtaining information tuples input question systems typically generate query executed retrieve answers figure illustrates process used parse sample questions kbqa system single-relation question answered single <head-entity relation tail-entity> tuple complex case constraints need handled multiple entities question. kbqa system ﬁgure performs tasks entity linking links n-grams questions entities relation detection identiﬁes relation question refers main focus work improve relation detection subtask explore contribute kbqa system. although general relation detection methods well studied community studies usually take task kbqa consideration. result signiﬁcant general relation detection studies kb-speciﬁc relation detection. first general relation detection tasks number target relations limited normally smaller contrast kbqa even small like freebasem contains relation types. second relation detection kbqa often becomes zero-shot learning task since test instances unseen relations training data. example simplequestions data golden test relations observed golden training tuples. third shown figure kbqa tasks like webquestions need predict chain relations instead single relation. increases number target relation types sizes candidate relation pools increasing difﬁculty relation detection. owing reasons relation detection signiﬁcantly challenging compared general relation detection tasks. paper improves relation detection cope problems mentioned above. first order deal unseen relations propose break relation names word sequences question-relation matching. second noticing figure kbqa examples three components. single relation example. ﬁrst identify topic entity entity linking detect relation asked question relation detection based detected entity relation form query search correct answer love find way. complex question containing entities. using grant show topic entity could detect chain relations starring roles-series pointing answer. additional constraint detection takes entity constraint ﬁlter correct answer swingtown candidates found topic entity relation. original relation names sometimes help match longer question contexts propose build relation-level word-level relation representations. third deep bidirectional lstms learn different levels question representations order match different levels relation information. finally propose residual learning method sequence matching makes model training easier results abstract question representations thus improves hierarchical matching. order assess proposed improved relation detection could beneﬁt kbqa task also propose simple kbqa implementation composed two-step relation detection. given input question candidate entities retrieved entity linker based question proposed relation detection model plays role kbqa process re-ranking entity candidates according whether connect high conﬁdent relations detected question text relation detection model. step important deal ambiguities normally present entity linking results. finding core relation topic entity selection much smaller candidate entity re-ranking. steps followed optional constraint detection step question cannot answered single relations finally highest scored query improved relation detection model hierarchical matching questions relations residual learning; demonstrate improved relation detector enables simple kbqa system achieve state-of-the-art results single-relation multi-relation kbqa tasks. relation extraction relation extraction important sub-ﬁeld information extraction. general research ﬁeld usually works pre-deﬁned relation given text paragraph target entities goal determine whether text indicates types relations entities not. result usually formulated classiﬁcation task. traditional methods rely large amount hand-crafted features recent research beneﬁts advancement deep learning word embeddings deep networks like cnns lstms attention models research assumes ﬁxed relation types thus zero-shot learning capability required. number relations usually large widely used coarse/ﬁne-grained relations; semeval task relations; tackbp relations although considers open-domain wikipedia relations. much fewer thousands relations kbqa. result work ﬁeld focuses dealing large number relations unseen relations. proposed relation embeddings low-rank tensor method. however relation embeddings still trained supervised number relations large experiments. relation detection kbqa systems relation detection kbqa also starts featurerich approaches towards usages deep networks attention models many relation detection research could naturally support large relation vocabulary open relation sets order goal open-domain question answering. different kbqa data sets different levels requirement open-domain capacity. example gold test relations webquestions observed training thus prior work task adopted close domain assumption like general research. data sets like simplequestions paralex capacity support large relation sets unseen relations becomes necessary. main solutions pre-trained relation embeddings like factorize relation names sequences formulate relation detection sequence matching ranking task. factorization works relation names usually comprise meaningful word sequences. example split relations word sequences single-relation detection. liang also achieve good performance webqsp wordlevel relation representation end-to-end neural programmer model. character tri-grams inputs question relation sides. golub propose generative framework single-relation kbqa predicts relation character-level sequenceto-sequence model. search assumes argument entities available. thus usually beneﬁts features attention mechanisms based entity information relation detection kbqa information mostly missing because question usually contains single argument entity could multiple types makes entity typing difﬁcult problem previous used entity information relation detection model. previous research formulates relation detection sequence matching problem. however questions natural word sequences represent relations sequences remains challenging problem. give overview types relation sequence representations commonly used previous work. relation name single token case relation name treated unique token. problem approach suffers relation coverage limited amount training data thus cannot generalize well large number opendomain relations. example figure treating relation names single tokens difﬁcult match questions relation names episodes written starring roles names appear training data relation embeddings random vectors thus comparable question embeddings hqs. relation word sequence case relation treated sequence words tokenized relation name. better generalization suffers lack global information original relation names. example figure word-level matching difﬁcult rank target relation starring roles higher compared incorrect relation plays produced. incorrect relation contains word plays similar question table example relation types relation tokens questions asking relation. topic entity replaced token could give position information deep networks. italics show evidence phrase relation token question. embedding space. hand target relation co-occurs questions related appearance training treating whole relation token could better learn correspondence token phrases like show play types relation representation contain different levels abstraction. shown table word-level focuses local information relation-level focus global information suffer data sparsity. since levels granularity pros cons propose hierarchical matching approach relation detection candidate relation approach matches input question word-level relation-level representations ﬁnal ranking score. section gives details proposed approach. section describes hierarchical sequence matching residual learning approach relation detection. order match question different aspects relation deal three problems follows learning question/relation representations. provide model types relation representation word-level relationlevel. therefore input relation becomes {rword ﬁrst tokens words last tokens relation names e.g. {episode written} {starring roles series} transform token word embedding bilstms hidden representations initialize relation sequence lstms ﬁnal state representations word sequence back-off unseen relations. apply max-pooling sets vectors ﬁnal relation representation table different parts relation could match different contexts question texts. usually relation names could match longer phrases question relation words could match short phrases. different words might match phrases different lengths. result hope question representations could also comprise vectors summarize various lengths phrase information order match relation representations different granularity. deal problem applying deep bilstms questions. ﬁrst-layer bilstm works word embeddings question words {q··· gets hidden representations second-layer bilstm works second hidden representations since second bilstm starts hidden vectors ﬁrst layer intuitively could learn general abstract information compared ﬁrst layer. note ﬁrst-layer question representations necessarily correspond word-level relation representations instead either layer question representations could potentially match either level relation representations. raises difﬁculty matching different levels relation/question representations; following section gives proposal deal problem. figure proposed hierarchical residual bilstm model relation detection. note without dotted arrows shortcut connections layers model compute similarity second-layer questions representations relation thus hierarchical matching. overcome difﬁculties adopt idea residual networks hierarchical matching adding shortcut connections bilstm layers. proposed ways hierarchical residual matching connecting resulting position ﬁnal question representation becomes maxs ≤i≤n. applying maxpooling pooling respectively setting max. finally compute matching score given srel cos. intuitively proposed method beneﬁt hierarchical training since second layer ﬁtting residues ﬁrst layer matching layers representations likely complementary other. also ensures vector spaces layers comparable makes second-layer training easier. training adopt ranking loss maximizing margin gold relation relations candidate pool lrel max{ srel srel} constant parameter. summarizes hierarchical residual bilstm model. question contexts different lengths encoded unlike standard usage deep bilstms employs representations ﬁnal layer prediction expect layers question representations complementary compared relation representation space important task since relation token correspond phrases different lengths mainly syntactic variations. example table relation word written could matched either single word question much longer phrase writer could perform hierarchical matching computing similarity layer separately scores. however give signiﬁcant improvement analysis section shows naive method suffers training difﬁculty evidenced converged training loss model much higher single-layer baseline model. mainly deep bilstms guarantee two-levels question hidden representations comparable training usually falls local optima layer good matching scores always weight close remark another hierarchical matching consists relying attention mechanism e.g. correspondence different levels representations. performs hr-bilstm sections elaborate relation detection help re-rank entities initial entity linking re-ranked entities enable accurate relation detection. kbqa task result beneﬁts process. entity re-ranking step question text input relation detector score relations connections least entity candidates elk. call step relation detection entity since work single topic entity usual settings. hr-bilstm described sec. question generating score srel relation using hr-bilstm best scoring relations re-rank original entity candidates. concretely entity associated relations given original entity linker score slinker score q∩re conﬁdent relation scores re-rank entities srerank slinker q∩re finally select entities according score srerank form re-ranked list example illustrate idea. given input question example relation detector likely assign high scores relations episodes written author profession. then according connections entity candidates writer mike kelley scored higher baseball player mike kelley former relations episodes written profession. method viewed exploiting entity-relation collocation entity linking. candidate entity step question text input relation detector score relations associated entity because single topic entity input step following question reformatting replace candidate entity mention following previous work kbqa system takes existing entity linker produce top-k linked entities question generate queries following four steps illustrated algorithm entity re-ranking question text input relation detector score relations associated entities relation scores re-rank generate shorter list containing top-k entity candidates compared previous approaches main difference additional entity reranking step initial entity linking. step observed entity linking sometimes becomes bottleneck kbqa systems. example simplequestions best reported linker could top- accuracy identifying topic entities. usually ambiguities entity names e.g. writer baseball player mike kelley impossible distinguish entity name matching. observed different entity candidates usually connect different relations propose help entity disambiguation initial entity linking relations detected questions. simplequestions single-relation kbqa task. consists freebase subset entities order compare previous research. also evaluated relation extractor data released proposed question-relation pairs relation detection model data set. kbqa evaluation also start entity linking results. therefore results compared reported results tasks. webqsp multi-relation kbqa task. entire freebase evaluation purposes. following s-mart entity-linking outputs. order evaluate relation detection models create relation detection task webqsp data set. question labeled semantic parse ﬁrst select topic entity parse; select relations relation chains connected topic entity corechain labeled parse positive label others negative examples. tune following hyper-parameters development sets size hidden states lstms learning rate whether shortcut connections hidden states max-pooling results number training epochs. relation detection experiments second-step relation detection kbqa entity replacement ﬁrst word vectors initialized pretrained word embeddings embeddings relation names randomly initialized since existing pre-trained relation embeddings usually support limited sets relation names. leave usage pre-trained relation embeddings future work. relation detection results table shows results relation detection tasks. ampcnn result yielded state-of-the-art scores outperforming several attention-based meth. constraint detection similar adopt additional constraint detection step based text matching. method viewed entitylinking sub-graph. contains steps sub-graph generation given scored query generated previous steps node collect nodes connecting relation generate sub-graph associated original query. entity-linking sub-graph nodes compute matching score n-gram input question entity name taking account maximum overlapping sequence characters matching score larger threshold constraint entity query attaching corresponding node core-chain. experiments task introduction settings simplequestions webqsp datasets. question datasets labeled gold semantic parse. hence directly evaluate relation detection performance independently well evaluate kbqa task. starting top- query suffers error propagation. however still achieve state-of-the-art webqsp sec. showing advantage relation detection model. leave future work beam-search feature extraction beam ﬁnal answer re-ranking like previous research. table accuracy simplequestions webqsp relation detection tasks shows performance baselines. bottom give results proposed model together ablation tests. ods. re-implemented bicnn model questions relations represented word hash trick character tri-grams. baseline bilstm relation word sequence appears best baseline webqsp close previous best result ampcnn simplequestions. proposed hr-bilstm outperformed best baselines tasks margins note using relation names instead words results weaker baseline bilstm model. model yields signiﬁcant performance drop simplequestions however drop much smaller webqsp suggests unseen relations much bigger impact simplequestions. ablation test bottom table shows ablation results proposed hr-bilstm. first hierarchical matching questions relation names relation words yields improvement datasets especially simplequestions second residual learning helps hierarchical matching compared weighted-sum attention-based baselines attention-based baseline tried model one-way variations one-way model gives better results. note residual learning signiﬁcantly helps webqsp might help hierarchical matching. leave directions future work. help much simplequestions. simplequestions even removing deep layers causes small drop performance. webqsp beneﬁts residual deeper architecture possibly dataset important handle larger scope context matching. finally webqsp replacing bilstm hierarchical matching framework results large performance drop. simplequestions much smaller. believe lstm relation encoder better learn composition chains relations webqsp better dealing longer dependencies. analysis next present empirical evidences show hr-bilstm model achieves best scores. webqsp analysis purposes. first hypothesis training weighted-sum model usually falls local optima since deep bilstms guarantee two-levels question hidden representations comparable. evidenced training layer usually gets weight close thus ignored. example gives weights -./. layers also gives much lower training accuracy compared hr-bilstm suffering training difﬁculty. second compared deep bilstm shortcut connections hypothesis relation detection training deep bilstms difﬁcult without shortcut connections. experiments suggest deeper bilstm always result lower training accuracy. experiments two-layer bilstm converges even lower achieved single-layer bilstm. setting twolayer model captures single-layer model special case result suggests deep bilstm without shortcut connections might suffers training difﬁculty. finally hypothesize hr-bilstm combination bilstms residual connections encourages hierarchical architecture learn different levels abstraction. verify this replace deep bilstm question encoder single-layer bilstms shortcut connections hidden states. decreases test accuracy gives similar training accuracy compared hr-bilstm indicating serious over-ﬁtting problem. proves residual deep structures contribute good performance hr-bilstm. kbqa end-task results table compares system published baselines stagg stateof-the-art webqsp ampcnn state-of-the-art simplequestions. since baselines specially designed/tuned particular dataset generalize well applied dataset. order highlight effect different relation detection models kbqa end-task also implemented another baseline uses kbqa system replaces hr-bilstm implementation ampcnn char--gram bicnn relation detectors compared baseline relation detector method includes improved relation detector improves kbqa task note contrast previous kbqa systems system joint-inference feature-based re-ranking step nevertheless still achieves better comparable results state-of-the-art. third block table details ablation tests proposed components kbqa systems removing entity re-ranking step signiﬁcantly decreases scores. since reranking step relies relation detection models shows hr-bilstm model contributes good performance multiple ways. appendix gives detailed performance re-ranking step. contrast conclusion constraint detection crucial system. probably joint performance topic entity core-chain detection accurate leaving huge potential constraint detection module improve. finally like stagg uses multiple relation detectors three models used) also top- relation detectors section shown last table gives signiﬁcant performance boost resulting state-of-the-art result simplequestions result comparable state-of-the-art webqsp. relation detection step kbqa signiﬁcantly different general relation extraction tasks. propose novel relation detection model hr-bilstm performs hierarchical matching questions relations. model outperforms previous methods relation detection tasks allows kbqa system achieve state-of-the-arts. future work investigate integration hr-bilstm end-to-end systems. example model could integrated decoder provide better sequence prediction. also investigate emerging datasets like graphquestions complexquestions handle characteristics general references junwei duan zhao ming zhou tiejun zhao. constraint-based question anproceedings swering knowledge graph. coling international conference computational linguistics technical papers. coling organizing committee osaka japan pages hannah bast elmar haussmann. accurate question answering freebase. proceedings international conference information knowledge management. pages jonathan berant andrew chou frostig percy liang. semantic parsing freebase question-answer pairs. proceedings conference empirical methods natural language processing. association computational linguistics seattle washington pages antoine bordes nicolas usunier alberto garciaduran jason weston oksana yakhnenko. translating embeddings modeling multirelational data. advances neural information processing systems. pages zihang conditional focused neural question answering largescale knowledge bases. proceedings annual meeting association computational linguistics association computational linguistics berlin germany pages cicero santos bing xiang bowen zhou. classifying relations ranking conproceedings volutional neural networks. annual meeting association computational linguistics international joint conference natural language processing association computational linguistics beijing china pages matthew gormley mark dredze. improved relation extraction feature-rich comproceedings positional embedding models. conference empirical methods natural language processing. association computational linguistics lisbon portugal pages kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recogproceedings ieee conference nition. computer vision pattern recognition. pages chen liang jonathan berant quoc kenneth forbus lao. neural symbolic machines learning semantic parsers freebase weak supervision. arxiv preprint arxiv. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionaladvances neural information processing ity. systems. pages thien nguyen ralph grishman. employing word representations regularization prodomain adaptation relation extraction. ceedings annual meeting association computational linguistics association computational linguistics baltimore maryland pages ankur parikh oscar t¨ackstr¨om dipanjan jakob uszkoreit. decomposable attention model natural language inference. proceedings conference empirical methods natural language processing. association computational linguistics austin texas pages bryan rink sanda harabagiu. classifying semantic relations combining lexical semantic resources. proceedings international workshop semantic evaluation. association computational linguistics uppsala sweden pages huan brian sadler mudhakar srivatsa izzeddin zenghui xifeng yan. generating characteristic-rich question sets evaluation. conference empirical methods natural language processing. association computational linguistics austin texas pages https//aclweb.org/anthology/d-. ralph grishman satoshi sekine. semi-supervised relation extraction large-scale word clustering. proceedings annual meeting association computational linguistics human language technologies. association computational linguistics portland oregon pages ngoc thang heike adel pankaj gupta hinrich sch¨utze. combining recurrent convolutional neural networks relation classiﬁcaproceedings conference tion. north american chapter association computational linguistics human language technologies. association computational linguistics diego california pages wenpeng bing xiang bowen zhou hinrich sch¨utze. simple question answering attentive convolutional neural network. proceedings coling international conference computational linguistics technical papers. coling organizing committee osaka japan pages mark dredze raman arora matthew gormley. embedding lexical features lowrank tensors. proceedings conference north american chapter association computational linguistics human language technologies. association computational linguistics diego california pages http//www.aclweb.org/anthology/n-. daojian zeng kang siwei guangyou zhou zhao. relation classiﬁcation convolutional deep neural network. proceedings coling international conference computational linguistics technical papers. dublin city university association computational linguistics dublin ireland pages peng zhou tian zhenyu bingchen hongwei attentionbased bidirectional long short-term memory networks relation classiﬁcation. proceedings annual meeting association computational linguistics association computational linguistics berlin germany pages linlin wang gerard melo zhiyuan liu. relation classiﬁcation multi-level atproceedings annual tention cnns. meeting association computational linguistics association computational linguistics berlin germany pages learning pronatural north ceedings american chapter association computational linguistics human language technologies. association computational linguistics diego california pages http//www.aclweb.org/anthology/n-. siva reddy yansong feng songfang huang dongyan zhao. question answering freebase relation extraction textual evidence. proceedings annual meeting association computational linguistics association computational linguistics berlin germany pages yang ming-wei chang. s-mart novel tree-based structured learning algorithms applied tweet entity linking. proceedings annual meeting association computational linguistics international joint conference natural language processing association computational linguistics beijing china pages wen-tau ming-wei chang xiaodong jianfeng gao. semantic parsing staged query graph generation question answering knowledge base. association computational linguistics wen-tau xiaodong christopher meek. semantic parsing single-relation quesproceedings antion answering. nual meeting association computational linguistics association computational linguistics baltimore maryland pages removing entity re-ranking step results signiﬁcant performance drop table evaluates re-ranker separate task. re-ranker results large improvement especially beam sizes smaller indicating another important usage proposed improved relation detection model entity linking re-ranking. given input question entity name denote lengths question entity name |ne|. mention entity n-gram compute longest consecutive common sub-sequence between denote length lengths measured number characters. based numbers compute proportions length overlap entity mention entity name entity name |m∩e| question |m∩e| ﬁnal score question mention linking special threshold date constraints. time stamps usually follow yearmonth-day format time webqsp usually years. makes overlap date entities questions entity names smaller deal this check whether dates questions could match years thus special threshold date constraints. filtering constraints answer nodes. sometimes answer node could connect huge number nodes e.g. question asking country answer candidate u.s.. observation webqsp datasets found time gold constraints answers entity types based observation constraint detection step answer nodes keep tuples type relations common.topic.notable types education.educational institution.school type etc.", "year": 2017}