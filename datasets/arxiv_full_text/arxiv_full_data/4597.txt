{"title": "The IBMAP approach for Markov networks structure learning", "tag": ["cs.AI", "cs.LG"], "abstract": "In this work we consider the problem of learning the structure of Markov networks from data. We present an approach for tackling this problem called IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC algorithm, designed for avoiding important limitations of existing independence-based algorithms. These algorithms proceed by performing statistical independence tests on data, trusting completely the outcome of each test. In practice tests may be incorrect, resulting in potential cascading errors and the consequent reduction in the quality of the structures learned. IBMAP contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-a-posteriori approach. The approach is instantiated in the IBMAP-HC algorithm, a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures. We present an extensive empirical evaluation on synthetic and real data, showing that our algorithm outperforms significantly the current independence-based algorithms, in terms of data efficiency and quality of learned structures, with equivalent computational complexities. We also show the performance of IBMAP-HC in a real-world application of knowledge discovery: EDAs, which are evolutionary algorithms that use structure learning on each generation for modeling the distribution of populations. The experiments show that when IBMAP-HC is used to learn the structure, EDAs improve the convergence to the optimum.", "text": "abstract work consider problem learning structure markov networks data. present approach tackling problem called ibmap together eﬃcient instantiation approach ibmap-hc algorithm designed avoiding important limitations existing independence-based algorithms. algorithms proceed performing statistical independence tests data trusting completely outcome test. practice tests incorrect resulting potential cascading errors consequent reduction quality structures learned. ibmap contemplates uncertainty outcome tests probabilistic maximum-aposteriori approach. approach instantiated ibmap-hc algorithm structure selection strategy performs polynomial heuristic local search space possible structures. present extensive empirical evaluation synthetic real data showing algorithm outperforms signiﬁcantly current independence-based algorithms terms data eﬃciency quality learned structures equivalent computational complexities. also show performance ibmap-hc real-world application knowledge discovery edas evolutionary algorithms structure learning generation modeling distribution populations. experiments show ibmap-hc used learn structure edas improve convergence optimum. schl¨uter bromberg edera lab. dharma artiﬁcial intelligence departamento sistemas informaci´on facultad regional mendoza universidad tecnol´ogica nacional argentina. tel. e-mail {federico.schluterfbrombergaedera}frm.utn.edu.ar present work ibmap approach robust learning markov network structures data together ibmap-hc eﬃcient hill-climbing instantiation approach. markov networks together bayesian networks belong family probabilistic graphical models computational framework compactly representing joint probability distributions. large list applications graphical models wide range ﬁelds areas computer vision image analysis computational biology biomedicine evolutionary computation among many others. probabilistic graphical models composed undirected directed graph numerical parameters node graph represents random variable domain edges encode conditional independences among them. reason graph also called independence structure distribution. importance independences factorize joint distribution domain variables factors subsets variables resulting important reductions space complexity representing distribution structure obtained knowledge human expert commonly hard obtain always enough design accurate structure. interesting problem attracted considerable attention learning automatically independence structure categorical data drawn unknown probability distribution however problem known general np-hard problem since number structures grows super-exponentially markov network structure learning broad approaches mainly considered literature score-based independence-based algorithms hand score-based algorithms combine measure goodness structure data metric complexity structure; instance maximize log-likelihood maximum likelihood parameters given structure. recently several eﬃcient instantiations approach developed hand independence-based algorithms proceed performing statistical independence tests data based outcome tests discards structures inconsistent test. approach eﬃcient correct assumptions practice presents quality problems assumptions correctness independence tests true practice data insuﬃcient. important mention score-based independence-based approaches motivated distinct learning goals. according existent literature score-based approaches better suited density estimation goal tasks inferences predictions required contrast independence-based methods better suited learning goals feature selection classiﬁcation knowledge discovery ibmap follows independence-based approach learning structure markov network. approach designed robust assumption correctness statistical tests valid. instead trusting outcome statistical tests data ibmap considers explicitly posterior probability independences given data. explained detail later posteriors tests combined posterior whole structure deciding output structure following well-known maximuma-posteriori approach. clearly circumvents cascading error traditional independence-based algorithms true structure longer discarded incorrect test results lower posterior probability. tests posterior probability true structure increase again. order evaluate improvements quality structures produced approach performed detailed systematic experiments synthetic datasets real-world datasets. cases compared structural errors structures learned ibmap-hc learned representative state-of-the-art competitors gsmn hhc-mn simple adaptation markov networks independence-based structure learning algorithm bayesian networks called note structural errors quality measure appropriate knowledge discovery algorithms using independence-based approach. additionally tested performance ibmap-hc real world application estimation distribution algorithms evolutionary algorithms able solve problems known hard traditional genetic algorithms edas variations well-known evolutionary algorithms replace crossover mutation stages generating population solutions sampling probability distribution learned selected population. experiment edas motivated fact quality structure learning expected inﬂuence results optimization. occurs structure learning step made generation optimization populations generated sampling distribution learned. accurate structure learned eﬀective sampling generating good solutions. experiment tested ibmap-hc markovianity optimization algorithm state-of-the-art based markov network structure learning. show improves convergence optimum ibmap-hc used learn structure. rest work organized follows. section presents overview independence-based learning approach motivates contribution. section presents ibmap approach section details ibmap-hc algorithm. section shows experiments synthetic real datasets section shows experiments edas. finally section summarizes work poses several possible directions future work. paper also includes appendices end. section provides background markov networks deﬁnes problem structure learning motivates independence-based approach. hereon capital letters denote single random variables sets variables bold. markov network representing underlying distribution domain random variables consists undirected graph potential functions deﬁned numerical parameters graph conditional independences independences problem structure learning takes input dataset assumed representative sample underlying distribution commonly structured tabular format column random variable domain data point. optimal solution problem perfectmap structure encodes dependences independences present closer perfect-map better structure learned better resulting markov network representing independence-based algorithms learn perfect-map performing succession statistical independence tests discarding iteration structures inconsistent outcome test deciding tests perform next based outcomes learned far. statistical independence test statistic computed testing random variables conditionally independent given conditioning variables disjoint subsets domain independence assertion denoted hx⊥⊥y computational cost test proportional number rows number variables involved test. examples independence tests used practice mutual information pearson’s bayesian test continuous gaussian data partial correlation test among others. several advantages independence-based algorithms. first learn structure without interleaving expensive task parameter estimation reaching sometimes polynomial complexities number statistical tests performed. complete model required parameters estimated learned structure. another important advantage algorithms guaranteed learn correct structure underlying distribution long following assumptions hold graph-isomorphism i.e. independences distribution encoded undirected graph; underlying distribution strictly positive i.e. every assignment iii) outcomes tests correct i.e. independences learned true unfortunately third assumption rarely true practice number contingency tables statistic computed grows exponentially number variables conditioning test. therefore effective dataset statistic computed decreases exponentially size thus degrading exponentially quality statistics. tests outcome incorrect independences independence-based algorithms produce commonly called cascade errors discard true underlying structure confuse algorithm test perform next. approach tackles main issue independence-based algorithms contemplating uncertainty outcome tests probabilistic maximum-a-posteriori approach. describe main contribution work ibmap approach markov network structure learning. approach avoids cascade errors traditional independence-based algorithms trust completely outcome statistical tests. this central idea ibmap pose structure learning task maximum-a-posteriori problem computing posterior probability possible structure given data. formally approach posterior computed combining outcome conditional independence assertions determine structure call closure structure. remaining section describes closure computing posteriors next section ibmap-hc algorithm presented eﬃciently instantiation optimization. deﬁnition undirected independence structure positive graph-isomorph distribution closure conditional independence assertions {ci} suﬃcient determining completely. posterior closure given data seen joint probability distribution individual independence assertions given data. applying chain rule assertions obtain best author’s knowledge method exists computing exactly probabilities independence assertions conditioned independence assertions data. common approximation assume independence assertions closure mutually independent. assumption made implicitly independence-based markov network structure learning algorithms statistical tests used black section presents structure learning algorithm ibmap-hc instantiation ibmap approach. ibmap-hc performs heuristic hill-climbing search space possible structures thus name. ﬁrst give high-level overview algorithm describe speciﬁc aspects closure used computing ib-score heuristic used speeding-up search complexity overall algorithm. ibmap-hc searches structure maximum ib-score considering neighboring structures structures result ﬂipping edge algorithm presents pseudo-code. algorithm input parameter dataset used computing statistical independence tests. search starts line creating structure nodes edges. then ib-score computed line saved variable current-score. hill-climbing search starts loop line loop iterates calling select-nextstructure function line select neighbor maximum score saved variable since number possible neighbor structures expensive cost computing ib-score them. explained detail section then line score best neighbor computed saved variable neighbor-score. algorithm stops neighbor proposed improve current score condition checked line termination criterion reached variables current-score re-assigned variables neighbor-score lines process repeated local optimum found. computing ib-score candidate structures deﬁne closure called markov blanket closure presented next subsection. closure designed determine structure number independence tests quadratic number variables domain. markov blanket closure closure follows deﬁnition closure designed using markov blanket domain variable denoted terms graphs markov blanket deﬁned nodes connected edge node structure i.e. adjacency set. terms independences allows consider conditionally independent non-adjacent variables graph given markov blanket. property deﬁne markov blanket closure closures computed independently variable. formally theorem undirected independence structure positive graphisomorph distribution markov blanket closure conditional independence assertions suﬃcient completely determining structure closure contains assertions number quadratic size domain assertions variables. allows decompose computation ib-score independent variable ib-scores incrementally score neighbor structure based previous computation score structure given diﬀers edge blankets aﬀected requiring recompute reusing remaining variable ib-scores. consequently cost computing line algorithm reduced tests i.e. tests. finally convenience explanation select-next-structure function next section decompose considering variable ib-score composed terms called pairwise ib-scores follows ib-score one. neighbor would required perform statistical tests computing ib-score using markov blanket closure resulting total cost tests ascent hill-climbing search. computing incrementally ib-score neighbor cost ascent still results cost statistical tests structure total cost tests ascent. order reduce expensive computation time ibmap-hc uses heuristic estimates optimal neighbor without single test computation i.e. cost test computations. select-next-structure function shown algorithm input parameter current structure corresponding score point already computed. function ﬁrst selects line optimal pair least accurate edge current structure done representing data structure contains pairwise scores using decomposable form then best neighbor constructed line copy pair ﬂipped returned. understand minimization shown line algorithm note number neighbors diﬀering edge number diﬀerent pairs variables i.e. pairs. point view seen pairwise ib-scores pair variables resulting following expression ib-score form clear minimization ﬁnds pair whose contribution smallest. assumption made heuristic structure resulting ﬂipping would similar maximizing ib-score among neighboring structures. explained section computing incrementally need recomputed. approximation made minimization consists assuming ignoring remaining terms based fact that expected strong change terms since posterior dependence used structure posterior independence used other. contrast terms ignored assumed mild change markov blanket change therefore assertions vary conditioning set. approximation possible pairwise ib-scores corresponding ﬂipped edge complementary structures since posterior independence posterior dependence sums allows estimate pairwise ib-score without single test computation. estimation made implicitly minimization. heuristic assumes ignored terms minimal impact search optimal neighbor. course approximation empirical results shed light eﬀectiveness. worst case approximation would result selection sub-optimal neighbor. this however diﬀerent many optimization algorithms follow sub-optimal paths given complexity problem impact approximation assessed empirically. later experiments show despite approximation approach useful avoiding cascade eﬀect traditional independence-based algorithms outperforming always state-of-the-art algorithms data scarce. additionally appendix presents empirical measurements landscape ib-score section summarizes resulting computational cost whole algorithm using hill-climbing search markov blanket closure select-nextstructure function. begin expensive operation algorithm computation ib-score initial structure line algorithm computed non-incrementally using tests markov blanket closure; cost tests. next main loop algorithm calling selectnext-structure function cost incremental computation line requires compute tests; cost finally denoting number ascents termination overall computational cost algorithm since obtained empirically experimental section shows measurements diﬀerent scenarios proving empirically source extra degree complexity grows sub-linearly resulting overall computational complexity statistical tests. section describes several experiments synthetic real datasets testing empirically robustness approach ibmap eﬃciency algorithm ibmap-hc. report detailed systematic experimental comparison ibmap-hc state-of-the-art independence-based structure learning algorithms. show comparison quality structures learned solution quality structures learned gsmn state-of-the-art independence-based algorithm terms quality. introduce also competitor called hhc-mn adaptation learning structure markov networks algorithm state-of-the-art independence-based algorithm learning bayesian networks. comparing algorithms ground using bayesian test statistical independence test. gsmn algorithm learns structure ﬁnding markov blanket variable domain algorithm solution structure constructed adding edge variable variables found markov blanket. algorithm learns markov blanket variable phases grow shrink phases. grow phase algorithm increases tentative markov blanket every variable found dependent conditioning currently tentative markov blanket. phase tentative markov blanket contains members true markov blanket potentially includes false positives non-members. false positives removed shrink phase variables found independent conditioned current markov blanket removed set. phase tentative markov blanket matches true markov blanket assumption correctness tests. algorithm learns structure learning parents children variable interleaved hiton-pc symmetry correction algorithm pseudo-code algorithm seen learning variable algorithm starts empty candidate ranking variables priority inclusion candidate unconditional dependence discarding variables found unconditionally independent then algorithm utilizes inclusion heuristic function accepts variable candidate set. variable inside candidate becomes independent given subset candidate algorithm removes variable candidate never considers again. inclusion function elimination strategy iterated interleaved variables examine inclusion. complexity hiton-pc largest size found complexity hiton-pc executed variable domain. case markov networks equivalent variable neighbors exactly markov blanket. therefore expected hiton-pc learns markov blanket markov network thus used part learn undirected structure. fact proven analytically here conﬁrmed empirically cases considered section. markov network learning algorithm simply omit ﬁnal step orients edges obtain markov blanket denoting resulting algorithm hhc-mn. ﬁrst experiments conducted synthetic datasets generated using gibbs sampler randomly generated markov networks allows systematic controlled study provides datasets known underlying structures control complexity problem better assess quality structures learned algorithm. measuring structural errors structures learned report hamming distance learned structure underlying i.e. false positive false negative edges learned structure. another quality measure work assessing structures learned well known f-measure harmonic mean precision recall quality measures commonly used information retrieval community. precision indicates good algorithm learning correct independences instead recall indicates good algorithm learning independences correct independences present real structure then f-measure fig. mean standard deviation repetitions hamming distance models learned algorithms gsmn hhc-mn ibmap-hc increasing sizes random synthetic datasets domain sizes rows. synthetic random markov networks generated domains binary variables. domain size random networks generated increasing connectivities considering edges ﬁrst variable pairs random permutation variable pairs. worth mentioning increasing values increasingly diﬃcult learn structure. given markov networks report quality structures learned gsmn hhc-mn ibmap-hc using portions dataset increasing number datapoints combination. independence structure determines factorization distribution potential functions subset variables clique structure. determine complete model must determine numerical parameters quantify potential functions. datasets generated correctly strongly represent direct dependencies encoded edges considered experiments pairwise cliques factorization models two-variable factors edge random structure generated numerical parameters correlation strong. that forced parameters result log-odds ratio pairwise figures show mean values standard deviations repetitions hamming distances f-measure structures learned algorithms considered respectively. plots ordered columns diﬀerent values rows diﬀerent values. expected results show algorithms complex underlying structure larger number structural errors value used. seen algorithm ﬁxed value amount errors grows also grows since gsmn hhc-mn follow traditional independence-based approach expected obtain good qualities data suﬃcient i.e. cases larger values lower values ﬁgures show clearly both ibmap-hc hhc-mn always learn structures qualities signiﬁcantly better gsmn. cases gsmn slowest convergence reduce structural errors. because selected domain sizes gsmn tend many false positives grow phase shrink phase require perform tests contains many variables i.e. reliable. produces numerous cascade errors. case hhc-mn seen structural errors reduced signiﬁcantly respect gsmn. improvements obtained elimination strategy well interleaving inclusion heuristic function elimination strategy. compared ibmap-hc latter always outperforms hhc-mn terms structural errors except following speciﬁc cases cases data seem suﬃcient hhc-mn improve quality algorithm ibmap-hc. underlying structures dense topology elimination strategy results eﬃcient. contrast case data suﬃcient fig. mean standard deviation repetitions f-measure models learned algorithms gsmn hhc-mn ibmap-hc increasing dataset sizes random synthetic datasets domain sizes rows. hhc-mn work well exponential size tests required elimination strategy. extreme case conditioning sets average variables cases tests require larger amounts data reliable. general ﬁgures conﬁrm ibmap-hc always outperforms signiﬁcantly competitors data scarce conﬁrm hypothesis probabilistic approach ibmap avoids cascade eﬀect traditional independence-based algorithms. also data suﬃcient qualities obtained competitive. fig. mean standard deviation repetitions runtime required algorithms gsmn hhc-mn ibmap-hc increasing dataset sizes random synthetic datasets domain sizes rows. main memory. results show clearly gsmn expensive algorithm cases tend many false positives grow phase shrink phase require perform tests contains many variables source extra computational cost. extreme cases ibmap-hc expensive gsmn cases hill-climbing search ibmap-hc seem expensive alternative. hhc-mn algorithm requires lowest computation time cases inclusion heuristic interleaved elimination strategy really eﬀective underlying structure value suﬃciently large obtain reliable tests. situations algorithm converge termination criterion quickly. instead conclude section show additional experiment conﬁrm empirically ibmap-hc achieves polynomial time complexities number random variables domain stated section shown figure presents measurements increasing problem sizes results obtained datasets generated previous experiments. ﬁgure shows average values repetitions problems increasing values x-axis line indicating grows sub-linearly. omit results diﬀerent values similar. second experiments synthetic datasets conducted underlying structures diﬀerent topology ising spin glasses models mathematical models ferro-magnetism statistical mechanics also used last decades many domains computer vision applications using models underlying structure datasets generated random ising models binary variables. figure shows results diﬀerent random repetitions. graphs ﬁgure ordered rows diﬀerent values showing mean value standard deviation hamming distance f-measure runtime ﬁrst second third columns respectively. ﬁgures show clearly both ibmap-hc hhc-mn always learn structures lower hamming distance higher f-measure gsmn cases gsmn algorithm slowest convergence reduce fig. mean standard deviation repetitions hamming distance f-measure runtime algorithms gsmn hhc-mn ibmap-hc increasing dataset sizes ising synthetic datasets domain sizes rows. structural errors among three algorithms. respect hhc-mn seen always lower structural quality ibmap-hc except speciﬁc case data seem suﬃcient hhc-mn improve quality ibmap-hc. general ﬁgures conﬁrm ibmap-hc outperforms signiﬁcantly competitors terms quality. also conﬁrm hypothesis probabilistic approach ibmap avoids cascade eﬀect traditional independence-based algorithms. regard computational complexity results figure shows corresponding running times expressed milliseconds. computer used running experiments described previous section. analysis runtime results similar runtime analysis previous section gsmn expensive cost large amount expensive tests hhc-mn good performance data suﬃcient ibmap-hc best performance data suﬃcient section show experiments real-world benchmark datasets obtained repositories machine learning datasets since underlying network unknown datasets possible compute neither hamming distance f-measure. instead utilize accuracy quality measure counts number conditional independences present data correctly encoded structure learned. measure used purpose related works contrast measures evaluate density complete probability distribution accuracy better suited goal learning work evaluate speciﬁcally structural errors. accuracy deﬁned normalized measure counting number matches comparison independence queries hold test also hold structure learned training set. conditional independences read learned structure vertex separation denotes possible conditional independence queries domain variables checked many queries independent test learned structure training set. then number matches normalized unfortunately size exponential approximated accuracy computed randomly samconducted experiment using real-world datasets listed table column one. datasets sorted domain size second column. dataset shuﬄed data divided training learning structure test computing accuracy table also shows information number attributes number datapoints available train test sets dataset used train input gsmn hhc-mn ibmap-hc algorithms accuracy obtained structure learned algorithm shown ﬁfth sixth seventh columns respectively. dataset best performance among three algorithms indicated bold. results show datasets ibmap-hc resulted better accuracy cases resulted ties remaining cases best results obtained hhc-mn gsmn cases ibmap-hc always outperforms competitors cases data seem scarce consistent results synthetic datasets ibmap-hc outperforms always competitors data scarce. table accuracy several benchmark data sets. structure learned using subsample called train accuracy computed using test set. evaluation measure best performance indicated bold. contrast benchmark datasets comes arbitrary applications present results evaluating ibmap-hc real world application knowledgediscovery estimation distribution algorithms variations well-known evolutionary algorithms perform selection variation stages replace crossover mutation stages estimation sampling task generating population. former stage estimate probability distribution current population generating next population sampling estimation stage edas estimate probability distribution dataset corresponding current population. associate gene random variable individual joint assignment variables selected population sample distribution. rationale replacing crossover methods estimation estimating distribution selected individuals best ﬁtted sampling stage would produce novel well-ﬁtted individuals. recently several markov network based edas proposed model distribution populations test-bed considered markovianity optimization algorithm state-of-the-art mn-based learns markov network structure population using efﬁcient structure learning algorithm based mutual information simple independence-based structure learning algorithm described detail work designed speciﬁcally moa. sampling conducted variation gibbs sampler requires structure model avoiding need learn model parameters. implementation takes advantage experts information indicating maximum number neighbor variables variable have denoted tested diﬀerent values observing great sensitivity value. algorithm ibmap-hc parameter. experiments value closest true value resulting best possible performance i.e. strongest competitor ibmap-hc. table results moa’ onemax problem increasing problem sizes terms critical population size mean standard deviation repetitions number ﬁtness evaluations required obtain global optimum. lower values better. conducted experiments compare ibmap-hc alternative structure learning within denoted moa′ denoting original version uses thesis better structure learning algorithm improves convergence optimum reached computing fewer evaluations ﬁtness individuals. versions tested benchmark functions widely used eda’s literature royal road onemax bit-string optimization tasks detailed reason benchmark functions widely used hard optimize ﬁtness landscape large areas discontinuous. context evolutionary algorithms functions model bit-string chromosome gene. royal road problem variables arranged groups size goal maximize number string adding ﬁtness count group otherwise adding example case individual separated groups ﬁrst third group contribute ﬁtness count example equals underlying independence structure learned therefore contains cliques size group. experiments used former known literature onemax. example ﬁtness onemax. clearly optimal individual problems table results moa’ royal road problem increasing problem sizes terms critical population size mean standard deviation repetitions number ﬁtness evaluations required obtain global optimum. lower values better. experiments iterated generations optimum reached whatever happened ﬁrst. several runs diﬀering initial population measured success rate fraction times optimum found. commonly used performance measure edas critical population size minimum population size success rate smaller values double beneﬁt runtime fewer ﬁtness evaluations reaching optima faster distribution estimation. report number ﬁtness evaluations required population size denoted robust algorithms expected require smaller values. measure royal road onemax version times population sizes then measured report average standard deviation runs. experiments population truncated selection size elitism used preventing diversity loss. parameter royal road onemax respectively. results presented table onemax problem table royal road problem. algorithms moa′ table reports values well average standard deviation increasing problem sizes onemax problem royal road problem lower values better. tables results show moa′ always present equal lower values also moa′ always outperforms royal road larger improvement moa′ requires fewer ﬁtness evaluations halved. onemax larger improvement moa′ requires fewer ﬁtness evaluations reduced quarter. interpretation results ibmap-hc estimates better distribution iteration. conﬁrm hypothesis compared structures learned algorithms synthetic datasets. dataset hamming distances ibmap-hc respectively. respectively; respectively. results show clearly quality ibmap-hc indeed outperforms finally highlight eﬃciency ibmap-hc allowed large problems genes size estimating structure many generations. paper proposes ibmap novel independence-based maximum-a-posteriori approach learning structure markov networks; ibmap-hc efﬁcient instantiation ibmap. approach avoids cascade errors traditional independence-based algorithms trust completely outcome statistical tests. this central idea ibmap pose structure learning task maximum-a-posteriori problem computing posterior probability possible structure given data. experiments comparing ibmap-hc state-of-the-art independence-based algorithms indicate method improves cases independence-based competitors equivalent computational complexities. ibmap-hc also tested practical challenging setting estimation distribution algorithms resulting faster convergence optimum state-of-the-art markov network algorithm selected benchmark functions. experimental results conclusions appendix conﬁrm eﬀectiveness structure selection strategy. therefore believe worth guiding future work improving ib-score measure i.e. relaxing independence assumption made equation well exploring alternative closure sets. also clearly worthwhile considering testing approach practical real world testbeds potentially comparing performance state-of-the-art score-based algorithms work funded grant pict- national agency scientiﬁc technological promotion foncyt argentina; grant pid- national technological university argentina; scholarship program teachers national technological university ministry science technology productive innovation; argentina. special thanks roberto santana siddhartha shakya help support implementing experiments edas. appendix presents formal proof markov blanket closure described deﬁnition section fact closure i.e. independence assertions completely determine structure used generate start reproducing necessary theoretical results extracted pairwise markov property intersection property conditional independence strong union property conditional independence satisﬁed markov network positive graph-isomorph distribution deﬁnition conditional independences among random variables graph-isomorph distribution satisfy following strong union property conditional independence proof. proof proceeds ﬁrst applying strong union property l.h.s. obtain hx⊥⊥y applying pairwise property conclude r.h.s. remaining proof need argue something similar counterproof. proof proceeds extending conditioning l.h.s. whole domain apply counter-positive reach r.h.s. that apply intersection property iteratively taking iteration pair containing independences l.h.s. ﬁrst iteration dependence l.h.s. following iterations dependence resulting applying intersection. cases take process detail. ﬁrst iteration take l.h.s. dependence independence ﬁrst obtaining intersection dependence take resulting dependence independence following denoted convenience seems intersection longer applied respective conditioning sets z∪{w match. however graph-isomorphism strong union property conditional independence satisﬁed therefore independence given conditioning follows independence given subset conditioning particular then hx⊥⊥w intersection therefore applied resulting following iteratively reach resulting conditioning result recalling theorem undirected independence structure positive graph-isomorph distribution markov blanket closure conditional independence assertions suﬃcient completely determining structure appendix report results experiment analyzes empirically landscape ib-score function synthetic datasets. experiment consists analysis surface ib-score complete search space possible structures. assess good hill-climbing search maximizing ib-score. exponential number possible networks domain ﬁrst instance explore complete landscape ib-score looks like datasets small domain size experiment used synthetic datasets similar used section fig. complete landscape ib-score synthetic datasets increasing dataset sizes rows. x-axis sort structures hamming distance correct structure. y-axis shows ib-score structures landscape. structure found ibmap-hc indicated diamond. plots figure show y-axes values ib-score possible structures sort structures x-axes hamming distance true underlying structure dataset note scores structures appear probabilities computed shown equation layout structures left less structural errors also expected higher value ib-score. therefore structures fig. fraction landscape ib-score synthetic datasets increasing dataset sizes rows. x-axis sort structures hamming distance correct structure. y-axis shows ib-score structures landscape. structure found ibmap-hc indicated diamond. plots ordered columns increasing values dataset rows diﬀerent values increasing complexity problem. analysis plots observed landscape shapes decreasing curve increasing value achieved precision statistical tests improves increasing second place diamond indicates position landscape structure learned ibmap-hc algorithm achieves always structure highest score value. also observed error structure learned ibmap-hc closer zero increasing conclude appendix worth noting results conﬁrm eﬀectiveness structure selection strategy maximizing ib-score complete landscape. reason conclude worth guiding future work improvement ib-score measure", "year": 2013}