{"title": "Scale Normalization", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.", "text": "difﬁculties training deep neural networks caused improper scaling layers. scaling issues introduce exploding gradient problems typically addressed careful scale-preserving initialization. investigate value preserving scale isometry beyond initial weights. propose methods maintaing isometry exact stochastic. preliminary experiments show determinant scale-normalization effectively speeds learning. results suggest isometry important beginning learning maintaining leads faster learning. goal many initialization methods preserve gradient signal goes backwards layer neural network glorot bengio saxe rnns preserving signal lead well-known vanishing exploding gradient problems hochreiter general learning neural nets much faster composite scales layers remains near constant problem saxe results line work suggest initially preserving scale conducive learning. however update rule enforce scale-preservation violate condition ﬁrst iteration. preserving scale continue speed learning ﬁrst iteration? evidence against. hand scale preserved throughout epochs network would fail learn non-isometric projections. however circumstantial evidence beneﬁt preserving scale training co-training unsupervised supervised objectives leads faster-learning generalizable networks rasmus least result scale-preserving effect unsupervised objective effectively regularizes singular values weight matrix. contribution work investigate utility scale-preserving constraints. separate effects unsupervised objective normalizing scale without optimizing reconstruction. preliminary results different methods indicate least ﬁrst iterations training scale normalization leads faster learning. preserving scale forward scale layer weight matrix effect length input function singular values backward scale layer incoming gradient. used calculate forward pass used calculate outcoming gradient backward pass. magnitude gradient given layer product original gradient’s magnitude scales layers scales greater exploding gradients; less vanishing gradients. saxe suggests using orthogonal matrices avoid complications scale. effectively makes matrices non-scaling orthonormal initialization scheme contrasts popular initializations glorot bengio based preserving variance forward backward passes. beneﬁt singular value interpretation scale interpretation thus unites notions forward backwards scales yields constructive method creating non-scaling weight matrices scaling function singular values relies much less assumptions scaling noted simply making unitary update would decorrelate destroy information contained make sense every neuron’s output range. orthonormalizing works initialization make sense course training. method preserving scale layer pseudo-determinant one. determinant matrix product eigenvalues pseudo-determinant product aggregate measure scales product singular values sets determinant one. alternatively determinant-normalziation sets geometric mean scales sense centering singular values. test determinant-normalization relu mnist models trained using vanilla batch size preliminary results indicate maintaining isometry useful learning least beginning. future work relate scale-normalization batch-normalization advanced optimization algorithms. experiments larger datasets cifar convolutional architectures progress. believe investigating isometry interplays learning speed bring insight speed learning future.", "year": 2016}