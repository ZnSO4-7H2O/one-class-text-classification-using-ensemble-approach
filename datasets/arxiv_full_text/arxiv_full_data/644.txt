{"title": "Adversarial Transformation Networks: Learning to Generate Adversarial  Examples", "tag": ["cs.NE", "cs.AI", "cs.CV"], "abstract": "Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "text": "multiple different approaches generating adversarial examples proposed attack deep neural networks. approaches involve either directly computing gradients respect image pixels directly solving optimization image pixels. work present fundamentally method generating adversarial examples fast execute provides exceptional diversity output. efﬁciently train feed-forward neural networks self-supervised manner generate adversarial examples target network networks. call network adversarial transformation network atns trained generate adversarial examples minimally modify classiﬁer’s outputs given original input constraining classiﬁcation match adversarial target class. present methods train atns analyze effectiveness targeting variety mnist classiﬁers well latest state-of-the-art imagenet classiﬁer inception resnet resurgence deep neural networks many realworld classiﬁcation tasks increased interest methods generate training data well weaknesses trained models. effective strategy achieve goals create adversarial examples trained models misclassify. adversarial examples small perturbations inputs carefully crafted fool network producing incorrect outputs. small perturbations used offensively fool models giving wrong answer defensively providing training data weak points model. seminal work szegedy goodfellow well much recent work shown adversarial examples abundant many ways discover them. given classiﬁer original inputs problem generating untargeted adversarial examples expressed optimization argminx∗ s.t. distance metric examples input space similarly generating targeted adversarial attack classiﬁer expressed argminx∗ s.t. target label chosen attacker. optimization problems solved using three broad approaches directly using optimizers like l-bfgs adam proposed szegedy carlini wagner optimizer-based approaches tend much slower powerful approaches. approximation single-step gradient-based techniques like fast gradient sign fast least likely class approaches fast requiring single forward backward pass target classiﬁer compute perturbation. approximation iterative variants gradient-based techniques approaches multiple forward backward passes target network carefully move input towards adversarial classiﬁcation. another axis compare considering adversarial attacks whether adversary access internals target model. attacks without internal access possible transferring successful attacks model another model szegedy papernot others. challenging class blackbox attacks involves access relevant model getting online access target model’s output explored papernot baluja tram`er papernot detailed discussion threat models. work propose adversarial transformation networks neural network transforms input adversarial example target network networks. atns untargeted targeted trained black-box white-box manner. work focus targeted white-box atns. inference. inference time input without requiring access gradient computations. means trained generate adversarial examples target network even faster single-step gradient-based approaches fast gradient sign long ||gf|| ||f||. loss functions. input-space loss function would ideally correspond closely human perception. however simplicity sufﬁcient. determines whether targeted; target refers class adversary cause classiﬁer output maximum value. work focus challenging case creating targeted atns deﬁned similarly equation note training labels target network required point process. required target network’s outputs therefore possible train atns self-supervised manner unlabeled data input make argmax reranking function. variety options reranking function. simplest onehot formulations make better signal already present encourage better reconstructions. work look reranking functions attempt keep particular maintains rank order targeted class order minimize distortions computing gft. speciﬁc used experiments following form additional parameter specifying much larger current classiﬁcation. norm normalization function rescales input valid probability distribution. perturbation generate perturbation sufﬁcient structure variation residual block tanh) represents core function small initial weight vectors structure makes easy network learn generate small effective perturbations. adversarial autoencoding atns similar standard autoencoders attempt accurately reconstruct original input subject regularization weight decay added noise signal. atns regularizer imposes additional requirement perturbation approaches order enforce plausible member generate values valid input range images sufﬁces activation function last layer tanh function; constrains output channel training objective resembles standard generative adversarial network training goal weaknesses classiﬁer. interesting note similarity work outside adversarial training paradigm recent feed-forward neural networks artistic style transfer images gatys originally proposed gradient descent procedure based back-driving networks modify inputs fully-trained network inputs maximize desired outputs hidden unit activations. unlike standard network training gradients used modify weights network here network weights frozen input changed. subsequent work ulyanov created method approximate results gradient descent procedure off-line trained neural network. ulyanov removed need gradient descent procedure operate every source image artistic style applied replaced single forward pass separate network. analagously generating adverarial examples separately trained network approximates usual gradient descent procedure done target network adversarial examples. begin empirical exploration train networks standard mnist digit classiﬁcation task networks trained tested data; vary weight initialization architecture shown table network convolution fully connected layers. input networks grayscale image output logit units. classiﬁerp classiﬁera architecture differ initialization weights. primarily classiﬁerp experiments section. networks used figure simple classiﬁcation network takes input image input emits classiﬁcation network. example shown input digit classiﬁed correctly takes input generates modiﬁed image classiﬁer outputs highest activation previous highest classiﬁcation second highest activation attempt create adversarial autoencoding target speciﬁc class given input image. trained particular classiﬁer illustrated figure takes original input image input outputs image target classiﬁer erroneously classify also constraint maintain ordering classes initially output classiﬁer. train atns classiﬁerp target digit example provided make concrete. classiﬁer given image digit successful ordering outputs follows classiﬁerp applied resulting image classiﬁer following ordering outputs desired classiﬁerp) training single atnt proceeds follows. weights classiﬁerp frozen never change training. every training image passed classiﬁerp obtain output described equation compute copying value easy better performance mnist experiments important variety architectures achieved similar accuracy state-ofthe-art performance. table average success atn− transforming image misclassiﬁed classiﬁerp. reduced ability fool classiﬁerp increases. read table cell percentage times classiﬁerp labeled middle cell percentage times classiﬁerp labeled kept original classiﬁcation second place. bottom cell percentage kept original classiﬁcation second place. renormalizing setting valid probability distribution. sets target class highest value maintaining relative order original classiﬁcations. mnist experiments empirically given train atnt generate minimizing using equation though weights classiﬁerp frozen error derivatives still passed train atn. explore several values balance loss functions. results shown table experiments. tried three architectures task trained three values targets full experiments shown table accuracies shown ability atnt transform input image classiﬁerp mistakenly classiﬁes measurement table average networks atn−. results. figure represents transformation atnt makes digits initially correctly classiﬁed example digits classiﬁed cases second highest classiﬁcation original correct classiﬁcation reconstructions shown figure largest smaller values shown bottom row. ﬁdelity underlying digit diminishes reduced. however loosening constraints stay similar original input number trials transfigure successful adversarial examples atnt classiﬁerp. highest bottom respectively. note decreased ﬁdelity underlying digit decreases. column block corresponds correct classiﬁcation image. corresponds adversarial classiﬁcation figure typical transformations made mnist digits classiﬁerp. black digits white background output classiﬁcations classiﬁerp. bottom classiﬁcation original classiﬁcation. classiﬁcation result classifying adversarial example. white digits black backgrounds mnist digits transformations adversarial examples. bottom mnist digits unmodiﬁed adversarial. images adversarial example classiﬁed argmax maintaining second highest output original classiﬁcation argmax former network able successfully fool classiﬁcation network increases dramatically seen table interestingly figure transformed digit appears. high example found could transformed successfully fool classiﬁerp. smaller values anomaly occur. figure provide closer look examples atnc points noted transformations maintain large empty regions image. unlike many previous studies attacking classiﬁers addition salt-and-pepper type noise appear majority generated examples shape digit dramatically change. desired behavior training networks maintain order beyond top-output minimal changes made image. changes often introduced patches light strokes become darker. vertical-linear components original images emphasized several digits; especially noticeable digits transformed digits difﬁcult consistent pattern emphasized cause classiﬁcation network fooled. novel aspect atns though cause target classiﬁer output erroneous top-class also trained ensure transformation preserves existing output ordering target-classiﬁer examples successfully transformed table gives average rank-difference outputs pre-and-post transformed images section explores three extensions basic atns increasing number networks atns attack using hidden state target network using atns serial parallel. table atnb trained defeat classiﬁerp. tested classiﬁers without training measure transfer. place percentage times classiﬁcation. place measures many times original class correctly placed place conditioned place correct unconditioned place results table clearly show transformations made general; tied network trained attack. even classiﬁera architecture classiﬁerp susceptible attacks different architectures. looking second place correctness scores ﬁrst seem counter-intuitive conditional probability correct second-place classiﬁcation remains high despite ﬁrst-place classiﬁcation. reason cases able successfully change classiﬁer’s choice second choice remained close second thereby maintaining high performance conditional second rank measurement. training multiple networks. possible create network able create single transform attack multiple networks? generalize better unseen networks? test this created receives training signals multiple networks shown figure earlier training reconstruction error remains. trained classiﬁcation signals three networks classiﬁerp classiﬁera. training proceeds exactly manner described earlier except attempts minimize three target networks time. results shown table first examine columns corresponding networks used training note success rates attacking three classiﬁers consistently high comparable next turn remaining networks adversary given access training. large increase success rates trained single target network however results match networks used training. possible training larger numbers target networks time could increase transferability adversarial examples. finally look success rates image transformations. images consistenly fool networks failure cases networks different? shown figure networks trained defeat majority transformations attacked three networks successfully. unseen networks results mixed; majority transformations successfully attacked single network. come ability attack networks ﬁrst-position. rather looking conditional-successes second-position numbers improved speculate extra hints provided classiﬁer’s internal activations could used also ensure second-place classiﬁcation input modiﬁcation also correctly maintained. table using internal states classiﬁer inputs adversary networks. larger font percentage times adversarial class classiﬁed top-space. smaller font many times original class correctly placed place conditioned place correct not. ﬁrst test started images digits test set. passed atns resulting images classiﬁed classiﬁerp. image measured many atns able successfully transform image trials successfully figure transformed examples work well networks? percentage examples worked exactly training networks. percentage examples worked exactly unseen networks. note measured independent test images. experiments thus classiﬁer treated white box. pieces information needed train atn. first actual outputs used create target vector. second error derivatives target vector passed propagated atn. section examine possibility opening classiﬁer accessing internal state. actual hidden unit activations example used additional inputs atn. intuitively goal maintain much similarity possible original image maintain order non-top-most classiﬁcations original image access activations convey usable signals. large number hidden units accompany convolution layers practice penultimate fully-connected layer results training atns extra information shown table interestingly salient difference figure parallel serial application atns. left examples original image transformed correctly atns. example ﬁrst column transformed classiﬁer output class second column classiﬁer output etc. middle histogram showing number images transformed successfully least atns used parallel. right serial adversarial transformation networks. ﬁrst column applied input image. second column applied output etc. examples atns successfully transformed previous image fool classiﬁer. note severe image degradation transformation networks applied sequence. second experiment constructed atns applied serially one-after-the-other. scenario ﬁrst applied image yielding applied yielding atn. goal whether transformations work previously transformed images. results chaining atns together manner shown figure transformations applied larger image degradation. expected ninth transformation majority images severely degraded usually recognizable. though expected degradation images additional surprising ﬁndings. first parallel application atns images successfully transformed atns. experiment images successfully transformed atns. improvement number all- successes applying atns parallel occurs transformation effectively diminishes underlying original image meanwhile pixels added cause misclassiﬁcation also trained minimize reconstruction error. overarching effect fading image chaining atns together. second interesting examine happens second-highest classiﬁcations networks also trained preserve. order preservation occur test. test worked perfectly input-image applied ﬁrst second classiﬁcations respectively. subsequently applied classiﬁcations etc. reason hold practice though networks trained maintain high classiﬁcation original digit trained maintain potentially small perturbations made achieve top-classiﬁcation therefore applied changes made survive transformation. nonetheless chaining adversaries becomes important training atns images previously modiﬁed atns sufﬁcient method address difference training testing distributions. left future work. explore effectiveness atns imagenet dataset consists million natural images categorized classes. target classiﬁer used experiments pre-trained state-of-the-art classiﬁer inception resnet top- single-crop error rate image validation top- error rate described fully szegedy trained atns p-atns described section attack training follows process described section takes input images scaled pixels channels each. autoencode images size ir-resize-conv small architecture avoids checkerboard artifacts common deconvolutional layers using bilinear resize layers downsample upsample stride convolutions; perturbation approach ir-base-deconv ir-conv-fc many parameters architectures large fully-connected layers. fully-connected layers cause network learn slowly autoencoding approach used learn perturbations quickly hyperparameter search. architectures across tasks trained hyperparameters. architecture task trained four networks target class binoculars soccer ball volcano zebra. total trained different atns attack good hyperparameters networks series grid searches reasonable parameter values learning rate using volcano target class. training runs terminated after epochs training steps batch size based parameter search results reported here learning rate runs trained epochs shufﬂed training images using adam optimizer tensorflow default settings. order avoid cherrypicking best results networks trained selected four images unperturbed validation ﬁgures paper prior training. training ﬁnished evaluated atns passing images validation measuring ir’s accuracy adversarial examples. table shows top- adversarial accuracy model/target combinations. approach superior perturbation approach terms top- adversarial accuracy terms training success. nonetheless results figures show using architecture like ir-conv-fc provide qualitatively different type adversary approach.the examples generated using perturbation approach preserve pixels original image expense small region large perturbations. contrast architectures distribute differences across wider regions image. however ir-base-deconv ir-conv-deconv tend exhibit checkerboard patterns common problem image generation deconvolutions checkerboarding ir-resize-conv avoids checkerboard pattern gives smooth outputs interestingly three networks many original high-frequency patterns replaced high frequencies encode adversarial signal. results ir-base-deconv show network architectures perform substantially differently trained p-atns atns. since p-atns learning perturb input networks much better preserving original image perturbations focused along edges corners image. form perturbations often manifests itself deepdream-like images figure approximately perturbation place used across input examples. placing perturbations manner less likely disrupt classiﬁcations thereby keeping lower. stark contrast atns creatively modify input seen figures adversarial diversity. figure shows atns capable generating wide variety adversarial perturbations targeting single network. previous approaches generating adversarial examples often produced qualitatively uniform results various amounts noise image generally concentrating noise pixels large gradient magnitude particular adversarial loss function. indeed hendrik metzen recently showed possible train detector previous adversarial attacks. perspective attacker then adversarial examples produced atns provide past defenses cat-andmouse game security since somewhat unpredictable diversity likely challenge approaches defense. perhaps much interesting consequence diversity potential application comprehensive adversarial training described below. figure adversarial diversity. left column selected zoomed samples. right columns successful adversarial examples different target classes variety atns. left zebra binoculars soccer ball volcano. images selected random successful adversaries target class. unlike existing adversarial techniques adversarial examples tend look alike adversarial examples exhibit great deal diversity quite surprising. example consider second image space shuttle zebra column case made lines tarmac darker organic somewhat evocative zebra’s stripes. clearly human would mistake image zebra. similarly dog’s face speckled orange dots sufﬁcient convince volcano. diversity improving effectiveness adversarial training diverse pool adversarial examples lead better network generalization. images ir-conv-deconv. images ir-resize-conv. image p-atn ir-conv-fc. using adveraries improving training. single step iterative gradient methods possible increase network’s robustness adversarial examples suffering small loss accuracy clean inputs. however works adversary network trained against. appears atns could used adversarial training architecture could provide substantially diversity trained model current adversaries. adversarial diversity might improve model test-set generalization adversarial robustness. atns quick train relative target network reliably produce diverse adversarial examples automatically checked quality could used follows train atns targeting random subset output classes checkpoint target network. atns trained replace fraction training batch corresponding adversarial examples subject constraints current classiﬁer incorrectly classiﬁes adversarial example target class loss adversarial example threshold indicates similar original image. given stops producing successful adversarial examples replace newly trained targeting another randomly selected class. manner throughout training target network would exposed shifting diverse adversaries atns trained fully-automated manner. deepdream perturbations. ir-conv-fc exhibits interesting behavior seen architectures. network builds perturbation generally contains spatially coherent recognizable regions target class. example figure consistent soccer-ball ghost image appears transformed images. methods goals perturbations quite different generated deepdream qualitative results appear similar. ir-conv-fc seems learn distill target network’s representation target class manner drawn across large fraction image. result hints direct conceptually resembles training many ways goal different gans focus using easy-to-train discriminator learn hard-to-train generator; adversarial training system focus using easy-to-train generators learn hard-to-train multi-class classiﬁer. note also adversarial example generation algorithm unlabeled data described section miyato also describe method using unlabeled data manner conceptually similar adversarial training. figure deepdream-style perturbations. four different images perturbed ir-conv-fc targeting soccer ball. images outlined successful adversarial examples images outlined green change ir’s top- classiﬁcation. network learned approximately perturbation images. perturbation resembles part soccer ball results akin found deepdream-like processes high frequency data. atns remove high frequency data images building reconstructions. likely limitations underlying architectures. particular three convolutional architectures difﬁculty exactly recreating edges input image spatial data loss introduced downsampling padding. consequently loss penalizes high conﬁdence predictions edge locations leading networks learn smooth boundaries reconstruction. strategy minimizes overall loss also places lower bound error imposed pixels regions high frequency information. lower bound loss regions provides network interesting strategy generating output focus adversarial perturbations regions input image high-frequency noise. strategy visible many interesting images figure example many networks make minimal modiﬁcation image substantial changes around edges dog’s face exactly error would high nonadversarial reconstruction. figure architecture comparisons. left adversarial autoencoding atns. right perturbation atns. networks ﬁgure trained hyperparameters apart target class varies among zebra soccer ball volcano. images bottom show absolute difference original image adversarial examples. substantial differences different architectures generate adversarial examples. ir-base-deconv ir-conv-deconv tend exhibit checkerboard patterns reconstructions common problem image generation deconvolutions general. ir-resize-conv avoids checkerboard pattern tends give smooth outputs. ir-base-deconv performs quite differently trained p-atn rather atn. p-atns focus perturbations along edges image corners presumably less likely disrupt classiﬁcations. p-atns turns networks learn mostly ignore input simply generate single perturbation applied input image without much change although perturbations networks target image vary substantially. stark contrast atns creatively modify input individually seen figure table imagenet architectures. maxpool deconv deconv deconv deconv image conv bilinear resize conv bilinear resize conv bilinear resize conv bilinear resize conv bilinear resize conv conv image conv conv conv deconv deconv deconv deconv image conv conv conv image amples. presented fundamentally different approach ﬁnding examples training neural networks convert inputs adversarial examples. method efﬁcient train fast execute produces remarkably diverse successful adversarial examples. hendrik metzen recently showed possible detect input adversarial current types adversaries. possible train detectors output. using signal additional loss improve outputs. similarly exploring discriminator training improve realism outputs. would interesting explore impact atns generative models rather classiﬁers similar work finally also possible train atns black-box manner similar recent work tram`er baluja using reinforce compute gradients using target network simply reward signal. references baluja shumeet covell michele sukthankar rahul. virtues peer pressure simple method discovering int. conf. computer analysis high-value mistakes. images patterns springer deng dong socher richard li-jia fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee nguyen yosinski jason clune jeff. deep neural networks easily fooled high conﬁdence predictions unrecognizable images. corr abs/. http//arxiv.org/abs/.. papernot nicolas mcdaniel patrick somesh fredrikson matt celik berkay swami ananthram. limitations deep learning adversarial settings. proceedings ieee european symposium security privacy papernot nicolas mcdaniel patrick goodfellow ian. transferability machine learning phenomena black-box attacks using adversarial samples. arxiv preprint arxiv. papernot nicolas mcdaniel patrick goodfellow somesh celik berkay swami ananthram. practical black-box attacks deep learning systems using adversarial examples. arxiv preprint arxiv. szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. arxiv preprint intriguing properties neural networks. arxiv. ulyanov dmitry lebedev vadim vedaldi andrea lempitsky victor texture networks feed-forward synthesis textures stylized images. corr abs/. http//arxiv.org/abs/.. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron advances bengio yoshua. generative adversarial nets. neural information processing systems miyato takeru maeda shin-ichi koyama masanori nakae ishii shin. distributional smoothing virtual adversarial training. international conference learning representations", "year": 2017}