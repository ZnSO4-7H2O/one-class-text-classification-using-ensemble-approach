{"title": "Tensorizing Generative Adversarial Nets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Generative Adversarial Network (GAN) and its variants demonstrate state-of-the-art performance in the class of generative models. To capture higher dimensional distributions, the common learning procedure requires high computational complexity and large number of parameters. In this paper, we present a new generative adversarial framework by representing each layer as a tensor structure connected by multilinear operations, aiming to reduce the number of model parameters by a large factor while preserving the quality of generalized performance. To learn the model, we develop an efficient algorithm by alternating optimization of the mode connections. Experimental results demonstrate that our model can achieve high compression rate for model parameters up to 40 times as compared to the existing GAN.", "text": "generative adversarial network variants demonstrate state-of-the-art performance class generative models. capture higher dimensional distributions common learning procedure requires high computational complexity large number parameters. paper present generative adversarial framework representing layer tensor structure connected multilinear operations aiming reduce number model parameters large factor preserving quality generalized performance. learn model develop efﬁcient algorithm alternating optimization mode connections. experimental results demonstrate model achieve high compression rate model parameters times compared existing gan. generative adversarial networks demonstrate high performance within class generative models. success accomplished algorithmic advance also recent growth computational capacity. example state-of-the-art generative adversarial models utilize enormous computational resource constructing deep adversarial models large number model parameters train powerful graphics processing units dependency expensive gpus train gans limits ﬂexibility deploy deep generative models standard computational systems mobile devices. another problem address paper multimodality dataset. natural datasets often possess multimodal structure. instance video described -dimensional tensor samples multilayer perceptron necessary input data linear space. thus vectorization operations applied sample datasets feeding network. ﬂattening process lead losing rich multi-modal information high-dimensional data curse dimensionality. several tensor decompositions tensor-train decomposition tensor ring decomposition overcome curse dimensionality. tensor decompositions linear growth space complexity dimension original tensor increases. enforcing low-rank constraints low-rank tensor approximation algorithm able preserve ﬂexibility represent high-dimensional tensors moderately small number model parameters. studies utilize low-rank tensor approximations artiﬁcial neural networks. particular novikov kossaiﬁ propose low-rank tensor approximation weight parameters shows applying lowrank tensor approximation lead reduction space complexity model. although applying decomposistion dense matrices demonstrates large factor compression rate ﬁnding optimal tt-ranks still remains difﬁcult problem. related works proposed framework represent hidden input output layers tensor structure. treat multidimensional input without vectorization operation aiming preserve original multi-modal information. also replace hidden layer multi-way arrays. reduce number model parameters improve computational efﬁciency fully connected layers modiﬁed multilinear contraction operation also theoretical grounding. call resulting layer tensor layer networks consisting tensor layers ft-nets stands fully tensorized networks. addition tensor layer ft-nets call adversarial networks ft-nets tgan. empirically demonstrate proposed framework achieve high compression rate model parameters real synthetical datasets. synthetical experiment observed model converges ground truth faster given approximately number model parameters. section review several preliminary knowledge basic tensor arithmetics generative adversarial nets. section introduce tensor layer. section demonstrate state-of-the-art learning algorithms swiftly applicable ft-net. section present experimental result followed conclusion section generative adversarial nets consist models called generator discriminator usually represented mlps. task discriminator correctly identify whether input belongs real data distribution pdata model distribution pmodel. given prior generator tries produce indistinguishable samples deceive discriminator. alternatively training discriminator generator aims implicitly learn distributions pdata pmodel. although cost function generator theoretically fulﬁlls purpose training process stabilized maximizing instead minimizing reason behind alternation saturated gradient caused discriminator outperforming generator early stage training. section introduce tensor layer using tensor algebra reviewed section intuitively tensor layer layer without vectorization. train artiﬁcial neural network common ﬂatten inputs feed network. particular given input vector following transformation applied neural networks proposed model number weight matrices connectionx layers order input tensor. establish transformation tensor layers applying combinations mode product operations input tensor addition bias tensor lies space input space. given n-way tensor ri×i···×in input transformation tensor layer rj×j×···×jn formulated multiple operations necessary construct tensor layer. particular n-mode product kronecker product hadamard product described below. comprehensive review tensor arithmetics notations order tensor number dimensions. vector matrix tensor order three higher denoted respectively. given nth-order tensor ri×i×···×in entry denoted xii...in in∀n denote integers ranged inclusive. mode-n ﬁbers vectors obtained ﬁxing every index n-th index tensor. mode-n matricization mode-n unfolding tensor denoted arranges mode-n ﬁbers columns resulting matrix. given matrices size ri×j hadamard product denoted resulting matrix also size n-mode product tensor ri×i×···×in matrix rj×in denoted entry resulting tensor denoted fig. visualization three-way tensor layer. given input three-way tensor ri×j×k three weight matrices rl×i rm×j rn×k output rl×m×n call resulting tensor tensor layer weight matrices. tensor layers demonstrate equivalent ﬂexibility fully connected layers weight matrices within connection. constraint imposed tensor layer size weight matrices. size weight matrices correlated size input tensor layer. transformation applied tensor layer equivalent transformation component tucker decomposition given core tensor casts multiplication tensor foldings core tensor shape tensor. paper network consisting tensor layers referred tf-nets. following section demonstrate application state-of-the-art gradient-based optimization methods tf-nets. multilayer models often trained gradient-based optimization algorithms. section demonstrate application optimization algorithms backpropagation ft-net. given ft-net input tensor ri×···×in×c tensor layers ×···×jn×c rk×···×kn×c build ft-net follows. rji×ii rki×ji function component wise activation function applied tensor note last mode input tensor denotes size batch thus apply mode product mode performing multilinear operation given tensor layer derive gradient respect weight matrix using equation note kronecker products weight matrices descending order. ordering dependent upon whether model adopts column-major order rowmajor order. paper adopt column-major ordering however orders adaptable learning. follows. given neural network three fully-connected layers number model parameters network model complexity network greatly reduced converting network ft-net. conversion number model parameters ft-net difﬁcult difference number parameters tensorized un-tensorized networks grow exponentially dimension input tensor increases. section describe experimental details tgan handwritten digits purpose experiment demonstrate computational space efﬁciency model applied real dataset. conducted experiments tgan gans mnist. particular larger amount model parameters compared tgan approximately fig. samples tgan three gans different size layers. blue-colored ﬁgure represents scatter plot samples generated training stage. green-colored ﬁgures represent kernel density estimation samples. prior every synthetical experiment. tgan three gans training step. experiment result demonstrates that; model converges original distribution quickly appropriate low-rank constraints assigned tgan useful capturing level spaces. present generative adversarial model tensorized structure tgan. major advantages tgan are; signiﬁcant reduction consumption computational resource capability capture multi-modal structure datasets. empirically demonstrated negligible impact quality samples generated framework compared samples generated achieving times reduction number model parameters. future work consider apply various tensor decomposition algorithms tensor layer. tensor-train decomposition could applied layer reduction number model parameters. fig. comparison mnist samples generated tgan gans. collected samples ﬁgure without cherry-picking. used prior sample randomly tgan gans. left right column-wise sample tgan training step respectively. table table showing conﬁgurations real synthetic experiments denotes tensor layer lying space ra×b×c denotes input layer size denotes output layer size denotes fully-connected layer size number model parameters tgan does. result experiment figure comparing samples generated tgan gans observable samples tgan demonstrate curvilinear feature samples generated gans report factor compression rate number model parameters between tgan times. table detailed description layer total number model parameters models. section empirically compare tgan gans using synthetic data. experiments tgan gans trained capture bivariate normal distributions. particular collected data points sampled clusters located circularly around point cluster populated bivariate normal distribution note frameworks approximately number trainable units. figure represents samples populated andrzej cichocki danilo mandic lieven lathauwer guoxu zhou qibin zhao cesar caiafa phan tensor decompositions signal processing applications two-way multiway component analysis ieee signal processing magazine vol. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio generative adversarial nets advances neural information processing systems alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial networks arxiv preprint arxiv. zhang hongsheng shaoting zhang xiaolei huang xiaogang wang dimitris metaxas stackgan text photo-realistic image synthesis stacked generative adversarial networks arxiv preprint arxiv. jiajun chengkai zhang tianfan bill freeman josh tenenbaum learning probabilistic latent space object shapes generative-adversarial modeling advances neural information processing systems jean kossaiﬁ aran khanna zachary lipton tommaso furlanello anima anandkumar tensor contraction layers parsimonious deep nets arxiv preprint arxiv. wenlin chen james wilson stephen tyree kilian weinberger yixin chen compressing neural networks hashing trick international conference machine learning", "year": 2017}