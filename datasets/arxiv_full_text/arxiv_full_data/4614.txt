{"title": "Temporal Difference Updating without a Learning Rate", "tag": ["cs.LG", "cs.AI"], "abstract": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.", "text": "derive equation temporal diﬀerence learning statistical principles. speciﬁcally start variational principle bootstrap produce updating rule discounted state value estimates. resulting equation similar standard equation temporal diﬀerence learning eligibility traces called however lacks parameter speciﬁes learning rate. place free parameter equation learning rate speciﬁc state transition. experimentally test learning rule oﬀers superior performance various settings. finally make preliminary investigations extend temporal diﬀerence algorithm reinforcement learning. combine update equation watkins’ sarsa oﬀers superior performance without learning rate parameter. ﬁeld reinforcement learning perhaps popular estimate future discounted reward states method temporal diﬀerence learning. unclear exactly introduced ﬁrst however ﬁrst explicit version temporal diﬀerence learning rule appears witten idea follows expected future discounted reward state task time compute estimate state information base estimate current history state transitions current history observed rewards equation suggests time value γvst+ provides information perhaps estimate increased vice versa. intuition gives following estimation heuristic state shortcoming method time step value last state updated. states last state also aﬀected changes last state’s value thus could updated too. happens called temporal diﬀerence learning eligibility traces history trace kept states recently visited. method update value state also back trace updating earlier states well. formally state eligibility trace computed powerful version temporal diﬀerent learning known main idea paper derive temporal diﬀerence rule statistical principles compare standard heuristic described above. superﬁcially work similarities lstd references therein). however lstd concerned ﬁnding least-squares linear function approximation developed general update time quadratic number features/states. hand algorithm exactly coincides td/q/sarsa ﬁnite state spaces novel learning rate derived statistical principles. therefore focus comparison td/q/sarsa. recent survey methods learning rate section derive least squares estimate value function. expressing estimate incremental update rule obtain form call section compare simple markov chain. test random markov chain section non-stationary environment section section derive methods policy learning based compare sarsa watkins’ simple reinforcement learning problem. section ends paper summary thoughts future research directions. empirical future discounted reward state actual rewards following state time steps rewards discounted future. formally empirical value state time future rewards geometrically discounted practice exact value always unknown depends rewards already observed also unknown future rewards. note visited state twice diﬀerent times imply observed rewards following state visit diﬀerent time. close possible true expected future discounted reward thus state would like close furthermore non-stationary environments would like discount evidence parameter formally want minimise loss function since depends future rewards equation used current form. next note self-consistency property respect rewards. speciﬁcally tail future discounted reward state depends empirical value time following examining equation usual update equation temporal diﬀerence learning eligibility traces however learning rate replaced learning rate derived statistical principles minimising squared loss estimated true state value. derivation exploited fact latter must self-consistent bootstrapped equation gives equation learning rate state transition time opposed standard temporal diﬀerence learning learning rate either ﬁxed free parameter transitions decreased time monotonically decreasing function. either case learning rate automatic must experimentally tuned good performance. derivation appears theoretically solve problem. ﬁrst term seems provide type normalisation learning rate though intuition behind clear meaning second term however understood follows measures often visited state recent past. therefore state value estimate based relatively samples state value estimate based relatively many samples. situation second term boosts learning rate st+. opposite situation less visited state reverse occurs learning rate reduced order maintain existing value ﬁrst test consider simple markov process states. step state number either incremented decremented equal probability unless system state case always transitions state following step. state transitions reward generated transition reward generated. transitions reward discount value computed true discounted value state running brute force monte carlo simulation. algorithm times markov chain computed root mean squared error value estimate across states time step averaged across run. optimal value expected given environment stationary thus discounting experience helpful. high system would learn fast brieﬂy becoming stuck. lower learning rate ﬁnal performance improved however initial performance much worse results tests appear figure similar tests performed larger smaller markov chains diﬀerent values consistently superior across tests. wonders whether fact implicit learning rate uses ﬁxed. test explored performance number diﬀerent learning rate functions state markov chain described above. found functions form always performed poorly however good performance possible setting correctly functions form results much closer averaged runs. results appear figure variable learning rate performing much better however still unable equation reduced learning rate would outperform evidence adapting learning rate optimally without need manual equation tuning. test markov process complex transition structure created random state markov process. creating transition matrix element probability uniformly random number interval otherwise. scaled transition states interpreted probability distribution state follows state compute reward associated transition parameter simply environment stationary. experimented range parameter settings learning rate decrease functions. found ﬁxed learning rate decreasing rate performed reasonable well never well results generated averaging runs shown figure although structure markov process quite diﬀerent used previous experiment results similar preforms well better beginning run. furthermore stability error towards better manual learning tuning required performance gains. parameter introduced equation reduces importance observations computing state value estimates. environment stationary useful however non-stationary environment need reduce value state values adapt properly changes environment. rapidly environment changing lower need make order rapidly forget observations. test setting used markov chain section reduced size states speed convergence. used markov chain ﬁrst time steps. point changed reward transitioning last state middle state time switched back original markov chain alternating models environment every steps. switch also changed target state values algorithm trying estimate match current conﬁguration environment. experiment expected optimal value fell would expect given phase steps long. optimal value around optimum learning rate around would expect algorithms pushed optimal value caused poor performance periods following switch environment hand setting produced initially fast adaption environment switch poor performance next environment change. accurate statistics averaged runs. results tests appear figure reason learns faster ﬁrst half ﬁrst cycle equally fast start following cycle. sure happening. could improve initial speed learnt last three cycles reducing however comes performance cost terms lowest mean squared error attained cycle. case non-stationary situation performed well. reinforcement learning algorithms watkins’ sarsa based temporal diﬀerence updates. suggests reinforcement learning algorithms based possible. ﬁrst experiment took standard sarsa algorithm modiﬁed obvious temporal diﬀerence update. presentation algorithm changed notation slightly make things consistent typical reinforcement learning. speciﬁcally dropped super script implicit algorithm speciﬁcation deﬁned reinforcement learning algorithm call given algorithm essentially changes standard sarsa algorithm code compute visit counter loop compute values replace temporal diﬀerence update. test standard sarsa used windy gridworld environment described page world grid squares agent move going either down left right. agent attempts move grid simply stays agent starts column receives reward ﬁnds column. make things diﬃcult wind blowing agent columns strong wind columns illustrated figure unlike original version problem continuing discounted task automatic transition goal state back start state. computed empirical future discounted reward point time. value oscillated also moving average values window length lasted time steps allowed level learning algorithm topped out. results appear figure averaged runs accurate statistics. despite putting considerable eﬀort tuning parameters sarsa unable achieve ﬁnal future discounted reward settings shown graph represent best ﬁnal value could achieve. comparison easily beat result slightly slower sarsa start. setting able achieve performance sarsa start however performance slightly better sarsa. combination superior performance fewer parameters tune suggest beneﬁts carry reinforcement learning setting. another popular reinforcement learning algorithm watkins’ similar sarsa above simply inserted temporal diﬀerence update usual algorithm obvious way. call algorithm hlq. test environment exactly used sarsa above. results time competitive nevertheless despite spending considerable amount time tuning parameters unable beat hlq. performance advantage relatively modest main beneﬁt achieved level performance without tune learning rate. derived equation setting learning rate temporal diﬀerence learning eligibility traces. equation replaces free learning rate parameter normally experimentally tuned hand. every setting tested stationary markov chains non-stationary markov chains reinforcement learning method produced superior results. theoretical understanding next step would prove method converges correct estimates. done certain assumptions learning rate decreases time. hopefully something similar proven method. terms experimental results would interesting diﬀerent types reinforcement learning problems clearly identify ability learning rate diﬀerently different state transition pairs helps performance. would also good generalise result episodic tasks. finally successfully merged sarsa watkins’ would also like done peng’s perhaps reinforcement learning algorithms.", "year": 2008}