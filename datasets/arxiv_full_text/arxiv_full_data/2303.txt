{"title": "Learning with Abandonment", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Consider a platform that wants to learn a personalized policy for each user, but the platform faces the risk of a user abandoning the platform if she is dissatisfied with the actions of the platform. For example, a platform is interested in personalizing the number of newsletters it sends, but faces the risk that the user unsubscribes forever. We propose a general thresholded learning model for scenarios like this, and discuss the structure of optimal policies. We describe salient features of optimal personalization algorithms and how feedback the platform receives impacts the results. Furthermore, we investigate how the platform can efficiently learn the heterogeneity across users by interacting with a population and provide performance guarantees.", "text": "consider platform wants learn personalized policy user platform faces risk user abandoning platform dissatisﬁed actions platform. example platform interested personalizing number newsletters sends faces risk user unsubscribes forever. propose general thresholded learning model scenarios like this discuss structure optimal policies. describe salient features optimal personalization algorithms feedback platform receives impacts results. furthermore investigate platform eﬃciently learn heterogeneity across users interacting population provide performance guarantees. machine learning algorithms increasingly intermediating interactions between platforms users. result users’ interaction algorithms impact optimal learning strategies; investigate consequence work. setting consider platform wants personalize service user. distinctive feature work platform faces risk user abandoning platform dissatisﬁed actions platform. algorithms designed platform thus need careful avoid losing users. many examples settings. near future smart energy meters able throttle consumers’ energy consumption increase eﬃciency power grid peak demand e.g. raising lowering level conditioning. lead cost savings utility companies consumers. however utility company aggressive throttling energy user might abandon program. heterogeneity housing appliances preferences customers important utility companies learn personalized strategies consumer. content creators face similar problem email dissemination. value sending e-mails e-mail also risks recipient unsubscribing taking away opportunity creator interact user future. another example mobile notiﬁcations. used improve user engagement experience. however platform sends many notiﬁcations upset user might turn notiﬁcations application. scenarios face decision problem more better; however threshold beyond user abandons rewards gained. work focuses developing insight structure optimal learning strategies settings. particularly interested understanding strategies take simple structure elaborate below. section introduce benchmark model learning abandonment. initial model consider platform interacts single user time. user threshold drawn distribution time platform chooses action ever exceeds user abandons; otherwise user stays platform earns reward dependent ﬁrst consider case distribution reward function known known challenge ﬁnding optimal strategy given user. consider problem maximizing expected discounted reward. intuitively might expect optimal policy increasing depends discount factor particular might serve user increasing levels long abandon. surprisingly main result shows case fact static policy maximizing one-step reward optimal problem. essentially user abandons threshold ever crossed value trying actively learn threshold. section consider adapt results and/or reward function unknown. case platform learn multiple user arrivals. relate problem learning unknown demand curve suggest approach eﬃciently learning threshold distribution reward function. finally section consider general model soft abandonment negative experience users abandon entirely continue platform probability. characterize structure optimal policy maximize expected discounted reward per-user basis; particular policy adaptively experiments suﬃcient conﬁdence commits static action. empirically investigate structure optimal policy well. related work abandonment setting quite unique aware work addresses setting. independently work model abandonment problem using actions; safe action risky action. naturally leads rather diﬀerent results. similarities mechanism design literature though focus strategic behavior agents work revenue management literature considers agents heuristic behaviour main focus dealing ﬁnite inventory seem problem closely related many problems reinforcement learning dynamic structure problem. however important diﬀerences. focus personalization; viewed lens corresponds single episode learn independent episodes hand focus learning optimal policy using multiple episodes information carries episodes. diﬀerences present novel challenges abandonment setting necessitate structure present setting. also related work safe reinforcement learning catastrophic states need avoided setting learner usually access additional information example safe region given. finally note work unlike safe avoiding abandonment hard constraint. consider setting heterogeneous users interact platform discrete time steps indexed focus problem ﬁnding personalized policy single user. user characterized sequence hidden thresholds {θt}∞ jointly drawn known distribution models heterogeneity across users. every time platform selects action given closed based chosen action platform obtains random reward expected reward action given assume stationary known platform. required results expect increasing. action exceeds threshold time process stops. formally stopping time denotes ﬁrst time exceeds threshold expected reward criterion. alternative approach consider maximizing average reward ﬁnite horizon; considering problem remains interesting direction future work. without imposing restrictions structure stochastic threshold process solution intractable. thus ﬁrst consider extreme cases threshold sampled start remains ﬁxed across time; thresholds independent across time. thereafter look robustness results deviate extreme scenarios. fixed threshold ﬁrst consider case threshold sampled beginning horizon remains ﬁxed. words intuitively might expect platform might gradually learn threshold starting increasing long user abandon. fact something quite diﬀerent main result optimal policy constant policy. proofs found supplemental material. sketch argument exists constant policy optimal. consider policy increasing suppose optimal. exists time compare actions policy would action time periods. first suppose user abandons either alternative outcome identical. appendix provide another proof result using value iteration. proof also characterizes optimal policy optimal value exactly remarkably optimal policy independent discount factor independent thresholds completeness also note extreme case suppose thresholds drawn independently distribution since correlation time steps follows immediately optimal policy constant policy simple form. robustness considered extreme threshold models shown constant policies albeit diﬀerent ones optimal. section look robustness results understanding happens interpolate sides considering additive noise threshold model. here threshold time consists ﬁxed element noise terms drawn independently. general optimal policy model increasing intractable posterior depends previous actions. however exists constant policies close optimal case noise terms either small large reﬂecting preceding results extreme cases. optimal policies constant policies thresholds simply shifted. show optimal policy worst scenario achieves compared optimal policy best case. details found appendix. similarly noise level suﬃciently large respect threshold distribution also exists constant policy close optimal. intuition behind follows. first noise level large platform receives little information step thus cannot eﬃciently update posterior furthermore high variance thresholds also reduces expected lifetime policy. combined factors make learning ineﬀective. shape particular lipschitz constant depend threshold distribution reward function noise distribution widens decreases. result bound relevant variance substantial relative spread summarize results show extreme cases thresholds drawn independently drawn once exists constant policy optimal. further class constant policies robust joint distribution thresholds close either scenarios. thus assumed heterogeneity across population mean reward function known platform focused personalization single user. natural platform lacks knowledge section show platform learn optimal policy eﬃciently across population. study problem within context ﬁxed threshold model described above naturally lends development algorithms learn populationlevel heterogeneity. particular give theoretical performance guarantees type algorithm show variant based moss performs better practice. also empirically show explore-exploit strategy performs well. learning setting focus attention ﬁxed threshold model consider setting users arrive sequentially ﬁxed threshold drawn unknown distribution support emphasize role learning users time consider stylized setting platform interacts user time deciding actions observing outcomes user next user arrives. inspired preceding analysis consider proposed algorithm uses constant policy user. furthermore assume rewards bounded otherwise drawn arbitrary distribution depends regret respect oracle measure performance learning algorithms oracle full knowledge threshold distribution reward function access realizations random variables. discussed section optimal policy oracle thus propose algorithm suitably discretized space prove upper bound regret terms number users. approach based earlier work learning demand curves. presenting details introduce algorithm standard multi-armed bandit problem. kleinberg leighton adapt result problem demand curve learning. follow approach discretize action space standard approach approximately optimal action. user algorithm selects constant action either need impose following assumptions function strongly convex thus unique maximum proof consists parts ﬁrst lemma bound diﬀerence best action best discretized action space. theorem show learning strategy small regret compared best arm. combined prove result. important note algorithm requires prior knowledge number users practice reasonable assume platform able estimate accurately otherwise well-known doubling trick employed slight cost. brieﬂy discuss lower bounds learning algorithms. restrict algorithms play constant policy user lower bound kleinberg leighton applies immediately. however algorithms dynamic policies users obtain information user’s threshold therefore easily estimate empirical distribution function. whether lower bound carries dynamic policies open problem. moss audibert bubeck give upper conﬁdence bound algorithm tighter regret bound standard multi-armed bandit problem. moss algorithm index policy index given policy quite similar algorithm suﬀer extra term regret bound. however cannot adapt bound abandonment setting worse dependence number arms. practice expect algorithm perform better algorithm superior multi-armed bandit algorithm. explore-exploit strategy next consider explore-exploit strategy ﬁrst estimates empirical distribution function uses optimize constant policy. algorithm assume zero reward learner observe particular user mimics strategy learner increases action time period learn threshold particular user arbitrary precision. directly estimates empirical distribution function require discretization better able capture structure model. note compared previous algorithm assume learner access reward function threshold distribution unknown. signal-to-noise ratio stochastic rewards large unrealistic platform exploring able observe large number rewards therefore able estimate reward function reasonably well. setup simplicity simulations focus stylized setting; observed similar results diﬀerent scenarios. assume rewards deterministic follow identity function threshold distribution uniform algorithm repetitions time steps plot cumulative regret paths. results cumulative regret paths shown figure observe moss higher variance indeed performs better standard algorithm despite lack theoretical bound. however explore-exploit strategy obtains lowest regret. first since aware reward function less uncertainty. importantly algorithm leverages structure problem discretize action space treat actions independently. finally note rewards stochastic moss even worse compared explore-exploit estimate mean reward function explore-exploit strategy assumes given. section consider softer version abandonment platform receives feedback user abandons. example consider optimizing number push notiﬁcations. user receives notiﬁcation decide open decide turn notiﬁcations. however likely action ignore notiﬁcation. platform interpret signal dissatisfaction work improve policy. feedback model incorporate user feedback expand model follows. suppose whenever current action exceeds threshold probability receive reward user remains probability user abandons. further assume platform time observes reward rewarded indicator ixt>θt. equivalent assuming user geometrically goal maximize expected discounted reward. note platform receive reward threshold crossed problem nontrivial even restrict attention single threshold model drawn ﬁxed time periods. figure shows numerically computed optimal policy threshold distribution uniform reward function probability abandonment depending whether feedback signal received optimal policy follows green line step time left right. note think optimal policy form bisection though explore entire domain particular conservative regarding users large example consider user threshold policy initially increasing thus partially personalizes threshold converge fact never comes close. call partial learning; next section demonstrate feature optimal policy general. partial learning partial learning refers fact optimal policy fully reduce uncertainty initially policy learns threshold using bisection-type search. however point learning risky figure visualization optimal policy discount factor model. follow tree left right next action follows following green line optimal action given point following line user abandoned. optimal policy switches constant policy. note happens even risk abandonment point even risk losing reward oﬀset potential gains getting accurate posterior partial learning occurs regularity conditions threshold distribution ensures posterior collapse lipschitz deﬁned following paragraph. thus suﬃciently small intervals conditional probability decreases rapidly move away lower bound interval. suppose posterior non-degenerate lipschitz following sense. proposition suppose increasing lr-lipschitz non-zero interior bounded furthermore assume non-degenerate lipschitz deﬁned above. exists optimal action state prove result analyzing value function corresponding dynamic program. result shows point potential gains better posterior threshold worth risk abandonment. especially true quite likely posterior. contrary belief threshold small little lose experimentation. note however result also holds signals abandonment. case risk signal outweights possible future gains. naturally probability override small condition also weakens leading larger intervals constant policies. aggressive conservative policies another salient feature structure optimal policies feedback model aggressiveness policy. particular policy aggressive ﬁrst action larger optimal constant policy absence feedback conservative smaller. noted before feedback beneﬁt adapting user thresholds. however value personalization users give feedback. optimal policy aggressive. case optimal policy aggressively target high-value users users unlikely abandon immediately. thus policy personalize high-value users later periods. forces policy careful longer horizon algorithm extract value even value user careful lose ﬁrst periods. long term value user threshold makes loss immediate reward gained aggressively targeting users high threshold. figure illustrates eﬀect. here deterministic rewards threshold distribution uniform similar eﬀect observed distributions reward functions well. machine learning algorithms deployed settings interact people important understand user behavior aﬀects algorithm. work propose novel model personalization takes account risk dissatisﬁed user abandons platform. leads unexpected results. show constant policies optimal ﬁxed threshold independent threshold models. shown small perturbations models constant policies robust though general ﬁnding optimal policy becomes intractable. setting platform faces many users know reward function population distribution threshold suitable assumptions shown ucb-type algorithms perform well theoretically providing regret bounds running simulations. also consider exploreexploit strategy eﬃcient practice requires knowledge feedback users leads sophisticated optimal learning strategies exhibit partial learning; optimal learning algorithm personalizes certain degree user. also found optimal policy conservative probability abandonment high aggressive probability low. abandonment models first sophisticated behaviour user abandonment considered. could take many forms total patience budget gets depleted threshold crossed. another model user playing learning strategy herself comparing platform multiple outside options. scenario user platform simultaneously learning other. user information second considered additional user information terms covariates. notiﬁcation example user activity seems like important signal preferences. models able incorporate information able infer parameters data beyond scope work important direction research. empirical analysis work focuses theoretical understanding abandonment model thus ignores important aspects real world system. believe potential gain additional insight empirical perspective using real-world systems abandonment risk. authors would like thank andrzej skrzypacz emma brunskill andreas krause carlos riquelme suggestions feedback. work supported stanford tomkat center national science foundation grant cns-. opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect views national science foundation.", "year": 2018}