{"title": "An Introduction to Deep Visual Explanation", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call \"deep visual explanation\" (DVE). \"Deep,\" because it is the development and performance of deep neural network models that we want to understand. \"Visual,\" because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and \"Explanation,\" because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.", "text": "practical impact deep learning complex supervised learning problems signiﬁcant much almost every artiﬁcial intelligence problem least portion thereof somehow recast deep learning problem. applications appeal signiﬁcant appeal increasingly challenged call challenge explainability generally traditional challenge debuggability outcomes deep learning process produce unexpected results little available theories tools help investigate potential causes unexpected behavior especially behavior could impact people’s lives. describe preliminary framework help address issue call \"deep visual explanation\" \"deep\" development performance deep neural network models want understand. \"visual\" believe rapid insight complex multi-dimensional model provided appropriate visualization techniques \"explanation\" spectrum instrumentation inserting print statements abductive inference explanatory hypotheses believe understanding deep learning relies identiﬁcation exposure hypotheses performance behavior learned deep model. exposition preliminary framework relatively straightforward image classiﬁcation examples variety choices initial conﬁguration deep model building scenario. careful complicated instrumentation expose classiﬁcation outcomes deep models using visualization also show initial results potential application interpretability. primary appeal deep learning predictive model automatically constructed suitable volume labeled inputs. increasing number demonstration applications staging deep learning exercise need outline details supervised learning problem terms input data leave creation predictive classiﬁer deep learning framework fundamental improvement current deep learning methods that unlike earlier shallow network layers deep learning automatically identiﬁes appropriate stratiﬁcation predictive model property ﬁnding appropriate multi-layer structures supervised classiﬁcation problem produced signiﬁcant advances systems especially rely accurate classiﬁcation including automated driving voice recognition natural language processing tasks image classiﬁcation. many components artiﬁcial intelligence systems include classiﬁcation components easy imagine construction accurate classiﬁcation components provide essential contribution overall intelligent systems. classiﬁers simple categories welldeﬁned relatively easy conﬁrm whether classiﬁer performing well. classiﬁcation complex e.g. classifying complex proteins potential docking targets potentially active pairings easy determine deep learned classiﬁer doing especially unexpected pairs predicted. surprise that long programming done always need supporting systems help programmers understand unexpected behaviour programs. primitive effective ideas like insertion print statements sophistication non-monotonic abductive hypothesis management systems motivation always instrument computational object interest reveal local behaviour provide insight whether \"unexpected\" outputs unanticipated insights bugs unintended modelling bias. make obvious heuristic choices instrument deep learned models assemble collection components provide suggestion approach idea deep visual explanation name arises focus understanding scope methods would potentially provide insight \"black box\" deep-learned models \"visual\" already believe trajectory successful applications deep learning sufﬁciently complex simply identifying human-manageable small parameters provide sufﬁcient insight \"explanation\" expect deep-learned models necessarily always partial always competing alternative explanatory hypotheses unexpected behaviour. following explain idea describing general method instrument deep learning architecture case example deep learned model representation called vgg- networks note that demonstrate value deep representation would expect generalize framework variety deep neural network learning representations. method proposes creation series multi-dimensional planes \"slice\" multi-layered deep-learned model examples methods learned-model attributes could selected displayed visualization plane provide insight overall classiﬁcation performance deep-learned model. description exhaust alternatives select visualization techniques identifying multi-level attributes provide \"best\" insight. rather like principled visualization methods don’t think single best methods that. instead want describe general idea reﬁned variety ways variety existing literature order create framework support understanding deep-learned models alternatives. reminder paper organized follows. section reviews related work. section presents proposed approach. section describe experiments initial framework ﬁnally section concludes preliminary results followed discussion extensive future work. last years deep learning algorithms shown impressive results variety classiﬁcation problems image classiﬁcation video action recognition natural language processing object tracking image segmentations many others. designing network architecture challenging task especially case trying understand performance. many questions encountered e.g. things don’t work performance bad? decision classiﬁcation outlier? class predicted another? debug observed error? output trusted? methods recently proposed address kinds questions. approach analyze deep network responds speciﬁc input image given prediction recent probabilistic method proposed authors assign weight feature respect class drawback approach that computationally expensive. algorithms proposed another interesting type explanation based network activation; popular methods proposed here. ﬁrst method called \"class activation mapping\" main idea estimate discriminative localization employed conﬁguration. computes linear relation feature maps weights ﬁnal layer. however drawback approach employed networks multiple fully connected layers. recently authors proposed relaxation generalization known computes gradients predicted class respect feature maps obtain weights. weights multiplied last pooling layer identify discriminative region. deep convolutional neural networks produce spatial information convolution layers. however information lost propagating fully connected layers. loss information makes explanation process challenging especially comes interpreting output sensitive data medical images. recognize explanations many different representations demonstration intended simple preliminary illustrate idea. immediate goal create explanation outcome dcnn i.e. identify discriminative pixels image inﬂuence ﬁnal prediction approach task restricted context assume convolution feature maps pooling layer contain relevant information class write solution i.e. input class using network compute evidence/explanation generally explanation composed fragments features crucial producing classiﬁcation output. explain compute low-spatial scale high-spatial scale activations every feature shown figure term \"activation\" here looking pixels activate create either high spatial scale computations. explanation fourier domain consider function representing transformation feature particular convolution layer. therefore transformation every size fourier domain written follows gaussians computed different represents transformation fourier space denotes inverse.equation computes types activations i.e. low-spatial scale activation high-spatial scale activation fourier space. advantage approach that spatial frequency coefﬁcients abruptly exhibit gradual cut; essential order preserve discriminative pixels. computing visual explanation observed activations contribute explaining decision refer problem noisy activations. address ﬁlter noise deﬁned using highlight features contributed substantially classiﬁcation. overall methodology depicted figure algorithm summarizes overall process. figure proposed framework input image passed network class prediction. classiﬁcation decision obtained explanation computed last pooling layer. simple case image classiﬁcation ultimate goals visual explanation context debugging precise determining component salient patch. therefore penalize activations contribute much algorithm handle this propose method called targeted-dve provide targeted explanation. algorithm removes pixel less inﬂuence best explanation. process identical previous approach except that slightly modify ﬁnal output obtained algorithm done computing evaluate visualization context dcnn classiﬁcations. used images common objects context challenge consists objects types. example network model adopted publicly available pre-trained vgg- network explaining dccn predictions results randomly selected images coco using vgg- classiﬁcations depicted figure comparison methods illustrated figure figure explaining decisions made vgg-. network makes correct predictions algorithm provides improved targeted explanation i.e. highlights discriminative pixels employed network. also show explanations incorrect predictions approach require training changing network architecture. model also require solving optimization problem moreover approach computationally efﬁcient computation time intel core seconds. finally algorithm identiﬁes relatively minimal discriminative/salient patch impacts output network. network images? also evaluated robustness algorithm blurring affect. blurred image using gaussian blur different figure result suggests network able predict blurred images correctly even though network looking right region. means network looking speciﬁc features image therefore resistant blurring effects. figure figure shows responds network gaussian blur. image less blurring affect bottom. network still able predict right class however network failed correctly predict class image bottom. understanding dcnn compresses information motivation explanatory function arises sensitive domains like medical diagnosis. case dcnn example need understand process propagating information output layer. understand attention changes propagate forward. understanding dcnn decisions medical domain explaining dcnn prediction medical domain also important decisions could impact people’s lives. show effectiveness method used pre-trained model skin lesion cancer classiﬁcation i.e. benign malignant. initial results visual explanation depicted figure network focusing sensitive region image make decision. introduce framework identifying explanations dcnn decisions. approach captures discriminative pixels considering activation high spatial scales fourier space. experimented simple version approach image classiﬁcation. also experimented potential applications interoperability explaining predictions made medical data. extension simple framework domains help determine framework extended sophisticated domains complex interpretable explanations. example within general framework abductive explanation explanations classiﬁcation language segments must include linguistic context speech require audio context. overall goal providing context trained partial model identiﬁcation plausible components give rise speciﬁc classiﬁcation output same. believe debugging complex multi-dimensional learned neural network models exhibit good performance debugged interpreted rationally improve performance.", "year": 2017}