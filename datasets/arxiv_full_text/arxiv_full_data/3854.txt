{"title": "Dynamic Evaluation of Neural Sequence Models", "tag": ["cs.NE", "cs.CL"], "abstract": "We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.", "text": "krause emmanuel kahembwe iain murray steve renals school informatics university edinburgh edinburgh scotland ben.krausee.kahembwei.murrays.renalsed.ac.uk present methodology using dynamic evaluation improve neural sequence models. models adapted recent history gradient descent based mechanism causing assign higher probabilities re-occurring sequential patterns. dynamic evaluation outperforms existing adaptation approaches comparisons. dynamic evaluation improves state-of-the-art word-level perplexities penn treebank wikitext- datasets respectively state-of-the-art character-level cross-entropies text hutter prize datasets bits/char bits/char respectively. sequence generation prediction tasks span many modes data ranging audio language modelling general timeseries prediction tasks. applications models include speech recognition machine translation dialogue generation speech synthesis forecasting music generation among others. neural networks applied tasks predicting sequence elements one-by-one conditioning history sequence elements forming autoregressive model. convolutional neural networks recurrent neural networks including long-short term memory networks particular achieved many successes tasks. however basic form models limited ability adapt recently observed parts sequence. many sequences contain repetition; pattern occurs likely occur again. instance word occurs document much likely occur again. sequence handwriting generally stay handwriting style. sequence speech generally stay voice. although rnns hidden state summarize recent past often unable exploit patterns occur repeatedly test sequence. paper concerns dynamic evaluation investigate candidate solution problem. approach adapts models recent sequences using gradient descent based mechanisms. show several ways improve past dynamic evaluation approaches section improved methodology achieve state-of-the-art results section section section design method dramatically reduce number adaptation parameters dynamic evaluation making practical wider range situations. section analyse dynamic evaluation’s performance varying time-scales distribution shifts demonstrate dynamically evaluated models generate conditional samples repeat many patterns conditioning data. rnn. however longer sequences history often contains re-occurring patterns difﬁcult capture using models ﬁxed parameters many domains dataset sequences generated point time history sequence slightly different distribution therefore adapting model parameters learned training justiﬁed. infer model parameters many sequence modelling tasks characterised sequences generated slightly different distributions scenario described above. generating distribution also change continuously across single sequence; instance text excerpt change topic. furthermore many machine learning benchmarks distinguish sequence boundaries concatenate sequences continuous sequence. thus many sequence modelling tasks could seen dynamic evaluation methods continuously adapt model parameters learned training time parts sequence evaluation. goal learn adapted parameters provide better model local sequence distribution dynamic evaluation applied present work long test sequence divided shorter sequences length deﬁne sequence shorter sequence segments initial adapted parameters used compute probability ﬁrst probability gives cross entropy loss gradient segment gradient used update model resulting adapted parameters procedure repeated sequence shown figure gradients loss backpropagated beginning computation linear sequence length. update applies maximum likelihood training step approximate current local distribution computational cost dynamic evaluation forward pass gradient computation data slight overhead apply update rule every sequence segment. autoregressive models dynamic evaluation conditions sequence elements already predicted evaluates valid log-probability sequence. dynamic evaluation also used generating sequences. case model generates sequence segment using ﬁxed weights performs gradient descent based update step applying dynamic evaluation sequence generation could result generated sequences consistent regularities meaning patterns occur generated sequence likely occur again. adaptive language modelling ﬁrst considered n-grams adapting recent history caching methods bellegarda recently neural cache approach closely related pointer sentinel-lstm used adaptive neural language modelling. neural caching recently used improve state-of-the-art word-level language modelling neural cache model learns type non-parametric output layer test time allows network adapt recent observations. past hidden state paired next input stored tuple hidden state observed output probabilities adjusted give higher weight output words coincided past hidden figure illustration dynamic evaluation. model evaluates probability sequence segments gradient respect probability used update model parameters model progresses next sequence segment. dashed edges distinguish dynamic evaluation static evaluation. neural cache closely relates dynamic evaluation methods added base model adaptation test time. main difference mechanism used recent history neural cache approach uses non-parametric nearest neighbours-like method whereas dynamic evaluation uses gradient descent based method change model parameters dynamically. methods rely autoregressive factorisation depend observing sequence elements predicted order perform adaptation. dynamic evaluation neural caching methods therefore applicable sequence prediction generation tasks directly general supervised learning tasks. drawback neural cache method cannot adjust recurrent hidden state dynamics. result neural cache’s ability capture information occurs jointly successive sequence elements limited. capability critical adapting sequences element little independent meaning e.g. character level language modelling. dynamic evaluation neural language models proposed mikolov approach simply used stochastic gradient descent updates every time step computing gradient fully truncated backpropagation time equivalent setting equation dynamic evaluation since applied character word-level language models previous work using dynamic evaluation considered aside explore depth. propose several changes mikolov dynamic evaluation method fully truncated backpropagation refer traditional dynamic evaluation. ﬁrst modiﬁcation reduces update frequency gradients backpropagated timesteps. change provides accurate gradient information also improves computational efﬁciency dynamic evaluation since update rule applied much less often. sequence segments length word-level tasks character-level tasks. next global decay prior bias model towards parameters learned training. motivation dynamic evaluation assumes local generating distribution constantly changing potentially desirable weight recent sequence history higher adaptation. adding global decay prior accomplishes causing previous adaptation updates decay exponentially time. global prior learning rate decay rate form update rule consider using rmsprop derived update rule learning rule place sgd. rmsprop uses moving average recent squared gradients scale learning rates weight. dynamic evaluation near start test sequence rmsprop gradients average therefore able leverage updates effectively. reason collect mean squared gradients training data rather recent test data given number training batches gradient training batch. mini-batch size computation becomes hyper-parameter larger mini-batches result smaller mean squared gradients. update rule call global prior experiments decay rate parameter proportionally tomsg. parameters high gradient affect dynamics network more makes sense decay faster. rmsnorm ismsg divided mean resulting normalized version ofmsg mean clip values rmsnorm greater sure decay rate exceed parameter. combining learning component regularization component results ﬁnal update equation refer global prior mini-batching sequences desirable test-time sequence modelling applications allows faster processing multiple sequences parallel. dynamic evaluation high memory cost mini-batching necessary store different parameters sequence mini-batch. therefore consider sparse dynamic evaluation variant updates smaller number parameters. introduce adaptation matrix initialized zeros. multiplies hidden state vector every time-step hidden state replaces propagated throughout network recurrent feed-forward connections. applying dynamic evaluation avoids need apply dynamic evaluation original parameters network reduces number adaptation parameters makes mini-batching less memory intensive. reduce number adaptation parameters using transform arbitrary subset hidden units. results matrix adaptation parameters. chosen much less number hidden units reduces number adaptation parameters dramatically. section experiment sparse dynamic evaluation character-level language models. applied dynamic evaluation word-level character-level language modelling. tasks evaluate dynamic evaluation base model. training base model tune hyper-parameters dynamic evaluation validation evaluate static dynamic versions model test set. also consider follow experiments analyse sequence lengths dynamic evaluation useful. code dynamic evaluation methodology available. train base models penn treebank wikitext- datasets compare performance static dynamic evaluation. experiments compare dynamic evaluation past approaches neural cache measure dynamic evaluation’s general performance across different models datasets. derived articles wall street journal. contains training tokens vocab size limited words. commonly used benchmarks language modelling. consider baseline models standard lstm implementation recurrent dropout recent state-of-the-art awd-lstm standard lstm taken chainer tutorial language modelling used lstm layers units each trained regularized recurrent dropout. standard lstm experiment traditional dynamic evaluation applied mikolov well modiﬁcation make building ﬁnal update rule described section ﬁnal update rule worked best experiments dynamic eval default refer update rule tables. applied dynamic evaluation powerful model asgd weight-dropped lstm awd-lstm vanilla lstm combines drop-connect recurrent weights regularization variant averaged stochastic gradient descent optimisation. model used layers tied input output embeddings intended direct replication awd-lstm using code implementation. results given table dynamic evaluation gives signiﬁcant overall improvements models dataset. dynamic evaluation also achieves better ﬁnal results neural cache standard lstm awd-lstm reimplementation improves state-of-the-art ptb. wikitext- roughly twice size million training tokens vocab size features articles non-shufﬂed order dependencies across articles adaptive methods https//github.com/benkrause/dynamic-evaluation https//github.com/chainer/chainer/tree/master/examples/ptb https//github.com/salesforce/awd-lstm-lm model rnn+lda+kn-+cache charcnn lstm variational lstm pointer sentinel-lstm variational lstm augmented loss variational cell variational lstm gradual learning lstm tuning lstm lstm neural cache lstm lstm traditional dynamic eval lstm dynamic eval lstm dynamic eval lstm dynamic eval lstm dynamic eval awd-lstm awd-lstm +neural cache awd-lstm awd-lstm dynamic eval model byte mlstm variational lstm pointer sentinel-lstm lstm tuning lstm lstm neural cache lstm lstm dynamic eval awd-lstm awd-lstm neural cache awd-lstm awd-lstm dynamic eval dynamic evaluation improves state-of-the-art perplexity wikitext- provides signiﬁcantly greater improvement neural caching base models. suggests dynamic evaluation effective exploiting regularities co-occur across non-shufﬂed documents. model stacked lstm stacked lstm traditional dynamic eval multiplicative integration lstm hyperlstm hierarchical multiscale lstm bytenet decoder lstm tuning recurrent highway networks fast-slow lstm mlstm mlstm sparse dynamic eval mlstm dynamic eval model multiplicative multiplicative integration lstm lstm batch normalised lstm hierarchical multiscale lstm recurrent highway networks mlstm mlstm dynamic eval consider dynamic evaluation text hutter prize datasets. hutter prize dataset comprised wikipedia text includes characters non-latin languages. million utf- bytes long contains unique bytes. similarly reported results split training validation testing. text dataset also derived wikipedia text removed lower cased characters english text plus spaces. hutter prize standard split training validation testing text. used multiplicative lstm base model datasets. mlstms tasks used hidden units embedding layer units weight normalization variational dropout adam training. also consider sparse dynamic evaluation described section hutter prize dataset. sparse dynamic evaluation adapted subset hidden units resulting adaptation matrix adaptation parameters. dynamic evaluation results section ﬁnal update rule given section results hutter prize given table results text given table dynamic evaluation achieves large improvements base models state-of-the-art results datasets. sparse dynamic evaluation also achieves signiﬁcant improvements hutter prize using adaptation parameters regular dynamic evaluation. figure average losses bits/char dynamic evaluation static evaluation plotted number characters processed; sequences hutter prize test european parliament dataset spanish averaged trials each. losses data point averaged sequence segments length cumulative. note different y-axis scales plots. measure time-scales dynamic evaluation gains advantage static evaluation. starting model trained hutter prize plot performance static dynamic evaluation number characters processed sequences hutter prize test sequences spanish european parliament dataset hutter prize data experiments show timescales dynamic evaluation gained advantage observed table divided hutter prize test sequences length applied static dynamic evaluation sequences using model methodology used obtain results table losses averaged across sequences obtain average losses time step. plots average cross-entropy errors number hutter characters sequenced given figure spanish experiments measure dynamic evaluation handles large distribution shifts training test time hutter prize contains little spanish. used ﬁrst million characters spanish european parliament data place hutter prize test set. spanish experiments used base model dynamic evaluation settings hutter prize. plots average cross-entropy errors number spanish characters sequenced given figure datasets dynamic evaluation gave noticeable advantage hundred characters. spanish advantage continued grow sequence processed whereas hutter advantage maximized viewing around characters. advantage dynamic evaluation also much greater spanish sequences hutter sequences. also drew character conditional samples static dynamic versions model viewing characters spanish. dynamic model continued apply dynamic evaluation sampling well process described section conditional samples given appendix. static samples quickly switched english resembled hutter prize data. dynamic model generated data spanish words number made words characteristics spanish words entirety sample. example kinds features dynamic evaluation able learn model work explores develops methodology applying dynamic evaluation sequence modelling tasks. experiments show proposed dynamic evaluation methodology gives large test time improvements across character word level language modelling. improvements language modelling applications speech recognition machine translation longer contexts including broadcast speech recognition paragraph level machine translation. overall dynamic evaluation shown effective method exploiting pattern re-occurrence sequences. chung bengio. hierarchical multiscale recurrent neural networks. iclr cooijmans ballas laurent courville. recurrent batch normalization. iclr fortunato blundell vinyals. bayesian recurrent neural networks. arxiv preprint arxiv. grave joulin usunier. improving neural language models continuous cache. iclr graves. generating sequences recurrent neural networks. arxiv preprint arxiv. lee. hypernetworks. iclr hochreiter schmidhuber. long short-term memory. neural computation hutter. human knowledge compression prize. http//prize.hutter.net inan khosravi socher. tying word vectors word classiﬁers loss framework language kingma adam method stochastic optimization. arxiv preprint arxiv. koehn. europarl parallel corpus statistical machine translation. summit volume merity xiong bradbury socher. pointer sentinel mixture models. iclr mikolov zweig. context dependent recurrent neural network language model. mikolov karaﬁát burget cernock`y khudanpur. recurrent neural network based language press wolf. using output embedding improve language models. eacl salimans kingma. weight normalization simple reparameterization accelerate training deep character samples generated dynamic version model trained hutter prize conditioned spanish characters. ﬁnal sentence fragment conditioning characters given reader generated text given bold tiene importancia este compromiso medida comisión organismo tiene montembre procedíns conscriptione tesalo pómienda hanemos siemina. pedrera orden señora presidente civil orden siemin presente relevante frónmida esculdad pludiore formidad president presidenta antidorne adamirmidad ciemano character samples generated static version model trained hutter prize conditioned spanish characters. ﬁnal sentence fragment conditioning characters given reader generated text given bold tiene importancia este compromiso medida comisión organismo tiene monde &lt;br&gt;there secret world except cape town seen comalo ball market seen closure eagle imprints dallas within country.&quot; topic increasingly small contract saying allan roth acquired government", "year": 2017}