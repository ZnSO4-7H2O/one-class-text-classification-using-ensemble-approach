{"title": "Character-based Neural Machine Translation", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.", "text": "neural machine translation reached state-of-the-art results. however main challenges neural still faces dealing large vocabularies morphologically rich languages. paper propose neural system using character-based embeddings combination convolutional highway layers replace standard lookup-based word representations. resulting unlimited-vocabulary afﬁxaware source word embeddings tested state-of-the-art neural based attention-based bidirectional recurrent neural network. proposed scheme provides improved results even source language morphologically rich. improvements bleu points obtained german-english task. machine translation algorithms transforming source language target language. last years popular approaches statistical phrase-based uses combination features maximise probability target sentence given source sentence recently neural approach appeared obtained state-of-the-art results. provides fully trainable model; uses word embeddings words independent anymore; easily extendable multimodal sources information weaknesses neural strong limitation vocabulary architecture difﬁcult computationally expensive tune parameters deep learning structure. paper neural baseline system follows encoder-decoder architecture attention introduce elements characterbased neural language model translation unit continues word continue using word embeddings related word input vector bidirectional recurrent neural network difference embeddings word longer independent vector computed characters corresponding word. system architecture changed using convolutional neural network highway network characters attention-based mechanism encoder. signiﬁcant difference previous work uses neural architecture without modiﬁcation deal subword units subword-based representations already explored natural language processing e.g. tagging name entity recognition parsing normalization learning word representations previous works show different advantages using character-level information. case characterbased neural architecture take advantage intra-word information proven extremely useful applications especially dealing morphologically rich languages. using character-based source word embeddings ceases unknown words source input size target vocabulary remains unchanged. although target vocabulary continues limitation standard neural system fact unknown words source helps reduce number unknowns target. moreover remaining unknown target words successfully replaced corresponding source-aligned words. consequence obtain signiﬁcant improvement terms translation quality rest paper organized follows. section brieﬂy explains architecture neural using baseline system. section describes changes introduced baseline architecture order characterbased embeddings instead standard lookupbased word representations. section reports experimental framework results obtained german-english task. finally section concludes contributions paper work. neural uses neural network approach compute conditional probability target sentence given source sentence approach used work follows encoder-decoder architecture.first encoder reads source sentence encodes sequence hidden states then decoder generates corresponding translation based encoded sequence hidden states encoder decoder jointly trained maximize conditional log-probability correct translation. baseline autoencoder architecture improved attention-based mechanism encoder uses bi-directional gated recurrent unit allows better performance long sentences. decoder also becomes word predicted based recurrent hidden state previously predicted word context vector. context vector obtained weighted annotations turn computed alignment model neural approach achieved competitive results standard phrase-based system evaluation word embeddings shown boost performance many tasks including machine translation. however standard lookupbased embeddings limited ﬁnite-size vocabulary computational sparsity reasons. moreover orthographic representation words completely ignored. standard learning process blind presence stems preﬁxes sufﬁxes kind afﬁxes words. solution drawbacks alternative character-based word embeddings recently proposed tasks language modeling parsing tagging even authors character transformation presented source target. however seem clear improvements. recently propose combination word characters neural experiments neural selected best character-based embedding architecture proposed language modeling. figure shows computation representation word starts character-based embedding layer associates word sequence vectors. sequence vectors processed convolution ﬁlters different lengths followed pooling layer. convolutional ﬁlter keep output maximum value. concatenation values already provides representation word vector ﬁxed length equal total number convolutional kernels. however addition highway layers shown improve quality language model also kept additional layers case. output second highway layer give ﬁnal vector representation source word replacing standard source word embedding neural machine translation system. target size still limited vocabulary softmax layer output network kept standard target word embeddings experiments. however results seem show afﬁx-aware representation source words positive inﬂuence components network. global optimization integrated model forces translation model internal vector representation target words follow afﬁx-aware codiﬁcation source words. data used german-english data including epps news commoncrawl. preprocessing consisted tokenizing truecasing normalizing punctuation ﬁltering sentences words language baseline systems phrase-based system built using moses standard parameters grow-ﬁnal-diag alignment goodturing smoothing relative frequencies gram language modeling using kneser-ney discounting lexicalized reordering among others. neural-based system built using software dlmt available github. generally used settings previous work networks embedding dimension batch size dropout. used vocabulary size thousand words german-english. also proposed replaced unknown words corresponding source word using alignment information. results table shows bleu results baseline systems character-based neural also include results char systems post-processing unknown words consists replacing unks corresponding source word suggested bleu results improve points german-to-english points english-to-german. reduction number unknown words goes direction german-to-english opposite direction. note berichten zufolge hofft indien darber hinaus einen vertrag verteidigungszusammenarbeit zwischen beiden nationen reportedly hopes india addition contract defence cooperation nations according reports india also hopes establish contract nations according reports india hopes treaty defence cooperation nations india also reportedly hoping deal defence collaboration nations durchtrainierte mainzer sagt sich dass ambitionierter rennradler mainz says ambitious mainz says ambitious mainz says ambitious racer well-conditioned mainz said ambitious racing cyclist habe jedoch nicht gesagt streiken wolle dass schwer folgen konkret vorherzusehen however strike difﬁcult predict consequences concrete however tell wanted difﬁcult predict consequences however wanted strike difﬁcult predict consequences said however strike making difﬁcult predict exactly consequences premierminister indiens japans trafen sich tokio prime minister india japan tokyo prime minister india japan tokyo prime ministers india japan tokyo india japan prime ministers meet tokyo beamten augen verloren ofﬁcials lost sight ofﬁcials lost eyes ofﬁcials lose sight causing ofﬁcers lose sight character-based embedding impact learning better translation model various levels seems include better alignment reordering morphological generation disambiguation. table shows examples kind improvements character-based neural system capable achieving compared baseline systems. examples show reduction source unknowns improves adequacy translation. examples show character-based approach able handle morphological variations. finally example shows appropriate semantic disambiguation. neural offers perspective managed. main advantages compared previous approaches e.g. statistical phrase-based translation faced trainable features optimized end-to-end scheme. however still remain many challenges left solve dealing limipaper proposed modiﬁcation standard encoder/decoder neural architecture unlimited-vocabulary character-based source word embeddings. improvement bleu points german-to-english points english-to-german. work supported framework program european commission international outgoing fellowship marie curie action also spanish ministerio econom´ıa competitividad european regional developmend fund contract tec--p miguel ballesteros chris dyer noah smith. improved transitionbased parsing modeling characters instead proceedings words lstms. conference empirical methods natural language processing pages lisbon portugal september. association computational linguistics. wang ling chris dyer alan black isabel trancoso ramon fermandez silvio amir luis marujo tiago luis. finding function form compositional character models proopen vocabulary word representation. ceedings conference empirical methods natural language processing pages lisbon portugal september. association computational linguistics. minh-thang luong christopher manning. achieving open vocabulary neural machine translation hybrid association comword-character models. putational linguistics berlin germany august. vic. boosting named entity guimar aes. recognition neural character embeddings. proceedings fifth named entity workshop pages beijing china july. association computational linguistics. bianca zadrozny. learning character-level representations part-of-speech tagging. tony jebara eric xing editors proceedings international conference machine learning pages ilya sutskever oriol vinyals sequence sequence quoc learning neural networks. ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages curran associates inc. botha phil blunsom. compositional morphology word representations language modelling. proceedings international conference machine learning beijing china jun. *award best application paper*. xinxiong chen zhiyuan joint maosong huan-bo luan. learning character word embeddings. qiang yang michael wooldridge editors ijcai pages aaai press. kyunghyun bart merrienboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder–decoder approaches. proc. eighth workshop syntax semantics structure statistical translation doha. grzegorz chrupala. normalizing tweets edit scripts recurrent neural embeddings. proceedings annual meeting association computational linguistics june baltimore volume short papers pages firat kyunghun roland memisevic yoshua bengio. montreal neural machine translation systems wmt. proc. workshop statistical machine translation lisbon. philipp koehn hieu hoang alexandra birch chris callison-burch marcello federico nicolas bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra constantin evan herbst. moses open source toolkit", "year": 2016}