{"title": "Graph Autoencoder-Based Unsupervised Feature Selection with Broad and  Local Data Structure Preservation", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Feature selection is a dimensionality reduction technique that selects a subset of representative features from high-dimensional data by eliminating irrelevant and redundant features. Recently, feature selection combined with sparse learning has attracted significant attention due to its outstanding performance compared with traditional feature selection methods that ignores correlation between features. These works first map data onto a low-dimensional subspace and then select features by posing a sparsity constraint on the transformation matrix. However, they are restricted by design to linear data transformation, a potential drawback given that the underlying correlation structures of data are often non-linear. To leverage a more sophisticated embedding, we propose an autoencoder-based unsupervised feature selection approach that leverages a single-layer autoencoder for a joint framework of feature selection and manifold learning. More specifically, we enforce column sparsity on the weight matrix connecting the input layer and the hidden layer, as in previous work. Additionally, we include spectral graph analysis on the projected data into the learning process to achieve local data geometry preservation from the original data space to the low-dimensional feature space. Extensive experiments are conducted on image, audio, text, and biological data. The promising experimental results validate the superiority of the proposed method.", "text": "section provide review literature related proposed method introduce paper’s notation standard. datasets denoted x··· rd×n sample denote data dimensionality number data points respectively. matrix denotes column matrix denotes entry matrix y··· rm×n embedding matrix denotes representation data point obtained low-dimensional subspace. denotes loss function denotes regularization function transformation matrix rm×d. methods diﬀer choice embedding loss regularizoation functions; examples presented below. long-standing well-known subspace learning-based unsupervised feature selection methods. mcfs embedding rm×n data ﬁrst learned based spectral clustering. that data points regressed learned embedding transformation matrix rm×d w;··· loss structure captured nearest neighbor graph preserved data embeddings low-dimensional subspace. meanwhile −norm performed transformation matrix guide feature selection simultaneously. subspace learning feature selection separate steps combined single framework. studies data point incorporate local data discriminative information feature selection framework. like mcfs mrfs udfs also assumes existence transformation matrix rm×c maps data low-dimensional feature selection methods self-representation. methods feature assumed representable linear combination features i.e. rd×d representation matrix rd×n rm×d weight matrix bias vector elementary nonlinear activation function. commonly used activation functions include sigmoid function hyperbolic tangent function optimization problem brought autoencoder minimize diﬀerence input data reconstructed/output data. speciﬁc given data x··· parameters output autoencoder input general approach minimize reconstruction error selecting parameter values since weight matrix applied directly input data column used measure importance corresponding data feature. therefore used regularization function promote feature selection detailed section objective function single-layer autoencoder based broad local data information low-dimensional subspace learning order characterize local data geometric structure construct k-nearest neighbor graph data space. edge weight between connected data points determined similarity points. paper choose cosine distance similarity measurement simplicity. therefore adjacency matrix graph small constant added avoid overﬂow since diﬀerentiable calculate subgradient element case. element subgradient arbitrary value interval gradient computational convenience. summary gafs compared algorithms include parameters adjust. experiment parameters tune others according grid-search strategy. algorithms select features dataset. graph-based algorithms number nearest neighbor graph algorithms projecting data onto low-dimensional space space dimensionality range gafs range hidden layer size match subspace dimensionality balance parameters given ranges available http//www.cad.zju.edu.cn/home/dengcai/data/code/laplacianscore.m available https//github.com/matrixlover/lsls/blob/master/fsspectrum.m available https//sites.google.com/site/alanzhao/home available http//www.cs.cmu.edu/ yiyang/udfs.rar available https//github.com/guangmingboy/githubs alternatively terminologies subspace dimensionality hidden layer size descriptions gafs. ﬁrst study parameter sensitivity gafs respect subspace dimensionality besides aforementioned manifold dimensionality range common proposed comparing algorithms also conducted experiments hidden layer size values investigate mance gafs sensitive hidden layer size given datasets exception yale performance hidden layer size apparently better reduced dimensionality performance variations small latter set. possible reason behind corresponding diﬀerent digits mnist diﬀerent classes coil; focus datasets space constraints. experiment empirically parameters mnist coil autoencoder training. order present clear illustration show", "year": 2018}