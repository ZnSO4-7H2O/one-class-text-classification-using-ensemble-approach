{"title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,  Gated, and Tree", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.", "text": "seek improve deep neural networks generalizing pooling operations play central role current architectures. pursue careful exploration approaches allow pooling learn adapt complex variable patterns. primary directions learning pooling function combining average pooling learning pooling function form tree-structured fusion pooling ﬁlters learned. experiments every generalized pooling operation explore improves performance used place average pooling. experimentally demonstrate proposed pooling operations provide boost invariance properties relative conventional pooling state several widely adopted benchmark datasets; also easy implement applied within various deep neural network architectures. beneﬁts come light increase computational overhead training modest increase number model parameters. recent resurgence neurally-inspired systems deep belief nets convolutional neural networks sum-and-max infrastructure derived signiﬁcant beneﬁt building sophisticated network structures bringing learning non-linear activations pooling operation also played central role contributing invariance data variation perturbation. however pooling operations little revised beyond current primary options average stochastic pooling patent disclosure ucsd docket forest convolutional neural network ﬁled march ucsd docket generalizing pooling functions convolutional neural network ﬁled sept paper desire bring learning responsiveness pooling operation. various approaches possible pursue particular. ﬁrst approach consider combining typical pooling operations within approach investigate strategies combine operations. strategies unresponsive; reasons discussed later call strategy mixed max-average pooling. strategy responsive; call strategy gated max-average pooling ability responsive provided gate analogy usage gates elsewhere deep learning. another natural generalization pooling operations allow pooling operations combined learned. hence second approach learn combine pooling ﬁlters learned. speciﬁcally learning performed within binary tree leaf associated learned pooling ﬁlter. consider internal nodes tree parent node associated output value mixture child node output values ﬁnally reach root node. root node corresponds overall output produced tree. refer strategy tree pooling. tree pooling intended learn pooling ﬁlters directly data; learn combine leaf node pooling ﬁlters differentiable fashion; bring together characteristics within hierarchical tree structure. mixing node outputs allowed responsive resulting tree pooling operation becomes integrated method learning pooling ﬁlters combinations ﬁlters able display range different behaviors depending characteristics region pooled. chitectures investigate replacing standard pooling operations proposed generalized pooling methods boosts performance standard benchmark datasets well larger complex imagenet dataset. attain state-of-the-art results mnist cifar svhn. proposed pooling operations used drop-in replacements standard pooling operations various current architectures used tandem performance-boosting approaches learning activation functions training data augmentation modifying aspects network architecture conﬁrm improvements used dsn-style architecture well alexnet googlenet. proposed pooling operations also simple implement computationally undemanding differentiable modest number additional parameters. related work current deep learning literature popular pooling functions include average stochastic pooling recent effort using complex pooling operations spatial pyramid pooling mainly designed deal images varying size rather delving different pooling functions incorporating learning. learning pooling functions analogous receptive ﬁeld learning however methods like lead difﬁcult learning procedure turn leads less competitive result e.g. error rate unaugmented cifar. since tree pooling approach involves tree structure learning observe analogy logic-type approaches decision trees logical operators approaches played central role artiﬁcial intelligence applications require discrete reasoning often intuitively appealing. unfortunately despite appeal logic-type approaches disconnect functioning decision trees functioning cnns output standard decision tree non-continuous respect input means standard decision tree able used cnns whose learning process performed back propagation using gradients differentiable functions. part allows pursue approaches ensure resulting pooling operation differentiable thus usable within network backpropagation. recent work referred auto-encoder trees also pays attention differentiable tree structures deep learning distinct method focuses learning encoding decoding methods using soft decision tree generative model. supervised setting incorporates multilayer perceptrons within decision trees simply uses generalizing pooling operations typical convolutional neural network structured series convolutional layers pooling layers. convolutional layer intended produce representations reﬂect aspects local spatial structures consider multiple channels speciﬁcally convolution layer computes feature response maps involve multiple channels within localized spatial region. hand pooling layer restricted within channel time condensing activation values spatiallylocal region currently considered channel. early reference related pooling operations found modern visual recognition systems pooling operations play role producing downstream representations robust effects variations data still preserving important motifs. speciﬁc choices average pooling pooling widely used many cnn-like architectures; includes theoretical analysis goal bring learning responsiveness pooling operation. focus approaches particular. ﬁrst approach begin pooling operations pooling average pooling learn combine them. within approach consider strategies combine ﬁxed pooling operations. strategies unresponsive characteristics region pooled; learning process strategy result effective pooling operation speciﬁc unchanging mixture average. emphasize unchanging mixture refer strategy mixed max-average pooling. strategy responsive characteristics region pooled; learning process strategy results gating mask. learned gating mask used determine responsive pooling average pooling; speciﬁcally value inner product gating mask current region being pooled sigmoid output used mixing proportion average. emphasize role gating mask determining responsive mixing proportion refer strategy gated max-average pooling. figure illustration proposed pooling operations mixed max-average pooling gated max-average pooling tree pooling indicate region pooled gating masks pooling ﬁlters denotes indicator function. experiment section report results parameter pooling layer option; network experiment pooling layers parameters network using standard pooling operations. found even simple option yielded surprisingly large performance boost. also obtain results simple average well option largest number parameters parameter combination layer/channel/region parameters mixed pooling layer using option observe increase number parameters corresponding boost performance pursue layer option. present pooling often used default cnns. touch relative performance pooling e.g. average pooling part collection exploratory experiments test invariance properties pooling functions common image transformations figure results indicate that evaluation dataset regimes either pooling average pooling demonstrates better performance light observation neither pooling average pooling dominates other ﬁrst natural generalization strategy call mixed max-average pooling learn speciﬁc mixing proportion parameters data. learning mixing proportion parameters several options learning mixing proportion parameter layer layer/region pooled layer/channel layer/region/channel combination. form mixed pooling operation scalar mixing proportion specifying speciﬁc combination average; subscript used indicate equation layer option. output loss function deﬁned automatically learn mixing proportion vanilla backpropagation learning given generalization strategies learn pooling operations themselves. this turn consider learning pooling operations also learning combine pooling operations. since combinations considered within context binary tree structure refer approach tree pooling. pursue details following sections. combining average pooling functions mixed max-average pooling conventional pooling operation ﬁxed either maximum opersimple average fave ation fmax maxi vector contains activation values local pooling region pixels image channel. mixed strategy per-layer option would total extra parameters relative standard pooling. gated strategy per-layer option would total extra parameters number parameters gating mask. mixed strategy detailed immediately uses fewer parameters nonresponsive; gated strategy involves parameters responsive. experiments mixed outperformed gated gate pooling layer. interestingly parameter gated network gate pooling layer also outperforms mixed option parameters except relatively large svhn dataset. touch below; section contains details. quick comparison mixed gated pooling results table indicate beneﬁt learning pooling operations learning. within learned pooling operations number parameters mixed strategy increased performance improves; however parameter count entire story. responsive gated max-avg strategy consistently yields better performance achieved extra parameters layer/rg/ch non-responsive mixed max-avg strategy. relatively larger svhn dataset provides sole exception found baseline mixed mixed gated table classiﬁcation error comparison baseline model corresponding networks pooling replaced pooling operation listed. superscripted indicates standard data augmentation report means standard deviations separate trials without model averaging. learned mixing proportion used combining pooling average pooling. mentioned earlier learned mixing proportion remains ﬁxed nonresponsive insofar remains matter characteristics present region pooled. consider responsive strategy call gated max-average pooling. strategy rather directly learning mixing proportion ﬁxed learning instead learn gating mask scalar result inner product gating mask region pooled sigmoid produce value mixing proportion. strategy means actual mixing proportion vary depending characteristics present region pooled. speciﬁc suppose denote values region pooled denote values gating mask. responx) sive mixing proportion given sigmoid funcσ learning gating mask layer layer/region pooled layer/channel layer/region/channel combination. suppress subscript denoting speciﬁc option since equations otherwise identical option. resulting pooling operation gated maxaverage pooling head-to-head parameter count every single mixing proportion parameter mixed max-average pooling strategy corresponds gating mask gated strategy take speciﬁc example suppose consider network pooling layers pooling regions parameter updates backpropagation. motivates instead internal node sigmoid gate funcx) tree pooling function tion pooling function ftree chain rule comparison baseline model proposed methods involving tree pooling. superscripted indicates standard data augmentation comparison making network deeper using conv layers investigate whether simply adding depth baseline network gives performance boost comparable observed proposed pooling operations report table additional experiments cifar count depth counting layer learned parameters extra layer depth number parameter layers baseline network additional standard convolution layers matches number parameter layers best performing pooling operations allow pooling operations combined learned. pooling layers remain distinct convolution layers since pooling performed separately within channel; channel isolation also means even option introduces largest number parameters still introduces fewer parameters convolution layer would introduce. basic version approach would involve combining learned pooling operations simply learning pooling operations form values pooling ﬁlters. step brings refer tree pooling learn pooling ﬁlters also learn responsively combine learned ﬁlters. aspects learning performed within binary tree leaf associated pooling ﬁlter learned training. consider internal nodes tree parent node associated output value mixture child node output values ﬁnally reach root node. root node corresponds overall output produced tree mixtures responsively learned. tree pooling intended learn pooling ﬁlters directly data; learn leaf node pooling ﬁlters differentiable fashion; bring together characteristics within hierarchical tree structure. leaf node tree associated pooling ﬁlter learned; node index denote pooling ﬁlter degenerate tree consisting single node pooling region would result scalar value nodes proceed fashion analogous case gated max-average pooling learned gating masks denoted pooling result arbitrary node thus overall pooling operation would thus result evaluating froot node. appeal tree pooling approach would limited could train proposed layer fashion integrated within network whole. would case attempted directly traditional decision tree since output presents points discontinuity respect inputs. reason discontinuity traditional decision tree output decision tree makes hard decisions; terminology used above hard decision node corresponds mixing proportion take value consequence type hard function differentiable turn interferes ability iterative figure controlled experiment cifar investigating relative beneﬁt selected pooling operations terms robustness three types data variation. three kinds variations choose investigate rotation translation scale. kind variation modify cifar test images according listed amount. observe that across types amounts variation proposed pooling operations investigated provide improved robustness transformations relative standard choices maxpool avgpool. method requires extra parameters obtains state-of-the-art error. hand making networks deeper conv layers adds many parameters yields test error drop conﬁguration explored. since follow additional conv layer relu networks correspond increasing nonlinearity well adding depth adding parameters. experiments indicate performance proposed pooling accounted simple effect addition depth/parameters/nonlinearity. comparison alternative pooling layers whether might similar performance boosts replacing pooling baseline network conﬁguration alternative pooling operations stochastic pooling pooling using stride convolution layer pooling simple ﬁxed proportion max-avg pooling performed another experiments unaugmented cifar. baseline error rate replacing pooling layers stacked stride convrelu lowers error adds extra parameters. using stochastic pooling adds computational overhead parameters results error. simple average computationally light yields error additional parameters. finally tree+gated max-avg conﬁguration adds parameters achieves state-of-the-art error. quick performance overview ease discussion collect observations subsequent experiments view highlighting aspects shed light performance characteristics proposed pooling functions. first seen experiment shown figure replacing standard pooling operations either gated max-avg tree pooling yielded boost cifar test accuracy test images underwent three different kinds transformations. boost observed across entire range transformation amounts transformations already observe improved robustness initial experiment intend investigate instances proposed pooling operations time permits. second performance attain experiments reported figure table table table table achieved modest additional numbers parameters e.g. cifar best performance uses additional parameters reduces test error cifar section details. alexnet experiment replacing maxpool layers proposed pooling operations gave relative reduction test error additional parameters imagenet section details. also investigate additional time incurred using proposed pooling operations; experiments reported timing section overhead ranges testing invariance properties going overall classiﬁcation results investigate invariance properties networks utilizing either standard pooling operations instances proposed pooling operations yield best performance begin training four different networks cifar training four pooling operations selected consideration; training details found sec. seek determine respective invariance properties networks evaluating accuracy various transformed versions cifar test set. figure illustrates test accuracy attained presence image rotation translation scaling cifar test set. timing order evaluate much additional time incurred proposed learned pooling operations measured average forward+backward time cifar image. case layer option used. additional computation time incurred ranges speciﬁcally baseline network took baseline mixed maxavg took baseline gated max-avg took baseline level tree pooling took ﬁnally baseline tree+gated max-avg took experiments evaluate proposed max-average pooling tree pooling approaches standard benchmark datasets mnist cifar cifar svhn imagenet control effect differences data data preparation match data data preparation used please refer detailed description. describe basic network architecture specify various hyperparameter choices. basic experiment architecture contains standard convolutional layers three mlpconv layers placed conv conv conv respectively. chose number channels layer analogous choices speciﬁc numbers provided sections dataset. follow every conv-type layers relu activation functions. ﬁnal mlpconv layer used reduce dimension last layer match total number classes different dataset overall model parameter count analogous proposed maxaverage pooling tree pooling layers pooling regions used mlpconv mlpconv layers provide detailed listing network conﬁgurations table supplementary materials. learning rate decreased whenever validation error stops decreasing; schedule experiments. momentum weight decay ﬁxed datasets another regularizer besides dropout. initial pooling ﬁlters pooling masks values sampled gaussian distribution zero mean standard deviation hyperparameter settings experiments reported tables model averaging done test time. classiﬁcation results tables show overall experimental results. baseline network trained conventional pooling. mixed refers network max-avg pooling strategy ﬁrst second pooling layers gated corresponding meaning. tree refers again tree pooling ﬁrst pooling layer only; improvement tree pooling used pooling layers. observation motivated consider following tree pooling layer gated max-avg pooling layer tree+max-average refers network conﬁguration tree pooling ﬁrst pooling layer gated max-average pooling second pooling layer. results produced network structure hyperparameter settings difference choice pooling function. table details. mnist mnist model channels conv conv channels mlpconv mlpconv respectively. preprocessing mean subtraction. tables show previous best results proposed pooling methods. cifar cifar model channels conv conv channels mlpconv mlpconv respectively. also performed experiment learned single pooling ﬁlter without tree structure obtained improvement baseline model. results indicate performance improves pooling ﬁlter learned improves also learn combine learned pooling ﬁlters. all-cnn method uses convolutional layers place pooling layers cnn-type network architecture. however standard convolutional layer requires many parameters gated max-average pooling layer tree-pooling layer pooling operations tree+max-avg network contable classiﬁcation error reported recent comparable publications four benchmark datasets single model data augmentation unless otherwise indicated. superscripted indicates standard data augmentation indicates cited work report results dataset. ﬁxed network conﬁguration using proposed tree+max-avg pooling yields state-of-the-art performance datasets ﬁguration parameters tree-pooling layer leaf nodes internal nodes parameters gating mask used gated max-average pooling layer best result contains total nearly parameters layers performing pooling like operations; relative cifar accuracies data augmentation experiment followed standard data augmentation procedure training augmented data observe trends seen data augmentation experiments. note reports error rate extensive data augmentation much wider deeper million parameter network times networks. cifar cifar model channels convolutional layers channels mlpconv mlpconv respectively. street view house numbers svhn model channels conv conv channels mlpconv mlpconv reterms amount data svhn spectively. larger training data much larger amount training data motivated explore performance might observe pursued layer/channel/region option even simple mixed max-avg strategy results huge increase total number parameters learn proposed pooling layers speciﬁcally total mixed max-avg strategy parameter pooling layer option increase using layer/channel/region option mixed max-avg strategy observe test error mnist cifar cifar+ cifar svhn. interestingly mnist cifar+ cifar mixed max-avg performance mixed max-avg gated max-avg cifar mixed max-avg worse either layer max-avg strategies. svhn result using mixed max-avg sets state art. imagenet experiment directly compete best performing result challenge involve many additional aspects beyond pooling operations) rather provide illustrative comparison relative beneﬁt proposed pooling methods versus conventional pooling dataset. network structure parameter setup simply replace ﬁrst pooling tree pooling replace second third pooling gated max-average pooling relative original alexnet adds parameters achieves relative error reduction googlenet conﬁguration uses gated max-avg pooling layers total extra parameters million standard googlenet. table shows direct comparison table batch normalization observations experiments experiment using proposed pooling operations boosted performance. ﬁxed network conﬁguration using proposed tree+max-avg pooling yields state-of-the-art performance mnist cifar svhn. observed boosts tandem data augmentation multi-view predictions batch normalization several different architectures nin-style dsn-style parameter alexnet -layer googlenet. netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning", "year": 2015}