{"title": "Multilayer bootstrap networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a nonparametric density estimator. It consists of a group of k-centroids clusterings. Each clustering randomly selects data points with randomly selected features as its centroids, and learns a one-hot encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric density estimator at each layer projects the input data space to a uniformly-distributed discrete feature space, where the similarity of two data points in the discrete feature space is measured by the number of the nearest centroids they share in common. The multilayer network gradually reduces the nonlinear variations of data from bottom up by building a vast number of hierarchical trees implicitly on the original data space. Theoretically, the estimation error caused by the nonparametric density estimator is proportional to the correlation between the clusterings, both of which are reduced by the randomization steps.", "text": "multilayer bootstrap network builds gradually narrowed multilayer nonlinear network bottom unsupervised nonlinear dimensionality reduction. layer network nonparametric density estimator. consists group k-centroids clusterings. clustering randomly selects data points randomly selected features centroids learns one-hot encoder one-nearest-neighbor optimization. geometrically nonparametric density estimator layer projects input data space uniformly-distributed discrete feature space similarity data points discrete feature space measured number nearest centroids share common. multilayer network gradually reduces nonlinear variations data bottom building vast number hierarchical trees implicitly original data space. theoretically estimation error caused nonparametric density estimator proportional correlation clusterings reduced randomization steps. principal component analysis simple widely used unsupervised dimensionality reduction method ﬁnds coordinate system original euclidean space linearly uncorrelated coordinate axes describe variances data. insuﬃcient capture highly-nonlinear data distributions many dimensionality reduction methods explored. dimensionality reduction core steps. ﬁrst step ﬁnds suitable feature space density data feature representation well discovered i.e. density estimation problem. second step discards noise components small variations data feature representation i.e. principal component reduction problem feature space. dimensionality reduction methods either linear nonlinear based connection data space feature space. paper focuses nonlinear methods categorized three classes. ﬁrst class kernel methods. ﬁrst projects data kernelinduced feature space conducts variants space. examples include kernel isomap locally linear embedding laplacian eigenmaps t-distributed stochastic neighbor embedding generalizations second class probabilistic models. assumes data generated underlying probability function takes posterior parameters feature representation. examples include gaussian mixture model latent dirichlet allocation third class autoassociative neural networks learns piecewise-linear coordinate system explicitly backpropagation uses output bottleneck layer representation. however feature representations produced aforementioned methods deﬁned continuous spaces. fundamental weakness using continuous space hard simple mathematical form transforms data space ideal continuous feature space since real-world data distribution non-uniform irregular. overcome diﬃculty large number machine learning methods proposed distance metric learning kernel learning kernel methods dirichlet process prior baysian probabilistic models advanced optimization methods applied. recently learning multiple layers nonlinear transforms named deep learning trend deep network contains nonlinear layers. layer consists group nonlinear computational units parallel. hierarchical structure distributed representation layer representation learning ability deep network exponentially powerful shallow network given number nonlinear units. however development deep learning mostly supervised e.g. words density local region probability distribution approximated counting events fall local region. paper focuses exploring idea. generate events resort random resampling statistics count events resort one-nearest-neighbor optimization binarize feature space discrete space. reduce small variations noise components data i.e. second step dimensionality reduction extend density estimator gradually narrowed deep architecture essentially builds vast number hierarchical trees discrete feature space. overall simple algorithm named multilayer bootstrap networks knowledge although ensemble learning triggered random resampling large family machine learning prevalent unsupervised dimensionality reduction. furthermore methods estimate density data discrete spaces random resampling extensions deep learning. paper organized follows. section describe mbn. section give geometric interpretation mbn. section justify theoretically. section study empirically. section introduce related work. section summarize contributions. contains multiple hidden layers output layer hidden layer consists group mutually independent k-centroids clusterings; k-centroids clustering output units indicates cluster; output units k-centroids clusterings concatenated input upper layer. output layer pca. network gradually narrowed bottom implemented setting parameter large possible bottom layer smaller smaller along increase number layers predeﬁned smallest reached. trained layer-by-layer bottom training layer given d-dimensional input data either lower layer original data space simply need focus training k-centroids clustering consists following steps figure network structure. dimension input data demo network colored square represents k-centroids clustering. layer contains clusterings. parameters layers respectively. outputs clusterings layer concatenated input upper layer. one-nearest-neighbor learning. representation input produced current clustering indicator vector indicates nearest centroid example second centroid nearest similarity metric centroids bottom layer customized e.g. squared euclidean distance mink maxk close distribution small variance euclidean distance similarity metric similarity smaller true. proposed method provides simple solution similarity metric learning problem. outputs feature representation follows. k-centroids clustering contributes neighboring centroid centroid partitions local region disconnected parts containing not. data point owns local region supported centroids clusterings closest data point. centroids partition local region many fractions. fraction represented unique binary code output space estimator illustrated fig. representation output feature space binary code represents local fraction data space located. representation similarity data points calculated counting number nearest centroids share common—a method frequentist methodology. fig. local region partitioned local region diﬀerence local region ampliﬁcation local region surfaces local regions discrete feature space. share common nearest centroids also share common nearest centroids similarity equals similarity note k-centroids clusterings able partition data space fractions maximum representation ability exponentially powerful single k-centroids clustering. dimensionality reduction deep ensemble architecture nonparametric density estimator captures variances input data however responsible reducing small variances noise components. reduce nonlinear variations build gradually narrowed deep network. network essentially reduces nonlinear variations data building vast number hierarchical trees data space implicitly. present geometric principle follows. suppose layers parameters layers respectively fig. know l-th layer partitions input data space disconnected small fractions. fraction encoded single point output feature space. points output feature space nodes trees. hence l-th layer nodes. bottom layer leaf nodes. layer root nodes number trees builds. important keep parameter k-centroids clusterings layer same. otherwise density centroids clusterings diﬀerent regard centroids coordinates axes local region coordinate axes built diﬀerent data spaces. inherit problems deep neural networks local minima overﬁtting small-scale data gradient vanishing since trained simply random resampling stacking. result trained many hidden layers needed small-scale large-scale data. overcome weakness propose compressive compressive uses neural network learn mapping function training data output training stage uses neural network prediction. generally applied speciﬁc task compressive learn mapping function input data output task directly. presented section core problem machine learning suitable similarity metric maps original data space uniformly-distributed feature space. give example importance. shown fig. similarity data points apart distribution large variance might similarity data points figure illustration similarity metric problem data space. similarity problem data points distribution local region area colored circle centered small hollow points cross area local regions shared centroids similarity problem data points distribution figure encoding local region data point k-centroids clustering k-centroids clusterings three k-centroids clusterings. centroids three k-centroids clusterings points colored black green respectively. local region centroid area circle around centroid. centroid edge local region drawn color. adjacent layers easy neighboring child nodes layer merged single father node l-th layer average. generalize property entire single root node l-th layer merging leaf nodes bottom layer means nonlinear variations among leaf nodes merged root node reduced completely. conclude highly invariant nonlinear variations data. special case that output feature spaces k-centroids clusterings layer single point. however practice never instead usually termination condition ground-truth number classes. termination condition makes k-centroids clustering stronger random guess although estimates density data discrete feature space estimation error small controllable. speciﬁcally shown fig. k-centroids clusterings partition local region disconnected small fractions maximum. easy imagine that local region increasing diversity centroids still reserved ensemble k-centroids clusterings approximates true data distribution. diversity important used randomized steps enlarge note data points distributed sparsely output feature space. hence seldom merges data points. words learns data representations instead agglomerative data clustering. suppose random samples training k-centroids clusterings identically distributed necessarily independent pairwise positive correlation coeﬃcient random samples focus analyzing given point assume true local coordinate invariant point around usually found density nearest centroids around goes inﬁnity also suppose projected given ﬁnite number nearest centroids {wv}v correlation coeﬃcient centroids {wv}v note used invariant reference point studying geometric interpretation know owns local space moreover parameter small enough locally linear. fact assumption features i.e. uncorrelated able assume follows multivariate normal distribution around identity matrix describes variance covariance matrix. denote following focus analyzing estimation error single dimension omit index clarity. known mean squared error regression problem decomposed summation squared bias component variance component theorem following corollaries easily corollary reduced eensemble reduced esingle esingle/v accordingly. corollary eensemble reaches lower bound ρesingle. corollary know important reduce adopted randomization steps reduce however although decreasing parameters help reduce also cause esingle rise. words reducing esingle reducing pair contradictory factors needs balance proper parameter setting. section ﬁrst introduce typical parameter setting demonstrate density estimation ability synthetic data sets ﬁnally apply dimensional output tasks visualization clustering document retrieval. data small-scale linear-kernel-based kernel toolbox mbn. data middlelargescale expectation-maximization insensitive parameters speciﬁed used default values. denote parameter layer control introduce parameter deﬁned relatively sensitive parameter data highly-nonlinear large value otherwise small value; nonlinearity data unknown default. four synthetic data sets non-uniform densities nonlinear variations used evaluation. gaussian data jain data pathbased data compound data respectively. parameter acute myeloid leukemia acute lymphoblastic leukemia biomedical data two-class problem consists training examples test examples example dimensions produced human genes. compared spectral applied k-means clustering low-dimensional outputs comparison methods well original highdimensional features. prevent local minima problem k-means clustering k-means clustering times picked clustering result corresponded optimal objective value k-means clustering among candidate objective values ﬁnal result. comparison method times reported average performance. parameter settings data sets follows. preserved largest eigenvalues corresponding eigenvectors. spectral output dimension ground-truth number classes adopted kernel. reported results ﬁxed kernel width behaves averagely best data sets well best result searching kernel width data average pairwise euclidean distance data. parameter selection methods spectral denoted spectralno_tuning spectraloptimal respectively. output dimension ground-truth number classes. reported results well best results searching data set. parameter selection methods denoted mbnno_tuning mbnoptimal respectively. evaluated clustering result terms normalized mutual information clustering accuracy. clustering results tables show mbnno_tuning achieves better performance spectralno_tuning mbnoptimal achieves better performance spectraloptimal. besides data sets table also conducted experiments following data sets lung-cancer biomedical data coil images usps images extended-yaleb images reuters- text corpus text corpus. experimental conclusions consistent results tables showed eﬀect parameter data fig. ﬁgure observe stable interval supposed achieve optimal performance across data sets; data highly variant setting large value yields good performance vise versa. generally nonlinearity data unknown setting safe. figure density estimation gaussian data presence outlier. cross denotes outlier. outlier gaussian data resolution sub-ﬁgure top-right corner high enough diﬀerentiate classes gaussian data. comparison nonlinear methods costly full mnist data except randomly sampled images images digit evaluation. visualization result fig. shows lowdimensional feature produced small within-class variance large between-class distance. demonstrate scalability larger data sets generalization ability unseen data trained training images evaluated eﬀectiveness test images reduce training cost. visualization result fig. shows full mnist provides clearer visualization small subset mnist. benchmark data sets used evaluation. cover topics speech processing chemistry biomedicine image processing text processing. details data sets experimental phenomena data sets similar reported phenomena small subset mnist representative fig. ﬁgure know prevent setting small value. empirically empirically applied document retrieval compared latent semantic analysis document retrieval method based larger data set— reuters newswire stories consist documents. data reuters newswire stories divided topics grouped tree structure. preserved leaf topics. result unlabeled documents. preprocessed document vector commonest word stems rainbow software entry vector word count. randomly selected half data training half test. recorded average accuracy queries test document retrieval setting query retrieved documents diﬀerent documents test set. unlabeled document retrieved considered mistake. unlabeled document used query relevant documents would retrieved means precisions unlabeled query levels zero. feature .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% timeout .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% experimental results fig. show small network reaches accuracy curve higher lsa; large network reaches accuracy curve higher lsa. results indicate enlarging network size improves generalization ability. ﬁrst studied generalization ability compressive test images mnist models compressive trained training images mnist. neural network compressive contains hidden layers rectiﬁed linear units layer. projects data space -dimensional feature space produced mbn. visualization results fig. show compressive produces identical studied generalization ability compressive retrieving reuters newsware stories. neural network structure mnist. projects data space -dimensional representation produced mbn. result fig. shows compressive produces almost identical accuracy curve mbn. prediction time compressive seconds respectively. related many methods statistics machine learning. introduce connection histogram-based density estimators bootstrap methods clustering ensemble vector quantization product experts sparse coding unsupervised deep learning. feature .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±.% timeout .%±.% .%±.% .%±. .%±.% .%±.% .%±. .%±.% .%±. .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% timeout .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% .%±.% resampling used build local coordinate systems hence data point sampled multiple times duplicated data points still viewed single coordinate axis. moreover cause k-centroids clusterings built different data spaces. however motivated shares many common properties bootstrap methods building base clustering random sample input de-correlating base clusterings random sampling features base clustering hence adopted phrase bootstrap clarify usage preventing confusion. clustering ensemble clustering technique uses consensus function aggregate clustering results mutually-independent base clusterings. base clustering usually used classify data ground-truth number classes. exceptional clustering ensemble method base k-means clustering produces subclass partition assigning parameter random value slightly larger ground-truth number classes. layer regarded clustering ensemble. however purpose estimate density data instead producing aggregated clustering result. moreover clear theoretical geometric explanations. also found setting random value work particularly also small. layer regarded vector quantizer input data space. codebook produced exponentially smaller produced traditional k-means clustering level quantization errors. similar idea named product quantization explored product quantization uses random histogram-based density estimation fundamental density estimation method estimates probability function accumulating events fall intervals function intervals equivalent length not. proposed density estimator essentially approach. interval approach deﬁned data space data points. accumulated events interval shared centroids data points. bootstrap resampling applied successfully machine learning. resamples data replacement. adopt standard bootstrap resampling. fact uses random subsampling without replacement also known delete-d jackknife resampling statistics reason adopt standard bootstrap resampling former designed reducing computational storage complexities developed purpose. aims providing simple overcome diﬃculty density estimation continuous space. hence generates larger codebooks common vector quantization methods transform sparse codes compact binary codes. aims combine multiple individual models multiplying them individual models complicated contains hidden variables general probability framework sampling features instead ﬁxed non-overlapping partition features uses random sampling data points train k-means clustering instead expectation-maximization optimization product quantization equals single layer mbn. also aware hierarchical product quantizer builds multiple sets sub-quantizers original feature. builds layer sub-quantizers output sparse feature lower layer fundamentally diﬀerent hierarchical product quantization method. indexes possible vectors data space called expert. major merit that function fully expressed mixture experts experts gaussian mixture model kmeans clustering expressed compactly experts minimum expense consists exponentially large number components. given learned dictionary sparse coding typically aims represents solve minhi q-norm sparse code data point hyperparameter controlling sparsity column called basis vector. understand connection sparse coding view hyperparameter controls number clusterings. speciﬁcally likely contains nonzero element. intuitively understand clustering learn sparse code. good value make small part elements nonzero. choice approximates method partitioning dictionary several subsets grouping basis vectors subset base clustering. motivated intuitive analysis introduce connection sparse coding formally follows learning abstract representations deep networks recent trend. geometric point view abstract representations produced reducing larger larger local variations data bottom framework trees built data spaces. example convolutional neural network merges child nodes pooling. hierarchical dirichlet process builds trees whose father nodes generate child nodes according prior distribution. merges child nodes reducing number nonlinear units gradually bottom subspace tree merges nodes reducing dimensions subspaces gradually. pcanet merges nodes reducing output dimensions local associated patches gradually. merges nodes reducing number randomly sampled centroids gradually. fundamental diﬀerence methods build eﬀective local coordinate systems layer. knowledge simple method needs little assumption prior knowledge. complicated paper proposed multilayer bootstrap network nonlinear dimensionality reduction. novel network structure expert k-centroids clustering whose centroids randomly sampled data points randomly sampled features; network gradually narrowed bottom composed novel components layer nonparametric density estimator random resampling. estimates density data correctly without model assumption. exponentially powerful single k-centroids clustering. estimation error proven small controllable. network deep ensemble model. essentially reduces nonlinear variations data building vast number hierarchical trees data space. trained many layers needed large-scale small-scale data. performs robustly wide range parameter settings. time storage complexities scale linearly size training data. supports parallel computing naturally. empirical results demonstrate eﬃciency training stage eﬀectiveness density estimation data visualization clustering document retrieval. also demonstrated high computational complexity test stage eliminated compressive mbn—a framework unsupervised model compression based neural networks. problem left selection parameter controls network structure. although performance good still large performance best hence select automatically important problem. author thanks prof. deliang wang providing computing resources. author also thanks action editor reviewers comments improved quality paper. work supported national natural science foundation china grant", "year": 2014}