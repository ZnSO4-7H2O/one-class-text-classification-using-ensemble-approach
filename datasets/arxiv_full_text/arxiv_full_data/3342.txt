{"title": "Incremental Robot Learning of New Objects with Fixed Update Time", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.RO"], "abstract": "We consider object recognition in the context of lifelong learning, where a robotic agent learns to discriminate between a growing number of object classes as it accumulates experience about the environment. We propose an incremental variant of the Regularized Least Squares for Classification (RLSC) algorithm, and exploit its structure to seamlessly add new classes to the learned model. The presented algorithm addresses the problem of having an unbalanced proportion of training examples per class, which occurs when new objects are presented to the system for the first time.  We evaluate our algorithm on both a machine learning benchmark dataset and two challenging object recognition tasks in a robotic setting. Empirical evidence shows that our approach achieves comparable or higher classification performance than its batch counterpart when classes are unbalanced, while being significantly faster.", "text": "abstract— consider object recognition context lifelong learning robotic agent learns discriminate growing number object classes accumulates experience environment. propose incremental variant regularized least squares classiﬁcation algorithm exploit structure seamlessly classes learned model. presented algorithm addresses problem unbalanced proportion training examples class occurs objects presented system ﬁrst time. evaluate algorithm machine learning benchmark dataset challenging object recognition tasks robotic setting. empirical evidence shows approach achieves comparable higher classiﬁcation performance batch counterpart classes unbalanced signiﬁcantly faster. order autonomous robots operate unstructured environments several perceptual capabilities required. skills cannot hard-coded system beforehand need developed learned time agent explores acquires novel experience. prototypical example setting work consider task visual object recognition robotics images depicting different objects received frame time system needs incrementally update internal model known objects examples gathered. last years machine learning achieved remarkable results variety applications robotics computer vision however methods developed off-line settings entire training available beforehand. problem updating learned model online addressed literature algorithms proposed context take account challenges characteristic realistic lifelong learning applications. speciﬁcally online classiﬁcation settings major challenge cope situation novel class added model. indeed learning algorithms require number classes known beforehand grow indeﬁnitely imbalance examples class many examples previously learned classes lead unexpected undesired behaviors precisely work theoretically empirically observe ∗equal contribution †icub facility istituto italiano tecnologia genoa italy. email lcsl cambridge usa. ‡dibris universit`a degli studi genova genoa italy. under-represented class likely ignored learned model favor classes training examples already observed sufﬁcient number examples provided also class. several methods proposed literature deal class imbalance batch setting rebalancing misclassiﬁcation errors accordingly however point work rebalancing cannot applied online setting without re-training entire model scratch every time example acquired. would incur computational learning times increase least linearly number examples clearly feasible scenarios training data grows indeﬁnitely. learns incrementally respect number examples classes accounts potential class unbalance. algorithm builds recursive version regularized least squares classiﬁcation achieve ﬁxed incremental learning times adding examples model efﬁciently dealing imbalance classes. evaluate approach standard machine learning benchmark classiﬁcation challenging visual object recognition datasets robotics. results highlight clear advantages approach classes learned incrementally. paper organized follows sec. overviews related work incremental learning class imbalance. sec. introduce learning setting discussing impact class imbalance presenting approaches adopted literature deal problem. sec. reviews recursive rlsc algorithm. sec. build previous sec. derive approach proposed work extends recursive rlsc allow addition classes ﬁxed update time dealing class imbalance. sec. report empirical evaluation method concluding paper sec. vii. incremental learning. problem learning continuous stream data addressed literature multiple perspectives. simplest strategy retrain system updated training whenever example received model previous iteration used initialization learn predictor reducing training time. approaches require store training data retrain points imbalance among class labels. simplicity following consider binary classiﬁcation setting postponing extension multiclass classiﬁcation section. refer reader details statistical learning theory classiﬁcation. optimal bayes classiﬁer least squares surrogate consider binary classiﬁcation problem input-output examples sampled randomly according distribution goal learn function minimizing overall expected classiﬁcation error argmin given ﬁnite observations yi}n randomly sampled denotes binary function taking value otherwise. solution called optimal bayes classiﬁer shown satisfy equation denoted conditional distribution given work denote marginal distribution bayes’ rule computing good estimates typically requires large training datasets often unfeasible practice. therefore so-called surrogate problem usually adopted simplify optimization problem asymptotically recover optimal bayes classiﬁer. sense wellknown surrogate approach consider least squares expected risk minimization empirical setting. solving problem practice provided ﬁnite yi}n training examples. settings typical approach estimator minimizing regularized empirical risk incremental approaches require keep previous data memory divided stochastic recursive methods. stochastic techniques assume training data randomly sampled unknown distribution offer asymptotic convergence guarantees ideal predictor however empirically observed methods perform well seeing training point once hence requiring perform multiple passes data problem referred catastrophic effect forgetting occurs training stochastic model examples ignoring previous ones recently attracted attention neural networks literature recursive techniques based name suggests recursive formulation batch learning algorithms. formulation typically allows compute current model closed form combination previous model observed example discuss detail sec. algorithm proposed work based recursive method. learning increasing number classes. classiﬁcation algorithms developed batch settings therefore require number classes known priori. however assumption often broken incremental settings since examples could belong previously unknown classes. problem dealing increasing number classes addressed contexts transfer learning learning learn settings consider scenario linear predictors learned model classes. then class observed associated predictor learned requirement close linear combination previous ones approaches recently proposed class hierarchy built incrementally classes observed allowing create taxonomy exploit possible similarities among different classes however methods incremental number examples require retrain system every time point received. class imbalance. problems related class imbalance previously studied literature addressed sec. iii. methods tackle issue proposed typically re-weighting misclassiﬁcation loss account class imbalance. however discuss sec. case square loss methods cannot implemented incrementally. problematic since imbalance among multiple classes often arises online settings even temporarily instance examples class observed ﬁrst time. so-called regularizer preventing solution overﬁt. indeed shown that mild assumptions distribution possible converge probability ideal number training points grows indeﬁnitely. sec. review method compute practice batch online settings. classiﬁcation rule associates every class highest likelihood however settings classes balanced approach could lead unexpected undesired behaviors. notice that bayes’ rule example labeled whenever fig. report example effect unbalanced data showing decision boundary optimal bayes classiﬁer varies takes values noticed classes maintain shape decision boundary remarkably affected value clearly online robotics setting effect could critically suboptimal reasons would like robot recognize high accuracy even objects less common seen. incremental settings whenever novel object observed ﬁrst time training examples available need loss weighting fairly also underrepresented classes. paper consider general approach rebalancing classiﬁcation loss standard learning problem similar ones begin noticing balanced setting namely classiﬁcation rule equivalent assigning class whenever vice-versa. want slightly modify misclassiﬁcation loss recover rule also unbalanced settings. propose apply weight loss obtaining problem fig. bayes decision boundaries standard rebalanced binary classiﬁcation loss multiple values data sampled according gaussian boundaries coincide separate increases. interestingly strategy changing weight classiﬁcation error loss naturally extended least squares surrogate. consider weighted least squares problem coding. alternative approach recover rebalanced optimal bayes classiﬁer least squares surrogate apply suitable coding function class labels namely optimal bayes decision rule corresponds function assigning label consequently rebalanced decision rule would assign class whenever function assigns weight class. generalizing binary case work denotes t-th entry vector extensions recoding rebalancing approaches setting follow analogously binary setting discussed sec. iii-d. particular coding function consists mapping vector basis et/ρ. note. previous sections presented analysis binary case considering coding class labels. done offer clear introduction classiﬁcation problem since need solve single least squares problem recover optimal bayes classiﬁer. alternatively could followed approach introduced section classes labels adopt surrogate labels would training distinct classiﬁers choosing predicted class argmax scores according approaches clearly equivalent since bayes classiﬁer corresponds respectively inequalities address problem solving empirical risk minimization introduced multiclass setting. ﬁnite training inputs yi}n labels work assume linear model classiﬁer namely matrix rd×t rewrite matrix ﬁxed time training examples observed consider learning process training data provided system time. iteration need compute rk×d rk×t matrices whose rows correspond ﬁrst training examples. computational cost evaluating according undesirable online setting grow indeﬁnitely. review computed incrementally this ﬁrst notice that construction computing requires operations computing requires inversion requires reduce cost inversion recall positive deﬁnite matrix cholesky known triangular) inversion computed principle computing cholesky decomposition still requires apply rank-one update cholesky decomposition previous step namely known require several implementations available cholesky rank-one updates; experiments used matlab routine cholupdate. therefore update computed since expensive operation multiplication particular computation independent current number training examples seen making algorithm suited online settings. section present approach incremental multiclass classiﬁcation account possibility extend number classes incrementally apply recoding approach introduced sec. iii. algorithm reported alg. propose modiﬁcation recursive rlsc allowing extend number classes constant time respect number examples seen far. denote number classes seen iteration possibilities ﬁrst case update rules explained section iv-b directly applied. second case update rule remains unchanged update needs account increase size however modify update rule without increasing computational cost ﬁrst adding column zeros namely requires operations. therefore strategy described indeed possible extend classiﬁcation capabilities incremental learner online operation without re-training scratch. following address problem dealing class imbalance incremental updates performing incremental recoding. algorithm incremental rlsc class recoding input hyperparameters initialize output learned weights iteration increment observe input output label main algorithmic difference standard rlsc variant recoding matrix containing output training examples. indeed according recoding strategy vector associated output label coded batch setting formulated matrix notation λid)−xy original output matrix replaced encoded version rn×t diagonal matrix whose t-th diagonal element clearly practice estimated empirically nt/n ratio number training examples belonging class total number examples). diagonal matrix class distribution estimators iteration computed incrementally keeping track number examples belonging computing kt/k note step requires since updating requires multiplying diagonal matrix requires computations dominated product requires therefore algorithm computationally equivalent standard incremental rlsc approach. mitigate issue propose introduce parameter raise element-wise power indeed noticed recover standard rlsc since applies full recoding. sec. vi-c discuss efﬁcient heuristic practice. diagonal matrix whose i-th entry equal class i-th training example. since changes every iteration possible derive rank-one update rule σkxk λid)− standard rlsc. empirically assessed performance alg. standard benchmark machine learning visual recognition tasks robotics. evaluate improvement provided incremental recoding classes imbalanced compared accuracy proposed method standard recursive rlsc presented sec. iv-b. competitor terms accuracy also considered rebalanced approach presented mnist benchmark composed greyscale pictures digits addressed -class digit recognition problem usually considered literature using nbal training images class. test obtained sampling ntest images class. used pixels images inputs linear classiﬁer. icubworld dataset visual object recognition robotics collected series sessions human teacher showed different objects icub humanoid robot addressed task discriminating objects instances dataset using available acquisition sessions object randomly sampling nbal ntest examples class. performed feature extraction speciﬁed i.e. taking activations layer caffenet convolutional neural network rgb-d washington visual object recognition dataset comprising objects belonging categories acquired recording image sequences object rotating turntable. addressed -class object categorization task averaging results splits speciﬁed subsampled cropped frame every full dataset following standard procedure. sampled nbal ntest images class performed feature extraction analogously icubworld using output caffenet’s layer. adopted following experimental protocol given dataset classes simulated scenario class observed selecting balanced remaining under-represented. trained classiﬁer balanced classes using randomly sampled dataset containing nbal examples class sampled validation nbal/ examples class. incrementally trained classiﬁer previous step sampling online nimb examples class. model selection performed using exclusively validation balanced classes following strategy described sec. vi-c. dataset averaged results multiple independent trials randomly sampling validation set. table report test accuracy imbalanced class entire test set. traditional batch learning settings rlsc model selection hyperparameter typically performed hold-out k-fold similar cross-validation techniques. incremental setting strategies cannot directly applied since examples observed online simple approach create validation hold every i-th example without using training iteration multiple candidate models trained incrementally different value highest validation accuracy selected prediction. however following argument sec. presence class imbalance strategy would often select classiﬁers ignore under-represented class. rebalancing validation loss necessarily solve issue could rather lead overﬁtting underrepresented class degrading accuracy classes since errors count less them. motivated empirical evidence discussed below work adopted model selection heuristic alg. guarantees degrade accuracy well-represented classes time achieving higher equal accuracy under-represented one. strategy evaluates accuracy candidate models incremental validation classes sufﬁcient number examples balanced test classes models trained according alg. varying best within predeﬁned range growing allows model maintains performance known classes improving under-represented one. validation). then choose model largest accuracy higher equal measured namely without coding. indeed seen fig. validation experiments icubworld grows classiﬁcation accuracy under-represented class increases fig. decreases remaining ones fig. heuristic chooses best trade-off performance degrade well-known classes time often improve underrepresented one. table report results three methods mnist icubworld rgb-d single underrepresented class observed similar behaviour classes. show accuracy classes under-represented note that under-represented class alg. consistently outperforms rlsc baseline account class imbalance learns models ignore class. also total accuracy results higher. interestingly robotics tasks outperforms loss rebalancing approach particularly examples under-represented class available. favorable since said rebalancing approach cannot implemented incrementally alg. separately under-represented class balanced classes classes trained examples. experiment class under-represented averaged results. noticed alg. much better imbalanced class comparable balanced ones resulting overall improved performance. point total accuracy datasets nimb comparable state art. indeed mnist achieve accuracy slightly lower reported linear classiﬁer pixels total accuracy alg. rgb-d approximately comparable state dataset icubworld dataset achieve accuracy line results reported fig. paper addressed problem learning online increasing number classes. motivated visual recognition scenario lifelong robot learning focused issues related class imbalance naturally arises object/category observed ﬁrst time. address problem proposed variant recursive regularized least squares classiﬁcation algorithm incorporates classes incrementally dynamically applies class recoding examples observed. updates performed constant time respect growing number training examples. evaluated proposed algorithm standard machine learning benchmark datasets visual recognition robotics showing approach indeed favorable online settings classes imbalanced. note that principle experiments used features extracted convolutional neural network could also directly trained network online stochastic gradient descent works empirically investigating end-to-end approach xiao zhang yang peng zhang error-driven incremental learning deep convolutional neural network largescale image classiﬁcation international conference multimedia bartlett jordan mcauliffe convexity classiﬁcation risk bounds journal american statistical association shawe-taylor cristianini kernel methods pattern anal lecun bottou bengio haffner gradient-based learning applied document recognition ieee proceedings pasquale ciliberto odone rosasco natale teaching icub recognize objects using deep convolutional neural networks icml workshop machine learning interactive systems vol. metta natale nori sandini vernon fadiga hofsten rosander lopes santos-victor icub humanoid robot open-systems platform research cognitive development neural networks vol. shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding international conference multimedia k¨ading rodner freytag denzler fine-tuning deep neural networks continuous learning scenarios accv workshop interpretation visualization deep neural nets fig. average test classiﬁcation accuracy standard incremental rlsc variant proposed work imbalanced balanced classes. models incrementally trained nimb grows described sec. vi-a. settings classes progressively included model exist still largely unexplored ﬁeld study scope work. method propose allows update predictor without using training data previous classes fast stable relying rich deep representations learned ofﬂine proven competitive state suitable online applications. work described paper supported center brains minds machines funded award ccf- firb project rbfrmac funded italian ministry education university research. acknowledge nvidia corporation donation tesla used research. simonyan zisserman very deep convolutional networks large-scale image recognition arxiv preprint schwarz schulz behnke rgb-d object recognition pose estimation based pre-trained convolutional neural network features icra sayed adaptive filters. wiley-ieee press duchi hazan singer adaptive subgradient methods online learning stochastic optimization journal machine learning research", "year": 2016}