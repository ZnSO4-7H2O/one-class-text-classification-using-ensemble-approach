{"title": "Fast Cross-Validation for Incremental Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Cross-validation (CV) is one of the main tools for performance estimation and parameter tuning in machine learning. The general recipe for computing CV estimate is to run a learning algorithm separately for each CV fold, a computationally expensive process. In this paper, we propose a new approach to reduce the computational burden of CV-based performance estimation. As opposed to all previous attempts, which are specific to a particular learning model or problem domain, we propose a general method applicable to a large class of incremental learning algorithms, which are uniquely fitted to big data problems. In particular, our method applies to a wide range of supervised and unsupervised learning tasks with different performance criteria, as long as the base learning algorithm is incremental. We show that the running time of the algorithm scales logarithmically, rather than linearly, in the number of CV folds. Furthermore, the algorithm has favorable properties for parallel and distributed implementation. Experiments with state-of-the-art incremental learning algorithms confirm the practicality of the proposed method.", "text": "grid search case k-cv session needs every combination hyper-parameters dramatically increasing computational cost even number hyper parameters small. avoid added cost much previous research went studying efﬁcient calculation estimate however previous work concerned special models problems exception izbicki methods typically limited linear prediction squared loss kernel methods various loss functions including twice-differentiable losses hinge loss works training time underlying learning algorithm size dataset main result states cv-estimate computable time. finally izbicki gives efﬁcient solution computational complexity) restrictive case models trained datasets combined constant time single model trained union datasets. although results appealing limited methods problems speciﬁc features. particular unsuitable data problems practical methods incremental linear even sub-linear time paper show calculation done efﬁciently incremental learning algorithms. section present method that mild natural conditions speeds calculation k-cv estimate incremental learning algorithms general learning setting explained section arbitrary performance measures. proposed method treecv exploits fact incremental learning algorithms need whole dataset once instead learn whatever data provided later update models data arrives without need trained whole dataset scratch. show section treecv computes guaranteed-precision approximation estimate algorithms produce example semi-supervised anomaly detection method g¨ornitz four hyper-parameters tune. thus testing possible combinations e.g. possible values hyper-parameter requires running times. cross-validation main tools performance estimation parameter tuning machine learning. general recipe computing estimate learning algorithm separately fold computationally expensive process. paper propose approach reduce computational burden cvbased performance estimation. opposed previous attempts speciﬁc particular learning model problem domain propose general method applicable large class incremental learning algorithms uniquely ﬁtted data problems. particular method applies wide range supervised unsupervised learning tasks different performance criteria long base learning algorithm incremental. show running time algorithm scales logarithmically rather linearly number folds. furthermore algorithm favorable properties parallel distributed implementation. experiments stateof-the-art incremental learning algorithms conﬁrm practicality proposed method. introduction estimating generalization performance core task machine learning. often estimate computed using k-fold cross-validation dataset partitioned subsets approximately equal size subset used evaluate model trained subsets produce numerical score; k-cv performance estimate obtained average obtained scores. signiﬁcant drawback k-cv heavy computational cost. standard method computing k-cv estimate train separate models independently fold requiring k-times work training single model. extra computational cost imposed k-cv especially high leave-one-out popular variant number folds equals number samples dataset. increased computational requirements become major problem especially used tuning hyper-parameters learning algorithms stable models. present several implementation details analyze time space complexity treecv section particular show computation time o-times bigger time required train single model major improvement compared k-times increase required naive computation estimate. finally section presents experimental results conﬁrm efﬁciency proposed algorithm. related work various methods often specialized speciﬁc learning settings proposed speed computation k-cv estimate. frequently efﬁcient k-cv computation methods specialized regularized leastsquares learning settings particular generalized cross-validation method computes loocv estimate time dataset size solution problem whole dataset; generalized k-cv calculation time pahikkala special case least-squares support vector machines cawley shows loocv computed time using cholesky factorization noted aforementioned methods inverse special matrix calculation; aforementioned running times therefore based assumption inverse available time). related idea approximating loocv estimate using notion inﬂuence functions measure effect adding inﬁnitesimal single point probability mass distribution. using notion debruyne propose approximate loocv estimate kernel-based regression algorithms twicedifferentiable loss function. bouligand inﬂuence functions generalized notion inﬂuence functions arbitrary distributions order calculate k-cv estimate kernel methods twice-differentiable loss functions. again methods need existing model trained whole dataset require running time. notable exception square-loss/differentiable loss requirement work cauwenberghs poggio propose incremental training method supportvector classiﬁcation show revert incremental algorithm unlearn data points obtain loocv estimate. loocv estimate obtained time similar single training incremental algorithm worst case. model exactly model trained union datasets izbicki compute k-cv estimate time. however assumption restrictive applies simple methods bayesian classiﬁcation. contrast roughly assume model updated efﬁciently data require models trained permutations data sufﬁciently similar exactly same. note estimate depends speciﬁc partitioning data calculated. reduce variance different partitionings k-cv score averaged multiple random partitionings. lssvms propose method efﬁciently compute score multiple partitionings resulting total running time number different partitionings number data points test set. case possible partitionings dataset used complete score obtained. mullin sukthankar study efﬁcient computation nearest-neighbor-based methods; method runs time dataset data point consists input outcome given sets example might {+−} binary classiﬁcation regression; unsupervised learning singleton {nolabel}. deﬁne model function that given input makes prediction given note prediction need outcome particularly unsupervised learning tasks. quality prediction assessed performance measure shown algorithm receives indices model fs..e trained chunks except normalized performance scores corresponding testing ˆfi..i model trained chunk treecv divides hold-out chunks groups scores groups separately recursively calling itself. precisely treecv ﬁrst updates model training second group chunks resulting model ˆfs..m makes recursive call then repeats procedure group chunks starting original model ˆfs..e received updates model time using ﬁrst group remaining chunks previously held recursion stops hold-out chunk case performance score model ˆfs..s directly calculated returned. calling treecv calculates ˆrk-cv figure shows example recursive call tree underlying algorithm calculating loocv estimate dataset four data points. note tree structure imposes order feeding chunks learning algorithm e.g. learned ﬁrst branch tree. accuracy treecv simplify analysis section next assume chunk size date dataset fraction cost training model whole data scratch. formally models deﬁne possible datasets possible sizes. disregarding computation incremental learning algorithm mapping that given model dataset returns updated model capture often needed internal states allow padding models extra information necessary still viewing models maps convenient. above usually result previous invocation another particular learns model dataset scratch using dataset important class incremental algorithms online algorithms update model data point time update algorithms make consecutive calls call updates latest model next remaining data point according random ordering points rest paper consider incremental learning algorithm ﬁxed given partitioning dataset subsets denote model learned chunks except thus k-cv estimate generalization performance denoted rk-cv given recursive cross-validation algorithm builds observation every training sets almost identical except chunks held testing other. naive k-cv calculation method ignores fact potentially wasting computational resources. using incremental learning algorithm able exploit redundancy ﬁrst learn model examples shared training sets increment differences different copies model learned. extra cost saving restoring model required approach comparable learning model scratch approach result considerable speedup. integer note models ˆfs..s used computing learned incrementally. learning algorithm learns model matter whether given chunks gradually ˆfs..s model used deﬁnition rk-cv rk-cv ˆrk-cv. assumption hold ˆrk-cv still close rk-cv long models ˆfs..s sufﬁciently similar corresponding models rest section formalize assertion. first deﬁne notion stability incremental learning algorithm. intuitively incremental learning algorithm stable performance models nearly matter whether learned incrementally batch. formally suppose dataset partitioned nonempty chunks ztest using ztest test data ztrain chunks ztrain training data. batch denote model learned training data provided time denote model provided incrementally |ztest| model test data ztest. deﬁnition algorithm gincrementally stable function dataset partition ztest ztrain |ztest| test performance models batch proof. prove ﬁrst statement only proof second part essentially identical. recall denote chunks used cross-validation. ztest ztrain denote union chunks used training depth recursion branch ending computation ˆri. then deﬁnition rtest rtest. therefore statement follows since ˆrk-cv rk-cv deﬁned averages respectively. theorem suppose data drawn independently distribution drawn independently data minf∈m denote model minimum expected loss. assume exist upper bounds mbatch minc excess risks batch trained data points mbatch) minc) particular online learning algorithms satisfying regret bound standard online-to-batch conversion results yield excess-risk bounds independent identically distributed data. similarly excess-risk bounds often available stochastic gradient descent algorithms scan data online learning algorithms batch version usually deﬁned running algorithm using random ordering data points sampling data points replacement. typically version also satisﬁes excess-risk bounds. thus previous theorem shows algorithms incrementally stable excess-risk bound samples. note incremental stability w.r.t. loss function whose excess-risk bounded. example visiting data points regret pegasos bounded features bounded using online-to-batch conversion kakade tewari gives excess risk bound o/n) hence pegasos stable w.r.t. regularized hinge loss o/n). similarly compact bounded features bounded convex loss stable w.r.t. convex loss experiments algorithms shown section finally note algorithms like pegasos could also used scan data multiple times. cases algorithms would useful incremental algorithms clear data point without major retraining previous points. currently method apply cases straightforward way. complexity analysis section analyze running time storage requirements treecv discuss practical issues concerning implementation including parallelization. memory requirements efﬁcient storage updates model crucial efﬁciency algorithm indeed call treecv correspond simply evaluating model chunk data treecv update original model ˆfs..e twice this treecv either store ˆfs..e revert ˆfs..e ˆfs..m. general type model model ˆfs..e modiﬁed in-place need create copy updated model ˆfs..m alternatively keep track changes made model update. whether copying save/revert strategy depends application learning algorithm. example model state compact copying useful strategy whereas model undergoes changes update save/revert might preferred. compared single learning algorithm treecv requires extra storage saving restoring models trains along way. parallelization used implementing treecv exactly branch every point execution algorithm. since largest height recursion branch model saved level branch total storage required treecv o)-times storage needed single model. treecv easily parallelized dedicating thread computation data groups used updating ˆfs..e call treecv. case typically needs copy model since threads needed able independently other; thus total number models treecv needs store since total nodes recursive call tree exactly model stored node. note standard parallelized calculation also needs store models. finally note treecv potentially useful distributed environment chunk data stored different node network. updating model given chunk relegated computing node model data needs communicated nodes. since every level tree chunk added exactly model total communication cost running time next analyze time complexity treecv calculating k-cv score dataset size previous simplifying assumption integer running time treecv analyzed terms running time learning algorithm time takes copy models throughout subsection following deﬁnitions notations denotes time required test model chunks treecv number chunks held ˆfs..e already trained data points. note total running time treecv calculate k-cv score dataset size accounts cost operations recursive function calls. analyze running time treecv following natural assumptions first assume slower data points provided batch rather one. assume updating model requires work comparable saving reverting changes made update. natural assumption since update procedure also writing changes. formally assume constant quick estimate running time assume moment idealized case since data points added models node level recursive call tree work required node n−jtu. nodes hence cumulative running time level nodes hence total running time algorithm denotes base- logarithm. running time single experiments section evaluate treecv compare standard calculation. consider incremental algorithms linear pegasos classiﬁcation least-square stochastic gradient descent linear least-squares regression squared loss parameter vectors constrained unit lball). following suggestions original papers take last hypothesis pegasos average hypothesis lsqsgd model. focus largedata regime algorithms learn data single pass. algorithms implemented python/cython numpy. tests single core computer intel xeon processor ram. used datasets repository downloaded libsvm website tested pegasos covertype dataset learning class rest classes. features scaled unit variance. regularization parameter following suggestion shalev-shwartz figure running time treecv standard k-cv different values function number data points averaged independent repetitions. pegasos; bottom least-square sgd. left column k-cv without permutations; middle column k-cv data permutation; right column loocv without permutations. naturally pegasos lsqsgd sensitive order data points provided vanilla implementation order data points ﬁxed advance whole computation. ﬁxed ordering chunks samples within chunk need train model chunks data points given training algorithm according hierarchical ordering. introduces certain dependence estimation procedure example model trained chunks visited data similar order trained eliminate dependence also implemented randomized version samples used training phase provided random order table shows values estimates computed different scenarios. observed standard method quite sensitive order points variance estimate really decay number folds increases expected decay randomized version. hand non-randomized version treecv show behavior automatic re-permutation happens during treecv might made folds less correlated. however randomizing order training points typically reduces variance treecv-estimate well. figure shows running times treecv standard method function pegasos lsqsgd ﬁrst columns show running times different values without randomizing order data points rightmost column shows running time loocv calculations. treecv outperforms standard method cases. notable treecv makes calculation loocv practical even fraction time required standard method example pegasos treecv takes around seconds computing loocv standard method takes around seconds furthermore variance reduction achieved randomizing data points comes price constant factor bigger running time comes fact training time time generating random perturbation linear number points conclusion presented general method treecv speed crossvalidation incremental learning algorithms. method applicable wide range supervised unsupervised learning settings. showed that mild conditions incremental learning algorithm used treecv computes accurate approximation k-cv estimate running time scales logarithmically running time standard method training separate models scales linearly experiments classiﬁcation regression using well-known incremental learning algorithms pegasos least-square conﬁrmed speedup predicted accuracy. model learned learning algorithm depends whether data provided incrementally batch estimate calculated method still close computed standard method lower variance. references venkatesh. fast crossvalidation algorithms least squares support vector machine kernel ridge regression. pattern recognition august c.-c. chang c.-j. lin. libsvm library support vector machines. transactions intelligent systems technology datasets available http//www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/. golub matt. generalized cross-validation large-scale problems. journal computational graphical statistics march golub heath wahba. generalized cross-validation method choosing good ridge parameter. technometrics izbicki. algebraic classiﬁers generic approach fast cross-validation online training parallel training. proceedings international conference machine learning pages jiang liao. efﬁcient approximation cross-validation kernel methods using bouligand inﬂuence function. proceedings international conference machine learning volume jmlr w&cp pages nguyen milanfar golub. efﬁcient generalized cross-validation applications parametric ieee transacimage restoration resolution enhancement. tions image processing september pahikkala boberg salakoski. fast n-fold cross-validation regularized least-squares. proceedings ninth scandinavian conference artiﬁcial intelligence", "year": 2015}