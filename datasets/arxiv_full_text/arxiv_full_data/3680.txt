{"title": "Variational Algorithms for Marginal MAP", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of \"mixed-product\" message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel \"argmax-product\" message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods.", "text": "marginal maximum posteriori probability estimation problem calculates mode marginal posterior distribution subset variables remaining variables marginalized important inference problem many models hidden variables uncertain parameters. unfortunately marginal np-hard even trees attracted less attention literature compared joint marginalization problems. derive general dual representation marginal naturally integrates marginalization maximization operations joint variational optimization problem making possible easily extend variational-based algorithms marginal map. particular derive mixed-product message passing algorithms marginal whose form hybrid max-product sum-product novel argmax-product message updates. also derive class convergent algorithms based proximal point methods including transforms marginal problem sequence standard marginalization problems. theoretically provide guarantees algorithms give globally locally optimal solutions provide novel upper bounds optimal objectives. empirically demonstrate algorithms signiﬁcantly outperform existing approaches including state-of-the-art algorithm based local search methods. keywords graphical models message passing belief propagation variational methods maximum posteriori marginal-map hidden variable models. graphical models bayesian networks markov random ﬁelds provide powerful framework reasoning conditional dependency structures many variables found wide application many areas including error correcting codes computer vision computational biology given graphical model estimated empirical data constructed domain expertise term inference refers generically answering probabilistic queries model computing marginal probabilities maximum posteriori estimates. although inference tasks np-hard worst case recent algorithmic advances including development variational methods family algorithms collectively called belief propagation provide approximate exact solutions problems many practical circumstances. work focus three common types inference tasks. ﬁrst involves maximization max-inference tasks sometimes called maximum posteriori probable explanation tasks look mode joint probability. second sum-inference tasks include calculating marginal probabilities normalization constant distribution finally main focus work marginal type mixed-inference problem seeks partial conﬁguration variables maximizes variables’ marginal probability remaining variables summed out. marginal plays essential role many practical scenarios exist hidden variables uncertain parameters. example marginal problem arise problem models hidden variables whose predictions interest robust optimization variant unknown noisily observed parameters marginalized w.r.t. prior distribution. also treated special case complicated frameworks stochastic programming decision networks three types inference tasks listed order increasing diﬃculty maxinference np-complete sum-inference p-complete mixed-inference npppcomplete practically speaking max-inference tasks host eﬃcient algorithms loopy max-product tree-reweighted dual decomposition suminference diﬃcult max-inference example models binary attractive pairwise potentials sum-inference p-complete maxinference tractable mixed-inference even much harder either maxsuminference problems alone marginal np-hard even tree structured graphs illustrated example fig. diﬃculty arises part operators commute causing feasible elimination orders much higher induced width summax-inference. viewed another marginalization step destroy dependency structure original graphical model making subsequent maximization step challenging. probably reasons much less work marginal joint marginalization despite importance many practical problems. practice common over-use simpler joint marginalization even marginal would appropriate. cause serious problems illustrate example empirical results section contributions. reformulate mixed-inference problem joint maximization problem free energy objective extends well-known log-partition function duality form making possible easily extend essentially arbitrary variational algorithms marginal map. particular propose novel mixed-product algorithm hybrid max-product sum-product special argmax-product message updates well convergent proximal point algorithm works iteratively solving pure marginalization tasks. also present junction graph variants algorithms work models higher order cliques. also discuss mean ﬁeld methods highlight connection expectation-maximization algorithm. give theoretical guarantees global local optimality algorithms cases variables form tree structured subgraphs. numerical experiments show methods provide signiﬁcantly better solutions existing algorithms including similar hybrid message passing algorithm jiang state-ofthe-art algorithm based local search methods. preliminary version work appeared ihler forward approach marginal viewing nodes hidden variables nodes parameters estimated; however prone getting stuck sub-optimal conﬁgurations. classical state-of-the-art approaches include local search methods markov chain monte carlo methods variational elimination based methods jiang recently proposed hybrid message passing algorithm similar form mixed-product algorithm without theoretical guarantees; show section jiang viewed approximation marginal problem exchanges order operators. another message-passing-style algorithm proposed recently altarelli general multi-stage stochastic optimization problems based survey propagation optimality guarantees relatively complicated form. finally ibrahimi introduces robust max-product belief propagation solving related worst-case robust optimization problem hidden variables minimized instead marginalized. best knowledge work ﬁrst general variational framework marginal provides ﬁrst strong optimality guarantees. begin section introducing background graphical models variational inference. introduce novel variational dual representation marginal section propose analogues bethe tree-reweighted approximations section class mixed-product message passing algorithms proposed analyzed section convergent alternatives proposed section based proximal point methods. discuss algorithm connection framework section extend algorithms junction graphs section finally present numerical results section conclude paper section since discrete functions tables; alternatively viewing vector interpreted natural parameter overcomplete exponential family representation. joint vector joint potential function maps factorization structure represented undirected graph node maps variable edge corresponds variables coappear factor function cliques purpose illustration mainly restrict scope pairwise models nodes edges i.e. however show extend algorithms unfortunately problem generally p-complete straightforward calculation requires summing exponential number terms. variational methods class approximation algorithms transform marginalization problem continuous optimization problem typically solved approximately. marginal polytope. marginal polytope concept variational inference. deﬁne marginal polytope local marginal probabilities extensible valid joint distribution i.e. dual form transforms marginalization problem continuous optimization make easier marginal polytope deﬁned exponential number linear constraints entropy term objective function diﬃcult calculate log-partition function. however provides framework deriving eﬃcient approximate inference algorithms approximating marginal polytope entropy bp-like methods. many approximation methods replace locally consistent polytope pairwise models singleton pairwise pseduo-marginals {τij consistent intersections i.e. free energy remains intractable typically approximate free energy combination singleton pairwise entropies requires knowing τij. example bethe free energy approximation sometimes abbreviate convenience. well-known loopy belief propagation algorithm pearl interpreted ﬁxed point algorithm optimize bethe free energy locally consistent polytope unfortunately bethe free energy non-concave function weighted collection spanning trees section detailed deﬁnition). approximation convex optimization problem guaranteed give upper bound true log-partition function. message passing algorithm similar loopy called tree reweighted derived ﬁxed point algorithm solving convex optimization mean-ﬁeld-based methods. mean-ﬁeld-based methods another approximate inference algorithms work restricting tractable distributions marginal polytope joint entropy tractable. precisely subset corresponds tractable distributions e.g. fully τi}. note joint entropy decomposes singleton entropies marginal guaranteed give lower bound log-partition function. unfortunately mean ﬁeld methods usually lead non-convex optimization problems often non-convex set. practice block coordinate descent methods adopted local optima attains maximum kronecker delta problem remains np-hard marginal polytope includes exponentially many inequality constraints. variational methods interpreted relaxing locally consistent polytop yielding linear relaxation original integer programming problem. note diﬀers lack entropy term; next section generalize similarity marginal map. figure example koller friedman marginal query tree requires exponential time complexity. marginalization destroys conditional dependency structure marginal distribution causing intractable maximization problem exact variable elimination method sequentially marginalizes nodes maximizes nodes time complexity length chain. nodes complement marginal problem seeks partial conﬁguration nodes marginalized nodes optimized. call type mixed-inference problem since involves type variable elimination operator. facilitate developing duality results formulate marginal terms exponential family representation marginal solution. although similar maxsum-inference marginal signiﬁcantly harder either them. classic example shown fig. marginal np-hard even tree structured graph main diﬃculty arises operators commute restricts feasible elimination orders nodes eliminated nodes. worst case marginalizing nodes destroy conditional independence among nodes making diﬃcult represent optimize even part alone tractable practical scenarios. marginal conﬁguration sense minimizes expected error here variables included error criterion example nuisance hidden variables direct interest unobserved inaccurately measured model parameters. contrast joint practice perhaps wide availability eﬃcient algorithms joint researchers tend over-use joint even cases marginal would appropriate. following example shows seemingly reasonable approach sometimes cause serious problems. example denote {rainy sunny} weather condition irvine {walk drive} whether alice drives walks school depending weather condition. assume probabilities task calculate likely weather condition irvine obviously sunny according marginal maxxb sunny gives correct answer. however full estimator gives answer rainy obviously wrong. paradoxically changed solution section present main result dual representation marginal problem dual representation generalizes sum-inference maxinference provides uniﬁed framework solving marginal problems. table primal dual forms three inference types. dual forms suminference max-inference well known; form marginal contribution work. intuitively operators primal form determine conditioning conditional entropy term dual form. remark theorem naturally integrates marginalization maximization subproblems joint optimization problem providing novel eﬃcient treatment marginal beyond traditional approaches treat marginalization subproblem sub-routine maximization problem. show section enables derive eﬃcient mixed-product message passing algorithms simultaneously takes marginalization maximization steps avoiding expensive possibly wasteful inner loop steps marginalization sub-routine. inference max-inference since reduces forms empty nodes respectively. table shows three forms together comparision. intuitively since entropy removed objective optimal marginal tends lower entropy probability mass concentrates optimal alternatively interpreted marginals obtained conﬁgurations clamping value remark unfortunately subtracting term causes subtle diﬃculties. first intractable calculate even joint necessarily inherit conditional dependency structure joint distribution. therefore dual optimization intractable even tree reﬂecting intrinsic diﬃculty marginal compared full marginalization. interestingly show sequel certiﬁcate optimality still obtained general tree graphs cases. secondly conditional entropy ha|b concave strictly concave respect creates additional diﬃculty optimizing since many iterative optimization algorithms coordinate descent lose typical convergence optimality guarantees objective function strongly convex. small positive constant. similar smoothing techniques also applied solve standard problem; e.g. hazan shashua meshi show following theorem smoothed dual approximation closely connected direct approximation primal domain. theorem transforms marginal problem variational form obviously decrease computational hardness. fortunately many well-established variational techniques summax-inference extended apply opening door deriving novel approximate algorithms marginal map. spirit wainwright jordan either relax simpler outer bound like replace fmix tractable form give algorithms similar loopy restrict tractable subset like give mean-ﬁeld-like algorithms. sequel demonstrate several approximation schemes mainly focusing bp-like methods pairwise free energies. brieﬂy discuss mean-ﬁeld-like methods connect section derive extension junction graphs exploits higher order approximations section framework easily adopted take advantage other advanced variational techniques like using higher order cliques advanced optimization methods like dual decomposition alternating direction method multipliers fbethe truncated bethe free energy whose entropy mutual information terms involve nodes truncated. tree φbethe equals true giving intuitive justiﬁcation. sequel give general theoretical conditions approximation gives exact solution empirically usually gives surprisingly good solutions practice. similar regular bethe approximation leads nonconvex optimization derive message passing algorithms provably convergent algorithms solve since outer bound ftrw concave upper bound true free energy guarantee φtrw always upper bound φab. knowledge provides ﬁrst known convex relaxation upper bounding marginal map. also optimize weights tab} tightest upper bound using methods show global optimality guarantees approximations circumstances. section always assume tree hence objective function tractable calculate given however optimization component remains intractable case marginalization step destroys decomposition structure objective function thus nontrivial bethe approximations behave case. weights part {ρij ﬁxed ones. crossing edges {ρij ∂ab} take arbitrary values corresponding diﬀerent free energy approximation methods. bethe free energy; correspond free energy {ρij} taken edge proof fact part tree guarantees marginalization exact. showing relaxation maximization problem applying standard relaxation arguments completes proof. remark. theorem works arbitrary values suggests fundamental tradeoﬀ hardness takes diﬀerent values. hand value controls concavity objective function hence diﬃculty ﬁnding global optimum; small enough ensure convex optimization larger causes become non-convex making diﬃcult apply thoerem hand value also controls likely solution integral larger emphasizes mutual information terms forcing solution towards integral points. thus solution free energy less likely integral bethe free energy causing diﬃculty applying theorem section reﬂect extrema tradeoﬀ concavity integrality respectively bethe approximation appears represent reasonable compromise often gives excellent performance practice. section give diﬀerent local optimality guarantees derived reparameterization perspective. derive message-passing-style algorithms optimize truncated bethe free energies instead optimizing truncated free energies directly leverage results theorem consider annealed versions general framework provides uniﬁed treatment approximating sum-inference max-inference mixed marginal problems simply taking diﬀerent weights. speciﬁcally algorithm hybrid max-product sum-product message updates novel argmax-product message update speciﬁc marginal problems. algorithm listed algorithm described following proposition algorithm intuitive interpretation sum-product max-product messages correspond marginalization maximization steps respectively. special argmax-product messages serves synchronize sum-product max-product messages restricts nodes currently decoded local marginal ψim∼i passes posterior beliefs back single critical feature mixed-product takes simultaneous movements marginalization maximization sub-problems parallel fashion computationally much eﬃcient traditional methods require fully solving marginalization sub-problem taking maximization step. advantage inherited general variational framework naturally integrates marginalization maximization sub-problems joint optimization problem. interestingly algorithm also bears similarity recent hybrid message passing method jiang diﬀers algorithm replacing special argmax-product messages regular max-product messages. make detailed comparison algorithms section show fact argmaxproduct messages lends algorithm several appealing optimality guarantees. important interpretation sum-product max-product reparameterization viewpoint message passing updates viewed moving probability mass local pseudo-marginals leaves product reparameterization original distribution ensuring consistency conditions ﬁxed points. viewpoints theoretically important useful proving optimality guarantees algorithms. section show mixed-product algorithm similar reparameterization interpretation based establish local optimality guarantee mixed-product mi→jmj→i maxxi bi∀i marginal solution decoded typical max-product note mixed-beliefs bij} diﬀerent local marginals τij} deﬁned rather softened versions τij}.their three mixed-consistency constraints exactly three types message updates algorithm constraint enforces regular summaxconsistency summaxproduct messages respectively. constraint corresponds argmax-product message update enforces marginals consistent assigned currently decoded solution corresponding solving local marginal maxxi maxxi problem bij. turns special constraint crucial ingredient mixed-product enabling prove guarantees strong local optimality solution. notation required. suppose subset nodes gc∪a subgraph induced nodes ec∪a call gc∪a semi-a-b subtree edges ec∪a\\eb form tree. words gc∪a semi-a-b tree tree ignoring edges entirely within fig. examples semi trees. following weiss weights {ρij} provably convex i∈∂i κi→i κi→j +κj→i figure examples semi trees. shaded nodes represent nodes unshaded nodes. graph semi tree labeled bold lines. conditions theorem ﬁxed point mixed-product locally optimal jointly perturbing nodes semi-a-b subtree proof mixed-consistency constraint fact gc∪a semi-a-b tree enables summation part eliminated away. remaining part involves nodes method weiss analyzing standard applied. remark. proof theorem relies transforming marginal problem standard problem eliminating summation part. therefore variants theorem derived using global optimality conditions convexiﬁed belief propagation linear programming algorithms werner wainwright leave future work. gc∪a semi tree part must tree theorem assumes implicitly. hidden markov chain fig. theorem implies local optimality hamming distance semi subtree fig. contain node. however theorem general much stronger especially part fully connected part interior regions disconnected part. examples fig. jiang proposed similar hybrid message passing algorithm repeated algorithm diﬀers mixed-product replacing argmaxproduct message update usual max-product message update show section diﬀerence gives algorithm diﬀerent properties fewer optimality guarantees mixed-product despite striking similarity algorithm diﬀerent properties share appealing variational interpretation optimality guarantees demonstrated mixed-product first unclear whether algorithm interpreted ﬁxed point algorithm maximizing similar variational objective function. second inherit optimality guarantees theorem despite similar reparameterization consistency conditions. disadvantages caused miss special argmax-product message update associated mixedconsistency condition critical ingredient proof theorem detailed insights algorithm mixed-product obtained considering special case full graph undirected tree. show case algorithm viewed optimizing approximate objective functions obtained rearranging operators orders require less computational cost mixed-product attempts maximize exact objective function message updates eﬀectively perform asynchronous coordinate descent steps. sequel illustrative example explain main ideas. here algorithm approximates exact marginal problem rearranging operators elimination order makes calculation easier. similar property holds general case undirected tree algorithm terminates ﬁnite number steps output solution eﬀectively maximizes approximate objective function obtained reordering operators along tree-order rooted node performance algorithm related error caused exchanging order operators. however exact optimality guarantees likely diﬃcult show maxuses diﬀerent order arrangement hence maximizes diﬀerent surrogate objective function given algorithm maximizes algorithm hand mixed-product belief propagation algorithm terminate ﬁnite number steps necessarily yield closed form solution undirected tree. however algorithm proceeds attempt optimize exact objective function. example show true solution guaranteed ﬁxed point algorithm mixed-belief current iteration maxx unique maxima. message sequence passed show update coordinate descent step monotonically improves true objective function towards local maximum. general models algorithm diﬀers sequential coordinate descent guarantee monotonic convergence. viewed parallel version coordinate descent ensures stronger local optimality guarantees shown theorem obvious disadvantage mixed-product lack convergence guarantees even undirected tree. section apply proximal point approach derive convergent algorithms directly optimize free energy objectives take form transforming marginal sequence pure sum-inference tasks. similar methods applied standard sum-inference max-inference solution iteration positive coeﬃcient. here distance called proximal function forces close typical choices euclidean bregman distances ψ-divergences guaranteed non-increasing iteration converges optimal solution regularity conditions. e.g. rockafellar tseng bertsekas iusem teboulle proximal algorithm closely related majorize-minimize algorithm convex-concave procedure case proximal point algorithm reduces algorithm iteratively solves smoothed free energy objective natural parameter updated iteration. intuitively proximal inner loop essentially adds back truncated entropy term canceling eﬀect adjusting opposite direction. typical choices include note proximal approach distinct annealing method would require annealing interestingly take inner maximization coeﬃcient vanish zero. problem reduces standard log-partition function duality corresponding pure marginalization task. interpretation transforming marginal problem sequence standard sum-inference problems. practice approximate ha|b pairwise entropy decomposition ha|b respectively. provably convex sense weiss k∈∂i κk→i κi→j κj→i resulting approximate algorithm case algorithm still valid proximal algorithm inherits convergence guarantees. practice uses approximations provably convex. interesting special case ha|b approximated bethe approximation. eﬀect optimization solved using standard belief propagation. although bethe form ha|b provably convex special cases tree structured practice approximation gives accurate solutions even general loopy graphs convergence longer theoretically guaranteed. global convergence guarantees proximal point algorithm also fail also possible develop globally inner update solved exactly. convergent algorithms without inner loops using techniques developed full marginalization problems leave future work. natural algorithm solving marginal problem expectationmaximization algorithm treating hidden variables parameters maximized. section show algorithm seen coordinate ascent algorithm mean ﬁeld variant framework. marginal polytopes respectively. note step happen dual sum-inference max-inference problem respectively. back primal update primal conﬁguration instead rewritten exactly update viewing parameters hidden variables. similar connections coordinate ascent method variational objectives discussed neal hinton wainwright jordan e-step m-step intractable insert various approximations. particular approximating mean-ﬁeld inner bound leads variational interesting observation obtained using bethe approximation solve e-step linear relaxation solve m-step; case em-like update equivalent solving discussed section represents extreme tradeoﬀ convexity integrality implied theorem strongly encourages vertex solutions sacriﬁcing convexity hence likely become stuck local optima. above restricted discussion pairwise models pairwise entropy approximations mainly purpose clarity. section extend algorithms leverage higher order cliques based junction graph representation higher order methods like generalized convex variants derived similarly. non-overlapping partition nodes satisfying words represents assignment node cluster clusters call max-clusters; correspondingly call sum-clusters. fig. example. similarly mixed-product algorithm algorithm also admits intuitive reparameterization interpretation strong local optimality guarantee. algorithm seen special case general junction graph algorithm derived ihler solving maximum expected utility tasks decision networks. details refer reader work. illustrate algorithms simulated models realistic diagnostic bayesian networks taken inference challenge. show bethe approximation algorithms perform best among tested algorithms including jiang hybrid message passing state-of-the-art local search algorithm implement mixed-product algorithm bethe weights regular sum-product max-product jiang hybrid message passing algorithm solutions extracted maximizing singleton marginals nodes. algorithms maximum iterations; case fail converge additional iterations damping coeﬃcient initialize algorithms random initializations pick best solution; mix-product jiang’s method additional trial initialized using sum-product messages reported perform well park darwiche jiang also proximal point version mixed-product bethe weights algorithm ha|b approximated bethe approximations. also implement approximation using convergent proximal point algorithm upper bounds valid algorithms converge. weights ˆha|b constructed ﬁrst selecting spanning trees augmenting spanning tree uniformly selected edge ∂ab; weights constructed provably convex using method trw-s kolmogorov proximal point algorithms maximum iterations maximum iterations weighted message passing updates inner loops addition compare algorithms samiam state-of-the-art implementation local search algorithm marginal default taboo search method maximum searching steps report best results among trials random initializations additional trial initialized also implement algorithm whose expectation maximization steps approximated sum-product max-product respectively. random initializations initialization sum-product marginals pick best solution. latent tree models. generate random trees size ﬁnding minimum spanning trees random symmetric matrices elements drawn uniform). take leaf nodes nodes non-leaf nodes nodes. fig. typical example. results latent tree models types grids shown fig. fig. fig. respectively. since globally optimal solution tractable calculate cases report approximate relative error deﬁned best solution found across algorithms. diagnostic bayesian networks. also test algorithms diagnostic bayesian networks taken inference challenge construct marginal problems randomly selecting varying percentages nodes nodes. since models pairwise implement junction graph versions mix-product proximal shown section fig. shows approximate relative errors algorithms local search percentage nodes varies. insights. across experiments mix-product proximal local search signiﬁcantly outperform algorithms proximal outperforms others circumstances. hidden markov chain example fig. three algorithms almost always number nodes large make diﬃcult explore solution space local search. hand mix-product tends degenerate coupling strength increases probably convergence gets worse increases. note approximation gives much less accurate solutions algorithms able provide upper bound optimal energy. similar phenomena observed trw-bp standard maxsuminference. hybrid message passing jiang signiﬁcantly worse mix-product proximal local search otherwise best among remaining algorithms. performs similarly jiang’s method. regular max-product sum-product among worst tested algorithms indicating danger approximating mixed-inference pure maxsuminference. interestingly performances max-product sum-product opposite trends fig. fig. fig. parts fully disconnected parts connected loopy max-product usually performs worse sum-product gets better coupling strength increases; sum-product hand tends degenerate increases. fig. pattern reversed max-product performs better sum-product figure results hidden markov chain fig. diﬀerent algorithms’ probabilities obtaining globally optimal solution among random trials. mix-product proximal local search figure typical latent tree model whose leaf nodes taken nodes non-leaf nodes nodes approximate relative energy errors diﬀerent algorithms upper bound obtained proximal function coupling strength presented general variational framework solving marginal problems approximately opening doors developing eﬃcient algorithms. particular show proposed mixed-product admits appealing theoretical properties performs well practice. potential future directions include improving performance truncated approximation optimizing weights deriving optimality conditions applicable even component form tree studying convergent properties mixed-product leveraging results learn hidden variable models data. nodes unshaded nodes; note part loopy graph part fully disconnected. approximate relative errors diﬀerent algorithms upper bound obtained proximal function coupling strength part exactly opposite fig. note part loopy part fully disconnected case. approximate relative errors diﬀerent algorithms upper bound obtained proximal function coupling strength figure results diagnostic bayesian networks inference challenge. diagnostic network. performances algorithms function percentage nodes. local search method tends degenerate number nodes large making diﬃcult search solution space. results averaged random trials.", "year": 2013}