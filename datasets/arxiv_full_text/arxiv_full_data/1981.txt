{"title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA  for the Design of Training Graphs", "tag": ["cs.AI", "cs.CV", "stat.ML"], "abstract": "Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a time series. Graph-based SFA (GSFA) is a supervised extension that can solve regression problems if followed by a post-processing regression algorithm. A training graph specifies arbitrary connections between the training samples. The connections in current graphs, however, only depend on the rank of the involved labels. Exploiting the exact label values makes further improvements in estimation accuracy possible.  In this article, we propose the exact label learning (ELL) method to create a graph that codes the desired label explicitly, so that GSFA is able to extract a normalized version of it directly. The ELL method is used for three tasks: (1) We estimate gender from artificial images of human faces (regression) and show the advantage of coding additional labels, particularly skin color. (2) We analyze two existing graphs for regression. (3) We extract compact discriminative features to classify traffic sign images. When the number of output features is limited, a higher classification rate is obtained compared to a graph equivalent to nonlinear Fisher discriminant analysis. The method is versatile, directly supports multiple labels, and provides higher accuracy compared to current graphs for the problems considered.", "text": "slow feature analysis unsupervised learning algorithm extracts slowly varying features time series. graph-based supervised extension solve regression problems followed post-processing regression algorithm. training graph speciﬁes arbitrary connections training samples. connections current graphs however depend rank involved labels. exploiting exact label values makes improvements estimation accuracy possible. article propose exact label learning method create graph codes desired label explicitly gsfa able extract normalized version directly. method used three tasks estimate gender artiﬁcial images human faces show advantage coding additional labels particularly skin color. analyze existing graphs regression. extract compact discriminative features classify traﬃc sign images. number output features limited higher classiﬁcation rate obtained compared graph equivalent nonlinear fisher discriminant analysis. method versatile directly supports multiple labels provides higher accuracy compared current graphs problems considered. keywords slow feature analysis nonlinear regression image analysis pattern recognition many classes slowness principle learning paradigms might explain least part self-organization neurons brain extract invariant representations relevant features. principle operates abstract level postulates relevant abstract information extracted environment typically changes much slower individual sensory inputs slowness principle probably ﬁrst formulated hinton online learning rules developed shortly f¨oldi´ak mitchison ﬁrst closed-form algorithm referred slow feature analysis recently extension supervised learning called graph-based proposed. contrast trained sequence samples gsfa trained so-called training graph vertices samples edge weights represent similarities corresponding labels. slowness requires minimization squared output diﬀerences consecutive pairs samples whereas gsfa pairs samples consecutive weighted deﬁned training graph. gsfa used explicitly exploit available labels establishing output similarity objective involving arbitrary samples. typically gsfa eﬀective extracting features tend concentrate label information allow accurate prediction labels implicitly solving supervised learning problem. although gsfa locality preserving projections originate diﬀerent backgrounds ﬁrst motivated diﬀerent goals applications mind close relation them sharing similar objective functions constraints. diﬀerences gsfa vertex weights independent edge weights gsfa invariant scale weights providing normalized objective function. possible gsfa compute features vice versa. results article might thus also interest community. real life many supervised learning problems solved applying feature extraction dimensionality reduction explicit supervised step another approach based gsfa ﬁrst uses gsfa supervised post-processes small number slow features conventional classiﬁcation regression algorithm. supervised might result higher accuracy unsupervised supervised learning problem mostly solved gsfa implicitly frequently concentrates label-predictive information features. therefore post-processing step crucial might simple mapping slow features label domain. however construction pre-deﬁned graphs takes account rank labels exact value simpliﬁcation might decrease estimation accuracy. article focus analysis design training graphs. explore approach solving regression problems gsfa based construction special training graph slowest feature extracted already label estimation linear scaling develop exact label learning method ﬁrst study slowest possible features extracted gsfa given graph feature space unrestricted. features also called optimal free responses computed continuous time wiskott using variational calculus. gsfa diﬀerent method based linear algebra cope discrete nature index takes place time. figure three approaches solving supervised learning problems. traditional common approach. previous approach using gsfa pre-deﬁned training graph deﬁned input samples node weights edge weights samples assumed ordered increasing label. approach proposed here consists single gsfa architecture trained specially constructed graph ﬁrst slow feature directly provides label estimation. label weighted zero mean weighted unit variance ﬁnal linear scaling included. corresponding training graph. method allows creation graph slowest possible feature label learned. moreover learn multiple labels simultaneously balance importance. property exploited learn auxiliary labels provide redundant coding original labels increase estimation accuracy. case later explained possible desirable emphasize importance original labels auxiliary ones. cascaded gsfa refers consecutive application multiple passes gsfa. advantage direct gsfa joint feature space complex. hierarchical gsfa similarly hierarchical divide-and-conquer approach extraction slow features high-dimensional data. hgsfa oﬀers excellent computational complexity compared direct gsfa good linear w.r.t number samples input dimensionality depending network architecture. graph designed proposed method used train gsfa cascaded gsfa hgsfa thus also applicable high-dimensional data. although method based theory mostly contributes deeper understanding gsfa also used practice providing higher accuracy predeﬁned graphs. general method results higher complexity compared gsfa trained eﬃcient pre-deﬁned graph still computationally viable datasets without resorting specialized hardware parallelization. next section shortly review gsfa. section propose method. section provide three applications. firstly solve regression problem gender estimation artiﬁcial images validating method. secondly analyze eﬃcient predeﬁned training graphs regression. thirdly method diﬀerent design training graph extraction compact features classiﬁcation yielding {x}n vertices vertex sample edges pairs samples index replaces time variable used sfa. edges directed typically symmetric weights {γnn}nn; weights associated training graphs classiﬁcation typically favor connections samples class means larger edge weights compared diﬀerent classes whereas training graphs regression favor connections samples similar labels. concept slowness generalized sequences samples training graphs. general goal extract features fulﬁll certain normalization restrictions minimize weighted squared output diﬀerences connected samples. formally gsfa optimization problem figure example training graph vertices. regular sample sequence used train sfa. sequence represented linear graph used gsfa. labels available samples reordered increasing/decreasing label called sample reordering graph. slowest second slowest constraints called weighted zero mean weighted unit variance weighted decorrelation respectively. similar normalization constraints except inclusion vertex weights. factors essential optimization problem provide invariance scale edge weights well scale vertex weights serve normalization purpose. gsfa yields features standard trained sequence generated using graph markov chain transition probabilities γnn/r thus emulate gsfa. however depending training graph used emulating gsfa computationally expensive. clustered graph described figure generates features useful classiﬁcation. optimization problem associated graph explicitly demands samples class typically mapped similar outputs. inter-group intra-group connections appear. notice since vertices group adjacent exactly neighbors likely mapped similar outputs gsfa. following gsfa complementary explicit regression step features solves original regression problem. several eﬃcient graphs regression besides serial graph. employ serial graph article consistently given good results various regression problems. graph vertices ordered grouped according discrete label. even though original label samples might diﬀer grouped together discrete label. represents sample edges represent connections ovals represent groups samples. samples groups discrete labels weight whereas samples extreme groups labels weight weight edges order apply linear algebra methods analyze gsfa matrix notation. follows assume edge weights symmetric consistency restriction fulﬁlled. restriction also written lagrange multiplier method critical points candidates optimal free responses. moment ignore weighted decorrelation constraint solve ﬁrst optimal free response consider remaining responses later. close relationship gsfa approach strongly related laplacian eigenmaps eigendef= q/diaguj weighted normalization vector gives rise critical point conditions also satisﬁed slowest possible solution critical point smallest ∆-value. show below ∆-value critical point directly related eigenvalue eigenvector q−/diagyj special case eigenvalue multiplicities. means optimal free responses rotation delta value. case optimal free responses unique rotation equivalent. training graph slowest feature could extracted gsfa equal normalized version label. notice problem diﬀers considerably original gsfa problem ﬁnding optimal solution given training graph feature space. approach extended multiple labels vertex-weights indicate priori likelihood information samples thus assumed given strictly positive. information absent vertex weights constant e.g. normalization constraints outputs generated gsfa must weighted zero mean weighted unit variance therefore learn weight-normalized label follows diag weighted label variance. then normalized label comlarger eigenvalues might result higher accuracy corresponding label. give intuition choose eigenvalues eigenvectors. general important labels larger eigenvalues less important ones. global scale eigenvalues irrelevant relative example wants learn single label delta value q−/diag eigenvalues zero. takes possible values resulting graph disconnected contain clusters. otherwise resulting graph connected condition necessarily implies resulting edge weights negative condition deal section q−/v/ eigenvalue γell free pseudo-response fulﬁlls equations therefore feasible solution similar properties optimal free responses. introduction uell reduce generality labels equivalent weighted zero mean learned; orthogonality uell expected weighted value noise feature noise feature randomly sampled zero-mean unit-variance distribution i.e. average fulﬁlls normalization conditions seen objective function obvious positive edge weight connecting samples expresses samples mapped close feature space. contrast negative edge weights express samples mapped apart possible thus encoding output dissimilarities. nevertheless weighted unit variance constraint still applies solutions unbounded. edge weights non-negative smallest possible value however negative edge weights allowed feasible features might feature would appear slower infeasible constant feature contradicting intuitive interpretation slowness. moreover negative edge section additional step method ensure training graph non-negative edge weights. concretely show transform training graph strictly positive vertex weights arbitrary edge weights graph vertex weights non-negative edge weights optimization problem deﬁned equivalent original optimization problem therefore objective function modiﬁed positive scaling factor constant positive oﬀset proving optimal free solutions training graph remain stable well order. gsfa applied repeatedly provide additional auxiliary labels derived original improve estimation accuracy. informally even though given gsfa node might able extract accurately might capable approximating features fk). since features derived label contain information determines hence posterior gsfa node might able disentangle features eﬀectively recover original label. explicitly promote appearance max) largest label value min) smallest one. notice argument cosine function ranges etc. sense features higher-frequency versions eigenvalues corresponding auxiliary labels must smaller original label. otherwise slowest features found might close auxiliary labels rather original one. term target labels refer original auxiliary labels present. interestingly regular inclusion auxiliary labels occurs automatically. slowest free response half period cosine function subsequent free responses higher-frequency harmonics ﬁrst main drawback computational eﬃciency compared eﬃcient pre-deﬁned training graphs marked large analyse eﬃciency explicit label learning considering main parts construction training graph training gsfa transformation target labels eigenvectors might require decorrelation step -dimensional vectors. term computation involves vector multiplications ujut putation takes operations. secondly computation expressed taking operations. thirdly solution generalized eigenvalue problem requires operations. therefore general training gsfa requires operations. typically avoid overﬁtting computation expensive part. might computed eﬃciently using optimized algorithms. eﬃcient pre-deﬁned graph used computed operations small compute operations. therefore special cases training gsfa takes operations respectively. section present three applications proposed method. first illustrate solve regression problem gsfa explicitly learning direct mapping images labels second application analyse pre-deﬁned graphs computing optimal free responses. third application method learn compact discriminative labels classiﬁcation. consider problem gender estimation artiﬁcial face images treated regression problem gender parameter deﬁned real value face modelling software. using subject identity gender speciﬁed rest parameters faces random. average pixel intensity image normalized multiplying appropriate factor. resulting images show subjects ﬁxed pose hair accessories ﬁxed illumination constant average pixel intensity black background. figure sample images. diﬀerent besides gender label also consider second color label average pixel intensity image normalization. normalization label cannot computed directly estimated cues subject’s apparent race face size. following experiment consider gender label later labels simultaneously. network used. eﬃciency reasons hierarchical gsfa used. teste -layer hgsfa network structure described table nodes network non-overlapping receptive ﬁelds composed expansion function label estimations. used three mappings slowest features label estimation ﬁrst mapping linear scaling inverts label normalization since sign arbitrary methods ﬁnal label estimation clipped valid label range results. table shows label estimation errors gender label. depending mapping ellg- ellg- graphs outperform rest. supports intuition auxiliary labels useful. target labels perform worse probably part output dimensionality intermediate nodes network without ﬁnal clipping step clearly accurate linear scaling methods similar accuracy clipping enabled. graphs explicitly supervised soft method provided better accuracy linear scaling method although diﬀerence less might expected. table gender estimation errors using various graphs either three features. linear regression mapping label estimated ﬁtted training data. chance level uses constant estimation results test data averaged runs. error using training graphs gender estimation only. error using training graphs experiment simultaneous gender color estimation. comparison serial graph results rmses whereas reordering graph results rmses accuracy graphs appears similar; however complex experiments serial graph typically accurate ellg- graph therefore slightly accurate serial simultaneous learning gender color. construct graph codes gender color simultaneously learning labels derived gender label derived color label. labels computed similarly learning gender using diﬀerent original labels. linearly decreasing eigenvalues. resulting graphs denoted ellgc-l total number target labels number auxiliary table color estimation errors using various graphs either three features. chance level results test data averaged runs. error using training graphs code color. error using training graphs simultaneously code gender color. eﬀect coding gender color simultaneously gender estimation shown table right ellgc-l graphs yield higher accuracy ellg-l graphs. results color estimation using ellgc-l graphs shown table right slowest feature extracted represents mostly gender. however also contains color information allows color estimation better chance level. features preserved ellgc-l graphs yield higher accuracy ellc-l graphs. similar experimental results reported e.g. shown estimation improves gender race labels also considered. learning label transformations. verify method learn labels implicitly described data. precisely gsfa learn labels distorted versions original gender label graphs constructed purpose denoted ellg-) ellg-) respectively. include auxiliary labels besides main distorted label. better approximate target labels complex nonlinearities used nodes hierarchical networks. network identical network except node quadratic expansion used instead .expo expansion. similarly network uses quadratic expansion layer th-degree polynomial expansion node. networks output dimension node layer avoid overﬁtting expansion layer. corresponding label estimations shown figure comparison also ellg- graph included. results prove method also used learn distortions main label. admittedly accuracy estimations decreases even though increase complexity feature space. figure plots show label estimations test data diﬀerent distorted versions learned. linear scaling mapping used. therefore estimations generated slowest feature. ground-truth values shown thicker black. rmse expressed brackets percentage chance level. plot analogous shows training data. section method section extract optimal free responses three graphs optimal free responses values fully characterize properties training graph provide another representation might useful scenarios. compute optimal free responses using delta values using therefore results obtained analytically. plot figure shows arbitrary label learned three diﬀerent graphs used purpose. samples used ease visualization plots behave similarly larger employed graphs follows. reordering graph extended edge weights γn−n− fulﬁll consistency restriction required method. weights introduce constant scaling delta values without consequence. serial graph groups samples each. ell- graph constructed original labels auxiliary labels computed using ﬁgure shows remarkable diﬀerence graphs number optimal free responses reordering graph serial graph ell- graph parameters above. arbitrary parameters figure arbitrary label three graphs used learn slowest optimal free responses graph plotted well delta values optimal free responses. ell- graph almost fully connected strongest connections displayed. samples index free responses index free responses also plotted original label. polarity free responses adjusted make negative ﬁrst sample. although graphs diﬀer considerably connectivity ﬁrst four optimal free responses somewhat similar shape. since graphs slowest free response increasing monotonic mapping would enough approximate label them. however serial graph slowest response constant within group might lower accuracy discretization error. ell- graph tailored learn particular label therefore exactly except oﬀset scaling. analysis makes clear serial ell- graphs selective reordering graph regarding features consider slow. illustrate might advantage consider scaled noisy version concretely i.i.d. zero-mean unit-variance noise signal. reordering graph used feature average ∆-value therefore feature would appear faster auxiliary feature hence gsfa node trained reordering graph discriminant analysis according theory classes features deﬁne dimensional subspace best separates classes. practice typically uses features contain discriminative information graph however number classes large might become expensive preserve features node size input subsequent nodes would multiple dimensionality would increased expansion function resulting large training complexity. instance consider -node nonlinear network classiﬁcation gsfa nodes ﬁrst layer top. suppose ﬁrst nodes output dimensionality considerable computational memory costs. therefore could code class information ﬁrst layer compactly could reduce output dimensionality ﬁrst-layer nodes reduce overﬁtting aiming increasing classiﬁcation accuracy. section theory explicit learning multiple labels compute compact features classiﬁcation using gsfa. classify images traﬃc signs german traﬃc sign recognition benchmark database traﬃc signs samples. training data number samples class namely them making total images. reach samples class images classes used times images used training distorted purpose distortion improve generalization provide invariances small misalignments. oﬃcial test data ensures images originate physical signs diﬀerent ones used training. test data consists undistorted images. used simple gsfa architecture applied ﬁrst reduce dimensionality principal components. afterwards quadratic gsfa applied using diﬀerent training graphs described below. finally since classiﬁcation problem nearest centroid classiﬁer used instead linear scaling. used express equal importance target labels. compact+ graph included show eﬀect auxiliary labels graph ﬁrst eigenvalues identical rest decrease linearly three decimal places shown. thus importance given auxiliary labels decreases graphs scale eigenvalues make equal table target labels used code class information compactly expressed function class number compact+ graph constructed labels whereas compact+ graph ﬁrst labels seen original ones rest auxiliary. classiﬁcation error plotted figure number slow features given nearest centroid classiﬁer ranges comparison clustered graph also evaluated. features compact+ graph results best accuracy error rate however error rate compact+ graph increases preserves features indicating performance. case features extracted diﬀerent contain information since mapped linearly. words ﬁrst free responses graphs describe subspace. single slow feature compact+ graph contains ideally discriminative information contrast ﬁrst features extracted clustered graph might sacriﬁce discriminative information minimize within-class variance figure classiﬁcation error gsfa trained compact+ compact+ clustered graph error function graph number slow features passed classiﬁer. clustered graph dropping even single feature might increase error rate signiﬁcantly. instance error rate using features computed clustered graph worse error rate features computed compact+ graph. performance test samples averaged runs assumption method feature space complex enough allow extraction features approximate binary labels. feature space poor compact graphs might bring advantage clustered one. might improve accuracy creating graph uses exactly target labels. number output features unclear graph coding several auxiliary labels decreasing article propose exact label learning construction training graphs ﬁnal label estimation linear transformation slowest feature extracted. method allows direct solution regression problems gsfa without recur supervised post-processing step. words given input sample ﬁrst feature computed using graph directly provides approximation label practice even better results achieved using feature supervised post-processing. crucial emphasize gsfa optimizes feature slowness depends particular training graph used label estimation accuracy. however method used training graphs deﬁne slowness objective requires optimizing output similarity function similarities intimately related desired label similarities. result feature slowness estimation accuracy objectives become supervised learning problems high-dimensional data great practical importance frequently result systems large computational demands. common approach apply feature extraction dimensionality reduction supervised learning algorithm. promising alternative approach hierarchical gsfa complexity scales cases even linearly w.r.t. input dimensionality number samples. furthermore hgsfa trained graph resulting architecture simple homogeneous shown figure proved usefulness method showing three types applications relevant practice regression multiple labels analysis training graphs classiﬁcation compact discriminative features. results show encoding auxiliary labels derived original improve performance. particularly relevant cascaded convergent hierarchical gsfa networks. auxiliary labels contain information determines original label. gsfa node receives information might able extract original label accurately without multiple labels learned simultaneously instance code diﬀerent aspects input multiple labels inspired biological systems complementary information channels observed might improve feature robustness example incomplete information learning gender color simultaneously yielded clearly smaller estimation errors gender color estimated separately. shows multiple label learning theoretically possible coding complementary information channels might boost accuracy practice. instance automatic system face processing might beneﬁt simultaneous extraction subject’s identity gender race pose expression. analytical practical results show strength serial graph single label available. case graph provided marginally better estimations serial computation time times larger. although shape slowest feature extracted serial graph might less similar label monotonic transformation slowest feature learned nonlinear supervised step might suﬃce approximate hence particularly promising application method multiple label learning. various methods mapping slowest feature label tested. linear scaling method interesting theoretical point view. however would expect provided worse accuracy test data soft method nonlinear supervised. therefore latter might preferred practical applications. moreover scenario supervised post-processing methods might computationally inexpensive input frequently low-dimensional although originally designed regression show also useful classiﬁcation particular labels learned. experiment traﬃc sign classiﬁcation shows beneﬁt using compact discriminative features implemented learning multiple binary labels. resulting system much smaller classiﬁcation error clustered graph number output dimenfewer features reducing number signals computed network might also reduce overﬁtting. although ideally binary target labels suﬃce perfect classiﬁcation experiments show additional target labels auxiliary labels improve classiﬁcation accuracy practice. future work scope might include construction training graphs eﬃcient time oﬀer versatility graphs. example eﬃcient training graphs learn single target label therefore interested designing eﬃcient graphs allow learning multiple labels. open research question choose auxiliary labels. clear depend original label unclear compute maximize estimation accuracy many coded eigenvalues used. classiﬁcation classes linearly decreasing eigenvalues provided great results eigenvalues might better large. hierarchical processing slowness principle powerful brain-inspired learning principles. strength originates theoretical foundations ﬁeld learning invariances generality slowness principle. practical supervised learning applications hgsfa provides good accuracy eﬃciency still proﬁts strong theoretical foundations. advantage relying general principles resulting algorithms application independent conﬁned particular problem input feature representation. course tuning network parameters integration problem-speciﬁc knowledge always possible additional performance. proposed method explores limits hgsfa valuable theoretical tool analysis design training graphs. however results show certain adaptations also suﬃciently robust applied practical computer vision machine learning tasks.", "year": 2015}