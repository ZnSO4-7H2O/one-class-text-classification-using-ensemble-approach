{"title": "One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation  of Text Data", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data.", "text": "abstract. recent technical scientiﬁc advances wealth information hidden unstructured text data ofﬂine/online narratives research articles clinical reports. mine data properly attributable innate ambiguity word sense disambiguation algorithm avoid numbers diﬃculties natural language processing pipeline. however considering large number ambiguous words language technical domain encounter limiting constraints proper deployment existing models. paper attempts address problem oneclassiﬁer-per-one-word algorithms proposing single bidirectional long short-term memory network considering senses context sequences works ambiguous words collectively. evaluated senseval- benchmark show result model comparable top-performing algorithms. also discuss applying additional modiﬁcations alleviates model fault need training data. word sense disambiguation important problem natural language processing right stepping stone advanced tasks pipeline applications machine translation question answering speciﬁcally deals identifying correct sense word among given candidate senses word presented brief narrative generally referred context. consider ambiguous word ‘cold sentence started give cold shoulder experiment possible senses cold cold temperature cold sensation common cold negative emotional reaction therefore ambiguous word cold speciﬁed along eﬀort develop supervised model leverages bidirectional long short-term memory network. network works neural sense vectors learned model training employs neural word vectors learned unsupervised deep learning approach called glove context words. evaluating onemodel-ﬁts-all network public gold standard dataset senseval- demonstrate accuracy model terms f-measure comparable state-of-the-art algorithms’. outline organization rest paper follows. section brieﬂy explore earlier eﬀorts discuss recent approaches incorporate deep neural networks word embeddings. main model employs blstm sense word embeddings detailed section present experiments results section supported discussion avoid drawbacks current model order achieve higher accuracies demand less number training data desirable. finally section conclude future research directions construction sense embeddings well applications model domains biomedicine. generally three categories algorithms supervised knowledgebased unsupervised. supervised algorithms consist automatically inducing classiﬁcation models rules labeled examples knowledge-based approaches dependent manually created lexical resources wordnet uniﬁed medical language system unsupervised algorithms employ topic modeling-based methods disambiguate senses known ahead time thorough survey algorithms refer navigli past years increasing interest training neural word embeddings large unlabeled corpora using neural networks word embeddings typically represented dense real-valued dimenembedding dimension vocabulary size. column matrix embedding vector associated word vocabulary matrix represents latent feature. vectors subsequently used initialize input layer neural network model. glove existing unsupervised learning algorithms obtaining vector representations words training performed aggregated global word-word co-occurrence statistics corpus. besides word embeddings recently computation sense embeddings gained attention numerous studies well. example chen adapted neural word embeddings compute diﬀerent sense embeddings showed competitive performance semeval- data long short-term memory introduced hochreiter schmidhuber gated recurrent neural network architecture designed address vanishing exploding gradient problems conventional rnns. unlike feedforward neural networks rnns cyclic connections making powerful modeling sequences. bidirectional lstm made reversed unidirectional lstms means able encode information preceding succeeding words within context ambiguous word necessary correctly classify sense. given document position target word model computes probability distribution possible senses related word. architecture model depicted fig. consist layers sigmoid layer fully-connected layer concatenation layer blstm layer cosine layer sense word embeddings layer contrast supervised neural networks generally softmax layer cross entropy hinge loss parameterized context words selects corresponding weight matrix bias vector ambiguous word’s senses network shares parameters words’ senses. remaining computationally eﬃcient structure aims encode statistical information across diﬀerent words enabling network select true sense blank space within context. replacement softmax layers sigmoid layer network need impose modiﬁcation input model. purpose contextual features going make input network also sense interested whether given context makes sense would provided network. next context words would transferred sequence word embeddings sense would represented sense embedding neuralnetworksenseclassiﬁerconsistingofonefully-connectedlayerandasigmoidunit.finallyanargmaxovertheoutputs outputsoftheﬁrsttwolayersarefedtotwolstmnetworkswithdiﬀerentdirections.thentheconcatenatedoutputsoflstmsisfedto componentsarecenteredaroundtheambiguousword.thecosinesimilaritiesbetweenthecontextwordsandtheexaminedsenseasthe fig.thesinglemodelofdeepbidirectionallstmforwordsensedisambiguationoftextdata.aseriesofcontext here vvvs one-hot representation sense corresponding sn}. one-hot representation vector dimension consisting |vs|− zeros single index indicates sense. size index indicates word context. size equal number words language choose column initialized using pre-trained word embeddings; work glove vectors used. cation layer result merge layer train network instance correct sense given context inputs ˆysi incorrect senses testing however among senses output network sense gives highest value ˆysi considered true sense ambiguous term words correct sense would relu means rectiﬁed linear unit; concatenated outputs right left traversing lstms blstm last context components met. weights bias hidden layer. senseval- data network evaluated consist separate training test samples. order hyper-parameters network training samples used validation advance. hyperparameters selected whole network trained training samples prior testing. loss function employed network even though common cross entropy loss function last unit sigmoidal classiﬁcation observed mean square error better results ﬁnal argmax classiﬁcation used. regarding parameter optimization rmsprop employed. also weights including embeddings updated training. dropout regularization technique neural network models randomly selected neurons ignored training. means contribution activation downstream neurons temporally removed forward pass weight updates applied neuron backward pass. eﬀect network becomes less sensitive speciﬁc weights neurons resulting better generalization network less likely overﬁt training data. network dropout applied embeddings well outputs merge fully-connected layers. following dropout logic dropword word level generalizations word dropout word zero dropword replaced speciﬁc tag. subsequently treated like word vocabulary. motivation dropword word dropout decrease dependency individual words training context. since replacing word dropout dropword observed change results word dropout applied sequence context words training. hyper-parameters determined validation presented table preprocessing data conducted lower-casing words documents removing numbers. results vocabulary size context size embedding size blstm hidden layer size dropout sense/word embeddings dropout lstm outputs dropout fully-connected layer word dropout sense embedding initialization word embedding initialization between-all-models comparisons senseval- task launched submissions received addressing task. afterward papers tried work data reported results separate articles well. compare result model top-performing low-performing algorithms show single model sits among top-performing algorithms considering algorithms ambiguous word separate classiﬁer trained table shows results top-performing low-performing supervised algorithms. ﬁrst algorithms represent state-of-the-art models supervised evaluated senseval-. multi-classiﬁer blstm consists deep neural networks make pre-trained word embeddings. lower layers networks shared upper layers network responsible individually classify ambiguous word network associated with. ims+adapted another model considers deep neural networks also uses pre-trained word embeddings inputs. contrast multi-classiﬁer blstm model relies features tags collocations surrounding words achieve result. models softmax constitutes output layers networks. htsa winner senseval- lexical sample. naive bayes system applied mainly words lemmas tags correction a-priori frequencies. irst-kernels utilizes kernel methods pattern abstraction paradigmatic syntagmatic information unsupervised term proximity british national corpus classiﬁers. likewise nusels makes classiﬁers combination knowledge sources considering ranking scores unsupervised methods outperform supervised algorithms. within-our-model comparisons besides several internal experiments examine importance hyper-parameters network investigated sequential follow cosine similarities computed true sense preceding succeeding context words carries pattern-like information encoded blstm. table presents results experiments. ﬁrst shows best result network described rows shows change applied network behavior network terms f-measure. middle part speciﬁcally concerned importance presence blstm layer network. introduced fundamental changes input structure network. generally expected cosine similarities closer words true sense larger incorrect senses’ however series cosine similarities encoded lstm network experimented. observe reverse sequential follow information bidirectional lstm shuﬄe order context words even replace bidirectional lstms diﬀerent fully-connected networks size achieved results notably less speciﬁcally importance using glove pre-trained word embeddings word dropout improves generalization context size plays important role ﬁnal classiﬁcation result results table notice single network despite eliminating problem large number classiﬁers still falls short compared state-of-the-art algorithms. based intuition supported preliminary experiments deﬁciency stems important factor blstm network. since sense embedding made publicly available sense embeddings initialized randomly; word embeddings initialized pre-trained glove vectors order beneﬁt semantic syntactic properties context words conveyed embeddings. separate spaces sense embeddings word embeddings come enforces delay alignment spaces turn demands training data. furthermore early misalignment allow blstm fully take advantage larger context sizes helpful. ﬁrst attempt deal problem pre-train sense embeddings techniques taking average glove embeddings deﬁnition content words senses taking average glove embeddings context words training samples give better result random initialization. preliminary experiments though replaced glove embeddings network sense embeddings showed considerable improvements results ambiguous words. means senses context words come vector space. words context would also represented possible senses words take. idea help improve results current model also avoid need large amount training data since senses seen places center context trained. contrast common one-classiﬁer-per-each-word supervised algorithms developed single network blstm able eﬀectively exploit word orders achieve comparable results best-performing supervised algorithms. single blstm network language domain independent applied resource-poor languages well. ongoing project also provided direction lead improvement results current network using pre-trained sense embeddings. future work besides following discussed direction order resolve inadequacy network regarding non-overlapping vector spaces embeddings plan examine network technical domains biomedicine well. case model evaluated dataset prepared national library medicine also construction sense embeddings using deﬁnitions senses tested. moreover considering many senses least unambiguous word representing sense also experiment unsupervised training network beneﬁts form quarry management training data automatically collected web.", "year": 2018}