{"title": "Minimal Effort Back Propagation for Convolutional Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "As traditional neural network consumes a significant amount of computing resources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet effective technique to alleviate this problem. In this technique, only a small subset of the full gradients are computed to update the model parameters. In this paper we extend this technique into the Convolutional Neural Network(CNN) to reduce calculation in back propagation, and the surprising results verify its validity in CNN: only 5\\% of the gradients are passed back but the model still achieves the same effect as the traditional CNN, or even better. We also show that the top-$k$ selection of gradients leads to a sparse calculation in back propagation, which may bring significant computational benefits for high computational complexity of convolution operation in CNN.", "text": "propose minimal effort back propagation method reduce calculation back propagation called meprop. idea compute small critical portion gradient information update corresponding minimal portion parameters learning step. update highly relevant parameters others stay untouched. hence technique results sparse gradients sparse update. words fewer gradients passed back rows columns weight matrix modiﬁed. experiments also show models using meprop robust less likely overﬁtting. extend technique convolutional neural network call meprop-cnn reduce calculation back propagation cnn. back propagation convolution operation transformed matrix multiplication operations forward propagation. neural networks matrix multiplication operation consumes computing resources operations plus minus address issue apply meprop-cnn like meprop feedforward model compared linear transformation convolution operation unique operation characteristic leads different behavior parameters updation. explained detail section implement sparse back propagation method reduce calculation makes complex convolution computation transformed sparse matrix multiplication. proposed method outperform original methods. traditional neural network consumes signiﬁcant amount computing resources back propagation propose simple effective technique alleviate problem. technique small subset full gradients computed update model parameters. paper extend technique convolutional neural network reduce calculation back propagation surprising results verify validity gradients passed back model still achieves effect traditional even better. also show top-k selection gradients leads sparse calculation back propagation bring signiﬁcant computational beneﬁts high computational complexity convolution operation cnn. convolutional neural networks achieved great success many ﬁelds object classiﬁcation face recognition although special network architecture makes possible abstract features layer layer high computational complexity convolution computation consumes large amount computing resources problem turns compute-intensive model. needs convolution computation matrix multiplication operation convolutional layer forward propagation hand almost amount computation needed back propagation. this method could reduce calculation great help reducing time consumption training process inference process. proposed meprop uses approximate gradients keeping top-k elements based magnitude values. top-k elements largest absolute values kept. example suppose vector top-k values denote indices vector approximate gradient parameter matrix input vector figure illustration meprop single computation unit neural models. original back propagation uses full gradient output vectors compute gradient parameters. proposed method selects top-k values gradient output vector backpropagates loss corresponding subset total model parameters. introduce meprop technique convolutional neural network reduce calculation back propagation. forward process computed usual small subset gradients used update parameters. speciﬁcally select top-k elements update parameters rest similar dropout technique. model meprop robust less likely overﬁtting. ﬁrst present proposed method describe implementation details. ﬁrst introduce meprop feedforward neural network. simplicity linear transformation unit brief enough explain understand detail proposed method forward propagation process feed forward neural networks. compared linear transformation section convolution computation unique operation feature leads different behavior parameters update. corresponding critical portion parameters updated learning step means several rows columns weight matrix modiﬁed. necessarily like mepropcnn. meprop operation generates sparse matrix intermediate gradients. take simple convolution computation example gradients probably sparse model. determinated difference convolution operation linear transformation. beneﬁt meprop-cnn sparse matrix operation necessary verify validity meprop architecture. dense matrix operations consume time back propagation. address propose mepropcnn technique lead sparse matrix operations beneﬁt transformation. apply proposed method every convolution ﬁlter respectively. element-wise operations original back propagation procedure kept operations already fast enough compared matrix-matrix matrixvector multiplication operations. illustrated figure common architecture convolutional neural networks apply method convolutional layers conv conv figure full gradients passed back fully–connected layers. note operations convolution transformed matrix-matrix matrix-vector multiplication operations rewrite since conduct experiments based tensorﬂow need make matrix transformation manually. operands transformed right matrix shape properly tensorﬂow framework need apply method gradients. consider chain rule used back propagation need gradi ents parameter matrix heretofore operations convolution transformed matrix-matrix matrix-vector multiplication operations process back propagation similar meprop decribed section always remember convolution different operations weight sharing units feature share parameters namely ﬁlters weights. weight sharing mechanism makes powerful extract abstract features layer layer. ﬁlters slide local patches feature maps current layer generate feature maps next layer. hand different ﬁlters hold different parameters work model independently urges obey property apply proposed method back propagation. select top-k gradients every feature respectively rather mixing feature maps together choosing them. concretely output current convolutional layer contains feature maps generated ﬁlters respectively forward propagation back propagation gradient matrix shape matrix slice gradient matrix pieces corresponding ﬁlters forward propagation apply top-k selection respectively top-k elements largest absolute values kept. thing pointed apply top-k selection current gradient matrix directly instead take account historical gradients scale. achieve applying exponential select top-k elements based matrix grad unselected elements selected sparse matrix replace original gradient matrix complete rest work previous layers also note relu activation function property relu tends lead sparsity less. check much sparsity relu max-pooling layer contribute gradients. train iteration sparsity sparse layers respectively. contrast sparsity sigmoid rate related kernel size max–pooling layer. speciﬁcally kernel size strides max-pooling layer experiments table max-pooling layer chooses maximum element grids unselected elements contribute next layer gradients locations back propagation. hence sparsity convolutional layer most full gradients also deep neural networks difﬁcult train reason distribution layer’s inputs changes training parameters previous layers change. ioffe szegedy propose method called batch normalization address problem. proven effective technique deep neural networks. convolutional layers additionally want normalization obey convolutional property—so different elements feature different locations normalized way. achieve this jointly normalize activations minibatch locations. figure illustration values feature across elements mini-batch spatial locations mini-batch size feature maps size effective mini-batch size learn pair parameters feature rather activation. side-effect constraint also obey convolutional property apply top-k operation convolutional layers back propagation. words apply top-k operation gradients matrix along feature dimension rather dimensions. take mnist experiment example output ﬁrst convolutional layer matrix size batch size height width eature forward propagation apply batch normalization featuremap respectively means eature pairs parameters model. similarly back propagation apply top-k operation feature respectively feature maps computed ﬁlters independently. riedmiller braun proposed direct adaptive method fast learning performs local adaptation weight update according behavior error function. tollenaere also proposed adaptive acceleration strategy back propagation. dropout proposed improve training speed ioffe szegedy distribution layer’s inputs changes training refer phenomenon internal covariate shift. batch normalization addresses problem performing normalization training mini-batch. lecun wiesler reveals network training converges faster inputs whitened i.e. linearly transformed zero means unit variances decorrelated. performming normalization inputs layer like whitening performance model would achieve ﬁxed distributions inputs would remove effects internal covariate shift. also note normalization procedure different training inference. mini-batch inputs compute mean variance training unbiased estimatation used inference. unbiased variance estimation expectation training mini-batches size sample variances. batch normalization achieves great success deep neural networks deep convolutional neural networks. merely adding batch normalization state-of-the-art image classiﬁcation model yields substantial speedup training. increasing learning rates removing dropout applying modiﬁcations afforded batch normalization model reaches previous state small fraction training steps batch normalization supplies regularize model like dropout proposed method combine method meprop-cnn batch normalization? batchnormalized meprop-cnn could still work properly? test verify idea experiments results shown follow. reduce risk overﬁtting. sparse coding class unsupervised methods learning sets overcomplete bases represent data efﬁciently poultney proposed sparse autoencoder model learning sparse over-complete features. proposed method quite different compared prior studies back propagation dropout sparse coding. sampled-output-loss methods limited softmax layer based random sampling method limitations. sparsely-gated mixture-ofexperts sparsiﬁes mixtureof-experts gated layer limited speciﬁc setting mixture-of-experts method limitations. also prior studies focusing reducing communication cost distributed systems quantizing value gradient -bit ﬂoat -bit. settings also different ours. demonstrate effectiveness method perform experiments mnist image recognition task. sample images database shouwn figure model without top-k batch normalization chosen baseline. implement proposed method meprop-cnn mnist image recognition task verify method. adam optimize model implementation detail implementation adam stay untouched. implement experiments based tensorﬂow mnist mnist dataset handwritten digits training examples test examples. images dataset gray scale images size pixel belong classes ranges mnist task convolutional layers max–pooling layers fully–connected layers output last fully–connected layer softmax layer produces distribution ten-class labels. architecture model shown figure table accuracy results based meprop-cnn mnist. decay means decay rate used momentum method. epoch means number epoches reach optimal score development data. model epoch used obtain test score. ﬁrst convolutional layer ﬁlters input image kernels size stride pixel. second convolutional layer takes input output previous pooling layer ﬁlters kernels size pooling window size max–pooling stride pixels. rectiﬁed linear unit activation function model. krizhevsky deep convolutional neural networks relus converge several times faster equivalents tanh units. table shows details parameter setting. perform top-k method back propagation except last fully–connected layer. hyper-parameters adam optimization follows learning rate mini-batch size intuitive idea layers different number neurons also different number gradients selected gradients front layers inﬂuenced gradients back layers back propagation factors taken account top-k ratios. experiments lots exploration proper parameters setting. experiments reveal sparse gradients back layers result performance much gradients information dropt causes parameters front layers converge appropriate values. table shows results different top-k values mnist dataset. mini batch size topk ratio ranges shown table. decay rate represents tradeoff usual ﬁrst evaluate model development data obtain optimal number iterations corresponding accuracy test data evaluated based best setting tuned development set. table meprop-cnns better accuracy baseline baseline test reveals baseline without meprop tends overﬁtting. momentum method higher decay rate always mean better result decay=. works better experiments. owing large momentum makes model inﬂexible means small ﬁxed subset gradients used others never chances selected. compared meprop-cnns keep ability keep small subset full gradients back propagation even better. main reason could minimal effort update modify weakly relevant parameters makes overﬁtting less likely similar effect dropout. batch normalization demonstrates ability accelerate convergence model batch normalization gets faster rate convergence higher accuracy shown table batch normalization top-k gets better accuracy full gradients. experiments gradients fully–connected layers processed meprop method top-k means gradients passed back convolutional layers back propagation. momentum meprop-cnn batch normalization consistent before proper decay rate works better experiments. results shown table trix multiplication operation forward propagation small subset gradients used update parameters. speciﬁcally select top-k elements update parameters rest similar dropout technique. enhance meprop technique momentum method stable results. experiments show method perform good even small subset gradients used what’s more ability avoid overﬁtting. method still able work compatibly batch normalization. future work would like apply proposed method lexical processing tasks beneﬁt method well. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu ghemawat sanjay goodfellow harp andrew irving geoffrey isard michael yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh man´e monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay vi´egas fernanda vinyals oriol warden pete wattenberg martin wicke martin yuan zheng xiaoqiang. tensorflow large-scale machine learning heterogeneous systems http//tensorflow. org/. software available tensorﬂow.org. dryden nikoli jacobs moon essen brian. communication quantization dataparallel training deep neural networks. proceedings workshop machine learning high performance computing environments ieee press glorot xavier bordes antoine bengio yoshua. proceedings deep sparse rectiﬁer neural networks. fourteenth international conference artiﬁcial intelligence statistics ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning taigman yaniv yang ming ranzato marc’aurelio wolf lior. deepface closing human-level performance face veriﬁcation. proceedings ieee conference computer vision pattern recognition krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems poultney christopher chopra sumit yann efﬁcient learning sparse representations energy-based model. advances neural information processing systems riedmiller martin braun heinrich. direct adaptive method faster backpropagation learning rprop neural networks ieee internaalgorithm. tional conference ieee seide frank droppo jasha gang dong. -bit stochastic gradient descent application data-parallel distributed training speech fifteenth annual conference internadnns. tional speech communication association shazeer noam mirhoseini azalia maziarz krzysztof davis andy quoc hinton geoffrey dean jeff. sparsely-gated mixture-of-experts layer. arxiv preprint arxiv. srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research morency louis-philippe okanohara daisuke tsujii junichi. modeling latent-dynamic shallow parsing latent conditional model improved inference.", "year": 2017}