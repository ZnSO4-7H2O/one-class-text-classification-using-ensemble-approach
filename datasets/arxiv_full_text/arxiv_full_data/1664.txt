{"title": "Generating Thematic Chinese Poetry using Conditional Variational  Autoencoders with Hybrid Decoders", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Computer poetry generation is our first step towards computer writing. Writing must have a theme. The current approaches of using sequence-to-sequence models with attention often produce non-thematic poems. We present a novel conditional variational autoencoder with a hybrid decoder adding the deconvolutional neural networks to the general recurrent neural networks to fully learn topic information via latent variables. This approach significantly improves the relevance of the generated poems by representing each line of the poem not only in a context-sensitive manner but also in a holistic way that is highly related to the given keyword and the learned topic. A proposed augmented word2vec model further improves the rhythm and symmetry. Tests show that the generated poems by our approach are mostly satisfying with regulated rules and consistent themes, and 73.42% of them receive an Overall score no less than 3 (the highest score is 5).", "text": "major progress made poetry generation even though existing approaches shown great power poetry automatic generation still suffer major problem lack consistent theme representation unique emotional expression. taking poem shown table instance consistent theme poem nostalgia. apparently every single line poem related closely theme emotion. recent work tried generate poems smooth consistent theme using topic planning scheme similar word extensions. still hard methods represent topics improve quality generated poems. paper solve difﬁculty learning themes poetry meanwhile leveraging boost generation corresponding poems. variational autoencoders proved effective topic representation using learned latent variables text generation regard possible solution. moreover since written poems composed certain intent seek conditional variational autoencoders recent modiﬁcation generate diverse images/texts conditioned certain attributes work hypothesize part intent represented form keywords conditions part expressed latent variables learned cvae. general cvae encoder decoder rnns usually faces vanishing lacomputer poetry generation ﬁrst step towards computer writing. writing must theme. current approaches using sequenceto-sequence models attention often produce non-thematic poems. present novel conditional variational autoencoder hybrid decoder adding deconvolutional neural networks general recurrent neural networks fully learn topic information latent variables. approach signiﬁcantly improves relevance generated poems representing line poem context-sensitive manner also holistic highly related given keyword learned topic. proposed augmented wordvec model improves rhythm symmetry. tests show generated poems approach mostly satisfying regulated rules consistent themes receive overall score less introduction poetry beauty simplicity. abstractness concise formats rules provide regularities ﬁrst target language generation. regularity especially ampliﬁed classical chinese poetry example quatrains poem consists four lines seven characters last character second fourth line follow rhythm tonal pattern requests characters particular positions hold particular tones terms ping example quatrain written wang famous poet tang dynasty shown table illustrated table good quatrain follow three pattern regularities mentioned above. besides rules poem expression certain theme human emotion. hold consistent semantic meanings emotional expressions. trivial create quatrain certain rules rhythm tone express consistent theme consistent feelings even people. automatically generating poetry contains want express primary task language generation. tent variable problem applied directly natural language generation. thus present novel cvae hybrid decoder contains deconvolutional neural networks recurrent neural networks fully learn information learned latent variables. addition propose vertical slices poems additional sentences training data wordvec model order improve rhythm symmetry delivered poems name augmented wordvec model also propose straightforward easily applied automatically evaluation metric rhythm score evaluation measure poetry rule-consistency. speciﬁcally contributions paper summarized follows propose conditional variational autoencoders learn theme information poetry lines. best knowledge represents ﬁrst attempt using cvae poetry generation. present novel conditional variational autoencoder hybrid decoder combining decnn general demonstrates capability learning topic information poems also addressing vanishing latent variable problem. introduce augmented wordvec model improve rhythm symmetry delivered poems. experiments show able boost rule-consistency generated poems also used search characters representing similar semantic meanings chinese poems. build chinese poetry generation system take users’ writing intent generation process. experimental results show system generate good quatrains satisfy rules consistent topic. related work poetry generation ﬁrst step toward experimenting language generation. according methodology used different approaches categorize methods three major directions i.e. approaches using rules/templates approaches using statistical machine translation models approaches using neural networks second kind approach involves various statistical machine translation methods. rather designing algorithms identify useful rules approaches using models whose parameters derived analysis bilingual text corpora regard previous line poem source language machine translation task posterior line target language sentence suffer lack deep understanding poems’ semantic meaning. address issue many approaches using neural networks proposed attracted much attention recent years. example proposed approach using recurrent neural networks generate poem line characterby-character lines generated previously contextual input. experimental results show quatrains reasonable quality generated using approach. following rnnbased approach proposed characterbased treating poem entire character sequence easily extended various genres song iambics. approach advantage ﬂexibility easy implementation long-sequence generation process causes instability poetry theme. avoid situation brought forward attention mechanism rnnbased framework encoded human intention guide poetry generation. proposed rnn-based poetry generation model iterative polishing scheme. speciﬁcally encoded users’ writing intent ﬁrst decoded using hierarchical recurrent neural network. recently proposed memory-augmented neural model trying imitate poetry writing process. approach uses augmented memory reﬁne poems generated neural model balance requirements linguistic accordance aesthetic innovation extent. parallel efforts made generating english poems. instance considered adding list similar words theme. follow third type approach automatically generate chinese poetry. introduced above mentioned neural models attempt produce poems regulated rules consistent theme meaningful semantics none consider represent poem theme boost results. address issue propose novel conditional variational autoencoder hybrid decoder learned latent variables combined conditional keywords able convey topic information entire poem. approaches overview human poets write poems according sketch ideas two-stage chinese poem generation approach i.e. writing intent representation thematic poem generation. speciﬁcally system take word sentence even document input containing users’ writing intent generate rule-complied theme-consistent poem sequentially using improved conditional variational autoencoder. similar work done main distinction work implemented neural model conditional variational autoencoders work. damping factor. empirically damping factor usually initial score number extracted keywords users’ input query less required conduct keyword extension candidate word highest textrank score selected keyword. chinese poetry generation since human poets create poems based plain outline believe keywords obtained ﬁrst stage generation framework partially represent users’ writing intent regard conditions cvae. deﬁne conditional distribution learning target approximate deep neural networks parameterized cvae trained maximize conditional likelihood given meanwhile minimizing regularizer posterior distribution prior distribution recognition network prior network approximate true posterior distribution prior distribution objective traditional cvae takes following form shown eqn. generative process summarized sampling latent variable generating cvae efﬁciently trained stochastic gradient variational bayes framework maximizing variational lower bound conditional likelihood fig. illustrates training procedure proposed conditional variational autoencoders hybrid decoders shown fig. bidirectional recurrent neural network long short term memory encoder encode concatenation current line corresponding keyword previously generated lines ﬁxedsize vectors concatenating last hidden states forward backward then simply represented adopt multiple layers residual connections layers learn describable suppose follows multivariate gaussian distribution diagonal covariance matrix thus recognition network prior network given writing intent representation stage sentence transformed four keywords i.e. represents sub-topic corresponding line thematic poem generation stage assuming keywords enough convey topic information entire poem line ﬁrst encoded latent variable learn distribution potential writing intent prior network generated decoding concatenation learned latent variable extracted expanded keyword result poem created automatically sub-topic provided corresponding keyword also topic messages stored latent variables learned current line previously generated lines corresponding keyword note seven-character quatrain given fig. produced automatically generation system. writing intent representation fact line quatrain consists seven characters hypothesize sub-topic line represented keyword. therefore important evaluate importance words extracted input query provided users. textrank measure importance different words. graph textrank vertex represents candidate word edges words indicate cooccurrence edge weight according total count co-occurrence strength words. textrank score computed iteratively convergence according following equation since easy cvae ignore latent variable directly using decoder inspired propose hybrid decoder cvae shown fig. name novel cvae conditional variational autoencoders hybrid decoders hybrid decoder composed deconvolutional neural networks recurrent neural networks. reason introduce decnn part decoder cvae build connection element learned latent variable then probability generated sequence represented however hard fully feed-forward architecture learn sequential information element multi-layer lstm decoder similar encoder added decnn layers model optimization although cvae achieved impressive results image generation non-trivial adapt natural language generators vanishing latent variable problem. annealing gradually increasing weight term training plays powerful role dealing problem. another solution word drop decoding sets certain percentage target words hurt performance drop rate high. thus adopt annealing instead word drop decoding training cvae. beyond that propose auxiliary solution help solve problem i.e. additional decnn reconstruction loss term eqn. regularize weighting parameter therefore loss function proposed cvae-hd represented below since combination keywords extracted users’ query latent variable learned cvae represent poetry theme representation keywords performance extent. therefore mine nature quatrains obtain good representation poetry word. notice lines quatrains mostly third fourth line corresponding characters position lines often match certain constraints semantic and/or syntactic relatedness. figure training procedure proposed conditional variational autoencoders hybrid decoders. black dashed lines represent residual connection layers sand) represent numbers mean绝 deliver similar meanings nonexistence. even though constraints quatrains strict chinese antithetical couplets propose initialize wordembedding vectors using augmented wordvec model enhance rhythm symmetry delivered poems. model adds vertical slices poems additional sentences training data based wordvec able boost optimization cvae improve rule-consistency generated poems also used search characters representing similar semantic meanings chinese poems. experimental setup dataset large-scale datasets used experiments. ﬁrst dataset chinese poem corpus containing traditional chinese poems various genres including tang quatrains song iambics yuan songs ming qing poems. dataset train wordembedding chinese characters. since focus generating quatrains four lines length seven characters line ﬁlter quatrains named chinese quatrain corpus train neural network model. speciﬁcally randomly choose poems validation poems testing non-overlap ones training. segment poems words calculate textrank score word. then word highest textrank score selected keyword line quatrain owns four keywords. training choose frequently used characters vocabulary. dimension word-embedding vectors recurrent hidden layers encoder part hybrid decoder contain hidden units number layers deconvolutional layers relu non-linearity decnn part hybrid decoder. kernel size table language modeling results performance automatic metrics test dataset. report negative likelihood perplexity component given parentheses. note reported bleu scores normalized mean score proposed rhythm score evaluation reported last column. corresponding best scores shown bold. stride number feature maps layer respectively. weighting parameter -dimensional latent variables. parameters model randomly initialized uniform distribution support model trained using adadelta algorithm mini-batch learning rate dropout technique also adopted dropout rate perplexity value validation used early stop training avoid overﬁtting learned model. evaluation generally difﬁcult judge quality poems generated computers. conduct automatic human evaluation verify feasibility availability proposed chinese poetry generation approach. comparative approach mainly compare proposed approach attention-based sequence-tosequence model presented proved capable generating different genres chinese poems. reasons choose compare rather others summarized aspects. first model fully compared previous methods rnnlm rnnpg anmt proved better them. second ﬁrst generation phase proposed approach i.e. writing intent representation similar procedure introduced second phase completely different. therefore comparing framework theirs inspect effects proposed conditional variational autoencoders hybrid decoders automatic evaluation poetry modeling results language modeling results test dataset shown table reconstruction perplexity negative likelihood component reported. addition this bleu evaluation method famous evaluation quality text also reported table bleu- normalize scale. represents line poems number characters rule rule represent tonal patterns rhyming patterns severally. results mean score terms demonstrated last column table higher mean score indicates approaches owning better capability generating poems regulated rhythm structure. note ground truth represents humanly written poems test dataset. table that compared baseline cvae+awv cvae-hd+awv outperform terms metrics. note represent approach using proposed augmented wordvec model appending plus sign original method e.g. ass+awv. compared ass+awv cvae+awv cvae improvement adding found cvae demonstrates advantage optimization poetry modeling. beyond this cvae-hd proposed novel cvae hybrid decoder outperforms cvae especially terms proves hybrid decoder relieve pressure vanishing latent variables certain extent. notice simpliﬁcation poetry although annealing proposed hybrid decoder adopted training harder poetry generation tackle vanishing latent variable problem general natural language generation. poetry character similarity measure similarity poetry characters verify superiority proposed model original wordvec one. taking poetry mentioned section instance similarity using using worth noticing similarity score model beyond that search similar words. instance search similar words obtain time-related chinese characters. poem contains related words autumn wind e.g. cranes september also delivers consistently gloomy theme emotion. poem moreover intuitively conforms tonal structural rules quatrains. collect quatrains based random queries input human-evaluation participants. average obtain readability score consistency score aesthetic score evocative score overall score. among generated poems receive overall score less human evaluation online test build web-based environment poetry generation system whose interface illustrated fig.. using website users input arbitrary query topic generate computer-written poem using proposed cvae-hd. invited users well-educated great passion poetry writing participate human evaluation. participants asked rate poems score range based subjective evaluation metrics including readability consistency aesthetic evocative overall conclusions work studied poetry generation. present two-step generation approach including writing intent representation thematic poem generation imitate poem creation process human poets. proposed conditional variational autoencoder hybrid decoder mine implicit topic information contained within poems lines. augmented wordvec model also proposed enhance rhythm symmetry delivered poems improve training procedure. generative neural model incorporate ﬂexibility represent theme message learning latent variables. conduct experiments several evaluation metrics compare proposed approach existing ones. experimental results demonstrate proposed poetry generation approach produce satisfying quatrains regulated rules consistent themes. proposed conditional variational autoencoder hybrid decoder proved outperform attention-based sequence-tosequence model. ishaan gulrajani kundan kumar faruk ahmed adrien taiga francesco visin david vazquez aaron courville. pixelvae latent arxiv preprint variable model arxiv. iulian vlad serban alessandro sordoni ryan lowe laurent charlin joelle pineau aaron courville yoshua bengio. hierarchical latent variable encoder-decoder model generating dialogues. aaai pages kihyuk sohn honglak xinchen yan. learning structured output representation using deep conditional generative models. advances neural information processing systems pages nitish srivastava geoffrey hinton alex krizhevsky ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research naoko tosa hideto obara michihiko minoh. hitch haiku interactive supporting sysinternational contem composing haiku poem. ference entertainment computing pages springer xiaofeng naoko tosa ryohei nakatsu. hitch haiku interactive renku poem composition supporting tool applied sightseeing navigation system. entertainment computing–icec pages jiyuan zhang yang feng dong wang yang wang andrew abel shiyue zhang andi zhang. flexible creative chinese poetry generation using neural memory. arxiv preprint arxiv. tiancheng zhao zhao maxine eskenazi. learning discourse-level diversity neural dialog models using conditional variational autoencoders. arxiv preprint arxiv.", "year": 2017}