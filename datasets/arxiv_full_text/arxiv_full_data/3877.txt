{"title": "Robust Subspace Clustering via Smoothed Rank Approximation", "tag": ["cs.CV", "cs.IT", "cs.LG", "cs.NA", "math.IT", "stat.ML"], "abstract": "Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.", "text": "matrix rank minimizing subject afﬁne constraints arises many application areas ranging signal processing machine learning. nuclear norm convex relaxation problem recover rank exactly restricted theoretically interesting conditions. however many real-world applications nuclear norm approximation rank function produce result optimum. seek solution higher accuracy nuclear norm paper propose rank approximation based logarithmdeterminant. consider using rank approximation subspace clustering application. framework model different kinds errors noise. effective optimization strategy developed theoretical guarantee converge stationary point. proposed method gives promising results face clustering motion segmentation tasks compared state-of-the-art subspace clustering algorithms. rm×n unknown matrix rm×n linear mapping denotes observations. unfortunately however minimizing rank matrix known np-hard challenging problem. consequently widely-used convex relaxation approach replace rank function nuclear i-th singular value nuclear norm technique shown effective encouraging low-rank solution nevertheless guarantee minimum nuclear norm solution coincide minimal rank many interesting circumstances heavily dependent singular values matrices nullspace variations nuclear norm demonstrated promising results e.g. singular value thresholding truncated nuclear norm another popular alternative approach compute usually nonconvex nonsmooth function. observed nonconvex approach succeed broader range scenarios however nonconvex optimization often challenging. overcome above-mentioned difﬁculties paper propose particular log∈ rn×n identity matrix. large nonzero singular values logdet function value much smaller nuclear norm. easy show logdet therefore logdet tighter rank approximation function nuclear norm. although similar function logdet proposed iterative linearization used local minimum required small leads signiﬁcantly biased approximation small singular values thus limited applications. smoothed schatten-p function well studied matrix completion nonetheless resulting algorithm rather sensitive parameter main contributions work follows efﬁcient algorithm devised optimize logdet associated nonconvex objective function; method pushes accuracy subspace clustering level. important application proposed logdet function low-rank representation based subspace clustering problem. signiﬁcant research effort subject past several years promising applications computer vision machine learning subspace clustering aims ﬁnding low-dimensional subspace group points based widely-used assumption high-dimensional data actually reside union multiple low-dimensional subspaces. assumption data could separated projected subspace. consequently subspace clustering mainly involves tasks ﬁrstly projecting data latent subspace describe afﬁnities points subsequently grouping data subspace. spectral clustering methods normalized cuts usually used second task cluster membership. besides spectral clustering-based subspace clustering method iterative algebraic statistical methods also available literature usually sensitive initialization noise outliers. typical spectral clustering-based subspace clustering methods local subspace afﬁnity sparse subspace clustering rank representation robust variant lrsc among them give promising results even presence large outliers corruption suppose data point written linear combination points dataset. tries sparsest representation data points l-norm. even subspaces overlap successfully reveal subspace structure ssc’s solution sometimes sparse form fully connected afﬁnity graph data single subspace uses lowest-rank representation depict similarity among data points. theoretically guaranteed succeed subspaces independent. rm×n store m-dimensional samples drawn union parameter represents unknown corruption l-norm l-norm |eij| suitable characterize sample-speciﬁc corrupi= tions outliers; often describes gaussian noise. able produce pretty competitive performance subspace clustering current literature. however solution might unique nuclear norm furthermore rank surrogate deviate true rank function. objective function nonconvex. design effective optimization strategy based augmented lagrangian multiplier method potentially applicable large-scale data decomposability admittance parallel algorithms. optimization method provide theoretical analysis convergence mathematically guarantees algorithm produce convergent subsequence converged point stationary point section present proposed robust subspace clustering algorithm clar clustering log-determinant approximation rank. basic theorems optimization algorithm presented below. updating converted scalar minimization problems following theorem also proved supplementary material. theorem unitarily invariant function optimal solution following problem algorithm smoothed rank minimization input data matrix rm×n parameters initialize rn×n repeat obtain update solve either according update multipliers complete procedure solving problem summarized algorithm since objective function nonconvex difﬁcult give rigorous mathematical proof convergence optimum. show supplementary material algorithm converges accumulation point accumulation point stationary point. experiments conﬁrm convergence proposed method. experimental results promising despite solution obtained proposed optimization method local optimum. obtain coefﬁcient matrix consider constructing similarity graph matrix since postprocessing coefﬁcient matrix often improves clustering performance using points. however excessively large would break afﬁnities points group. used experiments thus post-processing procedure lrr. obtaining directly utilize ncuts cluster samples. section apply clar subspace clustering benchmark databases extended yale database hopkins motion database clar compared state-of-the-art subspace clustering algorithms lrsc. segmentation error rate used evaluate subspace clustering performance deﬁned percentage erroneously clustered samples versus total number samples data considered. parameters tuned achieve best performance. general corruptions noise slight value relatively large. experiments used. inﬂuences convergence speed adopt often done literature. fair comparison follow experimental settings stop program maximum iterations relative difference reached. experiments implemented intel core .ghz macbook memory. code available https//github.com/sckangz/logdet. eyaleb consists frontal face images individuals lighting conditions. task cluster images individual subspaces identity. eyaleb challenging subspace clustering large noise corruptions seen sample images figure model noise image resized -dimensional vector. divide subjects four groups i.e. consider choices group choices ﬁrst three groups. datasets respectively. mean median error rates datasets corresponding reported table seen clar outperforms methods signiﬁcantly. subjects involved error rate clar remains level methods increase drastically. particular challenging case subjects mean clustering error rate clar improves compared best result provided ssc. implies method robust in-sample outliers. table also observe clustering error rates much larger methods. potentially based heavily inﬂuenced outliers. addition advantage method much signiﬁcant respect low-rank representation based algorithms lrsc; example improvement cases subjects respectively. veriﬁes importance good rank approximation. subsection evaluate robustness clar motion segmentation problem important step video sequences analysis. given multiple image frames dynamic scene motion segmentation cluster points views different motions undertaken moving objects. hopkins motion database contains video sequences along features extracted tracked frames sequence. since trajectories associated motion reside distinct afﬁne subspace dimension every motion corresponds subspace. figure gives sample images. applied model noise. table shows clustering results hopkins dataset. clar achieves best results cases. speciﬁcally average clustering error rate motions three motions. also show computational time table computational time less though lrsc. figure demonstrates sensitivity algorithm shows performance clar quite stable varies pretty large range. also test values give much difference error rate. since problem nonconvex repeat experiments using different random initializations still similar results tuning parameters. thus clar appears quite insensitive initilizations. paper study matrix rank minimization problem log-determinant approximation. surrogate better approximate rank function. application study robust subspace clustering problem. minimization algorithm based type augmented lagrangian multipliers method developed optimize associated nonconvex objective function. extensive experiments face clustering motion segmentation demonstrate effectiveness robustness proposed method shows superior performance compared state-of-the-art subspace clustering methods. fazel matrix rank minimization applications ph.d. dissertation thesis stanford university recht fazel parrilo guaranteed minimum-rank solutions linear matrix equations nuclear norm minimization vidal tutorial subspace clustering ieee signal processing magazine vol. malik normalized cuts image segmentation pattern analysis machine intelligence ieee transactions vidal favaro rank subspace clustering pattern recognition letters vol. exact subspace segmentation outlier detection low-rank representation international conference", "year": 2015}