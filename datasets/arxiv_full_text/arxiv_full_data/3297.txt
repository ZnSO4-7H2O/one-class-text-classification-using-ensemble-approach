{"title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control  from Raw Images", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.", "text": "introduce embed control method model learning control non-linear dynamical systems pixel images. consists deep generative model belonging family variational autoencoders learns generate image trajectories latent space dynamics constrained locally linear. model derived directly optimal control formulation latent space supports long-term prediction image sequences exhibits strong performance variety complex control problems. control non-linear dynamical systems continuous state action spaces problems robotics broader context reinforcement learning autonomous agents. prominent class algorithms solve problem model-based locally optimal control algorithms ilqg control approximate general nonlinear control problem local linearization. combined receding horizon control machine learning methods learning approximate system models algorithms powerful tools solving complicated control problems however either rely known system model require design relatively low-dimensional state representations. real autonomous agents succeed ultimately need algorithms capable controlling complex dynamical systems sensory input only. paper tackle difﬁcult problem. stochastic optimal control methods applied directly control image data would face major obstacles. first sensory data usually high-dimensional i.e. images thousands pixels rendering naive solution computationally infeasible. second image content typically highly non-linear function system dynamics underlying observations; thus model identiﬁcation control dynamics non-trivial. problems could principle addressed designing advanced algorithms approach optimal control images problem differently turning problem locally optimal control high-dimensional non-linear systems identifying low-dimensional latent state space locally optimal control performed robustly easily. learn latent space propose deep generative model belonging class variational autoencoders derived ilqg formulation latent space. resulting embed control system probabilistic generative model holds belief viable trajectories sensory space allows accurate long-term planning latent space trained fully unsupervised. demonstrate success approach four challenging tasks control images compare range methods unsupervised representation learning. aside also validate deep up-convolutional networks powerful generative models large images. denotes time steps system state applied control system noise. function arbitrary smooth system dynamics. equivalently refer equation using notation assume multivariate normal distribution σξ). assume given access visual depictions state restriction requires solving joint state identiﬁcation control problem. simplicity following assume fully observed depiction relax assumption later. goal infer low-dimensional latent state space model optimal control performed. seek learn function mapping high-dimensional images low-dimensional vectors control problem solved using instead accounts system noise; equivalently σω). assuming moment function learned ﬁrst deﬁne latent space introduce model thereafter. stochastic locally optimal control latent spaces inferred latent state image state transition dynamics latent space i.e. lat. thus models changes occur control applied underlying system latent space analogue assuming known optimal controls trajectory length dynamical system derived minimizing function gives expected future costs following instantaneous costs denotes terminal costs state action sequences respectively. contains sufﬁcient information i.e. inferred alone differentiable cost-minimizing controls computed algorithms optimal control algorithms approximate global non-linear dynamics locally linear dynamics time step locally optimal actions found closed form. formally given reference trajectory current estimate optimal trajectory together corresponding controls system linearized rnz×nz rnu×nu cost weighting matrices zgoal inferred representation goal state. also assume throughout paper. combination equation gives local linear-quadratic-gaussian formulation time step solved algorithms iterative linear-quadratic regulation approximate inference control result trajectory optimization step locally optimal trajectory corresponding control sequence representation fulﬁll three properties must capture sufﬁcient information must allow accurate prediction next latent state thus implicitly next observation xt+; prediction next latent state must locally linearizable valid control magnitudes given representation properties particular require capture possibly highly non-linear changes latent representation transformations observed scene induced control commands. crucially particularly hard model subsequently linearize. circumvent problem taking direct approach instead learning latent space transition model linearized combined algorithms directly impose desired transformation properties representation learning. select properties prediction latent space well locally linear inference next observation according equation easy. transformation properties desire latent representation formalized directly ilqg formulation given section formally following equation latent representation gaussian σω). infer ﬁrst require method sampling latent states. ideally would generate samples directly unknown true posterior however access following variational bayes approach overview) resort sampling approximate posterior distribution parameters inference model work always diagonal gaussian distribution rnz×nz computed encoding neural network outputs activation last hidden layer given henc learnable parameters encoding network including weight matrices biases parameterizing mean variance gaussian distribution based neural network gives natural expressive model latent space. additionally comes beneﬁt reparameterization trick backpropagate gradients loss function based samples latent distribution. generative model using approximate posterior distribution generate observed samples ˜xt+ latent samples enforcing locally linear relationship latent space according equation yielding following generative model decomposed atσtat note transition dynamics generative model operates inferred latent space takes untransformed controls account. learn latent space transition dynamics linearizes non-linear observed dynamics locally linear applied controls reconstruction image performed passing sample multiple hidden layers decoding neural network computes mean generative bernoulli distribution response last hidden layer decoding network. hdec parameters decoding network including weight matrix bias make learned generative parameters transition model ˆqψ. remains specify linearization matrices rnz×nz rnz×nu offset predicted. following approach distribution means covariance matrices predict local transformation parameters samples third neural network parameters based hidden representation htrans refer transformation network. speciﬁcally parametrize transformation matrices offset denotes vectorization therefore circumvent estimating full matrix size choose perturbation identity matrix executing another action ˆzt+ result valid latent state since transition model conditional samples coming inference network thus long-term predictions fail. nutshell divergence encodings transition model results generative model accurately model markov chain formed observations. learning stochastic gradient variational bayes training model data containing observation tuples corresponding controls obtained interactions dynamical system. using data learn parameters inference transition generative model minimizing variational bound true data negative log-likelihood plus additional constraint latent representation. complete loss function given isotropic gaussian distribution mean zero unit variance. second divergence equation additional contraction term weight enforces agreement transition inference models. term essential establishing markov chain latent space corresponds real system dynamics divergence also seen prior latent transition model. note terms computed analytically model training approximate expectation sampling. speciﬁcally take sample input transform sample using equation give valid sample ˆqψ. jointly learn parameters model minimizing using sgd. evaluate model four visual tasks agent plane obstacles visual version classic inverted pendulum swing-up task balancing cart-pole system control three-link larger images. described detail below. model training. consider different network types model standard fully connected neural networks three layers work well moderately sized images used planar swing-up experiments; deep convolutional network encoder combination up-convolutional network decoder which accordance recent ﬁndings literature found adequate model larger images. training performed using adam throughout experiments. training data tasks generated randomly sampling state observations actions corresponding successor states. plane used samples inverted pendulum cart-pole system used complete list architecture parameters hyperparameter choices well in-depth explanation up-convolutional network speciﬁed supplementary material. make code video containing controlled trajectories systems available http//ml.informatik.uni-freiburg.de/research/ec model variants. addition embed control dynamics model derived above also consider variants removing latent dynamics network htrans i.e. setting output equation obtain variant estimated globally linear matrices instead replace transition model network estimating dynamics non-linear function linearize planning estimating jacobians described section obtain variant nonlinear latent dynamics. baseline models. thorough comparison exhibit complicated nature tasks also test baseline models plane inverted pendulum task standard variational autoencoder deep autoencoder trained autoencoding subtask visual problems. given data used training model remove actions tuples disregard temporal context images. autoencoder training learn dynamics model latent space approximating section also consider variant slowness term latent representation full description variant given supplementary material. optimal control algorithms. perform optimal control latent space different models employ trajectory optimization algorithms iterative linear quadratic regulation approximate inference control vaes methods operate mean distributions ˆqψ. aico additionally makes local gaussian covariances except experiments planar system control performed model predictive control fashion using receding horizon scheme introduced obtain closed loop control given image ﬁrst passed encoder obtain latent state locally optimal trajectory subsequently tt+t minztt+t found optimizing ﬁxed small horizon controls applied system transition observed control sequence horizon figure true state space planar system examples inferred spaces different models. spaces spanned generating images every valid position agent embedding respective encoders. starting found using last estimated trajectory bootstrap. note planning performed entirely latent state without access observations except depiction current state. compute cost function required trajectory optimization assume knowledge observation xgoal goal state sgoal. observation transformed latent space costs computed according equation agent planar system move bounded two-dimensional plane choosing continuous offset xy-direction. high-dimensional representation state black-and-white image. obstructed circular obstacles task move bottom right image starting random position image. encodings obstacles obtained prior planning additional quadratic cost term penalizing proximity them. depiction observations control performed together corresponding state values embeddings latent space shown figure ﬁgure also clearly shows fundamental advantage model competitors separately trained autoencoders make aesthetically pleasing pictures models failed discover underlying structure state space complicating dynamics estimation largely invalidating costs based distances said space. including latent dynamics constraints end-to-end models hand yields latent spaces approaching optimal planar embedding. test long-term accuracy accumulating latent real trajectory costs quantify whether imagined trajectory reﬂects reality. results models starting random positions executing pre-computed actions summarized table using seperate test evaluating reconstructions. methods achieve reconstruction loss difference accumulated real costs trajectory show superiority model. using globally locally linear model trajectories planned latent space good trajectories planned real state. models besides fail give long-term predictions result good performance. next turn task controlling classical inverted pendulum system images. create depictions state rendering ﬁxed length line starting center image angle corresponding pendulum position. goal task swing-up balance underactuated pendulum resting position exemplary observations reconstructions system given figure visual inverted pendulum task algorithm faces additional difﬁculties observed space non-markov angular velocity cannot inferred single image second discretization errors rendering pendulum angles small pixel images make exact control difﬁcult. restore markov property stack images thus observing one-step history. table comparison different approaches model learning pixels planar pendulum system. compare models respect prediction quality test sampled transitions respect performance combined note trajectory costs latent space necessarily comparable. real trajectory cost computed dynamics simulator executing planned actions. true models real trajectory costs planar system pendulum. success deﬁned reaching goal state staying \u0001-close rest trajectory statistics quantify different starting positions. marks separately trained dynamics networks. success percent velocities positions data remarkable table compares different models quantitatively. model best terms reconstruction performance model resulting stable swing-up balance behavior. explain failure models fact non-linear latent dynamics model cannot guaranteed linearizable control magnitudes resulting undesired behavior around unstable ﬁxpoints real system dynamics task globally linear dynamics model inadequate. finally consider control complex dynamical systems images using layer convolutional inference layer up-convolutional generative network resulting -layer deep path input reconstruction. speciﬁcally control visual version classical cartpole system history pixel images well three-link planar robot based history pixel images. latent space -dimensional experiments. real state dimensionality cart-pole four controlled using figure true state space inverted pendulum task overlaid successful trajectory taken agent. learned latent space. trajectory traced latent space. images reconstructions showing current positions history figure left trajectory cart-pole domain. ﬁrst image real images dreamed model. notice discretization artifacts present real image. right exemplary observed predicted images trajectory visual robot domain goal marked red. previous experiments model seems problem ﬁnding locally linear embedding images latent space control performed. figure depicts exemplary images problems trajectory executed system. costs trajectories slightly worse trajectories obtained aico operating real system dynamics starting start-state supplementary material contains additional experiments using domains. context representation learning control review) deep autoencoders similar baseline models applied previously e.g. lange riedmiller direct route control based image streams taken recent work deep end-to-end q-learning atari games mnih well kernel based deep policy learning robot control close approach recent paper wahlstr¨om autoencoders used extract latent representation control images non-linear model forward dynamics learned. model trained jointly thus similar non-linear variant comparison. contrast model formulation requires pre-processing neither ensure long-term predictions latent space diverge linearizable. stated above system belongs family vaes generally similar recent work kingma welling rezende gregor bayer osendorfer additional parallels work recent advances training deep neural networks observed. first idea enforcing desired transformations latent space learning data becomes easy model appeared several times already literature. includes development transforming auto-encoders recent probabilistic models images second learning relations pairs images although without control received considerable attention community last years broader context model related work state estimation markov decision processes discussion) through e.g. hidden markov models kalman ﬁlters presented embed control system stochastic optimal control high-dimensional image streams. approach extraction latent dynamics model constrained locally linear state transitions. evaluation four challenging benchmarks revealed embeddings control performed ease reaching performance close achievable optimal control real system model. thank radford metz dewolf sharing code well dosovitskiy useful discussions. work partly funded grant within priority program autonomous learning brainlinks-braintools cluster excellence watter funded state graduate funding program baden-w¨urttemberg. references jacobson mayne. differential dynamic programming. american elsevier todorov generalized iterative method locally-optimal feedback control theodorou. probabilistic differential dynamic programming. proc. nips levine koltun. variational policy search trajectory optimization. proc. nips kingma welling. auto-encoding variational bayes. proc. iclr rezende mohamed wierstra. stochastic backpropagation approximate inference toussaint. robot trajectory optimization using approximate inference. proc. icml jordan ghahramani jaakkola saul. introduction variational methods mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature cohen welling. transformation properties learned visual representations. iclr taylor sigal fleet hinton. dynamical binary latent variable models memisevic. learning relate images. ieee trans. pami langford salakhutdinov zhang. learning nonlinear dynamic models. icml west harrison. bayesian forecasting dynamic models osendorfer soyer smagt. image super-resolution fast approximate convolutional sparse coding. proc. iconip lecture notes computer science. springer international publishing state transition matrix factorization divergence alluded main paper estimation full local state transition matrix rnz×nz equation requires transition network predict parameters. using arbitrary state transition matrix also inconveniently requires inversion said matrix computing divergence penalty equation started experiments using full matrix quickly found rank pertubation identity matrix could used instead without loss performance benchmarks. contrary resulting networks fewer parameters thus easier train. give derivation process divergence therefore equation computed. reformulation represent vtrt need estimated transition network reducing number outputs divergence multivariate gaussians given main point behind derivation presented following make partial derivatives divergence efﬁciently computable. cannot take trace determinant numerical algorithms able take gradients symbolic form. aside that like process batch samples computation convenient form require excessive amounts tensor products between. start simpliﬁcation images three-link task) still generate full resolution images decoder network. high-dimensional images generation fully connected neural networks simply option. thus decided up-convolutional networks recently show powerful models image generation set-up models basically mirror convolutional architecture used encoder. speciﬁcally convolution followed max-pooling step encoder network introduce up-sampling convolution step decoder network. complete network architecture given below. similar up-convolution networks used dosovitskiy upsampling strategy simple perforated upsampling described enforcing temporal slowness learning previously found good proxy learning representations reinforcement learning representation learning videos also consider variant slowness term latent representation enforcing similarity encodings temporally close images. achieved augmenting standard objective lbound additional divergence term latent posterior indeed seems slightly better coherence similar states latent spaces e.g. depicted figure main paper. experiments show slowness term alone sufﬁce structure latent space locally linear predictions control become feasible. autoencoding. able reconstruct given observations basic necessity model work. reconstruction cost drives model identify single states observations. decoding next state. planning possible decoder must able generate correct images transitions dynamics model performed. case know latent states encoding transition model coincide thus preventing planning. optimizing latent trajectory costs. action sequences achieving speciﬁed goal determined completely locally linearized dynamics latent space. therefore minimizing trajectory costs latent space again necessity successful control. optimizing real trajectory costs. action sequence determined latent dynamics deciding criterion whether reﬂects true state trajectory costs. therefore carrying dreamed plans reality optimality criterion every model. make different models comparable cost matrices evaluation necessarily optimization. reﬂected four criteria evaluation table paper. reconstruction current next state speciﬁed mean loss case bernoulli distributions cross entropy error function robot used last experiment main paper simulated using dynamics generated maplesim http//www.maplesoft.com/products/maplesim/ simulator wrapped python visualized producing inputs using pygame. simulated fairly standard robot three links. length links masses corresponding links compare efﬁcacy different models combined optimal control algorithms always reported cost latent space well real trajectory cost. compute real cost evaluated cost function latent space using real system states execution different cost matrices fair comparison. upper bound performance achievable control models also computed true system cost applying ilqr/aico model real system dynamics. model available since experiments performed simulation. experimental setup datasets created advance training validation test split. models trained ones incorporate transition information trained images dimages extracted original dataset slowness trained pairs images subset dpairs models full order image-only autoencoders extracted latent representations combined actions ddynamics low-dimensional representations trained dynamics mlps thus ensuring methods trained exactly data. used orthogonal weight initialization every layer described main paper adam used learning rule networks. found techniques fundamentally important stabilizing training achieving good reconstructions methods. methods also clearly helped hyperparameter search needed methods minimum. process training could make three phases unfolding latent space overcoming trivial solution minimization latent term. architectures used experiments follows conv. convolutions) input image dimensions action dimensions latent space dimensionality encoder relu relu relu linear decoder relu relu linear dynamics relu relu output layer input image dimensions action dimension latent space dimensionality encoder relu relu linear decoder relu relu linear dynamics relu relu output layer input image dimensions action dimension latent space dimensionality encoder relu relu relu relu relu decoder relu relu up-sampling relu up-sampling dynamics relu relu linear dynamics relu relu linear controlled dynamics pendulum starts without velocity. timesteps full force applied right followed angle timesteps full force left. finally similar images section figure shows multi-step predictions cartpole system. depict important cases long-term prediction cart-pole standing still cart-pole moving right changing direction poles angular velocity pole moving farthest right. long-term predictions model high quality. note uncontrolled dynamics predictions show slight bias pole moving right attribute problem fact discretization errors image rendering process pole angle make hard predict small velocities accurately. figure shows segment controlled trajectory three-link executed system. note that contrast ﬁgures supplementary material show long-term prediction rather steps trajectory taken system combined model predictive control. additional visualizations controlled trajectories tasks refer supplementary video. table compare variety models terms real trajectory cost task success percentage cart-pole robot arm. results averaged different starting states ﬁxed goal state. cart-pole always starts goal state small additive gaussian noise success deﬁned preventing pole falling angle rad. three-link system begins random conﬁguration goal unroll joints stay \u0001-close position. results show non-linear variant perform task successfully although still large performance two. conclude error linearizing non-linear dynamics training corresponding model grows point longer allowing accurate control system. figure dreamed trajectories uncontrolled controlled dynamics cart-pole system. image shows initial conﬁguration encoded resulting images right half column generated without additional input following dynamics latent space. left column depicts uncontrolled case middle column shows controlled trajectory torque applied step right column trajectory torque applied step. prediction history image omitted depictions. table comparison trajectory costs different approaches cart-pole threelink task. standard autoencoder variational autoencoder global model omitted table failed task compare well aico deals covariance matrices estimated latent space performed additional experiment cart-pole three-link robot task comparing ilqr. performed model predictive control using locally linear model starting different start states each. remaining settings given section figure frames extracted trajectory executed embed control system. left column shows real images corresponding transitions taken mdp. middle right column show prediction history image current image based previous images.", "year": 2015}