{"title": "Human Understandable Explanation Extraction for Black-box Classification  Models Based on Matrix Factorization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In recent years, a number of artificial intelligent services have been developed such as defect detection system or diagnosis system for customer services. Unfortunately, the core in these services is a black-box in which human cannot understand the underlying decision making logic, even though the inspection of the logic is crucial before launching a commercial service. Our goal in this paper is to propose an analytic method of a model explanation that is applicable to general classification models. To this end, we introduce the concept of a contribution matrix and an explanation embedding in a constraint space by using a matrix factorization. We extract a rule-like model explanation from the contribution matrix with the help of the nonnegative matrix factorization. To validate our method, the experiment results provide with open datasets as well as an industry dataset of a LTE network diagnosis and the results show our method extracts reasonable explanations.", "text": "account underlying decision making constraint. approach help service provider understand decision making logic model quickly decide whether model acceptable not. consequence result help signiﬁcantly reduce efforts model inspection. explaining black model easy task general variety algorithms build classiﬁcation model. works analyze model predicts result given instance. understand reason classiﬁcation result image classiﬁcation problem visualization techniques saliency used similar silence trust problem prediction result tackled approach explain reason prediction results model instance level enough inspect characteristics model. method based results. extract common decision constraints explanations classiﬁcation results using nonnegative matrix factorization. explanations classiﬁcation result explanations shares characteristics model underlying decision making constraints. instance experts classify system diagnosis issues several categories decision making constraints even cannot recognize explicitly. constraints reﬂected explanations instances. main contribution paper propose analytic method model explanation applicable general classiﬁcation models. introduce concept contribution matrix explanation embedding constraint space using matrix factorization. experimentally show embedded explanations well clustered according classiﬁcation category instances. using explanation embedding extract model explanation rule-like form. also perform experiments open datasets well industrial dataset show validity approach practice. particular many industrial datasets especially defect detection diagnosis systems consist numerical attributes meaning. method therefore focus dataset numeric attributes. recent years number artiﬁcial intelligent services developed defect detection system diagnosis system customer services. unfortunately core services black-box human cannot understand underlying decision making logic even though inspection logic crucial launching commercial service. goal paper propose analytic method model explanation applicable general classiﬁcation models. introduce concept contribution matrix explanation embedding constraint space using matrix factorization. extract rule-like model explanation contribution matrix help nonnegative matrix factorization. validate method experiment results provide open datasets well industry dataset network diagnosis results show method extracts reasonable explanations. recent years number artiﬁcial intelligent services developed defect detection system diagnosis system customer services. core services consists classiﬁcation model learned historical data classiﬁed domain experts. better model many advanced techniques applied like ensemble boosting deep learning techniques helped improving performance classiﬁcation model unfortunately model black-box problem human canunderstand model makes decision. blackbox model difﬁculty give trust users even model shows classiﬁcation error. problem model inspection. viewpoint service providers inspection model behavior crucial launching commercial services. since service providers potentially risks applying black-box model spend much time verify model reliability proved increases development costs signiﬁcantly. service providers still preferred hybrid machine learning based model rule based model services. section would develop mathematical model describe precisely model explanation. classiﬁcation model function space instances {··· category space. since want focus dataset consisting numerical attributes without categorical attribute suppose number attributes. subset size called dataset. instances dataset classiﬁed i.e. purpose model explanation understand classiﬁcation constraints category. numerical attributes easiest intuitively understand constraint checking whether value attribute certain interval not. enough rough constraints approximate partition done following. advantage approximating instead approximating function component product intervals. component directly matched rule-like explanation simple understand. existing literatures decision tree considered interpretable model algorithms extract decision tree classiﬁcation model developed decision tree advantage understanding reasoning process since decision tree path root leaf intuitively represents process decision making. however decision tree still hard understand simple rule instability tree structure understanding model issues machine learning ﬁeld. however understanding black-box model difﬁcult general variety algorithms dependency data characteristics. decision tree support vector machine still popular variations published decades algorithms intuitive structures interpreted unfortunately guarantee sufﬁciently good performance practice often need apply advanced techniques improve performance ensemble boosting makes model black like deep neural network promising technique nowadays. visualization techniques saliency often used order understand classiﬁcation results image classiﬁcation problem. visualize evidences image classiﬁcation model measures sensitivity classiﬁcations analyzes activation difference marginal distribution small area. approaches speciﬁed image classiﬁers easy extended applications. overcome difﬁculty model understanding explanation black model studied recent years. lipton discussed meaning interpretability properties interpretable models tuner introduced several axioms deﬁne concept explanation eligibility explanation terms conditional probability model explanation system also developed explain results black model based axioms. riberio tackled trust problem proposed method explain classiﬁcation result idea classiﬁcation model locally approximated perturbing instance approximation explain much feature affects decision. local perturbation also used interpreting prediction results image classiﬁcation domain. shrikumar tackled issue proposed method determine important features backpropagating contributions neurons. ross develop method train model right reasons differentiable classiﬁer neural network considered ﬁrst order derivative classiﬁer explanation designed loss function consideration explanation. literatures instance level explanation focused model level explanation overlooked. model level explanation necessarily needs understand underlying decision making constraint given classiﬁcation model. proposed simpliﬁcation method bayesian network model rule extraction methods proposed restricted support vector machine. bastani understand reasoning process black model extracting decision tree approximates classiﬁcation model much attribute instance contributes classiﬁcation result. next step construct contribution matrix extract common behaviors contribution matrix help nonnegative matrix factorization. detail step explained following subsections. interpretable instance explanation understand behavior classiﬁcation model ﬁrst need explain attributes instance mainly affect result measure amount impacts proper way. consider local perturbation given instance paper perturbation applied interpretable features instead attributes instance borrowing idea introduced advantage approach complexity explanations controllable restricting number interpretable features. interpretable feature space ﬁnite dimension whose dimension represents interpretable feature. numerical attributes consider interval attribute value interpretable feature explained previous section. since able deal ﬁnite number features domain attributes discretized. recalling rectangle form product intervals indicator form interpretable feature. upper bounded interval enough since implies denote indicators half-bounded interval. interpretable space |f|. element implies corresponding indices feature directly used classify categories without computation understandability restrict binary classiﬁcation model whose decision boundary given linear function classiﬁed target category suitable given instance optimization problem formulated following. every instance category mutually disjoint. this decide category suitable instance even lies boundary areas different categories. follows needs complex constraints complexity increases. consequently leads barely understandable explanations although approximates accurately. however approach extracting ﬁnding soft constraint since allowed overlapped. complexity explanation needs increase much increases. fact viewpoint extracting understandable explanations unnecessary accurate approximation performance measure order measure quality model explanation score. note model explanation category induces binary classiﬁcation consider restriction binary classiﬁcation i.e. others. considering classiﬁcation result true value relative accuracy performance dataset deﬁned score relative dataset note dependency maximization achievable solving subproblems maximizes value category decompose problem ﬁnding subproblems. therefore category target category assume binary classiﬁcation model labels following sections. vector interpreted underlying decision making constraint classiﬁcation model value element represents relationship feature classiﬁcation result instance. base vector contribution matrix contains information feature combination simultaneously affects decision making category. hence call column space constraint space base vector constraint space. consider base vector threshold ﬁlters meaningless features base vector constraints taking every feature satisﬁes instance take corresponding constraint taken. combining constraints obtain rectangle underlying decision making constraint convenience denotes entire space instances. value becomes smaller takes constraints rule complex. hand larger value yields less complex constraint column vector geometrically interpreted projection onto constraint space. speciﬁcally effect base vector instance. call column vector embedded explanation. common decision making constraint classiﬁcation rule applied determine category. hence embedded explanations expected concentrated several clusters almost vectors cluster labeled category. later section validate assumption experimental results. suppose clusters embedded explanations denote g··· since focused binary classiﬁcation frequent label cluster g··· clusters elements mostly labeled target category vector centroid vector representative interpreted follows. base vector contributes category weight average. suppose instance classiﬁed target category corresponding embedded explanation cluster take largest element corresponding base vector related important explanation therefore corresponding instance call contribution feature feature respect positively affects decision otherwise implies feature negatively affects. later section help lime evaluate deﬁning score represents relative difference results moreover collecting instance explanation deﬁne contribution matrix model respect model explanation contribution matrix intuition extract model explanation contribution matrix contains underlying constraints decision making. deﬁnition matrix contributions feature. column vectors grouped several clusters according decision making constraints classiﬁcation model found using matrix factorization. particular nonnegative matrix factorization base vectors whose elements positive represent much feature affects. consequence rule-like explanations achievable interpreting base vectors. contribution matrix contains positive negative values need nonnegative matrix apply nmf. consider decomposition positive matrix deﬁned φijφij analogously negative matrix instead consider transformed contribution matrix dimension whose elements nonnegative. negative contribution feature instance means preferable take complementary fea= corresponding feature ture thus features. consider nonnegative matrix integer nonnegative matrices dimension respectively solution observations extract explanation target category cluster. consider threshold take important base vectors affect signiﬁcantly. take base vectors denote vectors combining corresponding conditions base vectors rectangle related underlying decision making constraint since clusters relevant target category collecting constraints ﬁnally obtain model explanation target category complexity depends value bounded value human understandable explanation consider constraint rmax number clusters optimization problem solving model explanation target category solving optimization problem categories ﬁnally method extract model explanation classiﬁcation model maximum number rectangles controlled setting rmax. also performance increasing rmax. value rmax increases model explanation better performance could complex since chance contain rectangles. hand value rmax decreases model explanation could simple performance decreases. consequently tradeoff complexity model explanation performance controlled rmax. closing subsection would remark generality approach. derived method viewpoint local perturbation introduced however method contribution matrix required extract rule-like model explanation means instance explanation step replaced techniques well-deﬁned interpretable space embedding figure box-plots purity clusters number clusters varies. x-axis plot represents number clusters y-axis represents purity cluster. plot redline indicates median value triangle marker indicates average value purity. regardless value value purity almost equal datasets. invisible cases. open datasets machine learning repository moreover show usability method practice also show results industrial dataset logs diagnose cause disconnections long-term evolution network. dataset since expert classiﬁes cause disconnections guideline categories causes ensure consistency. consequently able check validity method comparing guideline results. privacy issue ﬁltered dataset obfuscation. dataset consists features whose names f··· categories {c··· experiments random forest adaptive boosting algorithm multi-layer perceptron algorithm used classiﬁcation models. choose algorithms since typical basic representatives ensemble boosting deep learning widely used nowadays. implementation based scikit-learn package train classiﬁcation models. also generate instance explanations help lime table performances model explanation. table abbreviations. wine wine origin dataset derm dermatology dataset network dataset. value accuracy column accuracy trained model. alcohol proline color inten. flavanoids koebner phenom. spongiosis disappearance granular layer clubbing rete ridges fibrosis papillary dermis inﬁltrate relu activations. finally adaboost uses logistic regression base estimator. order train classiﬁcation model split dataset training dataset test dataset table summarizes performances proposed method terms deﬁned algorithm provides relatively high performance cases. means model explanation method well approximates partition original model however resulting performance dermatology dataset relatively large variation algorithms. simplicity model explanation method limitation representing implicit feature like linear combination features. also affects clustering embedded vectors yields variance method increases algorithms. issue resolved considering suitable interpretable space embedding figure dimensional t-sne embedded explanations. category embedded explanation indicated color. cases embedded explanations category well clustered several groups. sumption evaluate purity clusters k-means clustering. results plotted figure purity cluster deﬁned fraction number embedded explanations cluster labeled common category cluster. y-axis graph figure represents purity clusters. value purity almost datasets. even though dermatology cases clusters purity median average almost means clusters high purity value. hence embedded explanations well concentrated validates assumption previous section. details also visualize embedded explanations figure visualize high-dimensional data using t-sne fact t-sne distills location data points embed r-plane still efﬁcient investigate high-dimensional data. model trained random forest algorithm. figure shows results dataset. category instance indicated color. cases embedded explanations category well clustered several groups. even clusters clearly separated wine origin dermatology datasets still possible groups embedded explanations. explanation extraction results investigate experiment performance model explanation method. nmf. decision trees algorithm. trained hidden layers nodes underlying decision making constraints classiﬁcation model. combining constraints model explanation classiﬁcation model. validity approach experiments performed open datasets well industrial dataset. experimental results show method extracts understandable reasonable rule-like explanation model. method help service provider quickly decide validity blackbox model also help reducing development costs intelligent service practice.", "year": 2017}