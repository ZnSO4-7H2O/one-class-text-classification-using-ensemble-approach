{"title": "Zero-Shot Event Detection by Multimodal Distributional Semantic  Embedding of Videos", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., \"changing a vehicle tire\") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.", "text": "m.elhoseinycs.rutgers.edu{jingen.liuhui.chengharpreet.sawhney}sri.com elgammalcs.rutgers.edu §rutgers university computer science department ‡sri international vision learning group several research works proposed facilitate performing zero-shot learning task establishing intermediate semantic layer events generally categories low-level representation multimedia content visual perspective. ﬁrst attribute learning representation zero-shot setting object recognition still images. attributes similarly adopted recognizing human actions attributes generalized denoted concepts context. later proposed concept based event retrieval videos inthewild. even though methods facilitate zero-shot event detection capture visual modality importantly assume relevant concepts query event manually deﬁned. manual deﬁnition concepts also known semantic query editing tedious task biased limitation human knowledge. instead automatically generating relevant concepts leveraging information distributional semantics. propose zero-shot event detection method multi-modal distributional semantic embedding videos. model embeds object action concepts well available modalities videos distributional semantic space. knowledge ﬁrst zero-shot event detection model built distributional semantics extends following directions semantic embedding multimodal information videos automatically determining relevance concepts/attributes free text query could useful applications retrieving videos free text event query based content. embed videos distributional semantic space measure similarity videos event query free text form. validated method large trecvid challenge. using event title query method outperformed state-of-the-art uses descriptions metric roc-auc metric. also order magnitude faster. every minute hundreds hours video uploaded video archival site youtube developing methods automatically understand events captured large volume videos necessary meanchallenging. important tasks direction event detection videos. main objective task determine relevance video event based video content cues event video could include visual objects scene actions detected speech detected text audio concepts search retrieval videos arbitrary events using free-style text unseen text particular dream computational video multi-media understanding. referred zero-shot event detection positive exemplar videos train detector. proliferation videos especially consumer-generated copyright association advancement artiﬁcial intelligence rights reserved. approaches rely whole text description event relevant concepts speciﬁed; example event descriptions used approaches supplementary materials practice however think typical event queries setting similar text-search based words instead model connection multimodal content videos. main question addressed paper event text query retrieve ranked list videos based content. contrast manually assign relevant concepts given event query. instead leverage information distributional semantic space trained large text corpus embed event queries videos space similarity could directly estimated. furthermore assume query comes form unstructured few-keyword query abbreviate method edise contributions. contributions paper listed follows studying few-keyword unstructured-text query detect/retrieve videos based multimedia content novel setting. show relevant concepts event query could automatically retrieved distributional semantic space assigned weight associated relevance; fig. birthday grooming animal example events. best knowledge work ﬁrst attempt model connection keywords multimodal information videos distributional semantics study propose different similarity metrics distributional semantic space enable event retrieval based concepts videos. uniﬁed framework capable embedding space; fig. method also fast makes applicable large number videos concepts attribute methods zero-shot learning based manually specifying attributes category methods focused attribute discovery apply mechanism. recently several methods proposed perform zero shot recognition representing unstructured text document terms drawback tfidf hardly matching terms capture semantically related terms model relate noisy videos instead still images. also wordnet adopted connect objects actions making hard apply setting heavily depending predeﬁned information like wordnet. recent interest especially computational linguistics’ community word-vector representation captures word semantics based context. word-vector representation recent algorithms enabled learning vectors billions words makes much semantically accurate. result models recently adopted several tasks including translation search several computer vision researchers explored using wordvector representation perform zero-shot learning object recognition embed object class name word-vector semantic space learnt models like worth mentioning zero-shot learning approaches also aforementioned work assume training training classes test classes. hence learn transformation correlate information between domains contrast zeroshot setting event retrieval rely mainly event information without seeing training events assumed recent zero-shot event retrieval methods hence exist seen events learn transformation from. differently also model multimodal connection free text query video information. context videos proposed method zero-shot event detection using salient words whole structured event description relevant concept already deﬁned event structured text description; also similarly adopted markovrandom-field language model proposed drawback model performs intensive processing concept. since determines relevance concept query event creating text document represent concept. document created web-querying concept name keywords merging retrieved pages. contrast model require step determine relevance event query. language model trained concept instantly added captured multimodal semantic embedding videos. editing. difference modeling embedding concepts allow zero-shot event retrieval. semantic space vector whose dimensionality number concepts. idea embed concepts video information event query distributional semantic space whose dimensionality independent number concepts. property together semantic properties captured distributional semantics feature approach advantages scalability concept size. having concepts affect representation dimensionality facilitating automatic determination relevant concepts given unstructured short event query example able automatically determine blowing candle concept relevant concept birthday party event. used complete text description event retrieval explicitly speciﬁes relevant concepts. class models improve zero-shot event detection performance reranking. jiang proposed multimodal pseudo relevance feedback self-paced reranking algorithms. main assumption behind models unlabeled test examples available examples given initial ranking high precision means reranking algorithms update conﬁdence video event without knowing conﬁdences remaining videos perform reranking. contrast goal different directly model probability few-keyword event-query given arbitrary video. hence work require initial ranking compute conditional probability video withinformation videos. method also times faster detailed experiments. event-query representation unstructured event title represent event query concept based retrieval. framework also allows additional terms specifically based retrieval. show retrieval different modalities concept based retrieval main focus work. few-keyword event query concept based retrieval denoted query keywords denoted respectively. hence setting ea}. concept denote whole concept setting include visual concepts audio concepts i.e. cd}. visual concepts include object scene action concepts. audio concepts include acoustic related concepts like water sound. performed experiment audio concepts trained mfcc audio features however found performance hence excluded ﬁnal experiments. accordingly ﬁnal performance mainly relies visual concepts concept based retrieval; i.e. denote member deﬁnition concept deﬁned concept’s name optionally related keywords; examples hence {c··· concept deﬁnitions number concepts. video representation zero-shot purpose video deﬁned three pieces information video denoted video denoted video concept representation denoted detected text respectively. used extract extract paper mainly focus visual video content challenging. video concept based representation deﬁned zero-shot event detection setting recognizing events videos without training examples based multimedia content including still-image concepts like objects scenes action concepts asr. given video goal compute embedding event query information video different modalities distributional semantic space relevance could directly computed; fig. speciﬁcally approach model function distributional semantic embedding respectively remove stop words applying embedding rest section organized follows. first present distributional semantic manifold embedding function applied concept deﬁnitions framework. then show determine automatically relevant concepts event title query assign relevance weight them illustrated fig. present concept relevance weighting separate section since might generally useful applications. finally present details derive embedding based proposed concept relevance weighting. distributional semantic model embedding start distributional semantic model train semantic manifold. denote trained semantic manifold vectorization function maps word space vec. denote dimensionality real vector returned corresponding pooled vectors θ)∀i) normalized unit length norm. another idea deﬁne similarity function sets. robustness used percentile-based hausdorff point metric similarity pair points computed cosine similarity. denote version used min{ event detection practice decomposed makes problem reduces deriving start later section could estimated. estimating work concepts linguistic meanings corresponding detection functions given video fig. space could viewed space meanings captured training text-corpus sparse points space corresponding visual detection functions given concepts zero shot event detection exploiting sparse points information captured space. derive probabilistic perspective starting marginalizing concept models learn vector word maximized training corpus; context window size. hence similarity high co-occurred context size training text-corpus based trained space deﬁne embed event query words. words directly embedded manifold function. accordingly represent sets word vectors denote regarding embedding concept deﬁned name optionally related keywords. hence corresponding word vectors used deﬁne space. relevance concepts event query deﬁne similarity function propose functions measure similarity ﬁrst inspired example show quality language model indicated closest vec. accordingly deﬁne version sets ﬁrstly pooled operation; denote pooling operation word vectors element element respectively. then cosine similarity computed. denote version fig. shows could used retrieve concepts relevant space. ﬁgure also shows embedding query relevant concept sets visualization. relevant concepts represented could replaced measure space. interesting observation chosen direct similarity representing query emc; proof appendix performs consistently better experiments. practice include among concepts highest assuming remaining concepts assigned makes items vanish; used hence concept detectors needs computed computational advantage. estimating directly embedded since sets words. hence model follows found similarity function appropriate asr/ocr text since normally contains text compared concept deﬁnition. also exploited interesting property nearest words arbitrary point retrieved. hence automatically augment nearest words event title using cosine similarity retrieval. found trick effective practice since automatically retrieve relevant words might appear fusion fuse weighted geometric mean focus visual concepts i.e. leverage information three types visual concepts object concepts action concepts scene concepts hence cs}; list concepts attached deﬁne object scene concept probabilities video frame action concepts video chunks. rest section summarizes concept detection objects scenes frame action concepts video chunk then show reduced video level probabilities. fig. shows example high conﬁdence concepts birthday party video. object concepts involve overfeat object concept detectors maps -imagenet categories. also adopt concept detectors face person publicity available detector scene concepts represented scene concepts word representation static features codebooks. used trecvid concepts concepts including scene categories like city hall way; concepts provided provided trecvid track. action concepts manually annotated automatically annotated concepts; detailed action concepts; please refer action concept detection method adopt. video level concept probabilities represent probabilities given video pooling operation chunks frames videos similar experiments evaluated average pooling. speciﬁcally video level probabilities object scene concepts respectively pooled frames selected every frames video level probability action concept pooled video chunks chunk size mean chunk length concept training chunks. finally pooling function. denote average pooling respectively. edise computational performance beneﬁts discuss computational complexity concept based edise asr/ocr based edise. fusion part negligible since constant time. concept based edise computational complexity computing mainly linear number videos denoted detail computational complexity alconstant hence video retrieval almost computational complexity video computational complexity computing number concepts. detail next computational complexity whole videos complexity let’s assume |ec| terms |ci| terms. then computational complexity |ci| |ec| usually terms case hence computational complexity dimensionality word vectors. experiments given complexity computational complexity number concepts. hence computational complexity computing videos however given event concepts relevant computed based concepts case sufﬁcient event zero shot retrieval retrieved nearest neighbor search close hence computational complexity reduced googlenews wordvec model used. hence complexity videos basically linear given constant complexity previous argument applies elements except complexity similarity function assuming |ec| |ci| bounded constant complexity videos also bigger constant compared asr/ocr based edise computational complexity respectively. concepts asr/ocr based retrieval. hence computational complexity respectively. since |eo| |vo| |ea| |va| dominating factor complexity evaluated method large trecvid show performance designated medtest containing videos. unless otherwise mentioned results trecvid med. distributional semantic models experiments trained wikipedia googlenews using wikipedia model trained billion words resulting vocabulary size words word vectors dimensions. googlenews model trained billion words resulting vocabulary size million words word vectors dimensions. objective having models compare well edise method performs depending size training corpus used train language model. rest section present concepts fusion results. concept based retrieval results section generated automatically retrieved concepts using event title. start comparing different settings method used language model concept based retrieval rank concepts. indicates computed language model adopted compare exactly setting. model evaluated pooling operations also different similarity measures space furthermore evaluated methods wikipedia gnews language models. order conclusive experiments eight settings model compared performed four different sets concepts table details concept sets attached using gnews language model consistently better using wikipedia language model. indicates word embedding model trained bigger text corpus captures semantics hence accurate setting. pooling behaves consistently better average pooling similarity measure consistently better interesting since indicates hypothesis using vector operations manifold better represent hence recommend ﬁnally model trained larger corpus concept pooling measure performance manifold. model’s ﬁnal setting consistently better ﬁnal performance events detailed ﬁgures attached next experiment shows ﬁnal performance using recommended setting framework whole concepts detailed earlier table shows ﬁnal performance compared concept detectors. hard method performs double performance concept set. even manual semantic editing applied performance still better without semantic editing. also show performance events different concepts object rank classeme best performing concepts numbers reported results indicate value concepts approach compared concepts. also report performance using overfeat concepts retrieve videos events. shows value involving action scene concepts compared still image concepts like overfeat zeroshot event detection. highlight results uses whole event description explicitly includes names relevant concepts. shows googlenews model better wikipedia model consistent concept retrieval results. fig. shows googlenews event asr. show performance events fig. order show value semantic modeling computed performance string matching method baseline basically increment score every exact match detected text words query. while model matching model query words asr/ocr detection semantic properties captured boosts performance compared string matching; table since semantically relevant terms query high cosine similarity tvec semantically related wj). hand hard matching basically assumes vectvec ifwi otherwise. also computed metric method hard matching method ocr; fig. average matching average matching report gnews model results compared indicate that achieve state-of-the-art performance even better asr/ocr; table table also shows asr&ocr map. fusion experiments related systems table start presenting summary earlier asr/ocr results test. comparing performances concepts performance hard ocr/asr much lower average zeroshot performance compared concepts visual work. indicates ocr/asr produces much higher false negatives compared visual concepts. fused conﬁdences achieved performance however average performance achieved lower concepts average performance high indicates measuring retrieval performance performance informative approach might achieve high lower average vice versa. achieved best performance system fusing concepts achieve average auc. found system achieves better state system gain signiﬁcantly average auc; gain table also discuss cprf mmprf spar reranking systems contrast system involve reranking. initial retrieval performance without reranking. interestingly achieved performance also without reranking. reranking methods assumes high precision initial ranking test videos available. without assumptions system without reranking performs better cprf mmprf spar re-ranking systems; table unfortunately performances available method compare with. regarding efﬁciency given representation videos concept retrieval experiment whole concept takes seconds cores intel xeon processor retrieval task events altogether. time spar takes rerank event intel xeon processor; since detect events given representation videos reported average detection time event minutes assuming feature representation videos indicates system faster detection. finally applied spar output initial ranking found improves hurts indicates reranking limited/harmful effect performance method. think since method already achieve high performance without re-ranking; details features experiment. proposed method zero shot event detection distributional semantic embedding video modalities event title query. fusing modalities method outperformed state challenging trecvid benchmark. based notion also showed automatically determine relevance concepts event based distributional semantic space. acknowledgements. work supported intelligence advanced research projects activity department interior national business center contract number d-pc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/nbc u.s. government. work also partially funded nsfusa award", "year": 2015}