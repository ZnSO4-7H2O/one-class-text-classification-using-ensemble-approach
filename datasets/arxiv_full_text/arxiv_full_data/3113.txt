{"title": "Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with  Implicit Low-rank Transformations", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Images seen during test time are often not from the same distribution as images used for learning. This problem, known as domain shift, occurs when training classifiers from object-centric internet image databases and trying to apply them directly to scene understanding tasks. The consequence is often severe performance degradation and is one of the major barriers for the application of classifiers in real-world systems. In this paper, we show how to learn transform-based domain adaptation classifiers in a scalable manner. The key idea is to exploit an implicit rank constraint, originated from a max-margin domain adaptation formulation, to make optimization tractable. Experiments show that the transformation between domains can be very efficiently learned from data and easily applied to new categories. This begins to bridge the gap between large-scale internet image collections and object images captured in everyday life environments.", "text": "olution centered objects well sometimes artiﬁcial backgrounds objects part scene images leading blurred appearances large degree occlusion truncation. transform-based domain adaptation overcomes bias learning transformation datasets. contrast classiﬁer adaptation learning transformation feature spaces directly allows perform adaptation even categories present datasets. especially large-scale recognition large number categories crucial beneﬁt because learn category models categories given source domain also target domain. transformations learned unsupervised manner using labels present domains maximize margin classiﬁer source transformed target data images seen test time often distribution images used learning. problem known domain shift occurs training classiﬁers object-centric internet image databases trying apply directly scene understanding tasks. consequence often severe performance degradation major barriers application classiﬁers real-world systems. paper show learn transform-based domain adaptation classiﬁers scalable manner. idea exploit implicit rank constraint originated max-margin domain adaptation formulation make optimization tractable. experiments show transformation domains efﬁciently learned data easily applied categories. begins bridge large-scale internet image collections object images captured everyday life environments. learning huge datasets comprised millions images promising directions towards closing human machine visual recognition abilities. tremendous success area large-scale visual recognition allowing learning tens thousands visual categories. however parallel researchers discovered bias induced current image databases performing visual recognition tasks across domains cripples performance although especially common smaller datasets like caltech- pascal datasets large image databases collected also introduces inherent bias. seen example comparing object images imagenet database figure object-centric data imagenet high respaper introduce novel optimization method enables transform-learning associated domain adaptation methods scale data. novel re-formulation optimization direct dual coordinate descent exploiting implicit rank constraint. although learn linear transformation between domains quadratic size number features used algorithm needs linear number operations iteration feature dimensions well number training examples. important beneﬁt compared methods need kernel space overcome high dimensionality transformation strategy impossible apply large-scale settings. obtained scalability method crucial allows transform-based domain adaptation datasets large number categories examples settings previous techniques unable reasonable time. experiments different datasets show various advantages transform-based methods generalization categories even handling domains different feature types. task domain adaptation different sets data typically considered source target domain drawn similar distinct distributions goal transfer knowledge source domain target domain. following brieﬂy review related work done areas domain adaptation well transfer learning. although transfer learning considers change conditional distribution rather change data distribution domain adaptation methods areas often similar principles ideas. domain adaptation applied different levels machine learning pipeline. example adaptive method combines target classiﬁer existing source classiﬁer linear combination continuous outputs. related adding regularization term objective forces target hyperplane parameter close source hyperplane aytar zisserman showed importance using scale-invariant similarity measure regularization term. furthermore authors proposed combination target source transductive svm. recently khosla introduced method jointly learn visual world model common across domains combination additive bias term individual domain. general classiﬁer adaptation methods often limited cases labeled training data given every class source well target domain. however often source domain training examples also labeled categories available. exploiting information learning visual classiﬁers categories target domain possible metric transformation-based methods. another line work started gopalan introduced domains points manifold subspaces. perform domain adaptation features mapped subspaces induced geodesic source target domain. yields several intermediate representations input data used learning classiﬁer. gong showed circumvent sampling ﬁnite number subspaces expressing representation kernel. contrast tommasi tackled domain adaptation problem learning shared subspace capturing domain-invariant properties categories. learning dataset done learning additional domain-speciﬁc transformation data. work saenko earliest papers investigate domain adaptation challenges visual recognition. idea work apply metric learning techniques allow estimating categoryindependent metric related target source examples used nearest neighbor classiﬁer. kulis extended work asymmetric transformations metrics using frobenius norm regularizer. major bottleneck approach number instance constraints pair source target examples need considered optimization fact transforms learned independently loss. therefore hoffman recently showed jointly learn transformation together parameters max-margin framework reduces number constraints number categories. linear transformation quadratic feature dimensionality kernelization used quadratic number training examples. scales poorly large data show experiments section intractable even modestly large-scale data. introduce method learning transformation implement comeasy apply bined large-scale architectures. scalable method applied supervised domain adaptation given source training examples goal learn linear transformation mapping target training data point source domain. transformation learned optimization framework introduces linear constraints transformed target training points information source thus generalizes methods demonstrate generality approach denote linear constraints source domain using hyperplanes denote ˜yij scalar represents measure intended similarity between target training data point ˜xj. general notation express standard transformation learning problem slack variables follows considered l-svm formulation although techniques presented paper also hold standard l-svm case. matrix regularized kernel matrix incorporating labels i.e. idea maintain update explicitly asymptotic time. note explicitly maintaining essential easily computable coordinate descent steps; therefore given change step update fulﬁlled whereas standard learning problems iteration linaer number operations feature dimensionality already provides sufﬁcient speed-up case learning domain transformations dimension source target feature space respectively features augmented training dimensionality impractical vision tasks high-dimensional input features. reason show following efﬁciently exploit implicit low-rank structure small number hyperplanes inducing constraints. note directly corresponds transformation learning problem proposed previous transformation learning techniques used bregman divergence optimization technique scales quadratically number target training examples number feature dimensions large-scale scenario considered paper impractical large number target training examples categories given well high dimensionality features. therefore show analysis dual coordinate descent optimization low-rank structure exploited allow efﬁcient optimization veriﬁed experimental evaluation. learning dual coordinate descent re-formulate vectorized optimization problem suitable dual coordinate descent allows efﬁcient optimization techniques. denote vectorized version matrix obtained concatenating rows matrix single column vector. deﬁnition write deﬁnitions equivalent soft-margin problem training t)˜n·k exploit result analysis using modifying efﬁcient coordinate descent solver proposed solves optimization problem dual form respect dual variables table asymptotic times iteration optimization single constraint taken account. source training points dimension target training points dimension computational complexity asymptotic times summarized table asymptotic time kernel bregman optimization used depends number source examples time need iteratively take constraint account independent number examples either source target domain. pass constraints takes time ﬁnally leads linear asymptotic time product number target points target dimension independent size source training set. therefore method allows using transform-based adaptation large-scale settings previous approaches unable all. identity regularizer described previous sections transformation low-rank structure using original mmdt formulation. situations small number categories restrictive class transformations. however using identity regularizer viβt allows estimate full rank matrices. efﬁcient updates coordinate descent iteration change signiﬁcantly omitted lack space. especially useful number categories small compared dimension source domain because size instead also allows efﬁcient updates computation time even independent number categories. matrix contains correlations hyperplanes also shows multi-task fashion approach vectors seen linear classiﬁers target domain matrix combines taking dependencies classes account. interesting important aspect method scenarios large number categories. linear classiﬁer mapped target domain therefore uses correlations categories similar transfer learning approaches allow efﬁcient α-updates need consider efﬁcient calculation feature vector norm section brieﬂy discuss implementation details solver used experiments code efﬁcient dual coordinate descent transform solver adapted liblinear made publicly available online. shrinking heuristics presented maintain dual variables zero optimization likely change future also implemented approach. algorithmic outline approach given figure caching techniques mentioned earlier cache scalar products allow fast computation. time vector updated cached values invalid updated next steps taken account. using fully randomized order dual variables suggested invalidation happens average every step leading probability cached value used between. reason consider random order iterate normally categories. therefore cached values blocks. convergence properties solver maintains convergence properties dual coordinate descent solvers. particular least linear convergence rate \u0001-accurate solution obtained iterations. following brieﬂy describe datasets used experiments source well target domain. imagenet ilsvrc whereas imagenet images obtained using object category names therefore contain large portion advertisement images creation database done searching scene categories labeling objects images afterwards. therefore signiﬁcant domain shift datasets fact torralba efros’s experiments consistently showed domain shift imagenet severe among pairs benchmark datasets surveyed. reason assembled challenge domain adaptation methods matching subset object categories dataset ones present hierarchy imagenet challenge matching category names datasets done using manually maintained wordnet matchings dataset using wordnet descriptions large descriptions mapped nodes wordnet subgraph related ilsvrc challenge; i.e. sets ilsvrc categories finally consider pairs labels ilsvrc category sets lead examples. leads total categories. ﬁnal examples consists tree chair cabinet table lamp curtain mountain desk fence mirror skyscraper bottle basket bench towel vase bannister ball stove bookcase magazine refrigerator bucket clock glass oven boat shoe dishwasher telephone airplane loudspeaker apparel keyboard gate bridge umbrella bicycle backpack laptop washer bathtub roof pitcher tower ﬂower apple teapot minibike printer garage guitar ashcan dune piano ship crane newspaper mouse microphone cliff bell elephant shirt toaster orange remote control knife helmet grape stick shop allow easy reproducibility results visual words features provided imagenet challenge. furthermore features database extracted computing visual words features inside given bounding boxes. also done feature extraction code provided imagenet challenge. bing/caltech dataset also bing dataset contains images category caltech dataset. contrast imagenet/sun scenario datasets created using internet search images category keywords. total dataset consists object categories. features dataset provided authors transform-based approach used largescale domain adaptation datasets achieves state-ofthe-art performance signiﬁcantly outperforming geodesic kernel method learn transformation large-scale datasets used transferring category models without target training examples even case different feature dimensions compare approach standard domain adaptation baseline linear trained target source training examples note category experiments classes training examples target domain svm-target baseline cannot used. furthermore evaluate performance geodesic kernel presented integrated nearest neighbor approach. metric learning approach shared latent space method compared approach medium-scale experiment tractable kernelized methods. experiments always source code authors. ﬁrst evaluate approach medium-scale dataset comprised ﬁrst categories bing/caltech dataset. setup also used allows compare optimization technique used also state-of-the-art domain adaptation methods data splits provided bing dataset used source domain source examples category. figure contains plot recognition results training time respect number target training examples category caltech dataset. figure shows solver signiﬁcantly faster used achieves recognition accuracy. furthermore outperforms state-of-the-art methods like learning time recognition accuracy. next experiment bing/caltech dataset imagenet/sun subset settings optimization techniques used cannot applied large number target training examples. furthermore test figure category scenario approach used learn transformation held-out categories transfer category models directly source domain without target examples. performance compared oracle svm-target mmdt target examples held-out categories. results given figure outperform geodesic method cases. focusing right plot notice method continues performance beneﬁt svm-target even number labeled target examples increases. small number training examples available several categories typical real-world datasets providing labeled training data possible categories without adaptation recognition rates less common classes cannot improved. figure shows results obtained inscene classiﬁcation provided target training examples test time given ground-truth bounding boxes context knowledge objects present image. goal algorithm assign weak labels given bounding-boxes. type scene knowledge considering images category obtain accuracy compared svm-target svm-source. contrast given exact number objects category image making problem setting difﬁcult realistic. beneﬁt method possibility transferring category models target domain even target domain examples available all. following experiment selected categories imagenet/sun dataset provided training examples source domain them. transformation figure results object classiﬁcation given bounding boxes scene prior knowledge columns show results svmsource svm-target transform-based domain adaptation using method. correct classiﬁcations highlighted green borders. ﬁgure best viewed color. figure transfer method even outperforms learning target domain labeled training examples. especially large-scale datasets like imagenet ability fast transformbased adaptation method provides huge advantage allows using visual categories provided source well target domain. furthermore experiment shows indeed learn category-invariant transformation compensate observed dataset bias transform-based domain adaptation also applied source target domain different feature dimensionality. show applicability method setting setup previous experiment computed -dimensional features objects dataset learned transformation dimensional features imagenet dataset. adaptation approach achieves recognition rate compared svm-target using target training example category. seen difﬁcult adaptation scenarios estimate domain transformation different categories completely different feature spaces. conclusions paper showed extend transformbased domain adaptation towards large-scale scenarios. method allows efﬁcient estimation categoryinvariant domain transformation cases large feature dimensionality large number training examples. done exploiting implicit low-rank structure transformation making explicit close connection standard max-margin problems efﬁcient optimization techniques them. method easy implement apply achieves signiﬁcant performance gains adapting visual recognition models learned biased internet sources real-world scene understanding datasets. important take-home message paper collecting annotated visual data necessarily help solving scene understanding general. however domain adaptation help bridge learning category-invariant transformations without signiﬁcant additional computational overhead.", "year": 2013}