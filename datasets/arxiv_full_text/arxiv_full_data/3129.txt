{"title": "Inner Product Similarity Search using Compositional Codes", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This paper addresses the nearest neighbor search problem under inner product similarity and introduces a compact code-based approach. The idea is to approximate a vector using the composition of several elements selected from a source dictionary and to represent this vector by a short code composed of the indices of the selected elements. The inner product between a query vector and a database vector is efficiently estimated from the query vector and the short code of the database vector. We show the superior performance of the proposed group $M$-selection algorithm that selects $M$ elements from $M$ source dictionaries for vector approximation in terms of search accuracy and efficiency for compact codes of the same length via theoretical and empirical analysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT features, $1M$ linear models and Netflix) demonstrate the superiority of the proposed approach.", "text": "paper addresses nearest neighbor search problem inner product similarity introduces compact code-based approach. idea approximate vector using composition several elements selected source dictionary represent vector short code composed indices selected elements. inner product query vector database vector eﬃciently estimated query vector short code database vector. show superior performance proposed group -selection algorithm selects elements source dictionaries vector approximation terms search accuracy eﬃciency compact codes length theoretical empirical analysis. experimental results large-scale datasets demonstrate superiority proposed approach. similarity search fundamental research topic area computational geometry machine learning. attracted interests computer vision pattern recognition popularity large scale highdimensional multimedia data. various technologies index structures compact codes developed solve similarity search problem diﬀerent similarity metrics paper interested designing compact code approach focus inner product similarity. inner product similarity search important task many vision applications. large scale retrieval images text queries multi-class categorization pamir approach trains large number linear models ranks queries image according scores evaluated linear models inner product similarity search problem. object detection task large number object classes consists step ﬁnding top-responded ﬁlters performing convolution operation sliding image windows ﬁlters also inner product similarity search problem. latent factor models widely used recommendation systems document matching matrix factorization latent semantic index also rely inner product similarity search best matches. propose compact code approach approximate inner product similarity search. approach based vector approximation algorithm using composition several vectors selected small approximation studied compact codes similarity search before. indices selected vectors form compact code call compositional codes describe data vector. finally inner product query vector database vector eﬃciently estimated query vector code database vector. compositional vector approximation viewed quantization algorithm ﬁnding nearest element larger dictionary produced source dictionary. study using -selection form compositional dictionary show equivalent performing -selection identical source dictionaries respectively. equivalence motivates generalize -selection using diﬀerent source dictionaries yielding so-called group -selection algorithm simultaneously learns source dictionaries performs joint -selections source dictionaries. advantage group -selection lies accurate vector approximation because larger compositional dictionary compact code length. experimental results ﬁnding similar sift features searching users similar interests discovering relevant liner models show excellent search accuracy. similarity search studied many research areas including computational geometry computer vision machine learning data mining algorithms developed approximate nearest neighbor search euclidean distance earth mover distance paper interested nearest neighbor search problem instead inner product similarity. challenges compared well-studied similarity search euclidean distance analyzed main diﬃculty inner product satisfy triangle equality makes algorithms depending suitable inner product search. cone tree based index structure designed exact inner product similarity search. fact exact search euclidean distance high-dimensional cases even slower naive linear many algorithms based compact codes similarity search euclidean distance including main categories hashing compressionbased source coding. hashing category consists random algorithms locality sensitive hashing learning based algorithms spectral hashing iterative quantization algorithms show promising performance searching euclidean distance cannot directly applied inner product similarity. compression-based source coding category includes k-means product quantization cartesian k-means shown achieve superior performance hashing codes little higher still acceptable query time cost. proposed approach belongs compression-based category speciﬁc adaptation inner product. closely related approaches product quantization cartesian k-means show constrained versions proposed approach. recent research hyperplane hashing studies problem ﬁnding points nearest hyperplane related inner product. diﬀerent maximum inner product problem approach addresses equivalent ﬁnding data vector minimum inner product query vector. concomitant hashing also able solve absolute maximum inner product problem. approximate nearest subspace search similarity based principal angles subspaces related cosine similarity search. approaches address diﬀerent problems comparable approach. paper study approximate inner product similarity search problem focus compact coding approach i.e. ﬁnding short codes represent database vectors. objective includes three aspects code representing database vector compact; similarity query vector accurately approximated using query compact code; evaluation query compact code quickly conducted. basic idea approach approximate database vector using compositional vector summation exemplar vectors exemplar vectors selected collection exemplars main work paper investigate application compact codes eﬀective eﬃcient inner product similarity approximation. suppose example represented code length proposed approach also exploits distributive property respect inner product operation addition operation distribution property evaluating inner product query compositional vector takes addition operation inner product values vectors computed whose time cost neglectable handling large scale data. vector approximation scheme using vectors expected better approximation thus yielding accurate inner product approximation. guaranteed property inner product approximation error upper-bounded vector approximation upper-bounded error property given data vector query vector distance approximation larger absolute diﬀerence true inner product approximate inner product upper-bounded upper bound rkqk related norms meaning bound depends query however solution inner product similarity search depend norm query queries diﬀerent norm solution i.e. maxx∈x maxx∈x arbitrary positive number. sense also holds accurate vector approximation potential lead better inner product similarity search. section ﬁrst introduce basic vector approximation approach kmeans clustering connect manner using -combination source dictionary approximate data vector. present proposed compositional code approach based -combination -selection group -selection. finally give analysis. k-means clustering method vector quantization. aims partition database points clusters whose centers form database point belongs cluster nearest center. application data approximation database point approximated nearest center equivalently using best -combination approximate database present basic compositional code approach uses -combination samples denoted i.e. using compositional approximate data vector manner could viewed two-step scheme ﬁrst producing compositional dictionary formed -combinations call source dictionary ﬁnding best element composite dictionary approximation data vector. instead separately learning source dictionary ﬁnding optimal -combinations similar sparse coding k-means jointly learn source dictionary combinations. k-dimensional binary vector represent combination entries valued others matrix size represent source dictionary column corresponding item source dictionary. objective function written follows relax constraint -combination elements combination distinct -selection elements necessarily diﬀerent. mathematically formulate case longer binary vector whose dimensionality represent -selection data vector concatenation subvectors subvector entry valued others objective function formulated follows furthermore extend -selection scheme so-called group selection scheme. group -selection combination elements sets matrix size taken time sets. whole formulation given following figure illustrations compositional dictionaries learnt kmeans -combination -selection group -selection random points. partition corresponds dictionary element dictionaries containing elements used form compositional dictionary. however increase code length keeps length represented code length denoted index non-zero entry implementation. note case time consumption computing similarity table becomes however large scale similarity search case increase computation cost brought increase number elements source dictionaries neglectable practice chosen small approximate inner product similarity evaluation cost large. figure illustrates compositional dictionaries k-means -combination -selection group -selection. adopt alternating optimization used lloyd sparse dictionary learning algorithms solve optimization problem equation clear denote optimization algorithm alternatively optimizes source dictionaries optimizes group -selection data vector update dictionary. objective function transformed kx−dbk data matrix group -selection matrix. quadratic optimization problem w.r.t. variable many algorithms designed solve problem. paper solve problem using closed-form solution. derivative respect zero closed-form solution online learning algorithm also borrowed acceleration. update group -selection optimizing given source dictionaries decomposed independent subproblems {minbn dbnk associated constraints optimizes group -selection separately. subproblem minimizing combinatorial problem generally np-hard. propose adopt greedy algorithm performing -selection optimizations source dictionaries best-ﬁrst manner. iteration greedy algorithm consists determining source dictionary -selection performed remaining source dictionaries selected ﬁnding best -selection selected source dictionary. former issue solved selecting best source dictionary reconstruction error given previous selected -selections minimal. latter issue solved selecting element source dictionary nearest residual -selections). given database {xn}n codes perform linear scan search nearest neighbors computing approximate similarity query database vector. inner product similarity approximated using compact code distributive property shows takes time inner products query dictionary elements already computed. aforementioned linear scan search process ﬁrst constructs similarity table storing inner products query dictionary elements whose time complexity summary overall search time complexity handling large scale data cost computing similarity table relatively small neglectable compared linear scan cost. case searching siftm time computing similarity table total search time case searching siftb ratio cost similarity table computation even much smaller. let’s transform group -selection case formulated equation cases. introduce three constraints easy show formulation equation extra constraint equivalent -selection case extra constraints reduced -combination case together three extra constraints reduced k-means case. reduction relations summarized following property. proofs ﬁrst three inequalities property obvious easily validated optimal solution -selection case feasible solution group selection case. contrast example optimal solution k-means case cannot form feasible solution -combination case compute cardinalities compositional dictionaries four cases show diﬀerence four algorithms another way. generally objective value would smaller cardinality compositional dictionary larger. cardinalities summarized follows. property shows group -selection scheme produces smallest objective values. words group -selection scheme leads accurate approximation average. based bound analysis property concluded group -selection achieve accurate inner product approximation given following corollary. time complexity. section described search process time complexity. following presents time complexity training process. training process iterative procedure iteration consists steps dictionary learning code updating. dictionary learning step updates dictionaries closed-form solution includes matrix multiplication matrix inversion time complexity code updating step involves computing code vector taking thus takes database vectors. word time complexity whole iteration process number iterations linear respect number vectors dimension training codes siftm dataset algorithm reaches convergence iterations takes seconds algorithm also beneﬁts parallel computing thus practical time consumption acceptable example computing codes learning vectors siftb completed within hours. connections discussions. summarize relations several closely-related algorithms. detailed analysis given supplementary material. product quantization cartesian k-means viewed constrained version group -selection algorithm subquantizer corresponds source dictionary approach source dictionary lies diﬀerent subspace dimension comparison order permutation similarity permutation orders used predictor closeness approach uses composition selected dictionary elements approximate vector thus uses inner product similarity approximation. proposed -selection group -selection schemes regarded sparse coding approach group sparsity coeﬃcients divided several groups sparsity constraints imposed group separately. particular coeﬃcients approach valued divided groups group non-sparsity degree conduct inner product similarity search experiments four data sets siftm siftb linear models netﬂix. siftm dataset consists -dimensional sift descriptors database vectors sift descriptors query vectors extracted inria holidays images siftb dataset contains billion sift features database vectors sift features learn vectors sift features query vectors extracted approximately million images. dataset consists around linear models weight vector database vectors learnt textual queries images frequently clicked commercial search engine textual query training samples using pamir approach -dimensional image features queries. netﬂix contains rating matrix users gave movies aims predict user ratings ﬁlms. shown inner product rating vectors users gave movies used evaluate similarity users’ interest help recommend ﬁlms users. experiments dimension rating vector reduced pca. descriptions datasets summarized table search quality measured recallp deﬁned fraction relevant instances retrieved among ﬁrst positions. relevant instances case r-nearest neighbors inner product similarity. measure equivalent precision measure evaluating returned results performing subsequent reranking scheme using exact inner product similarity retrieved items following approximate search step. true nearest neighbors inner product similarity computed comparing query database vectors using features. report performance compositional code using proposed three schemes -combination -selection group -selection. results searching diﬀerent numbers nearest neighbors diﬀerent numbers bits position shown figure result following results obtained ﬁxing tuning vary number bits. group -selection performs best consistent analysis group -selection best average vector approximation inner product approximation. addition results figure observed bits result better performance searching number nearest neighbors performance searching nearest neighbors number bits decreases. tization eigenvalue allocation scheme optimized product quantization cartesian k-means results achieved using variables compared algorithms. also report random projection based locality sensitive hashing designed cosine similarity search comparison. experimental results siftm shown figure approach superior algorithms. recall bits larger algorithms. also observe improvement bits becomes smaller bits. algorithms bits result smaller approximation error thus inner product approximation quality becomes closer. figure shows results large dataset siftb. consistent improvements approach achieves improvement recall bits -nn. comparison hashing algorithm algorithm much better case using code length. experiment indicates query time cost algorithm times hashing algorithm. however algorithm using code half length still outperforms hashing algorithm. example observed figure method using bits achieves search accuracy compared hashing using bits case time cost even smaller hashing. experimental results shown figure case inner product similarity search aims linear models query image best equivalently meaning query image relevant textual queries associated linear models. retrieved linear models viewed soft attributes applied image search ranker. respect large scale classiﬁcation approach provide fast prediction large number categories approach rather hierarchical label trees recently studied present performance show approximate search algorithms close exact search algorithm. results shown figure recall improvement bits second best others approach performs best. also show performance netﬂix dataset figure task experiment retrieves similar users viewing rating vector feature user applied mine ﬁlms query user might interested ﬁlms rated similar users. approach performs much better bits showing advantage approach small codes improvement bits little small might come code bits already able well characterize diﬀerences. point observed four comparisons approach consistently performs best algorithm always performs second best. last show advantage potential application learning large scale image classiﬁers beyond similarity search. image classiﬁcation large scale shown achieve state-of-the-art performance high-dimensional signatures show data compression necessary support eﬃcient in-ram training stochastic gradient decent training features large loaded memory normal pcs. training process needs decompress compact code pass decompressed version iteration. expected decompressed version close feature possible. thus ﬁrst closeness i.e. average feature approximation error decompressed vector) criterion. performance reported -dimensional ﬁsher vectors extracted inria holidays dataset contains query corresponding relevant images ukbench recognition benchmark images contains images. conclusions datasets hold larger scale datasets. results shown tables approach achieve best vector approximation. popular category classiﬁcation algorithms large-scale image classiﬁcation linear variants e.g. used training equivalently depends inner product approximation dual formulation based kernel matrix formed inner products training features. also compare inner product approximation accuracy ¯xj)]) shown tables inner product matter algorithm really compute inner product. addition also show approximate inner product similarities also superior preserving semantic similartable performance comparison application data compression using short codes holidays dataset. vector approximate error. ipae inner product approximation error paper studies approximate inner product similarity search problem introduces compact code approach compositional code using group selection. vector approximation approach accurate without increasing code length. similarity search eﬃcient evaluating approximate inner product query compact code computationally cheap. future generalize approach search euclidean distance similarity measures. there similar theorem showing maximum value approximate inner product condition. diﬀerently provide upper-bound approximation error present succinct proof. idea product quantization decompose space cartesian product dimensional subspaces quantize subspace separately. vector decomposed subvectors quantization dictionaries subspaces centers cmk}. vector represented concatenation centers i.e. entries zero except part corresponding subspace equal cmk. approximation vector using concatenation equivalent composition cartesian k-means extends subspace decomposition performing rotation product quantization rotated space rotation subquantizers jointly optimized. similar product quantization vector approximation cartesian k-means equivalent analysis vector approximation approach using product quantization cartesian k-means viewed constrained version approach subquantizer corresponds source dictionary approach source dictionary lies diﬀerent subspace dimension case subspace full-ranked. subspaces full-ranked equivalence still holds. order permutation index algorithm approximate nearest neighbor search. aims predict closeness data vectors according order distances towards distinguished anchor vectors data vector sorts anchor objects closest farthest i.e. permutation anchor objects {ci∗ similarity orders used predictor closeness corresponding elements. contrast approach ﬁnds partial permutation source dictionary uses composition approximate data vector yielding compact code representation. query stage distance query approximated data vector fast eﬃcient tablelookup addition operations computed approximate closeness. sparse coding basis vectors {φk} data vector linear combination basis vectors αkφk non-zero coeﬃcients coeﬃcient vector i.e. small. proposed -combination scheme viewed special sparse coding approach coeﬃcients valued sparsity ﬁxed coding group sparsity extension sparse coding coeﬃcients {φk} divided several groups sparsity constraints imposed group separately. proposed -selection group selection schemes regarded special coding approach group sparsity coeﬃcients valued divided groups group non-sparsity degree dictionary updated algorithm computing closed-form solution computation consists matrix multiplication operation matrix multiplication operation inverse operation multiplication operation note matrix k-dimensional vector entries others easily shown size size size step takes sparse matrix step transformed sparse takes instead step takes vector bnbt table average vector approximation error database vectors average inner product approximation error query vector nearest database vectors using bits. vector approximate error. ipae inner product approximation error. table average vector approximation error database vectors average inner product approximation error query vector nearest database vectors using bits. code updated optimizing separately. computed minbn dbnk solve greedy algorithm performing -selection optimizations source dictionaries best-ﬁrst manner. m-th optimization involves selecting best -selection source dictionaries contains d-dimensional exemplar vectors. costs select best exemplar vector. optimizations performed thus cost updating summary time complexity updating main paper show inner product similarity search performance four datasets. report average approximation error using composition selected vectors vector approximation database vectors average inner production approximation error using approximated vector query vector nearest database vector shown tables observe average vector approximation error approach smallest inner product approximation error also smallest. gives another evidence approach achieve best similarity search performance. adaptation cosine similarity search. inner product equivalent cosine similarity case database vectors norm. approach ﬁnds optimal composition however without making composition vector keep norm. future study approximating vector maintaining norm e.g. extending spherical k-means clustering. extension similarity search euclidean distance. experiments show using compact codes learnt approach euclidean distance based similarity search achieves better search accuracy product quantization cartesian k-means. distributive property respect euclidean distance operation addition operation hold general time cost evaluating approximate euclidean distance using codes produced approach little large. source dictionaries mutually orthogonal time cost reduced constant coeﬃcient", "year": 2014}