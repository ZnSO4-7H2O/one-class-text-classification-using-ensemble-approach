{"title": "Sequential Dimensionality Reduction for Extracting Localized Features", "tag": ["cs.CV", "cs.LG", "cs.NA", "math.NA", "stat.ML"], "abstract": "Linear dimensionality reduction techniques are powerful tools for image analysis as they allow the identification of important features in a data set. In particular, nonnegative matrix factorization (NMF) has become very popular as it is able to extract sparse, localized and easily interpretable features by imposing an additive combination of nonnegative basis elements. Nonnegative matrix underapproximation (NMU) is a closely related technique that has the advantage to identify features sequentially. In this paper, we propose a variant of NMU that is particularly well suited for image analysis as it incorporates the spatial information, that is, it takes into account the fact that neighboring pixels are more likely to be contained in the same features, and favors the extraction of localized features by looking for sparse basis elements. We show that our new approach competes favorably with comparable state-of-the-art techniques on synthetic, facial and hyperspectral image data sets.", "text": "linear dimensionality reduction techniques powerful tools image analysis allow identiﬁcation important features data set. particular nonnegative matrix factorization become popular able extract sparse localized easily interpretable features imposing additive combination nonnegative basis elements. nonnegative matrix underapproximation closely related technique advantage identify features sequentially. paper propose variant particularly well suited image analysis incorporates spatial information takes account fact neighboring pixels likely contained features favors extraction localized features looking sparse basis elements. show approach competes favorably comparable state-of-the-art techniques synthetic facial hyperspectral image data sets. linear dimensionality reduction techniques powerful tools representation analysis high dimensional data. well-known widely used principal component analysis dealing nonnegative data sometimes crucial take account nonnegativity decomposition able interpret meaningfully. reason nonnegative matrix factorization introduced shown useful several applications document classiﬁcation emission control microarray data analysis; words rows form approximate basis rows weights needed reconstruct given entries corresponding advantage basis elements interpreted data nonnegativity weights make easily interpretable activation coeﬃcients. paper focus imaging applications particular blind hyperspectral unmixing describe next section. hyperspectral image three dimensional data cube providing electromagnetic reﬂectance scene varying wavelengths measured hyperspectral remote sensors. reﬂectance varies wavelength materials energy certain wavelengths scattered absorbed different degrees referred spectral signature material; e.g. materials reﬂect light certain wavelengths others absorb wavelengths. property hyperspectral images used uniquely identify constitutive materials scene referred endmembers classify pixels according endmembers contain. hyperspectral data cube represented dimensional pixel-by-wavelength matrix rn×m columns original images converted spectral signatures pixels rows entry represents reﬂectance i-th pixel j-th wavelength. linear mixing model spectral signature pixel results additive linear combination nonnegative spectral signatures endmembers contains. case allows model hyperspectal images nonnegativity spectral signatures abundances given approximates spectral signature pixel approximated additive linear combination spectral signatures endmembers weighted coeﬃcients representing abundance k-th endmember i-th pixel. have number endmembers image. matrix called abundance matrix matrix endmember matrix. figure illustrates decomposition urban hyperspectral data cube. unfortunately opposed diﬃcult problem moreover decomposition general non-unique recomputed scratch factorization rank modiﬁed. reasons variant referred nonnegative matrix underapproximation recently proposed allows compute factors sequentially; presented next section. nonnegative matrix underapproximation introduced order solve sequentially compute rank-one factor time ﬁrst compute etc. words tries identify sparse localized features sequentially. figure decomposition urban hyperspectral image http//www.agc.army.mil/ constituted mainly endmembers road grass dirt kind roof tops trees. matrix spectral signature endmember matrix abundance corresponding endmember contains abundance pixels endmember. modiﬁcations original algorithm made adding prior information model also often done nmf; e.g. references therein. precisely variants proposed paper include sparsity constraints spatial information nmu. allows extract localized features images eﬀectively. present algorithm section show section competes favorably comparable state-of-the-art techniques synthetic hyperspectral facial image data sets. described introduction solving problem allows compute sequentially rank-one factor time preserving nonnegativity. approximate solutions rank-one obtained using lagrangian dual rn×m matrix containing lagrangian multipliers underapproximation constraints. authors prove ﬁxed problem minu≥v≥ called lagrangian relaxation equivalent minu≥v≥ equivalent scaling variables based observations original algorithm optimizes alternatively variables updates lagrangian multipliers accordingly. advantage scheme relatively simple optimal solution given written closed form original algorithm iterates following steps note given optimal rescaling rank-one factor approximate parameter example equal iteration index guarantee convergence note also original algorithm shares similarities power method used compute best rank-one unconstrained approximation; discussion ﬁrst information incorporate neighboring pixels likely contained features. addition spatial information improves decomposition images generating spatially coherent features paper anisotropic total variation regularization; e.g. deﬁne neighborhood pixel four adjacent pixels evaluate spatial coherence following function pixel neighbors. note norm used able preserve edges images opposed norm would smooth out; e.g. penalty used incorporate spatial information nmu. second information incorporated proposed algorithm feature contain relatively small number pixels. hyperspectral imaging translates fact pixel usually contains endmembers vectors abundance matrix non-zero elements. authors demonstrated incorporating sparsity prior leads better decompositions. added regularization term objective function based -norm heuristic approach order minimize non-zero entries objective function composed three terms ﬁrst relates classical least squares residual second enhances sparsity abundance vector third improves spatial coherence. regularization parameters used balance inﬂuence three terms. solution given although problem convex order approximate solution subproblem combine iterative reweighted least squares standard projected gradient scheme proposed details. finally simple block-coordinate descent scheme used good solutions problem achieved applying following alternating scheme optimizes block variables keeping ﬁxed involves non-diﬀerentiable -norm term objective function note that since hence diﬀerentiable feasible set. authors suggested iteratively re-weighted least squares approximate -norm. t-th iteration current iterate. hence non-diﬀerentiable term approximated convex quadratic diﬀerentiable term standard gradient descent scheme smooth convex optimization gradient largest singular value since matrix rather large computational costly compute exactly largest eigenvalue hence several steps power method estimate lipschitz constant choice penalty parameters general diﬃcult choose priori ‘optimal’ values penalty parameters fact diﬃcult know sparsity spatial coherence unknown localized features. moreover diﬃcult relate parameters note choice also depends scaling input matrix multiplied constant ﬁrst term increased constant. computational cost computational cost algorithm variants requires operations compute rank-one factor total operations. cost resides matrix-vector products update lagrangian multipliers hence cost linear algorithms convergence convergence analysis algorithm nontrivial combines several strategies diﬃcult analyze. fact algorithm based langragian relaxation lagrangian variables updated guarantee convergence scheme combined reweighted least squares approach approximate nonsecond part validate good performance pnmu real data sets compare algorithms widely used data sets namely cuprite hyperspectral image cbcl face data set. enhancing term abundance matrix snmf target sparsity matrix given sparsity abundance matrix obtained pnmu. hence compare pnmu snmf meaningfully comparable sparsity levels done compare snmf. running machine equipped intel xeron dual core code available online https//sites.google.com/site/nicolasgillis/code. maxiter iter experiments paper. choice first observe inﬂuence pnmu choose reasonable values parameters synthetic data sets. noise level vary figure shows average match pnmu randomly generated synthetic data sets diﬀerent values pnmu identiﬁes perfectly four materials. snmu performs relatively well returns basis elements salt-and-pepper noise. lnmu returns spatially coherent basis elements mixture several materials snmf return spatially less coherent basis elements snmf returns localized ones identifying relatively well materials remark code generate synthetic compare diﬀerent algorithms available https sites. google. com/ site/ nicolasgillis/ moreover easily change noise level number size materials number wavelengths. values chosen correspond high noise level pnmu still performs perfectly below pnmu outperforms approaches reasonable values noise level. percent diﬀerent algorithms. observe values smaller pnmu outperforms approaches able generate basis elements match much smaller higher noise levels performance pnmu degrades rapidly. snmf performs relatively well although match always higher high noise levels performs better pnmu although values noise basis elements generated relatively poor. cuprite hyperspectral image. surprisingly already observed previous section parameters play critical role. however opposed classical completely unsupervised prior information require human supervision tuning parameters choosing good trade-oﬀ reconstruction accuracy spatial coherence sparsity features. adding constraint factorization process leads increase approximation error constrained variants return sparser and/or spatially coherent solutions. already noted fair compare directly approximation error computes solution sequentially. order compare quality generated sparsity patterns postprocess solutions obtained algorithms optimizing non-zero entries refer improved relative approximation post-processing step. sets experimentation conducted. ﬁrst experiments show behavior proposed method test eﬀectiveness correctly detecting endmembers widely used hyperspectral image cuprite image reducing noise abundance maps. second experiment shows eﬀectiveness pnmu correctly detecting parts facial images widely used data literature namely mit-cbcl face data set. cuprite data widely used assess performance blind hyperspectral unmixing techniques. represents spectral data collected mining area southern nevada cuprite. sensitivity pnmu parameters show sensitivity pnmu parameters show wide range abundance elements diﬀerent values extract abundance columns penalty spatial coherence varies observe increasing improves spatial coherence abundance images. however high values lead blurred images furthermore necessary point sparsity locality constraints inﬂuence other. indeed higher sparsity implies less pixels present abundance element improves spatial coherence lowest value sparsity abundances second highest value spatial coherence pnmu abundance maps below) even images rather noisy edges deﬁning materials always well identiﬁed; e.g. abundance elements sub-ﬁgure generates sparser spatially coherent features relative error comparable bases nmu. particular edge delimitation several materials better deﬁned lnmu abundance maps although still rather noisy relatively dense sub-ﬁgure edges materials still well deﬁned fact compared snmu quite naturally increases sparsity abundances improves spatial coherence although relative error slightly increased observed images less noisy edges materials well deﬁned hence pnmu combines advantages methods data set. fact able identify many endmembers materials identiﬁed) among which left right bottom desert varnish alunite chalcedony hematite alunite kaolinite goethite amorphous iron oxydes goethite chalcedony montmorillonite muscovite kaolinite k-alunite kaolonite buddingtonite. previous experiments show eﬀectiveness proposed method identifying materials compose hyperspectral image. apply pnmu mit-cbcl face database faces pixels each. widely used data literature used figure displays abundance images obtained diﬀerent algorithms table reports numerical results. hyperspectal images pnmu provides good trade-oﬀ reconstruction error sparsity spatial coherence. lnmu generate mixed abundances diﬀerent parts faces represented sparse methods able detect unmixed parts faces among these parts returned pnmu paper variant proposed namely pnmu taking account sparsity spatial coherence abundance elements. numerical experiments shown eﬀectiveness pnmu correctly generating sparse localized features images better trade-oﬀ sparsity spatial coherence reconstruction error.", "year": 2015}