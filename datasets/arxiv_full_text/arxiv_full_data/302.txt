{"title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual  Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.", "text": "figure propose novel splits datasets vqa-cp vqa-cp respectively enable stress testing models mismatched priors train test. vqa-cp datasets created distribution answers question type design different train test existing models tend largely rely strong language priors existing datasets suffer signiﬁcant performance degradation vqacp datasets. propose novel model built existing model explicitly grounds visual concepts images consequently signiﬁcantly outperforms vqa-cp datasets. gence. visual question answering poses rich challenges spanning various domains computer vision natural language processing knowledge representation reasoning. last years received attention number datasets curated variety deep-learning models developed however number studies found despite recent progress today’s models heavily driven superﬁcial correlations training data lack sufﬁcient visual grounding seems faced difﬁcult learning problem models typically resort latching onto language priors training data point ignoring image e.g. overwhelmingly replying ‘how many questions ‘what color ‘white’ ‘yes’. number studies found today’s visual question answering models heavily driven superﬁcial correlations training data lack sufﬁcient image grounding. encourage development models geared towards latter propose setting every question type train test sets different prior distributions answers. speciﬁcally present splits datasets call visual question answering changing priors first evaluate several existing models setting show performance degrades signiﬁcantly compared original setting. second propose novel grounded visual question answering model contains inductive biases restrictions architecture speciﬁcally designed prevent model ‘cheating’ primarily relying priors training data. speciﬁcally gvqa explicitly disentangles recognition visual concepts present image identiﬁcation plausible answer space given question enabling model robustly generalize across different distributions answers. gvqa built existing model stacked attention networks experiments demonstrate gvqa signiﬁcantly outperforms vqa-cp vqa-cp datasets. interestingly also outperforms powerful models multimodal compact bilinear pooling several cases. gvqa offers strengths complementary trained evaluated original datasets. finally gvqa transparent interpretable existing models. fundamentally problematic nature train-test splits presence strong priors. result models intrinsically memorize biases training data demonstrate acceptable performance test set. problematic benchmarking progress becomes unclear source improvements models learned ground concepts images driven memorizing priors training data. help disentangle factors present splits datasets called visual question answering changing priors splits created re-organizing train splits respective datasets distribution answers question type design different test split compared train split important thing note change distribution underlying perceptual signals images train test. generalization across different domains images active research area focus work. change distribution answers question type train test. hypothesis reasonable expect models answering questions ‘right reasons’ recognize ‘black’ color test time even though ‘white’ popular answer ‘what color questions train fig. demonstrate difﬁculty vqa-cp splits report performance several existing models splits. ﬁnding performance tested existing models drops signiﬁcantly trained evaluated splits compared original splits ﬁnding provides conﬁrmation novel insight growing evidence literature behavior models also propose novel grounded visual question answering model contains inductive biases restrictions architecture speciﬁcally designed prevent ‘cheating’ primarily relying priors training data gvqa motivated intuition questions provide pieces information recognized? visual concepts image need reasoned answer question said? space plausible answers signals. learning question-answer pairs plausible color plate white test time rely correlation speciﬁc plate image question about. gvqa explicitly disentangles visual concept recognition answer space prediction. gvqa built existing model stacked attention networks experiments demonstrate gvqa signiﬁcantly outperforms types questions proposed vqa-cp datasets interestingly also outperforms powerful models multimodal compact bilinear pooling several cases also show gvqa offers strengths complementary trained evaluated original datasets finally gvqa transparent existing models produces interpretable intermediate outputs unlike existing models countering priors order counter language priors dataset balance every question collecting complementary images every question. thus every question proposed dataset similar images different answers question. construction language priors signiﬁcantly weaker dataset. however train test distributions still similar. ‘white’ popular color train test ‘tennis’ popular sport popular count etc. leveraging priors train still beneﬁt model test time. balance yes/no questions abstract scenes dataset similar manner. recently propose evaluation metrics compensate skewed distribution question types skewed distribution answers within question type test set. works indicate increasing interest community focus models less driven training priors visually grounded. compositionality. related ability generalize across different answer distributions ability generalize novel compositions known concepts learned training. compositionality studied various forms vision community. zero-shot object recognition using attributes based idea composing attributes detect novel object categories studied compositionality domain image captioning focusing structured representations study compositionality domain synthetic images questions limited vocabulary objects attributes. recently propose compositional split dataset called c-vqa consists real images questions test extent existing models answer compositionally novel questions. however even cvqa splits distribution answers question type change much train test. hence models relying priors still generalize test set. developed neural module networks consist different modules specialized particular task. modules composed together based question structure create model architecture given question. report performance model vqa-cp datasets performance degrades signiﬁcantly original setting proposed setting zero-shot also explored study setting test questions contain least unseen word. propose answering questions unknown objects orthogonal efforts work focus studying unseen words/concepts recognized testing. instead interested studying extent model visually grounded evaluating ability generalize different answer distribution question type. splits ensure concepts seen test time present training extent possible. vqa-cp vqa-cp splits created distribution answers question type different test data compared training data. splits created re-organizing training validation splits datasets respectively using following procedure question grouping questions question type ground truth answer grouped together. instance {‘what color dog?’ ‘white’} {‘what color plate?’ ‘white’} grouped together whereas {‘what color dog?’ ‘black’} different group. grouping done merging pairs train splits. question types provided datasets. greedily re-splitting greedy approach used redistribute data points vqa-cp train test splits maximize coverage vqa-cp test concepts vqa-cp train split test splits datasets creation vqa-cp splits requires access answer annotations publicly available test sets. peer review work team possibility creating vqa-cp test split using competition track future challenges. figure distribution answers question type vary signiﬁcantly vqa-cp train test splits. instance ‘white’ ‘red’ commonly seen answers train ‘what color’ ‘black’ frequent answer test. computed random sample questions. making sure questions question type ground truth answer repeated test train splits. procedure loop groups created above every iteration current group vqa-cp test split unless group already assigned vqa-cp train split. always maintain concepts belonging groups vqa-cp test split covered groups vqa-cp train split. pick group covers majority concepts groups assigned either split group vqa-cp train split. approach results coverage test question concepts train split vqa-cp coverage test answers train split’s answers vqa-cp vqacp train consists images questions answers vqa-cp test consists images questions answers fig. shows distribution answers several question types ‘what color’ ‘what sport’ ‘how many’ etc. train test splits vqa-cp dataset distributions answers given question type signiﬁcantly different. instance ‘tennis’ frequent answer question table compare performance existing models vqa-cp test splits performance splits performance tested existing models degrades signiﬁcantly setting compared original setting. type ‘what sport’ vqa-cp train split whereas ‘skiing’ frequent answer question type vqa-cp test split. however dataset distribution given question type similar across train splits vqa-cp splits similar differences seen question types well ‘are’ ‘which’. demonstrate difﬁculty vqa-cp splits report performance following baselines existing models trained vqa-cp vqa-cp train splits evaluated corresponding test splits. compare performance trained train splits evaluated corresponding splits. results presented table q-type prior predicting popular training answer corresponding question type deeper lstm question predicting answer using question alone deeper lstm question normalized image baseline model. neural module networks model designed compositional nature. stacked attention networks widely used models vqa. note that ideally performance baseline vqa-cp test zero answers given question type different test train. inter-human disagreement datasets performance slightly higher multimodal compact bilinear pooling winner challenge brief descriptions models appendix table performance tested existing models drops signiﬁcantly vqacp setting compared original setting. note even though architecture compositional design performance degrades vqa-cp datasets. posit additional lstm encoding question encode priors dataset. also note d-lstm norm model suffers largest drop overall performance compared models perhaps models powerful visual processing another interesting observation table ranking models based overall performance changes vqa-cp outperforms whereas vqa-cp outperforms san. brief discussion trends different question types please appendix model introduce grounded visual question answering model previous approaches directly image-question tuples answers gvqa breaks task steps look locate object image patch needed answer question recognize visual concepts patch answer identify space plausible answers question return appropriate visual concept recognized visual concepts taking account concepts plausible. instance gvqa asked ‘what color dog?’ identiﬁes answer color name locates patch image corresponding recognizes various visual concepts ‘black’ ‘dog’ ‘furry’ ﬁnally outputs concept ‘black’ recognized concept corresponding color. another novelty gvqa treats answering yes/no questions visual veriﬁcation task i.e. veriﬁes visual presence/absence concept mentioned question. instance gvqa asked person wearing shorts?’ identiﬁes concept whose visual presence needs veriﬁed ‘shorts’ answers ‘yes’ ‘no’ depending whether recognizes shorts image gvqa depicted figure given question image question ﬁrst goes question classiﬁer gets classiﬁed yes/no yes/no. yes/no questions gvqa components activated visual concept classiﬁer takes input image features extracted qmain given question extractor answer cluster predictor whose input entire question. outputs answer predictor produces answer. yes/no questions gvqa components activated concept extractor whose input entire question. outputs visual veriﬁer predicts ‘yes’ ‘no’. present details component below. visual concept classiﬁer responsible locating image patch needed answer question well producing visual concepts relevant located patch. e.g. given ‘what color next car?’ responsible attending region outputting concepts ‘bus’ attributes color count etc. consists -hop attention module based stacked attention networks followed stack binary concept classiﬁers. image attention module form activations last pooling layer vgg-net prevent memorization answer priors question type question ﬁrst passed language extractor remove question type substring produce qmain embedded using lstm attention module. multi attention produces weighted linear combination image region features vgg-net weights corresponding degree attention region. followed fully connected layers stack binary concept classiﬁers cover concepts seen train. trained binary logistic loss every concept. concepts constructed extracting objects attributes pertinent answer training pairs retaining frequent ones. object concepts grouped single group attribute concepts clustered multiple small groups using kmeans clustering glove embedding space total clusters. concept clustering required purpose generating negative samples required train concept classiﬁers since question indicate objects attributes absent image negative data generated using following assumptions attended image patch required answer question dominant object every object dominant attribute attribute category given assumptions concept cluster treated positive concepts cluster treated negatives. note subset concept clusters activated question training activated clusters contribute loss. question classiﬁer classiﬁes input question categories yes-no yes-no using glove embedding layer lstm layers. yes-no questions feed rest feed acp. answer cluster predictor identiﬁes type expected answer activated yes/no questions. consists glove embedding layer lstm followed layers classify questions clusters. clusters created k-means clustering answer classes embedding answer glove space. concept extractor extracts question concepts yes/no questions whose visual presence needs veriﬁed image using based extraction system. e.g. cone green?’ extract ‘green’. extracted concept embedded glove space followed layers transform embedding space concepts combined please description below. answer predictor given visual concepts predicted concept category predicted ap’s role predict answer. categories correspond concept clusters given alignment output easily mapped vector dimensions output simply copying dimensions positions pertaining respective cluster dimensions. resulting embedding added element-wise embedding followed layers softmax activation yielding distribution answer categories visual veriﬁer given visual concepts predicted embedding concept whose visual presence needs veriﬁed vv’s role verify presence/absence concept vcc’s predictions. speciﬁcally embedding added element-wise embedding followed layers softmax activation yielding distribution categories ‘yes’ ‘no’. predictor predictions training data. training ground truth labels yes/no yes/no questions question classiﬁer. testing ﬁrst question classiﬁer classify questions yes/no yes/no. feed questions respective modules obtain predictions test set. please refer appendix implementation details. experimental results experiments vqa-cp vqa-cp model accuracies table shows performance gvqa model comparison vqa-cp vqa-cp datasets using evaluation metric accuracies presented broken yes/no number categories. seen table proposed architectural improvements show signiﬁcant boost overall performance vqa-cp vqa-cp datasets. worth noting owing modular nature gvqa architecture easily swap attention modules vcc. interestingly vqa-cp dataset gvqa also outperforms overall metric spite built relatively simpler attention module using relatively less powerful image features compared resnet- used mcb. vqa-cp dataset gvqa outperforms overall metric yes/no number questions. performance model components question classiﬁer vqa-cp test lstm based question classiﬁer obtains accuracy. top- test accuracy questions whose answers attribute clusters questions whose answers object clusters. top- accuracy rises note accuracies computed using automatically created clusters. weighted mean test score across classiﬁers individual concepts weighted number positive samples reﬂecting coverage concept test set. please refer appendix accuracies vqa-cp dataset. order evaluate role various gvqa components report experimental results replacing component gvqa traditional counterpart i.e. modules used traditional models instance gvqa lstm represents model gvqa replaced lstm. results presented table along result full gvqa model reference. gvqa qmain performance model entire question less proposed model note even feeding entire question gvqa outperforms thus demonstrating removing question type information helps isn’t main factor behind better performance gvqa. gvqa lstm replace lstm using loss). overall performance drops drop yes/no questions. expected result given table shows gvqa signiﬁcantly outperforms yes/no questions crucial component yes/no pipeline. gvqa vccloss remove loss treat output layer intermediate layer whose activations passed answer predictor trained end-to-end using loss. overall performance improves biggest improvement performance questions suggests introducing visual concept loss model pipeline hurts. although removing loss training end-to-end loss achieves better performance model longer transparent using loss design choice would make based desired accuracy interpretability trade off. gvqa vccloss lstm replacing lstm gvqa vccloss hurts overall performance biggest drop other questions suggests helps signiﬁcantly absence loss addition adds interpretability gvqa. also trained evaluated gvqa train splits datasets. results presented table gvqa achieves overall accuracy less san. surprising well-established heavy language priors existing models memorize train exploit test whereas gvqa designed vision improves grounded models like gvqa show improved performance models leverage priors training data. moreover important note gain vqa-cp much higher loss gvqa performs overall less gvqa outperforms yes/no questions. shows priors weaker gvqa san’s performance decreases. also trained evaluated gvqavccloss datasets found performs worse gvqa order check whether gvqa strengths complementary computed oracle san’s gvqa’s performance oracle i.e. pick predictions model higher accuracy test instance. seen table oracle overall performance higher suggesting gvqa complementary strengths. also note oracle higher oracle suggesting gvqa’s complementary strengths another model inspired this report performance ensemble gvqa table ensemble combines outputs models using product conﬁdences model. gvqa outperforms overall especially better yes/no questions. also found ensemble gvqavccloss performs worse datasets hence gvqa better complement gvqavccloss addition transparent. figure qualitative examples gvqa model. left predicts plausible clusters given question. predicts various visual concepts. show concepts predicted corresponding predicted cluster. given predictions answer predictor able extract correct answer ‘baseball’. right smiling concept extracted whose visual presence vcc’s predictions veriﬁed visual veriﬁer resulting ‘yes’ ﬁnal answer. figure left gvqa explain predicted ‘green’ given image given question. given image bananas green many food since asked color said ‘green’. san’s prediction ‘yellow’ unlike gvqa san’s architecture doesn’t facilitate producing explanation makes difﬁcult understand saying saying. right gvqa incorrectly answer question seen gvqa incorrect perhaps says ‘black’ instead ‘blue’. time also says ‘pants’. perhaps looking right region image still answering incorrectly suggesting color classiﬁer needs improvement. existing models provide insights. architecture design gvqa makes transparent existing models produces interpretable intermediate outputs unlike existing models. show example predictions gvqa fig. intermediate outputs provide insights gvqa predicting predicting hence enable system designer identify causes error. easy existing models. fig. shows examples comparing contrasting gvqa’s intermediate outputs help explain successes failures possible existing models. appendix examples. conclusion propose setting every question type train test sets different prior distributions answers. introduce novel splits existing datasets stress test models changing priors. finally propose novel grounded model outperforms strong baselines proposed splits provides strengths complementary existing models original splits. gvqa transparent interpretable existing models. gvqa ﬁrst step towards building models visually grounded design. future work involves developing models utilize best worlds answering question based knowledge priors world conﬁdence answer predicted result visual grounding low. acknowledgements. thank yash goyal useful discussions proposed splits model. work funded part career awards awards grant grant n--- allen distinguished investigator award paul allen family foundation google faculty research awards amazon academic research awards views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor. fig. shows distribution answers several question types ‘what color’ ‘what sport’ ‘how many’ etc. train test splits vqa-cp dataset distributions answers given question type signiﬁcantly different train test. instance ‘tennis’ frequent answer question type ‘what sport’ vqa-cp train split whereas ‘baseball’ frequent answer question type vqa-cp test split. similar differences seen question types well ‘does’ ‘which’. figure distribution answers question type vary signiﬁcantly vqa-cp train test splits. instance ‘white’ ‘black’ commonly seen answers train ‘what color’ ‘red’ frequent answer test. computed random sample questions. deeper lstm question predicting answer using question alone encodes question using lstm passes encoding multi-layered perceptron classify answers. deeper lstm question normalized image baseline model. model consists multi-layered perceptron normalized image embeddings question embeddings produces distribution answers. neural module networks model designed compositional nature. model consists composable modules module speciﬁc role given image natural language question image decomposes question linguistic substructures using parser determine structure network required answer question. multimodal compact bilinear pooling winner challenge uses multimodal compact bilinear pooling predict attention image features also combine attended image features question features. question-type trends model performance vqacp examining accuracies models different question types shows performance drop vqa-cp larger question types others. vqa-cp models show signiﬁcant drop questions questions vqa-cp test split correct answer ‘yes’ whereas prior answer questions starting vqa-cp train split ‘no’. models tend answer vqa-cp test questions ‘no’ driven prior training data. examples question types vqa-cp resulting signiﬁcant drop performance models you’ ‘are there’ ‘how many people are’ ‘what color the’ ‘what sport ‘what room etc. examples question types vqa-cp resulting drop performance models ‘are there’ ‘how many people ‘what color the’ ‘what animal ‘what the’ etc. question classiﬁer single layer lstm hidden state train using binary crossentropy loss. answer cluster predictor single layer lstm hidden state train using cross-entropy loss visual concept classiﬁer single layer lstm hidden state encode qmain vgg-net extract activations last pooling layer -hop attention architecture binary cross-entropy loss train classiﬁer vcc. given training instance subset concept clusters activated activated clusters contribute loss. question classiﬁer rmsprop optimizer base learning rate answer predictor visual veriﬁer adam optimizer base learning rate respectively. implementation using torch question classiﬁer vqa-cp test lstm based question classiﬁer obtains accuracy. top- test accuracy questions whose answers attribute clusters questions whose answers object clusters. accuracy rises note accuracies computed using automatically created clusters. weighted mean test score across classiﬁers individual concepts weighted number positive samples reﬂecting coverage concept test set. table presents full results three models gvqa gvqavccloss along ensembles oracle performances. gvqavccloss performs worse gvqa similar gvqa addition interpretability gvqa overall better gvqavccloss original splits. another observation gvqavccloss oracle san)’s overall performance higher suggesting gvqavccloss strengths complementary note oracle san) higher oracle suggesting gvqavccloss’s complementary strengths another model inspired this report performance ensemble gvqavccloss san) table ensemble combines outputs models using product conﬁdences model. unlike gvqa outperform hence gvqa better complement gvqavccloss addition transparent. fig. fig. show qualitative examples vqa-cp test along gvqa’s san’s predicted answers. also shown intermediate outputs gvqa provide insights gvqa figure transparency gvqa. examples gvqa’s intermediate predictions help explain predicted predicted. top-left predicts following visual concepts blue person skiing jacket. predicts cluster corresponding colors. finally gvqa predicts ‘blue’ answer. gvqa predicts ‘blue’ because visual concepts predicted ‘blue’ represents color. looking attention maps indicate gvqa seeing blue san’s prediction ‘orange’ unlike gvqa san’s architecture facilitate producing explanation makes difﬁcult understand saying saying. top-right gvqa looking regions covered snow gvqa correctly predicts ‘winter’ whereas incorrectly predicts ‘summer’ unclear why. bottom-left concept extractor predicts ‘happy’ whose visual presence veriﬁed looking region corresponding kid’s face. bottom-right gvqa focuses larger part scene correctly recognizes ‘bathroom’. figure transparency gvqa. examples gvqa incorrectly answer question. however gvqa’s intermediate predictions help explain incorrect. top-left gvqa vcc’s predictions indicate perhaps looking ﬁeld veriﬁed attention map. san’s attention suggests looking ball still explain predicting ‘soccer’. perhaps confusing ball soccer ball. top-right attention maps gvqa look similar other. however looking acp’s vcc’s prediction suggest indeed seeing ‘pasta’ still predicting ‘carrots’ incorrectly predicting cluster corresponding vegetables instead cluster corresponding pasta. bottom gvqa looking smartphone incorrectly answers ‘no’ recognize phone smartphone. however correctly predicts ‘phone’ ‘electronic’ ‘black’ ‘right’.", "year": 2017}