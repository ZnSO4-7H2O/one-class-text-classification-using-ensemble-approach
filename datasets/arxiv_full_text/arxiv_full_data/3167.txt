{"title": "Learning Efficient Structured Sparse Models", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.", "text": "present comprehensive framework structured sparse coding modeling extending recent ideas using learnable fast regressors approximate exact sparse codes. purpose propose eﬃcient feed forward architecture derived iteration block-coordinate algorithm. architecture approximates exact structured sparse codes fraction complexity standard optimization methods. also show using diﬀerent training objective functions proposed learnable sparse encoders restricted approximants exact sparse code pre-given dictionary rather used full-featured sparse encoders even modelers. simple implementation shows several orders magnitude speedup compared state-of-the-art exact optimization algorithms minimal performance degradation making proposed framework suitable real time largescale applications. sparse coding problem representing signals sparse linear combination elementary atoms given dictionary. sparse modeling aims learning parametric dictionaries data themselves. addition attractive theoretical level large class signals well described model demonstrated numerous state-of-the-art results diverse applications. main challenge optimization-based sparse coding modeling approaches relatively high computational complexity. consequently signiﬁcant amount eﬀort devoted developing efﬁcient optimization schemes despite permanent progress reported literature state-of-the-art algorithms require tens hundreds iterations converge making infeasible real-time large applications. recent works proposed trade precision sparse representation computational speed learning non-linear regressors capable producing good approximations sparse codes ﬁxed amount time. insightful work introduced novel approach regressors multi-layer artiﬁcial neural networks particular architecture inspired successful optimization algorithms solving sparse coding problems. regressors trained minimize predicted exact codes given training set. unlike previous predictive approaches system introduced architecture capable producing accurate approximations true sparse codes since allows approximate explaining away take place inference details). paper propose several extensions including consideration general sparse coding paradigms adding online adaptation underlying dictionary/model thereby extending applicability fast encoding framework. proposed approach used predeﬁned dictionary learn online manner data vectors several important structured sparsity settings cast particular cases sparse coding mentioned above often referred lasso basis pursuit group sparse coding generalization standard sparse coding cases dictionary sub-divided groups known active inactive simultaneously groups overlap completely included one; collaborative sparse coding generalizing concept structured sparse coding collections input vectors promoting given patterns non-zero elements coeﬃcient matrix state-of-the-art approaches solving rely family proximal splitting methods references therein). next brieﬂy introduce proximal methods algorithm solving hierarchical sparse coding problems used construct trainable sparse encoders. convex diﬀerentiable lipschitz continuous gradient convex extended real valued possibly non-smooth. clearly problem falls category considering dictionary learned framework related recent eﬀorts producing based sparse representations references therein. interpreted online trainable sparse auto-encoder sophisticated encoder simple linear decoder. higher complexity proposed architecture encoder allows system produce accurate estimates true structured sparse codes. section brieﬂy present general problem hierarchical structured sparse coding section discuss optimization algorithm used inspire architecture encoders. section present sparse encoders objective functions used training. experimental results real audio image analysis tasks presented section finally conclusions drawn section underlying assumption sparse models input vectors reconstructed accurately linear combination basis vectors small number non-zero coeﬃcients. structured sparse models further assume pattern non-zero coeﬃcients exhibits speciﬁc structure known priori. rm×p dictionary m-dimensional dexes then deﬁne group structure collection groups atoms g|g|}. input vector corresponding structured sparse code associated group structure obtained solving regularizer function seen generalization regularizer used standard sparse coding latter arises special case singleton groups {{}{} {p}} setting such eﬀect groups natural generalization obtained standard sparse coding turns atoms groups lasso problem particular case hilasso proximal operator reduces scalar soft-thresholding operator algorithm corresponds popular iterative shrinkage thresholding algorithm cording choose block coordinate strategy block updated time paper refer algorithm block-coordinate forward-backward algorithm review similar algorithms). iterates bcofb forward-backward method becomes particularly interesting proximal operator computed exactly eﬃciently e.g. standard group-structured sparse coding. groups directly. however exist important exceptions hierarchical setting tree-structured groups discussed sequel. accelerated versions proximal methods largely studied literature improve convergence rate variants fastest exact solvers available still require tens hundreds iterations achieve convergence. following sections elaborate standard versions algorithm since interested constructing architecture proposed sparse encoders. simplify notation henceforth formulate derivations case two-level hierarchical sparse coding referred hilasso captures essence hierarchical sparse models generalization layers collaborative scheme straightforward. approximations typically useless. still even learning parameters hopeless expect regressor produce good sparse codes input data. showed network approximate well sparse codes input vectors coming distribution used training. rameters inputs. diﬀerentiability respect parameters allows gradient descent methods training diﬀerentiability respect inputs allows backpropagation gradients sparse encoders modules bigger globally-trained systems. extend gregor&lecun’s idea hierarchical sparse code regressors. consider feed-forward architecture based bcofb layer implements single iteration bcofb proximal method encoder architecture depicted figure layer essentially consists nonlinear proximal operator followed group selector linear operation corresponding group. network parameters initialized algorithm particular case architecture obtained. followed gregor&lecun considering encoders regressors whose role reproduce faithfully possible ideal sparse codes produced iterative sparse coding algorithm achieved training networks minimize discrepancy outputs network corresponding propose consider neural network sparse coders regressors approximating iterative algorithm full-featured sparse encoders right. achieve paradigm shift abanideal sparse codes introduce alternative updated k-th iteration according selection rule. inspired coordinate descent algorithm introduced standard sparse coding propose heuristic variant bcofb algorithm updates group order make sparse coding feasible real time settings recently proposed learn non-linear regressors capable producing good approximations sparse codes ﬁxed amount time main idea construct parametric regressor parameters collectively denoted minimizes loss function training xn}. here exact sparse code obtained solving lasso problem approximation. setting generic application oﬀ-the-shelf regressors later shown produce relatively low-quality approximations proposed particular regressors implemented truncated form ista algorithms. essentially regressors multi-layer artiﬁcial nn’s layer implements single iteration ista cod. example architecture learned parameters network matrices element-wise thresholds naturally alternative learning could simply parameters prescribed algorithm terminating small number iterations. however means guaranteed truncated algorithm produce best sparse code approximation number layers; practice without learning truncated adapting network parameters newly arriving data using online learning algorithm ﬁxing sparse codes adapting dictionary using online dictionary learning algorithm note stages completely free iterative sparse coding translates latency computational complexity allowing real time applications. proposed sparse modeling framework allows naturally incorporate side information training data vectors making learning supervised. space limitations prevent elaborating setting; follows outline several examples leaving details extended version paper. group hierarchical lasso case know data vector desired active groups. incorporating information training objective possible using separately training vector values promote activation knowingly active group high values discourage activation knowingly inactive group. applications data vectors come pairs knowingly similar dissimilar vectors want minimize natural distance sparse codes similar vectors maximizing distance dissimilar ones. scenario particular interest retrieval applications sparse data representations desirable amenability eﬃcient indexing. incorporating similarity preservation term training objective common practice metric learning challenging sparse coding fact sparse codes produced iterative algorithm faces problem finally many applications data euclidean structure supervised learning used construct optimal discriminative metric. achieved example replacing euclidean ﬁtting term mahalanobis counter discriminative projection matrix. scenarios desirable combine sparse modeling metric learning. problem considered general sparse coding problem viewed mapping data vector corresponding sparse code minimizing aggregate ﬁtting term regularizer since latter objective trusted indication code quality train network minimize ensemble average training replaced obtaining objective given application therefore select objective appropriate regularizer corresponding problem structure sparse encoder architecture consistent structure train latter minimize objective representative data vectors. found selecting sparse encoder structure consistent training objective inherent structure problem crucial production high-quality sparse codes. sparse encoders based nn’s trained minimizing non-convex function training therefore prone local convergence overﬁtting argue practical problems dictionary also found solving non-convex dictionary learning problem based representative data distribution. consequently unless dictionary constructed using domain knowledge sparse encoders conceptually diﬀerent using iterative sparse modeling algorithms. rameters alternating network training dictionary update iterations. essentially extends proposed eﬃcient sparse coding framework full-featured sparse modeling detailed next. interpreting nn’s standalone sparse encoders removing reference exact sparse codes makes training problem completely unsupervised. consequently train network data vectors sparse coding. allows using proposed framework online learning applications. full online sparse modeling scenario consists initializing dictionary ﬁxing dictionary training objective nn’s implemented matlab built-in acceleration executed state-of-the-art intel xeon nvidia tesla gpu. even means optimized code propagation -dimensional vector layer structured network proposed bcofb architecture takes seconds equivalent .µsec spent vector layer. several orders magnitude faster exceptionally optimized multithreaded spams hilasso code executed cpu. beneﬁts parallelization possible ﬁxed datapath complexity encoder compared iterative solver. experiments training performed using gradient descent safeguarded armijo rule. refer sparse encoders obtained minimizing gregor&lecun’s objective function error output exact encoder. explicitly stated sparse encoders trained using speciﬁc objective function figure performance diﬀerent sparse encoders measured using lasso objective function sample number online learning experiment. shown three groups patches corresponding diﬀerent texture images brodatz dataset. following sparse coders compared exact sparse codes unstructured unstructured lasso networks trained class; contained layers. used lasso objective. dictionaries atoms used. increase dictionary size exhibit signiﬁcant performance improvement. table summarizes misclassiﬁcation rates sparse encoders. performance sparse encoder decreases increase dictionary size discrepancy exact codes drops. hand better performance terms lasso objective consistently correlates better classiﬁcation rates makes lasso favorable choice. dictionary adaptation training lasso encoder brings small improvement performance diminishing dictionary size. attribute relative complexity data. patches three images brodatz texture dataset patches ordered three consecutive blocks patches image. dictionary size ﬁxed atoms. used lasso objective. pared standard online dictionary learning unstructured lasso dictionary adaptation given window initialized network parameters previous windows. latter case dictionary initialized random subset ﬁrst data vectors reference also compared following three oﬄine algorithms trained distinct patches extracted images standard dictionary learning unstructured unstructured lasso nn’s used layers. performance measured terms lasso objective reported figure exact oﬄine sparse encoder achieved best results among oﬄine encoders. however outperformed exact online encoder ability adapt dictionary speciﬁc class data. performance lasso online encoder slightly inferior exact lasso oﬄine; online version performs better network parameters dictionary adapt current class data. finally oﬄine encoder lowest signiﬁcantly inferior performance. experiment shows that drop performance compared exact encoder relatively computational complexity lasso online encoder tremendously lower ﬁxed. ﬁrst evaluate performance structured sparse encoders speaker identiﬁcation task reproduced application authors hilasso automatically detect present speakers given mixed signal. repeat experiments using proposed eﬃcient structured sparse encoders instead. dataset consists recordings diﬀerent radio speakers females three males. samples used training rest testing. within testing data sets waveforms created containing isolated speakers another containing possible combinations mixtures speakers. signals decomposed overlapping local time frames samples overlap properties signal remain stable within frame. -dimensional feature vector obtained audio frame short-time power spectrum envelope details). five undercomplete dictionaries atoms trained single speaker minimizing lasso objective combined single structured dictionary containing atoms. increasing dictionary size exhibited negligible performance beneﬁts. speaker identiﬁcation performed ﬁrst encoding test vector structured dictionary measuring energy groups. energies sum-pooled time samples selecting labels highest two. following structured sparse encoders compared exact hilasso codes unstructured trained exact hilasso codes structured trained codes structured network discriminative cost function regularization term weights independently data vector promote discourage activation groups corresponding knowingly active silent speakers respectively nn’s used single structured dictionary contained layers bcofb architecture. table summarizes obtained misclassiﬁcation rates. remarkable using structured architecture instead unstructured counterpart number layers dictionary increases performance nearly factor two. discriminative objective improves performance. surprisingly using nn’s layers cedes correct classiﬁcation rate. structured architecture showed crucial roll producing accurate structured sparse codes. show observation also valid general setting. repeated experiment before randomly generated synthetic data truly structure sparse representation given dictionary results summarized figure eling real time large scale applications. framework includes diﬀerent objective functions reconstruction classiﬁcation allows diﬀerent sparse coding structures hierarchical group similarity addresses online learning scenarios. simple implementation already achieves several order magnitude speedups compared state-of-the-art minimal cost performance opening door practical algorithms following demonstrated success sparse modeling various applications. extension proposed approach structured sparse modeling problems robust non-negative matrix factorization available http//www.eng.tau.ac.il/~bron/ publications_conference.html published elsewhere lack space.", "year": 2012}