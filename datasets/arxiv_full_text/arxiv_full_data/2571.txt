{"title": "Learning to Optimize", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.", "text": "algorithm design laborious process often requires many iterations ideation validation. paper explore automating algorithm design present method learn optimization algorithm believe ﬁrst method automatically discover better algorithm. approach problem reinforcement learning perspective represent particular optimization algorithm policy. learn optimization algorithm using guided policy search demonstrate resulting algorithm outperforms existing hand-engineered algorithms terms convergence speed and/or ﬁnal objective value. current approach designing algorithms laborious process. first designer must study problem devise algorithm guided mixture intuition theoretical and/or empirical insight general design paradigms. needs analyze algorithm’s performance prototypical examples compare existing algorithms. algorithm falls short must uncover underlying cause clever ways overcome discovered shortcomings. iterates process arrives algorithm superior existing algorithms. given often protracted nature process natural question automate paper focus automating design unconstrained continuous optimization algorithms powerful ubiquitous tools used areas science engineering. extensive work past several decades yielded many popular methods like gradient descent momentum conjugate gradient l-bfgs. algorithms share commonality hand-engineered steps algorithms carefully designed human experts. deep learning achieved tremendous success automating feature engineering automating algorithm design could open similar performance gains. learn better optimization algorithm observing execution. formulate problem reinforcement learning problem. framework particular optimization algorithm simply corresponds policy. reward optimization algorithms converge quickly penalize not. learning optimization algorithm reduces ﬁnding optimal policy solved using reinforcement learning method. differentiate algorithm performs learning algorithm learned henceforth refer former learning algorithm learner latter autonomous algorithm policy. off-the-shelf reinforcement learning algorithm known guided policy search demonstrated success variety robotic control settings show empirically autonomous optimization algorithm learn converges faster and/or ﬁnds better optima existing hand-engineered optimization algorithms. early work explored general theme speeding learning accumulation learning experience. line work known learning learn meta-learning considers problem devising methods take advantage knowledge learned related tasks train faster problem today better known multi-task learning transfer learning. contrast proposed method learn accelerate training procedure itself without necessarily requiring training related auxiliary tasks. different line work known programming demonstration considers problem learning programs examples input output. several different approaches proposed liang represents programs explicitly using formal language constructs hierarchical bayesian prior programs performs inference using mcmc sampling procedure graves represents programs implicitly sequences memory access operations trains recurrent neural learn underlying patterns memory access operations. subsequent work proposes variants model different primitive memory access operations expressive operations non-differentiable operations others consider building models permit parallel execution training models stronger supervision form execution traces line work replicate behaviour simple existing algorithms examples rather learn algorithm better existing algorithms. rich body work hyperparameter optimization studies optimization hyperparameters used train model learning rate momentum decay factor regularization parameters. methods rely sequential model-based bayesian optimization others adopt random search approach gradientbased optimization hyperparameter setting corresponds particular instantiation optimization algorithm methods viewed search different instantiations optimization algorithm. proposed method hand search space possible optimization algorithms. addition presented objective function hyperparameter optimization needs conduct multiple trials different hyperparameter settings optimal hyperparameters. contrast training complete autonomous algorithm knows choose hyperparameters on-the-ﬂy without needing different hyperparameter settings even presented objective function seen training. best knowledge proposed method represents ﬁrst attempt learn better algorithm automatically. reinforcement learning setting learner given choice actions take time step changes state environment unknown fashion receives feedback based consequence action. feedback typically given form reward cost objective learner choose sequence actions based observations current environment maximizes cumulative reward minimizes cumulative cost time steps. reinforcement learning problem typically formally represented markov decision process consider ﬁnite-horizon continuous state action spaces deﬁned tuple states actions probability density initial states transition probability density conditional probability density successor states given current state action function maps state cost discount factor. objective learn stochastic policy conditional probability density actions given current state expected cumulative cost minimized. problem ﬁnding cost-minimizing policy known policy search problem. enable generalization unseen states policy typically parameterized minimization performed representable policies. solving problem exactly intractable selected special cases. therefore policy search methods generally tackle problem solving approximately. many practical settings characterizes dynamics unknown must therefore estimated. additionally often equally important minimize cost earlier later time steps henceforth focus undiscounted setting i.e. setting guided policy search method performing policy search continuous state action spaces possibly unknown dynamics. works alternating computing target distribution trajectories encouraged minimize cost agree current policy learning parameters policy standard supervised fashion sample trajectories executing policy close sample trajectories drawn target distribution. target trajectory distribution computed iteratively ﬁtting local time-varying linear quadratic approximations dynamics cost respectively optimizing restricted class linear-gaussian policies subject trust region constraint solved efﬁciently closed form using dynamic programming algorithm known linear-quadratic-gaussian refer interested readers details. consider general structure algorithm unconstrained continuous optimization outlined algorithm starting random location domain objective function algorithm iteratively updates current location step vector computed functional objective function current location past locations. framework subsumes existing optimization algorithms. different optimization algorithms differ choice first-order methods depends gradient objective function whereas second-order methods depends gradient hessian objective function. particular following choice yields gradient descent method denotes step size denotes momentum decay factor. therefore learn able learn optimization algorithm. since difﬁcult model general functionals practice restrict dependence objective function objective values gradients evaluated current past locations. hence simply modelled function objective values gradients along trajectory taken optimizer next step vector. observe execution optimization algorithm viewed execution ﬁxed policy state consists current location objective values gradients evaluated current past locations action step vector used update current location transition probability partially characterized location update formula policy executed corresponds precisely choice used optimization algorithm. reason also denote policy hand. formulation searching policies corresponds searching possible ﬁrst-order optimization algorithms. reinforcement learning learn policy need deﬁne cost function penalize policies exhibit undesirable behaviours execution. since performance metric interest optimization algorithms speed convergence cost function penalize policies converge slowly. assuming goal minimize objective function deﬁne cost state objective value current location. encourages policy reach minimum objective function quickly possible. since policy stochastic general model dimension action conditional state independent gaussian whose mean given regression model variance learned constant. choose parameterize mean using neural appealing properties universal function approximator strong empirical performance variety applications. guided policy search learn parameters policy. training consisting different randomly generated objective functions. evaluate resulting autonomous algorithm different objective functions drawn distribution. autonomous optimization algorithm offers several advantages hand-engineered algorithms. first autonomous optimizer trained real algorithm execution data whereas hand-engineered optimizers typically derived analyzing objective functions properties satisﬁed objective functions arise practice. hence autonomous optimizer minimizes amount priori assumptions made objective functions instead take full advantage information actual objective functions interest. second autonomous optimizer hyperparameters need tuned user. instead computing step direction must combined user-speciﬁed step size autonomous optimizer predicts step direction size jointly. allows autonomous optimizer dynamically adjust step size based information acquired objective function performing optimization. finally autonomous optimizer trained particular class objective functions able discover hidden structure geometry class objective functions. test time exploit knowledge perform optimization faster. store current location previous gradients improvements objective value previous iterations state. keep track information pertaining previous time steps experiments. speciﬁcally dimensions state space encode following information initially dimensions corresponding historical information zero. current location used compute cost; policy depend absolute coordinates current location exclude input neural net. small neural model policy. architecture consists single hidden layer hidden units. softplus activation units used hidden layer linear activation units used output layer. training objective imposed guided policy search takes form squared mahalanobis distance mean predicted target actions along terms dependent variance policy. also regularize entropy policy encourage deterministic actions conditioned state. coefﬁcient regularizer increases gradually later iterations guided policy search. initialize weights neural randomly regularize magnitude weights. initially target trajectory distribution mean action given state time step matches step vector used gradient descent method momentum. choose best settings step size momentum decay factor objective function training performing grid search hyperparameters running noiseless gradient descent momentum hyperparameter setting. training sample trajectories length time steps objective function training set. iteration guided policy search sample trajectories distribution discard trajectories preceding iteration. learn autonomous optimization algorithms various convex non-convex classes objective functions correspond loss functions different machine learning models. ﬁrst learn autonomous optimizer logistic regression induces convex loss function. learn autonomous optimizer robust linear regression using geman-mcclure m-estimator whose loss function non-convex. finally learn autonomous optimizer two-layer neural classiﬁer relu activation units whose error surface even complex geometry. denote weight vector bias respectively denote feature vector label instance denotes coefﬁcient regularizer +e−z experiments choose objective convex train autonomous algorithm learns optimize objectives form. training consists examples objective functions whose free variables case assigned concrete values. hence objective function training corresponds logistic regression problem different dataset. construct training randomly generate dataset instances function training set. instances drawn randomly multivariate gaussians random means covariances half drawn each. instances gaussian assigned label instances different gaussians assigned different labels. train autonomous algorithm objective functions. evaluate test random objective functions generated using procedure compare popular hand-engineered algorithms gradient descent momentum conjugate gradient l-bfgs. baselines best hyperparameter settings tuned training set. algorithm objective function test compute difference objective value achieved given algorithm achieved best competing figure mean margin victory algorithm optimizing logistic regression loss. higher margin victory indicates better performance. objective values achieved algorithm objective functions test set. lower objective values indicate better performance. best viewed colour. algorithms every iteration quantity refer margin victory. quantity positive current algorithm better algorithms negative otherwise. figure plot mean margin victory algorithm iteration averaged objective functions test set. conjugate gradient l-bfgs diverge oscillate rare cases even though autonomous algorithm gradient descent momentum not. reﬂect performance baselines majority cases exclude offending objective functions computing mean margin victory. shown autonomous algorithm outperforms gradient descent momentum conjugate gradient almost every iteration. margin victory autonomous algorithm quite high early iterations indicating autonomous algorithm converges much faster algorithms. interesting note despite seen trajectories length training time autonomous algorithm able generalize much longer time horizons test time. l-bfgs converges slightly better optima autonomous algorithm momentum method. surprising objective functions convex l-bfgs known good optimizer convex optimization problems. show performance algorithm objective functions test figures figure autonomous algorithm converges faster algorithms. figure autonomous algorithm initially converges faster algorithms later overtaken l-bfgs remaining faster optimizers. however eventually achieves objective value l-bfgs objective values achieved gradient descent momentum remain much higher. next consider problem linear regression using robust loss function. ensure robustness m-estimator parameter estimation. popular choice geman-mcclure estimator induces following objective denote weight vector bias respectively denote feature vector label instance constant modulates shape loss function. experiments loss function convex either preceding section objective function training function form realized values dataset objective function generated drawing random samples four multivariate gaussians random mean identity covariance matrix. points drawn gaussian labels generated projecting along random vector adding randomly generated bias perturbing i.i.d. gaussian noise. figure mean margin victory algorithm optimizing robust linear regression loss. higher margin victory indicates better performance. objective values achieved algorithm objective functions test set. lower objective values indicate better performance. best viewed colour. autonomous algorithm trained objective functions. evaluate randomly generated objective functions using metric above. shown figure autonomous algorithm outperforms hand-engineered algorithms except early iterations. dominates gradient descent conjugate gradient l-bfgs times make progress quickly momentum method initially. however around iterations able close surpass momentum method. optimization problem conjugate gradient l-bfgs diverge quickly. interestingly unlike previous experiment l-bfgs longer performs well could caused non-convexity objective functions. figures show performance objective functions test set. figure autonomous optimizer converges fastest also reaches better optimum algorithms. figure autonomous algorithm converges fastest able avoid oscillations hamper gradient descent momentum reaching optimum. finally train autonomous algorithm train small neural classiﬁer. consider two-layer neural relu activation hidden units softmax activation output units. cross-entropy loss combined regularization weights. train model need optimize following objective rh×d rp×h denote ﬁrst-layer second-layer weights biases denote input target class label instance denotes coefﬁcient regularizers denotes component experiments error surface known complex geometry multiple local optima making challenging optimization problem. training consists objective functions corresponds objective training neural different dataset. dataset generated generating four multivariate gaussians random means covariances sampling points each. points gaussian assigned random label either make sure points dataset assigned label. evaluate autonomous algorithm manner above. shown figure autonomous algorithm signiﬁcantly outperforms algorithms. particular evidenced sizeable sustained margin victory autonomous optimizer momentum method autonomous optimizer able reach much better optima less prone getting trapped local optima compared methods. also larger compared exhibited previous sections suggesting hand-engineered algorithms sub-optimal figure mean margin victory algorithm training neural classiﬁers. higher margin victory indicates better performance. objective values achieved algorithm objective functions test set. lower objective values indicate better performance. best viewed colour. challenging optimization problems potential improvement learning algorithm greater settings. non-convexity conjugate gradient l-bfgs often diverge. performance examples objective functions test shown figures shown autonomous optimizer able reach better optima methods largely avoids oscillations methods suffer from. presented method learning better optimization algorithm. formulated reinforcement learning problem optimization algorithm represented policy. learning optimization algorithm reduces optimal policy. used guided policy search purpose trained autonomous optimizers different classes convex nonconvex objective functions. demonstrated autonomous optimizer converges faster and/or reaches better optima hand-engineered optimizers. hope autonomous optimizers learned using proposed approach used solve various common classes optimization problems quickly help accelerate pace innovation science engineering. references jonathan baxter rich caruana mitchell lorien pratt daniel silver sebastian thrun. nips workshop learning learn knowledge consolidation transfer inductive systems. https//web.archive.org/web//http//www.cs.cmu.edu/afs/cs.cmu. edu/user/caruana/pub/transfer.html accessed eric brochu vlad cora nando freitas. tutorial bayesian optimization expensive cost functions application active user modeling hierarchical reinforcement learning. arxiv preprint arxiv. chelsea finn duan trevor darrell sergey levine pieter abbeel. learning visual feature spaces robotic manipulation deep spatial autoencoders. arxiv preprint arxiv. łukasz kaiser ilya sutskever. neural gpus learn algorithms. arxiv preprint arxiv. karol kurach marcin andrychowicz ilya sutskever. neural random-access machines. arxiv preprint percy liang michael jordan klein. learning programs hierarchical bayesian approach. proceedings international conference machine learning pages dougal maclaurin david duvenaud ryan adams. gradient-based hyperparameter optimization sebastian thrun lorien pratt. learning learn. springer science business media ricardo vilalta youssef drissi. perspective view survey meta-learning. artiﬁcial intelligence greg yang. access neural turing machine. arxiv preprint arxiv. wojciech zaremba tomas mikolov armand joulin fergus. learning simple algorithms", "year": 2016}