{"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and  Persistent Contrastive Divergence", "tag": ["cs.NE", "cs.LG", "stat.ML", "62M45", "I.2.6"], "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different biases and variances for stochastic gradient estimates of individual data points. It is well known that CD yields a biased gradient estimate. In this paper we however show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the mean of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "text": "delalleau carreira-perpinan hinton hand known fast reaching good results addition computationally light sampling procedure claimed beneﬁt variance gradient estimates however current authors aware rigorous research whether claim holds true magnitude effect hand persistent contrastive divergence empirically shown require lower learning rate longer training authors propose learning rate required since model weights updated markov chain runs means order sample distribution close stationary distribution weight cannot change rapidly. however similar reasons updates assumed variance subsequent updates likely correlated leading possibly undesirable momentum updates. behavior would effectively increase variance mean subsequent updates requiring either larger minibatches smaller learning rates. paper explore variances exact stochastic gradient estimates. hope shed light observed fast speed learning required learning rate learning compared learning. thereby hope contribute understanding difference beyond already well documented bias contrastive divergence persistent contrastive divergence popular methods training weights restricted boltzmann machines. however methods approximate method sampling model distribution. side effect approximations yield signiﬁcantly different biases variances stochastic gradient estimates indiwell known vidual data points. yields biased gradient estimate. paper however show empirically lower stochastic gradient estimate variance exact sampling mean subsequent estimates higher variance exact sampling. results give explanation ﬁnding used smaller minibatches higher learning rates pcd. popular methods train restricted boltzmann machines include contrastive divergence persistent contrastive divergence although theoretical research focused properties methods methods still used similar situations choice often based intuition heuristics. restricted boltzmann machine boltzmann machine visible neuron connected hidden neurons hidden neuron visible neurons edges type neurons. deﬁnes energy state assigns following probability state boltzmann distriz exp{−e bution parameters normalizes probabilities one. likelihood training data point hence exp{−e sampling positive phase gradient likelihood easy sampling negative phase popular method solve sampling negative phase contrastive divergence negative particle sampled approximately running markov chain limited number steps positive particle another method called persistent contrastive divergence solves sampling related method negative particle sampled positive particle rather negative particle last data point order examine variance gradient estimates empirical approach. train evaluate variance gradient estimates different sampling strategies different stages training process. sampling strategies cd-k ranging assumed correspond almost unbiased stochastic gradient. addition test cd-k independent samples negative particle sampled random training example. variance i-cd separates effect negative particle close data distribution general effect negative particle close positive particle question. three different data sets. ﬁrst reduced size mnist pixel images ﬁrst training data points digit totaling data points. second data center pixels ﬁrst cifar images converted gray scale. third caltech silhouettes pixel black white images. binarize number hidden neurons equal number visible neurons. biases initialized zero weights initially sampled zero-mean normal distribution standard deviation number visible hidden neurons respectively. train model evaluate variance gradient estimates epochs. adaptive learning rate initial learning rate weight decay. gradient estimates ﬁnal sampling step probabilities hidden unit activations omitted. gradient estimate therefore based sampled binary visible unit activations continuous hidden unit activation probabilities conditional visible unit activations. process called rao-blackwellisation often used practice. variance calculated individual gradient estimates based positive negative particle each. practice gradient usually estimated averaging mini-batch independent samples diminishes variance nfold. ignore bias gradient estimates. analyzing subsequent gradient estimates negative particles ﬁrst estimate sampled steps random training example. subsequent estimates averaged positive particle randomly sampled data step negative particle sampled previous negative particle. learning occurs subsequent estimates. therefore disentangle effects weight iterate results different random initializations weights evaluate variance sampling gradient estimates individual training examples times training example data set. variance calculated weight matrix element separately variances individual weights averaged. figure variance contrastive divergence indeed smaller exact sampling negative particle. also variance estimates quickly increases number steps. however effect signiﬁcant later stages training. phenomenon expected model expected well later stages training weights close small initial random weights. sample negative particle different training example positive particle figure variance similar even larger compared variance exact sampling. although trivial variance i-cd estimates higher interesting result i-cd loses variance advantage exact sampling. result supports hypothesis variance precisely stems fact negative particle sampled positive particle negative particle sampled limited number steps random training example. subsequent updates figure variance indeed considerably higher independent sampling. again expected effect stronger later training evaluation done. looking magnitude variance difference mean subsequent updates multiple times smaller variance pcd. effect means ignoring effects effect weight updates would need considerably smaller learning rates larger minibatches reach variance minibatch. magnitude substantial might explain empirical ﬁnding performs best smaller learning rates contrastive divergence persistent contrastive divergence often used training weights restricted boltzmann machines. contrastive divergence claimed beneﬁt variance gradient estimates using stochastic gradients. persistent contrastive divergence could hand suffer high correlation subsequent gradient estimates poor mixing markov chain estimating model distribution. paper empirically conﬁrmed ﬁndings. experiments three data sets variance gradient estimates considerably lower independently sampling many steps model distribution. conversely variance mean subsequent gradient estimates using signiﬁcantly higher independent sampling. swersky kevin chen marlin freitas nando. tutorial stochastic approximation algorithms training restricted boltzmann machines information theory applicadeep belief nets. tions workshop ieee tieleman tijmen. training restricted boltzmann machines using approximations likelihood gradient. proceedings international conference machine learning williams christopher agakov felix analysis contrastive divergence learning gaussian boltzmann machines. institute adaptive neural computation effect mainly observable towards training. effect indicates variance perspective would require considerably lower learning rates larger minibatches known biased estimator therefore seems choice trade-off bias variance. although results paper practically signiﬁcant approach paper purely empirical. theoretical analysis variance gradient estimates would therefore warranted conﬁrm ﬁndings. addition intend repeat experiments larger data train models replace baseline better approximations exact sampling using e.g. enhanced gradient parallel tempering kyunghyun raiko tapani ilin alexander. enhanced gradient adaptive learning rate trainproceedings restricted boltzmann machines. international conference machine learning krizhevsky alex hinton geoffrey. learning multiple layers features tiny images. master’s thesis department computer science university toronto", "year": 2013}