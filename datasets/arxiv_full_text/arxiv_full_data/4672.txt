{"title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "tag": ["cs.AI", "cs.LG"], "abstract": "We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.", "text": "lifelong learning systems real-world domains suffer curse dimensionality. state action spaces increase becomes difﬁcult model solve tasks encountered. addition planning potentially inﬁnite time-horizons well efﬁciently retaining reusing knowledge pose non-trivial challenges. challenging high-dimensional domain incorporates many elements found lifelong learning minecraft. minecraft popular video game whose goal build structures travel adventures hunt food avoid zombies. example screenshot game seen figure minecraft open research problem impossible solve entire game using single technique instead solution minecraft solving sub-problems using divide-and-conquer approach providing synergy various solutions. agent learns solve sub-problem acquired skill reused similar sub-problem subsequently encountered. propose lifelong learning system ability reuse transfer knowledge task another efﬁciently retaining previously learned knowledgebase. knowledge transferred learning reusable skills solve tasks minecraft popular video game unsolved high-dimensional lifelong learning problem. reusable skills refer deep skill networks incorporated novel hierarchical deep reinforcement learning network architecture using techniques deep skill array skill distillation novel variation policy distillation learning skills. skill distillation enables hdrln efﬁciently retain knowledge therefore scale lifelong learning accumulating knowledge encapsulating multiple reusable skills single distilled network. h-drln exhibits superior performance lower learning sample complexity compared regular deep network sub-domains minecraft. lifelong learning considers systems continually learn tasks domains course lifetime. lifelong learning large open problem great importance development general purpose artiﬁcially intelligent agents. formal deﬁnition lifelong learning follows. deﬁnition lifelong learning continued learning tasks domains course lifetime lifelong learning system. lifelong learning system efﬁciently effectively retains knowledge learned; selectively transfers knowledge learn tasks; ensures effective efﬁcient interaction truly general lifelong learning system shown figure therefore following attributes efﬁcient retention learned task knowledge lifelong learning system minimize retention erroneous knowledge. addition also computationally efﬁcient storing knowledge long-term memory. selective transfer lifelong learning system needs ability choose relevant prior knowledge solving tasks order learn reusable skills lifelong learning setting framework needs able learn skills learn controller determines skill used reused able efﬁciently accumulate reusable skills. recent works perform skill learning works focused learning good skills explicitly shown ability reuse skills scale respect number skills lifelong learning domains. emergence deep speciﬁcally deep q-networks agents equipped powerful non-linear function approximator learn rich complex policies using networks agent learns policies image pixels requiring less domain speciﬁc knowledge solve complicated tasks different variations algorithm exist refer vanilla version unless otherwise stated. deep learning approaches perform sub-goal learning approaches rely providing task sub-goal agent prior making decision. kulkarni also rely manually constructing sub-goals a-priori tasks utilize intrinsic motivation problematic complicated problems designing good intrinsic motivations clear non-trivial. paper present novel lifelong learning system called hierarchical deep reinforcement learning network architecture shown figure claim provide end-to-end solution basic building blocks truly general lifelong learning framework h-drln controller learns solve complicated tasks minecraft learning reusable skills form pre-trained deep skill networks knowledge retained incorporating reusable skills h-drln deep skill module. types deep skill modules array multi-skill distillation network novel variation policy distillation applied learning skills. multi-skill distillation enables h-drln efﬁciently retain knowledge therefore scale lifelong learning encapsulating multiple reusable skills single distilled network. solving task h-drln selectively transfers knowledge form temporal abstractions solve given task. taking advantage temporally extended actions h-drln learns solve tasks lower sample complexity superior performance compared vanilla dqns. figure lifelong learning lifelong learning system efﬁciently retains knowledge selectively transfers knowledge solve tasks. upon solving task knowledge base reﬁned knowledge added system. systems approach ensures efﬁcient effective interaction many tasks encountered agent lifelong learning setting naturally decomposed skill hierarchies minecraft example consider building wooden house seen figure task decomposed sub-tasks chopping trees sanding wood cutting wood boards ﬁnally nailing boards together. here knowledge gained chopping trees also partially reused cutting wood boards. addition agent receives task build small city agent reuse skills acquired ‘building house’ task. high-dimensional lifelong learning setting minecraft learning skills reuse skills non-trivial. efﬁcient knowledge retention transfer increasing exploration efﬁciently solving tasks ultimately advancing capabilities minecraft agent. reinforcement learning provides generalized approach skill learning options framework options temporally extended actions also referred skills macro-actions options shown theoretically experimentally speed convergence rate algorithms. refer options skills. main contributions novel hierarchical deep reinforcement learning network architecture includes h-drln controller deep skill module. h-drln contains basic building blocks truly general lifelong learning framework. show potential learn reusable deep skill networks perform knowledge transfer learned dsns tasks obtain optimal solution. also show potential transfer knowledge related tasks without additional learning. efﬁciently retain knowledge h-drln performing skill distillation variation policy distillation learning skills incorporate deep skill model solve complicated tasks minecraft. empirical results learning h-drln sub-domains minecraft array distilled skill network. also verify improved convergence guarantees utilizing reusable dsns within h-drln compared vanilla dqn. previous research lifelong learning designing truly general challenging task. previous works lifelong learning focused solving speciﬁc elements general lifelong learning system shown table according deﬁnition lifelong learning agent able efﬁciently retain knowledge. typically done sharing representation among tasks using distillation latent basis agent also learn selectively past knowledge solve tasks efﬁciently. works focused spatial transfer mechanism i.e. suggested learning differentiable weights shared representation tasks contrast brunskill suggested temporal transfer mechanism identiﬁes optimal skills past tasks learns skills tasks. finally agent systems approach allows efﬁciently retain knowledge multiple tasks well efﬁcient mechanism transfer knowledge solving tasks. work incorporates basic building blocks necessary performing lifelong learning. lifelong learning deﬁnition efﬁciently transfer knowledge previous tasks solve target task utilizing skills show skills reduce sample complexity complex minecraft environment suggest efﬁcient mechanism retain knowledge multiple skills scalable number skills. serves state selects action receives bounded reward rmax maximum attainable reward discount factor. following agents action choice transitions next state consider inﬁnite horizon problems cumulative return time γt−trt. action-value function represents expected return observing state taking action policy optimal action-value function obeys fundamental recursion known bellman equation deep networks algorithm approximates optimal function convolutional neural network optimizing network weights expected temporal difference error optimal bellman equation minimized estatrtst+ notice ofﬂine learning algorithm meaning tuples {stat collected agents experience stored experience replay buffer stores agents experiences time-step purpose ultimately training parameters minimize loss function. apply minibatch training updates sample tuples experience random pool stored samples maintains separate q-networks. current q-network parameters target q-network parameters θtarget. parameters θtarget every ﬁxed number iterations. order capture game dynamics double double prevents overly optimistic estimates value function. achieved performing action selection current network evaluating action target network θtarget yielding ddqn target update terminal otherwise γqθtarget). ddqn utilized paper improve learning performance. skills options macro-actions skill temporally extended control structure deﬁned triple states skill initiated intra-skill policy determines skill behaves encountered states termination probabilities determining skill stop executing. parameter typically function state time semi-markov decision process planning skills performed using smdp theory. formally smdp deﬁned ﬁve-tuple states skills transition probability kernel. assume rewards received timestep bounded represents expected discounted rewards received execution skill initialized state solution smdp skill policy skill policy skill policy mapping states probability distribution skills action-value function represents long-term value taking skill state thereafter always selecting skills according polt= γtrt| transition probability r|st deﬁnitions optimal skill value function given following equation policy distillation distillation method transfer knowledge teacher model student model process typically done supervised learning. example teacher student separate deep neural networks student network trained predict teacher’s output layer different objective functions previously proposed. paper input teacher output softmax function train distilled network using mean-squared-error loss cost softmaxτ action values teacher student networks respectively softmax temperature. training cost function differentiated according student network weights. policy distillation used transfer knowledge teachers single student typically done switching between teachers every ﬁxed number iterations during training process. student learning multiple teachers separate student output layer assigned teacher trained task layers shared. section present in-depth description h-drln architecture extends facilitates skill reuse lifelong learning. skills incorporated h-drln deep skill module incorporate either array distilled multi-skill network. deep skill module pre-learned skills represented deep networks referred deep skill networks trained a-priori various sub-tasks using version algorithm regular experience replay detailed background section. note choice architecture principal suitable networks used place. deep skill module represents dsns. given input state skill index outputs action according corresponding policy πdsni. propose different deep skill module architectures array array pre-trained dsns represented separate dqn. distilled multi-skill network single deep network represents multiple dsns. here different dsns share hidden layers separate output layer trained policy distillation distilled skill network allows incorporate multiple skills single network making architecture scalable lifelong learning respect number skills. h-drln architecture diagram h-drln architecture presented figure here outputs h-drln consist primitive actions well skills. h-drln learns policy determines execute primitive actions reuse pre-learned skills. h-drln chooses execute primitive action time action executed single timestep. however h-drln chooses execute skill executes policy πdsni terminates gives control back h-drln. gives rise necessary modiﬁcations needed make order incorporate skills learning procedure generate truly hierarchical deep network optimize objective function incorporates skills; construct stores skill experiences. solve tasks encountered lifelong learning scenario agent needs able adapt game dynamics learn reuse skills learned solving previous tasks. experiments show ability minecraft agent learn dsns sub-domains minecraft ability agent reuse navigation domain solve complex task termed two-room domain potential transfer knowledge related tasks without additional learning. demonstrate ability agent reuse multiple dsns solve complex-domain different deep skill modules demonstrate architecture scales lifelong learning. state space mnih state space represented image pixels last four image frames combined down-sampled pixel image. actions primitive action space consists actions move forward rotate left rotate right break block pick item place rewards domains agent gets small negative reward signal step non-negative reward upon reaching ﬁnal goal training episode lengths steps single dsns room domain complex domain respectively. agent initialized random location ﬁrst room room complex domains. evaluation agent evaluated training using current learned architecture every optimization steps dsns evaluation averaged agent’s performance steps respectively. success percentage successful task completions evaluation. figure h-drln architecture outputs correspond primitive actions dsns deep skill module represents skills. receives input skill index outputs action according corresponding skill policy. architecture deep skill module either array distilled multi-skill network. skill objective function mentioned previously hdrln extends vanilla architecture learn control primitive actions skills. h-drln loss function structure equation however instead minimizing standard bellman equation minimize skill bellman equation specifically skill initiated state time executed duration h-drln target function given skill experience replay extend regular incorporate skills term skill experience replay differences standard s-er. firstly sampled skill tuple calculate discounted cumulative rewards generated whilst executing skill. second since skill executed timesteps store transition state st+k rather st+. yields skill tuple skill executed time training ﬁrst experiment involved training dsns sub-domains minecraft including navigation domains pickup domain placement domain respectively. break domain placement domain except ends break action. domains come different learning challenges. navigation domain built identical walls provides signiﬁcant learning challenge since visual ambiguities respect agent’s location navigation domain provides different learning challenge since obstacles occlude agent’s view exit different regions room pick break placement domains require navigating speciﬁc location ending execution primitive action order train different dsns vanilla architecture performed grid search optimal hyper-parameters learning dsns minecraft. best parameter settings found include higher learning ratio higher learning rate less exploration implemented modiﬁcations since standard minecraft emulator slow frame rate modiﬁcations enabled agent increase learning game states. also found smaller experience replay provided improved performance probably task relatively short time horizon rest parameters vanilla remained unchanged. tuned hyper-parameters dsns managed solve corresponding sub-domains almost success shown table room domain domain consists two-rooms ﬁrst room shown figure corresponding exit note exit ﬁrst room identical exit navigation domain second room contains goal goal navigation domain agent’s available action consists primitive movement actions navigate dsn. skill reusability/knowledge transfer trained h-drln architecture well vanilla two-room domain. noticed important observations. h-drln architecture solves task single epoch generates signiﬁcantly higher reward compared vanilla dqn. h-drln makes knowledge transfer reusing trained one-room domain solve two-room domain. able identify exit ﬁrst room navigates agent exit. also able navigate agent exit second room completes task. temporally extended action lasts multiple time steps therefore increases exploration agent enabling learn solve task faster vanilla dqn. epochs vanilla completes task success percentage. sub-optimal performance wall ambiguities causing agent stuck sub-optimal local minima. number epochs agent completes task using h-drln success. performing additional learning network. found surprising without training two-room domain generated higher reward compared vanilla speciﬁcally trained two-room domain epochs. figure summarizes success percentage comparison different architectures two-room domain. vanilla h-drln start h-drln average success percentages respectively. performance sub-optimal compared h-drln architecture still manages solve two-room domain. exciting result shows potential dsns identify solve related tasks without performing additional learning. training h-drln deep skill module section discuss results training utilizing h-drln deep skill module solve complex minecraft domain. experiments section utilized ddqn train h-drln ddqn baseline unless otherwise stated. complex minecraft domain domain consists three rooms. within room agent required perform speciﬁc task. room navigation task agent needs navigate around obstacles reach exit. room contains tasks. pickup task whereby agent required navigate collect block center room; break task agent needs navigate exit break door. finally room placement task whereby agent needs place block collected goal location. agent receives non-negative reward successfully navigates room collects block breaks door room places block goal location room otherwise agent receives small negative reward timestep. note agent needs complete three separate tasks receiving sparse non-negative reward. agent’s available action original primitive actions well dsn’s navigate pickup break placement. training distilling multiple dsns mentioned h-drln section ways incorporate skills deep skill module array multi-skill distillation. array multiskill distillation utilize four pre-trained dsns dsns collectively form array. multi-skill distillation utilized pre-trained dsns teachers distil skills directly single network using distillation setup shown figure described background section. trained tested distilled network separately three individual rooms performance room shown table temperatures high success percentages indicate agent able successfully complete task using single distilled network. contrast policy distillation novelty lies ability distil skills single network also learn control rule switches skills solve given task. training h-drln show results training h-drln array h-drln ddqn array h-drln ddqn distilled multi-skill network compared ddqn baseline. learning curves seen figure performed trials times architecture measured success rates h-drln h-drln ddqn h-drln ddqn distilled multi-skill network respectively. calculate values averaged success percentages ﬁnal epochs. note distilled h-drln higher average success rate h-drln’s ddqn lower variance. ddqn unable solve task. combination wall ambiguities requiring time learn. h-drln able overcome ambiguities also learns reuse skills. also trained ddqn intrinsic rewards enabled solve task. however required signiﬁcantly larger amount training time compared h-drln result therefore omitted. also provided ﬁrst results learning deep skill networks minecraft lifelong learning domain. dsns learned using minecraft-speciﬁc variation algorithm. minecraft agent also learns reuse dsns tasks h-drln. incorporate multiple skills h-drln using array scalable distilled multi-skill network novel variation policy distillation. addition show h-drln provides superior learning performance faster convergence compared ddqn making skills. work also interpreted form curriculum learning here ﬁrst train network solve relatively simple sub-tasks knowledge obtained solve composite overall task. also show potential perform knowledge transfer related tasks without additional learning. architecture also potential utilized domains doom labyrinth recently shown deep networks tend implicitly capture hierarchical composition given task future work plan utilize implicit hierarchical composition learn dsns. addition learn skills online whilst agent learning solve task. could achieved training teacher networks whilst simultaneously guiding learning student network perform online reﬁnement previously learned skills; train agent real-world minecraft domains. research supported part european communitys seventh framework programme grant agreement intel collaborative research institute computational intelligence", "year": 2016}