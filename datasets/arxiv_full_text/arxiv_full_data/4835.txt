{"title": "Learning from the memory of Atari 2600", "tag": ["cs.LG", "cs.AI"], "abstract": "We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in NIPS and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.", "text": "train number neural networks play games bowling breakout seaquest using information stored memory video game console atari consider four models neural networks diﬀer size architecture networks information contained mixed networks information information screen. benchmark used convolutional model proposed received comparable results considered games. quite surprisingly case seaquest able train ram-only agents behave better benchmark screen-only agent. mixing screen lead improved performance comparing screen-only ram-only agents. atari controller perform actions work intended learn actions proﬁtable given state screen memory. work based deep q-learning reinforcement learning algorithm learn play atari games using input screen. deep q-learning algorithm builds q-learning algorithm simplest form iteratively learns values state-action pairs lets agent choose action highest value. instance atari games implies evaluating pairs action positions controller. task infeasible similarly learning values realistic task real-world games chess feasibility issues generalizations q-learning algorithm intended limit number parameters function depends. arbitrarily restrict number features learned instead using manually devised features deep qlearning algorithm presented builds process training neural network. since every neural network composition priori unknown linear maps ﬁxed non-linear maps deep q-learning algorithm learn coeﬃcients unknown linear maps. deep q-learning algorithm game states actions immediate rewards passed deep convolutional network. type network abstracts features screen various patterns screen identiﬁed similar. network number output nodes possible action predicts cumulative game rewards making moves corresponding actions. number decisions made process designing deep q-learning algorithm details) step probability making random move decreases course training previous game states stored replay memory; updates q-learning limited random batch events polled memory updates unknown linear maps neural network performed according gradient squared loss function measures discrepancy estimation given network actual reward. work computational infrastructure including decisions e.g. declare ﬁxed pre-deﬁned functions example declare value state-action pair certain shape appears bottom-left corner screen otherwise declare value enemy appeared right otherwise. q-learning algorithm learns best values original algorithm improved number ways includes changing network architecture choosing better hyperparameters improving speed algorithm optimizes neural network’s loss function. attempts proved successful made deep q-learning algorithm state-of-the-art method playing atari games. instead screen treat state atari machine game state. work implemented classical planning algorithm state. since atari consists bytes eﬃciently search low-dimensional state space. nevertheless learning happens gameplay depends time limit single move. contrast learning process happens gameplay particular agent play real-time. best knowledge ram-based agent depending search presented cite results ram. work deep q-learning algorithm instead using screens inputs network pass state state screen together. following sections describe games used evaluation well network architectures tried hyperparameters tweaked. changes deep q-learning algorithm proposed came cost making computations involved comparing work decided reference result basic work state single training neural network contained roughly hours using experimental setup describe below. decision also motivated preliminary character study wanted make sure indeed console memory contains useful data extracted training process using deep q-learning algorithm. perspective basic results seem perfect benchmark verify feasibility learning ram. refer benchmark architecture nips paper. figure left right bowling breakout seaquest. vertical bars bottom every screenshot represent state memory black representing lighter color corresponding higher values given memory cell. we’ve chosen games oﬀers distinct challenge. breakout relatively easy game player’s actions limited moves along horizontal axis. picked breakout disastrous results learning would indicate fundamental problem learning. deep q-network seaquest constructed plays amateur human level reason consider game tempting target improvements. also game state elements possibly detected ram-only network bowling seems hard game deep q-network models. interesting target ram-based networks visualizations suggest state changing slightly. experiment mean complete training single deep q-network. paper quote numerical outcomes experiments performed. experiments made nathan sprague’s implementation deep q-learning algorithm theano lasagne code uses arcade learning environment standard framework evaluating agents playing atari games. code instructions found github experiments performed linux machine equipped nvidia graphics card. training process consists epochs interleaved test periods. test period model current parameters probability random action number test steps figures show average result episode epoch. breakout tested networks best training-time results. test consisted choosing random seeds performing steps. networks including nips received results consistently lower breakout best result model weaker obtained network nips. seaquest best result obtained network better best result obtained network nips. bowling methods give slight improvement network nips considered approaches learning illustrated figure seem poor outcome terms gameplay satisfactory. decided include paper experiments bowling leave topic research. training basic ram-only network leads high variance results epochs. sign overﬁtting. tackle problem we’ve applied dropout standard regularization technique neural networks. dropout simple eﬀective regularization method. consists turning probability neuron training i.e. setting output neuron regardless input. backpropagation parameters switched nodes updated. then testing neurons work course normal training exception neuron’s output multiplied make skewed training. intuition behind dropout method forces node learn absence nodes. work shows experimental evidence dropout method indeed reduces variance learning process. ram’s evaluation method diﬀer scores presented average trials consisting long period learning long period testing nevertheless results much worse dqn-based method presented here. we’ve enabled dropout probability turning neuron applies nodes except output ones. implemented dropout ram-only networks ram. method oﬀers improvement network leading best result seaquest paper. best epoch results presented table intermediate training results seaquest shown figure learning rate parameter algorithm rmsprop decides parameters changed step. bigger learning rates correspond moving faster parameter space making learning faster noisy. expected drastic changes performance consecutive epochs illustrated figures come stepping optimal values taking steps. case decreasing step size lead slower learning combined higher precision ﬁnding minima loss function. atari designed analog output device frames appearing screen every second. simplify search space impose rule action repeated ﬁxed number frames. ﬁxed number called frame skip. standard frame skip used frame skip agent makes decision second. decision made benchmark agent nips trained frame skip frames used training along rewards coming them. dictated fact hardware limitations atari games sometimes blink i.e. show objects every frames. example game space invaders enemy spaceship shoots direction player shots seen screen every second frame agent sees frames wrong parity would access critical part game information. work suggests choosing right frame skip inﬂuence performance learning algorithms figure table show signiﬁcant improvement performance model case seaquest. quite surprisingly variance results appeared much lower higher frame skip. noticed case breakout high frame skips frame skip lead disastrous performance. hence tested lower frame skip frame skip received results slightly weaker frame skip hopes future work integrate information information screen order train ultimate atari agent. work made ﬁrst steps towards goal. consider mixed network architectures. ﬁrst mixed concatenate output last hidden layer convolutional network input last layer apply linear transformation without following non-linearity. obtained results presented table reasonable particularly impressive. particular notice improvement benchmark nips network embedded mixed architectures. suggests mixed mixed models additional information memory used productive way. visualized ﬁrst layers neural networks attempt understand work. column figure corresponds nodes ﬁrst layer trained network corresponds memory cells. color cell given column describes whether high value cell negatively positively inﬂuence activation level neuron. figure sugtrained number neural networks capable playing atari games bowling breakout seaquest. novel aspect work networks information stored memory console. games agents screen-only agent nips. case seaquest even simple architecture appropriately chosen frame skip parameter well agent standard parameters performs better benchmark nips agent. case breakout performance screen-only agent nips still reasonable. case bowling methods presented well paper helpful agents play rudimentary level. since case seaquest performance ram-only networks quite good natural next target would games pacman space invaders similarly seaquest oﬀer interesting tactical challenges. reading state running deep q-learning algorithm gives access practically unlimited stream atari memory states. stream build recurrent neural network takes account previous states. work angluin introduced concept learning structure ﬁnite state machine queries counter-examples. game atari identiﬁed ﬁnite state machine takes input memory state action outputs another memory state. interested devising neural network would learn structure ﬁnite state machine. successive layers network would learn sub-automata responsible speciﬁc memory cells later layers would join automata automaton would whole memory state.", "year": 2016}