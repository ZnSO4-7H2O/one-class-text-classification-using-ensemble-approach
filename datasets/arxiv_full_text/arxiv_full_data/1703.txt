{"title": "Bethe Projections for Non-Local Inference", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Many inference problems in structured prediction are naturally solved by augmenting a tractable dependency structure with complex, non-local auxiliary objectives. This includes the mean field family of variational inference algorithms, soft- or hard-constrained inference using Lagrangian relaxation or linear programming, collective graphical models, and forms of semi-supervised learning such as posterior regularization. We present a method to discriminatively learn broad families of inference objectives, capturing powerful non-local statistics of the latent variables, while maintaining tractable and provably fast inference using non-Euclidean projected gradient descent with a distance-generating function given by the Bethe entropy. We demonstrate the performance and flexibility of our method by (1) extracting structured citations from research papers by learning soft global constraints, (2) achieving state-of-the-art results on a widely-used handwriting recognition task using a novel learned non-convex inference procedure, and (3) providing a fast and highly scalable algorithm for the challenging problem of inference in a collective graphical model applied to bird migration.", "text": "many inference problems structured prediction naturally solved augmenting tractable dependency structure complex non-local auxiliary objectives. includes mean ﬁeld family variational inference algorithms softhard-constrained inference using lagrangian relaxation linear programming collective graphical models forms semi-supervised learning posterior regularization. present method discriminatively learn broad families inference objectives capturing powerful non-local statistics latent variables maintaining tractable provably fast inference using non-euclidean projected gradient descent distance-generating function given bethe entropy. demonstrate performance ﬂexibility method extracting structured citations research papers learning soft global constraints achieving state-of-theart results widely-used handwriting recognition task using novel learned non-convex inference procedure providing fast highly scalable algorithm challenging problem inference collective graphical model applied bird migration. structured prediction shown great success modeling problems complex dependencies output variables. practitioners often undirected graphical models encode conditional dependency relationships graph. however tractability exact inference models limited graph’s treewidth often yielding harsh tradeoff model expressivity tractability. graphical models good modeling local dependencies variables importance surrounding context determining meaning words phrases. however sensitivity cyclic dependencies often renders unsuitable modeling preferences certain globally consistent states. example canonical task part-of-speech tagging clear enforce constraint every sentence least verb without increasing likelihood every token predicted verb. concatenated vector node clique marginals entropy marginal polytope parameters. face tradeoff adding long-range dependencies directly model increases clique size thus complexity problem size rendering inference intractable. however linear scoring function breaks cliques preventing enforcing global regularities way. work propose augment inference objective instead optimize here arbitrary parametric function entire concatenated marginal vector depend input features. since non-linear enforce many types non-local properties. interestingly whenever convex whenever inference easy underlying model i.e. solving tractable solve using non-euclidean projected gradient methods using bethe entropy distance-generating function. unlike many message-passing algorithms procedure maintains primal feasibility across iterations allowing anytime algorithm. furthermore non-convex also show convergence local optimum finally ﬁrst work consider modeling global preferences augmenting tractable base inference objective non-local terms. example generalized meanﬁeld variational inference algorithms augment tractable distribution non-linear nonconvex global energy function scores terms full model using products marginals special case non-local inference framework present algorithms solving problem much general compelling applications. additionally modeling utility provided global preferences motivated work dual decomposition inference loopy globally-constrained models decomposed repeated calls inference tractable independent subproblems seen wide success ease implementation since reuses existing inference routines black boxes. however technique restricted modeling linear constraints imposed priori. similarly types constraints also imposed expectations posterior distribution semi-supervised learning posterior regularization generalized expectation contrast methods designed discriminatively learn expressive inference procedures minimal domain knowledge required rather regularizing inference learning. first provide efﬁcient algorithms solving marginal inference problem performing prediction associated distribution convex non-convex global energy functions. that provide learning algorithm parametrized functions using interpretation approximate variational inference probabilistic model. algorithms easy implement rely simple wrappers around black-box inference subroutines. experiments demonstrate power generality approach achieving state-of-the-art results several tasks. extract accurate citations research papers learning discriminative global regularities valid outputs outperforming strong dual decomposition-based baseline benchmark task achieve state-of-the-art results learned non-convex non-local energy function guides output decodings near dictionary words. finally general algorithm solving provides large speed improvements challenging task inference chain-structured collective graphical models applied bird migration denote discrete variables collection input features. deﬁne conditional distribution expθ s)/z mapping sufﬁcient statistics differentiable vector-valued mapping expθ conditional random ﬁelds assume given graph structure maps vector capturing joint settings clique going forward often suppress explicit dependency ﬁxed model called markov random ﬁeld given distribution deﬁne expected sufﬁcient statistics operator statistics above concatenated vector node clique marginals. therefore marginal inference task ﬁnding marginal distribution equivalent computing expectation tree-structured graphical models bijection true general graphs. trees entropy bethe entropy deﬁned example wainwright jordan marginal polytope correspond tree-structured crfs marginal inference performed efﬁciently using dynamic programming. experiments focus graphs. however inference algorithms present extended general graphs wherever marginal inference tractable using convex entropy approximation local polytope relaxation. mrf. assuming available marginal inference routine assume tractability example using dynamic program. avoid predicting locally maximizing nodes’ marginals since would necessarily yield feasible outputs. instead solving could introduced global energy terms objective directly values rather expectations however yields difﬁcult combinatorial optimization problem prediction yield natural learn parametrization global energy. section demonstrates using energy terms deﬁned marginals performing inference associated performs well better technique designed directly perform subject global penalty terms. second variational interpretation characterizes variational approximation complex joint distribution assume isolated marginal inference tractable alternative structured distribution efﬁcient inference algorithm. speciﬁcally assume solved furthemore assume convex function conditional input features going forward often surpress dependence above normalizing constant combined distribution. note linear inference would tractable since distribution would decompose cliques surprisingly intractable reason about non-local terms approximate variational distribution connection variational approximation proposition derived appendix here assume clique structure show minimizing variational approximation ||pc) given yields parametrized compactly proposition discuss relationship general meanﬁeld inference section although analysis section assumes convexity inference techniques applied nonconvex discussed section learning algorithm produces state-of-the-art results even nonconvex regime benchmark task. expected node scores clique scores entropy also global functions marginals. concrete citation extraction experiments example employ simple structure next provide complementary interpretations variational inference class tractable probability distributions yield precisely variational expression. however useful ﬁrst helps motivate prediction algorithm second helps characterize learning algorithm section variational proposition ﬁxed output inference augmented objective equivalent output standard inference clique structure base model modiﬁed parameter therefore characterize joint distribution ﬁrst ﬁnding solving deﬁning parameters even conveniently inference technique section iteratively estimates namely dual iterate algorithm ultimately many prediction problems seek single output conﬁguration rather inferred distribution outputs. proposition suggests simple prediction procedure ﬁrst variational distribution parametrized parameter then perform yields projection objective form terms come lagrangian relaxation regularity constraints corresponds dual variables. originally employed linear constraints marginals extend framework arbitrary convex differentiable functions. similarly inference problem arises perform posterior inference assuming observations corrupted noise model. tarlow zemel also present method learning certain forms non-local losses max-margin framework. goals different learning methods. impose non-local terms order regularize learning process allow cope minimal annotation. instead increase expressivity model performing inference every test example using different since depends input features. since effectively ‘learning regularizer’ fully-labeled data learning approach section differs methods. finally unlike frameworks employ non-convex terms experiments. algorithmic consequences non-convexity discussed section present approach solving using noneuclidean projected gradient methods require access procedure marginal inference base distribution well access gradient energy function pose algorithms composite minimization framework gives access wide variety algorithms discussed supplementary material. technique estimates marginal properties complex joint distribution using clique marginals tractable base distribution necessarily fully factorized. induces partitioning cliques represented directly deﬁne clique marginals product distribution relevant nodes’ marginals account energy terms full model involving cliques absent simple base model energy base model augmented extra function note non-linear non-convex. work generalizes allowing arbitrary non-linear interaction terms components powerful example citation extraction experiments section expressing global terms standard graphical model would require many factors touching variables. local coordinate ascent mean-ﬁeld frustrated rigid global terms. gradient-based method avoids issues updating marginals simultaneously. dual decomposition popular method performing inference complex structured prediction models leveraging repeated calls tractable submodels family models solvable dual decomposition limited however terms link submodels must expressible linear constraints. similar techniques based alternating direction method multipliers adapted marginal inference problems marginal inference submodels tractable. however non-local terms deﬁned linear functions settings graphical model nodes non-linear terms provide practitioners expressive means learn enforce regularities inference output. posterior regularization learning measurements liang generalized expectations family closely-related techniques performing unsupervised semi-supervised learning conditional distribution generative model using expectation-maximization e-step latent variables come directly inference model instead projection onto expectations obeying global regularity properties. composite minimization family techniques minimizing functions form oracle allows compute minimizations closed form problems form often solved algorithm called proximal gradient minimizes convex using decreasing sequence learning rates note requirement proximal gradient generalizes projected gradient descent since unconstrained minimization might take feasible region computing update requires projecting onto reason squared euclidean distance computing updates performing projection. fact squared term replaced bregman divergence. family algorithms includes mirror descent dual averaging algorithms average gradient en¯gt countered far. beneﬁt require learning rate parameter using strongly convex regularizer. interpreted projection onto using bregman divergence generated strongly convex function non-euclidean proximal algorithms especially helpful unable compute projection terms euclidean distance using different bregman divergence. show exactly case problem projected inference marginal oracle allows project terms divergence. maintain tractability avoid using however entropy function exponentially-large simplex instead optimize structured factorized marginal polytope corresponding structured bethe entropy tree-structured models identical values different inputs. remains show strong convexity rda. proposition trees nodes negative bethe −-strongly convex entropy function respect -norm interior marginal polytope deﬁnitions hand present bethe-rda projected inference algorithm algorithm corresponds instantiating −hb. note simplicity algorithm choosing intuitively appealing algorithm amounts calling marginal inference oracle iteratively modiﬁed parameters. proposition convex energy functions convex sequence primal averages algorithm converges optimum variational objective suboptimality structure energy functions speciﬁcally lipschitz-continuous gradient modify algorithm nesterov’s acceleration technique details found achieve convergence updates inference algorithms well-deﬁned. importantly since motivations developing non-local inference interacts data complex manner prevents using standard learning techniques exponential family. namely easily differentiate likelihood respect since requires differentiating output convex optimization procedure extra term prevents conjugate duality relationships available exponential family. could used automatic methods differentiate iterative inference procedure found learning algorithm works well. employ variational learning algorithm presented algorithm alternately updating parameters tractable crf-structured variational distributions updating parameters assuming following surrogate likelihood given approximations given update using algorithm given update taking single step direction gradient surrogate likelihood avoid taking gradient step since gradients depend update break property therefore recompute every time update parameters. overall remains show compute gradients standard likelihood gradient unlike loopy belief propagation however since derive algorithms framework composition minimization access wealth theoretical guarantees. based results theory optimization ﬁrstorder surrogate loss functions appendix propose small modiﬁcation algorithm asymptotic convergence condition even non-convex energies. practice unmodiﬁed algorithm also works well problems experimentally section good performance inference learning non-convex energy functions. seek learn parameters underlying base model respectively. training examples. variational distribution resulting applying proposition namely parameters ﬁrst apply algorithm task performing text ﬁeld segmentation umass citation dataset contains strings citations research papers segmented ﬁelds modeling approach closely follows anzaroot extract segmentations using linearchain segmentation model large ‘soft’ linear global regularity constraints. candidate labeling. imagine example constrain predicted segmentations predicted last names ﬁrst names. then numbers ﬁrst last names computed linear measurements lasts respectively. hard constraint would enforce lasts relaxed anzaroot penalty term parameters underlying linear-chain model. dual decomposition style algorithm solving crucially relies speciﬁc structure hinge terms learn hundreds ‘soft constraints’ using perceptron-style algorithm. consider measurement vectors impose non-local terms marginals rather speciﬁc values further smoothed hinge functions improve convergence rate inference variational distribution solving marginal inference version instance inference framework linear measurements simple interpretation gradient respect equals gradient scalar loss current marginals times difference linear measurements ground truth labels inferred marginals. algorithm expensive double-loop structure. practice sufﬁcient employ ‘doubly-stochastic’ version given algorithm sample training example perform single gradient step demonstrate simplicity implementing learning algorithm avoid abstract derivative notation algorithm specializing case experiments however sometimes linear measurements. overall experiments fast doubly-stochastic approach algorithm solely since performs well. general learning algorithms guaranteed converge approximate complex interaction alternating updates. practice however terminating ﬁxed number iterations yields models generalize well. finally recall notation suppresses potential dependence assume differentiable function features therefore experiments depends perform gradient updates parametrization application chain rule. table comparison scores citation extraction dataset. compare inference scores nonlocal energy model specialized dual decomposition model anzaroot variants learn global regularities signiﬁcantly improve performance. anzaroot ﬁrst learn chain parameters training set. then learn parameters development using algorithm tune hyperparameters development performance. train test time ignore terms present results table measuring segmentlevel baseline chain slightly higher accuracy baseline approach anzaroot possibly optimization differences. augmented model matches slightly beats soft dual decomposition procedure. especially impressive employ specialized linear-programming solver learning algorithm adapted task inference hingeloss soft constraints whereas simply plug general learning inference algorithms non-local structured prediction applicable energy functions. comparable performance provides experimental evidence intuition preferences conﬁgurations expressed functions expectations. anzaroot solve penalized problem directly prediction algorithm ﬁrst ﬁnds distribution satisfying preferences performs standard inference distribution. finally figure present results demonstrating algorithm’s high performance obtained using calls test example inference underlying chain model. section analyze empirical convergence behavior algorithm table character-wise accuracy baselines models using learned non-local energies handwriting recognition dataset. note word classiﬁer baseline also given character-wise accuracy comparison. table character-wise accuracy baselines models using learned non-local energies handwriting recognition dataset. note word classiﬁer baseline also given character-wise accuracy comparison. structured prediction cascades weiss achieve high performance dataset using extremely high order cliques characters consider small number candidate outputs. state-of-the-art results reproduced table excellent performance large-clique models consequence fact data contains unique words written different people. model access enough higher-order context problem becomes much easier solve. mind design non-convex non-local energy functions. energies intended regularize predictions close known elements vocabulary. base model standard linear-chain image features nodes features function takes concatenated vector node edge marginals sums node marginals giving global unigram expected sufﬁcient statistics. {ui} indicate unique vectors applying train empirical sufﬁcient statistics data case simply gives vectors length containing unigram counts unique word train set. intuition would like able nudge results inference chain model pulling inferred close global statistics vectors. following non-convex non-local energy function model learn variants model differently parametrize dependence ﬁrst single bias feature non-local energy. second conditions global representation sequence concretely approximate kernel mean using random fourier features simply involves multiplying image feature vector sequence random matrix rows applying pointwise non-linearity taking linear function average vector. results experiments seen table adding non-local energy brings performance well baseline bigram chain model training procedure able give substantially better performance depends input features. energy based unigram sufﬁcient statistics able capture relative ordering letters vocabulary words structured prediction cascades models capture. motivates consider another energy function. {wi} {µn} unique vectors concatenated node marginal statistics train set. gives vectors length length distinct train word. next deﬁne different energy function base chain model table comparison runtime interior point solver sheldon v.s. algorithm different problem sizes cardinality edge potentials underlying graphical model marginal inference previous work cgms successfully applied modeling bird migration. here base model linear chain representing time series bird locations. observed variable corresponds counts bird watchers different locations. observations assumed poisson distributed rate proportional true count birds present. task infer underlying migration patterns. case underlying graph tree ‘hard learning algorithm sheldon algorithm specialized model. therefore sheldon provide additional experimental evidence alternating surrogate-likelihood optimization works well practice. learning procedure sheldon computationally expensive solve instances using interior-point solver inner loop. special case trees algorithm directly applicable using synthetic data code obtained authors compare generic solver algorithm solving instances table method achieves large speed-up loss solution accuracy results show inference learning framework allows tractable modeling non-local dependency structures resistant traditional probabilistic formulations. approaching structured modeling independence assumptions arbitrary penalty functions marginal vectors open many modeling possibilities. additionally generic gradient-based inference method achieve substantial speedups preimplement featurized non-featurized versions model. noted structured prediction cascades giving model access level highorder structure data makes inference problem extremely easy. model outperforms best structured prediction cascades results note improvement using featurized non-featurized course since dataset actual labels valid different input sequences length mismatches arguably classiﬁcation problem much structured prediction problem. address this create another baseline constrained -class logistic regression classiﬁer global meanmap features variants structured model report results table also tune number random fourier features hyperparameter give classiﬁer much expressive power possible. performance still signiﬁcantly best structured models indicating interplay between local global structure important. collective graphical models next demonstrate proximal gradient-based inference framework dramatically speeds approximate inference collective graphical models cgms method structured learning inference noisy aggregate observation data. large-scale dependency structure represented graphical model nodes represent single variables aggregate sufﬁcient statistics large sets underlying variables corrupted noise model. work supported part center intelligent information retrieval part darpa agreement number fa--- part grant cns-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect sponsor.", "year": 2015}