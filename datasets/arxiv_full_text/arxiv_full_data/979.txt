{"title": "Pointer Networks", "tag": ["stat.ML", "cs.CG", "cs.LG", "cs.NE"], "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.", "text": "introduce neural architecture learn conditional probability output sequence elements discrete tokens corresponding positions input sequence. problems cannot trivially addressed existent approaches sequence-to-sequence neural turing machines because number target classes step output depends length input variable. problems sorting variable sized sequences various combinatorial optimization problems belong class. model solves problem variable size output dictionaries using recently proposed mechanism neural attention. differs previous attention attempts that instead using attention blend hidden units encoder context vector decoder step uses attention pointer select member input sequence output. call architecture pointer show ptr-nets used learn approximate solutions three challenging geometric problems ﬁnding planar convex hulls computing delaunay triangulations planar travelling salesman problem using training examples alone. ptr-nets improve sequence-to-sequence input attention also allow generalize variable size output dictionaries. show learnt models generalize beyond maximum lengths trained hope results tasks encourage broader exploration neural learning discrete problems. recurrent neural networks used learning functions sequences examples three decades however architecture limited settings inputs outputs available ﬁxed frame rate recently introduced sequence-to-sequence paradigm removed constraints using input sequence embedding another embedding output sequence. bahdanau augmented decoder propagating extra contextual information input using content-based attentional mechanism developments made possible apply rnns domains achieving state-of-the-art results core problems natural language processing translation parsing image video captioning even learning execute small programs nonetheless methods still require size output dictionary ﬁxed priori. constraint cannot directly apply framework combinatorial problems size output dictionary depends length input sequence. paper address limitation repurposing attention mechanism create pointers input elements. show resulting architecture name pointer networks trained output satisfactory solutions three combinatorial optimization problems computing planar convex hulls delaunay triangulations symmetric planar travelling salesman problem resulting models produce approximate solutions problems purely data driven fashfigure sequence-to-sequence processes input sequence create code vector used generate output sequence using probability chain rule another rnn. output dimensionality ﬁxed dimensionality problem training inference ptr-net encoding converts input sequence code generating network step generating network produces vector modulates content-based attention mechanism inputs output attention mechanism softmax distribution dictionary size equal length input. propose architecture call pointer simple effective. deals fundamental problem representing variable length dictionaries using softmax probability distribution pointer. apply pointer model three distinct non-trivial algorithmic problems involving geometry. show learned model generalizes test problems points training problems. pointer model learns competitive small scale approximate solver. results demonstrate purely data driven approach learn approximate solutions problems computationally intractable. sequence-to-sequence model given training pair sequence-to-sequence model computes conditional probability using parametric model estimate terms probability chain rule i.e. training examples. long short term memory model time step input sequence reached time special symbol input model. model switches generation mode network encounters special symbol represents termination output sequence. note model makes statistical independence assumptions. separate rnns call former encoder latter decoder generative rnn. inference given sequence learnt parameters used select sequence highest probability i.e. finding optimal sequence computationally impractical combinatorial number possible output sequences. instead beam search procedure best possible sequence given beam size. sequence-to-sequence model output dictionary size symbols ﬁxed equal since outputs chosen input. thus need train separate model prevents learning solutions problems output dictionary size depends input sequence length. assumption number outputs model computational complexity however exact algorithms problems dealing costly. example convex hull problem complexity attention mechanism adds computational capacity model. content based input attention vanilla sequence-to-sequence model produces entire output sequence using ﬁxed dimensional state recognition input sequence constrains amount information computation generative model. attention model ameliorates problem augmenting encoder decoder rnns additional neural network uses attention mechanism entire sequence encoder states. notation purposes deﬁne encoder decoder hidden states respectively. lstm rnns state output gate component-wise multiplied cell activations. compute attention vector output time follows softmax normalizes vector attention mask inputs learnable parameters model. experiments hidden dimensionality encoder decoder vector square matrices. lastly concatenated used hidden states make predictions feed next time step recurrent model. describe simple modiﬁcation attention model allows apply method solve combinatorial optimization problems output dictionary size depends number elements input sequence. sequence-to-sequence model section uses softmax distribution ﬁxed sized output dictionary compute equation thus cannot used problems size output dictionary equal length input sequence. solve problem model using attention mechanism equation follows softmax normalizes vector output distribution dictionary inputs learnable parameters output model. here blend encoder state propagate extra information decoder instead pointers input elements. similar condition equation simply copy corresponding pci− input. method attention model seen application content-based attention mechanisms proposed also note approach speciﬁcally targets problems whose outputs discrete correspond positions input. problems addressed artiﬁcially example could learn output coordinates target point directly using rnn. however inference solution respect constraint outputs back inputs exactly. withconstraints predictions bound become blurry longer sequences shown sequence-to-sequence models videos following sections review three problems considered well data generation protocol. training data inputs planar point sets elements each cartesian coordinates points convex hull delaunay triangulation solution corresponding travelling salesman problem. cases sample uniform distribution outputs sequences representing solution associated point figure illustration input/output pair convex hull delaunay problems. used example baseline develop models understand difﬁculty solving combinatorial problems data driven approaches. finding convex hull ﬁnite number points well understood task computational geometry several exact solutions available general ﬁnding solution complexity number points considered. vectors uniformly sampled elements indices corresponding positions sequence special tokens representing beginning sequence. figure illustration. represent output sequence start point lowest index counter-clockwise arbitrary choice helps reducing ambiguities training. delaunay triangulation delaunay triangulation points plane triangulation circumcircle every triangle empty point interior. exact solutions available number points example outputs corresponding sequences representing triangulation point triple integers corresponding position triangle vertices beginning/end sequence tokens. figure note permutation sequence represents triangulation additionally triangle representation three integers also permuted. without loss generality similarly convex hulls training time order triangles incenter coordinates choose increasing triangle representation. without ordering models learned good ﬁnding better ordering ptr-net could better exploit part future work. arises many areas theoretical computer science important algorithm used microchip design sequencing. work focused planar symmetric given list cities wish shortest possible route visits city exactly returns starting point. additionally assume distance cities opposite direction. np-hard problem allows test capabilities limitations model. input/output pairs similar format convex hull problem described section cartesian coordinates representing cities chosen randomly square. permutation integers representing optimal path consistency training dataset always start ﬁrst city without loss generality. generate exact data implemented held-karp algorithm ﬁnds optimal solution larger producing exact solutions extremely costly therefore also considered algorithms produce approximated solutions implements christoﬁdes algorithm. latter algorithm guaranteed solution within factor optimal length. table shows performed test sets. extensive architecture hyperparameter search ptr-net done work presented here used virtually architecture throughout experiments datasets. even though likely gains obtained tuning model felt model hyperparameters operate problems would make main message paper stronger. result models used single layer lstm either hidden units trained stochastic gradient descent learning rate batch size random uniform weight initialization gradient clipping generated training example pairs observe overﬁtting cases task simpler used convex hull guiding task allowed understand deﬁciencies standard models sequence-to-sequence approach also setting expectations purely data driven model would able achieve respect exact solution. reported metrics accuracy area covered true convex hull compute accuracy considered output sequences represent polygon. simplicity computed area coverage test examples output represents simple polygon algorithm fails produce simple polygon cases simply reported fail. results presented table note area coverage achieved ptr-net close looking examples mistakes problems come points aligned mistake common source errors algorithms solve convex hull. seen order inputs presented encoder inference affects performance. points true convex hull seen late input sequence accuracy lower. possibly network enough processing steps update convex hull computed latest points seen. order overcome problem used attention mechanism described section allows decoder look whole input time. modiﬁcation boosted model performance signiﬁcantly. inspected attention focusing observed pointing correct answer input side. inspired create ptr-net model described section outperforming lstm lstm attention model advantage inherently variable length. bottom half table shows that training model variety lengths ranging single model able perform quite well lengths trained impressive fact model extrapolate lengths never seen training. even results satisfactory indirectly indicate model learned simple lookup. neither lstm lstm attention used given without training model delaunay triangulation test case connected ﬁrst problem ﬁnding convex hull. fact delaunay triangulation given points triangulates convex hull points. reported metrics accuracy triangle coverage percentage note that case input point output sequence fact set. consequence permutation elements represent triangulation. figure examples model convex hulls delaunay trained points tested points. failure lstm sequence-to-sequence model convex hulls shown note baselines cannot applied different length training. using ptr-net model obtained accuracy triangle coverage accuracy triangle coverage produce precisely correct triangulation obtained triangle coverage. middle column figure example considered planar symmetric travelling salesman problem np-hard third problem. similarly ﬁnding convex hulls also sequential outputs. given ptrnet implements algorithm unclear would enough capacity learn useful algorithm solely data. discussed section feasible generate exact solutions relatively small values used training data. larger importance good efﬁcient algorithms providing reasonable approximate solutions exist. used three different algorithms experiments example beam search procedure consider valid tours. otherwise ptr-net model would sometimes output invalid tour instance would repeat cities decided ignore destination. procedure relevant least instances would produce valid tour. ﬁrst group rows table show ptr-net trained optimal data except since feasible computationally interestingly using worst algorithm data train ptr-net model outperforms algorithm trying imitate. second group rows table show ptr-net trained optimal data cities generalize beyond that. results virtually perfect good seems break beyond contrasts convex hull case able generalize factor however underlying algorithms greater complexity could explain phenomenon. conclusions paper described ptr-net architecture allows learn conditional probability sequence given another sequence sequence discrete tokens corresponding positions show ptr-nets used learn solutions three different combinatorial optimization problems. method works variable sized inputs something baseline models cannot directly. even impressively outperform baselines ﬁxed input size problems models applied. previous methods rnnsearch memory networks neural turing machines used attention mechanisms process inputs. however methods directly address problems arise variable output dictionaries. shown attention mechanism applied output solve problems. doing opened class problems neural networks applied without artiﬁcial assumptions. paper applied extension rnnsearch methods equally applicable memory networks neural turing machines. future work show applicability problems sorting outputs chosen inputs. also excited possibility using approach combinatorial optimization problems. would like thank rafal jozefowicz ilya sutskever quoc samy bengio useful discussions topic. would also like thank daniel gillick help ﬁnal manuscript.", "year": 2015}