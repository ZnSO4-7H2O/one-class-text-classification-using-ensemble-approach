{"title": "Credible Review Detection with Limited Information using Consistency  Analysis", "tag": ["cs.AI", "cs.CL", "cs.IR", "cs.SI", "stat.ML"], "abstract": "Online reviews provide viewpoints on the strengths and shortcomings of products/services, influencing potential customers' purchasing decisions. However, the proliferation of non-credible reviews -- either fake (promoting/ demoting an item), incompetent (involving irrelevant aspects), or biased -- entails the problem of identifying credible reviews. Prior works involve classifiers harnessing rich information about items/users -- which might not be readily available in several domains -- that provide only limited interpretability as to why a review is deemed non-credible. This paper presents a novel approach to address the above issues. We utilize latent topic models leveraging review texts, item ratings, and timestamps to derive consistency features without relying on item/user histories, unavailable for \"long-tail\" items/users. We develop models, for computing review credibility scores to provide interpretable evidence for non-credible reviews, that are also transferable to other domains -- addressing the scarcity of labeled data. Experiments on real-world datasets demonstrate improvements over state-of-the-art baselines.", "text": "abstract. online reviews provide viewpoints strengths shortcomings products/services inﬂuencing potential customers’ purchasing decisions. however proliferation non-credible reviews either fake incompetent biased entails problem identifying credible reviews. prior works involve classiﬁers harnessing rich information items/users might readily available several domains provide limited interpretability review deemed non-credible. paper presents novel approach address issues. utilize latent topic models leveraging review texts item ratings timestamps derive consistency features without relying item/user histories unavailable longtail items/users. develop models computing review credibility scores provide interpretable evidence non-credible reviews also transferable domains addressing scarcity labeled data. experiments real-world datasets demonstrate improvements state-of-the-art baselines. motivation online reviews hotels restaurants consumer goods movies books drugs etc. invaluable resource internet users providing wealth related information potential customers. unfortunately corresponding forums tripadvisor yelp amazon others increasingly game manipulative deceptive reviews fake incompetent biased example recent studies depict yelp reviews might fake yelp internally rejects user submissions not-recommended; similar ﬁgures reported reviews amazon. starting work research efforts undertaken automatically detect non-credible reviews. parallel industry developed standards ﬁlter illegitimate reviews. although details disclosed studies suggest ﬁlters tend fairly crude instance exploiting user activity like number reviews posted treating users whose ratings show high deviation mean/majority ratings suspicious. policy seems over-emphasize trusted long-term contributors suppress outlier opinions mainstream. moreover ﬁlters also employ several aggregated metadata thus hardly viable items initially reviews often active users newcomers community. state research topic cast problem review credibility binary classiﬁcation task review either credible deceptive. supervised semi-supervised methods developed largely rely features users activities well statistics item ratings. techniques also consider spatio-temporal patterns user activities like addresses user locations burstiness posts item item group correlation measures across users items however classiﬁers built mostly geared popular items meta-information user histories activity correlations always available. example someone interested opinions long-tail bed-and-breakfast rarely visited town helped methods. several existing works consider textual content user reviews tackling opinion spam using word-level unigrams bigrams features along speciﬁc lexicons psycholinguistic lexicon wordnet affect learn latent topic models classiﬁers although methods achieve high classiﬁcation accuracy various gold-standard datasets provide interpretable evidence certain review classiﬁed non-credible. problem statement paper focuses detecting credible reviews limited information namely absence rich data user histories community-wide correlations long-tail items. extreme case provided review texts ratings item. goal compute credibility score reviews provide possibly interpretable evidence explaining certain reviews categorized non-credible. approach proposed method learn model based latent topic models combining limited metadata provide novel notion consistency features characterizing review. lda-based joint sentiment topic model cast user review texts number informative facets. per-item aggregating text among reviews item also per-review. allows identify score highlight inconsistencies appear review community’s overall characterization item. perform item whole also latent facets separately. additionally learn inconsistencies discrepancy contents review rating temporal bursts number reviews written short span time targeting item. propose kinds inconsistencies form assets credibility scoring model support vector machine classiﬁcation ordinal ranking. contribution summary contributions summarized model develop novel consistency model credibility analysis reviews works limited information particular attention long-tail items offers interpretable evidence reviews classiﬁed non-credible. tasks investigate credibility scores affect overall ranking items. address scarcity labeled training data transfer learned model yelp amazon rank top-selling items based credible user experiments perform extensive experiments tripadvisor yelp amazon demonstrate viability method advantages state-of-the-art baselines dealing long-tail items providing interpretable evidence. previous works fake review opinion spam detection primarily focused different aspects problem linguistic analysis approach exploits distributional difference wordings authentic manually-created fake reviews using word-level features. however artiﬁcially created fake review datasets studied tasks give away explicit features dominant real-world data. conﬁrmed study yelp ﬁltered reviews n-gram features performed poorly despite outstanding performance amazon mechanical turk generated fake review dataset. additionally linguistic features text sentiment readability score flesch reading ease etc.) textual coherence rules based probabilistic context free grammar studied. rating activity analysis absence proper ground-truth data prior works make simplistic assumptions e.g. duplicates near-duplicates fake make extensive background information like brand name item description user history addresses location etc. thereafter regression models trained features used classify reviews credible deceptive. works also crude ad-hoc language features like content similarity presence literals numerals capitalization. contrast works approach uses limited information users items catering broad domain applications. harvest several consistency features user rating review text give interpretation review deemed non-credible. learning rank supervised models also developed rank items constructed item feature vectors techniques optimize measures like discounted cumulative gain kendall-tau reciprocal rank generate item ranking similar training data based feature vectors. technique show performance improved removing non-credible item reviews. previous works linguistic analysis explore distributional difference wordings deceptive authentic reviews. general authentic reviews tend sensorial concrete language deceptive reviews higher usage nouns adjectives prepositions determiners coordinating conjunctions; whereas deceptive reviews shown verbs adverbs superlatives manifested exaggeration imaginary writing. found authentic hotel reviews speciﬁc spatial conﬁgurations aspects like location amenities cost; whereas deceptive reviews focus aspects external item reviewed extreme opinions also found dominant deceptive reviews assert stances whereas authentic reviews balanced view analyzing item several aspects. implicitly exploit features latent facet model reviewer opinion important facets item consideration overall rating distribution obtained facet level opinions. order explicitly capture distributional difference language credible non-credible reviews word-level unigram bigram language features shown outperform ﬁne-grained linguistic features using psycholinguistic features part-of-speech tags also experimented wordnet affect capture ﬁne-grained emotional dimensions which however seen perform well. general bigram features capture context-dependent information extent together simple unigram features performed best. also observed presence absence words mattered frequency credibility analysis. model features length normalized retaining punctuations capitalization non-credible reviews manifesting exaggeration tend overuse latter features feature vector construction consider vocabulary unique unigrams bigrams corpus token type review compute presence/absence words type occurring thus constructing feature vector length∀i denoting indicator function given review snippets like hotel offers free wi-ﬁ different facets present reviews along corresponding sentiment polarities. since work present model requiring limited prior information extract latent facets review text without help explicit facet seed words. ideal machinery wi-ﬁ latent facet cluster like network internet computer access .... also want extract sentiment expressed review facet. interestingly although free polarity example free conjunction wi-ﬁ expresses positive sentiment service offered without charge. hope although free individual polarity appears neighborhood words known polarities helps joint discovery facets sentiment labels free wi-ﬁ internet without extra charge ideally facet cluster similar polarities using co-occurrence similar words positive polarities. work joint sentiment topic model approach jointly discover latent facets along expressed polarities. consider reviews written users items rating assigned review review document consists sequence words denoted ...wnd} word drawn vocabulary indexed consider facet assignments ...zk} sentiment label assignments ...ll} possible facets label possible sentiment labels. adds layer sentiment addition topics standard assumes document associated multinomial distribution facets sentiment labels symmetric dirichlet prior denotes probability occurrence facet polarity document topics multinomial distribution words drawn vocabulary symmetric dirichlet prior denotes probability word belonging facet polarity generative process sentiment label ﬁrst chosen document-speciﬁc rating distribution symmetric dirichlet prior thereafter chooses facet conditioned subsequently word conditioned exact inference possible intractable coupling between thus collapsed gibbs sampling approximate inference. denote count word occurring document belonging facet polarity conditional distribution latent variable given equation operator count indicates marginalization i.e. summing counts values corresponding position subscript denotes value variable excluding data position. extract following features latent facet model enabling detect inconsistencies reviews ratings items credibility analysis. user review facet description facet-label distribution different items differ; items certain facets important dimensions. instance battery life ease consumer electronics important color; hotels certain services available free charged elsewhere. similarly user reviews involving less relevant facets item discussion e.g. downrating hotels allowing pets also detected. given review item sequence words previously word latent facet dimension consider sentiment label maximizes facet-label-word distribution aggregate dimension used words. facet-label distribution review feature vector classiﬁer ﬁgure importance different latent dimensions also captures domain-speciﬁc facet-label importance. user review rating user-assigned rating corresponding review consistent opinion expressed review text. example user unlikely give average rating item expresses positive opinion important facets item. inferred rating distribution review consisting sequence words learned computed word consider facet label jointly maximizes facet-labelword distribution aggregate words facets. absolute deviation user-assigned rating estimated rating user text taken component overall feature vector. user rating previous works opinion spam found fake reviews tend overtly positive overtly negative opinions. therefore also component overall feature vector detect cues extreme ratings. temporal burst typically observed group spamming number reviews posted targeting item short span time. consider reviews {dj} timepoints {tj} posted speciﬁc item. temporal burstiness review weigh temporal proximity reviews capture burst. user review item description general description facets outlined user review item differ much majority. example majority says hotel offers free wi-ﬁ user review says internet charged presents possible inconsistency. facet model corresponds word clusters facet label different sentiment labels. experiments however feature play weak role presence inconsistency features. reviews item obtain facet-label distribution item. jensenshannon divergence symmetric smoothed version kullback-leibler divergence feature. depicts much facet-label distribution given review diverges general opinion people item. earlier works review spam show user-dependent models detecting user-preferences biases perform well credibility analysis. however information always available especially newcomers active users community. besides show spammers tend open multiple fake accounts write reviews malicious activities using accounts sparsely avoid detection. therefore instead relying extensive user history simple proxies user activity easier aggregate community user rating behavior absolute deviation review rating mean median rating user items well ﬁrst three moments user rating distribution capturing scenario user typical rating behavior across items. item rating pattern absolute deviation item rating mean median rating obtained users captures extent user disagrees users item quality; ﬁrst three moments item rating distribution captures general item rating pattern. since detect credible reviews case limited information split activity behavioral features components activity− using features straightforward obtained tuple userid itemid review rating easily available even long-tail items newcomers; activity+ using listed features. however latter requires additional information might always available takes long time aggregate items/users. feature vector construction review user construct behavioral feature vector using features. credible review classiﬁcation ﬁrst task classify reviews credible not. review user construct joint feature vector support vector machines classiﬁcation reviews. maps examples high dimensional space constructs hyperplane separate categories examples. although inﬁnite number hyperplanes possible constructs largest functional margin given distance nearest point hyperplane side points mapped space classiﬁed category based side hyperplane lies. linear kernel shown perform best text classiﬁcation tasks. regularized loss dual formulation liblinear package default parameters. report classiﬁcation accuracy -fold cross-validation ground-truth tripadvisor yelp. item ranking scarcity ground-truth data pertaining review credibility suitable evaluate model examine effect non-credible reviews relative ranking items community. instance case popular items large number reviews even fraction non-credible effect would severe would long-tail items fewer reviews. simple goodness item aggregate ratings reviews using also obtain ranking items. model ﬁlter noncredible reviews aggregate ratings credible reviews re-compute item ranks. evaluation measures kendall-tau rank correlation co-efﬁcient effectiveness rankings reference ranking instance sales rank items amazon. measures number concordant discordant pairs whether ranks elements agree based scores total number combinations possible. given observations pair observations said concordant either discordant otherwise. ranks tied neither discordant concordant. kendall-tau-b measure allows rank adjustment. consider number concordant discordant tied pairs tied pairs respectively whereby kendall-tau-b given however conservative estimate multiple items typically topselling ones amazon rating therefore second estimate considers non-zero tied ranks concordant. note that item zero-rank reviews classiﬁed non-credible. high positive value kendall-tau indicates series positively correlated; whereas value close zero indicates independent. domain transfer yelp amazon typical issue credibility analysis task scarcity labeled training data. ﬁrst task labels yelp spam filter train model. however ground-truth labels available amazon. although principle train model myelp yelp ﬁlter non-credible reviews amazon. transferring learned model yelp amazon entails using learned weights features yelp analogous ones amazon. however process encounters following issues facet distribution yelp different amazon therefore facet-label distribution corresponding learned feature weights yelp cannot directly used latent dimensions different. additionally speciﬁc metadata like check-in user-friends elite-status missusing components common yelp amazon ﬁrst re-train model myelp yelp remove non-contributing features amazon. direct transfer model weights yelp amazon assumes distribution credible non-credible reviews corresponding feature importance domains necessarily true. order boost certain features better identify non-credible reviews amazon tune soft margin parameter svm. c-svm slack variables optimizes minwbξi≥ subject regularization parameters positive negative class respectively. parameters provide trade wide margin made moving around certain points incurs penalty {cξi}. high value instance places large penalty mis-classifying instances negative class therefore boosts certain features class. value increases model starts classifying reviews non-credible. worse case reviews item classiﬁed non-credible leading aggregated item rating zero. optimal value varying interval ...} using validation amazon shown figure observe increases also increases till certain point non-credible reviews ﬁltered stabilizes. ranking previous approach uses model myelp trained yelp reference ranking amazon used evaluating item ranking using kendall-tau measure. objective obtain good item ranking based credible reviews model mamazon directly optimizes kendall-tau using reference ranking training labels. allows entire feature space available amazon including explicit facet-label distribution full vocabulary could used earlier. feature space constructed similarly yelp. goal ranking learn ranking function concordant given ordering items. objective learn data pairs although problem known np-hard approximated using techniques pairwise slack variables ξij. optimization problem equivalent classifying operating review text associated rating unigrams bigrams vocab. token types word token type review indicator presence/absence words facets sentiment labels resp. pairwise difference vectors corresponding labels indicating ranked ahead. implementation maximizes empirical kendall-tau minimizing number discordant pairs. unlike classiﬁcation task labels per-review ranking task requires labels per-item. consider fijk feature vector review item indexing element feature vector. aggregate feature vectors element-wise reviews item obtain feature vector parameter initialization sentiment lexicon consisting positive negative polarity bearing words used initialize review text based facetlabel-word tensor prior inference. consider number topics yelp amazon review sentiment labels {+−} initialized randomly. symmetric dirichlet priors datasets ground-truth work consider following datasets available ground-truth information. tripadvisor dataset consists reviews tripadvisor positive negative sentiment comprising credible noncredible reviews popular chicago hotels. authors crawled credible reviews online review portals like tripadvisor; whereas non-credible ones generated users amazon mechanical turk. dataset review text sentiment label corresponding hotel names information users items. yelp dataset consists recommended reviews non-recommended reviews given yelp ﬁltering algorithm restaurants chicago. review gather following information userid itemid timestamp rating review metadata. meta-data consists user activity information outlined section reviews marked recommended yelp spam ﬁlter considered ground-truth comparing accuracy credible review detection proposed model. yelp spam ﬁlter presumably relies linguistic behavioral social networking features amazon dataset used consists around reviews nearly users items three domains namely consumer electronics software sports items. review gather information tuple yelp. however metadata dataset rich yelp consisting helpfulness votes reviews. further exists explicit ground-truth characterizing reviews credible deceptive amazon. re-rank items using learning rank implicitly ﬁltering possible deceptive reviews compare ranking item sales rank considered pseudo ground-truth. comparison baselines following state-of-the-art baselines comparison proposed model. language model baselines consider unigram bigram language model baselines shown outperform baselines using psycholinguistic features part-of-speech tags information gain etc. take best baseline work combination unigrams bigrams. proposed model enriches using length normalization presence absence features latent facets etc. recently proposed doc-to-vec model based neural networks overcomes weakness bag-of-words models taking context words account learns dense vector representation document train doc-to-vec model dataset baseline model. addition also consider readability review sentiment scores hypothesis writing styles would random diverse customer background. measures reader’s ability comprehend text measured function total number characters words sentences present review sentiment tries capture fraction occurrences positive/negative sentiment words total number words used. activity rating baselines given tuple userid itemid rating review metadata yelp dataset extract possible activity rating behavioral features users proposed speciﬁcally utilize number helpful feedbacks review title length review rating brand names percent positive negative sentiments average rating rating deviation features classiﬁcation. further based recent work also user check-in user elite status information additional features comparison. empirical evaluations experimental setup considers following evaluations credible review classiﬁcation study performance various approaches distinguishing credible review non-credible one. since forms binary classiﬁcation task consider balanced dataset containing equal proportion data classes. yelp dataset item randomly sample equal number credible non-credible reviews tripadvisor dataset already balanced. table shows -fold cross validation accuracy results different models datasets. observe proposed consistency behavioral features exhibit around improvement yelp∗ classiﬁcation accuracy best performing baselines since tripadvisor dataset review text user/activity models could used there. experiment could also performed amazon ground-truth credibility labels reviews absent. item ranking task examine effect non-credible reviews ranking items community. experiment performed amazon using item sales rank ground reference ranking yelp provide item rankings. sales rank provides indication well product selling amazon.com highlights item’s rank corresponding category. baseline item ranking based aggregated rating reviews item. ﬁrst model myelp trained yelp ﬁlters non-credible reviews aggregating review ratings item. second model mamazon trained amazon using svm-rank reference ranking training labels. -fold cross-validation results reported measures kendall-tau table respect reference ranking. svm-rank since ties. ﬁrst model performs substantially better baseline which turn outperformed second model. order effectiveness approach dealing long-tail items perform additional experiment best performing model i.e. mamazon model kendall-tau-m rank correlation items less reviews different domains amazon observe model performs substantially well even items reviews performance progressively getting better reviews per-item. language model bigram language model performs well tripadvisor dataset setting task. workers amazon mechanical turk tasked writing fake reviews guideline knowing hotel amenities website writing reviews. therefore quite difﬁcult facet model contradictions mismatch facet descriptions. consequently facet model gives marginal improvement combined language model. hand yelp dataset real-world therefore noisy. bigram language model doc-to-vec hence perform good previous dataset; neither facet model isolation. however components together give signiﬁcant performance improvement ones isolation also really just like perfect little good space pretty everything come back still right deﬁnitely enough much super free around delicious fresh favorite selection sure friendly dish since huge menu large easy last room guests location time probably helpful great something nice small better sweet though loved happy love anything actually home dirty mediocre charged customer service signature lounge view city nice place hotel staff good service never overpriced several times wait staff signature room outstanding establishment architecture foundation long waste glamour food service terrible great place never wonderful atmosphere signature bill never good food management great food money worst horrible manager service rude table shows unigrams bigrams contributing language feature space joint model credibility classiﬁcation given feature weights c-svm. credible reviews contain function content words balanced opinions highly contributing features mostly unigrams. whereas non-credible reviews contain extreme opinions less function words sophisticated content words consisting signature bigrams catch readers’ attention. behavioral model activity based model perform best isolation combined language consistency features joint model exhibits around improvement performance. additional meta-data like user elite check-in status improves performance activity based baselines typically available newcomers community. model using limited information performs better activity baselines using ﬁne-grained information items user history. incorporating additional user features boosts performance. consistency features order effectiveness facet based consistency features perform ablation tests remove consistency model aggregated model signiﬁcant performance degradation yelp∗ dataset. tripadvisor dataset performance reduction less compared yelp reasons outlined before. consistency features yelp amazon. ratings deceptive reviews corroborate textual description irrelevant facets inﬂuencing rating target item contradicting users expressing extreme opinions without explanation depicting temporal burst ratings etc. principle features also used detect anomalous phenomena like group-spamming scope work. ranking task ranking task amazon ﬁrst model myelp trained yelp tested amazon using c-svm performs much better baseline exploiting various consistency features. second model mamazon never inside james. never checked never visited bar. favorite hotels chicago. james friendly area. loves there. learn actually evangelical christians working proselytize coffee farmers from. this. used turbo since never until now. can’t turbo doesn’t software updates because hurricane katrina. book amazon offers joke provides forward written kalanithi. don’t sample writing know appeals. great. camera takes pictures. greati give starskeep dan’s apartment beautiful great downtown location... highly recommend working nsra... super friendly demonstrating conﬁdent... condo listing activity really stepped in... trained amazon using svm-rank outperforms former exploiting power entire feature space domain-speciﬁc proxy labels unavailable former. long-tail items table shows gradual degradation performance second model mamazon dealing items lesser number reviews. nevertheless observe give substantial kendall-tau correlation reference ranking reviews per-item demonstrating effectiveness model dealing long-tail items. present novel consistency model using limited information detecting noncredible reviews shown outperform state-of-the-art baselines. approach overcomes limitation existing works make ﬁne-grained information available long-tail items newcomers community. importantly prior methods designed explain detected review non-credible. contrast make different consistency features latent facet model derived user text ratings explain assessments method. develop multiple models domain transfer adaptation model performs well ranking tasks involving long-tail items reviews per-item.", "year": 2017}