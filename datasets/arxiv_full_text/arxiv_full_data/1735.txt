{"title": "WordRank: Learning Word Embeddings via Robust Ranking", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.", "text": "embedding words vector space gained attention recent years. stateof-the-art methods provide efﬁcient computation word similarities low-dimensional matrix embedding motivation often left unclear. paper argue word embedding naturally viewed ranking problem ranking nature evaluation metrics. then based insight propose novel framework wordrank efﬁciently estimates word representations robust ranking attention mechanism robustness noise readily achieved dcg-like ranking losses. performance wordrank measured word similarity word analogy benchmarks results compared state-of-the-art word embedding techniques. algorithm competitive state-of-thearts large corpora outperforms signiﬁcant margin training limited million tokens wordrank performs almost well existing methods using billion tokens popular word similarity benchmark. multi-node distributed implementation wordrank publicly available general usage. embedding words vector space semantic syntactic regularities words preserved important sub-task many applications natural language processing. mikolov generated considerable excitement machine learning natural language processing communities introducing neural network based model call wordvec. shown wordvec produces state-of-the-art performance word similarity well word analogy tasks. word similarity task retrieve words similar given word. hand word analogy requires answering queries form ab;c? words vocabulary answer query must semantically related related best illustrated concrete example given query kingqueen;man? expect model output woman. impressive performance wordvec ﬂurry papers tried explain improve performance wordvec theoretically empirically interpretation wordvec approximately maximizing positive pointwise mutual information levy goldberg showed directly optimizing gives good results. hand pennington showed performance comparable wordvec using modiﬁed matrix factorization model optimizes loss. somewhat surprisingly levy showed much performance gains word embedding methods certain hyperparameter optimizations system-design choices. words sets careful experiments existing word embedding models less perform comparably other. conjecture because high level methods based following template large text corpus eliminate infrequent words compute word-context co-occurrence count matrix; context word appears less distance away given word text tunable parameter. word context co-occurrence count. learns function approximates transformed version xwc. different methods differ essentially transformation function parametric form example glove uses dimensional vectors denotes euclidean product approximates xwc. hand levy goldberg show wordvec seen using glove trying approximate pairwise mutual information number negative samples. paper approach word embedding task different perspective formulating ranking problem. given word output ordered list context words words cooccur appear list. rank denotes rank list typical ranking losses optimize following objective word-context pairs co-occur corpus ranking loss function monotonically increasing concave casting word embedding ranking distinctive advantages. first method discriminative rather generative; words instead modeling directly model relative order values row. formulation naturally popular word embedding tasks word similarity/analogy since instead likelihood word interested ﬁnding relevant words given context. second casting word embedding ranking problem enables design models robust noise focusing differentiating relevant words kind attention mechanism proved useful deep learning issues critical domain word embedding since co-occurrence matrix might noisy grammatical errors unconventional language i.e. certain words might cooccur purely chance phenomenon acute smaller document corpora collected diverse sources; it’s challenging sort relevant words large vocabulary thus kind attention mechanism trade resolution relevant words resolution less relevant words needed. show experiments method mitigate issues; million tokens method performs almost well existing methods using billion tokens popular word similarity benchmark. notation denote word denote context. words vocabulary denoted context words denoted denote word-context pairs observed data denote contexts cooccured given word similarly denote words co-occurred given context size denoted |·|. inner product vectors denoted loss function hand consider class monotonically increasing concave functions. monotonicity natural requirement argue concavity also important derivative always non-increasing; implies ranking loss sensitive list becomes less sensitive lower list intuitively desirable interested small number relevant contexts frequently co-occur given word thus willing tolerate errors infrequent contexts. meanwhile insensitivity bottom list makes model robust noise data either grammatical errors unconventional language. therefore single ranking loss function serves different purposes ends curve left hand side curve encourages high resolution relevant words right hand side becomes less sensitive infrequent possibly noisy words. demonstrate experiments fundamental attribute method contributes superior performance compared state-of-the-arts training limited this similar attention mechanism found human visual system able focus certain region image high resolution perceiving surrounding image resolution achieve learning ranking model parametrized sort contexts given word terms context’s inner product score word rank speciﬁc context list written loss function otherwise. since discontinuous function follow popular strategy machine learning replaces loss convex upper bound popular loss function binary classiﬁcation hinge loss logistic loss enables construct following convex upper bound rank rank rank weight word context quantifying association them monotonically increasing concave ranking loss function measures goodness rank hyperparameters model whose role discussed later. following pennington function contains summations expensive compute large corpus. although stochastic gradient descent used replace summation random sampling summation cannot avoided unless linear function. work around problem propose optimize linearized upper bound objective function obtained ﬁrst-order taylor approximation. observe concavity figure visualizing different ranking loss functions deﬁned eqs. lower part truncated order visualize functions better. visualizing ρ/α) different included illustrate dramatic scale differences monotonically non-increasing consequently rank hence loss function gives contexts high ranks order focus attention list. rate algorithm gives determined hyperparameters illustration effect example plots different figure intuitively viewed scale parameter viewed offset parameter. equivalent interpretation choosing different values modify behavior ranking loss problem dependent fashion. experiments found common setting often yields uncompetitive performance setting generally gives good results. contain expensive summations unbiased estimator i.e. hand optimize exactly using putting everything together yields stochastic optimization algorithm wordrank specialized variety ranking loss functions weights many possible instantiations). algorithm contains detailed pseudo-code. seen algorithm divided stages stage updates another updates note time complexity ﬁrst stage since cost update lines independent size corpus. hand time complexity updating line expensive. amortize cost employ tricks update iterations update exploit fact computationally expensive operation involves matrix matrix multiplication calculated efﬁciently sgemm routine blas parallelization updates lines remarkable property update need read variables ξwc. means updates another triplet variables performed independently. observation developing parallel optimization strategy distributing computation updates among multiple processors. lack space details including pseudo-code relegated supplementary material. work sits intersection word embedding ranking optimization. discussed sec. sec. it’s also related attention mechanism widely used deep learning. therefore review related work along three axes. word embedding. already discussed related work word embedding introduction. essentially wordvec glove derive word representations modeling transformation directly wordrank learns word representations robust ranking. besides state-of-the-art techniques ranking-based approaches proposed word embedding recently e.g. however adopt pair-wise binary classiﬁcation approach linear ranking loss example employ hinge loss positive/negative word pairs learn word representations used implicitly evaluate ranking losses. discussed sec. beneﬁt attention mechanism robustness noise since linearity treats ranking errors uniformly; empirically sub-optimal performances often observed experiments. recently extending skip-gram model wordvec incorporates additional pair-wise constraints induced rdparty knowledge bases wordnet learns word representations jointly. contrast wordrank fully ranking-based approach without using additional data source training. robust ranking. second line work relevant wordrank ranking objective score functions ranking inspired latent collaborative retrieval framework weston writing rank indicator functions upper bounding convex loss usunier using corresponds well-known pairwise ranking loss hand observed corresponds popular ranking metrics used search ranking robirank algorithm proposed considered special function derive efﬁcient stochastic optimization procedure. however showed paper general class monotonically increasing concave functions handled efﬁciently. another important difference approach hyperparameters modify behavior critical achieve good empirical results. ding vishwanathan proposed logt context robust binary classiﬁcation concerned ranking formulation general applies variety ranking losses weights rwc. optimizing distributing computation across processors inspired work distributed stochastic gradient matrix factorization attention. attention important advancements deep learning recent years widely used state-of-the-art image recognition machine translation systems recently attention also applied domain word embedding. example intuition contexts created equal wang assign importance weight word type context position learn attention-based continuous bagof-words model. similarly within ranking framework wordrank expresses context importance introducing auxiliary variable gives contexts high ranks order focus attention list. experiments ﬁrst evaluate impact weight ranking loss function test performance using small dataset. pick best performing model compare wordvec glove closely follow framework levy careful training corpus models trained combined corpus billion tokens consists wikipedia dump billion tokens news crawl billion tokens billion word language modeling benchmark almost billion tokens umbc webbase corpus around billion tokens. pre-processing pipeline breaks paragraphs sentences tokenizes lowercases corpus stanford tokenizer. clean dataset removing non-ascii characters punctuation discard sentences shorter tokens longer tokens. obtain dataset billion tokens ﬁrst billion tokens wikipedia. want experiment smaller corpus extract subset contains speciﬁed number tokens. co-occurrence matrix construction glove code construct co-occurrence matrix matrix used train glove wordrank models. constructing must choose size vocabulary context window whether distinguish left context right context. follow ﬁndings design choices glove symmetric window size decreasing weighting function word pairs words apart contribute total count. speciﬁcally corpus small larger corpora larger window size alleviates data sparsity issue small corpus expense adding noise parameter settings used experiments summarized table performance. vector combination originally motivated ensemble method later levy provided different interpretation effect cosine similarity function show adding context vectors effectively adds ﬁrst-order similarity terms second-order similarity function. experiments vector combination boosts performance word analogy task training small dataset large enough vector combination doesn’t help anymore. interestingly word similarity task vector combination detrimental cases sometimes even substantially. therefore always word similarity task word analogy task unless otherwise noted. evaluation word similarity datasets evaluate word similarity partitioned subsets wordsim similarity wordsim relatedness mechanical turk rare words simlex- contain word pairs together human-assigned similarity judgments. word representations evaluated ranking pairs according cosine similarities measuring spearman’s rank correlation coefﬁcient human judgments. word analogies task google analogy dataset contains word analogy questions partitioned semantic syntactic questions. question correctly answered algorithm selects word exactly correct word question synonyms thus counted mistakes. ways answer questions namely using cosadd cosmul details). report scores using cosadd default indicate cosmul gives better performance. this possible since optimize ranking loss absolute scores don’t matter long yield ordered list correctly. thus wordrank’s less comparable generated glove employs point-wise loss. handling questions out-of-vocabulary words papers ﬁlter questions out-of-vocabulary words reporting performance. contrast experiments word question vocabulary corresponding question marked unanswerable score zero. decision made size vocabulary increases model performance still comparable across different experiments. impact sec. argued need adding weight ranking objective also presented framework deal variety ranking loss functions study utility ideas. report results million token dataset table similarity task test analogy task google analogy test set. best scores task underlined. means used uniform weight means comparison also include results using robirank seen table adding weight improves performance cases especially word analogy task. among four functions performs best word similarity task suffers notably analogy task performs best overall. given used code provided authors https// bitbucket.org/d_ijk_stra/robirank. although related robirank attribute superior performance wordrank weight introduction hyperparameters many implementation details. comparison state-of-the-arts section compare performance wordrank wordvec glove using code provided respective authors. fair comparison glove wordrank given input co-occurrence matrix eliminates differences performance window size artifacts parameters used wordvec. moreover embedding dimensions used three methods wordvec train skip-gram negative sampling model since produces state-of-the-art performance widely used community glove default parameters suggested results provided figure seen size corpus increases general three algorithms improve prediction accuracy tasks. expected since larger corpus typically produces better statistics less noise co-occurrence matrix corpus size small wordrank yields best performance signiﬁcant margins among three followed wordvec glove; size corpus increases further word analogy task wordvec glove become competitive wordrank eventually perform neck-to-neck consistent ﬁndings indicating number tokens large even simple algorithms perform well. hand wordrank dominant word similarity task cases since optimizes ranking loss explicitly aligns naturally objective word similarity methods; million tokens method performs almost well existing methods using billion tokens word similarity benchmark. side note similar .-billion-token wikipedia corpus wordvec glove performance scores somewhat better close results reported pennington wordvec glove scores .-billiontoken dataset close reported -billion-token dataset. believe discrepancy primary extra attention paid pre-process wikipedia corpora. scribed sec. moreover breakdown performance models google word analogy dataset semantic syntactic subtasks. results listed table seen wordrank outperforms wordvec glove similarity tasks google analogy subtasks. understand whether wordrank produces syntatically semantically meaningful vector space following experiment best performing model produced using billion tokens compute nearest neighbors word cat. visualize words dimensions using t-sne seen figure ranking-based model indeed capable capturing semantic syntactic regularities english language. evaluate model performance word similarity/analogy tasks best performing models trained .-billion-token corpus predict word similarity datasets deproposed wordrank ranking-based approach learn word representations large scale textual corpora. prominent difference between method state-of-the-art tech dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. proceedings international conference learning representations elia bruni gemma boleda marco baroni khanh tran. distributional semantics technicolor. proceedings annual meeting association computational linguistics pages ronan collobert jason weston. uniﬁed architecture natural language processing deep neural networks mulproceedings internatitask learning. tional conference machine learning pages acm. ding vishwanathan. richard zemel john shawe-taylor john lafferty chris williams alan culota editors advances neural information processing systems evgeniy gabrilovich yossi matias ehud rivlin zach solan gadi wolfman eytan ruppin. placing search context concept revisited. transactions information systems gemulla nijkamp haas sismanis. large-scale matrix factorization distributed stochastic gradient descent. conference knowledge discovery data mining pages felix hill reichart anna korhonen. simlex- evaluating semantic models similarity estimation. proceedings seventeenth conference computational natural language learning. niques wordvec glove wordrank learns word representations robust ranking model wordvec glove typically model transformation co-occurrence count directly. moreover ranking loss function wordrank achieves attention mechanism robustness noise naturally usually lacking ranking-based approaches. attributes signiﬁcantly boost performance wordrank cases training data sparse noisy. multi-node distributed implementation wordrank publicly available general usage.", "year": 2015}