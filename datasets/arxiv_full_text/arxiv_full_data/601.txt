{"title": "Character-Level Language Modeling with Hierarchical Recurrent Neural  Networks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "text": "recurrent neural network based character-level language models extremely useful modeling out-of-vocabulary words nature. however performance generally much worse word-level language models since clms need consider longer history tokens properly predict next one. address problem proposing hierarchical architectures consist multiple modules different timescales. despite multi-timescale structures input output layers operate character-level clock allows existing training approaches directly applicable without modiﬁcations. models show better perplexity kneser-ney -gram wlms billion word benchmark parameters. also present real-time character-level end-to-end speech recognition examples wall street journal corpus replacing traditional mono-clock clms proposed models results better recognition accuracies even though number parameters reduced language models show probability distribution sequences words characters important many speech document processing applications including speech recognition text generation machine translation classiﬁed character- word- context-levels according unit input output. character-level probability distribution next characters generated based past character sequences. since number alphabets small english example input output quite simple. however word-level usually needed character-level modeling disadvantaged utilizing long period past sequences. however problem word-level model complexity input output vocabulary size supported bigger million. memory. statistical language model demands large memory space often exceeding vocabulary size large also combinations needs considered. recent years language modeling based recurrent neural networks actively investigated however based wlms still demand billions parameters large vocabulary size. work propose hierarchical based combine advantageous characteristics characterwordlevel lms. proposed network consists low-level highlevel rnns. low-level employs character-level input output provides short-term embedding high-level operates word-level rnn. high-level need complex input output receives characterembedding information low-level network sends word-prediction information back low-level compressed form. thus considering input output proposed network although contains word-level model inside. low-level module operates character input clock high-level runs space sentence boundary tokens separates words. expect hierarchical extended processing longer period information sentences topics contexts. clms need consider longer sequence history tokens predict next token wlms smaller unit tokens. therefore traditional n-gram models cannot employed clms. thanks recent advances rnns rnn-based clms begun show satisfactory performances especially deep long short-term memory based clms show excellent performance successfully applied end-to-end speech recognition system training clms training data ﬁrst converted sequence one-hot encoded character vectors characters include word boundary symbols space optionally sentence boundary symbols <s>. then shown figure trained predict next character minimizing cross-entropy loss softmax output represents probability distributions next character. many attempts make wlms understand characterlevel inputs. successful approaches encode arbitrary character sequence ﬁxed dimensional vector called word embedding feed vector word-level lms. convolutional neural networks used generate word embeddings achieved state results english penn treebank corpus similar cnn-based embedding approach used large lstm networks billion word benchmark also achieving state perplexity. bidirectional lstms employed instead cnns word embedding. however approaches still generate output probabilities wordlevel. although character-level modeling approach output word probability introduced using softmax base lstm network still runs word-level clock. approach different ones many ways. first base model character-level instead wlms extend model enhance model consider long-term contexts. therefore output probabilities generated character-level clocks. property extremely useful character-level beam search end-to-end speech recognition also input output model traditional character-level rnns thus training algorithm recipe used without modiﬁcations. furthermore proposed models signiﬁcantly less number parameters compared wlm-based ones since size model directly depend vocabulary size training set. note similar hierarchical concept used character-level machine translation however propose general hierarchical unidirectional architecture applied various applications. input forget output gate values respectively memory cell state output activation layer logistic sigmoid function elementwise multiplication operator. equations generalized setting original equations differentiable extended equations clock reset signals also differentiable. therefore existing gradient-based training algorithms rnns backpropagation time employed training extended versions without modiﬁcations. proposed hierarchical architectures several modules different clock rates depicted figure higher level module employs slower clock rate lower module lower level module reset every clock higher level module. speciﬁcally hierarchy levels consists submodules. submodule operates external clock reset signal lowest level module fastest clock rate hand higher level modules slower clock rates cl−t also lower level modules reset higher level clock signals cl+t. hidden activations module next higher level module delayed time step avoid unwanted reset cl+t hidden activation vector embedding vector contains compressed short-term context information. reset module higher level clock signals helps module concentrate compressing short term information rather considering longer dependencies. next higher level module process short-term information generate long-term context vector back lower level module delay context propagation. character-level language modeling two-level hrnn letting character-level module word-level module. word-level module clocked word boundary input usually whitespace character. input softmax output layer connected character-level module current word boundary token information given word-level module. since hrnns scalable architecture expect hrnn extended modeling sentence-level contexts adding additional sentence-level module case sentence-level clock becomes input character sentence boundary token <s>. also word-level module clocked word boundary input sentence boundary input <s>. paper experiments performed two-level hrnn clms. propose types two-level hrnn architectures. shown figure models lstm layers submodule. note connection weight matrix. hlstm-a architecture lstm layers character-level module receives one-hot encoded character input. therefore second layer character-level module generative model conditioned context vector. hand hlstmb second lstm layer character-level module direct connection character inputs. instead word embedding ﬁrst lstm layer second lstm layer makes ﬁrst second layers character-level module work together estimate next character probabilities context vector given. experimental results show hlstm-b efﬁcient applications. since character-level modules reset word-boundary token context vector wordlevel module source inter-word context information. therefore model trained generate context vector contains useful information probability distribution next word. perspective word-level module hrnn architectures considered word-level input word embedding vector output compressed descriptor next word probabilities. although proposed model consists several modules different timescales jointly trained bptt described section proposed hrnn based clms evaluated text datasets wall street journal corpus billion word benchmark also present end-to-end speech recognition example hlstm clms employed preﬁx tree-based beam search decoding. rnns trained truncated backpropagation time also adadelta nesterov momentum applied weight update. regularization method dropout employed. training accelerated using gpus training multiple sequences parallel section models compared wlms literature terms word-level perplexity word-level models directly converted bits-per-character standard performance measure clms follows wall street journal corpus designed training benchmarking automatic speech recognition systems. perplexity experiments used non-verbalized punctuation version training data inside corpus. dataset consists million words percent total data held ﬁnal evaluation participate training. alphabets converted uppercases. table shows perplexities traditional mono-clock deep lstm hlstm based clms held-out set. note size means network consists lstm layers layer contains memory cells. hlstm models show better perplexity performanes even number lstm cells parameters much smaller deep lstm networks. especially hlstm-b network size lower perplexity deep lstm model even parameters. important reset character-level modules wordlevel clocks helping character-level modules better concentrate short-term information. observed table removing reset functionality character-level module hlstm-b model results degraded performance. non-ensemble perplexities wlms literature presented table kneser-ney smoothed -gram model strong non-neural baseline. standard deep based clms hard beat terms perplexity. however surprising hlstm models table shows better perplexities does. based model combined maximum entropy -gram feature shows much better results proposed hlstm based models. however like wlms also needs large number parameters cannot handle out-of-vocabulary words. billion word benchmark dataset contains billion words roughly thousand words vocabulary. followed standard splitting training test data byte utf- encoded text regarded character. therefore size character large amount training data weeks training time hlstm-b experiments conducted size shown table large word-level perplexity models. therefore improvement perplexity expected bigger networks. though number parameters model model. also model performs better lightrnn word-level parameters ours. however much lower perplexities reported sparse non-negative matrix maximum entropy feature based model number parameters respectively. recently state perplexity reported single model parameters. model basically large lstm however convolutional neural network used generate word embedding arbitrary character sequences input lstm therefore model handle word inputs however still model runs word-level clock. section apply proposed clms end-to-end automatic speech recognition system evaluate models practical situation measuring perplexities. clms trained training data section unlike wlms proposed clms small number parameters employed real-time character-level beam search. incremental speech recognition system proposed used evaluation. acoustic model unidirectional lstm end-to-end trained connectionist temporal classiﬁcation loss using non-verbalized punctuation portion training set. acoustic features dimensional log-mel ﬁlterbank coefﬁcients energy delta double-delta values extracted every hamming window. beam-search decoding performed preﬁx-tree depth-pruning width-pruning insertion bonus weight beam width results summarized table observed perplexity word error rate strong correlation. shown table achieve better replacing traditional deep lstm proposed hlstm-b reducing number parameters paper hierarchical based clms proposed. hrnn consists several submodules different clock rates. therefore capable learning long-term dependencies well short-term details. experimental results billion benchmark show hlstm-b networks signiﬁcantly outperform kneser-ney -gram parameters. although rnn-based wlms show better performance models impractically many parameters. hand shown speech recognition example proposed model employed real-time speech recognition less million parameters. also clms handle words nature great advantage end-to-end speech recognition many tasks. interesting future work train clock signals instead using manually designed ones. peter brown john cocke stephen della pietra vincent della pietra fredrick jelinek john lafferty robert mercer paul roossin statistical approach machine translation computational linguistics vol. wang ling isabel trancoso chris dyer alan black character-based neural machine translation proceedings annual meeting association computational linguistics vol. douglas paul janet baker design wall street journal-based corpus proceedings workshop speech natural language. association computational linguistics kyuyeon hwang wonyong sung character-level incremental speech recognition recurrent neural networks ieee international conference acoustics speech signal processing john bridle probabilistic interpretation feedforward classiﬁcation network outputs relationships statistical pattern recognition neurocomputing springer ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson billion word benchmark measuring progress statistical language modeling arxiv preprint arxiv. wang ling tiago lu´ıs lu´ıs marujo ram´on fernandez astudillo silvio amir chris dyer alan black isabel trancoso finding function form compositional character models open vocabulary word representation conference empirical methods natural language processing geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. kyuyeon hwang wonyong sung single stream parallelization generalized lstm-like rnns ieee international conference acoustics speech signal processing ieee tom´aˇs mikolov statistical language models based neural networks ph.d. thesis brno university technology reinhard kneser hermann improved backing-off m-gram language modeling international conference acoustics speech signal processing ieee vol. shihao vishwanathan nadathur satish michael anderson pradeep dubey blackout speeding recurrent neural network language models large vocabularies international conference learning representations alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning.", "year": 2016}