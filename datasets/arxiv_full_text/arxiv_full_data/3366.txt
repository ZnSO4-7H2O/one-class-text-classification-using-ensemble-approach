{"title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard \"deep learning\" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.", "text": "paper introduce image representation based multilayer kernel machine. unlike traditional kernel methods data representation decoupled prediction task learn shape kernel supervision. proceed ﬁrst proposing improvements recently-introduced convolutional kernel networks context unsupervised learning; then derive backpropagation rules take advantage labeled training data. resulting model type convolutional neural network optimizing ﬁlters layer equivalent learning linear subspace reproducing kernel hilbert space show method achieves reasonably competitive performance image classiﬁcation standard deep learning datasets cifar- svhn also image super-resolution demonstrating applicability approach large variety image-related tasks. past years deep neural networks convolutional recurrent ones become highly popular solving various prediction problems notably computer vision natural language processing. conceptually close approaches developed several decades greatly beneﬁt large amounts labeled data made available recently allowing learn huge numbers model parameters without worrying much overﬁtting. among reasons explaining success engineering effort deep learning community various methodological improvements made possible learn complex models would required weeks computations traditional resurgence neural networks non-parametric models based positive deﬁnite kernels dominant topics machine learning approaches still widely used today several attractive features. kernel methods indeed versatile; long positive deﬁnite kernel speciﬁed type data considered—e.g. vectors sequences graphs sets—a large class machine learning algorithms originally deﬁned linear models used. family include supervised formulations support vector machines unsupervised ones principal canonical component analysis k-means spectral clustering. problem data representation thus decoupled learning theory algorithms. kernel methods also admit natural mechanisms control learning capacity reduce overﬁtting hand traditional kernel methods suffer several drawbacks. ﬁrst computational complexity grows quadratically sample size computation gram matrix. fortunately signiﬁcant progress achieved solve scalability issue either exploiting low-rank approximations kernel matrix random sampling techniques shift-invariant kernels second disadvantage critical; decoupling learning data representation kernel methods seem nature incompatible end-to-end learning—that representation data adapted task hand cornerstone deep neural networks main reason success. main objective paper precisely tackle issue context image modeling. speciﬁcally approach based convolutional kernel networks recently introduced similar hierarchical kernel descriptors local image neighborhoods mapped points reproducing kernel hilbert space kernel trick. then hierarchical representations built kernel compositions producing sequence feature maps akin convolutional neural networks inﬁnite dimension. make image model computationally tractable convolutional kernel networks provide approximation scheme interpreted particular type convolutional neural network learned without supervision. perform end-to-end learning given labeled data simple effective principle consisting learning discriminative subspaces rkhss project data. implement idea context convolutional kernel networks linear subspaces layer jointly optimized minimizing supervised loss function. formulation turns type convolutional neural network non-standard parametrization. network also admits simple principles learn without supervision learning subspaces indeed achieved efﬁciently classical kernel approximation techniques demonstrate effectiveness approach various contexts consider image classiﬁcation benchmarks cifar- svhn often used evaluate deep neural networks; then adapt model perform image super-resolution challenging inverse problem. svhn cifar- datasets obtain competitive accuracy error rates respectively without model averaging data augmentation. image up-scaling outperform recent approaches based classical convolutional neural networks believe results highly promising. image model achieves competitive performance different contexts paving many applications. moreover results also subject improvements. particular gpus limited ability exhaustively explore model hyper-parameters evaluate accuracy large networks. also investigate classical regularization/optimization techniques dropout batch normalization recent advances allowing train deep networks gain scalability start exploring directions currently working implementation plan publicly release along current implementation. related deep shallow kernel machines. goals make bridge kernel methods deep networks ideally reach best worlds. given potentially attractive features combination several attempts made past unify schools thought. ﬁrst proof concept introduced arc-cosine kernel admits integral representation interpreted one-layer neural network random weights inﬁnite number rectiﬁed linear units. besides multilayer kernel obtained kernel compositions then hierarchical kernel descriptors convolutional kernel networks extend similar idea context images leading unsupervised representations multiple kernel learning also related work since notable attempt introduce supervision kernel design. provides techniques select combination kernels predeﬁned collection typically requires already good kernels collection perform well. related work backpropagation algorithm fisher kernel introduced learns parameters gaussian mixture model supervision. comparison approach require probabilistic model learns parameters several layers. finally note concurrent effort conducted bayesian community deep gaussian processes complementing frequentist approach follow paper. section present principles convolutional kernel networks generalizations improvements original approach essentially model builds upon four ideas detailed illustrated figure model single layer. given positive deﬁnite kernel implicitly deﬁnes hilbert space called reproducing kernel hilbert space along mapping embedding kernel value corresponds inner product ϕ)h. called kernel trick approach used obtain nonlinear representations local image patches precisely consider image number channels e.g. pixel coordinates typically two-dimensional grid. given image patches size represented vectors denote usual euclidean norm inner-product respectively dot-product kernel sphere. speciﬁcally smooth taylor expansion non-negative coefﬁcients ensure positive deﬁniteness example arc-cosine gaussian kernels used given vectors unit -norm choose instance idea project onto ﬁnite-dimensional subspace rkhs convolution layers. representation patches rkhs requires ﬁnite-dimensional approximations computationally manageable. original model exploiting integral form kernel. speciﬁcally given patches convolutional kernel networks provide vectors kernel value close euclidean inner product applying transformation overlapping patches input image spatial obtained patch centered pixel location approximation scheme interpreted output feature one-layer convolutional neural network. conceptual drawback data points approximated vectors live rkhs issue solved using variants nyström method consists projecting data onto subspace ﬁnite dimension task adapted approach build database patches randomly extracted various images normalized unit -norm perform spherical k-means algorithm obtain centroids unit -norm. then patch approximated projection onto p-dimensional subspace =span projection onto admits natural parametrization explicit formula applied pointwise arguments. then spatial introduced obtained computing quantities patches image contrast-normalization involving norm applying pointwise non-linear function applying linear transform κz)−/ every pixel location multiplying norm making homogeneous. words obtain particular convolutional neural network non-standard parametrization. note learning requires performing k-means algorithm computing inverse square-root matrix κz)−/; therefore training procedure fast. then worth noting encoding function kernel reminiscent radial basis function networks whose hidden layer resembles without matrix κz)−/ normalization. difference rbfns model nevertheless signiﬁcant. rkhs mapping absent rbfns indeed multilayer construction presented shortly network layer takes points rkhs’s previous layer input corresponding rkhs inner-product. best knowledge similar multilayer and/or convolutional construction radial basis function network literature. figure variant convolutional kernel networks illustrated layers local patches mapped rkhs kernel trick projected ﬁnite-dimensional subspace =span small blue crosses right represent points supervision optimizing consists minimizing projection residuals. supervision subspace optimized back-propagation. going layer layer achieved stacking model described shifting indices. idea linear pooling equivalent linear pooling ﬁnite-dimensional previous steps transform image vector encodes point representing information local image neighborhood small shifts leading another ﬁnite-dimensional smaller resolution point interpreted linear combination points since linear subspace. note linear pooling step originally motivated approximation scheme match kernel point view critically important here. following ﬁrst three principles described above input image transformed another straightforward apply procedure obtain another etc. going hierarchy vectors represent larger larger image neighborhoods invariance gained pooling layers akin classical convolutional neural networks. multilayer scheme produces sequence maps vector encodes point—say fk—in linear subspace thus implicitly represent image layer spatial fk)hk mentioned previously mapping rkhs multilayer construction. given larger image neighborhoods represented patches size mapped point cartesian product space hek×ek endowed natural inner-product; ﬁnally kernel deﬁned patches seen kernel larger image neighborhoods previous section described variant convolutional kernel networks linear subspaces learned every layer. achieved without supervision k-means algorithm leading small projection residuals. thus natural introduce also discriminative approach. smooth convex loss function measures prediction true label given positive deﬁnite kernel images classical empirical risk minimization formulation consists ﬁnding prediction function rkhs associated minimizing objective parameter controls smoothness prediction function respect geometry induced kernel hence regularizing reducing overﬁtting training convolutional kernel network layers positive deﬁnite kernel deﬁned k-th ﬁnite-dimensional feature maps respectively corresponding maps deﬁned previous section. kernel also indexed represents network parameters—that subspaces equivalently ﬁlters then formulation becomes equivalent frobenius norm extends euclidean norm matrices abuse seen matrices rpk×|ωk|. then supervised convolutional kernel notation maps network formulation consists jointly minimizing respect rpk×|ωk| respect ﬁlters whose columns constrained euclidean sphere. computing derivative respect ﬁlters since consider smooth loss function e.g. logistic squared hinge square loss optimizing respect achieved gradient-based method. moreover convex also fast dedicated solvers references therein). optimizing respect ﬁlters involved lack convexity. objective function differentiable hope good stationary point using classical stochastic optimization techniques successful training deep networks. that need compute gradient using chain rule—also called backpropagation instantiate rule next lemma found useful simplify calculation. lemma consider image represented matrix rp×|ω| associated label call k-th feature obtained encoding network parameters then consider perturbation ﬁlters assume proposition consider quantities introduced lemma denote simplicity. construction seen matrix rpj×|ωj|; linear operator extracts overlapping patches matrix size pj−e |ωj−|; diagonal matrix whose diagonal entries carry -norm columns short zj)−/; matrix size |ωj−|×|ωj| performing linear pooling operation. then gradient loss respect ﬁlters given proof presented appendix quantities appear admit physical interpretations multiplication performs downsampling; multiplication performs upsampling; multipliperforms -normalization columns; cation right seen spatial convolution ﬁlters ﬁnally combines patches spatial adding pixel location respective patch contributions. computing gradient requires forward pass obtain maps backward pass composes functions complexity forward step dominated convolutions convolutional neural networks. cost backward pass forward constant factor. assuming ≤|ωj−| typical lower layers require computation upper ones expensive cost ejbj zjbj also pre-compute eigenvalue decompositions whose cost reasonable performed minibatch. off-diagonal elements ejzjbj also computed since zero elementwise multiplication diagonal matrix. practice also replace εi)−/ corresponds performing regularized projection onto finally small offset added diagonal entries optimizing hyper-parameters kernels. using kernel objective differentiable respect hyper-parameters large amounts training data available overﬁtting issue optimizing training loss taking gradient steps respect parameters seems appropriate instead using canonical parameter value. otherwise involved techniques needed; plan investigate strategies future work. hybrid convex/non-convex optimization. recently many incremental optimization techniques proposed solving convex optimization problems form large ﬁnite references therein). methods usually provide great speed-up stochastic gradient descent algorithm without suffering burden choosing learning rate. price rely convexity require storing memory full training set. solving ﬁxed network parameters means storing maps often reasonable data augmentation. partially leverage fast algorithms non-convex problem adopted minimization scheme alternates steps make forward pass data compute maps minimize convex problem respect using accelerated miso algorithm make pass projected stochastic gradient algorithm update ﬁlters network parameters initialized unsupervised learning method described section preconditioning sphere. kernels deﬁned sphere; therefore natural constrain ﬁlters—that columns matrices zj—to unit -norm. result classical stochastic gradient descent algorithm updates iteration ﬁlter follows proj ∇zlt estimate gradient computed minibatch learning rate. practice found convergence could accelerated preconditioning consists optimizing change variable reduce correlation gradient entries. unconstrained optimization heuristic involves choosing symmetric positive deﬁnite matrix replacing update direction ∇zlt q∇zlt equivalently performing change variable optimizing constraints present case simple since q∇zlt descent direction. fortunately possible exploit manifold structure constraint perform appropriate update concretely choose matrix layer equal inverse covariance matrix patches layer computed initialization network parameters. perform stochastic gradient descent steps sphere manifold change variable leading .=qz)qzz)q∇zlt]. heuristic critical update proj component simply improvement relegate mathematical details appendix automatic learning rate tuning. choosing right learning rate stochastic optimization still important issue despite large amount work existing topic e.g. references therein. paper following basic heuristic initial learning rate chosen large enough; then training loss evaluated update weights training loss increases epochs simply divide learning rate perform back-tracking replacing current network parameters previous ones. active-set heuristic. classiﬁcation tasks easy samples often negligible contribution gradient instance squared hinge loss gradient vanishes margin greater one. motivates following heuristic consider active samples initially them remove sample active soon obtain zero computing gradient. subsequent optimization steps active samples considered epoch randomly reactivate inactive ones. consider datasets cifar- svhn contain images classes. cifar- medium-sized training samples test ones. svhn larger training examples test ones. evaluate performance -layer network designed hyper-parameters layer learn ﬁlters choose kernels deﬁned initial parameters layers patches subsampling pooling factor except layer factor layers simply patches subsampling. cifar- parameters kept ﬁxed training svhn updated ﬁlters. squared hinge loss setting perform multi-class classiﬁcation input network pre-processed local whitening procedure described optimization heuristics previous section notably automatic learning rate scheme gradient momentum parameter following regularization parameter number epochs ﬁrst running algorithm validation split training set. chosen near canonical parameter range number epochs initial learning rate minibatch size present results table along performance achieved recent methods without data augmentation model voting/averaging. context best published results obtained generalized pooling scheme achieve test error svhn cifar- positions method reasonably competitive ballpark deeply supervised nets network network lack space results reported include single supervised model. preliminary experiments supervision show also obtain competitive accuracy wide shallow architectures. instance two-layer network ﬁlters achieves error cifar-. note also unsupervised model outperforms original ckns best single model gives indeed training architecture approach orders magnitude faster gives another aspect study model complexity. well preliminary experiments encouraging. reducing number ﬁlters layer yields indeed error cifar- svhn. precise comparison supervision various network complexities presented another venue. image up-scaling challenging problem convolutional neural networks obtained signiﬁcant success here follow replace traditional convolutional neural networks supervised kernel machine. speciﬁcally images converted ycbcr color space upscaling method applied luminance channel make comparison possible previous work. then problem formulated multivariate regression one. build database patches size randomly extracted dataset removing image .jpg overlaps test images. versions patches build using matlab function imresize upscaled back using bicubic interpolation; then goal predict high-resolution images blurry bicubic interpolations. blurry estimates processed -layer network patches ﬁlters every layer without linear pooling zero-padding. pixel values predicted linear model applied -dimensional vectors present every pixel location last layer square loss measure optimization procedure kernels identical ones used processing svhn dataset classiﬁcation task. pipeline also includes pre-processing step remove input images local mean component obtained convolving images averaging ﬁlter; mean component added back up-scaling. evaluation consider three datasets standard super-resolution; kodim kodak image database available http//rk.us/graphics/kodak/ contains high-quality images compression demoisaicing artefacts. evaluation procedure follows using code author’s page. present quantitative results table upscaling simply used twice model learned upscaling followed downsampling. clearly suboptimal since model trained up-scale factor naive approach still outperforms baselines trained end-to-end. note also proposes data augmentation scheme test time slightly improves results. appendix also present visual comparison approach whose pipeline closest ours supervised kernel machine instead cnns. fox. object recognition hierarchical kernel descriptors. cvpr broomhead lowe. radial basis functions multi-variable functional interpolation adaptive saul. kernel methods deep learning. adv. nips damianou lawrence. deep gaussian processes. proc. aistats dong tang. learning deep convolutional network image super-resolution. zhang sun. deep residual learning image recognition. proc. cvpr ioffe szegedy. batch normalization accelerating deep network training reducing internal c.-y. gallagher zhang deeply-supervised nets. proc. aistats mairal harchaoui. universal catalyst ﬁrst-order optimization. adv. nips chen yan. network network. proc. iclr mairal koniusz harchaoui schmid. convolutional kernel networks. adv. nips netzer wang coates bissacco reading digits natural images rahimi recht. random features large-scale kernel machines. adv. nips schölkopf smola. learning kernels support vector machines regularization optimization williams seeger. using nyström method speed kernel machines. adv. nips zeiler fergus. stochastic pooling regularization deep convolutional neural networks. thus assume unit -norm without loss generality perform projection span normalized patch applying inverse rescaling. then denote orthogonal projection patch unit -norm deﬁned since vectors provided spherical k-means algorithm unit -norm. assuming invertible κz)−κx). projection normalized patches parametrized κz)−κx) κz)−κx) respectively. then small regularization improves condition number κz). modiﬁcation interpreted performing slightly regularized projection onto ﬁnitedimensional subspace computation gradient respect filters compute gradient loss function lemma start analyzing effect perturbing every quantity involved obtain desired relations proceeding recall deﬁnition precise deﬁnition landau notation here simply means then easy obtain form given using right order following elementary calculus rules matrices appropriate sizes also preconditioning heuristic sphere section present preconditioning heuristic optimizing sphere inspired second-order optimization techniques smooth manifolds following consider gradient descent steps manifold. fundamental operation thus projection operator onto tangent space point operator deﬁned sphere gradient descent sphere equivalent projected gradient descent optimizing manifold natural descent direction projected gradient pz∇l. case sphere gradient step manifold equivalent classical projected gradient descent step particular step size constraint pre-conditioning equivalent performing change variable. unconstrained optimization faster convergence usually achieved access estimate inverse hessian ∇l—assuming twice differentiability—and using descent direction )−∇l instead then obtain newton method. exact hessian available costly compute and/or invert however common instead constant estimate inverse hessian denoted call pre-conditioning matrix. finding appropriate matrix difﬁcult general learning linear models typical choice inverse covariance matrix data case preconditioned gradient descent step consists update ηq∇l. matrix deﬁned similarly context convolutional kernel networks explained main part paper. useful interpretation preconditioning optimizing change variable. deﬁne indeed objective then minimizing equivalent minimizing respect relation q/w. moreover constraint regular gradient descent algorithm equivalent preconditioned gradient descent remark hessian equal q/∇lq/ equal identity coincides inverse hessian general course case hope obtain hessian better conditioned thus resulting faster convergence. preconditioning smooth manifold requires care. unfortunately using second-order information optimizing constraint smooth manifold simple optimizing since quantities qpz] feasible descent directions. however point view sees pre-conditioning change variable give right direction follow. optimizing fact equivalent optimizing smooth manifold present quantitative comparison table using structural similarity index measure known better reﬂect quality perceived humans psnr; commonly used evaluate quality super-resolution methods then present visual comparison several approaches figures focus notably classical convolutional neural network since pipeline essentially differs supervised kernel machine instead convolutional neural networks. subjective evaluation observe methods perform equally well textured areas. however approach recovers better thin high-frequency details eyelash baby ﬁrst image. zooming various parts easy notice similar differences images. also observed ghosting artefacts near object boundaries method case approach. figure visual comparison image up-scaling. column corresponds different method images converted ycbcr color space up-scaling method applied luminance channel only. color channels up-scaled using bicubic interpolation visualization purposes. sckn perform similarly textured areas sckn provides signiﬁcantly sharper artefact-free edges best seen zooming computer screen appropriate viewer smooth image content.", "year": 2016}