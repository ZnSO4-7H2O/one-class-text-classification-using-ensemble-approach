{"title": "Language-Based Image Editing with Recurrent Attentive Models", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We investigate the problem of Language-Based Image Editing (LBIE) in this work. Given a source image and a natural language description, we want to generate a target image by editing the source im- age based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each re- gion of the image a termination gate to dynamically determine in each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework has been validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the- art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset, laying the foundation for future research.", "text": "investigate problem language-based image editing work. given source image natural language description want generate target image editing source image based description. propose generic modeling framework sub-tasks lbie language-based image segmentation image colorization. framework uses recurrent attentive models fuse image language features. instead using ﬁxed step size introduce region image termination gate dynamically determine inference step whether continue extrapolating additional information textual description. effectiveness framework validated three datasets. first introduce synthetic dataset called cosal evaluate end-to-end performance lbie system. second show framework leads state-of-theart performance image segmentation referit dataset. third present ﬁrst language-based colorization result oxford- flowers dataset laying foundation future research. work develop automatic language-based image editing system. given source image sketch grayscale image natural image system automatically generate target image editing source image following natural language instructions provided users. system wide range applications computer-aided design virtual reality illustrated figure envision fashion designer presents sketch pair shoes customer provide modiﬁcations style color verbal description taken lbie system change original design. ﬁnal output revised enriched design would meet customers requirement. figure showcases lbie systems still using button-controlled touchscreen interface lbie provides natural user interface future systems users easily modify base environment natural language instructions. lbie cover broad range tasks image generation shape color size texture position etc. paper focuses basic sub-tasks lbie language-based segmentation colorization shapes colors. example figure given grayscale image expression ﬂower petals yellow stigmas middle segmentation model identify region image petals stigmas colorization model paint pixel suggested color. intertwined task segmentation colorization distribution target images multi-modal sense pixel deﬁnitive ground truth segmentation necessarily color. example pixels petals figure based textual description speciﬁc numeric values color space uniquely speciﬁed. system required colorize petals based real-world knowledge. another uncertainty lies fact input description figure interactive design interface sketch shoes presented customer gives verbal instruction modify design insole shoes brown. vamp heel purple shining. system colorize sketch following customer’s instruction. figure image left initial virtual environment. user provides textual description afternoon light ﬂooded little room window shining ground front brown bookshelf made wood. besides bookshelf lies sofa light-colored cushions. blue carpet front sofa clock dark contours it.... system modiﬁes virtual environment right image. might cover every detail image. undescribed regions leaves given example rendered based past memory. summary ultimate goal generate images guided constraint natural language expressions also align common sense real world. language-based image segmentation studied previously however task challenging textual description task often contains multiple sentences expressions simple phrases. best knowledge language-based colorization studied systematically before. previous work images generated either solely natural language expressions based another image instead want generate target image based natural language expressions source image. related tasks discussed detail section unique challenge language-based image editing complexity natural language expressions correlation source images. like example shown figure description usually consists multiple sentences. sentence might refer multiple objects source image. many details also need inferred multiple sentences. human edits source image based textual description often keep mind sentences related region/object image back sentences multiple times editing region. behavior going back often varies region region depending complexity description region. investigation problem carried cosal dataset introduced purpose detailed section figure left sketch image. middle grayscale image. right color image language-based image editing system take either ﬁrst images input generate third color image following natural language expression ﬂower petals yellow stigmas middle. goal design generic framework sub-tasks language-based image editing. diagram model shown figure inspired observation aforementioned introduce recurrent attentive fusion module framework. fusion module takes image features encoding source image convolutional neural network textual features encoding natural language expression lstm outputs fused features upsampled deconvolutional network target images. fusion module recurrent attentive models employed extract distinct textual features based spatial features different parts image. termination gate introduced region control number steps interacts textual features. gumbel-softmax reparametrization trick used end-to-end training entire network. details models training process described section contributions summarized follows deﬁne task language-based image editing present generic modeling framework based recurrent attentive models sub-tasks introduce synthetic dataset cosal evaluate end-to-end performance lbie system. achieve state-of-the-art performance language-based image segmentation referit task language-based image editing studied community taken signiﬁcant steps several related areas including language based object detection segmentation image-to-image translation generating images text image captioning visual question answering machine reading comprehension etc. summarize types inputs outputs related tasks table figure high-level diagram model composed convolutional image encoder lstm text encoder fusion module deconvolutional upsampling layer optional convolutional discriminator. recurrent attentive models applied visual question answering fuse language image features stacked attention network proposed identiﬁes image regions relevant question multiple attention layers progressively ﬁlter noises pinpoint regions relevant answer. image generation sequential variational auto-encoder framework draw shown substantial improvement standard variational auto-encoders similar ideas also explored machine reading comprehension models take multiple iterations infer answer based given query document novel neural network architecture called reasonet proposed reading comprehension. reasonet performs multi-step inference number steps determined termination gate according difﬁculty problem. reasonet trained using policy gradient methods. task language-based image segmentation ﬁrst proposed given image natural language description system identify regions image correspond visual entities described text. authors proposed end-to-end approach composed three main components convolutional network encode source images lstm network encode natural language descriptions fully convolutional classiﬁcation upsampling network pixel-wise segmentation. differences approach integrating image text features. region image extracted spatial features concatenated textual features. inspired alignment model approach spatial feature aligned different textual features based attention models. approach yields superior segmentation results benchmark dataset. generative adversarial networks widely used powerful tool image generation. conditional gans often employed constraints generated image needs satisfy. example deep convolutional conditional gans used synthesize images based textual descriptions proposed conditional gans image-to-image translation. different tasks lbie takes image text input presenting additional challenge fusing features source image textual description. overview proposed modeling framework shown based neural networks generic language-based image segmentation colorization tasks. framework composed convolutional image encoder lstm text encoder fusion network generates fusion feature integrating image text features deconvolutional network generates pixel-wise outputs upsampling fusion feature optional convolutional discriminator used training colorization models. image encoder image encoder multi-layer convolutional neural network given source image size encoder produces spatial feature position feature containing d-dimensional feature vector language encoder language encoder recurrent long short-term memory network. given natural language expression length ﬁrst embed word vector word embedding matrix lstm produce word contextual vector encodes contextual information word order word-word dependencies. resulting language feature recurrent attentive fusion module fusion network fuses text information image feature outputs fusion feature position containing editing feature vector fusion network devised mimic human image editing process. region source image fusion network reads language feature repeatedly attention different parts time enough editing information collected generate target image region. number steps varies region region. spatial feature poition containing vector representation editing information state. initial state spatial feature source image sequence internal states modeled convolutional gated recurrent units described below. termination gates termination gates image region termination gate generates binary random variable according current internal state image fusion process image region stops editing region feature vector image region terminate gates true fusion process entire image completed fustion network outputs fusion feature deﬁne inference algorithm describes stochastic inference process fusion network. state sequence hidden dynamic chained attention c-gru recurrent fashion. fusion network outputs image region editing feature vector ti-th step controlled termination gate varies region region. algorithm stochastic inference fusion network require spatial feature image. require rk×l language feature expression. ensure fusion feature rd×. image decoder image decoder multi-layer deconvolutional network. takes input fusion feature produced fusion module unsamples produce editing size target image number classes segmentation colorization. neural network extract features image. model discriminator serves function judging whether image looks natural whether compatible language descriptions. latter constraint left loss described later. loss training denote loss eζζζ expectation taken categorical variables generated termination gates loss output ζζζ. sample space exponential size intractable entire sample space. denote density naive approach approximation subsample loss update parameters gradient monte carlo estimate loss subset sampled distributon experiments found monte carlo estimate suffers high variance. resolve issue employ gumbel-softmax reparameterization trick replaces every sampled another random variable generated gumbel-softmax distribution segmentation segmentation assume unique answer pixel whether referred stage segmentation. response size produces probability class pixel. pixel-wise softmax cross-entropy loss training colorization colorization high-level goal generate realistic images constraint natural language expressions input scene representations introduce mixture loss loss optimization discriminator parametrized introduced constructing loss. response predicted color channels. combined grayscale source image produce generated color image generator loss loss taking input loss channels target image response conducted three experiments validate performance proposed framework. dataset cosal introduced test capability understanding multi-sentence descriptions associating inferred textual features visual features. framework also yielded state-of-the-art performance benchmark dataset referit image segmentation. third experiment carried oxford- flowers dataset language-based colorization task. dataset synthetic dataset cosal colorizing shapes artiﬁcial language introduced studying correlation images multi-sentence descriptions. image dataset consists nine shapes paired textual description image. goal task deﬁned given blackwhite image corresponding description train model automatically colorize nine shapes following textual description. figure shows example task requires sophisticated coreference resolution multi-step inference logical reasoning. dataset created follows ﬁrst divide white-background image regions. region contains shape randomly sampled shapes shape ﬁlled color choices chosen random. position size shape generated uniform random variables. illustrated figure difﬁculty task increases number color choices. experiments specify start point. descriptive sentences image divided categories direct descriptions relational descriptions. former prescribes color certain shape latter depicts shape conditional another understand direct descriptions model needs associate speciﬁed shape textual features. relational description adds another degree difﬁculty calls advanced inference capability relational/multi-step reasoning. ratio direct descriptions relational descriptions varies among different images colors shapes image uniquely determined description. experiment randomly generated images corresponding descriptions training purpose images descriptions testing. metric task average nine shapes background evaluation metric. speciﬁcally region compute intersection-over-union ratio total intersection area total union area predicted colors ground truth colors. also compute background image. classes computed entire test averaged. model implementation six-layer convolutional network implemented image feature extractor. layer kernel stride output dimension relu used nonlinearity layer max-pooling layer kernel size inserted every layers. sentence textual description encoded bidirectional lstms share parameters. lstms units. fusion network attention model units cells units termination gate uses linear hidden state cell. convolutional layers kernel size output dimension fused features classiﬁer. upsampling layer implemented single-layer deconvolutional network kernel size stride upsample classiﬁer original resolution. upsampling layer initialized bilinear transforms. maximum termination steps vary model reduced simply concatenating features extracted convolutional network last vector lstm. results table shows average models respectively. results show model attention achieves better performance relational descriptions dataset. direct descriptions models achieve similar performance. demonstrates framework’s capability interpreting multiple sentences associating source image. figure illustrates model interprets nine sentences inference step. sentences attended ﬁrst step. yellow green attended next consecutive steps. observation experiment model ﬁrst extracted information direct descriptions focused relational descriptions additional reasoning. dataset referit dataset composed photographs real world scenes along natural language descriptions distinct objects photographs dataset contains different object categories including animals people buildings objects background elements training development datasets include images. figure right ground truth image. left illustration sentences attended time step. yellow green represent ﬁrst second third time step respectively. metric following metrics evaluation overall intersection-over-union predicted ground truth region averaged entire test set; precisionthreshold percentage test data whose prediction ground truth threshold. thresholds model implementation vgg- architecture used image encoder images size textual descriptions encoded lstm units. fusion network attention model uses units cells units classiﬁer upsampling layer similar implementation section maximum number inference steps relu used convolutional layer. l-normalization applied parameters network. results table shows experimental results model baseline methods referit dataset. framework yields better precision hypothesis outperformance resulted unique attention mechanism used fusion network efﬁciently associate individual descriptive sentences different regions source image. much discrepancy models probably fact textual descriptions dataset simple. examples semantic labels provided supplementary materials figure first original images. second results image-to-image translation model without text input. third results model taking textual descriptions account. dataset experiment oxford- flowers dataset contains images ﬂower categories. image textual descriptions following split dataset classes training classes testing. given grayscale image ﬂower description shapes colors ﬂower goal colorize image following description. model implementation ﬁfteen-layer convolutional network similar used encoding images. textual descriptions encoded lstm units. fusion network attention model uses units cells units. image encoder composed deconvolutional layers followed convolutional layers upsample fusion feature target image space maximum length spatial discriminator composed layers convolutional networks stride output dimension discriminator score average ﬁnal output. relu used nonlinearity following convolutional layer except last uses sigmoid function. results lack available models task compare framework previous model developed image-to-image translation baseline colorizes images without injection texts. colorization results randomly-sampled images test shown figure without text input baseline approach often colorizes images color framework generate ﬂowers similar original colors speciﬁed texts. figure also provides examples images generated arbitrary text input using trained model. paper introduce problem language-based image editing propose generic modeling framework sub-tasks lbie language-based image segmentation colorization. heart proposed framework fusion module uses recurrent attentive models dynamically decide region image whether continue text-to-image fusion process. models demonstrated superior empirical results three datasets including referit dataset image segmentation oxford- flower dataset colorization proposed cosal dataset evaluating end-to-end performance lbie system. future extend framework image editing subtasks. another interesting area build dialogue-based image editing system allows users edit images interactive natural way.", "year": 2017}