{"title": "Beyond the One Step Greedy Approach in Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, $n$-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.", "text": "famous policy iteration algorithm alternates policy improvement policy evaluation. implementations algorithm several variants latter evaluation stage n-step trace-based returns analyzed previous works. however case multiple-step lookahead policy improvement despite recent increase empirical evidence strength knowledge carefully analyzed yet. work introduce ﬁrst analysis. namely formulate variants multiple-step policy improvement derive algorithms using deﬁnitions prove convergence. moreover show recent prominent reinforcement learning algorithms fact instances framework. thus shed light empirical success give recipe deriving algorithms future study. policy iteration lies core reinforcement learning many planning on-line learning methods classic algorithm repeats consecutive stages -step greedy policy improvement w.r.t. value function estimate evaluation value function w.r.t. greedy policy. although multiple variants evaluation task considered usually considered policy update -step greedy improvement. conducting policy improvement using common -step greedy approach speciﬁc choice necessarily appropriate one. indeed empirically recently suggested greedy approaches w.r.t. multiple steps perform better w.r.t. -step. notable examples alphago alpha-go-zero there approximate online version multiple-step greedy improvement implemented monte carlo tree search celebrated mcts algorithm instantiates several steps lookahead improvement encompasses additional historical impressive accomplishments dating back past century previous decade best knowledge despite empirical successes multiple-step greedy policy improvement never rigorously studied before. motivation work suggest possible algorithms spirit. paper organized follows. start deﬁning h-greedy policy policy greedy w.r.t. horizon steps. using deﬁnition introduce h-pi algorithm class algorithms multiple-step greedy policy improvement guarenteed convergence optimal policy. stress term n-step return often refered literature used context policy evaluation therefore avoid confusion choose denote multiple-step greedy policy letter introduce novel class optimal bellman operators controlled continuous parameter operator used deﬁne greedy policy κ-greedy policy leading algorithm name κ-pi. analogous famous algorithm improvement stage. algorithm parameter interpolates single-step evaluation update inﬁnitehorizon evaluation update similarly recover traditional -step greedy policy improvement update inﬁnite-horizon greedy policy i.e. optimal policy. roughly speaking κ-greedy policy viewed allowing make ‘interpolation’ h-greedy policies. similarly previous paragraph letter avoid confusion parameter remarkably show computing κ-greedy policy equivalent solving optimal policy surrogate κγdiscounted stationary mdp. linear operator optimal bellman operator γ-contraction mappings norm. known single ﬁxed point respectively. standard -step greedy policies w.r.t. furthermore given coincides stationary optimal policies. words every policy -step greedy w.r.t. optimal vice versa. section introduce h-greedy policy generalization -step greedy policy. leads formulate algorithm name h-pi. h-pi derived replacing improvement stage -step greedy policy h-greedy policy. prove convergence show inherits properties n\\{}. h-greedy policy w.r.t. value function belongs following policies notation eπ...πh− means condition trajectory induced choice actions πh−) starting state since argument vector maximization component-wise i.e. wish choice actions jointly maximize entries vector. thus h-greedy policy chooses ﬁrst optimal action non-stationary optimal control problem horizon equality suggests policy equivalently interpreted -step greedy policy w.r.t. h−v. although former view natural fact latter section’s proofs based. thus h-greedy policies w.r.t. expressed follows additional generalization introduce κλ-pi. algorithm similar improvement step κ-pi policy evaluation stage ‘relaxed’ similarly λ-pi κλ-pi illustrates difference parameters. former controls depth evaluation task similar previous works latter controls depth improvement step. next discuss relation work existing literature argue offers generalized view several recent impressive empirical advancements seemingly unrelated thus show relevance proposed mathematical framework current state-of-the-art algorithms. conclude empirical display inﬂuence parameters basic planning task. empirically demonstrate best performance obtained non-trivial choices them. motivates future study algorithms derived introduced framework work. consider inﬁnite-horizon discounted markov decision process framework. deﬁned -tuple ﬁnite state space ﬁnite action space transition kernel reward function discount factor. stationary policy probability distribution policy state value state γtr)] notation means expectation conditioned event following policy vector r|s|. conciseness shall notations γtr)] holds component-wise denote reward value time known remark generalization standard -step greedy operation recovers taking call greedy operator amounts identifying states best ﬁrst action h-horizon optimal operators consider algorithm algorithm assignments hold point-wise. algorithm alternates identifying h-greedy policy solving h-horizon optimal control problem estimating value policy. h-pi algorithm produces sequence policies component-wise increasing values directly implies convergence precise convergence speed generalizing several known results h-pi. begin following lemma essentially consequence fact γh-contraction lemma sequence vπk∞}k≥ contracting coefﬁcient thus convergence rate generalizes known convergence rate original algorithm next theorem describes result respect termination algorithm. theorem h-pi algorithm converges remark note fact γh-contraction imply existing results extend h-pi replacing instance rightmost term logarithm theorem remark notice complexity term theorem decreasing function limit running single iteration h-pi sufﬁcient ﬁnding optimal value-policy pair. however although total number iterations reduced increasing iteration expected computationally costly. section introduce additional novel generalization -step greedy policy κ-greedy policy. similarly previous section newly introduced greedy policy leads algorithm shall name κ-pi. generalization based deﬁnition κ-optimal bellman operator. operator also naturally lead value iteration algorithm κ-vi. later section shall highlight relation κ-greedy previously introduced h-greedy policy. optimality know made non-negative coefﬁcients follows then since deduce consequence tκv∗ lastly show tκv∗ second equality holds thus since ﬁxed point deduce thus show opposite direction assume thus holds hence indeed previous subsection derived κ-optimal bellman operator deﬁned induced κ-greedy policy. operators lead consider algorithm κ-pi algorithm assignments hold component-wise. algorithm repeats consecutive steps identifying κ-greedy policy solving optimal policy surrogate stationary reduced discount factor estimating value policy. shall iterative process guaranteed converge optimal policy-value pair wish solve. thus surrogate stationary depends according basic theory surrogate optimal value. shall denote optimal value refer κ-optimal bellman operator. note that remark κ-optimal bellman operator generalization optimal bellman operator recovers taking i.e. additionally applying equivalent solving original γ-discounted mdp; tκ=v. values applying amounts solving stationary reduced discount factor solution obtained using generic planning model-free model-based algorithm. previously analyzed reported solving smaller discount factor general easier. similarly section shall prove κ-pi algorithm inherits many properties start showing monotonicity property κ-greedy operator. lemma component-wise equality optimal policy. algorithms generalization similar λ-pi interpolates standard shall describe estimate value κ-greedy policy surrogate smaller horizon shaped reward show still yields convergence optimal policy value. thus κλ-pi ease policy evaluation phase κ-pi solving simpler task. λ-pi improvement stage common -step greedy policy evaluation stage relaxed applying operator instead fully estimating value start formulating operator κ-pi. propriate generalization analogous framework following linear operator consider κλ-pi improvement stage similar κ-pi algorithm; however evaluation step apply indeed setting recover κ-vi setting recover κ-pi. moreover setting obtain class λ-pi algorithms. notice whereas λ-pi leave future work question whether κλ-pi makes sense point paper reached general algorithmic formulation. shall prove convergence κλ-pi also provide sensitivity analysis shows errors propagate along steps. indeed interest approximations computing κ-greedy policy updating value function. following theorem lemma upper bound maximal number iteration takes κ-pi algorithm terminate theorem κ-pi algorithm converges remark case h-pi complexity term theorem decreasing function complements fact κ-pi converges iteration again increases iteration expected computationally demanding since surrogate less discounted. lastly using κ-optimal bellman operators derive non-trivial generalization algorithm name κ-vi. κ-vi algorithm repeatedly applies convergence. convergence proof κ-vi rate convergence thus follows easily corollary lemma next section shall group κ-pi κ-vi single larger class algorithms contains both. second equality proposition reveals connection existing works planning shorter horizons generalized advantage estimation using terminology work approach equivalent performing single κ-greedy improvement step. theory developped paper suggests reason stop step. implemented algorithm equivalent on-line policy gradient variant κ-pi algorithm. proposition work states objective function considered describes surrogate being solved improvement step κ-pi. moreover work evaluation algorithm estimates value current policy similarly evaluation stage κ-pi. interpret empirically demonstrated trade-off trade-off eventually alpha-go algorithms interpreted asynchronous online version h-pi algorithm. section empirically test hκ-pi algorithms grid-world problem. mentioned before h-pi performing greedy step amounts solving h-horizon optimal control problem κ-pi amounts solving γκ-discounted stationary cases conducting operations practice done either generic planning model-free modelbased algorithm. here implement hκ-greedy step algorithm. former case simply steps latter case stop value change norm less choice note equivalent solving problem conduct simulations simple deterministic grid-world problem actions {‘up’‘down’‘right’‘left’‘stay’}. experiment randomly chose single state placed reward states reward drawn uniformly considered problem terminal state. also entries initial value function drawn hκ-pi counted total number calls simulator. call takes state-action pair input returns current reward next state. also generalize similar results λ-pi following subscript notation refers iteration algorithm theorem assume algorithm employ noisy versions steps iteration section compare κ-greedy policy h-greedy policy furthermore connect previous works literature framework developed paper. value function n\\{} deﬁne following random variable future h-step return figure empirical performance h-pi κ-pi λ-pi different grid sizes shown results average experiments. standard deviation shown errorbars. h-pi. total queries simulator convergence κ-pi. total queries simulator convergence ‘effective’ planning horizon iteration. λ-pi using κλ-pi total queries simulator convergence empirical optimal performance h-pi κ-pi introduced work better empirical optimal performance λ-pi. figure shows average number calls simulator function value experiments conducted times. ﬁgure depicts optimal computational efﬁciency obtained value parameters trivial empirically ’optimal’ parameter values slowly grow grid dimension comparison measured empirical performance λ-pi simulations show performance λ-pi inferior algorithms. work introduced formulated possible approaches generalize traditional -step greedy operator. borrowing general principle behind proposed family techniques solving single complex problem iteratively solving smaller subproblems. showed discussed approaches coherent previous lines work generalizing existing analyses -step greedy policy hκ-greedy policies. particular derived algorithms showed convergence. introducing analyzing κλ-pi demonstrated κ-pi used ‘relaxed’ value estimation effects noise controlled. last least making connections recent empirical works work sheds light reasons impressive success. conducted simulations example shown generic algorithm used greedy step κh-pi frameworks. empirically demonstrated interesting consider online versions algorithmic schemes introduced here would probably require consider least time scales theoretical side ﬁrst attempts direction suggest previous approaches proving online convergence straightforwardly generalized. currently working extension. potential practical extension work would consider state-dependent value function though details generalization need written carefully believe machinery developped still hold. expect convergence assured rates would intricately depend function. using approach algorithm designer could ‘prior knowledge’ learning phase. general though understanding better choice function would good intriguing deserves future investigation. deﬁned matrix deﬁne following alternative error δk++max \u0001k−ξ constant vector made ones. since sequence policies generated original algorithm error would generated algorithm erros similar invariance argument assume without loss generality then browne cameron powley edward whitehouse daniel lucas simon cowling peter rohlfshagen philipp tavener stephen perez diego samothrakis spyridon colton simon. survey monte carlo tree search methods. ieee transactions computational intelligence games busoniu lucian munos remi. optimistic planning markov decision processes. international conference artiﬁcial intelligence statistics aistats- volume journal machine learning research workshop conference proceedings palma canary islands spain april https//hal.archives-ouvertes. fr/hal-. françois-lavet vincent fonteneau raphael ernst damien. discount deep reinforcement learning towards dynamic strategies. arxiv preprint arxiv. grill jean-bastien valko michal munos rémi. blazing trails beating path sample-efﬁcient monte-carlo planning. neural information processing systems barcelona spain december https//hal.inria.fr/hal-. hren jean-francois munos rémi. optimistic planeuropean workning deterministic systems. shop reinforcement learning france https//hal.archives-ouvertes. fr/hal-. jiang kulesza alex singh satinder lewis richard. dependence effective planning horizon model accuracy. proceedings international conference autonomous agents multiagent systems international foundation autonomous agents multiagent systems mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous internamethods deep reinforcement learning. tional conference machine learning munos rémi. bandits monte-carlo tree search optimistic principle applied optimization planning. technical report https//hal. archives-ouvertes.fr/hal-. pages. scherrer bruno. performance bounds lambda policy iteration application game tetris. journal machine learning research january https//hal.inria.fr/ hal-. scherrer bruno. improved generalized upper bounds complexity policy iteration. mathematics operations research february moor... https//hal.inria.fr/ hal-. markov decision processes dynamic programming analysis algorithms. schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. international conference machine learning schulman john moritz philipp levine sergey jordan michael abbeel pieter. high-dimensional continuous control using generalized advantage estimation. arxiv preprint arxiv. silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature silver david hubert thomas schrittwieser julian antonoglou ioannis matthew guez arthur lanctot marc sifre laurent kumaran dharshan graepel thore mastering chess shogi self-play general reinforcement learning algorithm. arxiv preprint arxiv. silver david schrittwieser julian simonyan karen antonoglou ioannis huang guez arthur hubert thomas baker lucas matthew bolton adrian mastering game without human knowledge. nature szörényi balázs kedenburg gunnar munos rémi. optimistic planning markov decision processes using generative model. advances neural information processing systems montréal canada december https//hal.inria.fr/hal-. h−vπk− h−vπk− since h−vπk− seen lemma since used non-negative take norm. using fact contraction norm contraction norm. thus taking norm sides prove claim. proof close spirit related proof also derived using relations explicitly give proof completeness. furthermore proof close spirit context presented work; explicitly show sets equal. last line packed inﬁnite introduced summation dummy variable since every realization random variables equality holds also holds mean. concludes proof ﬁrst equality since explicitly shown equality optimization criteria. solution optimization problem ﬁnding κ-greedy policy remains invariant addition since constant w.r.t. optimization problem. adding yields expression calculated ﬁrst section proof. concludes proof since optimization problem thus κ-greedy policies unaffected manipulation.", "year": 2018}