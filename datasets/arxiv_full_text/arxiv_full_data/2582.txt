{"title": "On the Expressive Power of Deep Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows:  (1) The complexity of the computed function grows exponentially with depth.  (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights.  (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.", "text": "propose approach problem neural network expressivity seeks characterize structural properties neural network family affect functions able compute. approach based interrelated measures expressivity uniﬁed novel notion trajectory length measures output network changes input sweeps along one-dimensional path. ﬁndings summarized follows complexity computed function grows exponentially depth. design measures expressivity capture non-linearity computed function. network transforms input measures grow exponentially depth. weights equal trained networks sensitive lower layer weights much less robust noise layer weights also perform better weights optimized well. trajectory regularization works like batch normalization. batch norm stabilizes learnt representation based propose regularization scheme trajectory regularization. despite successes understanding neural network architectures achieve empirical successes still lacking. includes even fundamental question neural network expressivity architectural properties neural network affect resulting functions compute ensuing performance. foundational question rich history prior work addressing expressivity neural networks. however challenging derive conclusions provide theoretical generality respect choices architecture well meaningful insights practical performance. indeed ﬁrst results question take highly theoretical approach using functional analysis show universal approximation results analysing expressivity comparisons boolean circuits studying network dimension results provided theoretically general conclusions shallow networks studied different deep models proven successful recent years. response several recent papers focused understanding beneﬁts depth neural networks results compelling take modern architectural changes account show speciﬁc choice weights deeper network results inapproximability shallow network. particular goal line work establish lower bounds showing separations shallow deep networks based hand-coded constructions speciﬁc network weights. even weight values used constructions robust small perturbations functions arise constructions tend toward extremal properties design evidence network trained data ever resembles function. neural network expressivity remained largely unanswered. first lack good understanding typical case rather worst case bounds deep networks consequently evaluate whether hand-coded extremal constructions provide reﬂection complexity encountered standard settings. second lack understanding upper bounds match lower bounds produced prior work; constructions used date place near limit expressive power neural networks still large gaps? finally understanding issues might begin draw connections network expressivity observed performance. contributions measures expressivity applications paper address challenges deﬁning analyzing interrelated measures expressivity neural networks; framework applies wide range standard architectures independent speciﬁc weight choices. begin analysis start training random initialization later derive insights connecting network expressivity performance. ﬁrst measure expressivity based notion activation pattern network units compute functions based discrete thresholds units thresholds range standard architectures consider network essentially computing linear function activation pattern; thus counting number possible activation patterns provides concrete measuring complexity beyond linearity network provides. give upper bound number possible activation patterns setting weights. bound tight matches hand-constructed lower bounds earlier work analysis notion transition changing input nearby input changes activation pattern. study behavior transitions pass input along one-dimensional parametrized trajectory central ﬁnding trajectory length grows exponentially depth network. trajectory length serves unifying notion measures expressivity leads insights behavior trained networks. speciﬁcally exponential growth trajectory length function depth implies small adjustments parameters lower network induce larger changes comparable adjustments higher network. demonstrate phenomenon experiments mnist cifar- network displays much less robustness noise lower layers better performance trained well. also explore effects regularization methods trajectory length network trains propose less computationally intensive method regularization trajectory regularization offers performance batch normalization. measures expressivity propose easily computable measures neural network expressivity capture expressive power inherent different neural network architectures independent speciﬁc weight settings. trajectory regularization based understanding effect batch norm trajectory length propose method regularization trajectory regularization offers advantages batch norm computationally efﬁcient. prior work studied propagation riemannian curvature random networks developing mean ﬁeld theory approach. here take approach grounded computational geometry presenting measures combinatorial ﬂavor explore consequences training. given neural network certain architecture associated function input represents parameters network. goal understand behavior changes changes values might encounter training across inputs ﬁrst major difﬁculty comes high dimensionality input. precisely quantifying properties entire input space intractable. tractable alternative study simple dimensional trajectories input space. formally deﬁnition given points trajectory curve tial increase linear regions depth architectures. also appeal zaslavsky’s theorem theory hyperplane arrangements show shallow network i.e. hidden layer number parameters deep network much smaller number linear regions number achieved choice weights deep network. derive much general result considering ‘global’ activation patterns entire input space prove fully connected network number hidden layers upper bound number linear regions achieve possible weight settings upper bound asymptotically tight matched construction given result written formally theorem upper bound number activation patterns denote fully connected network hidden layers width inputs number activation patterns upper bounded relu activations omn) hard tanh. prove inductive proof regions hyperplane arrangement. proof found appendix. noted introduction result differs earlier lower-bound constructions upper bound applies possible sets weights. analysis also prove notion linear region introduced. given neural network piecewise linear activations function computes also piecewise linear consequence fact composing piecewise linear functions results piecewise linear function. measure expressive power different architectures count number linear pieces determines nonlinear function fact change linear region caused neuron transition output layer. precisely deﬁnition ﬁxed neuron piecewise linear region transitions inputs activation function switches linear region relu transition would given neuron switching hard tanh switching saturation linear middle region saturation generic trajectory thus deﬁne number transitions undergone output neurons sweep input instead concentrating output neurons however look pattern entire network. call activation patten deﬁnition deﬁne activation pattern string form }num neurons }num neurons network encoding linear region activation function every neuron input weights overloading notation slightly also deﬁne number distinct activation patterns sweep along distinct activation pattern corresponds different linear function input combinatorial measure captures much expressive simple linear mapping. figure deep networks piecewise linear activations subdivide input space convex polytopes. take three hidden layer relu network input four units layer. left pane shows activations ﬁrst layer only. input neurons ﬁrst hidden layer associated line depicting activation boundary. left pane thus four lines. second hidden layer neuron line input space corresponding on/off line different region described ﬁrst layer activation pattern. centre pane shows activation boundary lines corresponding second hidden layer neurons green green lines ‘bend’ boundaries. finally right pane adds on/off boundaries neurons third hidden layer purple. lines bend black green boundaries image shows. ﬁnal convex polytopes corresponds activation patterns network unit square polytope representing different linear function. theorem regions input space given corresponding function neural network relu hard tanh activations input space partitioned convex polytopes corresponding different linear function region. result independent interest optimization linear function convex polytope results well behaved loss function easy optimization problem. understanding density regions training process would likely shed light properties loss surface improved optimization methods. picture network’s regions shown figure empirically tested growth number activations transitions varied along understand behavior. found bounded linearities especially tanh hard-tanh observe exponential growth depth scale parameter initialization also affects observations also experimented sweeping weights layer trajectory counting different labellings output network. ‘dichotomies’ measure discussed further appendix also exhibits growth properties figure figure number transitions seen fully connected networks different widths depths initialization scales circular trajectory mnist datapoints. number transitions grows exponentially depth architecture seen rate growth seen increasing architecture width plotted surprising dependence scale initialization explained figure picture showing trajectory increasing depth network. start circular trajectory feed fully connected tanh network width pane second left shows image circular trajectory transformed ﬁrst hidden layer. subsequent panes show projections latent image circular trajectory transformed hidden layers. ﬁnal pane shows trajectory transformed hidden layers. fact turns reason exponential growth depth sensitivity initialization scale. returning deﬁnition trajectory deﬁne immediately related quantity trajectory length deﬁnition given trajectory deﬁne length standard length denote before fully connected networks hidden layers width initializing weights biases relu hard tanh random neural network dimensional trajectory trival perpendicular component figure look trajectory growth different initialization scales trajectory propagated convolutional architecture cifar- relu activations. analysis theorem fully connected networks trajectory growth holds convolutional architectures also. note decrease trajectory length seen layers expected layers pooling layers. figures growth input trajectory relu networks cifar- mnist. cifar network convolutional observe layers also result similar rates trajectory length increases fully connected layers. also would expected pooling layers reduce trajectory length. discuss upper bounds appendix. figure number transitions linear trajectory length. compare empirical number transitions length trajectory different depths hard-tanh network. repeat comparison variety network architectures different network width weight variance hard tanh case formally prove relation trajectory length transitions assumption assume sweep neurons saturated unless transitioning saturation endpoints happens rapidly. have theorem transitions proportional trajectory length fank hard tanh network hidden layers width schematic image depicting seen figure proof found appendix. rough outline follows look expected growth difference point curve small perturbation layer layer denotanalysis complicated statistical dependence image input instead form recursion looking component difference perpendicular image input layer i.e. explore insights gained applying measurements expressivity particularly trajectory length understand network performance. examine connection expressivity stability inspired this propose method regularization trajectory figure pick single layer conv trained high accuracy cifar noise layer weights increasing magnitudes testing network accuracy initial layers network least robust noise ﬁgure shows adding noise magnitude ﬁrst layer results drop accuracy amount noise added ﬁfth layer barely results drop accuracy. pattern seen many different initialization scales even scaling used experiment. analysis network expressivity offers interesting takeaways related parameter functional stability network. proof theorem perturbation input would grow exponentially depth network. easy analysis limited input layer applied layer. form would means perturbations weights lower layers costly perturbations upper layers exponentially increasing magnitude noise result much larger drop accuracy. figure train conv network cifar- noise varying magnitudes exactly layer shows exactly this. also converse holds initializing network trained single layer different depths network found monotonically increasing performance layers lower network trained. shown figure figure appendix. figure demonstration expressive power remaining depth mnist. plot train test accuracy achieved training exactly layer fully connected neural mnist. different lines generated varying hidden layer chosen train. layers kept frozen random initialization. training lower hidden layers leads better performance. networks width weight variance hard-tanh nonlinearities. note train second hidden layer onwards number parameters trained remains ﬁxed. figure training increases trajectory length even typical initialization values propagate circular trajectory joining cifar datapoints conv without batch norm look trajectory length changes training. training causes trajectory length increase exponentially depth note step network exponential growth regime. observe even networks aren’t initialized exponential growth regime pushed training. taking measures trajectories training without batch norm trajectory length tends increase training shown figures figure experiments networks appendix. initialized trained high test accuracy cifar mnist. cases trajectory length increases training progresses. architecture note even smaller weight initialization weight norms increase training shown figure pushing typically initialized networks exponential growth regime. scales outgoing activations parameter learnt. implementation typically scale additional loss constant reduce magnitude comparison classiﬁcation loss. results figure show trajectory regularization batch norm perform comparably considerably better using batch norm. advantage using trajectory regularization don’t require different computations performed train test enabling efﬁcient implementation. initial growth trajectory length enables greater functional expressivity large trajectory growth learnt representation results unstable representation witnessed figure figure train another conv cifar time batch normalization. batch norm layers reduce trajectory length helping stability. figure growth circular trajectory datapoints batch norm layers conv cifar. network initialized typical note batch norm layers step poorly behaved division close variance. hundred gradient steps continuing onwards batch norm layers reduce trajectory length stabilising representation without sacriﬁcing expressivity. motivated fact batch normalization decreases trajectory length hence helps stability generalization consider directly regularizing trajectory length replace every batch norm layer used conv figure trajectory regularization layer. layer adds loss figure replace batch norm layer cifar conv trajectory regularization layer described section training trajectory length easily calculated piecewise linear trajectory adjacent datapoints minibatch. trajectory regularization achieves performance batch norm albeit slightly train time. however trajectory regularization behaves train test time simpler efﬁcient implement. characterizing expressiveness neural networks understanding expressiveness varies parameters architecture challenging problem difﬁculty identifying meaningful notions expressivity linking analysis implications networks practice. paper presented interrelated expressivity measures; shown tight exponential bounds growth measures depth networks offered unifying view analysis notion trajectory length. analysis trajectories provides insights performance trained networks well suggesting networks practice sensitive small perturbations weights lower layers. also used explore empirical success batch norm developed regularization method trajectory regularization. matus telgarsky. representation beneﬁts deep feedforward networks. arxiv preprint arxiv. james martens arkadev chattopadhya toni pitassi richard zemel. representational efﬁciency restricted boltzmann machines. advances neural information processing systems pages monica bianchini franco scarselli. complexity neural network classiﬁers comparison shallow deep architectures. neural networks learning systems ieee transactions poole subhaneil lahiri maithra raghu jascha sohldickstein surya ganguli. exponential expressivity deep neural networks transient chaos. advances neural information processing systems pages sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing interproceedings internal covariate shift. national conference machine learning icml lille france july pages measures expressivity properties neural network performance. also natural connection between adversarial examples trajectory length adversarial perturbations small distance away input space result large change classiﬁcation understanding trajectories original input adversarial perturbation grow might provide insights phenomenon. another direction partially explored paper regularizing based trajectory length. simple version presented performance gains might achieved sophisticated method. references alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature chris piech jonathan bassen jonathan huang surya ganguli mehran sahami leonidas guibas jascha sohl-dickstein. deep knowledge tracing. advances neural information processing systems pages razvan pascanu guido montufar yoshua bengio. number response regions deep feed forward networks piece-wise linear activations. arxiv preprint arxiv. proof. show inductively partitions input space convex polytopes hyperplanes. consider image input space ﬁrst hidden layer. neuron deﬁnes hyperplane input space letting relu hyperplanes hard-tanh. considering hyperplanes neurons ﬁrst layer hyperplane arrangement input space polytope corresponding speciﬁc activation pattern ﬁrst hidden layer. assume partitioned input space convex polytopes hyperplanes layers consider speciﬁc polytope activation pattern layers constant input λjxj constant term comprising bias output saturated units. setting expression zero gives hyperplane equation time equation valid deﬁned hyperplane either partition output pattern also constant theorem follows. implies dimensional trajectory ‘double back’ repeat activation patterns. particular seeing transition never return region left. simple example trajectory straight line corollary transitions output patterns afﬁne trajectory afﬁne dimensional trajectory input neural network partition intervals every time neuron transitions. every interval unique network activation pattern generalizing dimensional trajectory many regions achieved entire input i.e. many distinct activation patterns seen? ﬁrst prove bound number regions formed hyperplanes theorem upper bound regions hyperplane arrangement suppose hyperplanes i.e. equations form number regions proof. hyperplane arrangement denoted speciﬁc hyperplane. number regions precisely number regions plus number regions particular recursive formula proof. first consider relu case. neuron hyperplane associated theorem ﬁrst hidden layer divides inputs space regions consider second hidden layer. every region ﬁrst hidden layer different activation pattern ﬁrst layer different hyperplane arrangement hyperplanes dimensional space contributing regions. particular total number regions input space result ﬁrst second hidden layers continuing hidden layers gives bound. similar method works hard tanh neuron produces hyperplanes resulting bound omn). notation preliminary results difference points trajectory given trajectory parallel perpendicular components given vectors write component perpendicular component parallel motivation consider decomposition resulting independence different components shown following lemma. lemma independence projections given vector respect independent random variables. rotational invariance random gaussian matrices i.e. gaussian matrix entries rotation also gaussian entries rotation lemma also gaussian furthermore partition entries evidently independent. ˜w⊥v also independent. following lemmas rotational invariance gaussians well distribution prove results expected norm random gaussian vector. lemma norm gaussian vector random gaussian vector proof. fact random gaussian follows distribution. means γ/)/γ mean distribution degrees freedom result follows noting expectation lemma multiplied expectation. useful bound ratios gamma function introduce following inequality provides extension gautschi’s inequality. theorem extension gautschi’s inequality deﬁnition non-zero entries form follows exactly zero entries centered gaussian variance ||x⊥||. lemma expected norm statement. apply theorem lower bound. first note view a⊥w⊥ ⊥aw⊥. subspace ﬁxed size making perpendicular commutes making perpendicular projecting everything subspace.) view random matrix lemma deﬁne rotation matrices respectively analogous proof. inequality seen intuitively geometrically diagonal covariance matrix contours ||x|| circular centered decreasing radially. however contours shifted centered around ||µ|| shifting back reduces norm. formal proof seen follows wish show result non-zero bias fact easily extend result case non-zero bias. insight note involves taking difference bias term enter expression computations hold equation becomes figure ﬁgure shows trajectory growth different initialization scales trajectory propagated fully connected network mnist relu activations. note described bound theorem trajectory growth exponential depth increases initialization scale width increases faster scale width expected statement proof upper bound trajectory growth hard tanh replace hard-tanh linear coordinate-wise identity provides upper bound norm. also recover distribution terms standard deviation figure exponential growth trajectory length depth random deep network hard-tanh nonlinearities. circular trajectory chosen random vectors. image trajectory taken layer network length measured. trajectory length layer terms network width weight variance determine growth rate. average ratio trajectory’s length layer relative length layer solid line shows simulated data dashed lines show upper lower bounds growth rate function layer width weight variance means expectation neuron layer sensitive transitions using this fact neurons layer sensitive follows random variable denoting number transitions layer measures expressivity mostly concentrated sweeping input along trajectory taking measures instead also sweep weights along trajectory look consequences ﬁxed inputs fact random initialization sweeping ﬁrst layer weights statistically similar sweeping input along trajectory particular letting denote ﬁrst layer weights particular input vector coordinate extending observation distribution expect similarities results sweeping weights sweeping input trajectories explore synthetic experiments primarily hard tanh figures proportionality transitions trajectory length extends dichotomies results expressive power afforded remaining depth. non-random inputs non-random functions well known question upper bounded sauer-shelah lemma discuss appendix random setting statistical duality weight sweeping input sweeping suggests direct proportion transitions trajectory length ﬁxed input. furthermore sufﬁciently uncorrelated class label transitions occur independently indeed show figure figure sweep weights layer trajectory count number labellings datapoints. ﬁrst layer statistically identical sweeping input thus similar results observed exponential increase depth architecture much slower increase width. plot number classiﬁcation dichotomies input vectors achieved sweeping ﬁrst layer weights hard-tanh network along one-dimensional great circle trajectory. show function depth several widths function width several depths. networks generated weight variance figure expressive power depends remaining network depth. plot number dichotomies achieved sweeping weights different network layers -dimensional great circle trajectory function remaining network depth. number achievable dichotomies depend total network depth number layers layer swept. number datapoints hard-tanh nonlinearities. blue dashed networks width weight variance line indicates possible dichotomies random dataset. figure plot number unique dichotomies observed function number transitions network undergone. datapoint corresponds number transitions dichotomies hard-tanh network different depth weights ﬁrst layer undergoing interpolation along great circle trajectory compare plots random walk simulation transition single class label ﬂipped uniformly random. dichotomies measured dataset consisting random samples networks weight variance blue dashed line indicates possible dichotomies. figure repeat similar experiment figure fully connected network cifar- mostly observe training lower layers leads better performance although expected overall performance impacted training single layer. networks width weight variance hard-tanh nonlinearities. train second hidden layer number parameters remains ﬁxed. theory applies training error generalisation accuracy remains constrained setting. figure training increases trajectory length smaller initialization values experiment plots growth trajectory length circular interpolation mnist datapoints propagated network different train steps. indicates start training purple training. training process increases trajectory length likely increase expressivity input-output enable greater accuracy.", "year": 2016}