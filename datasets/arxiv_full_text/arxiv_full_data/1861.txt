{"title": "Neural Text Generation: A Practical Guide", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Deep learning methods have recently achieved great empirical success on machine translation, dialogue response generation, summarization, and other text generation tasks. At a high level, the technique has been to train end-to-end neural network models consisting of an encoder model to produce a hidden representation of the source text, followed by a decoder model to generate the target. While such models have significantly fewer pieces than earlier systems, significant tuning is still required to achieve good performance. For text generation models in particular, the decoder can behave in undesired ways, such as by generating truncated or repetitive outputs, outputting bland and generic responses, or in some cases producing ungrammatical gibberish. This paper is intended as a practical guide for resolving such undesired behavior in text generation models, with the aim of helping enable real-world applications.", "text": "diagnostics common issues rare out-of-vocabulary words decoded output short truncated ignore portions input decoded output repeats lack diversity deployment neural networks recently attained state-of-the-art results many tasks machine learning including natural language processing tasks sentiment understanding machine translation. within number core tasks involve generating text conditioned input information. prior last years predominant techniques text generation either based template rule-based systems well-understood probabilistic models n-gram log-linear models rule-based statistical models however despite fairly interpretable well-behaved require infeasible amounts hand-engineering scale—in case rule template-based models—or tend saturate performance increasing training data hand neural network models text despite sweeping empirical success poorly understood sometimes poorly behaved well. figure illustrates trade-oﬀs types systems. help adoption usage neural text generation systems detail practical suggestions developing systems. include brief overview training decoding procedures well suggestions training models. primary focus however advice diagnosing resolving pathological behavior decoding. take long time retrain models comparatively cheap tune decoding procedure; hence it’s worth understanding quickly deciding whether retrain. figure illustrates feedback loops improving diﬀerent components model training decoding procedures. despite growing body research information best practices tends scattered often depends speciﬁc model architectures. starting hyperparameters suggested advice guide intended architecture-agnostic possible error analysis emphasized instead. helpful ﬁrst read background section remaining sections read independently. guide focuses advice training decoding neural encoder-decoder models text generation tasks. roughly speaking source target assumed order dozens tokens. primary focus guide decoding learning word embeddings goal learning thought vectors sentences remained elusive previously mentioned also consider sequence labeling classiﬁcation tasks. global coherence. remains challenge curse dimensionality well neural networks failing learn abstract concepts predominant next-step prediction training objective. consequently focus natural language precise guide cover natural language generation entails generating documents longer descriptions structured data. primary focus tasks target single sentence— hence term text generation opposed language generation. based systems reasonable option. consider example seminal work eliza computer program intended emulate psychotherapist—that based pattern matching rules generating responses. general neural-based systems unable perform dialogue state management required systems. consider task generating summary large collection documents. soft attention mechanisms used neural systems currently direct condition amount text. size output vocabulary input/source sequence length output/target sequence length encoder hidden states denotes representation timestep decoder hidden states denotes representation timestep attention matrix attention weight decoder timestep encoder state representation denotes equality follows chain rule probability. depending choose tokenize text vocabulary contain characters wordpieces/byte-pairs words unit. tasks consider paper divide sequence input source sequence output target sequence example machine translation tasks might sentence english translated sentence chinese. case model beyond tasks described ﬁrst half table many techniques described paper also extend tasks intersection text modalities. example speech recognition sequence features computed short snippets audio corresponding text transcript image captioning image corresponding text description. could also include sequence labeling another task instead figure figure illustrating generic encoder-decoder model architecture assume guide. several choices possible encoder decoder architectures well attention mechanism. show outputs single timestep. consider tasks clear one-to-one correspondence source target. lack correspondence leads issues decoding focus section reasoning applies sequence classiﬁcation tasks sentiment analysis. encoder-decoder models also referred sequence-to-sequence models developed machine translation rapidly exceeded performance prior systems depite comparatively simple architectures trained end-to-end source directly target. neural network-based approaches count-based methods methods involving learning phrase pair probabilities used language modeling translation. prior recent encoder-decoder models feed-forward fully-connected neural networks shown work well language modeling. models simply stack aﬃne matrix transforms followed nonlinearities input following hidden layer however networks fallen favor modeling sequence data require deﬁning ﬁxed combine input current time step previous hidden state summarizing previous time steps many diﬀerent gating mechanisms developed architectures ease optimization also used masking avoid peeking ahead future inputs training using convolutions beneﬁt training parallelizing across time dimension instead computing next hidden state step time. recurrent convolutional networks modeling sequences typically rely timestep attention mechanism acts shortcut connection target output prediction relevant source input hidden states. highlevel decoder timestep decoder representation used compute weight encoder representation example could done using product αijej decoder along y<i. recent models rely purely attention mechanisms masking also shown obtain good better results cnn-based models describe attention mechanism detail section thus maximizing log-likelihood training data. previous ground truth inputs given model predicting next index sequence training method sometimes referred teacher forcing. inability current datasets memory well faster convergence gradient updates computed minibatches training sentences. stochastic gradient descent well optimizers adam shown work well empirically. recent reserarch also explored methods training sequence models using reinforcement learning separate adversarial loss writing however aforementioned training method primary workhorse training models. decoding given source sequence seek generate target maximizes scoring function greedy decoding simply take argmax softmax output distribution timestep feed input next timestep. thus timestep single hypothesis. although greedy decoding work surprisingly well note often result probable output hypothesis since path probable overall despite including output argmax since it’s usually intractable consider every possible branching factor number timesteps instead perform beam search iteratively expand hypotheses token time every search iteration keep best hypotheses beam width beam size. here’s full beam search procedure detail surprising result neural models relatively small beam sizes yield good results rapidly diminishing returns. further larger beam sizes even yield worse results. example beam size work marginally better beam size beam size work worse finally oftentimes incorporating language model scoring function help improve performance. since need trained target corpus train language models much larger corpuses parallel data. objective decoding maximize joint probability despite issues simple procedure works fairly well. however arise many cases beam search without language model result optimal outputs. inherent biases decoding procedure. describe diagnose tackle problems arise section basic attention mechanism used attend portions encoder hidden states decoder timestep many extensions applications. attention also previous decoder hidden states called self-attention used components separate encoder-decoder model instead encoder hidden states grave also used monitor training progress well inspect correspondences input output sequence network learns. last bit—that attention matrix typically follows correspondences input output—will useful discuss methods guiding decoding procedure section hence detail attention matrix here. attention matrix columns rows number output timesteps number input timesteps. every discrete probability distribution encoder hidden states guide assume equal number input timesteps case encoder column entry threshold suggests corresponding encoder input ignored decoder. likewise multiple values threshold suggests encoder hidden states repeatedly used multiple decoder challenges developing text generation systems satisfying automated metric evaluating ﬁnal output system. unlike classiﬁcation sequence labeling tasks cannot precisely measure output quality text generation systems barring human evaluation. perplexity always correlate well downstream metrics automated otherwise. common metrics based n-gram overlap rouge bleu rough approximations often capture linguistic ﬂuency cohererence metrics especially problematic open-ended generation tasks summarization dialogue. recent results shown though automated metrics great distinguishing systems performance passes baseline nonetheless useful ﬁnding examples performance poor also consistent evaluating similar systems thus despite issues current automated evaluation metrics assume model development; however manual human evaluation interspersed well. increasingly advanced libraries building computation graphs performing automatic diﬀerentiation signiﬁcant portion software development process devoted data preparation. broadly speaking data collected remains cleaning tokenization splitting training test data. important consideration cleaning setting character encoding—for example ascii utf-—for libraries python’s unidecode save time. cleaning comes less easily-speciﬁed tasks splitting text sentences tokenization. present recommend stanford corenlp extensive options better handling sentence word boundaries available libraries. alternative performing tokenization avoid altogether. instead working word level instead operate character level intermediate subword units models result longer sequences overall empirically subword models tend provide good trade-oﬀ sequence length handling rare words section discusses beneﬁts subword models detail. ultimately using word tokens it’s important consistent tokenization scheme inputs system—this includes handling contractions punctuation marks quotes hyphens periods denoting abbreviations sentence boundaries character escaping etc. suggestions multilingual preprocessing welcome. https//github.com/stanfordnlp/corenlp stanford tokenizer page https//nlp.stanford.edu/software/tokenizer.html detailed list options. heuristics suﬃcient handling many issues training models. start getting model overﬁt tiny subset data quick sanity check. loss explodes keep reducing learning rate doesn’t. model overﬁts apply dropout weight decay doesn’t. gradient clipping often crucial avoid exploding gradient problem using reasonably large learning rate. variants periodically annealing learning rate validation loss fails decrease typically helps signiﬁcantly. etc.) using several last model checkpoints. validation cross entropy loss ﬁnal performance correlate well signiﬁcant diﬀerences ﬁnal performance across checkpoints similar validation losses. suppose you’ve trained neural network encoder-decoder model achieves reasonable perplexity validation set. running decoding generation using model. simplest greedy decoding described section there beam search decoding yield additional performance improvements. however it’s rare things simply work. section intended quick reference encountering common issues decoding. first besides manual inspection it’s helpful create diagnostic metrics debugging diﬀerent components text generation system. despite training encoder-decoder network source target decoding procedure introduce additional components clear components prioritize trying improve performance combined system; hence helpful ablative analysis language model suggestions measuring performance several reasonably spaced values plotting performance trend; measuring perplexity language model trained varying amounts training data data would helpful yields diminishing returns; measuring performance training language model several diﬀerent domains cases it’s diﬃcult obtain data close target domain. languages large vocabularies especially languages rich morphologies rare words become problematic choosing tokenization scheme results token labels feasible model output softmax. approach ﬁrst used deal issue simply truncate softmax output size assign remaining decoding search procedure hypotheses terminate token. decoder network learn place probability token target fully generated; however sometimes suﬃciently probability. length hypothesis grows total probability decreases. thus normalize probability length hypothesis shorter hypotheses favored. illustrates example hypothesis terminates early. issue exacerbated incorporating language model term. simple ways resolving issue normalizing log-probability score adding length bonus. length bonus replace score hyperparameter. note normalizing total log-probability length equivalent maximizing root probability adding length bonus equivalent multiplying probability every timestep baseline intuitively every source timestep attention matrix places full probability source timestep aggregated decoding timesteps coverage penalty zero. otherwise penalty incurred attending source timestep. repeating outputs common issue often seem expose neural versus template-based systems. simple measures include adding penalty model reattends previous timesteps attention shifted away. easily detected using attention matrix manually selected threshold. finally fundamental issue consider repeating outputs poor training model parameters. passing attention vector part decoder input predicting another training-time method dialogue often common responses many diﬀerent conversation turns generic responses don’t know common problem. similarly problems many possible source inputs much smaller possible target outputs diversity outputs issue. simple method trying encourage diversity decoded outputs. practice however method penalizing low-ranked siblings step beam search decoding procedure shown work well another sophisticated method maximize mutual information source target signiﬁcantly diﬃcult implement requires generating n-best lists although speed decoding huge concern trying achieve state-of-the-art results concern deploying models production real-time decoding often requirement. beyond gains using highly parallelized hardware gpus using libraries optimized matrix-vector operations discuss techniques improving runtime decoding. consider factors determine runtime decoding algorithm. beam search algorithms consider runtime scale linearly beam size runtime often scale approximately quadratically hidden size network ﬁnally linearly number timesteps thus decoding might complexity thus possible methods speeding decoding include developing heuristics prune beam ﬁnding best trade-oﬀ size vocabulary decoder timesteps batching multiple examples together caching previous computations performing much computation possible within compiled computation graph. describe techniques training dealing undesired behavior natural language generation models using neural network decoders. since training models tends timeconsuming decoding worth making sure decoder fully debugged committing training additional models. encoder-decoder models evolving rapidly hope techniques useful diagnosing variety issues developing system.", "year": 2017}