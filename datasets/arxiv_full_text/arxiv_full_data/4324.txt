{"title": "Zero Shot Recognition with Unreliable Attributes", "tag": ["cs.CV", "stat.ML"], "abstract": "In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like \\emph{striped} and \\emph{four-legged}, one can construct a classifier for the zebra category by enumerating which properties it possesses---even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.", "text": "principle zero-shot learning makes possible train recognition model simply specifying category’s attributes. example classiﬁers generic attributes like striped four-legged construct classiﬁer zebra category enumerating properties possesses—even without providing zebra training images. practice however standard zero-shot paradigm suffers attribute predictions novel images hard right. propose novel random forest approach train zero-shot models explicitly accounts unreliability attribute predictions. leveraging statistics attribute’s error tendencies method obtains robust discriminative models unseen classes. devise extensions handle few-shot scenario unreliable attribute descriptions. three datasets demonstrate beneﬁt visual category learning zero training examples critical domain rare categories categories deﬁned visual recognition research achieved major successes recent years using large datasets discriminative learning algorithms. typical scenario assumes multi-class task ample labeled training images class interest. however many realworld settings meet assumptions. rather system closed thoroughly trained object detectors would like acquire models categories minimal effort training examples. essential cope long-tailed distribution objects world also support applications categories emerge dynamically—for example scientist deﬁnes phenomenon interest detected visual data. zero-shot learning offers compelling solution. zero-shot learning novel class trained description—not labeled training examples general requires learner access mid-level semantic representation human teacher deﬁne novel unseen class specifying conﬁguration semantic properties. visual recognition semantic properties attributes shared among categories like black ears rugged. supposing system predict presence attribute novel images adding category model amounts deﬁning attribute signature example even without labeling images zebras could build zebra classiﬁer instructing system zebras striped black white etc. interestingly computational models attribute-based recognition supported cognitive science literature researchers explore humans conceive objects bundles attributes natural categories appear convex regions conceptual space axes corresponding psychological quality dimensions furthermore category systems evolve provide maximum information least cognitive effort mapping categories attribute structures novel human judgments extrapolated based people associate predicates object names principle could perfectly predict attribute presence zero-shot learning would offer elegant solution generating novel classiﬁers problem however can’t assume perfect attribute predictions. visual attributes practice quite difﬁcult learn accurately—often even object categories themselves. many attributes correlated another abstract linguistic properties diverse visual instantiations thus attribute-based zero-shot recognition remains proof concept realm practice falling short alternate transfer methods propose approach train zero-shot models explicitly accounts unreliability attribute predictions. whereas existing methods take attribute predictions face value method training acknowledges known biases mid-level attribute models. speciﬁcally develop random forest algorithm that given attribute signatures category exploits attribute classiﬁers’ receiver operating characteristics select discriminative predictable decision nodes. generalize idea account unreliable class-attribute associations. finally extend solution few-shot setting small number category-labeled images also available training. demonstrate idea three large datasets object scene categories show clear advantages status models. results suggest valuable role attributes play low-cost object category learning spite inherent difﬁculty learning reliably. existing zero-shot models take two-stage classiﬁcation approach given novel image ﬁrst attributes predicted class label predicted function attributes. example unseen object class described binary indicator vector attributes; image mapped unseen class signature similar attribute predictions. probabilistic direct attribute prediction method takes similar form adds priors classes attributes computes prediction unseen class label. topic model variant explored model gained traction often used work methods ours training unseen class amounts specifying attribute signature. contrast approach none existing methods account attribute unreliability learning unseen category. results dramatic impact generalization. stress attribute unreliability distinct attribute strength. former pertains reliable mid-level classiﬁer whereas latter pertains strongly image exhibits attribute probabilistic attributes). bounds tolerable error mid-level classiﬁers given work propose solution mitigate inﬂuence uncertainty. two-stage attribute-based formulation common alternative zero-shot strategy exploit external knowledge class relationships adapt classiﬁers unseen class. example unseen object’s classiﬁer estimated combining nearest existing classiﬁers imagenet hierarchy combining classiﬁers based label co-occurrences similar spirit label embeddings feature embeddings exploit semantic information zero-shot predictions. unlike models focus deﬁning categories language-based description advantage giving human supervisor direct control unseen class’s deﬁnition even attribute signature unlike observed existing trained model. zero-shot models generalize few-shot case small number labels available show proposed random forest model learn simultaneously signatures labeled images enabling few-shot learning unreliable attribute predictions. acknowledging attribute classiﬁers often unreliable recent work abandons purely semantic attributes favor discovering mid-level features detectable discriminative class labels however guarantee discovered features align semantic properties particularly nameable ones. typically makes inapplicable zero-shot learning since human supervisor longer deﬁne unseen class concise semantic terms. nonetheless attempt assign semantics post-hoc demonstrate method beneﬁt zero-shot learning discovered attributes well. idea handling unreliable attributes random forests related fractional tuples handling missing values decision trees approach points missing values distributed tree proportion observed values data. similar concepts explored handle features represented discrete distributions propagate instances soft node memberships. approach also entails propagating training instances proportion uncertainty. however zero-shot scenario distinct accordingly training testing domains differ important ways. training time rather build decision tree labeled data points construct tree using unseen classes’ attribute signatures. then test time inputs attribute classiﬁer predictions. furthermore show propagate signatures data points tree simultaneously makes possible account inter-dependencies among input dimensions also enables few-shot extension. given vocabulary visual attributes unseen class described terms attribute signature m-dimensional vector gives association attribute class typically association values would binary—meaning attribute always present/absent class—but also real-valued ﬁne-grained data available. model unseen class single signature however straightforward handle case class multi-modal deﬁnition learning zero-shot model mode. whether attribute vocabulary hand-designed discovered approach assumes expressive enough discriminate categories. suppose unseen classes interest training images. zero-shot method takes input attribute signatures dataset images labeled attributes produces classiﬁer unseen class output. test time goal predict unseen class appears novel image. following ﬁrst describe initial stage building attribute classiﬁers introduce zero-shot random forest trained attribute signatures next explain augment training procedure account attribute unreliability signature uncertainty finally present extension few-shot learning attribute-based zero-shot method ﬁrst must train classiﬁers predict presence absence attributes novel images. importantly images used train attribute classiﬁers come variety objects/scenes need contain instances unseen categories. fact attributes shared across category boundaries precisely allows zero-shot learning. denote attribute training comprised images. descriptor image binary mdimensional label vector specifying attributes present image i.e. indicates attribute present learn mapping descriptors attribute presence scores train support vector machines attribute. denote probabilistic output m-th computed platt scaling. addition random forest training disjoint validation training consisting attribute-labeled images gauge error tendencies attribute classiﬁer. entails evaluating data’s receiver operating characteristic values given operating point example false positive rate attribute determined count instances zero-shot random forests next introduce contribution random forest model zero-shot learning. basic formulation signature random forest first deﬁne basic random forest training algorithm zero-shot setting. main idea train ensemble decision trees using attribute signatures—not image descriptors vectors attribute predictions. zero-shot setting training information available. later test time image hand apply trained random forest estimate class posteriors. recall k-th unseen class deﬁned attribute signature treat signature lone positive exemplar class discriminatively train random forests distinguish signatures take one-versus-all approach training forest unseen class. training class class signatures negatives. class build ensemble decision trees breadth-ﬁrst manner. tree learned recursively splitting signatures subsets node starting root. denote indicator vector length records signatures appear node root node signatures present following typical random forest protocol training instances recursively split according randomized test; compares dimension signature threshold propagates left child right child depending outcome yielding indicator vectors speciﬁcally il|. thus training must choose things node query attribute threshold represented jointly split sample limited number combinations choose maximizes expected information gain igbasic entropy general distribution -norm indicator vectors sums occurrences signature binary. since training zero-shot forest discriminate class rest distribution class labels node length- vector image xtest compute predicted attribute signature given novel applying attribute svms. then predict posterior class traverse leaf node tree forest. denote fraction positive training instances leaf node tree forest class somehow perfect attribute classiﬁers basic zero-shot random forest would sufﬁcient. next show adapt training procedure deﬁned account unreliability. accounting attribute prediction unreliability training exemplars true attribute signatures unseen class test images approximate estimates attributes contain. therefore augment zero-shot random forest account unreliability training. main idea generalize recursive splitting procedure given signature pursue multiple paths tree. critically paths determined false positive/true positive rates individual attribute predictors. expand idealized training signature distribution predicted attribute space. essentially preemptively builds appropriate cushion expected errors choosing discriminative splits. implementing idea requires primary extensions formulation sec. inject validation data associated receiver operating characteristics tree formation process redeﬁne information gain account partial propagation training signatures. explain components turn next. addition signatures node maintain subset validation data data recursively propagated tree following splits chosen. denote validation data inherited node root records fracnode tional occurrence training signature class node root split node signature splits left right child nodes according receiver operating characteristic attribute operating point speciﬁed particular have m-th attribute’s label image true positive false negative rates threshold respectively; false positive true negative rates. illustrate equation means consider class elephant known attribute gray. gray attribute classiﬁer chosen threshold ﬁres gray samples validation i.e. tpr=. fraction elephant signature passed positive node. process repeats levels fractions single elephant signature reached leaf nodes. thus single class signature emulates estimated statistics full training class-labeled instances attribute predictions. stress things validation data propagation. first data labeled attributes only; unseen class labels never features information gain computation. role estimate values. second recursive sub-selection validation data important capture dependency tpr/fpr’s higher level splits. example select split root fractional signatures pushed left child must meaning candidate split left child correct accounted compute would used thus formulation properly accounts dependencies attributes selecting discriminative thresholds issue addressed existing methods missing uncertain features building zero-shot tree conscious attribute unreliability choose split maximizing expected information gain according fractionally propagated signatures distribution discriminative splits criterion distinguish unseen classes also persevere strong signal spite attribute classiﬁers’ error tendencies. means trees prefer reliable attributes discriminative among classes well less reliable attributes coupled intelligently selected operating points remain distinctive. furthermore omit splits that though highly discriminative terms idealized signatures found unlearnable among validation data. example extreme case attribute classiﬁer cannot distinguish positives negatives meaning tpr=fpr signatures classes equally likely propagate left right i.e. yields information gain eqn. thus method explicitly making best imperfect attribute classiﬁcation inherently prefers learnable attributes. proposed approach produces classiﬁers unseen categories zero category-labeled images. attribute-labeled validation data plays important role solution’s robustness. data perfectly represent true attribute errors images unseen classes training procedure would equivalent building random forest test samples’ attribute classiﬁer outputs. accounting class signature uncertainty beyond attribute classiﬁer unreliability framework also deal another source zeroshot uncertainty instances class often deviate class-level attribute signatures. tackle this redeﬁne soft indicators eqn. appending term account annotation noise equivalently implementation perturbed copies class exemplar attribute signature forest training data. please details. extending few-shot random forests approach admits natural extension few-shot training. case given attribute signatures also dataset consisting small number images class labels. essentially signatures prior selecting good tree splits also satisfy traditional training examples. information gain signatures deﬁned sec. information gain images deﬁned sec. latter reﬂects fact training images like evaluation data actual attribute classiﬁer outputs thus require uncertainty model propagation. using notation shortcuts few-shot training recursively select split maximizes combined information gain controls role signature-based prior. intuitively expect lower values sufﬁce size increases since training examples precisely learn class’s appearance. few-shot extension interpreted learn random forests descriptive priors. datasets setup three datasets animals attributes apascal/ayahoo objects scene attributes datasets capture wide array categories attributes attribute-labeled images originate seen classes dataset respectively; class labels solely attribute annotations. unseen class splits speciﬁed randomly select unseen classes three features provided datasets include color histograms sift phog others details). following train attribute svms combined χ-kernels kernel feature channel method reserves attribute-labeled images validation data pools remaining train ﬁnal attribute classiﬁers. method baselines access exactly amount attribute-labeled data. report random forest results mean standard error measured random trials. build random forests trees. based cross-validation tree depths generate tests node apy- sun-). validation data points reach node revert computing statistics full validation rather baselines addition several state-of-the-art published results ablated variants method also compare baselines signature random forests trained classattribute signatures described sec. without attribute unreliability model direct attribute prediction leading attribute-based zero-shot object recognition method widely used literature controlled noise experiments approach designed overcome unreliability attribute classiﬁers. glean insight works ﬁrst test controlled noise test images’ attribute predictions. start hypothetical perfect attribute classiﬁer scores class progressively noise represent increasing errors predictions. examine scenarios attribute classiﬁers equally noisy average noise level varies attribute. details noise model. figure shows results using awa. deﬁnition methods perfectly accurate zero noise. attributes unreliable however approach consistently better. furthermore gains notably larger second scenario noise levels vary attribute illustrating approach properly favors learnable attributes discussed sec. contrast signature-rf liable break even minor imperfections attribute prediction. results afﬁrm method beneﬁts estimating accounting classiﬁer noisiness avoiding uninformative attribute classiﬁers. real unreliable attributes experiments next present zero-shot results method applied three challenging datasets using real attribute classiﬁers. table shows results. method signiﬁcantly outperforms existing method important result today commonly used model zero-shot object recognition whether using exact formulation similar non-probabilistic variants furthermore demonstrates modeling conﬁdence attribute’s presence test image inadequate; idea characterize error tendencies training valuable. substantial improvements signature-rf also conﬁrm imperative model attribute classiﬁer unreliability. gains especially large fewer positive training samples attribute leading less reliable attribute classiﬁers—exactly method needed most. repeat experiment reducing randomly chosen images attribute training gain widens points table also helps isolate impact components method model signature uncertainty recursive propagation validation data latter compute tpr/fprs globally full validation dataset rather node-speciﬁc subsets aspects contribute full method’s best performance finally ours+true provides upper bound accuracy achievable method datasets; result attainable unseen class images validation data also points interesting direction future work better model expected error rates images unseen attribute combinations. initial attempts regard included focusing validation data seen class images signatures like unseen classes impact negligible. figure compares method published results using named discovered attributes. using standard named attributes method comfortably outperforms prior methods. further discovered attributes perform comparably attribute decoding method achieving state-of-the-art well-studied zero-shot benchmark. result obtained using simple generalization method handle continuous attribute strength signatures quantizing dimension bins. finally demonstrate few-shot extension. figure shows results function amount labeled training images prior-weighting parameter rely solely training images rely solely attribute signatures i.e. zero-shot learning. baseline compare method uses solely training images learn unseen classes clear advantage attribute signature prior few-shot random forest training. furthermore that expected optimal shifts towards samples added; even training images prior plays role star curve indicates value method selects automatically cross-validation. introduced zero-shot training approach models unreliable attributes—both classiﬁer predictions uncertainty association unseen classes. results three challenging datasets indicate method’s promise suggest elegance zero-shot learning need abandoned spite fact visual attributes remain difﬁcult predict reliably. future work plan develop extensions accommodate inter-attribute correlations random forest tests multi-label random forests improve scalability many unseen classes. section contains details supplementary material nips paper zero-shot recognition unreliable attributes omitted paper meet length constraints. shows unlearnable attributes avoided method. discusses details signature uncertainty model introduced shows few-shot results continuation above. gives additional details controlled noise experiments lists database test classes chosen random. sanity check show accounting classiﬁer unreliability detailed also inherently avoids unlearnable attributes. extreme case completely unlearnable attributes classiﬁer cannot tell positives negatives tpr=fpr candidate split tested node involves attribute signatures classes equally likely propagate left right i.e. plugging eqn. means plugging eqn. summarize method deal uncertainty class-attribute signatures. achieved appropriately modifying soft indicator vectors implementation terms amounts adding perturbed copies exemplar signature training set. describe former detail show equivalent latter. probabilities simply respectively validation data subset node respectively. account class signature uncertainty expand probabilities terms tpr/fnr term reﬂecting signature uncertainty. speciﬁcally denote true attribute signature instance opposed annotated signature then second term captures non-trivial dependency true attribute value annotation. changes computation probabilities exactly model effect training data expansion adding inﬁnite number perturbed variants attribute signatures perturbed ak). expanding probabilities thus implicitly assumed following structure dependencies among implementing attribute uncertainty model observed generally common instances labeled positive given attribute actually negative reverse uncommon. understandable class-level associations e.g. images class person have hands number reasons class person have hands class-level attribute signature. reason restricted ﬂipping positive bits attribute signatures. based cross-validation ﬂipped fractions bits respectively. alone zero-shot recognition beneﬁt modeling uncertainty attribute annotations. believe little scope in-class variation attribute signatures among scenes since attributes four types functional affordances materials surface properties spatial envelope types attributes closely related scene labels themselves unlikely missing class instances reasons occlusion since usually localized speciﬁc parts instance images e.g. images belonging mountain category nearly always marked affordance climbing unlikely climbing affordance would taken away mountain pictured. contrast person images without hands discussed above simply occlusions. shows few-shot results shots similar paper. interestingly zero-shot learning approach beats -shot attribute prediction based learning overall trends remain similar discussed paper. synthetic attribute classiﬁer scores used constructed corrupting hypothetical perfect attribute classiﬁer’s scores progressively increasing noise. speciﬁcally noise setting synthetic attribute classiﬁer scores class noise level decrease scores positive samples increase scores negative samples adding noise follows drawn exponential distribution mean η−e−n/η. keeps scores scenarios shown paper following. scenario classiﬁers corrupted noise drawn exponential distribution mean scenario draw mean noise attribute classiﬁer exponential distribution whose mean test classes picked random were inn/indoorﬂea market/indoor classroom outhouse/outdoor chemical plant mineshaft lake/natural shoe shop school archive. acknowledgements thank christoph lampert felix graciously sharing code data comparison reuse.", "year": 2014}