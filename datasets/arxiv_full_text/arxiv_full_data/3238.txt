{"title": "Accuracy of MAP segmentation with hidden Potts and Markov mesh prior  models via Path Constrained Viterbi Training, Iterated Conditional Modes and  Graph Cut based algorithms", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In this paper, we study statistical classification accuracy of two different Markov field environments for pixelwise image segmentation, considering the labels of the image as hidden states and solving the estimation of such labels as a solution of the MAP equation. The emission distribution is assumed the same in all models, and the difference lays in the Markovian prior hypothesis made over the labeling random field. The a priori labeling knowledge will be modeled with a) a second order anisotropic Markov Mesh and b) a classical isotropic Potts model. Under such models, we will consider three different segmentation procedures, 2D Path Constrained Viterbi training for the Hidden Markov Mesh, a Graph Cut based segmentation for the first order isotropic Potts model, and ICM (Iterated Conditional Modes) for the second order isotropic Potts model.  We provide a unified view of all three methods, and investigate goodness of fit for classification, studying the influence of parameter estimation, computational gain, and extent of automation in the statistical measures Overall Accuracy, Relative Improvement and Kappa coefficient, allowing robust and accurate statistical analysis on synthetic and real-life experimental data coming from the field of Dental Diagnostic Radiography. All algorithms, using the learned parameters, generate good segmentations with little interaction when the images have a clear multimodal histogram. Suboptimal learning proves to be frail in the case of non-distinctive modes, which limits the complexity of usable models, and hence the achievable error rate as well.  All Matlab code written is provided in a toolbox available for download from our website, following the Reproducible Research Paradigm.", "text": "pixelwise image segmentation using markovian prior models depends several hypothesis determine number parameters general complexity estimation prediction algorithms. markovian neighborhood hypothesis order isotropy conspicuous properties set. paper study statistical classiﬁcation accuracy different markov ﬁeld environments pixelwise image segmentation considering labels image hidden states solving estimation labels solution equation observed image model. emission distribution assumed models difference lays markovian prior hypothesis made labeling random ﬁeld. priori labeling knowledge modeled second order anisotropic markov mesh classical isotropic potts model. models consider three different segmentation procedures path constrained viterbi training hidden markov mesh graph based segmentation ﬁrst order isotropic potts model second order isotropic potts model. provide uniﬁed view three methods investigate goodness classiﬁcation studying inﬂuence parameter estimation computational gain extent automation statistical measures overall accuracy relative improvement kappa coefﬁcient allowing robust accurate statistical analysis synthetic real-life experimental data coming ﬁeld dental diagnostic radiography. algorithms using learned parameters generate good segmentations little interaction images clear multimodal histogram. suboptimal learning proves frail case non-distinctive modes limits complexity usable models hence achievable error rate well. keywords hidden markov models hidden potts models gaussian mixtures image segmentation relative improvement conﬁdence intervals kappa coefﬁcient agreement bayesian approaches image segmentation classiﬁcation even compression devised since early work causal markov mesh abend harley kanal sixties hidden markov random ﬁeld models pioneered besag seventies. basically consists embedding problem probabilistic framework modeling pixel random variable given likelihood embedding knowledge hidden labels prior distribution. issue using markovian models segmentation applications parameter estimation usually computationally expensive task. practice often trade accepted accuracy estimation running time estimation algorithm chen interesting review ﬁeld. common approach consists alternatively restoring unknown segmentation based maximum posteriori rule estimating model parameters using observations restored data. random markov fields available iterative approximated solutions based gibbs distribution prior. case instance popular algorithm besaj makes pseudo-likelihood approximation. complete monte carlo markov chain methods potts model segmentation estimation implemented pereyra gimenez pseudolikelihood estimator smoothness parameter second order potts model proposed included fast version besaj’s iterated conditioned modes segmentation algorithm. comparisons classical smoothness estimators like introduced frery also provided. levada also discussed map-mrf approximations combining suboptimal solutions based several different order isotropic potts priors. direct competitors methods discussed celeux based mean ﬁeld like approximations expectation-maximization algorithm potts models ﬁrst order neighborhoods consider conditional probabilities rather restorations hidden data. algorithm widely used context incomplete data particular estimate independent mixture models simplicity. binary segmentation problems background-object problems addressed successfully graph theoretical methods. greig showed graph cuts used binary image restoration solutions map-mrf problem. boykov proposed framework used graph cuts globally optimal object extraction method n-dimensional images. blake used mixture markov gibss random ﬁeld approximate regional properties segments spatial interaction segments. standard graph algorithm exact optimal solution certain class energy functionals however many cases number labels assigning graph nodes twoand minimization energy functions becomes np-hard. approximate optimization boykov developed α-expansion-move swap move algorithms deal multi-labeling problems general energy functionals. date survey graph theoretical approaches image segmentation given peng introduced ﬁrst analytic solution true anisotropic hidden markov model studied strictly-causal nearest-neighbor show exact decoding possible. block-based causal modeling assumed training testing images shared underlying markovian model transition probabilities. estimated interblock transition probabilities pathconstrained viterbi algorithms training step. pyun introduced method blockwise image segmentation combining mode approximation method besaj simplest noncausal markovian model bond-percolation model multistate extension assume homogeneity spatial coherence among images estimated hidden parameters testing step using stochastic procedure computation time comparable using causal hmms blockwise segmentation aerial images. proposed pseudo noncausal splitting noncausal model multiple causal hmms solved distributed computing framework pixelwise segmentations. decoding approximations also made testing step discussed perronin sargin references therein. said before issue using markovian prior models segmentation applications complexity parameter estimation. methods considering isotropic neighborhoods smooth important quasi linear structures images methods consider non-isotropic neighborhoods like causal dhmm non-isotropic potts involve parameters estimate. markovian hypothesis assumed labeling random markov field different order develop different estimation strategies. keep multivariate gaussian model emission distributions. ﬁrst neighborhood system ensure assumptions second order causal markov mesh model introduced extra assumption neighborhood probabilities related notion pixel’s past. second neighborhood system introduce gibbs distribution form ﬁrst second order isotropic potts model smoothness parameter models consider three different segmentation procedures path constrained viterbi testing hidden markov mesh simple proposal graph based segmentation ﬁrst order isotropic potts model iterated conditional modes second order isotropic potts model. methods propose uniﬁed framework consisting three stages. ﬁrst stage image modeling initial labeling. second stage general parameter estimation third stage computing approximated solution. previous work causal dhmm blockwise pixelwise segmentation assume training testing images share underlying markovian model transition probabilities estimate probabilities training step simply test images. however homogeneity assumption arguable many images. paper assume spatial coherence vary image image thus estimate parameters transitions probabilities testing step. also discuss speciﬁc choices constraining paths possibilities viterbi decoding produces computation time comparable using sections discuss detail equations markov mesh prior model potts prior model approximation choices made implementations pcvt icm. goal paper give uniﬁed view approximations theoretically algorithmically compare classiﬁcation rates synthetic real data available ground truth. section introduce design simulated experiments statistics used evaluate classiﬁcation. implementation issues prospects given section matlab code written provided toolbox available download website following reproducible research paradigm. many effective computational tools practical problems image processing computer vision devised markov random fields modeling. practical problems label image domain pixelwise given discrete labels help priori modeling hypothesis like potts model. corresponds special case markov random field image graph computes graph partition minimal total perimeter one-label regions. favor particular order labels comparison another similar model introduced ishikawa partitions image graph multiple linearly ordered labels mrf-based energy function proposed potts model unary potential deﬁned graph node pairwise potential given graph edge. markov random ﬁelds provide convenient prior modeling spatial interactions pixels. markov random fields ﬁrst introduced vision geman geman pixels image size labels site deﬁned called neighborhood neighborhood system collection sets {∂ij satisﬁes labeling problem assign label label site sites thus labeling mapping denote labeling {sij}. possible labeling denoted paper consider ﬁnal segmentation {sij} realizations markov random ﬁeld. means possible realization holds general labeling ﬁeld directly observable experiment. estimate realized conﬁguration based observation related means likelihood function represents model’s parameters. popular estimate maximize posteriori estimation. map-mrf framework popularized vision geman geman estimation consists maximizing posterior probability point view bayes estimation estimate minimizes risk zero-one cost function. using bayes rule estimate sake completeness consider multispectral images pixel vector assume observed pixel intensities multivariate gaussian mixtures emission probabilities given state multivariate gaussian mrfs generalizations markov processes speciﬁed either joint distribution local conditional distributions. however local conditional distributions subject nontrivial consistency constraints ﬁrst approach commonly used. paper consider types markov random fields prior constraints labeling ﬁeld causal markov meshes gibbs random ﬁelds. deﬁning gibbs random ﬁelds need deﬁne clique. sites called clique member neighbor members. gibbs random ﬁeld speciﬁed joint gibbs distribution cliques normalizing constant real functions called clique potential functions. model conditional distribution state label corresponding pixel given evidence image observed process supposed emitted hidden markov field considered multivariate gaussian mean covariance matrix depends classes. thus given observed pixel intensities posteriori distribution classes involves observed class initial given maximum likelihood. using calculate uij) equal ﬁrst second order neighborhood depicted figure assumption reduces equation smaller nonlinear equation coefﬁcients counting number patches certain conﬁgurations; details. equation solved paper using brent’s algorithm iterated conditional modes iterative algorithm rapidly converges local maximum function closest initial segmentation provided user. usually initial segmentation provided maximum likelihood. iteration modiﬁes label pixel label probable given neighborhood conﬁguration. given observations ﬁnds suboptimal solution algorithm ﬁrst term equivalent ones used classiﬁer. second term contextual component scaled parameter smooths initial segmentationif reduces clusters coherence. rule reduced maximum likelihood effect reversed data importance ﬁnal segmentation. second term called interaction potential smoothness function density given ﬁrst order potts model. smoothness term incorporates notion piecewise smooth world penalizes assignments label neighboring nodes differently. potential functions ones given general energy functions minimized graph algorithm image considered weighted graph vertices’s pixels edges links neighboring pixels. binary graph problem additional nodes known source sink terminals added graph. terminals correspond labels assigned nodes i.e. pixels image. edges separates source sink terminals subsets edges separate terminals. weights edges capacity cut. implementation used weights potential functions graph implementations weights deﬁned submodular functions. goal minimum i.e. edge weights minimum. figure representative diagram showing process partitioning input image. multiple label case multiway leave pixel connected label. ensures every multi-way separates terminals must correspond valid labeling conﬁguration associated mrf. solve problem used α−expansion move algorithm algorithm minimizes energy function binary variables repeatedly minimizing energy function binary variables using max-ﬂow/min-cut method. starts arbitrary labeling performs iterative optimization cycles process converges. cycle consists iterating labels running α−expansion move every label involves ﬁnding labeling obtained increasing number labels pixel deﬁne following causal relationship represents past {sij− si−j} neighborhood past. assume causal order markov mesh model stating that state transition matrix dimension transition matrix markov process past states left actual pixel. yields order image. instead lining pixels would done one-dimensional case moving top-left pixel bottom-right pixel. thus initial probabilities depend ﬁrst state write word hidden usually added whole model comes fact markov mesh observed considered hidden. proved causal relationship implies general markov field hypothesis diagonal neighborhood stated introduction probability label given whole labeling speciﬁcation depends values pixels depicted figure means diagonal operates isolating element neighboring diagonals suggest extension viterbi algorithm compute probable sequence states given initial values. optimal combination states solves given whole hidden markov model labeling ﬁeld observed gaussian intensity process. path image every diagonal marks step. diagonal consists states tz+w− makes total lmin possible state combinations considering main diagonal. therefore exact decoding problem np-hard problem. produce approximated solution work constraining possible state combinations. viterbi training algorithm iterative algorithm estimates parameters ﬁnds sequence states best explains data given estimated parameters. procedure starts setting initial parameters done using prior information educated guess non-contextual estimation. using initial step algorithm follows next steps convergence initialize segmentation maximum likelihood segmentation parameter estimation given sequence estimation decoding choosing best paths viterbi decoding using paths. let’s suppose initial sequence obtained algorithm sequence obtained viterbi previous step. empirical estimations transition probabilities distributions parameters several different approximations literature iterative decoding. sargin proposed algorithm iteratively updates posterior distribution rows columns i.e. determining horizontal vertical forward-backward probabilities combining approximate values product horizontal vertical probabilities. simplistic approach represent dependency neighbors horizontal vertical conditionals column wise constrained application belief propagation. models deviate original makovian assumptions paper follow called path constrained viterbi training algorithm restricts possibilities diagonal strings states propose labeling updates parameters pseudo-expectation maximization using labeling convergence. describe equations involved process. figure viterbi decoding possible states viterbi transformation. path constrained tracking back optimal path state sequences using values saved hidden state map. paper propose following selection. ﬁrstly assume evaluate likelihood given diagonal state sequence simply multiplying likelihoods pixel without considering statistical dependencies pixels i.e. compute thus likely state sequence entry likely state pixel’s observation diagonal particular implementation obtain next sequences considering sequences result changing state chains ordered using largest likelihood chosen. discussion section comment incidence selection sequences convergence implementation. viterbi decoding chosen paths. figure show schematic example decoding algorithm follows idea original viterbi algorithm using diagonals pivotal points. call diagonal state sequence index diagonal bsdk emission probability sequence diagonal assumption pixel statistically independent neighbors. finally calculate transition probability sequence diagonal sequence diagonal section report experiments three algorithms described article path contrained viterbi training iterated conditional modes graph comparison purposes also provide results applying supervised unsupervised maximum likelihood classiﬁcation algorithm supervised case initialized real means variances known simulation parameters taken case real images. unsupervised experiment selects initial parameters modes histogram case unimodal distributions starting random means. algorithm among tested ones take account spatial information. algorithms initialized outputs convergence. pcva code made carry experiments designed scratch matlab platform. used matlab statistical toolbox scripts literature initialization also made non-parametric segmentation algorithms like k-means means variances gaussian hypothesis observations estimated labeled output. studied examples observe signiﬁcant differences using k-means initialization method. implemented version parameter estimated iteration maximizing current pseudo-likelihood brent algorithm. visiting scheme consists dividing image support windows updating labels cycle together figure visiting scheme minimizes convergence delays introduced oscillations sites. graph algorithm solution found minimum transportation network using boykov kolmogorov available code also benchmark code widely used segmentation multimodal images classes parameter estimated initially maximizing current pseudo-likelihood brent algorithm particularized order neighborhood system. goals paper quantify gain classiﬁcation accuracy produced imposing markovian hypothesis labeling ﬁeld. performance evaluated following statistical measures overall accuracy kappa relative improvement. kappa statistic deﬁned computes proportion coincidences reference reconstructed images classiﬁcation scheme considered good overall accuracy appropriate also report relative improvement index related benchmark em-ml classiﬁcation deﬁned investigated several examples. ﬁrst compared performance algorithms terms quality segmentations simulated real data classes later consider complex scenarios studying scanned intra oral x-ray images hand made ground truth digitalized x-ray images level noise introduced scanner main characteristic smoothness joint gray level histogram. classes quite distinguishable naked form distinctive mode joint histogram making segmentation difﬁcult. image subtraction image enhancement ﬁltering common image processing research areas working digitalized digital x-ray imagery. caution advised dentists abuse enhancement algorithms digital x-ray devises often introduce artifacts images defect excess leading possible misdiagnosis. image data analyzed paper kindly provided odontologist j.g. flesia image processing section dentistry department university c´ordoba also helped judge discrimination capabilities algorithms. ﬁrst image scanned bitewing x-ray image shows typical implant consisting titanium screw background bone tissue. bitewing x-rays used detect decay teeth changes bone density caused disease. also useful determining proper crown marginal integrity ﬁllings. particularity image figure bimodal histogram figure segmentations observed image applying different algorithms given figure case interesting illustrates algorithms obtain good unsupervised segmentations real images image bimodal histogram without need pre-ﬁltering enhancement. pcvt slightly better delineates right side implant looks fused bone. figure inverse digitalized dental x-ray image shows titanium implant screw surrounded bone. original image histogram em-ml segmentation segmentation segmentation pcvt segmentation. discuss experiments made synthetic color image test procedures ability segment noisy images unimodal bimodal mixture distributions. unimodal distributions simulated ﬁlling class gaussian data high variances; modes start emerge variances reduced. examples segmentations obtained shown figure corresponding parameters performance statistics given tables indication algorithms ability restore truth addition visual assessment. problems difﬁcult algorithms using spatial information clearly outperform independent mixture model terms restoration cases supervised unsupervised. moreover hidden potts algorithms produce notably lower error rates pcvt. conﬁdence intervals kappa values mostly revel segmentations statistically different good choice initial means supervised unimodal problems essential guarantee convergence pcvt case also illustrates that images generated severely overlapping mixtures considered pcvt performance decreases noise increases. occurs lesser extent since particular situation classiﬁcations turns advantage. also provide results problems noisy image smoothed ﬁltering allowing modes emerge. case performance boosted algorithms surpassing accuracy kappa value figure third row. bidimensional hidden markov models quite interesting models prior labeling ﬁeld gaussian mixture observations. nevertheless complexity ﬁnal equations calls reductions approximations figure first second noisy synthetic image histogram segmentations. third unimodal synthetic image ﬁltered histogram unsupervised segmentations. frail ﬁnal estimations. section want discuss inﬂuence selection best sequences decoding execution time overall performance. previous work pcvt explicitly discuss points give particulars level complexity added algorithm decision select possible sequences inﬂuence segmentation output decision. besides reduced number computations estimating parameters transition probabilities training stage using information testing images. personal implementation figure noisy logo em-ml segmentation em-icm segmentation em-gc segmentation em-pcvt segmentation conﬁdence intervals kappa statistic relative improvement methods related maximum likelihood classiﬁcation relative improvement pcvt related number sequences retained. allows user number sequences involved decoding preset value. worked small supervised study -color logo technological university degraded gaussian noise. used gaussian densities class-dependent means true noise parameters made experiments setting number path sequences allowed decoding study shows allowing probable sequences relative improvement working probable time execution goes minutes minutes intel processor memory laptop. also show table number iterations stabilizes sequences allowed. means algorithm uses resources search better minimum found sequences. second scanned bitewing x-ray image shows central molar tooth partial views neighbor teeth gums background figure three classes image account differences tooth enamel dentin. enamel thin hard material covers dentin main body teeth protects harsh temperatures. initialized algorithms automatic supervised classiﬁcation. figure observe segmentations that supervised case teeth correctly segmented dentine clearly differentiated tooth nerve enamel. initializing automatic figure gives binary division background teeth gum. example pcvt makes slightly better since background better separated teeth. joint histogram image figure show least three distinctive modes; figure histogram shown panel corresponding x-ray image incisive ﬂat. corresponding segmentations shown panels made four classes distinguish well tooth enamel dentin background. expert’s opinion pcvt method separates correctly enamel background dentin. section work imagery obtained image processing section dentistry department university c´ordoba. samples part response bone tissue study pups mothers stressed continuous light conditions histological studies conducted samples conﬁrm bone loss. since studies destroy samples based conventional dental x-ray images carefully taken digitalized ﬂatbed scanner document study. image used segmented odontologist j.g. flesia different classes. pups three days image’s gray scale histogram shows main classes background ﬁltering. older figure segmentation inverse digitalized dental x-ray image central molar tooth partial views neighbor teeth gums background. first algorithms initialized modes histogram ﬁxed variance second automatic gaussian mixtures. rows left right pcvt. rats bone loss documented comparing clear region shown experiment figure region control group. days rats early bone inserted soft part undeveloped difﬁcult segment. advice experts considered following experiments data experiment classes complete sample background. experiment classes teeth jawbone rest image. experiment three classes hard parts soft parts background. experiment four classes hard parts ﬂesh cartilage background. table report kappa overall accuracy four experiments. conﬁdence intervals give different conclusions them. experiment graph highest kappa value statistically different others. experiment results statistically different highest kappa value followed graph cut. experiment pcvt highest kappa value followed graph statistically different. pcvt best performance reduces background noise much better methods maintaining accuracy. striking feature detected also two-circles synthetic image. pcvt background perfectly recovered errors introduced object. solution problem could consider subclasses inside main regions. segmentation problems contained problem thus pcvt rates compared merging confusion matrix problem needed. conclude pcvt works much better number classes closer reality. instead starts convergence problems performance better classes involved. method inﬂuenced number classes considered. paper deals markov random ﬁeldmodel based image segmentation. discussed different markovian prior models three noticeable estimation algorithms path constrained viterbi training iterated conditional modes graph cut. presentation uniﬁed framework advantage give better insight respective theoretical properties. graph originally designed class images extended later multimodal images. images distinctive modes algorithm tendency converge reduced classes. robust aspect since never fails deliver segmentation usually better smoothness parameter estimation point implementations. underestimation produces dirty segmentation images known contain number homogeneous patches initialization high re-estimating iteration convergence. pcvt algorithm gave segmentation considered smoothed version capability moving space possible segmentations away saddle point lays. knowledge authors comparison ever made hidden potts models hidden complexity algorithms quite different. pott’s graph parameter estimate pcvt transitions probabilities estimate besides gaussian parameters. nevertheless execution time order algorithms pcvt constrained probable sequences diagonals. allowing sequences gives degrees freedom choosing best labeling; however values probabilities sequences studies working sequences increases complexity without allowing realistic combinations possible sequences. comparing contextual segmentations synthetic real images outputs em-ml algorithms independent mixture models three algorithms show signiﬁcant improvement terms segmentation smoothness. conﬁrms gain dealing spatial dependencies. graph segmentation algorithm completely written authors. provide matlab comprehensive toolbox pcvt algorithms graph wrapper calling boykov kolmogorov code tested several real synthetic images author’s knowledge pcvt implementation also distributable code available hmm. continue enhancing toolbox since support idea general benchmark framework markovian segmentation methods.", "year": 2013}