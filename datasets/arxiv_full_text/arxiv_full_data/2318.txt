{"title": "Setting up a Reinforcement Learning Task with a Real-World Robot", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots. In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard. Our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls. We show that highly reliable and repeatable experiments can be performed in our setup, indicating the possibility of reinforcement learning research extensively based on real-world robots.", "text": "choices crucial effective reproducible learning performance. unfortunately research works realworld robots typically describe many details alone study effects controlled manner although notable exceptions exist work address question real-world robotic task off-the-shelf implementation standard method perform effectively reliably. address developing reacher task robotic agent learns reach arbitrary target positions low-level actuations robotic using trial error. task easy solve simulation difﬁcult real-world robots learning method rllab implementation trpo popular learning method robust performance respect hyper-parameters. reacher describe steps elements setting real-world tasks including medium data transmission concurrency ordering delays computations low-level actuation types frequencies operating them. exploring different variations elements study individual contributions difﬁculty robot learning. variability time delays choosing action representation nontrivially processed actuation highly detrimental learning. accounting effects show possible achieve effective performance realworld robots also repeatability learning scratch highly reliable manner even repeats hours different times using different physical robots. abstract— reinforcement learning promising approach developing hard-to-engineer adaptive solutions complex diverse robotic tasks. however learning real-world robots often unreliable difﬁcult resulted adoption reinforcement learning research. difﬁculty worsened lack guidelines setting learning tasks robots. work develop learning task robotic bring light elements task setup study contributions challenges robots. learning performance highly sensitive setup thus oversights omissions setup details make effective learning reproducibility fair comparison hard. study suggests mitigating steps help future experimenters avoid difﬁculties pitfalls. show highly reliable repeatable experiments performed setup indicating possibility reinforcement learning research extensively based real-world robots. despite recent successes real-world robots under-utilized quest general reinforcement learning agents time primarily conﬁned simulation. underutilization largely frustrations around unreliable poor learning performance robots. although several methods recently shown highly effective simulations often yield poor performance applied off-the-shelf real-world tasks. ineffectiveness sometimes attributed integral aspects real world including slow rate data collection partial observability noisy sensors safety frailty physical devices. barrier contributed reliance indirect approaches simulation-to-reality transfer collective learning sometimes compensate failures learn single stream real experience. oft-ignored shortcoming real-world research lack benchmark learning tasks standards setting experiments robots. experiments simulated robots typically done benchmark tasks easily available simulators standardized interfaces relieving experimenters many task-setup details action space action cycle time system delays. hand setting learning task real-world robots obvious. experimenter work establishing device-speciﬁc sensorimotor interface learning agent robot well determining aspects environment deﬁne learning task. reinforcement learning task formulation reinforcement learning task composed agent environment interacting modeled formally markov decision process agent interacts environment discrete time steps step agent receives environment’s state information scalar reward signal agent uses stochastic policy governed probability distribution def== pr{at a|st select action environment transitions state produces reward next time step using transition probability distribution def== pr{st+ r|st agent’s performance typically evaluated terms def== future accumulated rewards known return γk−trk+ discount factor. goal agent typically policy maximizes expected return. policies often learned estimating action values q-learning directly parameterizing policy optimizing policy parameters trpo. practice agent receive environment’s full state information rather observes partially real-valued observation vector framework task described primarily using three elements observation space action space reward function. section design reacher task robot call reacher. design similar openai-gym reacher agent learns reach arbitrary target positions direct torque control simulated two-joint robotic arm. parameterizing policy nonlinearly neural network policy-search method trpo solve reacher reasonably well thousand time steps. designing task based reacher allows reasonable expectation learning time utilize choices already made isolate challenges emerge design decisions hardware interface. following describe interface robot details reacher task. lightweight ﬂexible industrial robot joints manufactured universal robots. lowlevel robot controller called urcontrol programmed communicating tcp/ip connection. robot controlled script-level using programming language called urscript. establishing connection send urscript programs computer urcontrol strings socket. urscript programs urcontrol real-time streams status packets every packet urcontrol contains sensorimotor information robot including angular positions velocities target accelerations currents joints. robot controlled urscript sending low-level actuation commands clock. urscript servoj command offers position control interface speedj offers velocity control interface. unlike reacher torque control interface. reacher actuate second third joints base. also extend task joints actuated called reacher observation vector includes joint angles joint velocities vector difference target ﬁngertip coordinates. unlike reacher include sines cosines joint angles targetposition coordinates simplify reduce observation space without losing essential information. include previous action part observation vector helpful learning systems delays reacher reward function deﬁned euclidean distance target ﬁngertip positions l-norm penalize large torques. reward function simplify dropping penalty term. reacher consists episodes interactions episode seconds long allow adequate exploration. ﬁngertip conﬁned within dimensional boundary reacher within -dimensional boundary reacher episode target position chosen randomly within boundary starts middle boundary. addition constraining ﬁngertip within boundary robot also constrained within joint-angular boundary avoid self-collision. several crucial aspects real-world task rarely studied simulations action cycle time medium connection choice actuation type concurrency delays computation. aspects main focus current work. simulated tasks natural perform agent environment-related computations synchronously desirable real-world tasks. figure shows computational steps executed sequentially typical simulated experiment episode. ﬁrst four computational steps environment related whereas last agent related. simulated world advances discretely time step change rest. simulated tasks comply framework time advance observing acting. overall latency ampliﬁed misplaced synchronization ordering computations result difﬁcult learning problem reduced potential responsive control. therefore design objective setting learning task manage minimize delays. different approaches proposed alleviate issue augmenting state space actions predicting future state action execution. approaches minimize delay compensate perspective learning agents. seldom discussed aspect issue different orderings concurrencies task computations different overall latencies. reacher implemented computational steps python distributed asynchronous processes robot communication process reinforcement learning process. exchange sensorimotor information actuation commands. figure depicts computational model reacher also serve computational model real-world tasks. computational steps step numbers directly relevant. robot communication process device driver collects sensorimotor data urcontrol sensor thread cycle time sends actuation commands separate actuator thread. process contains environment thread checks spatial boundaries computes observation vector reward function based sensorimotor packets updates actuation command actuator thread based actions fast loop. agent thread process deﬁnes task time steps determines action cycle time. makes learning updates computes actions using agent’s policy pass. learning agent rllab implementation trpo. performs computationally expensive learning updates infrequently every episodes. scheduled updates episodes ensure interfere normal course agent’s sensorimotor experience. thus learning updates trpo occur agent thread every action cycle time step. computational model real-world tasks figure suggests concurrency certain ordering computations avoid unnecessary system delays. example splitting robot communication process threads allows asynchronous communication physical devices. splitting process threads allows checking safety constraints faster concurrently action updates. moreover suggest making learning updates updating action unlike step simulated tasks computed opposite order. helps dispatch actions soon computed instead waiting learning updates increase observation-to-action delays. computational model also extends robotic tasks comprising multiple devices robot communication process device allows agent access sensorimotor information fast. natural consider pairing mobile robot limited onboard computing power computationally powerful base station wi-fi bluetooth rather ethernet. wi-fi commonly introduces variability inter-arrival time streamed packets. reacher robot communication process communicates urcontrol tcp/ip connection. ethernet connection baseline setup communicating ethernet allows tighter control system latency. however also test effect using wi-fi connection. figure shows variability packet inter-arrival times wired wireless connections measured using packets. packets sent every urcontrol. inter-arrival time consistently around wired connection times wireless connection much variability complete range varying different robotic devices operate different ways. robots allow external computers write directly control table. robot controller controls actuators based control table wait instructions external computer. robots provide interface controller controls actuators based actuation commands repeatedly sent external computer. refer transmission commands external computer robot robot actuations. reacher choose robot-actuation cycle time default action cycle time also known time-step duration time subsequent action updates agent’s policy. choosing cycle time particular task obvious literature lacks guidelines investigations task-setup element. shorter cycle times include superior policies ﬁner control. however changes subsequent observation vectors short cycle times perceptible agent result learning problem hard impossible existing learning methods. long cycle times limit possible policies precision control also make learning problem easier. cycle time prolonged much also start impede learning rate slowing data-collection rate. concurrent computational model possible choose action cycle times different robotactuation cycle time. action cycle time longer actuator thread repeats sending command robot command computed based action. fortuitously beneﬁt agents long action cycle times. example reach target quickly gain momentum repeating similar robot actuations many times. agents short cycle time must learn gain momentum agents long cycle time free design. choose action cycle time baseline setup compare effects shorter longer cycle times. choosing action space difﬁcult real-world tasks physical-robot controllers usually designed learning low-level control mind. control variable strong cause-and-effect relationship robot’s state immediate future appropriate choice actions. torques accelerations often chosen fig. compare actions based velocity position controls showing cross-correlations three motor signals target torque target acceleration measured current actions simulated robot tasks. interested using low-level actuation command controlling make task similar reacher. allows position velocity controls sending commands every able velocity control directly actions using position control directly feasible. randomly-generated initial policies direct position control generated sequences angular position commands caused violent abrupt movements emergency stops. choose direct velocity control baseline setup compare smoothed form position control. avoid abrupt robot movements direct velocity control needed restrict angular speeds rad/s leading-axis acceleration maximum rad/s. position control needed apply smoothing twice actions become proxy second derivative desired positions; applying smoothing could avoid abrupt movements. smoothing technique described follows agent time step action vector measured joint positions qdes desired joint position sent position-control command ﬁrst derivative variable. clipb operator clips value action cycle time ymax −ymin according angle safety boundary. choose default value gain positioncontrol command according urscript api. urcontrol modulates position velocity commands moving robot. figure shows cross-correlation action types three different motor signals based data collected random agent action cycle time. motor signals target acceleration target torque measured current closely related subsequent motor events signals actions highest correlations motor signals packets target acceleration torque three packets measured current. observation driven choice baseline cycle learning performance different reacher setups. baseline setup allowed highly repeatable experiments effective learning fig. performance independent runs seeds provided similar learning curves. using wireless connection applying different sources artiﬁcial delays detrimental learning different degrees. using short long action cycle times resulted worse learning performance compared baseline setup velocity control outperformed smoothed position control signiﬁcantly different action cycle times. time longer velocity control higher correlation motor signals position control except target torque. correlations velocity control concentrated single time shift whereas correlations position control linger multiple consecutive time shifts indicating indirect relationship. make variations baseline task setup investigate impact different elements. variation task setups used trpo hyper-parameters discount factor batch-size episodes step size best overall performance different robotic tasks. policy deﬁned normal distribution mean standard deviation represented neural networks. policy critic networks hidden layers nodes each. experiment independent trials observe average returns time. neural networks notorious dependence performance initial weights. recently henderson reminded easily wrong conclusions could drawn experiments deep reinforcement learning methods applied carefully. exempliﬁed showing algorithm appear achieve signiﬁcantly different performance experiment repeated using different sets randomization seeds. therefore different methods setups seem signiﬁcantly different simply random chance ensuing different pseudorandom number sequences them. took extra caution setting experiments ensure task-setup variation initial networks sequences target positions used. validate correctness experimental setup repeated baseline experiment four times shown figure different seeds. trial consists time steps minutes agent-experience time three hours total real-time including resets. time learning curve improved signiﬁcantly agent achieved higher average returns resulting effective consistent reaching behavior shown companion video. notably learning curves quite similar seed even though generated running trial multiple hours different days physical units. testament precision stability trpo reliability experimental task setups. figure shows impact using wireless connection. solid lines average returns shaded regions standard errors. wireless connection resulted signiﬁcant deterioration performance compared baseline setup wired connection ascribed variabilities delays arrival sensorimotor packets computer actuation commands urcontrol. study impacts injected artiﬁcial exponential random delays crudely modeled wi-fi transmission delays separately action updates sending actuation commands urcontrol. actionupdate delays effect similar observation delays also caused inefﬁcient implementations. delays make learning problem difﬁcult adding uncertainty actions affect subsequent observations whereas delaying robot actuations additionally affect robot’s operation. figure shows random action-update delay mean caused signiﬁcant deterioration learning performance. hand adding small random robot-actuation delay mean devastated learning completely. figure show impact choosing different action cycle times. performance deteriorated cycle time decreased hand performance improved increased deteriorated signiﬁcantly increased figure show learning performance direct velocity smoothed position controls different action cycle times smoothed position control performed signiﬁcantly worse velocity control cases. finally investigated whether baseline setup remained effective six-joint control applying reacher accommodate higher complexity problem explored policy critic networks larger thank richard sutton lavi shpigelman joseph modayil cindy thoughtful suggestions feedback helped improve manuscript. also thank william gautham vasan recording editing companion video. brockman cheung pettersson schneider schulman tang zaremba openai gym. arxiv preprint arxiv.. degris pilarski sutton model-free reinforcement learning continuous action practice. american control conference duan chen houthooft schulman abbeel benchmarking deep reinforcement learning continuous control. international conference machine learning holly lillicrap levine deep reinforcement learning robotic manipulation asynchronous off-policy updates. ieee international conference robotics automation pp–. henderson islam bachman pineau precup meger deep reinforcement learning matters. arxiv preprint arxiv.. hester stone texplore real-time sampleefﬁcient reinforcement learning robots. machine learning katsikopoulos engelbrecht markov decision processeswith delays asynchronous cost collection. ieee transactions automatic control levine finn darrell abbeel end-toend training deep visuomotor policies. journal machine learning research rusu ve˘cer´ık roth¨orl heess pascanu hadsell sim-to-real robot learning pixels progressive nets. conference robot learning schuitema wisse ramakers jonker design bipedal walking robot online autonomous reinforcement learning. ieee/rsj international conference intelligent robots systems schulman levine abbeel jordan moritz trust region policy optimization. international conference machine learning pp–. sutton barto reinforcement learning introduction press. walsh nouri littman learning planning environments delayed feedback. autonomous agents multi-agent systems yahya kalakrishnan chebotar levine collective robot reinforcement learning distributed asynchronous guided policy search. ieee/rsj international conference intelligent robots systems work designed developed learning task step-by-step robot discuss elements real-world task setups. discussion summarized following hypotheses real-world robotic learning tasks used guideline baseline setup system delays occurring different computational stages generally detrimental learning. consequently wired communications preferable wireless ones. studied validity ﬁrst three hypotheses creating variations baseline setup study largely supported them. demonstrated learning performance could highly sensitive setup elements speciﬁcally system delays choice action spaces. performance less sensitive different action cycle times comparison. results suggest mitigating delays source likely beneﬁcial indicating prospect last hypothesis. study comprises small step toward comprehensive understanding real-world learning tasks requires thorough investigations validations using different tasks learning methods sets hyperparameters. baseline setup allowed conduct highly reliable repeatable real-world experiments using offthe-shelf method. served strong testament viability extensive experimentations research real-world robots despite barriers frustrations around robots reproducibility deep research general.", "year": 2018}