{"title": "Adversarial Ladder Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "The use of unsupervised data in addition to supervised data in training discriminative neural networks has improved the performance of this clas- sification scheme. However, the best results were achieved with a training process that is divided in two parts: first an unsupervised pre-training step is done for initializing the weights of the network and after these weights are refined with the use of supervised data. On the other hand adversarial noise has improved the results of clas- sical supervised learning. Recently, a new neural network topology called Ladder Network, where the key idea is based in some properties of hierar- chichal latent variable models, has been proposed as a technique to train a neural network using supervised and unsupervised data at the same time with what is called semi-supervised learning. This technique has reached state of the art classification. In this work we add adversarial noise to the ladder network and get state of the art classification, with several important conclusions on how adversarial noise can help in addition with new possible lines of investi- gation. We also propose an alternative to add adversarial noise to unsu- pervised data.", "text": "unsupervised data addition supervised data lead signiﬁcant improvement training discriminative neural networks. however best results achieved training process divided parts ﬁrst unsupervised pre-training step done initializing weights network weights reﬁned supervised data. recently neural network topology called ladder network idea based properties hierarchichal latent variable models proposed technique train neural network using supervised unsupervised data time called semi-supervised learning. technique reached state classiﬁcation. hand adversarial noise improved results classical supervised learning. work adversarial noise ladder network state classiﬁcation several important conclusions adversarial noise help addition possible lines investigation. also propose alternative adversarial noise unsupervised data. used initialize weights networks using supervised data reﬁne parameters adjust task hand. pre-trained neural network already learnt important features represent underlying distribution data subsets. deep belief networks based training pairs restricted boltzmann machines kind probabilistic energy-based model perform ﬁnne-tunning parameters. deep boltzmann machines hidden layer create deep topology perform tunning. hand autoencoders neural networks output target input. autoencoder divided parts encoder decoder. encoder takes input start reducing dimensionality hidden layer neural network represents dimension. decoder topology encoder starts last layer encoder perform operations output closed possible input. autoencoder trained achieve property minimizing squared error between input reconstruction. take pretrained encoder perform ﬁnne-tunning parameters using supervised data. means using supervised unsupervised data learning procedure. idea semi supervised learning unsupervised learning features correlates well already found features suitable task. suitability driven supervised learning procedure. mixing learning schemes stalling learning procedure fact targets learning schemes diﬀerent. side unsupervised learning tries encode necessary information reconstructing input. supervised learning focused ﬁnding abstract invariant features discriminate diﬀerent inputs. unsupervised features relative position size face description maybe necessary discriminative task discriminative features tipicaly information data structure unseful represent able discard information necessary reconstruction encode information layer. example suppose supervised learning ﬁnds useful characteristic layer represent particular layer. level unsupervised learning needs kind representation keep reconstruction error information unuseful supervised learning performance. unsupervised learning could able representing information another level could still keep reconstruction error supervised learning could information needed level. latent variable models models learning unconditional probability distributions particularity given latent variable reconstruct observed variable means likelihood probability distribution. means proceedure depends also random procedure someway model adds bits information reconstruction. formally discret distribution mean likelihood distribution given projection observed space deviation given noise process. learn parameters using main bottleneck latent variable models implies computing structure models well semi supervised learning paradigm allows discarding information fact information somehow added reconstruction. however hidden layer latent variable model unable discard information needs represent necessary information represent hidden layer. solution hierarchical latent variable models information somehow distributed diﬀerent variables variable able adding bits information. ideas models modelling observed variable probability distribution implies hidden variable somehow information making hierarchy allows information discarding. ladder network neural network topology implements idea latent variable models make suitable mixing supervised unsupervised learning time. topology network given ﬁgure topology allows information discarding level long needed. reconstruction depends level also encoder part. means extra added information reconstruction loose information level encoded levels. note topology lower level inﬂuence signal higher level matter encoder decoder path. means features neural network learns somehow distributed network long supervised learning reﬁnes kind feature need level. special attention added noise encoder. noise serves purposes. ﬁrst implementing denoising principle denoising autoencoder serves good regularizator. encoded signal decoder. special thing remark ladder network uses batch normalization purposes. first avoid internal covariate shift avoid encoder output constant values easiest ones denoise perturbation lies quantiﬁcation interval. model correctly classify sample. discover neural networks robust adversarial examples examples nearly similar highly increase misclassiﬁcation error. ﬁrst thing ladder network robust adversarial examples. reason trained model error test set. corrupt test adversarial noise random noise ensuring power noises same. ﬁgure shows sample mnist test corrupted types noise. cannot sample suppose higher missclassiﬁcation error. diﬀerences test error. conclude ladder network robust adversarial examples. clear adversarial noise focus meausuring robust discriminative network reason computing adversarial examples using derivative supervised cost. adversarial noise applied purerly supervised learning. clear ﬁrst steps optimization process would true means adversarial noise push towards direction cost increase. means adversarial noise someway resemble gaussian noise problem adding also adding gaussian noise. long network well trained adversarial noise push samples towards decision bound. ﬁgure figure ﬁgure represents supervised data sets drawn gaussian distributions decision bound. applied adversarial noise data computing light light blue datasets. data sets pushed towards decision error thus towards other. case present supervised data motivate idea unsupervised data. samples correctly classiﬁed pushed towards decision threshold. train adding unsupervised noise force network learn denoise samples sensible directions. means extracting features represent bounding someway want network represent distribution want features useful supervised data. perform preliminar experiments mnist. dataset evaluated fully connected network labels convolutional network labelled data. details networks topologies found hyperparameters searched task. unsupervised data check evaluate sign function derivative think phase information derivative vector important learning unsupervised case restrict norm vector control power noise. could made norm noise equal computed sign function setting dimensionality input image. supervised data tried ways computing adversarial noise. remark decisions made fully connected full labelled mnist task evaluated tasks. next tables show results. also perform experiment mnist label convolutional show adding adversarial noise unsupervised data useful. results average diﬀerent runs. improve results ladder network adding adversarial noise. future work focus four diﬀerent lines. first removing gaussian noise adding adversarial noise computed unsupervised samples exposed. second adding supervised adversarial noise layer topology. third extending validation wider experimentation tasks. finally depending results diﬀerent explored approaches.", "year": 2016}