{"title": "Contextual Bandits with Stochastic Experts", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "We consider the problem of contextual bandits with stochastic experts, which is a variation of the traditional stochastic contextual bandit with experts problem. In our problem setting, we assume access to a class of stochastic experts, where each expert is a conditional distribution over the arms given a context. We propose upper-confidence bound (UCB) algorithms for this problem, which employ two different importance sampling based estimators for the mean reward for each expert. Both these estimators leverage information leakage among the experts, thus using samples collected under all the experts to estimate the mean reward of any given expert. This leads to instance dependent regret bounds of $\\mathcal{O}\\left(\\lambda(\\pmb{\\mu})\\mathcal{M}\\log T/\\Delta \\right)$, where $\\lambda(\\pmb{\\mu})$ is a term that depends on the mean rewards of the experts, $\\Delta$ is the smallest gap between the mean reward of the optimal expert and the rest, and $\\mathcal{M}$ quantifies the information leakage among the experts. We show that under some assumptions $\\lambda(\\pmb{\\mu})$ is typically $\\mathcal{O}(\\log N)$. We implement our algorithm with stochastic experts generated from cost-sensitive classification oracles and show superior empirical performance on real-world datasets, when compared to other state of the art contextual bandit algorithms.", "text": "consider problem contextual bandits stochastic experts variation traditional stochastic contextual bandit experts problem. problem setting assume access class stochastic experts expert conditional distribution arms given context. propose upper-conﬁdence bound algorithms problem employ diﬀerent importance sampling based estimators mean reward expert. estimators leverage information leakage among experts thus using samples collected experts estimate mean reward given expert. leads instance dependent regret bounds term depends mean rewards experts smallest mean reward optimal expert rest quantiﬁes information leakage among experts. show assumptions typically implement algorithm stochastic experts generated cost-sensitive classiﬁcation oracles show superior empirical performance real-world datasets compared state contextual bandit algorithms. modern machine learning applications like recommendation engines computational advertising testing medicine inherently online. settings task take sequential decisions proﬁtable also enable system learn better future. instance computational advertising system task sequentially place advertisements users’ webpages dual objective learning preferences users increasing click-through rate attribute systems well-known exploration exploitation trade-oﬀ. principled method capture trade-oﬀ study multi-armed bandit problems k-armed stochastic bandit problems studied several decades. formulated sequential process time step k-arms selected. upon selection k-th returns stochastic reward expected reward starting work major focus regret diﬀerence total reward accumulated genie optimal policy chosen online policy. current state-of-art algorithms achieve regret order-wise optimal here corresponds expected reward best next best one. denoting context expectation taken unknown distribution best expert naturally deﬁned expert highest mean reward. expected diﬀerence rewards genie policy always chooses best expert online algorithm employed policy-designer deﬁned regret. problem well-studied literature popular approach reduce contextual bandit problem supervised learning techniques argmin-oracles practice class experts generated online training cost-sensitive classiﬁcation oracles trained resulting classiﬁers/oracles provide reliable conﬁdence scores given context especially well-calibrated conﬁdence scores eﬀectively k-dimensional probability vector entry probability classiﬁer/oracle choosing best given context. motivated observation propose variation traditional experts setting term contextual bandits stochastic experts. assume access class stochastic experts deterministic. instead expert conditional probability distribution arms given context. expert conditional distribution denoted random variable denoting chosen context. additional beneﬁt setting allows derive regret bounds terms closeness soft experts quantiﬁed divergence measures rather terms total number arms before task compete expert class highest mean reward. expected reward stochastic expert deﬁned mean reward observed drawn conditional distribution propose upper-conﬁdence style algorithms contextual bandits stochastic experts problem employ importance sampling based estimators mean rewards various experts. prove instance-dependent regret guarantees algorithms. main contributions paper listed next section. components approach importance sampling based estimators mean rewards experts. estimators based observation samples collected expert reweighted likelihood/importance ratios averaged provide estimate mean reward another expert. sharing information termed information leakage utilized various settings ﬁrst estimator adaptive variant well-known clipping technique proposed estimator presented however carefully adapt clipping threshold online manner order achieve regret guarantees. also propose importance sampling variant classical median means estimator estimator also designed utilize samples collected experts together estimate mean reward given expert. deﬁne estimator best knowledge importance sampling used conjunction median means technique literature before. provide novel conﬁdence guarantees estimator depends chi-square divergences conditional distributions various experts. independent interest. propose contextual bandits stochastic experts problem. design based algorithms problem based importance sampling based estimators mentioned above. show utilizing information leakage experts leads regret guarantees scale sub-linearly number experts. information leakage experts ﬁrst estimator governed pairwise log-divergence measure second estimator chi-square divergences characterize leakage. mean rewards optimal expert second best. parameter depends gaps mean rewards optimum experts various sub-optimal ones. normalized diﬀerence squares gaps adjacent sub-optimal experts ordered gaps. assumption suboptimal gaps uniformly distributed bounded interval show parameter expectation. deﬁne parameter explicitly section information leakage. existing instance-independent bounds contextual bandits scale problem dependent bounds near optimal dependence depend numbers arms. however depends divergence measure associated information leakage problem besides analysis empirically show divergence based approach rivals performs better eﬃcient heuristics contextual bandits real-world data sets. empirically validate algorithm three real world data-sets state contextual bandit algorithms implemented vowpal wabbit implementation online training cost-sensitive classiﬁcation oracles generate class stochastic experts. show algorithms better regret performance data-sets compared algorithms. contextual bandits studied literature several decades starting simple setting discrete contexts linear contextual bandits ﬁnally general experts setting work focus experts setting. contextual bandits experts ﬁrst studied paper interested stochastic version problem context rewards arms generated unknown ﬁxed distribution. ﬁrst strategies explored setting explore-then-commit epsilon-greedy style strategies achieve polylog) instance-independent regret scaling. notable among algorithms contextual bandit problem supervised learning assume access cost-sensitive classiﬁcation oracles. algorithms heavily optimized vowpal wabbit study contextual bandits stochastic experts problem experts deterministic functions mapping contexts arms conditional distributions arms given context. importance sampling based estimators leverage information leakage among stochastic experts. adaptive clipped importance sampling estimator mean rewards experts introduced estimator studied best-arm/pure explore setting study cumulative regret problem need adjust parameters estimator online manner. addition introduce importance sampling based median means style estimator paper leverage information leakage among experts. discrete time-steps time-horizon interest. time nature draws vector unknown ﬁxed probability distribution. here reward context vector revealed policy-designer whose task choose possibilities. reward chosen revealed policy-designer. place notational convenience. figure bayesian network denoting joint distribution random variables given time-step contextual bandit setting. denotes context denotes chosen denotes reward chosen also depends context observed. distribution reward given chosen context marginal context remain ﬁxed time slots. however conditional distribution chosen given context dependent stochastic expert time-step. stochastic experts consider class stochastic experts {π··· conditional probability distribution random variable denoting chosen context. shorthand denote conditional distribution follows context observed. policy-designer chooses stochastic expert drawn probability distribution policy-designer. stochastic reward revealed. joint distribution random variables denoting context chosen reward observed respectively time modeled bayesian network shown fig. joint distribution class experts. deﬁne pπkp distribution corresponding random variables expert chosen expected reward expert denoted denotes expectation respect distribution best expert given maxk∈ objective minimize regret till time deﬁned section deﬁne -divergence metrics important analyzing estimators. similar divergence metrics deﬁned analyze clipped estimator context best identiﬁcation problem. addition divergence metric also deﬁne chi-square divergence metric useful analyzing median means based estimator first deﬁne conditional -divergence. conditional distributions note deﬁnition marginal distribution marginal given nature’s inherent distribution contexts. work concerned speciﬁc -divergence metrics deﬁned follows. section propose general upper-conﬁdence bound style strategy utilizes structure problem converge best expert much faster naive strategy treats expert bandit problem. observations framework rewards collected expert give valuable information mean another expert owing bayesian network factorization joint distribution propose estimators mean rewards diﬀerent experts leverage information leakage among experts importance sampling. estimators deﬁned section propose meta-algorithm designed estimators corresponding conﬁdence intervals control regret. here denotes estimate mean reward expert time denotes upper conﬁdence bound corresponding estimator time propose estimators utilize samples observed various experts provide estimate mean reward expert section deﬁne estimators estimating mean rewards given expert. estimators eﬀectively leverage information leakage samples collected various experts importance sampling. observation enables following equation clipped estimator estimator introduced context pure exploration problem. here analyze estimator cumulative regret setting parameters estimator need adjusted diﬀerently. denote number times expert invoked algorithm intuition clipped estimator weighted average samples collected diﬀerent experts sample scaled importance ratio suggested also clip importance ratios larger clipper level. clipping introduces bias decreases variance. clipper level carefully chosen trade-oﬀ bias variance. clipper level values weights dependent divergence terms mkj’s. divergence large means samples expert valuable estimating mean expert therefore weight /mkj applied. similarly clipper level log)mkj restrict aditive bias lemma implied theorem include proof section appendix. lemma shows clipped estimator pool samples experts order estimate mean expert variance estimator depends depends log-divergences number times expert invoked. median means estimator introduce second estimator based wellknown median means technique estimation. median means estimators popular statistical estimation underlying distributions heavy-tailed estimator mean expert time obtained following steps divide total samples log) groups fraction samples expert preserved. choose ni/l samples expert group. calculate empirical mean expert intuition mean every group weighted average samples expert rescaled importance ratios. similar clipped estimator however importance ratios clipped particular level. estimator bias-variance trade-oﬀ controlled taking median means groups. number groups needs carefully in-order control bias-variance trade-oﬀ. section provide instance dependent regret guarantees algorithm estimators proposed clipped estimator median means estimator mink=k∗ expected reward optimum expert second best. deﬁne parameter later section depends gaps expected rewards various experts optimal one. algorithm uses clipped estimator regret scales /∆). similarly case median means estimator regret scales /∆). maximum log-divergence maximum chi-square divergence experts respectively. gaps optimum expert sub-optimal ones distributed uniformly random show parameter expectation. contrast experts used separate arms naive application ucb- bounds would yield regret scaling ease exposition results re-index experts using indices regret guarantees clipped estimator provided assumption assume log-divergence terms bounded maxij mij. position present main theorems provides regret guarantees algorithm using estimator theorem suppose assumption holds. regret algorithm time using estimator bounded follows figure plots progressive validation loss till time plotted function time performance algorithms drug consumption dataset performance algorithms stream analytics dataset performance algorithms letters dataset legend. remark note guarantees term containing number arms. dependence implicitly captured divergence terms among experts. fact number arms large expect divergence based algorithms perform comparatively better algorithms whose guarantees explicitly depend phenomenon observed practice empirical validation real world data-sets section also show empirically term grows slowly number experts real-world data-sets. empirical result included appendix section empirically test algorithms three real-world multi-class classiﬁcation datasets state algorithms contextual bandits experts. multi-class classiﬁcation dataset converted contextual bandit scenario features contexts. time-step feature sample point revealed following contextual bandit algorithm chooses classes reward observed correct class otherwise bandit feedback correct class never revealed chosen. method widely used benchmark contextual bandit algorithms fact implemented vowpal wabbit algorithm batches. starting batch experts trained prior data cost-sensitive classiﬁcation oracles also update divergence terms experts estimated data observed far. batch algorithm deployed current experts. pseudo-code procedure provided algorithm xgboost logistic regression scikit-learn calibration base classiﬁers cost-sensitive oracles. bootstrapping used generate diﬀerent experts. starting batch experts added. constants practice. settings held ﬁxed three data-sets without parameter tuning. provide details appendix appendix also show dependent term theoretical bounds grows much slower compared ucb- bounds number experts increase stream analytics dataset implementation algorithm found compare vopal wabbit implementations following algorithms \u0001-greedy parameter ’–epsilon online cover parameter ’–cover bagging parameter ’–bag drug consumption data dataset part repository. data respondents dimensional continuous features history drug use. drugs study entry bandit algorithm selects drug recently used reward observed o.w. reward observed. performance algorithms shown fig. d-ucb median moments clearly performs best terms average loss followed d-ucb clipped estimator. d-ucb median moments converges average loss within samples. stream analytics data dataset collected using stream analytics client. samples dimensional mixed features classes entry bandit algorithm selects correct class reward observed o.w. reward observed. performance algorithms shown fig. data-set bagging performs best closely followed versions d-ucb bagging strong competitor empirically however algorithm lacks theoretical guarantees. bagging converges average loss d-ucb median moments converges average loss letters data dataset part repository. samples hand-written english letters hand-crafted visual features classes corresponding letters. entry bandit algorithm selects correct letter reward observed o.w. reward observed. performance algorithms shown fig. versions d-ucb signiﬁcantly outperform others. median moments based version converges average loss clipped version converges average loss study problem contextual bandits stochastic experts. propose style algorithms diﬀerent importance sampling estimators leverage information leakage stochastic experts. provide instance-dependent regret guarantees based algorithms. algorithms show strong empirical performance real-world datasets. believe paper introduces interesting problem setting studying contextual bandits opens opportunities future research include better regret bounds problem instance-dependent lower-bound.", "year": 2018}