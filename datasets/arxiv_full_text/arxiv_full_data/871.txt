{"title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.", "text": "introduce convolutional neural network operates directly graphs. networks allow end-to-end learning prediction pipelines whose inputs graphs arbitrary size shape. architecture present generalizes standard molecular feature extraction methods based circular ﬁngerprints. show data-driven features interpretable better predictive performance variety tasks. recent work materials design used neural networks predict properties novel molecules generalizing examples. difﬁculty task input predictor molecule arbitrary size shape. currently machine learning pipelines handle inputs ﬁxed size. current state off-the-shelf ﬁngerprint software compute ﬁxed-dimensional feature vectors features inputs fully-connected deep neural network standard machine learning method. formula followed training molecular ﬁngerprint vectors treated ﬁxed. paper replace bottom layer stack function computes molecular ﬁngerprint vectors differentiable neural network whose input graph representing original molecule. graph vertices represent individual atoms edges represent bonds. lower layers network convolutional sense local ﬁlter applied atom neighborhood. several layers global pooling step combines features atoms molecule. predictive performance. using data adapting task hand machine-optimized ﬁngerprints provide substantially better predictive performance ﬁxed ﬁngerprints. show neural graph ﬁngerprints match beat predictive performance standard ﬁngerprints solubility drug efﬁcacy organic photovoltaic efﬁciency datasets. parsimony. fixed ﬁngerprints must extremely large encode possible substructures without overlap. example used ﬁngerprint vector size removed rarely-occurring features. differentiable ﬁngerprints optimized encode relevant features reducing downstream computation regularization requirements. interpretability. standard ﬁngerprints encode possible fragment completely distinctly notion similarity fragments. contrast feature neural graph ﬁngerprint activated similar distinct molecular fragments making feature representation meaningful. figure left visual representation computational graph standard circular ﬁngerprints neural graph ﬁngerprints. first graph constructed matching topology molecule ﬁngerprinted nodes represent atoms edges represent bonds. layer information ﬂows neighbors graph. finally node graph turns ﬁxed-length ﬁngerprint vector. right detailed sketch including bond information used operation. circular ﬁngerprints state molecular ﬁngerprints extended-connectivity circular ﬁngerprints circular ﬁngerprints reﬁnement morgan algorithm designed encode substructures present molecule invariant atom-relabeling. circular ﬁngerprints generate layer’s features applying ﬁxed hash function concatenated features neighborhood previous layer. results hashes treated integer indices written ﬁngerprint vector index given feature vector node graph. figure shows sketch computational architecture. ignoring collisions index ﬁngerprint denotes presence particular substructure. size substructures represented index depends depth network. thus number layers referred ‘radius’ ﬁngerprints. creating differentiable ﬁngerprint space possible network architectures large. spirit starting known-good conﬁguration designed differentiable generalization circular ﬁngerprints. section describes replacement discrete operation circular ﬁngerprints differentiable analog. hashing purpose hash functions applied layer circular ﬁngerprints combine information atom neighboring substructures. ensures change fragment matter small lead different ﬁngerprint index activated. replace hash operation single layer neural network. using smooth function allows activations similar local molecular structure varies unimportant ways. indexing circular ﬁngerprints indexing operation combine nodes’ feature vectors single ﬁngerprint whole molecule. node sets single ﬁngerprint index determined hash feature vector. pooling-like operation converts arbitrary-sized graph ﬁxed-sized vector. small molecules large ﬁngerprint length ﬁngerprints always sparse. softmax operation differentiable analog indexing. essence atom asked classify belonging single category. classiﬁcation label vectors produces ﬁnal ﬁngerprint. operation analogous pooling operation standard convolutional neural networks. figure pseudocode circular ﬁngerprints neural graph ﬁngerprints differences highlighted blue. every non-differentiable operation replaced differentiable analog. canonicalization circular ﬁngerprints identical regardless ordering atoms neighborhood. invariance achieved sorting neighboring atoms according features bond features. experimented sorting scheme also applying local feature transform possible permutations local neighborhood. alternative canonicalization apply permutation-invariant function summation. interests simplicity scalability chose summation. circular ﬁngerprints interpreted special case neural graph ﬁngerprints large random weights. because limit large input weights tanh nonlinearities approach step functions concatenated form simple hash function. also limit large input weights softmax operator approaches one-hot-coded argmax operator analogous indexing operation. algorithms summarize algorithms highlight differences. given ﬁngerprint length features layer parameters neural graph ﬁngerprints consist separate output weight matrix size layer well hidden-to-hidden weight matrices size layer possible number bonds atom experiments demonstrate neural ﬁngerprints large random weights behave similarly circular ﬁngerprints. first examined whether distances circular ﬁngerprints similar distances neural ﬁngerprint-based distances. figure shows scatterplot pairwise distances circular neural ﬁngerprints. fingerprints length calculated pairs molecules solubility dataset distance measured using continuous generalization tanimoto similarity measure given second examined predictive performance neural ﬁngerprints large random weights circular ﬁngerprints. figure shows average predictive performance solubility dataset using linear regression ﬁngerprints. performances methods follow similar curves. contrast performance neural ﬁngerprints small random weights follows different curve substantially better. suggests even random weights relatively smooth activation neural ﬁngerprints helps generalization performance. figure left comparison pairwise distances molecules measured using circular ﬁngerprints neural graph ﬁngerprints large random weights. right predictive performance circular ﬁngerprints neural graph ﬁngerprints ﬁxed large random weights neural graph ﬁngerprints ﬁxed small random weights performance neural graph ﬁngerprints large random weights closely matches performance circular ﬁngerprints. demonstrate neural graph ﬁngerprints interpretable show substructures activate individual features ﬁngerprint vector. feature circular ﬁngerprint vector activated single fragment single radius except accidental collisions. contrast neural graph ﬁngerprint features activated variations structure making interpretable allowing shorter feature vectors. solubility features figure shows fragments maximally activate predictive features ﬁngerprint. ﬁngerprint network trained inputs linear model predicting solubility measured feature shown positive predictive relationship solubility activated fragments containing hydrophilic r-oh group standard indicator solubility. feature shown bottom strongly predictive insolubility activated non-polar repeated ring structures. figure examining ﬁngerprints optimized predicting solubility. shown representative examples molecular fragments activate different features ﬁngerprint. feature predictive solubility. bottom feature predictive insolubility. toxicity features trained model architecture predict toxicity measured different datasets figure shows fragments maximally activate feature predictive toxicity separate datasets. figure visualizing ﬁngerprints optimized predicting toxicity. shown representative samples molecular fragments activate feature predictive toxicity. predictive feature identiﬁes groups containing sulphur atom attached aromatic ring. bottom predictive feature identiﬁes fused aromatic rings also known polycyclic aromatic hydrocarbons well-known carcinogen. constructed similar visualizations semi-manual determine toxic fragments activated given neuron searched hand-made list toxic substructures chose correlated given neuron. contrast visualizations generated automatically without need restrict range possible answers beforehand. several experiments compare predictive performance neural graph ﬁngerprints standard state-of-the-art setup circular ﬁngerprints fully-connected neural network. experimental setup pipeline takes input smiles string encoding molecule converted graph using rdkit also used rdkit produce extended circular ﬁngerprints used baseline. hydrogen atoms treated implicitly. convolutional networks initial atom bond features chosen similar used ecfp initial atom features concatenated one-hot encoding atom’s element degree number attached hydrogen atoms implicit valence aromaticity indicator. bond features concatenation whether bond type single double triple aromatic whether bond conjugated whether bond part ring. training architecture training used batch normalization also experimented tanh relu activation functions neural ﬁngerprint network layers fullyconnected network layers. relu slight consistent performance advantage validation set. also experimented dropconnect variant dropout weights randomly zero instead hidden units found worse validation error general. experiment optimized minibatches size using adam algorithm variant rmsprop includes momentum. hyperparameter optimization optimize hyperparameters used random search. hyperparameters methods optimized using trials cross-validation fold. following hyperparameters optimized learning rate initial weight scale penalty ﬁngerprint length ﬁngerprint depth size hidden layer fully-connected network. additionally size hidden feature vector convolutional neural ﬁngerprint networks optimized. solubility aqueous solubility molecules measured drug efﬁcacy half-maximal effective concentration vitro molecules sulﬁde-resistant strain falciparum parasite causes malaria measured organic photovoltaic efﬁciency harvard clean energy project uses expensive simulations estimate photovoltaic efﬁciency organic molecules. used subset molecules dataset. predictive accuracy compared performance circular ﬁngerprints neural graph ﬁngerprints conditions ﬁrst condition predictions made linear layer using ﬁngerprints input. second condition predictions made one-hidden-layer neural network using ﬁngerprints input. settings differentiable parameters composed models optimized simultaneously. results summarized table experiments neural graph ﬁngerprints matched beat accuracy circular ﬁngerprints methods neural network ﬁngerprints typically outperformed linear layers. software automatic differentiation software packages theano signiﬁcantly speed development time providing gradients automatically handle limited control structures indexing. since required relatively complex control indexing order implement variants algorithm used ﬂexible automatic differentiation package python called autograd package handles standard numpy code differentiate code containing loops branches indexing. computational cost neural ﬁngerprints asymptotic complexity number atoms depth network circular ﬁngerprints additional terms matrix multiplies necessary transform feature vector step. precise computing neural ﬁngerprint depth ﬁngerprint length molecule atoms using molecular convolutional features layer costs practice training neural networks circular ﬁngerprints usually took several minutes training ﬁngerprints network took order hour larger datasets. limited computation layer complicated make function goes layer network next? paper chose simplest feasible architecture single layer neural network. however fruitful apply multiple layers nonlinearities message-passing step make information preservation easier adapting long short-term memory architecture pass information upwards. limited information propagation across graph local message-passing architecture developed paper scales well size graph ability propagate information across graph limited depth network. appropriate small graphs representing small organic molecules used paper. however worst case take depth network distinguish graphs size avoid problem proposed hierarchical clustering graph substructures. tree-structured network could examine structure entire graph using layers would require learning parse molecules. techniques natural language processing might fruitfully adapted domain. inability distinguish stereoisomers special bookkeeping required distinguish stereoisomers including enantomers cis/trans isomers circular ﬁngerprint implementations option make distinctions. neural ﬁngerprints could extended sensitive stereoisomers remains task future work. work similar spirit neural turing machine sense take existing discrete computational architecture make part differentiable order gradient-based optimization. neural nets quantitative structure-activity relationship modern standard predicting properties novel molecules compose circular ﬁngerprints fully-connected neural networks regression methods. used circular ﬁngerprints inputs ensemble neural networks gaussian processes random forests. used circular ﬁngerprints inputs multitask neural network showing multiple tasks helped performance. neural graph ﬁngerprints closely related work build neural network graph-valued inputs. approach remove cycles build graph tree structure choosing atom root. recursive neural network leaves root produce ﬁxed-size representation. graph nodes possible roots possible graphs constructed. ﬁnal descriptor representations computed distinct graphs. many distinct graphs atoms network. computational cost method thus grows size feature vector number atoms making less suitable large molecules. convolutional neural networks convolutional neural networks used model images speech time series however standard convolutional architectures ﬁxed computational graph making difﬁcult apply objects varying size structure molecules. recently others developed convolutional neural network architecture modeling sentences varying length. neural networks ﬁxed graphs introduce convolutional networks graphs regime graph structure ﬁxed training example differs different features vertices graph. contrast networks address situation training input different graph. neural networks input-dependent graphs propose neural network model graphs interesting training procedure. forward pass consists running message-passing scheme equilibrium fact allows reverse-mode gradient computed without storing entire forward computation. apply network predicting mutagenesis molecular compounds well page rankings. also propose neural network model graphs learning scheme whose inner loop optimizes training loss rather correlation newly-proposed vector training error residual. apply model dataset boiling points molecular compounds. paper builds ideas following differences method replaces complex training algorithms simple gradientbased optimization generalizes existing circular ﬁngerprint computations applies networks context modern qsar pipelines neural networks ﬁngerprints increase model capacity. unrolled inference algorithms others noted iterative inference procedures sometimes resemble feedforward computation recurrent neural network. natural extension ideas parameterize inference step train neural network approximately match output exact inference using small number iterations. neural ﬁngerprint viewed light resembles unrolled message-passing algorithm original graph. generalized existing hand-crafted molecular features allow optimization diverse tasks. making operation feature pipeline differentiable standard neural-network training methods scalably optimize parameters neural molecular ﬁngerprints end-toend. demonstrated interpretability predictive performance ﬁngerprints. data-driven features already replaced hand-crafted features speech recognition machine vision natural-language processing. carrying task virtual screening drug design materials design natural next step.", "year": 2015}