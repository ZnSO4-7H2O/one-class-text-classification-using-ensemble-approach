{"title": "Learning Structured Outputs from Partial Labels using Forest Ensemble", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Learning structured outputs with general structures is computationally challenging, except for tree-structured models. Thus we propose an efficient boosting-based algorithm AdaBoost.MRF for this task. The idea is based on the realization that a graph is a superimposition of trees. Different from most existing work, our algorithm can handle partial labelling, and thus is particularly attractive in practice where reliable labels are often sparsely observed. In addition, our method works exclusively on trees and thus is guaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video surveillance scenario, where activities are modelled at multiple levels.", "text": "learning structured outputs general structures computationally challenging except tree-structured models. thus propose efﬁcient boosting-based algorithm adaboost.mrf task. idea based realization graph superimposition trees. different existing work algorithm handle partial labelling thus particularly attractive practice reliable labels often sparsely observed. addition method works exclusively trees thus guaranteed converge. apply adaboost.mrf algorithm indoor video surveillance scenario activities modelled multiple levels. growing research interest developing probabilistic temporal graphical models recognising human activities sensory data. paper address important aspect problem multiple levels abstraction activity often composed several sub-activities. popular approach deal hierarchical nature build cascaded model level modelled separately output lower levels subsequently used input upper levels approach sub-optimal information higher level often discriminative infer lower levels modelled. moreover layered approach often suffers so-called cascading error problem error introduced lower level propagate higher tasks. better holistic approach build joint representation layers. emerging methods include generative/directed models abstract hidden markov models hierarchical hmms dynamic bayesian networks discriminative/undirected counterparts hierarchical conditional random ﬁeld dynamic general generative models useful jointly modelling activity semantics observed sensory data thus dealing well hidden variables. contrary discriminative models make less assumption nature sensory data thus potentially achieve higher classiﬁcation accuracy generative assumption violated. application interest problem activity recognition indoor environment. solution problem provide important technology building intelligent surveillance systems. propose paper dcrf modelling activities multiple scales. problem known difﬁcult hierarchical nature activities. dcrf used paper generalisation powerful probabilistic formulation known conditional random ﬁeld expressive representation scheme seamlessly integrates domain knowledge temporal regularities time encodes complex interdependency semantic levels. however expressiveness come great challenges problem parameter estimation arbitrary structures. known parameter estimation generally intractable perform exactly maximum likelihood setting general mrfs except tree structures. earliest popular methods deal intractability optimise pseudo-likelihood instead achieves efﬁciency limiting individual nodes graph neighbours. however method cannot handle missing variables unfortunately often happen real situations. sampling-based methods mcmc theoretically attractive often impractical extremely slow convergence. state-of-the-art methods often involves approximate inference algorithms pearl’s belief propagation recent method introduced wainwright jaakkola willsky methods efﬁcient biggest problem guarantee convergence. addition inference applied learning conditional mrfs issue also explored paper. main contribution paper introduction adaboosted markov random forests learning method operates markovian trees step proved achieve global optimum mild assumptions. distinct existing work discriminative models including work adaboost.mrf also handle partial labels) important enhancement real-world applications. essence adaboost.mrf based ranking-based multiclass boosting algorithm called adaboost.mr round adaboost.mrf selects best trained spanning tree network based performance weighted error. data adaptively re-weighted address hard-to-classify instances. selected spanning tree weighted combined recover original graph. finally parameters graph convex combination selected tree parameters. since method works exclusively trees inference efﬁcient convergence guaranteed. also prove mild assumptions adaboost.mrf reaches unique optimum. furthermore since adaboost.mrf considers variables mrfs partial labels problem also effectively handled. evaluate adaboost.mrf activity data obtained indoor video monitoring scenario. compare adaboost.mrf maximum likelihood method used inference engines. evaluate effectiveness discriminative dcrfs generative methods implement variant layered hidden markov models handle partially observed state variables make comparable dcrfs considered paper. also show multi-level dcrfs perform better ﬂat-crfs information added. paper continue related background section section introduces adaboost.mrf algorithm discusses convergence. section shows model learn multi-level activities dcrfs followed section present experimental results. discussion given section conclusion section since starting point work conditional random ﬁeld multiclass boosting shall provide brief related work background problems respectively section. work based conditional random ﬁelds markov random ﬁeld undirected graph represents joint distribution state variables corresponds node graph. simplicity assume variables domain receive assignments ﬁxed discrete values ...|s|}. conditional graph associated data observation knowledge structure within observation required. edge-set graph speciﬁes cliques supports feature vector maps observation clique-based state variable global feature vector. standard deﬁnes conditional distribution state variable given observation exponential family distribution however inference general networks known intractable except trees limited tree-widths. tree-structures well-known method two-pass belief propagation procedure takes time compute quantities needed learning number nodes tree. structures trees extract inference intractable approximate methods mean ﬁelds loopy belief propagation widely used. recently proposed method wainwright jaakkola willsky also offers interesting alternative compute so-called pseudo-marginals based minimising upper bound logpartition function. like efﬁcient message passing scheme. however guaranteed converge formal proof extensive empirical evaluation found literature. adaboost.mrf contrast works tree inference thus guaranteed converge known analytical complexity. besides best knowledge ﬁrst perform learning using conditional mrfs setting. crfs also applied activity recognition recently works reported promising results. however inherent intractability general structures either simple chains trees assumed approximate inference methods used. conditional random ﬁelds example structured output models learning structured output models based principles maximum likelihood example large-margin search latter computation feature expectation replaced ﬁnding probable labelling. learning partial labels addressed past decade related problems include weak supervision indirect supervision partial labels arise components observed i.e. observed missing components respectively. setting parameter learning requires maximise conditional incomplete log-likelihood instead shown section reviews multi-class boosting algorithm known adaboost.mr based work developed. adopt functional view boosting paper. given pool weak learners {hm} boosting setting seek learn subset wkhk i{·} indicator function. rank loss basically number possibilities system misclassiﬁes data. loss vanishes system correctly classiﬁes data instances. difference exponential loss log-loss numerical scale function log-loss. however authors show losses give close results given enough data. paper suggests boosting regarded alternative maximum likelihood estimation another related angle boosting-style algorithms derived standard adaboost.mr addresses simple classiﬁcation data interest structure. structured data crfs boosting applied algorithm relies approximate inference address missing variables. similarly work limited tractable crfs. section present novel boosting algorithm parameter estimation general conditional random ﬁelds termed adaboost.mrf. consider general case state label visible component missing component i.e. apart summing visible component also introduce extra regularisation term numerical stability. round boosting strong learner updated adding ‘weak-learner’ previous αlhl weight weak learner ensemble. weak learner weight chosen minimise loss i.e. since interested distribution sensible choose weak learner however stated before distribution deﬁned general markov networks used computation weak learner becomes intractable. propose spanning trees graph weak learner serving approximation whole network. thus learner weak sense approximation true model moderate tractable complexity. conditional distribution visible state variables given observation respect spanning tree deﬁne weak learner choice also allows incorporation missing information since thus strong learner collection trees term boosting method adaboost.mrf figure shows example simple network spanning trees. defer discussion choice tree-based distributions relationship model distribution later sections. shall continue derivation bound formulated loss goal provide tractable method compute bound. although exponential loss meaningful unfortunately intractable compute alone minimising subsection propose replace loss tractable upper bound using tree likelihood. recall boosting procedure incremental substituting yields following expression step weak learners convergence complexity previous subsection suggested loss minimise. recall collection spanning trees need trees minimise subsection present iterative procedure select trees guarantee reduce loss. also provide analysis convergence time complexity procedure. however functional gradient functional direction belong function space direct optimisation apply. authors propose best pointing decreasing direction i.e. ﬁnal result satisfying interpretation functional gradient descent step tries solve maximum re-weighted log-likelihood problem tree select best tree largest re-weighted log-likelihood. boosting proceeds trees likely selected others accumulated weights trees different. learner added ensemble factor used control data weights i.e. weights approach uniform distribution. since weight increases interpretation given data instance weak learner less likely average previous weak learners adaboost.mrf increase weight data instance. different usual boosting behaviour data weight increases strong learner fails correctly classify instance. adaboost.mrf seems maximise data likelihood rather minimise training error particularly desirable density estimation. provide formal support convergence tree selection procedure search direction satisfying condition equation called gradient-related following convergence result given lipschitz continuity condition i.e. −∇lh mh−h function space gradient-related search direction reasonably small step size satisﬁes lipschitz continuity condition satisﬁed case twice differentiable hessian bounded constant hard analytically implementation step size small constant found sufﬁcient experiments. algorithm terminates cannot weak learner satisﬁes condition equation running time adaboost.mrf scales linearly number trees takes inference time. consider limited spanning trees enough cover whole network quite moderate. example fully connected network need grid-like network sufﬁcient point successfully estimated parameters individual trees thus strong learner enough classiﬁcation purposes. however ultimate goal estimate parameters original network superimposition individual trees. subsection presents method approximate estimation. assume tree distribution also belongs exponential family different parameters global feature vector require parts parameters correspond cliques outside trees zero. rewrite combined model turns logarithmic opinion pool special case general ensemble framework. model expert provide estimate true distribution aggregator indeed minimiser weighted kullback-leibler divergences relations works relation approaches work shows closer true distribution average individual experts boosting algorithm seen estimator weighting factors {αl}. offers interesting discussion relation markov networks logop properties desirable aggregators logop satisﬁes. method based idea superimposition union subnetworks node edge belongs aggregated network must belong individual sub-networks. authors consider combination different models share underlying simple chain structure. model trained independently combined using logop. model weights{αl} estimated maximising likelihood combined models. approach long underlying structure tractable. another related idea product-of-experts weights unity. sampling used overcome intractability converge within limited time. contrast method efﬁcient deals directly trees. application interest model indoor activities person observed cameras mounted ceilings kitchen shown figure environment activities naturally acted hierarchical manner. consider levels activity abstraction modelled using two-layer dynamic conditional random ﬁeld bottom level presents primitive atomic activities go-to-cupboard at-thefridge. higher-order activities captured higher level having-snack shortmeal. differing original setting dcrf allow missing labels figure partially labelled dcrf equivalent chain crf. filled circles bars data observations empty circles missing labels shaded labels visible. model thus call model partially labelled dcrf note although two-level dcrf considered paper construction readily generalized model complex semantics richer levels hierarchy temporal interactions. original ph-dcrf exact estimation marginals carried collapsing states current time mega-state performing forward-backward procedure infeasible deep models. approximate inference using methods complexity number message passing rounds number edges network state size node. however number rounds convergence known analytically theoretical estimate yet. adaboost.mrf inference trees takes time number nodes network. thus data instances trees adaboost.mrf costs total time gradient evaluation since need take account. similarly wjw-based requires time. fully connected networks |e|= grid dcrfs take trees former case latter case total complexity gradient evaluation wjw-based adaboost.mrf similar constant summarise complexities table dataset used experiment collected previous work using system shown figure captured video sequences training sequences testing. observations sequences noisy coordinates actor walking scene acquired using background subtraction tracking algorithm. consider complex activities evaluating aspect missing labels randomly provide half labels level training. testing assignments resulted pearl’s loopy max-product algorithm compared ground-truth. data described above input dcrfs simply sequences coordinates. time slice extract vector elements observation sequence {gm} corresponds coordinates velocities speed respectively. fully specify model consider three types feature functions potentials network data-association corresponding node potentials temporal-relation corresponding state transition potentials level cross-semantic-relation corresponding parent-child potentials across different levels. amount look-ahead look-back positive inteγ state choose reasonable gers computation current primitive activity correlated surrounding observation features level however instant information velocities offer limited help since complex activities often span long periods. instead using real coordinates data association quantize squares room. also much larger windows avoid computational overhead take −s−s adaboost.mrf algorithm described figure requires speciﬁcation spanning trees used weak classiﬁers. given grid structure considered experiment many spanning trees extracted. however since nature problem temporal regularities slice structure repeated time natural decompose network trees structural repetition maintained. hint noticeable trees stand shown figure roughly corresponds bottom chains respectively. method number trees dynamic models respect markov assumption reduced drastically. impose restrictions state interact level right right number trees manageable comparing adaboost.mrf dcrfs implement learning methods based exact inference. also evaluate effectiveness dcrfs layered hmms output bottom used input hmm. since difﬁcult encode rich feature information lhmms without producing large state space limit lhmms features discretised positions differences current position previous next ones. implementation lhmms differs original extended handle partially observed states. test whether adding layers improve performance model simple flat-crf data lower level. learning algorithms initialised uniformly. segmentation purposes report macro-averaged scores per-label basis. parameter optimisation log-likelihood initially used limited memory quasi-newton method suggested literature seems slower converges prematurely poor solutions exact inference. conjugate-gradient method works better experiments. markov forests iterations boosting round initial parameters previously learned ones since need meet condition inference loop stopped messages converged rate rounds. appears ﬁnal performance sensitive choice convergence rates fairly stable wjw. example scores bottom level corresponding rates respectively. report case appears best terms accuracy speed. learning algorithms dcrfs stopped iterations converged rate performance adaboost.mrf alternatives reported figure table respectively. overall enough training time adaboost.mrf performs comparably methods based wjw. exact inference method gives slightly better result expected cost much slower training time. however stressed inference adaboost.mrf always converges guaranteed generally intractable exact method. complexity evaluation log-likelihood gradient known ﬁxed adaboost.mrf generally dependent convergence criteria much distribution different table also shows choice discriminative model generative model activity recognition problem justiﬁed. lhmms worse ﬂat-crfs bottom layer dcrfs layer. furthermore dcrfs variants consistently accurate crfs. result consistent explained fact information encoded dcrfs. rely boosting capacity boost weak learners strong need reach maximum weighted log-likelihood round. simply training iterations take partial results long condition met. speedup learning initialise parameters weak learner previously learned values. procedure interesting interpretation tree-structured graphs. select best spanning trees anymore algorithm simply optimise re-weighted log-likelihood stage-wise manner. argue approach attractive information data distribution used guide maximum likelihood estimation create diverse weak classiﬁers. non-tree graphs guided derived assuming tree mixing coefﬁcients known advance. recall assumed parameters shared among trees. relax assumption allowing trees share common parameters joint parameters trees rewrite loss noted formulation adaboost.mrf require unique existence distribution tree. instead freedom choose many distributions wish provided distributions expressed log-linear form thus given feature pool create distributions incorporates subset features. adaboost.mrf proceeds particular tree-based distribution picked time thus implicitly performing feature selection capacity. reasonable expect feature combination result poor likelihood thus selected equation. presented novel method using boosting parameter estimation general markov networks partial labels. algorithm adaboost.mrf offers efﬁcient tackle intractability maximum likelihood method breaking model tractable trees combining recover original networks. apply algorithm problem multi-level activity recognition segmentation using recently proposed dcrfs. would like stress however adaboost.mrf limited dcrfs readily applied arbitrary crfs. addition discriminatively furthermore experiments appears adaboost.mrf exhibits structure learning behaviour since selectively pick trees frequently others giving higher weights trees. important issue left unanswered automatically select optimal tree round without knowing trees advance. plan investigate aspects adaboost.mrf wider range applications.", "year": 2014}