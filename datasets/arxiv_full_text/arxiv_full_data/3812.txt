{"title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks  for Large Vocabulary Speech Recognition", "tag": ["cs.CL", "cs.NE"], "abstract": "Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task.", "text": "long short-term memory based acoustic modeling methods recently shown give state-of-the-art performance speech recognition tasks. achieve performance improvement research deep extensions lstm investigated considering deep hierarchical model turned efﬁcient shallow one. motivated previous research constructing deep recurrent neural networks alternative deep lstm architectures proposed empirically evaluated large vocabulary conversational telephone speech recognition task. meanwhile regarding multi-gpu devices training process lstm networks introduced discussed. experimental results demonstrate deep lstm networks beneﬁt depth yield state-of-the-art performance task. recently context dependent deep neural network hidden markov model become dominant framework acoustic modeling speech recognition however given speech inherently dynamic process researchers pointed recurrent neural networks considered alternative models acoustic modeling cyclic connections rnns exploit selflearnt amount temporal context makes rnns better suited sequence modeling tasks. unfortunately practice conventional rnns hard trained properly vanishing gradient exploding gradient problems described address problems literature proposed elegant architecture called long short-term memory lstms conventional rnns successfully used many sequence labeling sequence prediction tasks. language modeling rnns used generative models word sequences remarkable improvements achieved standard n-gram models. handwriting recognition lstm networks applied long time which bidirectional lstm networks trained connectionist temporal classiﬁcation demonstrated performing better hmm-based system. speech synthesis blstm network also applied notable improvement obtained language identiﬁcation lstm based approach proposed compared i-vector systems better performance achieved. recently lstm networks also introduced phoneme recognition task robust speech recognition task large vocabulary speech recognition task shown state-of-the-art performances. subsequently sequence discriminative training lstm networks investigated signiﬁcant gain obtained. researches acoustic modeling depth feed-forward neural networks lead expressive models. lstms conventional rnns inherently deep time expressed composition multiple nonlinear layers unfolded time. paper explores depth lstms deﬁned depth space. based earlier researches constructing deep rnns work possible approaches explored extend lstm networks deep ones various deep lstm networks empirically evaluated compared large vocabulary mandarin chinese conversational telephone speech recognition task. although lots attentions attracted deep lstm networks paper summaries approaches constructing deep lstm networks different perspectives suggests alternative architectures yield comparable performance. lstm architecture recurrent hidden layer consists recurrently connected subnets known memory blocks. memory block contains self-connected memory cells three multiplicative gates control information. lstm cell information cell guarded learned input output gates. later order provide cells reset themselves forget gate added addition modern lstm architecture contains peephole weights connecting gates memory cell improve lstm’s ability learn tasks require precise timing counting internal states illustrated fig. where logistic sigmoid function respectively input gate forget gate output gate cell input activation cell state vectors size hidden vector diagonal weight matrices peephole connections. cell input cell output non-linear activation functions generally paper tanh. number theoretical results support deep hierarchical model efﬁcient representing functions shallow paper focused constructing deep lstm networks. architecture conventional rnns carefully analyzed three points deepened inputto-hidden function hidden-to-hidden transition hiddento-output function. paper three points stacked lstms several novel architectures extend lstm networks deep ones introduced follows. convenience simpliﬁed illustration lstm shown fig. ﬁrstly. fig. illustrations different strategies constructing lstm based deep rnns. conventional lstm; lstm input projection; lstm output projection. lstm deep input-to-hidden function; lstm deep hiddento-output function; stacked lstms call architecture lstm output projection layer however architecture proposed earlier literature address computation complexity learning lstm network called lstm projected. perspective paper architecture considered increase depth hidden-to-hidden transition although beneﬁcial tackling computation complexity issue. noticed that lstm-op architecture linear activation units used projection layer like literature suggested. contrast must non-linear activation units used projection layer lstm-ip. deep transition discussed increasing depth hidden-to-hidden transition. thus architectures obtained illustrated fig. fig. details architecture shown fig. multiple layer transformation added cell input activation means calculation equation changed typical make input-to-hidden function deep using higher-level representations dnns input rnns. literature reported better phoneme recognition performance could achieved applying strategy rnns. previous studies based conventional rnns research method adopted constructing deep lstm networks illustrated fig. applied large vocabulary speech recognition task. perhaps straight-forward construct deep lstm network stack multiple lstm layers other. speciﬁcally output lower lstm layer input upper lstm layer. stacked lstm networks combine multiple levels representations ﬂexible long range context introduced acoustic modeling speech recognition showed signiﬁcant performance improvement obtained compared shallow one. implement lstm network training multi-gpu devices. training procedure truncated back-propagation though time learning algorithm adopted. sentence training split subsequences equal length tbptt illustrated fig. adjacent subsequences overlapping frames toverlap gradients computed subsequence back-propagated start. computational efﬁciency operates parallel subsequences different utterances time. updated parameters lstm networks continues next subsequences utterances.besides order train networks multi-gpu devices asynchronous stochastic gradient descent adopted. experiments took days train shallow conventional lstm network cells four devices -hour speech corpus training lstm layer took around times much time training fullconnection feed-forward hidden layer. evaluate lstm networks large vocabulary speech recognition task hkust mandarin chinese conversational telephone speech recognition corpus collected transcribed hong kong university science technology contains -hour speech calls training calls development respectively. experiments around -hour speech randomly selected training used validate network training original development corpus used speech recognition test used training hyper-parameters determination procedures. speech dataset represented frames melscale log-ﬁlterbank coefﬁcients along ﬁrst second temporal derivatives. experiments feed-forward dnns used concatenated features produced concatenating current frame frames left right context. however inputs lstm networks current features used. trigram language model used experiments estimated using transcriptions acoustic model training set. hybrid approach acoustic modeling lstm networks dnns neural networks’ outputs converted pseudo likelihood state output probability framework. networks trained based alignments generated well-trained gmm-hmm systems tied context dependent states cross-entropy objective function used networks. network training learning rate decreased exponentially. tried initial ﬁnal learning rates speciﬁc network architecture stable convergence network. experiments initial learning rates ranged ﬁnal learning rate always one-tenth corresponding initial one. training procedure lstm networks strategy introduced applied scale gradients. besides since information future frames helps making lstm networks better decisions current frame also delayed output state labels frames. firstly baseline performance summarized table training subspace kaldi toolkit used. dnns experiments hidden layers. layer relu model relu units layer pnorm model pnorm units hyperparameter group size conv model convolutional layers three relu layers. found that character error rates baseline gmm-hmm dnn-hmm comparable reported experiments conducted evaluate deep lstm networks shown fig. training procedure lstm networks tbptt ﬁxed toverlap ﬁxed four gpus used operated parallel subsequences time. order construct lstm network deep input-tohidden function constructed lstm network putting lstm network three feed-forward intermediate layers feed-forward layer relu units. network indicated -layer relu lstm table similarly trained model indicated -layer conv -layer relu lstm. deep hidden-to-output function lstm network indicated lstm -layer relu table constructed adding three feed-forward intermediate hidden layers lstm layer feed-forward hidden layer relu units. stacked lstms network also evaluated which three conventional lstms stacked layer lstm cells. three networks trained using discriminative pre-training algorithm concretely training procedure -layer relu lstm three relu hidden layers ﬁrstly pre-trained original output softmax layer replaced random initialized lstm layer along output softmax layer. finally whole network jointly optimized. comparing results listed table baseline performance -layer conventional lstm network even worse feed-forward dnns. making deep hiddento-hidden transitions obvious performance improvements obtained especially lstm-op. besides performance also improved making deep input-to-hidden hidden-tooutput functions. noted that lstm-op yield comparable performance stacked lstms reached similar conclusion possible design train deeper variant lstm network combines different methods together. instance stacked lstm-ops network constructed combining deep hidden-to-hidden transition stack lstms. combining different methods potential improve performance. thus experiments conducted evaluate selected combinations methods constructing deep lstm networks hidden layer conﬁguration experiments described above. results listed table best performance obtained combining lstm-op deep hidden-to-output function. results table that performance improved stacking lstm-ips lstm-ops. however network lstm-op layer three feed-forward intermediate layers yielded worse performance lstm-op network needed researched. noteworthy network three full-connection hidden layers lstm-op layer yielded best performance required less computations stacked experimental results revealed deep lstm networks beneﬁt depth. compared shallow lstm network relatively reduction obtained. compared feed-forward dnns deep lstm networks reduce relatively reduction. paper explored novel approaches construct long short-term memory based deep recurrent neural networks number theoretical results support deep hierarchical model efﬁcient representing functions shallow paper focused constructing deep lstm networks shown give state-of-the-art performance acoustic modeling speech recognition tasks. inspired discussion construct deep rnns several alternative architectures constructed deep lstm networks three points input-to-hidden function hidden-to-hidden transition hidden-to-output function. furthermore paper deeper variants lstms also designed combining different points. work lstm network training implemented multi-gpu devices truncated bptt learning algorithm adopted experiments discovered lstm rnns also quickly trained devices. empirically evaluated various deep lstm networks large vocabulary mandarin chinese conversational telephone speech recognition task. experiments revealed constructing deep lstm architecture outperformed standard shallow lstm networks dnns. besides lstm-op followed three feedforward intermediate layers outperformed stacked lstm-ops. however believe work preliminary study construct deep lstm networks. many efforts need done architectures lstm networks. architectures explored evaluated future work lstm-ip network three non-linear activation projection layers stacked lstms network followed multiple feed-forward intermediate layers lstm network input output project layers deep architectures maxout unit improved lstm layer dahl deng acero context-dependent pretrained deep neural networks large-vocabulary speech recognition ieee trans. audio speech lang. processing vol. graves liwichi fern´andez novel connnectionist system unconstrained handwriting recognition ieee trans. pattern analysis machine intelligence vol.", "year": 2014}