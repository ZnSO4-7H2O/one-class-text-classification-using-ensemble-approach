{"title": "Exponentiated Gradient Exploration for Active Learning", "tag": ["cs.LG", "cs.AI", "I.2"], "abstract": "Active learning strategies respond to the costly labelling task in a supervised classification by selecting the most useful unlabelled examples in training a predictive model. Many conventional active learning algorithms focus on refining the decision boundary, rather than exploring new regions that can be more informative. In this setting, we propose a sequential algorithm named EG-Active that can improve any Active learning algorithm by an optimal random exploration. Experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods.", "text": "abstract. active learning strategies respond costly labelling task supervised classiﬁcation selecting useful unlabelled examples training predictive model. many conventional active learning algorithms focus reﬁning decision boundary rather exploring regions informative. improve active learning algorithm optimal random exploration. experimental results show statistically signiﬁcant appreciable improvement performance approach existing active feedback methods. active learning learning algorithm access large unlabeled examples oracle query label individual example. queries oracle assumed expensive feasible label small subset. thus goal algorithm actively select examples good hypothesis learned using labelled examples possible. many conventional active learning algorithms choose label points near decision boundary current hypothesis. function well active learner aware important regions instance space i.e. large examples learner’s hypothesis misclassify since hasn’t seen labelled examples them. active learners good labelling examples near boundary reﬁne conduct random searching large regions instance space would incorrectly classify. recent work done sense considers random exploration active learning. author considers types active learner ﬁrst dedicated exploit based reﬁning decision boundary second dedicated explore random exploration gets reward exploration. drawback approach fact that approach takes long time optimal random exploration rate. tackle problem paper propose algorithm named eg-active improve remaining paper organized follows. section reviews related works. section describes model proposed algorithm. experimental evaluation illustrated section last section concludes paper points possible directions future work. variety algorithms proposed literature employing various query strategies. popular strategy called uncertainty sampling active learner queries point whose label uncertain uncertainty label usually calculated using entropy variance label distribution. authors introduced query-bycommittee strategy committee potential models agree currently labelled data maintained point committee members disagree with considered querying. strategies include maximum expected reduction error variance reducing query strategies query optimal point. recently random exploration used diﬀerent domains recommender system information retrieval. example authors model contextual bandit problem. authors propose algorithm perform random recommendation according risk upsetting user. however knowledge paper addressing random exploration active learning. authors address problem randomly choosing exploration exploitation round receive feedback eﬀective exploration. impact exploration measured induced change learned classiﬁer exploratory example labelled added training set. active learner updates probability exploring subsequent rounds based feedback received. however none optimisation techniques used compute optimal exploration work done improve uncertainty sampling technique. reward. metric used measure variation hypothesis learned model iterations. hypothesis learned model varies reward. deﬁne function variation model. labelled unlabelled trainses deﬁne vectors i.e. vectors real-valued predictions iteration algorithm runs sampling procedure select ﬁnite candidates. probabilities associated candidates uniformly initialized updated exponentiated gradient updating rule increases probability candidate leads user’s click. first assume ﬁnite number candidate values denoted learn optimal set. eg-active introduce stands probability using \u0001-active algorithm. probabilities initialized beginning iteratively updated iterations. algorithm weights keep track performance update using algorithm. idea increase algorithm receives click using finally algorithm calculates normalizing smoothing. algorithm shows eg-active. corporate supervised algorithm simulate experiments expert unannotated corpus using rule based algorithm trained uttrances. note objective evaluation observe improvement random exploration existing active learning. experiments consider version rule based algorithm without training iteration active learning tries selects unannoteted interesting utterances annotate integrate training rule based algorithm. relating results newer versions verify usefulness proposed approach. moreover calculate regret every iterations process iterations correspond budget term labelling. addition random compare methods constructing groups algorithms ﬁrst group state algorithm described related work sampling committee request uncertainty sampling density weight methods. second group contains modiﬁed state algorithms added ﬁxed random exploration existing state algorithms example .-qbc means probability algorithm random exploration otherwise request committee. fourth group added dynamic random exploration existing state algorithms done example p-qbc algorithm uses strategy compute probability random exploration. several observations regarding diﬀerent active learning algorithms. observe plot ﬁxed tuned random exploration lead result. conﬁrms pure exploration interesting conﬁrms need dynamic random exploration tuning. dynamics exploration leads improvement result active learning shown p-us p-cbq p-wd. expected eg-active egactive eg-active eﬀectively best convergence rates. eg-active eg-active eg-active decrease average regret respectively factor baseline. improvement comes optimization strategy deﬁning exploration. paper proposed improvement active learning considering random exploration. validated work data realworld application shown proposed model oﬀers promising results. study yields conclusion considering great chosen random exploration used active learning algorithm increases result. considering results plan investigate public benchmarks also plan study random exploration bandit algorithms. lewis gale. sequential algorithm training text classiﬁers. proceedings annual international sigir conference research development information retrieval sigir pages york springer-verlag york inc. osugi scott. balancing exploration exploitation algorithm active machine learning. data mining fifth ieee international conference pages pp.– laﬀerty ghahramani. combining active learning semi-supervised learning using gaussian ﬁelds harmonic functions. icml workshop continuum labeled unlabeled data machine learning data mining pages", "year": 2014}