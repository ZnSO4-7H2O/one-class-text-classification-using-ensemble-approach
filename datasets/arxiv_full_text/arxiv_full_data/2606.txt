{"title": "Improving Sampling from Generative Autoencoders with Markov Chains", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. We call the distribution to which the inference model maps observed samples, the learned latent distribution, which may not be consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively decoding and encoding, which allows us to sample from the learned latent distribution. Since, the generative model learns to map from the learned latent distribution, rather than the prior, we may use MCMC to improve the quality of samples drawn from the generative model, especially when the learned latent distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion.", "text": "focus generative autoencoders variational adversarial autoencoders jointly learn generative model alongside inference model. generative autoencoders trained softly enforce prior latent distribution learned inference model. call distribution inference model maps observed samples learned latent distribution consistent prior. formulate markov chain monte carlo sampling process equivalent iteratively decoding encoding allows sample learned latent distribution. since generative model learns learned latent distribution rather prior mcmc improve quality samples drawn generative model especially learned latent distribution prior. using mcmc sampling able reveal previously unseen differences generative autoencoders trained either without denoising criterion. unsupervised learning beneﬁted greatly introduction deep generative models. particular introduction generative adversarial networks variational autoencoders plethora research learning latent variable models capable generating data complex distributions including space natural images models extensions operate placing prior distribution latent space learn mappings latent space space observed data interested autoencoding generative models models learn generative mapping also inferential mapping speciﬁcally deﬁne generative autoencoders autoencoders softly constrain latent distribution match speciﬁed prior distribution achieved minimising loss lprior latent distribution prior. includes vaes extensions vaes also adversarial autoencoders whilst autoencoders also learn encoding function together decoding function latent space necessarily constrained conform speciﬁed probability distribution. distinction generative autoencoders; still deterministic functions functions deﬁned input respectively however outputs functions constrained practically type functions maps maps training however encoder training data samples decoder samples encoder encoder decoder learn mappings process encoding decoding interpreted sampling conditional probabilities respectively. conditional distributions sampled using encoding decoding functions learned parameters encoding decoding functions respectively. decoder generative autoencoder used generate samples consistent data. traditional approaches sampling generative autoencoders approach data generating distribution. however approach likely generate samples similar training data rather generating novel samples consistent training data. approach suggests obtain realistic generations latent samples drawn rather followed limited number latent samples drawn using ﬁrst steps approach however drawbacks discussed approach introduce alternative method sampling drawbacks. main contribution formulation markov chain monte carlo sampling process generative autoencoders allows sample iteratively sampling chain starting arbitrary chain converges zt→∞ allowing draw latent samples several steps mcmc sampling. practical perspective achieved iteratively decoding encoding easily applied existing generative autoencoders. optimised close initial sample drawn improving quality samples within iterations. interpolating latent encodings guarantee stays within high density regions previously addressed using spherical rather linear interpolation high dimensional space however approach attempts keep figure data generating distribution. access samples drawing samples training data. conditional distribution modeled encoder maps samples samples ideal encoder maps samples known prior distribution reality encoder maps samples unknown distribution conditional distribution modeled decoder maps samples training decoder learns samples drawn rather samples drawn decoder sees samples regularisation latent space encourages close note lprior optimal overlaps fully figure prior work spherically interpolating faces using attempt gradually generate sunglasses results visual artifacts around eyes. model fails properly capture desired change orientation face resulting three partial faces middle interpolation. work result steps mcmc sampling applied latent samples used generate original interpolations discolouration around eyes disappears model settling either generating generating glasses. model moves away multiple faces interpolation producing faces appropriate orientations. within rather trying sample instead applying several steps mcmc sampling interpolated samples sampling unrealistic artifacts reduced whilst methods generate realistic samples rely adjusting encodings observed data mcmc allows walk latent sample probable regions learned latent distribution resulting convincing generations. demonstrate mcmc sampling improves generations vaes aaes high-dimensional important previous studies shown dimensionality scaled intrinsic latent dimensionality observed data. second contribution modiﬁcation proposed transition operator mcmc sampling process denoising generative autoencoders. generative autoencoders trained using denoising criterion reformulate original mcmc sampling process incorporate noising denoising processes allowing mcmc sampling denoising generative autoencoders. apply sampling technique models. ﬁrst denoising introduced found mcmc sampling revealed beneﬁts denoising criterion. second model denoising constructed applying denoising criterion aae. modiﬁcations cost function. dvae daae effects denoising crtierion immediately obvious initial samples. training generative autoencoders denoising criterion reduced visual artefacts found generations interpolations. effect denoising criterion revealed sampling denoising models using mcmc sampling. main tasks machine learning learn explanatory factors observed data commonly known inference. given data sample would like corresponding latent encoding another task learn inverse generative mapping given corresponding general coming suitable criterion learning mappings difﬁcult. autoencoders solve tasks efﬁciently jointly learning inferential mapping generative mapping using unlabelled data self-supervised fashion basic objective autoencoders minimise reconstruction cost lreconstruct original data reconstruction examples lreconstruct include squared error cross-entropy loss θ))] loss autoencoders cast probablistic framework considering samples attempting learn conditional distributions respectively lreconstruct representing negative log-likelihood reconstruction given encoding autoencoder possible create novel passing knowledge appropriate choices beyond obtained solution constrain latent space encoding model maps observed samples. achieved additional loss lprior penalises encodings away speciﬁed prior distribution review types generative autoencoders vaes aaes take different approaches formulating lprior. consider case constructed stochastic neurons produce outputs speciﬁed probability distribution lprior used constrain distribution outputs leaves problem estimating gradient autoencoder expectation would typically addressed monte carlo method. vaes sidestep constructing latent samples using deterministic function source noise moving source stochasticity input leaving network deterministic standard gradient calculations—a technique commonly known reparameterisation trick consists deterministic function erep outputs parameters probability distribution plus source noise. case diagonal covariance gaussian erep maps vector means vector standard deviations noise together encoder outputs samples hadamard product. vaes attempt make samples encoder match using divergence parameters probability distribution outputted erep parameters prior distribution giving lprior dklp multivariate gaussian analytical divergence simpliﬁed considering unit gaussian resulting lprior another approach deterministically output encodings rather minimising metric probability distributions using parameters turn density ratio estimation problem goal learn conditional distribution bution framework solves density ratio estimation problem transforming class estimation problem using networks ﬁrst network training discriminator network trained maximise probability samples real distribution minimise probability samples fake distribution case plays role second network generator network generates fake samples. networks compete minimax game receives gradients learns better fool training objective networks given lprior argminφ argmaxψ argminφ argmaxψ eqφp log]. formulation create problems training instead trained minimise log)) provides ﬁxed point dynamics result applying framework encoder autoencoder deterministic general viewpoint generative autoencoders fulﬁll purpose learning useful representations observed data. another widely used class autoencoders achieve denoising autoencoders motivated idea learned features robust partial destruction input require encoding inputs capturing statistical dependencies inputs corrupted data recovered daes presented corrupted version input must still reconstruct original input noisy inputs cre˜ ated sampling corruption process. denoising criterion ldenoise applied type autoencoder replacing straightforward reconstruction criterion lreconstruct; reconstruction criterion applied noisy inputs lreconstruct; θ)). encoder used model samples drawn such construct denoising generative autoencoders training autoencoders minimise ldenoise lprior. might expect differences samples drawn denoising generative autoencoders non-denoising counterparts. however figures show case. address case dvaes claiming noise mapping requires adjusting original objective function. work orthogonal theirs others adjust training model focus purely sampling generative autoencoders training. claim existing practice drawing samples generative autoencoders conditioned suboptimal quality samples improved instead conditioning mcmc sampling. consider case sampling generative autoencoders used draw samples section showed important sampling condition drawn rather often done practice. however show initial markov sampling used produce chain samples produces samples distribution used draw meaningful samples conditioned speed convergence initialise distribution close drawing drawing samples according transition operator produces markov chain. transition operator homogeneous parameters encoding decoding functions ﬁxed sampling. show stationary distribution sampling markov chain theorem deﬁnes ergodic markov chain z...zt} chain converge stationary distribution arbitrary initial distribution. stationary distribution proof. markov chain ergodic must irreducible aperiodic satisfy requirements sufﬁcient show since every would reachable every show giving providing proof section supplementary material. lemma stationary distribution chain deﬁned proof. transition operator deﬁned equation asymptotic distribution converges deﬁnition marginal joint distribution lprior used learn conditional distribution work inspired bengio denoising autoencoders cast probabilistic framework denoising distribution corruption distribution. represents space corrupted samples. bengio deﬁne transition operator markov chain using conditional distributions whose stationary distribution assumption perfectly denoises samples. chain initialised samples training data used generate chain samples work generalised include corruption process mapped data samples latent variables create type network called generative stochastic networks however gsns latent space regularised prior. work similar several approaches proposed bengio rezende bengio rezende deﬁne transition operator terms xt−. bengio generate samples initial drawn observed data rezende reconstruct samples corrupted version data sample. contrasts bengio rezende work deﬁne transition operator terms initialise samples drawn prior distribution directly sample from sample conditioned although initial samples poor likely generate novel ﬁrst step mcmc sampling would achieved using bengio al.’s rezende al.’s approach. able draw initial prior constrain close prior distribution bengio latent space either explicitly modeled constrained further rezende explicitly assume distribution latent samples drawn matches prior instead assume samples drawn distribution necessarily match prior propose alternative method sampling order improve quality generated image samples. motivation also different rezende since sampling generate improved novel data samples sampling denoise corrupted samples. effect regularisation method choice lprior effect much improvement gained using mcmc sampling assuming optimisation process converges reasonable solution. ﬁrst consider case vaes minimise dklp minimising divergence penalises model contains samples outside support true distribution might mean captures part means sampling draw region captured suggests mcmc sampling improve samples trained vaes walking towards denser regions generally speaking using reverse divergence training dklqφ] penalises model produces samples outside support minimising divergence samples likely well. aaes qφ)] hand regularised using entropy given qφ)]. minimising cost function attempts comdkl promise aforementioned extremes. however still suggests samples outside expect aaes also beneﬁt mcmc sampling. utilise deep convolutional basis autoencoder models. although recommendations radford standard architectures adopt sensible defaults autoencoder encoder mimicking dcgan’s discriminator decoder mimicking generator. encoder uses strided convolutions rather max-pooling decoder uses fractionally-strided convolutions rather ﬁxed upsampling. convolutional layer succeeded spatial batch normalisation relu nonlinearities except decoder utilises sigmoid function constrain output values minimise cross-entropy original reconstructed images. although results blurry images regions ambiguous hair detail extra loss functions improve visual quality generations avoid confounding results. although capable approximating complex probabilistic posteriors construct output deterministic such ﬁnal layer encoder part aaes convolutional layer deterministically outputs latent sample adversary fully-connected network dropout leaky relu nonlinearities. erep vaes output twice size corresponds means standard deviations diagonal covariance gaussian distribution. models prior isotropic gaussian zero mean unit variance primary dataset celeba dataset consists images celebrities dcgan ﬁrst generative neural network model show convincing novel samples dataset used ever since qualitative benchmark amount quality samples. figures supplementary material also include results svhn dataset consists images house numbers extracted google street view images datasets perform preprocessing cropping centre create square image resizing train generative autoencoders epochs training split datasets using adam denoising generative autoencoders additive gaussian noise mapping experiments using torch library evaluation generate novel samples decoder using initially sampled also show spherical interpolations four images testing split depicted figure perform several steps mcmc sampling novel samples interpolations. process training mode batch normalisation i.e. normalise inputs using minibatch rather population statistics normalisation partially compensate poor initial inputs training distribution. compare novel samples models below leave interpolation results figures supplementary material. figure samples dvae daae trained celeba dataset. show initial samples conditioned mainly result recognisable faces emerging noisy backgrounds. step mcmc sampling unrealistic generations change noticeably continue steps. hand realistic generations i.e. samples region high probability change much. adversarial criterion deterministic aaes difﬁcult optimise dimensionality high. observe training aaes daaes empirical standard deviation less means fails approximate closely achieved dvae. however means effect mcmc sampling pronounced quality samples noticeably improving steps. side-effect suboptimal solution learned networks denoising properties daae noticeable novel samples. autoencoders consist decoder encoder function learned parameters. functions used draw samples conditional distributions refers space observed samples refers space latent samples. encoder distribution maps data samples data generating distribution latent distribution decoder distribution maps samples concerned generative autoencoders deﬁne family autoencoders regularisation used training encourage close known prior commonly assumed similar samples used sample decoder make assumption sufﬁciently close instead derive mcmc process whose stationary distribution allowing directly draw samples conditioning samples samples drawn consistent training data. experiments compare samples zi’s obtained mcmc sampling show mcmc sampling improves initially poor samples also show artifacts samples induced interpolations across latent space also corrected mcmc sampling validate work showing denoising properties denoising generative autoencoders best revealed mcmc sampling. mcmc sampling process straightforward applied easily existing generative autoencoders. technique orthogonal powerful posteriors aaes vaes combination could result improvements generative modeling. finally basic mcmc process opens doors apply large existing body research sampling methods generative autoencoders. yoshua bengio eric thibodeau-laufer guillaume alain jason yosinski. deep generative stochastic networks trainable backprop. journal machine learning research proceedings international conference machine learning volume goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning diederik kingma jimmy adam method stochastic optimization. proceedings international conference learning representations arxiv preprint arxiv. https//arxiv.org/pdf/.v.pdf. diederik kingma welling. auto-encoding variational bayes. proceedings international conference learning representations arxiv preprint arxiv. https//arxiv.org/abs/.. anders boesen lindbo larsen søren kaae sønderby winther. autoencoding beyond pixels using proceedings international conference machine learning learned similarity metric. arxiv preprint arxiv. http//jmlr.org/proceedings/ papers/v/larsen.pdf. yuval netzer wang adam coates alessandro bissacco andrew reading dignips workshop deep learning unnatural images unsupervised feature learning. supervised feature learning https//static.googleusercontent.com/media/ research.google.com/en//pubs/archive/.pdf. alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional international conference learning representations danilo jimenez rezende shakir mohamed daan wierstra. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning arxiv preprint arxiv. https//arxiv.org/pdf/..pdf. pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning supplementary material proof require possible generated network. assuming model trained using sufﬁcient number training samples xtrain model inﬁnite capacity model xtrain able draw sample xtrain reality xtrain possible model inﬁnite capacity. however modeled using deep neural network assume sufﬁcient capacity capture training data well. further deep neural networks able interpolate samples high dimensional spaces therefore assume large number training samples almost drawn note wish generate human faces deﬁne xall space possible faces distribution xtrain space faces made training data. then practically even well trained model learns interpolate well captures distriinterpolated versions must possible generate possible described function ensure want show function allows represent samples vaes aaes construct produce different ways. output encoder output always gaussian hence limitation produce. ensures provided encoder eaae deep neural network consisting multiple convolutional batch normalisation layers. ﬁnal layer eaae fully connected layer without activation function. input nodes fully connected layer function fi=...m means given ai=...m learned weights fully connected layer. consider three cases case complete bases possible generate one-to-one mapping provided restricted values take. case overcomplete bases holds provided restricted values take. case undercomplete bases possible generate instead many mapping. network must learn complete overcomplete bases must restricted values take network encouraged learn overcomplete bases learning large number ai’s—speciﬁcally basing network dcgan architecture —more times dimensionality using batch normalisation layers throughout network ensure values spread capturing close-to-gaussian distribution encouraging inﬁnite support. shown that certain reasonable assumptions means hence another step. therefore markov chain described transition operator deﬁned equation irreducible aperiodic necessary conditions ergodicity. figure interpolating faces using dvae. rows face original interpolation whilst second rows result steps mcmc sampling applied latent samples used generate original interpolation. qualitative difference compared vaes desaturation generated images. figure interpolating faces using daae. rows face original interpolation whilst second rows result steps mcmc sampling applied latent samples used generate original interpolation. although performs poorly regularisation effect denoising clearly seen daae applying mcmc sampling figure samples dvae daae trained svhn dataset. samples models imitate blurriness present dataset. although numbers visible initial sample dvae produce recognisable numbers initial samples steps mcmc sampling. although daae fail produce recognisable numbers ﬁnal samples still clear improvement initial samples. figure interpolating google street view house numbers using dvae. rows house number original interpolations whilst second rows result steps mcmc sampling. original interpolation produces symbols resemble numbers observed models attempt move samples towards realistic numbers interpolation -digit numbers image results meaningless blur middle interpolation. steps mcmc sampling models instead produce recognisable -digit numbers note contrast poor denoising models particular struggle recover meaningful images", "year": 2016}