{"title": "On the Performance of Network Parallel Training in Artificial Neural  Networks", "tag": ["cs.AI", "cs.NE", "cs.PF", "stat.ML"], "abstract": "Artificial Neural Networks (ANNs) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. However, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment. In this work we propose an implementation of Network Parallel Training through Cannon's Algorithm for matrix multiplication. We show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive; this point varies by network complexity. We also show through empirical efficiency calculations that the speedup obtained is superlinear.", "text": "abstract—artiﬁcial neural networks received increasing attention recent years applications span wide range disciplines including vital domains medicine network security autonomous transportation. however neural network architectures becoming increasingly complex increasing need obtain real-time results models become pivotal parallelization mechanism speeding network training deployment. work propose implementation network parallel training cannon’s algorithm matrix multiplication. show increasing number processes speeds training point process communication costs become prohibitive; point varies network complexity. also show empirical efﬁciency calculations speedup obtained superlinear. artiﬁcial neural networks anns computational tools inspired biological namesake. anns particularly useful estimating functions arbitrary complexity using given example data. inturn makes well suited pattern recognition tasks successfully applied dimensionality reduction time series prediction data mining. long fringes research community since inception recent advances computer material sciences allowed unprecedented growth computation power thus allowing possibility even sophisticated networks. drawbacks modern anns time takes train network. society becomes increasingly dependent machine learning applications real-time output on-line training neural network systems critical positive user experiences usefulness systems. thus speeding neural network training parallel algorithm propose become essential efﬁcient deployment machine learning applications. case parallelization also reinforced increase availability multicore cpus general purpose graphical processing units recent years. display suboptimal performance utilized tasks. consequently impractical modern scalability platforms clusters cloud computing infrastructure. dahl present pattern parallel training algorithm parallel training. multiple anns duplicated different processes. process trained randomly selected subset training examples. epoch process broadcasts weight updates applied anns. effect reducing training time faster convergence. approach presented similar work also node parallelization strategy communicating weights corresponding ghost neurons. strategy requires process inputs. therefore follows becomes onerous amount memory required. contribution novelty proposed approach centers memory efﬁciency using cannon’s algorithm require inputs process specialized hardware required. remainder paper organised follows section describes artiﬁcial neural networks backpropagation proposed implementation network parallel training. section sets experiments. section discusses experimental results. conclude paper section deﬁned weighted directed graph vertices likened neurons; edges synapses connections neurons. weights sensitivities e.g. highly-weighted connection contribute excitation neuron. concern fully-connected feed-forward networks. feed-forward network graph non-linear activation function output neuron previous layer weight connection neuron’s bias. choice non-linearity function gained popularity recent years relu rectiﬁed linear unit. leaky relu deﬁned cannon’s algorithm matrix multiplication compute activation output unit. shifting local matrices within cannon’s algorithm deﬁned algorithm allows partial products required calculation activations unit available within corresponding process. training neural network done stochastic gradient descent sgd. gradient loss function computed single sample. network parameters updated using expression θ−η∇l learning rate. process repeated convergence criterion satisﬁed. loss function typically distance network output ground truth label regularization i.e. given ground truth label network output regularization strength common extend mini-batch processing batch samples processed gradient update computed mean sample’s gradient. improvement momentum learning consecutive updates direction contribute momentum direction. network parallel training sometimes referred unit parallelization. divides network units within layer across different processes. simplest form process responsible calculating activation single unit layer practical setup units divided linearly load balanced manner. ﬁgure example. cannon’s algorithm popular parallelization strategy matrix multiplication. algorithm efﬁciently computes matrix multiplications shifting rows columns candidate matrices processes based deﬁned rules. algorithmm shows cannon’s algorithm candidate matrices within grid processes. context backpropagation cannon’s algorithm effectively allows communication weights nodes placed within different processes. implement neural network using message passing interface cart create function create virtual cartesian grid topology enables parallelization depicted data created artiﬁcially imposing linear relationship inputs outputs yik. inputs created sampling random numbers uniformly bias input output vector created using used barrier synchronize starting training routine immediately stored wall-clock time. done training completed. runtime data presented relative runtimes table networks cluster increasing number processors. resulting runtimes seconds shown ﬁgure together relative performance statistics table seen single layer networks runtimes decrease processes increase obtain optimal point processes; point communication cost becomes prohibitive resulting increasing runtimes. number nodes increased runtime also increases might imply additional cost redundant communication startup related unused nodes. increasing number layers increases optimal number processes gives empirical evidence runtimes linear number layers .the runtimes network increase quadratically relative layered network number nodes thus optimal number processes layer network rather surprising runtimes layered network lower single layered processes signiﬁcant margin number nodes. point require investigation. speedup curve ﬁgure shows similar results runtime plot exactly optimal process parameters. efﬁciency plot ﬁgure shows processor efﬁcient processors overall beneﬁt additional processes decreases. parallelizing neural network training cluster systems. proceedings iasted international conference parallel distributed computing networks. pdcn anaheim acta press; suri nnrr deodhare nagabhushan parallel levenbergmarquardt-based neural network training linux clusters case study. linux clusters icvgip indian conference computer vision graphics image processing ahmadabad; pethick liddle werstein huang parallelization backpropagation neural network cluster computer. international conference parallel distributed computing systems wilkinson allen parallel programming techniques applications using networked workstations parallel computers. upper saddle river prentice-hall inc.; important observation efﬁciency plots processor efﬁciency exceeds certain regions implies seedup superlinear thus meaning adding additional processes certain point actually increases speedup possibly reduction access times. implementation network parallel training using cannon’s algorithm proposed. running times speedup efﬁciency evaluated networks varying complexity. results show using strategy signiﬁcant speedup obtained. effect even pronounced complex networks. also showed regions speedup superlinear thus increasing efﬁciency resource utilization. would interesting explore different means parallelization per-unit based split weight matrices. topologies tested relatively limited size. parallelized neural networks mostly necessary networks unusually large size otherwise communication time going outweigh beneﬁts parallel execution many data dependencies. data model used simple linear function depending variable. relevant scope work implementing sophisticated means data generation would interesting next step. another aspect explore local matrix multiplication improved. perhaps ofﬂoading matrix multiplication graphics processing unit would beneﬁcial.", "year": 2017}