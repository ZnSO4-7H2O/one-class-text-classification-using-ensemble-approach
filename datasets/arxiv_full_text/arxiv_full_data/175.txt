{"title": "Sensitivity and Generalization in Neural Networks: an Empirical Study", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.  We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization $-$ such as full-batch training or using random labels $-$ correspond to lower robustness, while factors associated with good generalization $-$ such as data augmentation and ReLU non-linearities $-$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.", "text": "roman novak yasaman bahri∗ daniel abolaﬁa jeffrey pennington jascha sohl-dickstein google brain {romann yasamanb danabo jpennin jaschasd}google.com practice large over-parameterized neural networks generalize better smaller counterparts observation appears conﬂict classical notions function complexity typically favor smaller models. work investigate tension complexity generalization extensive empirical exploration natural metrics complexity related sensitivity input perturbations. experiments survey thousands models various fully-connected architectures optimizers hyper-parameters well four different image classiﬁcation datasets. trained neural networks robust input perturbations vicinity training data manifold measured norm input-output jacobian network correlates well generalization. establish factors associated poor generalization full-batch training using random labels correspond lower robustness factors associated good generalization data augmentation relu non-linearities give rise robust functions. finally demonstrate input-output jacobian norm predictive generalization level individual test points. empirical success deep learning thus eluded interpretation existing lenses computational complexity numerical optimization classical statistical learning theory neural networks highly non-convex models extreme capacity train fast generalize well. fact large networks demonstrate good test performance larger networks often generalize better counter would expected classical measures dimension. phenomenon observed targeted experiments historical trends deep learning competitions course work observation odds occam’s razor principle parsimony applied intuitive notion function complexity resolution apparent contradiction examine complexity functions conjunction input domain. seem decisively complex restrained narrow input domain appear differently remains linear function input networks behave differently close data manifold away therefore analyze complexity models capacity distinguish different inputs neighborhood datapoints words sensitivity. study simple metrics presented them norm input-output jacobian correlates generalization wide variety scenarios. figure networks trained training accuracy cifar left increasing capacity model allows overﬁtting models model maximum parameter count yields best generalization right train loss correlate well generalization best model training loss many orders magnitude higher models generalize worse observation rules underﬁtting reason poor generalization low-capacity models. similar ﬁndings case achievable training loss. work considers sensitivity context image classiﬁcation tasks. interpret observed correlation generalization expression universal prior image classiﬁcation functions favor robustness expect similar prior exist many perceptual settings care taken extrapolating ﬁndings tasks prior justiﬁed compare sensitivity identical trained networks differ single hyper further associates sensitivity generalization unrestricted manner i.e. comparing networks wide variety hyper-parameters width depth non-linearity weight initialization optimizer learning rate batch size. prior works examined measures related ones consider. particular pascanu mont´ufar raghu investigated expressive power fully-connected neural networks built piecewise-linear activation functions. functions piecewise-linear input domain number linear regions input space divided measure nonlinear function function many linear regions capacity build complex ﬂexible decision boundaries. argued upper bound number linear regions scales exponentially depth polynomially width speciﬁc construction examined. raghu derived tight analytic bound considered number linear regions generic networks random weights would appropriate instance initialization. however evolution measure training investigated before. examine related measure number hidden unit transitions along one-dimensional trajectories input space trained networks. motivation measure discussed another perspective function complexity gained studying robustness perturbations input. indeed rasmussen ghahramani demonstrate problem complexity measured number parameters limited utility model selection measuring output variation allows invocation occam’s razor. work apply related ideas large-scale practical context neural networks billion free parameters discuss potential ways sensitivity permits application occam’s razor neural networks sokolic provide theoretical support relevance robustness measured input-output jacobian generalization. derive bounds generalization terms jacobian norm within framework algorithmic robustness results provide empirical support conclusions extensive number experiments. several recent papers also focused deriving tight generalization bounds neural networks propose theoretical bounds paper establish correlation metrics generalization substantially larger experimental setting undertaken prior works. context regularization increasing robustness perturbations widely-used strategy data augmentation noise injection weight decay max-pooling indirectly reduce sensitivity model perturbations rifai sokolic explicitly penalize frobenius norm jacobian training objective. work relate several mentioned regularizing techniques sensitivity demonstrating extensive experiments improved generalization consistently coupled better robustness measured single metric input-output jacobian norm ﬁndings conﬁrm common-sense expectations others challenge intuition makes neural network robust ﬁndings demonstrates inductive bias towards robustness stochastic mini-batch optimization compared full-batch training interpreting regularizing effect inductive bias previously studied shown rigorous experiments increasing width singlehidden-layer network improves generalization analogy matrix factorization drawn motivate constraining norm weights instead count. neyshabur explored several weight-norm based measures complexity scale size model. measures frobenius norm jacobian similar nature however particular metric considered best knowledge ﬁrst evaluate large-scale setting sensitivity inputs attracted interest context adversarial examples several attacks locate points poor generalization directions high sensitivity network certain defences regularize model penalizing sensitivity employing decaying non-linearities however work adversarial examples relates highly speciﬁc perturbations similarly speciﬁc kind generalization paper analyzes average-case sensitivity typical generalization. propose simple measures sensitivity fully-connected neural network respect input assume employs piecewise-linear activation function like relu. itself composition linear piecewise-linear functions piecewise-linear splitting input space disjoint regions implementing single afﬁne mapping each. measure aspects sensitivity answering local sensitivity measure adopt frobenius norm class probabilities jacobian /∂xt /∂xj) softmax function. given points interest xtest estimate sensitivity function regions average jacobian norm refer simply jacobian norm. note require labels xtest. interpretation. frobenius norm estimates averagecase sensitivity around indeed consider inﬁnitesimal gaussian perturbation detect change linear region need able identify analogously raghu network piecewiselinear activations given input assign code neuron network identiﬁes linear region pre-activation neuron. e.g. relu unit assigned pre-activation value less greater respectively. similarly relu unit code assigned since linear regions. then concatenation codes neurons network uniquely identiﬁes linear region input given encoding scheme detect transition detecting change code. sample equidistant points closed one-dimensional trajectory count transitions along quantify number linear regions norm directional derivative ∂c/∂ amounts dirac delta function transition sampling multiple trajectories around different points estimate sensitivity metric refer simply transitions number transitions. assure sampling trajectory close data manifold construct horizontal translations image xtest pixel space similarly augment training data horizontal vertical translations corresponding experiments earlier metric require knowing label xtest. interpretation. draw qualitative parallel number transitions curvature function. measure curvature function total norm directional derivative ﬁrst derivative along path equidistant samples similar deﬁned equation quantiﬁes amount change linear regions nonbinary way. however estimating densely sampled trajectory computationallyintensive task reason instead count transitions. deﬁned sensitivity metrics describe learned function around data average. analyze measures data manifold simply measuring along circular trajectories input space intersect data manifold certain points generally away following subsections study analyzes performance large number fully-connected neural networks different hyper-parameters optimization procedures. except speciﬁed include models achieve training accuracy. allows study generalization disentangled properties like expressivity trainability outside scope work. order efﬁciently evaluate compute-intensive metrics wide range hyperparameters settings consider fully-connected networks. extending investigation complex architectures left future work. random ellipse. trajectory extremely unlikely pass anywhere near real data indicates function behaves random locations input space never encountered training. ellipse passing three training points different class trajectory pass three data points traverses images linear combinations different-class images expected outside natural image space. sensitivity function along trajectory allows comparison behavior data manifold approaches moves away three anchor points. ellipse three training points class. trajectory similar previous given dataset used experiment expected traverse overall closer data manifold since linear combinations digit likely resemble realistic image. comparing transition density along trajectory points different classes allows assessment sensitivity changes response approaching data manifold. that according jacobian norm transitions metrics functions exhibit much robust behavior around training data visualize effect figure plot transition boundaries last layer neural network training. training observe training points regions transition density. observed contrast neural network behavior near away data strengthens empirical connection draw sensitivity generalization also conﬁrms that mentioned certain quality function used model comparison input domain always accounted for. consider practical context model selection. given perfectly trained neural networks model better generalization implement less sensitive function? figure %-accurate mnist network implements function much stable near training data away left depiction hypothetical circular trajectory input space passing three digits different classes highlighting training point locations center jacobian norm input traverses elliptical trajectory. sensitivity drops signiﬁcantly vicinity training data remaining uniform along random ellipses. right transition density behaves analogously. according metrics input moves points different classes function becomes less stable moves points class. consistent intuition linear combinations different digits data manifold same-class digits §a.. experimental details. figure transition boundaries last layer -dimensional slice input space deﬁned training points left boundaries training. right training transition boundaries become highly non-isotropic training points lying regions lower transition density. §a.. experimental details. figure improvement generalization using correct labels data augmentation relus mini-batch optimization consistently coupled reduced sensitivity measured jacobian norm transitions correlate generalization considered scenarios except comparing optimizers point plot corresponds neural networks share hyper-parameters optimization procedure differ certain property indicated axes titles. coordinates along axis reﬂect values quantity title plot respective setting networks reached training accuracy cifar settings §a.. experimental details plot interpretation. figure many possible hyper-parameter conﬁgurations train models share parameters optimization procedure differ single binary setting network pairs select network reached training accuracy whole training generalization sensitivity values used coordinates point corresponding pair networks position point respect diagonal visually demonstrates conﬁguration smaller generalization lower sensitivity. perform large-scale experiment establish direct relationships sensitivity generalization realistic setting. contrast selected locations input space varied single binary parameter impacting generalization sweep simultaneously many different architectural optimization choices main result presented figure indicating strong relationship jacobian norm generalization. contrast figure app. demonstrates number transitions alone sufﬁcient compare networks different sizes number neurons networks strong inﬂuence transition count. figure jacobian norm correlates generalization considered datasets. point corresponds network trained training accuracy §a.. §a.. experimental details bottom plots respectively. established correlation sensitivity generalization averaged large test investigate whether jacobian norm predictive generalization individual points. demonstrated figure jacobian norm point predictive cross-entropy loss relationship linear even bijective particular certain misclassiﬁed points jacobian norm many orders magnitude smaller correctly classiﬁed points however remark consistent tendency points highest values jacobian norm mostly misclassiﬁed. similar noisier trend observed networks trained using -loss depicted figure observations make jacobian norm promising quantity consider contexts active learning conﬁdence estimation future research. figure jacobian norm plotted individual test point loss. plot shows random networks respective training accuracy network unique color. jacobian norm plotted cross-entropy loss. plots experimentally conﬁrm relationship established figure app.. bottom jacobian norm plotted -loss networks trained -loss exhibits similar behavior. §a.. experimental details figure app. similar observations datasets. investigated sensitivity trained neural networks input-output jacobian norm linear regions counting context image classiﬁcation tasks. presented extensive experimental evidence indicating local geometry trained function captured input-output jacobian predictive generalization many different contexts varies drastically depending close training data manifold function evaluated. established connection cross-entropy loss jacobian norm indicating remain informative generalization even level individual test points. interesting directions future work include extending investigation complex architectures machine learning tasks. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. peter bartlett dylan foster matus telgarsky. spectrally-normalized margin bounds neural networks. advances neural information processing systems anna choromanska mikael henaff michael mathieu g´erard arous yann lecun. loss surfaces multilayer networks. artiﬁcial intelligence statistics yann dauphin razvan pascanu aglar g¨ulehre kyunghyun surya ganguli yoshua benidentifying attacking saddle point problem high-dimensional non-convex optigintare karolina dziugaite daniel roy. computing nonvacuous generalization bounds deep neural networks many parameters training data. arxiv preprint arxiv. daniel golovin benjamin solnik subhodeep moitra greg kochanski john karro sculproceedings ley. google vizier service black-box optimization. sigkdd international conference knowledge discovery data mining yulei jiang richard lorenzo pesce karen drukker. study effect noise injection training artiﬁcial neural networks. neural networks ijcnn international joint conference ieee nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. jaehoon yasaman bahri roman novak schoenholz jeffrey pennington jascha sohldickstein. deep neural networks gaussian processes. international conference learning representations https//openreview.net/forum?id=bea-m-z. vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning behnam neyshabur ryota tomioka nathan srebro. search real inductive bias role implicit regularization deep learning. proceeding international conference learning representations workshop track abs/. behnam neyshabur srinadh bhojanapalli nathan srebro. pac-bayesian approach spectrally-normalized margin bounds neural networks. international conference learning representations https//openreview.net/forum?id=skz_wfbcz. nicolas papernot patrick mcdaniel somesh matt fredrikson berkay celik ananthram swami. limitations deep learning adversarial settings. security privacy ieee european symposium ieee pascanu montufar bengio. number response regions deep feed forward networks piece-wise linear activations. international conference learning representations december salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning ercan solak roderick murray-smith william leithead douglas leith carl rasmussen. derivative observations gaussian process models dynamic systems. advances neural information processing systems christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. arxiv preprint arxiv. zhang bengio hardt recht vinyals. understanding deep learning requires rethinking generalization. international conference learning representations november linear region encoding encoding linear region point described guarantees different regions obtain different codes different codes might assigned region neurons layer network saturated however probability arrangement drops exponentially width hence ignored work. equality discrete continuous versions equation becomes exact high-enough sampling density narrow linear regions missed consecutive points change line consecutive points zi+). computational efﬁciency also assume neurons transitions simultaneously extremely unlikely context random initialization stochastic optimization. figure app. depiction trajectory input space used count transitions deﬁned interpolation horizontal translations single digit results complex trajectory constrains points close translation-augmented data allows tractable estimate transition density around data manifold. metric used compare models straight lines indicate boundaries different linear regions figure app. number transitions contrast figure generally correlate generalization gap. left networks train accuracy cifar. right networks least training accuracy cifar. §a.. experimental details. figure app. jacobian norm plotted individual test point loss fashion-mnist cifar. figure plot shows random networks respective training network unique color. §a.. experimental details. occam’s razor heuristic model comparison based complexity. given dataset occam’s razor gives preference simpler models bayesian interpretation heuristic simplicity deﬁned evidence often computed using laplace approximation. assumptions evidence shown inversely proportional number parameters model. therefore given uniform prior competing hypothesis classes class posterior higher model fewer parameters. remarking models parameters spread probability mass evenly across datasets virtue able similarly suggests preferring models fewer parameters assuming evidence unimodal peaks close dataset interest. occam’s razor neural networks. seen figure reasoning apply neural networks best achieved generalization obtained model around times many parameters simplest model capable ﬁtting dataset hand murray ghahramani telgarsky demonstrate concrete examples high number free parameters model doesn’t necessarily entail high complexity. hand large body work expressivity neural networks shows ability compute complex functions increases rapidly size zhang validates also easily complex functions stochastic optimization. classical metrics like dimension rademacher complexity increase size network well. indicates weights neural network actually correspond usable capacity hence smear evidence along large space datasets making dataset interest less likely. potential issues. conjecture laplace approximation evidence simpliﬁed estimation occam’s factor terms accessible volume parameter space might hold neural networks context stochastic optimization particular account combinatorial growth accessible volume parameter space width increases similarly comparing evidence probability distributions datasets difference neural networks drastic figure app. nuanced depicted figure app. evidence ratio highly dependent particular dataset. interpret work deﬁning hypothesis classes based sensitivity hypothesis task) observing strongly non-uniform prior classes enables model comparison. indeed least context natural images classiﬁcation putting prior number parameters kolmogorov complexity hypothesis extremely difﬁcult. however statement true classiﬁcation function robust small perturbations input much easier justify. such prior favor robustness sensitivity might fare better prior speciﬁc network hyper-parameters. interpret correlation sensitivity generalization observe work. explain large networks tend converge less sensitive functions. conjecture large networks access larger space robust solutions solving highly-underdetermined system ﬁtting dataset small models converge extreme weight values overconstrained data. however investigation needed conﬁrm hypothesis. figure app. occam’s razor simpliﬁed expectation hypothesized reality. datasets input target dimensions matching particular dataset sorted according evidence large model left right along horizontal axis. left classic simpliﬁed depiction bayesian occam’s razor. evidence small model parameters narrow support dataset space peaked. model dataset well falls close peak outperforms larger model parameters wider support. right suggested potential reality neural networks. evidence small model peaks higher large model might nonetheless concentrate majority probability mass simple functions evidence curves might intersect small angle. case dataset lying close intersection models bayesian evidence ratio depends exact position respect intersection. analyze relationship jacobian norm cross-entropy loss individual test points studied target class jacobian. begin relating derivative target class probability per-point cross-entropy loss correct integer class). denote drop argument de-clutter notation jacobian expressed limit inﬁnite width fully bayesian training deep network logits distributed exactly according gaussian process similarly entry logit jacobian also corresponds independent draw gaussian process therefore plausible jacobian norm consisting square independent gaussian samples correct limits tend towards constant. figure app. jacobian norm true class output probability tightly related cross-entropy loss. point corresponds test inputs trained network cifar lines depict analytic bounds equation bottom experiment plotting full jacobian norm cross-entropy. solid lines correspond lower bound equation norm approximation equation §a.. experimental details figures app. empirical evaluation relationship multiple datasets models. experiments implemented tensorﬂow executed help vizier networks trained cross-entropy loss. networks trained without biases. computations done -bit precision. learning rate decayed factor every epochs. unless speciﬁed otherwise initial weights drawn normal distribution zero mean variance relu relu hardsigmoid; tanh hardtanh number inputs current layer. inputs normalized zero mean unit variance terms ddimensional sphere radius dimensionality input. reported values applicable evaluated whole training test sets sizes respectively. e.g. generalization deﬁned difference train test accuracies evaluated whole train test sets. applicable trajectories/surfaces input space sampled points. ﬁgures except app. plotted error bars reported quantity usually evaluated times random seeds unless speciﬁed otherwise. e.g. network said %-accurate training means randomlyinitialized networks %-accurate training. weight initialization training shufﬂing data augmentation picking anchor points data-ﬁtted trajectories selecting axes zero-centered elliptic trajectory depend random seed. random zero-centered ellipse obtained generating axis vectors normallydistributed entries zero mean unit variance sampling points ellipse given axes. random data-ﬁtted ellipse generated projecting three arbitrary input points onto plane fall vertices equilateral triangle projecting circumcircle back input space. relevant ﬁgure -layer relu-network width trained mnist steps using momentum images randomly translated wrapping pixels direction horizontally vertically well randomly ﬂipped along axis randomly rotated degrees clockwise counter-clockwise. sampling grid input space obtain projecting three arbitrary input points plane described §a.. resulting triangle centered it’s vertices distance form origin. then sampling grid points square projected back input space. relevant ﬁgures networks trained steps batch size using momentum. learning rate momentum term coefﬁcient data augmentation consisted random translation input pixels direction wrapping horizontally vertically. input also ﬂipped horizontally probability applying data augmentation network unlikely encounter canonical training data hence conﬁgurations achieved training accuracy. however veriﬁed networks trained data augmentation reached higher test accuracy analogues without ensuring generalization shrinks simply lower training accuracy. dataset networks width depth activation function {relu relu hardtanh hardsigmoid} evaluated random seeds relevant ﬁgures app.. networks trained steps random hyper-parameters; training complete checkpoint step used instead available. using l-bfgs maximum number iterations space available hyper-parameters included learning rates applicable. secondary coefﬁcients ﬁxed default values tensorﬂow implementations respective optimizers; batch sizes standard deviations initial weights multiplied default value relevant ﬁgures app.. networks either cross-entropy -loss trained steps whole datasets evaluated random subsets test images. hyper-parameters were non-linearity width depth learning rate optimizer random seed used. dataset random subset conﬁgurations among %-accurate networks plotted relevant ﬁgure app.. networks trained whole cifar training evaluated networks random test subset size hyper-parameters consisted non-linearity width depth random seed considered. single random %-accurate network drawn compare experimental measurements analytic bounds jacobian norm. time compute limitations experiment conﬁgurations small size likely evaluated however based experience smaller experiments believe bias ﬁndings paper allowing validated across wide range scenarios.", "year": 2018}