{"title": "Semi-Supervised Generation with Cluster-aware Generative Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.", "text": "many real life data sets contain small amount labelled data incorporating partial knowledge generative models straightforward risk overﬁtting towards labelled data. overﬁtting avoided ﬁnding good scheme updating parameters like introduced models semi-supervised classiﬁcation however difference optimizing model towards optimal classiﬁcation accuracy generative performance. introduce clusteraware generative model extension improves generative performances able model natural clustering higher feature representations discrete variable model trained fully unsupervised performances improved using labelled class information helps constructing well deﬁned clusters. generative model added labelled data information seen parallel humans rely abstract domain knowledge order efﬁciently infer causal model property induction labelled observations supervised deep learning models stochastic units able learn multiple levels feature abstraction. vaes however addition stochastic layers often accompanied built-in pruning effect higher layers become disconnected therefore exploited model cagem possibility learning representation higher stochastic layers model clusters data drastically reduces issue. results model able disentangle factors variation data extracts hierarchy features beneﬁcial generation phase. using labelled data points present state log-likelihood performance permutationinvariant models mnist improvement respect comparable models omniglot data set. main focus paper semi-supervised generation also show model able achieve competitive semi-supervised classiﬁcation results. deep generative models trained large amounts unlabelled data proven powerful within domain unsupervised learning. many real life data sets contain small amount labelled data points typically disregarded training generative models. propose cluster-aware generative model uses unlabelled information infer latent representation models natural clustering data additional labelled data points reﬁne clustering. generative performances model signiﬁcantly improve labelled information exploited obtaining log-likelihood nats permutation invariant mnist also achieving competitive semi-supervised classiﬁcation accuracies. model also trained fully unsupervised still improve log-likelihood performance respect related methods. variational auto-encoders rezende generative adversarial networks shown promising generative performances data complex high-dimensional distributions. approaches spawn numerous related deep generative models model data points like large unlabelled training data also semi-supervised classiﬁcation semi-supervised classiﬁcation points training data endowed class labels plethora unlabelled data aids improve supervised classiﬁcation model. variational auto-encoder deﬁnes deep generative model data depends latent variable hierarchy latent variables e.g. figure graphical representation. joint distribution two-level generative model given gaussian distributions diagonal covariance matrix typically parameterized gaussian bernoulli distribution probability distributions generative model parameterized using deep neural networks whose parameters denoted training performed optimizing evidence lower bound lower bound intractable log-likelihood obtained using jensen’s inequality inactive stochastic units common problem encountered training vaes bottom-up inference networks given called inactive units higher layers stochastic variables -layer model example vaes often learn i.e. variational approximation uses information coming data point rewrite elbo introduced variational distribution approximation model’s posterior distribution deﬁned bottom-up dependency structure variable model depends variable hierarchy similar generative model mean diagonal covariance gaussian distributions deﬁning inference network parameterized deep neural networks depend parameters figure graphical representation. learn parameters jointly maximizing elbo stochastic gradient ascent using monte carlo integration approximate intractable expectations computing variance gradients reparameterization trick using ideas chen notice inactive units layers stochastic units justiﬁed poor local maxima also modelling point view. chen give bits-back coding interpretation variational inference generative model form data stochastic units paper shows decoder powerful enough explain structure data convenient model incur extra optimization cost kl||p]. inactive units -layer therefore seen caused ﬂexible distribution able explain structure data without using information making inference network reverse dependencies among random variables generative model results bottom-up inference network performs feature extraction fundamental learning good representation data. starting data construct higher levels abstraction ﬁrst variables ﬁnally variable includes global information used generative model. order make higher representation expressive skip-connection however fundamental improve performances model. clear want exploit power additional stochastic layers need deﬁne beneﬁts encoding meaningful information greater cost ||pθ] model pay. discuss below achieve aiding generative model representation learning. hierarchical models parameterized deep neural networks ability represent ﬂexible distributions. however previous section seen units higher stochastic layers often become inactive. show help model exploit higher stochastic layers explicitly encoding useful representation i.e. ability model natural clustering data also needed semi-supervised generation. favor higher-level global information extending generative model discrete variable representing choice different clusters data. joint distribution computed marginalizing model cluster-aware generative model call figure graphical representation. introduced categorical distribution cat) depends solely needs therefore stay active model able represent clusters data. dependence also represent cluster-dependent information. maximize jointly updating stochastic gradient ascent parameters generative model variational approximation. computing gradients summation performed analytically whereas intractable expectations main goal learn representations lead good generative performance interpret classiﬁcation additional labelled data secondary task aids learning feature space easily separated clusters. form semi-supervised clustering know data points belong cluster free learn data manifold makes possible. optimal features classiﬁcation task could different representations learned generative task. important update parameters distributions generative model inference model using labelled data information. done carefully model could prone overﬁtting towards labelled data. deﬁne subset containing parameters subset containing parameters represent incoming arrows figure update parameters jointly maximizing objective {xu} unlabelled training points labelled ones standard categorical cross-entropies classiﬁers respectively. notice consider crossentropies function meaning gradients cross-entropies respect parameters distributions depend labelled data match relative magnitudes between elbo cross-entropies done numbers unlabelled labelled data points scaling constant. applications class label information data points training set. following show cagem provides natural exploit additional labelled data improve performance generative model. notice semi-supervised generation approach differs traditional semisupervised classiﬁcation task uses unlabelled data improve classiﬁcation accuracies case fact labelled data supports generative task. nevertheless experiment cagem also leads competitive semi-supervised classiﬁcation performances. evaluate cagem computing generative loglikelihood performance mnist omniglot datasets. model parameterized feedforward neural networks linear layers figure visualizations cagem- -dimensional space. middle plot shows latent space generate random samples class conditional random samples mesh grid relative placement samples scatter plot corresponds digit mesh grid. bernoulli distributed outputs simply deﬁne feedforward neural network sigmoid activation function output. dense layers rectiﬁed linear unit non-linearity batch-normalization collect statistics batch-normalization unlabelled inference. log-likelihood experiments apply temperature kl-terms ﬁrst epochs training stochastic layers deﬁned -layered neural feed-forward networks respectively units layer. training performed using adam optimizer initial learning rate annealing every epochs. experiments implemented theano lasagne parmesan. datasets report unsupervised semisupervised permutation invariant log-likelihood performance mnist also report semi-supervised classiﬁcation errors. input data dynamically binarized elbo evaluated taking importanceweighted samples denoted evaluate performance cagem different numbers labelled samples referred cagem-labels. used labelled data randomly sampled evenly across class distribution. experiments across datasets architecture. table shows generative log-likelihood performances different variants cagem mnist data set. labelled samples better generative performance even though results directly comparable since cagem exploits small fraction supervised information using labelled samples cagem model achieves state log-likelihood performance permutation invariant mnist simple layered model. also trained adgm- maaløe order make fair comparison generative log-likelihood semi-supervised setting reached performance nats. indicates models highly optimized improving semi-supervised classiﬁcation accuracy suboptimal choice generative modeling. cagem could beneﬁt usage nonpermutation invariant architectures suited image data autoregressive decoders used vlae fully unsupervised cagem- results show deﬁning clusters higher stochastic units achieve better performances closely related iwae lvae models. ﬁnally interesting table cagem- performs well even number clusters different number classes labelled data set. table test log-likelihood permutation invariant non-permutation invariant mnist. denotes number stochastic layers number importance weighted samples used inference number predeﬁned clusters used. adding labelled information compared lvae plots variable cagem- cagem- shown figure cagems encode clustered information higher stochastic layer. forms less class-dependent clusters compared semi-supervised cagem- latent space nicely separated clusters. regardless labelled information added inference cagem manages activate high amount units cagem obtain kl||p] nats lvae stochastic layers obtains nats. generative model cagem enables random samples sampling class variable feeding class conditional samples ﬁxing figure shows generation mnist digits cagem- images generated applying linearly spaced mesh grid within latent space performing random generations conditional generations generating samples cagem clear latent units capture different modalities within true data distribution namely style class. regardless fact cagem designed optimize semi-supervised generation task model also used classiﬁcation using classiﬁer table show semi-supervised classiﬁcation accuracies obtained cagem comparable omniglot dataset consists different alphabets handwritten characters character sparsely represented. task alphabets cluster information representation divide correspondingly. table improvement table semi-supervised test error benchmarks mnist randomly chosen evenly distributed labelled samples. experiment times different labelled subsets reported accuracy mean value. comparable architectures however performance reported auto-regressive models indicates alphabet information strong dataset like mnist. also indicated accuracy cagem- reaching performance samples model found figure stochastic units stays active modeling performances greatly beneﬁt capturing natural clustering data. recent works presented alternative methods mitigate problem inactive units training ﬂexible models deﬁned hierarchy stochastic layers. burda used importance samples improve tightness elbo showed training objective helped activating units -layer vae. sønderby trained ladder variational autoencoders composed layers stochastic units using top-down inference network forces information higher stochastic layers. contrarily bottom-up inference network cagem top-down approach used lvaes enforce clear separation role stochastic unit proven fact encode class information. longer hierarchies stochastic units unrolled time found sequential setting applications problem inactive stochastic units appears using powerful autoregressive decoders mitigated fact data information enters model time step. discrete variable cagem introduced able deﬁne better learnable representation data helps activating higher stochastic layer. combination discrete continuous variables deep generative models also recently explored several authors. maddison jang used continuous relaxation discrete variables makes possible efﬁciently train model using stochastic backpropagation. introduced gumbel-softmax variables allow sacriﬁce log-likelihood performances avoid computationally expensive integration rolfe presents class probabilistic models combines undirected component consisting bipartite boltzmann machine binary units directed component multiple layers continuous variables. traditionally semi-supervised learning applications deep generative models variational auto-encoders generative adversarial networks shown that whenever small fraction labelled data available supervised classiﬁcation task beneﬁt additional unlabelled data work consider semi-supervised problem different perspective show generative task cagem beneﬁt additional labelled data. by-product model however also obtain competitive semi-supervised classiﬁcation results meaning cagem able share statistical strength generative classiﬁcation tasks. modeling natural performance cagem could improved using powerful autoregressive decoders ones also even ﬂexible variational approximation could obtained using auxiliary variables normalizing ﬂows work shown perform semisupervised generation cagem. showed cagem improves generative log-likelihood performance similar deep generative approaches creating clusters data higher latent representations using unlabelled information. cagem also provides natural reﬁne clusters using additional labelled information improve modelling power. entropy distribution kl-divergence. expected log-likelihood simply baseline entropy data generating distribution minus deviation data generating distribution model distribution. latent variable model choose ignore latent variables happens expression falls back log-likelihood without latent variables. therefore condition advantageous model latent variables argument also used understand harder additional layers latent variables become active. two-layer latent variable model variational distribution decompose likelihood bound using ppplat expression falls back one-layer model whether second layer stochastic units depend upon potential diminishing return terms likelihood relative extra kl-cost approximate posterior. thank ulrich paquet fruitful feedback. research supported danish innovation foundation nvidia corporation donation titan gpus. marco fraccaro supported microsoft research scholarship programme. references bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning workshop neural information processing systems bengio yoshua courville aaron vincent pascal. representation learning review perspectives. ieee transactions pattern analysis machine intelligence goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems. ishaan kumar kundan ahmed faruk taiga adrien visin francesco vazquez david courville aaron. pixelvae latent variable model natural images. arxiv e-prints november ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning kingma diederik rezende danilo jimenez mohamed shakir welling max. semi-supervised learning proceedings deep generative models. international conference machine learning kingma diederik salimans jozefowicz rafal chen sutskever ilya welling max. improved variational inference inverse autoregressive ﬂow. advances neural information processing systems. burda yuri grosse roger salakhutdinov ruslan. accurate conservative estimates loglikelihood using reverse annealing. proceedings international conference artiﬁcial intelligence statistics chen kingma diederik salimans duan dhariwal prafulla schulman john sutskever ilya abbeel pieter. variational lossy autoencoder. international conference learning representations lake brenden salakhutdinov ruslan tenenbaum josh. one-shot learning inverting compositional causal process. advances neural information processing systems. maaløe lars sønderby casper sønderby søren winther ole. auxiliary deep generative models. proceedings international conference machine learning fraccaro marco sønderby søren kaae paquet ulrich winther ole. sequential neural models advances neural information stochastic layers. processing systems. rasmus antti berglund mathias honkala mikko valpola harri raiko tapani. semi-supervised advances neural learning ladder networks. information processing systems sønderby casper kaae raiko tapani maaløe lars sønderby søren kaae winther ole. ladder variational autoencoders. advances neural information processing systems", "year": 2017}