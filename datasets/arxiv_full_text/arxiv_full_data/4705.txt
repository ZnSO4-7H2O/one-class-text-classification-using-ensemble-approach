{"title": "Safe Exploration of State and Action Spaces in Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.", "text": "paper consider important problem safe exploration reinforcement learning. reinforcement learning well-suited domains complex transition dynamics high-dimensional state-action spaces additional challenge posed need safe eﬃcient exploration. traditional exploration techniques particularly useful solving dangerous tasks trial error process lead selection actions whose execution states result damage learning system consequently agent begins interaction dangerous high-dimensional state-action space important question arises; namely avoid damage caused exploration state-action space. introduce pi-srl algorithm safely improves suboptimal albeit robust behaviors continuous state action control tasks eﬃciently learns experience gained environment. evaluate proposed method four complex tasks automatic parking pole-balancing helicopter hovering business management. reinforcement learning type machine learning whose main goal ﬁnding policy moves agent optimally environment generally formulated markov decision process many methods used important complex tasks tasks focused maximizing long-term cumulative reward researchers paying increasing attention long-term reward maximization also safety approaches sequential decision problems well-written reviews matters also found nevertheless important ensure reasonable system performance consider safety agent application dangerous tasks exploration techniques oﬀer guarantees issues. thus using techniques dangerous control tasks important question arises; namely ensure exploration state-action space cause damage injury while time learning optimal policies? matter words ensuring agent able explore dangerous environment safely eﬃciently. many domains exploration/exploitation process lead catastrophic states actions learning agent helicopter hovering control task case involving high risk since policies crash helicopter incurring catastrophic negative reward. exploration/exploitation high probability random action selection). another example found portfolio theory analysts expected portfolio maximizes proﬁt avoiding risks considerable losses since maximization expected returns necessarily prevent rare occurrences large negative outcomes diﬀerent criteria safe exploration needed. exploration process policies evaluated must conducted extreme care. indeed environments method required explores state-action space safe manner. paper propose policy improvement safe reinforcement learning algorithm safe exploration dangerous continuous control tasks. method requires predeﬁned baseline policy assumed suboptimal predeﬁned baseline policies used diﬀerent ways approaches. work koppejan whiteson singlelayers perceptrons evolved albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software approach viewed simple form population seeding proven advantageous numerous evolutionary methods work mart´ın lope weights neural networks also evolved inserting several baseline policies initial population. minimize possibility evaluating unsafe policies approach prevents crossover mutation operators permitting anything tiny changes initial baseline policies. paper present pi-srl algorithm novel approach improving baseline policies dangerous domains using pi-srl algorithm composed different steps. ﬁrst baseline behavior approximated using behavioral cloning techniques order achieve goal case-based reasoning techniques used successfully applied imitation tasks past second step pi-srl algorithm attempts safely explore state-action space order build accurate policy previously-learned behavior. thus cases obtained previous phase improved safe exploration state-action space. perform exploration small amounts gaussian noise randomly added greedy actions baseline policy approach. exploration strategy used successfully previous works capable producing safe actions supposedly risky states addition present deﬁnition risk based agent unknown known space. described section greater detail deﬁnition completely diﬀerent traditional deﬁnitions risk found literature paper also reports experimental results obtained application approach four diﬀerent domains automatic parking pole-balancing competition helicopter hovering business management domain propose learning near-optimal policy which learning phase minimize crashes pole disequilibrium helicopter crashes company bankruptcies respectively. important note comparison approach agent optimal exploration policy possible since proposed domains know optimal exploration policy regarding organization remainder paper section introduces deﬁnitions section describes detail learning approach proposed. section evaluation performed four mentioned domains presented. section discusses related work section summarizes main conclusions study. sections term return used refer expected cumulative future discounted γtrt term reward used refer single real value used illustrate concept safety used approach navigation problem presented figure navigation problem presented figure control policy must learned particular start state goal state given demonstration trajectories. environment assume task diﬃcult stochastic complex dynamic environment stochasticity makes impossible complete task using exactly trajectory every time. additionally problem supposes demonstrations baseline controller performing task also given. demonstrations composed diﬀerent trajectories covering well-deﬁned region state space approach based addition small amounts gaussian noise perturbations baseline trajectories order better ways completing task. noise aﬀect baseline trajectories diﬀerent ways depending amount noise added which turn depends amount risk taken. risk desired noise added baseline trajectories consequently improved behavior discovered however intermediate level risk desired small amounts noise added baseline trajectories trajectories complete task discovered. cases exploration trajectories leads robot unknown regions state space robot assumed able detect situations risk function baseline behavior return safe known states. instead high risk desired large amounts noise added baseline trajectories leading discovery trajectories iteration process leads robot progressively safely explore state action spaces order improved ways complete task. degree safety exploration however depend risk taken. paper follow notation presented geibel deﬁnition concept risk. study geibel associate risk error states non-error states former understood state considered undesirable dangerous enter. terms agent enters error state current episode ends damage learning system whereas enters non-error state episode ends normally without damage. thus geibel deﬁne risk respect policy probability state sequence generated execution policy terminates error state deﬁnition states risk assume continuous n-dimensional state space state vector real numbers dimension individual domain similarly assume continuous m-dimensional action space action vector real numbers dimension additionally agent considered endowed individual domain memory case-base size memory element represents state-action pair case agent experienced before. consists state-action pair agent experienced past associated value thus ﬁrst element represents case’s problem part corresponds state following element depicts case solution ﬁnal element value function associated state state composed continuous state variables action composed continuous action variables. agent receives state ﬁrst retrieves nearest neighbor according given similarity metric performs associated action. paper euclidean distance similarity metric euclidean distance metric useful value function expected continuous smooth throughout state space however since value function unknown priori euclidean distance metric particularly suitable many problems many researchers begun distance metric learn adapt order achieve better results distance metric learning techniques would certainly desirable order induce powerful distance metric speciﬁc domain consideration lies outside scope present study. paper therefore focused domains euclidean distance proven successful pole-balancing helicopter hovering control simba traditionally case-based approaches density threshold order determine case added memory. distance nearest neighbor greater case added. sense parameter deﬁnes size classiﬁcation region case case within classiﬁcation region case considered known state. hence cases describe case-based policy agent deﬁnition given case-base composed cases density threshold state considered known min≤i≤η unknown cases. formally known states unknown states figure graphically represents relationship known/unknown error/nonerror states. green area image denotes safe case-based policy learnt area state space corresponding initial known space. agent following policy always green area resulting episodes without damages. consequently subset non-error states also form part known space. formally subsets non-error states belonging known unknown spaces respectively yellow area figure well subset remaining non-error states. formally area) order explore improved behaviors avoiding error states process adjusting known space space used safe better policies algorithm forget ineﬀectual known states shown section present subsection advantages using teacher knowledge namely provide initial knowledge task learned support exploration process highlighted. furthermore explain believe knowledge indispensable tackling highly complex realistic problems large continuous state action spaces particular action result undesirable consequence. strategy results random exploration state action spaces gather knowledge task. enough information discovered environment algorithm’s behavior improve. random exploration policies however waste signiﬁcant amount time exploring irrelevant regions state action spaces optimal policy never encountered. problem compounded domains extremely large continuous state action spaces random exploration never likely visit regions spaces necessary learn optimal policies. additionally many real tasks real robots random exploration gather information environment cannot even applied. real robots considered suﬃcient information much information real robot gather environment. finally impossible avoid undesirable situations high-risk environments without certain amount prior knowledge task random exploration would require undesirable state visited labeled undesirable. however visits undesirable states result damage injury agent learning system external entities. consequently visits states avoided earliest steps learning process. mitigating diﬃculties described above ﬁnite sets teacher-provided examples demonstrations used incorporate prior knowledge learning algorithm. teacher knowledge used general ways either bootstrap learning algorithm derive policy examples. ﬁrst case learning algorithm provided examples demonstrations bootstrap value function approximation lead agent relevant regions space. second teacher knowledge used refers learning demonstration approaches policy derived ﬁnite demonstrations provided teacher. principal drawback approach however performance derived policy heavily limited teacher ability. circumvent diﬃculty improve performance exploring beyond provided teacher demonstrations raises question agent encounters state demonstration exists furnishing agent initial knowledge helps mitigate problems associated random exploration alone suﬃcient prevent undesirable situations arise subsequent explorations undertaken improve learner ability. additional mechanism necessary guide subsequent exploration process agent kept away catastrophic states. paper teacher rather policy derived current value function approximation used selection actions unknown states. prevent agent encountering unknown states exploration process would requesting beginning teacher demonstration every state state space. however strategy possible computational infeasibility given extremely large number states state space fact teacher forced give action every state given many states ineﬀectual learning optimal policy. consequently pi-srl requests teacher action action actually required paper supposes teacher available task learned teacher taken baseline behavior. although studies examined robotic teachers hand-written control policies simulated planners great majority date made human teachers. paper uses suboptimal automatic controllers teachers taken teacher’s policy. deﬁnition policy considered baseline behavior three assumptions made able provide safe demonstrations task learnt prior knowledge extracted; able support subsequent exploration process advising suboptimal actions unknown states reduce probability entering error states return system known situation; performance optimal. optimal baseline behaviors certainly ideal behave safely non-optimal behaviors often easy implement generate optimal ones. pi-srl algorithm uses baseline behavior diﬀerent ways. first uses safe demonstrations provide prior knowledge task. step algorithm builds initial known space agent derived safe case-based policy purpose mimicking second step pi-srl uses support subsequent exploration process conducted improve abilities previously-learnt exploration process continues action requested required agent unknown state step acts backup policy case unknown state intention guiding learning away catastrophic errors least reducing frequency. important note baseline behavior cannot demonstrate correct action every possible state. however baseline behavior might able indicate best action cases action supplies should least safer obtained random exploration. order maximize exploration safety seems advisable movement state space arbitrary rather known space expanded gradually starting known state. exploration carried perturbation state-action trajectories generated policy perturbation trajectories accomplished addition gaussian random noise actions order obtain ways completing task. thus gaussian exploration takes place around current approximation action current known state min≤j≤η action performed sampled gaussian distribution mean action output given instance selected denotes algorithm action output probability selecting action shape gaussian distribution depends parameter study used width parameter. large values imply wide bellshaped distribution increasing probability selecting actions diﬀerent current action small value implies narrow bell-shaped distribution increasing probability selecting actions similar current action assume hence value directly related amount perturbation added state-action trajectories generated policy higher values imply greater perturbations greater probability visiting unknown states. deﬁnition parameter considered risk parameter. large values increase probability visiting distant unknown states hence increase probability reaching error states. exploratory actions drive agent edge known space force slightly beyond unknown space search better safer behaviors. period time execution exploratory actions increases known space improves abilities previously-learned safe case-based policy risk parameter well design parameters must selected user. section guidelines selection oﬀered. nearby states similar optimal actions. continuous state spaces impossible agent visit every state store value table. generalization techniques needed. large smooth state spaces similar states expected similar values similar optimal actions. therefore possible experience gathered environment limited subset state space produce reliable approximation much larger subset must also note that proposed domains optimal action also considered safe action sense never produces error states similar actions similar states tend produce similar eﬀects. considering deterministic domain action performed state always produces state st+. stochastic domain understood intuitively execution explained earlier present study uses euclidean distance similarity metric proven successful proposed domains. result assumption approximation techniques used actions generate similar eﬀects continuous action spaces need grouped together action generalization techniques even greater paper assumption also allows assume values increase probability visiting known states hence exploring less taking less risks greater values increase probability reaching error states. ﬁrst step pi-srl approach behavioral cloning using allow software agent behave similar manner teacher policy whereas approaches named diﬀerently according learned prevent terminological inconsistencies here consider behavioral cloning area whose goal reproduction/mimicking underlying teacher policy using behavioral cloning case built using agent’s state received environment well corresponding action command performed teacher. pi-srl objective ﬁrst step properly imitate behavior using cases stored case-base. point important question arises; namely case-base learnt using sample trajectories provided that learning process resulting policy derived mimics behavior baseline behavior function maps states actions words function that given state provides corresponding action paper want build policy derived case-base composed cases that state case minimum euclidean distance dist retrieved corresponding action returned. intuitively assumed built simply storing cases gathered interaction environment limited number episodes episodes expects resulting able properly mimic behavior however informal experimentation helicopter hovering domain shows case helicopter hovering episodes prohibitive number cases stored policy derived case-base unable correctly imitate baseline behavior instead continuously crashes helicopter. indeed order mimic large continuous stochastic domains approach requires larger number episodes consequently prohibitive number cases. fact perfectly mimic domains inﬁnite number cases would required. figure attempts explain believe learning process work. region space represented simply storing cases derived form shown. stored case covers area space represents centroid voronoi region. previously-learned policy used state presented action performed corresponding case euclidean distance dist less stored cases. however policy provide action situation action provided diﬀerent point policy said classify state obtained class policy said classify state desired class furthermore understood case-base stored possible pairs classiﬁcation error. able generate domain actions would always identical dist however stochastic large continuous domain leads visiting unexplored regions case space respect closest case unexplored regions visited diﬀerence obtained class derived desired class derived large probability error states might visited initialized value case computed second step algorithm section additionally step uses case-based risk function determine whether state considered risky state risky -nearest neighbor strategy followed otherwise algorithm performs action using baseline behavior case cnew built added case-base starting empty case-base learning algorithm continuously increases competence storing experiences. however number reasons inﬂow cases limited. large case-bases increase time required closest cases example. partially solved using techniques reduce retrieval time nevertheless reduce storage realization retrieval within certain amount time cannot guaranteed removal cases inevitable. eﬃcient approach problem removal least-frequently-used elements result step constrained case-base describing safe case-based policy mimics baseline behavior though perhaps deviation formally estimate utility baseline behavior computed averaging rewards accumulated trials. then shown figure returns state accumulated averaged following policy derived case base important note algorithm term return following ﬁrst occurrence refers expected return whereas returns refers list composed return diﬀerent episodes. principal reasons using method allows quickly easily estimate state values successful wide variety domains state-value function randomly added actions policy complete task. algorithm used improve baseline behavior learned previous step depicted figure algorithm composed four steps performed episode. distance metric equation order determine perceived degree risk state case-based risk function used case action performed computed using equation case cnew built added list cases occurred episode important note case built replacing action corresponding closest case action resulting application random state unknown states action performed suggested baseline behavior deﬁnes safe behavior case built added list cases episode actions performed using agent known state. finally reward obtained episode accumulated immediate reward obtained action performed state computing state-value function unknown states. step state-value function states considered unknown previous step computed. previous step state-value function states algorithm proceeds manner similar ﬁrst-visit algorithm figure case return unknown state computed averaged since episode considered return computed taking account ﬁrst visit state episode although state could appear multiple times rest episode. updating cases using experience gathered. updates made cases gathered episodes cumulative reward similar best episode found point using threshold good sequences provided updates since shown sequences experiences cause adaptive agent converge stable useful policy whereas sequences cause agent converge unstable policy also prevents degradation initial performance computed ﬁrst step algorithm episodes episodes errors updates. step types updates appear namely replacements additions cases. again algorithm iterates case listcasesepisode known state compute case corresponding state note case listcasesepisode built line algorithm replacing action corresponding case turn could potentially lead higher return thus better policy. hasselt wiering also update value function using actions potentially lead higher return. error positive considered good selection reinforced. algorithm reinforcement carried updating output error positive. similar linear reward-inaction update learning automata sign error used measure success. pi-srl updates case-base actual improvements observed thus avoiding slow learning plateaus value space errors small. shown empirically procedure result better policies step size depends size error important note replacements produce smooth changes form updating understood risk-seeking approach overweighting transitions successor states promise above-average return additionally prevents degradation ensuring replacements made action potentially lead higher instead known state case added finally algorithm removes cases necessary complex scoring metrics calculate cases removed given moment proposed several authors. forbes andres suggest removal cases contribute least overall approximation driessens ramon pursue error-oriented view propose deletion cases contribute prediction error examples. principal drawback sophisticated measures complexity. determination case removed involves computation score value turn requires least retrieval regression respectively entire repeated sweeps case-base entail enormous requiring computation k-nearest neighbors around approaches well-suited systems learning adjusted time requirements high-dimensional state space requiring larger case-bases proposed here. rather paper propose removal least-frequently-used cases. idea seems intuitive insofar least-frequently-used cases usually contain worse estimates corresponding state’s value; although strategy might lead function approximator forgets valuable experience made past despite this pi-srl performs successfully domains proposed using strategy demonstrated section thus ability forget ineﬀectual main diﬃculties applying pi-srl algorithm given problem decide appropriate parameter values threshold risk parameter update threshold maximum number cases incorrect value parameter lead mislabeling state known really unknown potentially leading damage injury agent. case risk parameter high values continuously result damage injury; values safe allow exploration state-action space suﬃcient reaching near-optimal policy. unlike parameter related risk instead directly related performance algorithm. parameter used determine good episode must respect best episode obtained since best episodes used update case-base value large episodes used update instead number updates insuﬃcient improving baseline behavior. finally high value allows large case-bases increasing computational eﬀort retrieval degrading eﬃciency system. contrast value might excessively restrict size case-base thus negatively aﬀect ﬁnal performance algorithm. subsection solid perspective given automatic deﬁnition parameters. parameter setting proposed taken suitable heuristics tested successfully wide variety domains actions. paper value parameter established computing mean distance states execution baseline behavior expressed another execution policy provides state-action sequence form thus value accidents important note pi-srl completely safe ﬁrst step algorithm executed. however proceeding performance algorithm heavily limited abilities baseline behavior. running subsequent exploratory process inevitable learner performance improved beyond baseline behavior. since agent operates state incomplete knowledge domain dynamic inevitable exploratory process unknown regions state space visited agent reach error state. however possible adjust risk parameter determine level risk assumed exploratory process. paper start values gradually increase. speciﬁcally propose beginning case-base estimated maximum number cases required properly mimic baseline behavior follows description value computed. figure presents trajectories followed baseline policy three diﬀerent domains deterministic slightly stochastic highly stochastic. domain diﬀerent sequences states produced represented sn}. smn} i-th state initial state ﬁnal state resulting trajectory episode deterministic domain diﬀerent executions always result trajectory. case maximum number cases cases computed episode stored. ﬁrst trajectory produced domain stored case-base. furthermore domain execute diﬀerent episodes obtaining diﬀerent trajectories. following execution episodes compute maximum distance i-th state ﬁrst trajectory i-th state produced trajectory max≤j≤m slightly stochastic domain maximum distance exceed threshold case max≤j≤m point assume i-th state trajectory least neighbor distance less thus i-th state added case-base. contrast highly stochastic domain maximum distance greatly exceeds threshold cases max≤j≤m domain estimate total number cases added case-base following important remember deterministic domain summation equation equal that therefore increase value element related increase stochasticity environment insofar greater stochasticity environment increases number cases required. finally number cases large nearly inﬁnite threshold increased make restrictive addition cases case-base. however increase also adversely aﬀect ﬁnal performance algorithm. section presents experimental results collected pi-srl policy learning four diﬀerent domains presented order increasing complexity parking problem pole-balancing helicopter hovering business simulator simba domains proposed learning near-optimal policy minimizes accidents pole disequilibrium helicopter crashes company bankruptcies respectively learning phase. four domains stochastic experimentation. helicopter hovering business simulator simba themselves stochastic additionally generalized domains made parking pole-balancing domains stochastic intentional addition random gaussian noise actions reward function. results pi-srl four domains compared yielded additional techniques namely evolutionary approach selected winner helicopter domain competition geibel wysotzki’s risk-sensitive approach evolutionary approach several neural networks cloning error-free teacher policies added initial population indeed winner helicopter domain agent highest cumulative reward winner must also indirectly minimize helicopter crashes insofar incur large catastrophic negative rewards. hand risk-sensitive approach deﬁnes risk probability reaching terminal error state starting initial parameter determines inﬂuence π-values compared ρπvalues. corresponds computation minimum risk policies. large values original value function multiplied dominates weighted criterion. geibel wysotzki consider ﬁnite action sets study algorithm adapted continuous action sets. value risk function approximation gaussian exploration around current action. experiments domain three diﬀerent values used modifying inﬂuence -values compared ρ-values. cases goal improve control policy while time minimizing number episodes agent damage injury. domain establish diﬀerent risk levels modifying risk parameter values according procedure described subsection important note baseline behavior used initialize evolutionary approach exactly used subsequently ﬁrst second step pi-srl. furthermore case-base risk-sensitive approach begin scratch since initialized safe case-based policy makes comparison performances fair possible taking account diﬀerent techniques make baseline behaviors. parking problem represented figure originates literature represented rectangle figure initially located inside bounded area represented dark solid lines referred driving area. goal learning agent navigate initial position garage entirely inside minimum number steps. cannot move outside driving area. figure shows possible paths take starting point garage obstacle order correctly perform task. consider optimal policy domain reaches goal state shortest time which time free failures. state space domain described three continuous variables namely coordinates center angle car’s axis coordinate system. modeled essentially control inputs speed steering angle suppose controlled steering angle thus action space described center center garage normalizing function scaling euclidean distance dist range inside garage agent receives reward whenever hits wall obstacle. steps receive reward thus diﬃculty problem lies reinforcement delay also fact linear velocity maximum steering angle simulation time step. gaussian noise added actions rewards standard deviation since noisy interactions inevitable real-world applications. adding noise actuators environment transform deterministic domain stochastic domain. important note noise added transform domain stochastic domain independent gaussian noise standard deviation used explore state action space second step pi-srl algorithm. case gaussian noise standard deviation used exploration added noise previously added actuators. paper initial position ﬁxed goal position domain designed baseline behavior average cumulative reward trial order perform pi-srl algorithm modeling baseline behavior step executed. result step safe case-based policy learned demonstrations provided baseline behavior computed following procedure described subsection resulting values respectively. figure parking task modeling baseline behavior step number steps trial executed case base baseline behavior cumulative reward trial baseline behavior learned safe case based policy figure graphically represents execution modeling baseline behavior step. diﬀerent learning processes presented number steps trial executed baseline behavior cases shown. beginning learning process empty case-base steps performed using baseline behavior learning process continues cases added safe case-based policy learned. around trials practically steps performed using cases rarely used means safe case-based policy learned. learning processes shown figure modeling baseline behavior step performed without collisions wall obstacle. words baseline behavior cloned safely without errors. figure shows cumulative reward three diﬀerent execution processes ﬁrst corresponding performance baseline behavior second corresponding previously-learned safe case-based policy third corresponding instancebased learning approach consisting storing cases memory. approach items classiﬁed examining cases stored memory determining similar case given particular similarity metric classiﬁcation nearest neighbor taken classiﬁcation item using -nearest neighbor strategy approach diﬀerent executions carried out. approach training process performed saving training cases produced baseline behavior trials figure shows safe case-based policy almost perfectly mimics behavior baseline behavior domain performance approach also similar. figure improving learned baseline behavior step parking problem cumulative reward episode diﬀerent risk conﬁgurations obtained pi-srl. cumulative reward episode evolutionary risksensitive approaches. cases episode ending failure marked. although failures produced performance nevertheless weak constant throughout whole learning process. additional experiments demonstrated increasing value increases number failures without improving performance. figure shows results evolutionary risk-sensitive approaches diﬀerent values. regarding former number failures higher obtained pi-srl approach ﬁnal performance similar. case latter performance higher agent consistently crashes wall. figure shows mean number failures cumulative reward approach trials circles corresponding pi-srl algorithm black triangles risk-sensitive approach blue square evolutionary approach. additionally figure shows asymptotes. horizontal asymptote established according cumulative reward obtained highest value. horizontal asymptote indicates higher values increase number failures without improving cumulative reward vertical asymptote ailures indicates reducing risk parameter reduce number failures. figure also shows performance additional risk levels figure using level risk additional random gaussian noise added actions algorithm free failures although performance improve respect safe case-based policy learned ﬁrst step algorithm. pi-srl medium level risk also free principle assume points figure corresponding pi-srl solutions save pi-srl high level risk pareto frontier since points strictly dominated solution domain solution pi-srl medium level risk strictly dominates nevertheless important note ultimate decision approach figure best depends criteria researcher. instance minimization number failures deemed important optimization criterion best approach pi-srl level risk similarly area) error states unknown states non-error states pi-srl adapts known space order safer better policies complete task. figure shows initial situation robust sense never results collisions suboptimal learning process progresses pi-srl ﬁnds shorter path park garage along upper side obstacle comes closer obstacle figure pi-srl ﬁnds even shorter path time along lower side obstacle. however still cases case-base corresponding older path along upper side obstacle indicates paths park car). finally figure cases corresponding suboptimal path along upper side obstacle removed replaced cases corresponding safe improved path along lower side obstacle. words pi-srl adapts known space exploration unknown space order improved behaviors. process adjusting known space following experiment becomes apparent domain noisy enough even taking risk agent could nevertheless perform poorly constantly produce collisions. experiment also serves explain domain noise never suﬃcient eﬃcient exploration space without action selection noise. experiment intentionally added noise actuators performed second step pi-srl again however time taking risk test added random gaussian noise standard deviation rather standard deviation used previously actuators. figure shows executions second step pi-srl algorithm x-axis indicating number trials y-axis cumulative reward episode failures marked blue experiments figure case-based policy level triangles. risk never produces failures. contrast experiments shown figure case-based policy continually collides wall although risk parameter furthermore increase performance also detected. increase noise actuators second step algorithm respect ﬁrst step takes agent beyond known space case-base learnt ﬁrst step pi-srl allows trajectories parking garage. situation exploration process guided follows. known state reached agent performs action retrieved without addition gaussian noise since risk parameter unknown state reached agent performs action advised baseline behavior using exploration process better trajectory found parking garage resulting cases episode corresponding unknown states added case-base slightly improving performance figure important note replacements cases change actions since replaced action previously retrieved plus certain amount gaussian noise standard deviation nevertheless given risk parameter actions retrieved case-base replaced. exploration process however lead optimal behavior since additional experiments demonstrate pi-srl behaves much worse higher value noise used actuators assume taking risk implies always performing actions discovering newer better actions provided learned case-base baseline behavior pi-srl replacements case-base executed towards promising action which case guarantees higher return. exploration necessary order obtain optimal behavior since without exploration better actions discovered pi-srl performance limited case-based policy learned ﬁrst step baseline behavior which must remember intended perform suboptimal policies. scaling angle position range episode composed steps although nevertheless prematurely pole becomes unbalanced cart falls track considered failures. parking problem gaussian noise added actions figure modeling baseline behavior step pole-balancing task number steps trial executed case-base baseline behavior cumulative reward trial learned safe case-based policy learnt demonstrations provided baseline behavior computed following procedure described subsection values respectively. figure shows diﬀerent learning processes modeling baseline behavior step. learning process figure shows number steps trial executed baseline behavior case-base beginning learning process case-base empty steps performed using baseline behavior learning process progresses however ﬁlled safe case-based policy learnt. learning process almost steps performed using cases rarely used. important note modeling baseline behavior step performed without failures case. previous task figure represents three independent execution processes using previouslylearned safe case-based policy baseline behavior approach based average cumulative reward episode almost perfectly clones approach which cases results pole disequilibrium cart falling track averages cumulative reward episode figure shows results pi-srl diﬀerent risk conﬁgurations. conﬁguration learning curves shown diﬀerent learning processes performed. additionally episode ending failure marked increase risk increases probability failure policy obtained nevertheless better terms figure improving learned baseline behavior step pole-balancing task cumulative reward episode diﬀerent risk conﬁgurations obtained pisrl. cumulative reward episode obtained evolutionary risksensitive approaches. cases episode ending failure marked. clearly algorithm greatest number failures. risk-sensitive approach agent selects actions result higher value also higher risk. contrary agent learns risk function selects actions lower risk also considerably weak performance. value produces intermediate policy. consequently concluded pi-srl high level risk obtains better policies less failures evolutionary risk-sensitive approaches. figure reinforces previous conclusions. mean number failures cumulative reward trials shown circles corresponding pi-srl black triangles corresponding risksensitive approach blue square corresponding evolutionary approach. ﬁgure also shows performance additional risk levels high level risk risk level represents inﬂection point higher levels risk produce failures without accompanying improvement cumulative reward. fact high level lastly figure shows evolution known space derived case-base diﬀerent trials high-risk learning process. graph error states unknown states known states non-error states represented. known space graph computed taking cases trials graph non-error states computed diﬀerent executions trial ﬁrst graph presents initial known space resulting modeling baseline behavior step. evolution figure demonstrates diﬀerent points. first pi-srl progressively adapts known space order encounter better behavior known space tends compressed toward center coordinates. fact reward greater angle pole cart position second risk failure pole-balancing domain greater early trials learning process. beginning learning process regions known space close error space. situation slight modiﬁcations actions consistently produce visits states learning process advances known space compressed toward origin coordinates away error space. consequently probability visiting error states decreases. example returning figure high-risk learning processes failures occur ﬁrst trials remaining occur last trials. suggested name objective domain make helicopter hover close possible deﬁned position duration established episode. task challenging main reasons. firstly state action spaces high-dimensional continuous secondly generalized domain whose behavior modiﬁed wind factor. helicopter episode composed steps although prematurely helicopter crashes. ﬁrst step pi-srl performed order imitate baseline behavior computed following procedure described subection values respectively. step performed resulting safe case-based policy figure shows learning processes modeling baseline behavior step. similar previous tasks learning processes progress number steps executed baseline behavior reduced number steps using case-base increases. learning process case-base stores safe case-based policy figure compares performance learned case-based policy approach. regarding ﬁrst average cumulative reward episode obtained perfectly mimic baseline behavior figure modeling baseline behavior step helicopter hovering task number steps trial executed case-base baseline behavior cumulative reward trial learned safe case-based policy approach. nevertheless performs safe policy without crashing helicopter. regard training process approach every case produced episodes baseline behavior stored. figure demonstrates approach consistently results helicopter crashes performance extremely learned safe casebased policy begins state-action space safely explored execution step pi-srl. figure shows results diﬀerent risk levels. pi-srl medium levels risk levels produce helicopter crashes pi-srl performance nevertheless quite weak. figure improving learned baseline behavior step helicopter hovering task cumulative reward episode diﬀerent risk conﬁgurations obtained pisrl. cumulative reward episode obtained evolutionary risksensitive approaches. cases episode ending failure marked. conversely high level risk established produces near-optimal policy number collisions. extensive experimentation demonstrates increasing risk improvement cumulative reward. figure shows results evolutionary approach which remembered selected winner competition domain well risk-sensitive algorithm diﬀerent values. comparison results evolutionary approach pi-srl shows similar cumulative reward also signiﬁcantly higher number crashes former latter. evolutionary approach crashes occur early steps learning process; pi-srl accidents occur advanced steps learning process. case risk-sensitive algorithm risk function learned around episode point agent selects lower-risk actions number crashes considerably reduced. agent selects actions resulting higher values without taking risk account performance improves expense increased number accidents. nevertheless whatever value number crashes higher performance worse pi-srl. information figure indicating mean number failures cumulative reward episodes approach complements conclusions made above. data computed independent executions approach. previous domains pi-srl indicated circles risk-sensitive approach black triangles evolutionary approach blue square. figure also shows figure evolution known space diﬀerent episodes helicopter hovering task. example representation single known state radar chart. known states episodes respectively pole-balancing domain figure shows evolution known space according case-base diﬀerent episodes high-risk learning process. case radar charts used high number features describing states. radar chart graphical method displaying multivariate data two-dimensionally. figure axis represents features state preserve simplicity representation charts generated normalizing absolute values features figure example representation single known state. value axis corresponds value individual feature state line drawn connecting feature values axis. line figure represents single state figures show known space according case-base episodes respectively. three charts represent single state rather states corresponding episode. thus graph known states marked state considered error state single feature value state greater limits computed taking account helicopter crashes velocity along main axes exceeds position helicopter orientation degrees target orientation. previous tasks figure indicates diﬀerent matters. first learning proceeds known space derived adjusted space used better safer policies. helicopter domain agent tries hover helicopter close possible target position since immediate rewards greater closer helicopter hovers origin. thus known space starts expand progressively concentrated origin coordinates regard second matter probability crashing since beginning known space already appears concentrated origin error space words beginning features known space error space limits decreasing probability visiting error state. previous experiments second step pi-srl performed using initial case-base free failures built ﬁrst step algorithm. following experiments show performance second step pi-srl diﬀerent initial policies used. figure shows performance policies used initial policies. continuous black line indicates performance initial safe case-based policy average cumulative reward episode used previous experiments prior execution step algorithm. remaining lines figure correspond performance three diﬀerent initializations case-base used experiments prior execution step algorithm. using poor initial policy helicopter crashed nearly episodes average cumulative reward episode calculated using diﬀerent poor initial policy helicopter crashed occasionally average cumulative reward episode finally near-optimal policy whereby helicopter hovering free failures yields average cumulative reward episode figure shows performance second step pi-srl starting case-base corresponding poor poor near-optimal policies presented figure figure dashed blue lines correspond case-base containing near-optimal policy continuous lines correspond case-base containing poor policy dashed green lines correspond case-base containing poor policy. experiments figure conducted using high level risk domain figure performance diﬀerent initial policies helicopter hovering task. performance diﬀerent executions second step pi-srl starting case-base containing policy three diﬀerent types poor poor near-optimal. policy high level risk level case-base worsen performance which fact appears improve slightly. second step pi-srl prevents degradation initial performance since updates cases case-base made using episodes. words updates made cases gathered episodes cumulative reward similar best episode found particular point using threshold example cumulative reward best episode episodes cumulative reward higher used update case-base good sequences experiences provided updates since proven good sequences experiences cause adaptive agent converge stable useful policy sequences cause agent converge unstable poor policy solid lines figure show using poor policy failures initial policy produces higher number failures using initial policy free failures. however despite poor initialization pi-srl nevertheless able learn near-optimal policy well policy free failures used initialize show poor initial policy many failures results decreased performance higher number failures produced even though nevertheless able learn better behavior. case algorithm falls local minimum probably biased poor initialization. cases poor policies number failures higher beginning learning process decreases learning process proceeds. poor poor initial policies close error space stark contrast initial policy shown figure which beginning already appears concentrated origin error space. business simulators powerful tools improving management decision-making processes. example tool simulator business administration simba competitive simulator since agents compete agents management diﬀerent virtual companies. simulator result twenty years experience university students business executives emulates business realities using variables relationships events present business world. objective provide users integrated vision company using basic techniques business management simplifying complexity emphasizing content principles greatest educational value experiments performed here learning agent competes handcoded agents decision-making simba episodic task decisions made sequentially. make business decision state must studied continuous decision variables must followed study state composed continuous variables episode composed steps although prematurely company goes bankrupt figure modeling baseline behavior step simba task number steps trial executed case-base baseline behavior cumulative reward trial learned safe case-based policy figure shows evolution number steps executed baseline behavior case-base learning processes performing modeling baseline behavior step. computed following procedure described subsection values respectively. episodes safe case-based policy learned. figure shows performance previouslylearned approach. study mean proﬁts episode figure improving learned baseline behavior step simba task mean proﬁts episode diﬀerent risk conﬁgurations obtained pi-srl agent hand-coded agents. mean proﬁts episode obtained evolutionary risk-sensitive agent hand-coded agents. cases episode ending failure noted. million euros obtained approach cases generated using baseline behavior episodes stored. experiments demonstrate simba contrast previous domains storing cases suﬃcient obtaining safe policy performance similar using modeling baseline behavior step safe case-based policy learned execute improving learned baseline behavior step. similar ﬁndings earlier tasks figure indicates medium levels risk produce bankruptcies performance nevertheless weak. highest level risk produces near-optimal policy number number failures. contrast figure presents results evolutionary risk-sensitive approaches former clearly yields highest number failures. risk-sensitive case number bankruptcies cases insuﬃcient learning risk function comparative results figure show pi-srl figure shows graphical representation diﬀerent solutions domain. shows mean number failures cumulative reward diﬀerent approaches episodes data computed independent executions approach. figure circles correspond pi-srl algorithm black triangles correspond risk-sensitive approach blue square corresponds evolutionary approach. reinforcement learning case-based reasoning techniques combined literature diﬀerent ways. work bianchi approach presented permitting cases heuristics speed algorithms. additionally sharma combination achieve transfer playing game across variety scenarios madrts commercial real time strategy game. also used state value function approximation however present study knowledge ﬁrst time used conjunction safe exploration dangerous domains. ﬁeld safe reinforcement learning three principal trends observed approaches based return variance risk-sensitive approaches based deﬁnition error states approaches using teachers. perform badly cases stochastic nature problem). mitigate problem agent maximize return associated worst-case scenario even though case highly unlikely. thus trend risk refers worst γtrt variance. example approach worst-case control worst possible outcome optimized worst case control strategies optimality criterion exclusively focused risk-avoiding policies. policy considered optimal worst-case return superior. approach however restrictive inasmuch takes rare scenarios fully account. worst case control mdps. concept establishes returns policy occur probability lower neglected. algorithm less pessimistic pure worst case control given extremely rare scenarios eﬀect policy. work heger idea weighting return risk namely expected value-variance criterion also introduced. usual return. shown that depending parameter policies high variance penalized enforced instead neuneier mihatsch consider worst-case-outcomes policy study authors demonstrate learning algorithm interpolates risk-neutral worst-case criterion limiting behavior exponential utility functions. noted approaches based variability return worst possible outcomes suited problems policy small variance produce large risk view risk present study however concerned variance return worst possible outcome instead fact processes generally possess unsafe states avoided. consequently address diﬀerent class problems dealt approaches focusing variability return. second trend approaches concept risk based deﬁnition error states fatal transitions. thus geibel instance establish risk function probability entering error state. instead hans consider transition fatal corresponding reward less given threshold ﬁrst case demonstrated section learned methods require error states visited repeatedly order approximate risk function subsequently avoid dangerous situations. second case concept risk joined reward. moreover mentioned studies either assume system dynamics known tolerate undesirable states exploration contrast paper deal problems high-dimensional continuous state-action spaces. regarding latter geibel write last trend approaches based teachers three diﬀerent ways bootstrap learning algorithm derive policy ﬁnite demonstration guide exploration process. work driessens sˇzeroski bootstraping procedure used relational ﬁnite demonstrations recorded human expert later presented regression algorithm. allows regression algorithm build partial q-function later used guide exploration state space using boltzmann exploration strategy. smart kaelbling also examples training runs bootstrap q-learning approach hedger algorithm. initial knowledge bootstrapped q-learning approach allows agent learn eﬀectively helps reduce time spent random actions. teacher behaviors also used form population seeding neuroevolution approaches evolutionary methods used optimize weights neural networks starting prototype network whose weights correspond teacher using technique competition helicopter hovering task winners martin developed evolutionary algorithm several teachers provided initial population. algorithm restricts crossover mutation operators allowing slight changes policies given teachers. consequently rapid convergence algorithm near-optimal policy ensured indirect minimization damage agent. however teachers included initial population resulting ad-hoc training regimen conducted competition. consequently proposed approach seems somewhat ad-hoc easily generalizable arbitrary problems. work koppejan neural networks also evolved beginning whose weights corresponds teacher behavior. approach proven advantageous numerous applications evolutionary methods koppejan’s algorithm nevertheless also seems somewhat ad-hoc designed specialized environments. approaches falling category framed according ﬁeld learning demonstration highlighting study abbeel based apprenticeship learning approach composed three distinct steps. ﬁrst teacher demonstrates task learned state-action trajectories teacher’s demonstration recorded. second step state-action trajectories seen point used learn dynamics model system. model optimal policy found using reinforcement learning algorithm. finally policy obtained tested running real system. work tang algorithm based apprenticeship learning also presented automaticallygenerating trajectories diﬃcult control tasks. proposal based learning parameterized versions desired maneuvers multiple expert demonstrations. despite approach’s potential strengths general interest inherently linked information provided demonstration dataset. result learner performance heavily limited quality teacher’s demonstrations. driessens sˇzeroski context relational also given teacher’s policy rather policy derived current q-function hypothesis selection actions. approach episodes performed teacher interleaved normal exploration episodes. mixture teacher normal exploration make easier regression algorithm distinguish beneﬁcial poor actions. context approaches include teacher advice advice used improve learner performance oﬀering information beyond provided demonstration dataset. approach following initial task demonstration teacher agent directly requests additional demonstration teacher diﬀerent states previously demonstrated states single action cannot selected certainty work pi-srl algorithm policy improvement safe reinforcement learning high-risk tasks described. main contributions algorithm deﬁnitions novel case-based risk function baseline behavior safe exploration state-action space. case-based risk function presented possible inasmuch policy stored case-base. represents clear advantage approaches e.g. evolutionary extraction knowledge known space agent impossible using weights neural-networks. additionally completely diﬀerent notion risk others found literature presented. according notion risk independent variance return reward function require identiﬁcation error states learning risk functions. rather concept risk described paper based distance known unknown space therefore domain-independent parameter koppejan also function identify dangerous states contrast approach deﬁnition function requires strong previous knowledge domain. furthermore approaches risk found literature tackle problems entirely continuous report results continuous domain consequently diﬃcult know certain approaches literature generalize easily arbitrary domains. paper presents pi-srl algorithm great detail demonstrates eﬀectiveness four entirely diﬀerent continuous domains parking problem pole-balancing helicopter hovering business management experiments presented paper demonstrate diﬀerent characteristics learning capabilities pi-srl algorithm. pi-srl obtains higher quality solutions. experiments section demonstrate that save helicopter hovering task pi-srl obtains cases best cumulative reward episode least number failures. additionally using pareto comparison criterion said that save high risk conﬁguration parking problem approach strictly dominated approach. pi-srl adjusts initial known space safe better policies. initial known space resulting ﬁrst step pi-srl modeling baseline behavior adjusted improved second step algorithm improving learned baseline behavior. additionally experiments demonstrate adjustment process compress known space away error space occasions require known space move closer error space event better policies found there. pi-srl works well domains diﬀerently structured state-action spaces value function vary sharply. although parking problem polebalancing domain helicopter hovering task business simulator represent diﬀerently structured problems experiments study nevertheless demonstrate pi-srl performs well each. furthermore even domains parking problem value function varies sharply presence obstacle experimental results demonstrate pi-srl nevertheless successfully handle diﬃculty. however impossible avoid failures known space edge edge error states algorithm would often ’explore’ error states. number failures depends distance known space error space. experiments pole-balancing helicopter hovering domains demonstrate number failures depends close known space error space. structure domains improving learned baseline behavior step algorithm tends concentrate known space origin coordinates away error space. greater distance known space error space lower number failures. additionally helicopter hovering known space beginning error space therefore initial distribution known space learned baseline policy later inﬂuences number failures obtained second step pi-srl. pi-srl completely safe ﬁrst step algorithm executed. however proceeding algorithm performance would heavily limited capabilities baseline behavior. learner performance improved beyond performance baseline behavior subsequent exploratory process second step pi-srl must carried out. since complete knowledge domain dynamic possessed however also inevitable that exploratory risk parameter allows user conﬁgure level risk assumed. algorithm user gradually increase value risk parameter order obtain better policies also assuming greater likelihood damage learning system. pi-srl performs successfully even poor initial policy failures used. experiments figure helicopter hovering domain demonstrate pi-srl able learn near-optimal policy despite poor initialization policy free failures used initialize case-base however figure also shows poor initial policy many failures used pi-srl decreases performance produces higher number failures although better behavior still learnt. case algorithm falls local minimum likely biased poor initialization. follows applicability method discussed allowing reader clearly understand scenarios proposed pi-srl approach applicable. applicability restricted domains following characteristics. mandatory scenario satisfy assumptions described section according ﬁrst assumption nearby states domain must necessarily similar actions. according other similar actions similar states produce similar eﬀects. fact similar actions lead similar states assumes degree smoothness dynamic behavior system which certain environments hold. however clearly explain section consider assumptions logical assumptions derived generalization principles literature applicability method limited size case-base required mimic baseline behavior. possible apply proposed approach tasks when ﬁrst step pi-srl algorithm modeling baseline behavior prohibitively large number cases required properly mimic complex baseline behaviors. case threshold increased restrict addition cases casebase. however increase adversely aﬀect ﬁnal performance algorithm. nevertheless experiments performed section demonstrate relatively simple baseline behaviors mimicked almost perfectly using manageable number cases. pi-srl algorithm requires presence baseline behavior. proposed method requires presence baseline behavior safely demonstrates task learned. baseline behavior conducted human teacher hand-coded agent. important note nevertheless presence baseline behavior guaranteed domains. finally logical continuation present study would take account automatic graduation risk parameter along learning process. example would particularly interesting exploit fact known space away error space order increase risk parameter contrary reduce close. future work aims deploy algorithm real environments inasmuch uncertainty real environments presents biggest challenge autonomous robots. autonomous robotic controllers must deal large number factors robotic mechanical system electrical characteristics well environmental complexity. however pi-srl algorithm learning processes real environments could reduce amount damage incurred consequently allow lifespan robots extended. might worthwhile mechanism algorithm detect known state lead directly error state. problems currently investigated. study partially supported spanish miciin projects tin--c tra- ccg-ucm/tic-. oﬀer gratitude special thanks raquel fuentetaja piz´an assistant professor universidad carlos madrid planning learning group generous invaluable comments revision paper. would also like thank jos´e antonio mart´ın assistant professor universidad complutense madrid invaluable comments regarding evolutionary algorithm.", "year": 2014}