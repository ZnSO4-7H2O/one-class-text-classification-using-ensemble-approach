{"title": "A General Framework for Interacting Bayes-Optimally with Self-Interested  Agents using Arbitrary Parametric Model and Model Prior", "tag": ["cs.LG", "cs.AI", "cs.MA", "stat.ML"], "abstract": "Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which FDM's independence and modeling assumptions do not hold. As a result, FDM does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. To overcome these practical limitations of FDM, we propose a generalization of BRL to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. Empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.", "text": "recent advances bayesian reinforcement learning shown bayes-optimality theoretically achievable modeling environment’s latent dynamics using flat-dirichletmultinomial prior. self-interested multiagent environments transition dynamics mainly controlled agent’s stochastic behavior fdm’s independence modeling assumptions hold. result allow agent’s behavior generalized across different states speciﬁed using prior domain knowledge. overcome practical limitations propose generalization integrate general class parametric models model priors thus allowing practitioners’ domain knowledge exploited produce ﬁne-grained compact representation agent’s behavior. empirical evaluation shows approach outperforms existing multi-agent reinforcement learning algorithms. introduction reinforcement learning agent faces dilemma acting optimally respect current possibly incomplete knowledge environment acting sub-optimally gain information model-based bayesian reinforcement learning circumvents dilemma considering notion bayes-optimality bayesoptimal policy selects actions maximize agent’s expected utility respect possible sequences future beliefs candidate models environment. unfortunately large belief space bayes-optimal policy approximately derived simple choice models model priors. example flat-dirichlet-multinomial prior assumes next-state distributions action-state pair modeled independent multinomial distributions separate dirichlet priors. despite common analyze benchmark algorithms perform poorly practice often fails exploit strucelaborate critical limitation lies independence assumption driven computational convenience rather scientiﬁc insight. identify practical examples context self-interested multi-agent uncertainty transition model mainly caused stochasticity agent’s behavior independence assumption hold consider example application problem placing static sensors monitor environmental phenomenon involves actively selecting sensor locations measurement predictive variances unobserved locations minimized. here phenomenon other agent measurements actions. important characterization phenomenon spatial correlation measurements neighboring locations/states makes fdm-based extremely ill-suited problem independence assumption. secondly despite computational convenience permit generalization across states thus severely limiting applicability practical problems large state space past observations come limited states. interestingly problems often possible obtain prior domain knowledge providing parsimonious structure agent’s behavior potentially resolve issue generalization. example consider using derive bayes-optimal policy autonomous navigate successfully among human-driven vehicles whose behaviors different situations governed small consistent latent parameters demonstrated empirical study gipps estimating/learning parameters possible generalize behaviors across different states. this however contradicts independence assumption fdm; practice ignoring results inferior performance shown section note that using parameter tying modiﬁed make agent’s behavior identical different states. simple generalization restrictive real-world tual parametric form opponent’s behavior; abstraction provides practitioners ﬂexibility choosing suitable degree parameterization. example simply multinomial distributions prior domain knowledge available. othpv erwise domain knowledge exploited produce ﬁne-grained representation time made compact generalize opponent’s behavior across different states opponent’s behavior learned monitoring belief possible particular belief updated step based history latest interactions recent one) using bayes’ theorem denote information state consists current state history latest interactions. opponent’s behavior stationary follows ease notations main results work presented case extension general case requires replacing case re-written difference bayesian modeling paradigm require respectively dirichlet prior multinomial likelihood dirichlet conjugate prior multinomial. practice conjugate prior desirable bebelongs dirichlet family cause posterior prior thus making belief update tractable bayes-optimal policy efﬁcient derived. despite computational convenience conjugate prior restricts practitioners exploiting domain knowledge design informed priors furthermore turns overkill make belief update tractable. particular show theorem that without assuming speciﬁc parametric form initial prior posterior belief still tractably represented even though necessarily conjugate distributions. indeed sufﬁcient guarantee derive tractable representation bayes-optimal policy using ﬁnite parameters shall seen later section theorem initial prior represented exactly using ﬁnite parameters posterior conditioned sequence observations also represented exactly parametric form. proof sketch. prove induction problems like examples agent’s behavior different states necessarily identical related common latent non-dirichlet parameters. consequently still huge putting practice interacting self-interested agents unknown behaviors. best knowledge ﬁrst investigated chalkiadakis boutilier offer myopic solution belief space instead solving bayes-optimal policy non-myopic. proposed bpvi method essentially selects actions jointly maximize heuristic aggregation myopic value perfect information average estimation expected utility obtained solving exact mdps respect samples drawn posterior belief agent’s behavior. moreover bpvi restricted work dirichlet priors multinomial likelihoods subject disadvantages modeling agent’s behavior. also bpvi demonstrated empirically simplest settings states. furthermore light examples agent’s behavior often needs modeled differently depending speciﬁc application. grounding context framework either domain expert struggles best prior knowledge supported models model priors agent developer re-design framework incorporate modeling scheme. arguably free lunch comes modeling agent’s behavior across various applications. cope difﬁculty framework ideally allow domain expert freely incorporate choice design modeling agent’s behavior. motivated practical considerations paper presents novel generalization call interactive integrate parametric model model prior agent’s behavior speciﬁed domain experts consequently yielding advantages agent’s behavior represented ﬁne-grained manner based practitioners’ prior domain knowledge compactly generalized across different states thus overcoming limitations fdm. show non-myopic bayes-optimal policy derived analytically solving i-brl exactly propose approximation algorithm compute efﬁciently polynomial time empirically evaluate performance i-brl bpvi using interesting trafﬁc problem modeled real-world situation modeling agent proposed bayesian modeling paradigm opposh nent’s behavior modeled probabilities selecting action state conditioned history vi}d latest interactions action taken agent i-th step. distributions parameterized abstracts acsimply results imply optimal value approximated arbitrarily closely ﬁnite piecewise linear α-functions shown αfunction associated action yielding expected utility true behavior opponent consequently overall expected reward assuming that starting learner selects action continues optimally thereafter. particular derived based constructive proof theorem however limited space state constructive process below. interested readers referred appendix detailed proof. speciﬁcally given holds follows αtsv set= follows also ting holds result optimal policy derived directly α-functions thus constructing essentially boils exhaustive enumeration possible pairs corresponding application compute though speciﬁes bottom-up procedure constructαut αing functions implicitly requires convenient parameterization α-functions closed application complete analytical derivation present ﬁnal result demonstrate α-function indeed parametric form. note theorem generalizes similar result proven latter shows that α-function represented linear combination multivariate monomials. practical algorithm building generalized result theorem presented section theorem denote family functions represented ﬁnite then optimal value interactive bayesian section ﬁrst extend proof techniques used theoretically derive agent’s bayes-optimal policy general class parametric models model priors opponent’s behavior particular show derived bayes-optimal policy also represented exactly using ﬁnite number parameters. based derivation naive algorithm devised compute exact parametric form bayesoptimal policy finally present practical algorithm efﬁciently approximate bayes-optimal policy polynomial time formally agent assumed interacting opponent stochastic environment modeled tuple ﬁnite agent depends joint action current state environment transitions state probability pr|s future payoff discounted constant factor finally described section opponent’s latent behavs} selected general class paraior metric models model priors subsumes recall idea underlying notion bayes-optimality maintain belief represents uncertainty surrounding opponent’s behavior stage interaction. thus action selected learner stage affects expected immesrs|b] posterior belief state latter inﬂuences future payoff builds information gathering option such bayes-optimal policy obtained maximizing expected discounted rewards abdλ. optimal policy learner deﬁned function maps belief action maximizing expected utility derived solving derive solution ﬁrst re-state well-known results concerning augmented belief-state single-agent also hold straight-forwardly general class parametric models model priors. theorem optimal value function steps-to-go converges optimal value function inﬁnite horizon practical approximation algorithm section introduce practical modiﬁcations backup algorithm addressing above-mentioned issues. ﬁrst address issue α-function explosion generalizing discrete pomdp’s pbvi solver used augmented belief-state α-functions yield optimal values sampled reachable beliefs computed resulting algorithm shown below pb-backup. particular since α-function fact linear combination functions natural choose basis functions besides easy sampled belief initial prior belief s}i=...|bs| selected basis convenience functions. speciﬁcally pb-backup operation projected onto function space deﬁned s}i=...|bs|. projection cast optimiza{φi tion problem minimizes squared difference between α-function projection respect sampled beliefs coefψv )ci. easy ﬁcients clearly holds shown that general class parametric models model priors α-function represented linear combination arbitrary parametric functions subsume multivariate monomials used exact algorithm intuitively theorems provide simple constructive method computing α-functions hence optimal policy. step sets constructed using latter computed previously step proof theorem sketch algorithm shown below backup algorithm steps compute ﬁrst second summation terms right-hand side re}ut spectively. then steps construct using respectively. thus iteratively backup sufﬁciently computing large value used approximate arbitrarily closely shown theorem however naive algorithm computationally impractical following issues α-function explosion number α-functions grows doubly exponentially planning horizon length parameter explosion average number parameters used represent α-function grows factor manifested practicality approach therefore depends crucially issues resolved described next. model driver’s acceleration deceleration reaction time imperfection unknown parameters distributed uniformly within corresponding ranges. parameterization cover variety drivers’ typical behaviors shown preliminary study. further understanding parameters readers referred besides time step vehicle moves current cell next probability remains cell cell probability expected time move forward cell current position respect current speed thus general underlying stochastic game states signiﬁcantly larger settings previous experiments. state vehicle actions mentioned previously vehicle actions corresponding levels speed according reactive model. goal vehicle domain learn vehicle’s reactive model adjust navigation strategy accordingly collision time spent cross intersection minimized. achieve goal penalize vehicle step reward successfully crosses intersection. collides vehicle penalize discount factor evaluate performance i-brl problem different sets reactive parameters generated uniformly ranges. parameters simulations estimate vehicle’s average performance particular compare algorithm’s average performance average performance fully informed vehicle knows exactly reactive parameters simulation rational vehicle estimates reactive parameters taking means ranges vehicle employing bpvi results shown fig. observed vehicle always performs signiﬁcantly better rational bpvi-based vehicles. particular vehicle manages reduce performance implementation pb-backup algorithm presented above instead maintaining exact parameters represent immediate functions evaluations need sampled beliefs real-time processing cost evaluating αfunction’s expected reward particular belief since sampling {λj} computation cost reduced makes action selection incur cost total. signiﬁcantly cheaper compared total cost online sampling re-estimating incurred bpvi also note since ofﬂine computational costs steps pbbackup projection cost cast cost solving system linear equations always polynomial functions interested variables optimal policy approximated polynomial time. experiments discussion section realistic scenario intersection navigation modeled stochastic game inspired near-miss accident darpa urban challenge. considering trafﬁc situation illustrated fig. autonomous vehicles enter intersection road segments discretized uniform grid cell size speed vehicle also discretized uniformly levels ranging m/s. stage system’s state characterized tuple specifying current positions velocities respectively. addition vehicle either accelerate decelerate maintain speed time step vehicle changes speed based parameterized reactive model contrast above-mentioned works focus convergence i-brl directly optimizes learner’s performance course interaction terminate successfully learn opponent’s behavior. main concern well learner perform before behavior converges. practical perspective seems appropriate goal reality agents interact limited period enough guarantee convergence thus undermining stability optimality criteria. context existing approaches appear disadvantage algorithms focus stability optimality tend select exploratory actions drastic effect without considering huge costs though notion security aims prevent learner selecting radical actions proposed security values always turn tight lower bounds optimal performance interested readers referred appendix detailed discussion additional experiments compare performances i-brl approaches respectively. note solving bayes-optimal policy efﬁciently addressed explicitly general prior paper actually avoid problem allowing agent sub-optimally bounded number steps. particular works asmuth littman araya-lopez guarantee that worst case agent nearly approximately bayes-optimal polynomially bounded number steps high probability. thus necessary point difference between i-brl worst-case approaches interested maximizing average-case performance certainty rather worst-case performance high probability guarantee. comparing performances beyond scope paper. paper describes novel generalization called i-brl integrate general class parametric models model priors opponent’s behavior. result i-brl relaxes restrictive assumption often imposed existing works thus offering practitioners greater ﬂexibility encoding prior domain knowledge opponent’s behavior. empirical evaluation shows i-brl outperforms bayesian marl approach utilizing called bpvi. i-brl also outperforms existing marl approaches focusing convergence shown additional experiments successfully bridged applying self-interested multi-agent settings. acknowledgments. work supported singaporemit alliance research technology subaward agreements r---- r----. fully informed rational vehicles roughly half. difference performance vehicle fully informed vehicle expected fully informed vehicle always takes optimal step beginning vehicle take cautious steps feels conﬁdent information collected interaction. intuitively performance mainly caused initial period caution. also since uniform prior reactive parameters conjugate prior vehicle’s behavior model bpvi-based vehicle directly maintain update belief using {θs}s s}v) instead however implicitly assumes {θs}s statistically independent true case since actually related unfortunately bpvi cannot exploit information generalize vehicle’s behavior across different states restrictive thus resulting inferior performance. related works self-interested marl several groups proponents advocating different learning goals following garnered substantial support stability self-play certain class learning opponents learners’ behaviors converge equilibrium; optimality learner’s behavior necessarily converges best policy certain class learning opponents; security learner’s average payoff must exceed maximin value game. example works littman bianchi akchurina focused gametheoretic approaches satisfy stability criterion selfplay. works bowling veloso suematsu hayashi tesauro developed algorithms address optimality stability criteria learner essentially converges best response opponents’ policies stationary; otherwise converges self-play. notably work powers shoham proposed approach provably converges \u0001best response class adaptive bounded-memory opponents simultaneously guaranreferences natalia akchurina. multiagent reinforcement learning algorithm converging nash equilibrium general-sum discounted stochastic games. proc. aamas pages hoang low. general framework interacting bayes-optimally self-interested agents using arbitrary parametric model model prior. arxiv. michael littman. nobuo suematsu akira hayashi. multiagent reinforcement learning algorithm using extended optimal response. proc. aamas gerald tesauro. extending q-learning proc. nips alternative choice basis functions section demonstrates another theoretical advantage framework ﬂexibility customize general pointbased algorithm presented section manageable forms respect different choices basis functions. interestingly customizations often allow practitioners trade effectively performance sophistication implemented algorithm simple choice basis functions reduce performance exchange bestows upon customization computationally efﬁcient easier implement. especially useful practical situations ﬁnding good enough solution quickly important looking better time-consuming solutions. functions rest section. particular {λi}n opponent’s models sampled initial belief also denote function returns otherwise. according section keep number parameters growing exponentially project α-function onto {ψi}n minimizing alternatively unconstrained squared difference between α-function projection proof sketches theorems section provides detailed proof sketches theorems mentioned section theorem optimal value function steps-to-go converges optimal value function inﬁnite horizon proof sketch. deﬁne maxa maxa maxa proof sketch. give constructive proof inducbuilt recursively. assuming tion shows holds proven also holds particular follows inductive assumption term multi-agent chain world problem system consists chain states agents; agent actions stage interaction agents move step forward back initial state depending whether coordinate action respectively particular agents receive immediate reward coordinating last state coordinating state except ﬁrst one. otherwise agents remain current state reward step payoffs discounted constant factor experiment compare performance ibrl state-of-the-art frameworks marl include bpvi hyper-q meta-strategy among works bpvi relevant ibrl hyper-q simply extends q-learning context multi-agent learning. meta-strategy default plays best response empirical estimation opponent’s behavior occasionally switches maximin strategy accumulated reward falls maximin value game. particular compare average performance frameworks tested different opponents whose behaviors modeled probabilities s}v) opponents independently randomly generated dirichlet distributions then opponent parameters simulations evaluate performance framework. results show i-brl signiﬁcantly outperforms others discrete distribution samples implies choosing {ψi}i basis functions ﬁnding corresponding projected solution identical solving easily implemented using existing discrete pomdp solvers additional experiments section provides additional evaluations proposed i-brl framework comparison existing works marl series stochastic games adapted simpliﬁed testing benchmarks used particular i-brl evaluated small typical application domains widely used existing works experiment opponent assumed make decision based last step interaction thus behavior modeled conditional probabilities s}v) encodes agent’s compare average performance i-brl bpvi hyper-q meta-strategy tested different opponents randomly generated dirichlet priors. results shown figs. results clearly observed i-brl also outperforms bpvi methods experiment consistent explanations previous experiment. also terms online processing cost i-brl requires hours complete simulations bpvi requires hours. pared i-brl expected bpvi mentioned section relies sub-optimal myopic information-gain function thus underestimating risk moving forward forfeiting opportunity backward information earn small reward. hence many cases chance getting reward accidentally over-estimated bpvi’s lack information. result makes expected gain moving forward insufﬁcient compensate risk besides also expected hyper-q’s meta-strategy’s performance worse ibrl’s since primarily focus criteria optimality security disadvantage context work explained previously section notably case maximin value meta-strategy ﬁrst state vacuously equal effectively lower bound algorithms. contrast i-brl directly optimizes agent’s expected utility taking account current belief possible sequences future beliefs result agent behaves cautiously always takes backward action sufﬁcient information guarantee expected gain moving forward worth risk addition i-brl’s online processing cost also signiﬁcantly less expensive bpvi’s i-brl requires hours complete simulations opponents bpvi requires hours. exchange speed-up i-brl spends hours ofﬂine planning reasonable trade-off considering critical agent meet real-time constraint interaction. iterative version well-known one-shot two-player game known prisoner dilemma player attempts maximize reward cooperating betraying other. unlike one-shot game game played repeatedly player knows history opponent’s moves thus opportunity predict opponent’s behavior based past interactions. stage interaction agents reward depending whether mutually cooperate betray other respectively. addition agent reward cooperates opponent betrays; conversely gets reward betraying opponent cooperates.", "year": 2013}