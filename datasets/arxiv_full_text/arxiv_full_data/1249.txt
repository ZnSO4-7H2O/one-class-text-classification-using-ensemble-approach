{"title": "Optimization on Product Submanifolds of Convolution Kernels", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Recent advances in optimization methods used for training convolutional neural networks (CNNs) with kernels, which are normalized according to particular constraints, have shown remarkable success. This work introduces an approach for training CNNs using ensembles of joint spaces of kernels constructed using different constraints. For this purpose, we address a problem of optimization on ensembles of products of submanifolds (PEMs) of convolution kernels. To this end, we first propose three strategies to construct ensembles of PEMs in CNNs. Next, we expound their geometric properties (metric and curvature properties) in CNNs. We make use of our theoretical results by developing a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles of PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD considering geometric properties of PEMs. In the experimental analyses, we employ G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The results show that geometric adaptive step size computation methods of G-SGD can improve training loss and convergence properties of CNNs. Moreover, we observe that classification performance of baseline CNNs can be boosted using G-SGD on ensembles of PEMs identified by multiple constraints.", "text": "propose algorithm optimization different ensembles pems generalizing methods employed kernel submanifolds next explore effect geometric properties pems convergence g-sgd using theoretical results. then employ results adaptive computation step size moreover provide example computation step size function optimization pems identiﬁed sphere addition propose three strategies order construct ensembles identical non-identical kernel spaces according employment input output channels cnns section best knowledge proposed g-sgd ﬁrst algorithm performs optimization different ensembles pems train cnns convergence properties. experimentally analyze convergence properties classiﬁcation performance cnns benchmark image classiﬁcation datasets cifar imagenet using various manifold ensemble schemes results observe g-sgd employed ensembles pems boost baseline state-of-the-art performance cnns. training samples suppose random variable drawn distribution measurable space class label image l-layer consists tensors {wl}l tensor composed kernels wcdl constructed layer channel kernel convolution layer compute abstract—recent advances optimization methods used training convolutional neural networks kernels normalized according particular constraints shown remarkable success. work introduces approach training cnns using ensembles joint spaces kernels constructed using different constraints. purpose address problem optimization ensembles products submanifolds convolution kernels. ﬁrst propose three strategies construct ensembles pems cnns. next expound geometric properties cnns. make theoretical results developing geometry-aware algorithm optimization ensembles pems train cnns. moreover analyze convergence properties g-sgd considering geometric properties pems. experimental analyses employ g-sgd train cnns cifar cifar- imagenet datasets. results show geometric adaptive step size computation methods g-sgd improve training loss convergence properties cnns. moreover observe classiﬁcation performance baseline cnns boosted using gsgd ensembles pems identiﬁed multiple constraints. recent works several methods suggested train deep neural networks using kernels various normalization constraints boost performance. spaces normalized kernels explored using riemannian manifolds stochastic optimization algorithms employed train cnns using kernel manifolds work suggest approach training cnns using multiple constraints kernels order learn richer features compared features learned using single constraints. address problem optimization ensembles products different kernel submanifolds identiﬁed different constraints kernels. however employ aforementioned riemannian algorithms pems train cnns observe early divergence vanishing exploding gradients problems. therefore elucidate geometric properties pems assure convergence local minima training cnns using proposed geometry-aware stochastic gradient descent contributions summarized follows image channel data matrix convolved kernel wcdl obtain feature xcl+ ˆxdl ˆxdl wcdl∗xcl∀c given batch samples denote loss function kernels utilized assuming contains single sample expected indices represents identity number kernel resides manifold layer. addition subset used determine subset kernel submanifolds aggregated construct satisﬁes following properties example illustration employment layer given figure suppose kernel tensor size number input output channels total kernel matrices size example construction ensemble input channels split kernels associated output channels subsets kernels. choosing sphere ﬁrst subset construct product using component manifolds sphere. similarly choosing stiefel second subset construct another product thus layer construct ensemble pems pems output channels split kernels corresponding input channels subsets kernels. choose ﬁrst subset construct product using choose second subset construct product thereby ensemble consisting pems pems split kernels subsets. output channels split kernels corresponding input channels subsets. choose subsets containing kernels subsets containing framework used model overlapping non-overlapping sets. ensembles constructed using overlapping sets kernels different constraints applied input output channels. example kernels belonging kernels belonging applied output channel previous example complicated conﬁgurations obtained using pio. experiments selected non-overlapping sets simplicity. consider theoretical experimental analyses overlapping sets future work. employed non-linear kernel submanifolds gradient descent generally performed three steps; projection gradients tangent spaces submanifolds movement kernels tangent spaces gradient descent direction iii) projection moved kernels onto submanifolds steps determined according geometric properties submanifolds sectional curvature metric properties. example euclidean space zero sectional curvature i.e. curved thereby steps performed using single step employs kernels residing euclidean space. however kernels belong unit sphere kernel space curved constant positive curvature. moreover different tangent space computed kernel located sphere. therefore nonlinearity operations transformations applied kernels implied curvature metric kernel spaces used gradient descent aforementioned three steps. addition martingale properties stochastic processes deﬁned kernels determined geodesics metrics gradients projected tangent spaces injectivity radius kernel spaces geometric properties pems different component submanifolds pems even constructed using identical submanifolds. example observe locally varying curvatures construct pems spheres kernel spaces complicated geometric properties obtained using proposed strategies especially constructing ensembles pems non-identical submanifolds thus order address problems consider geometric properties kernel submanifolds training cnns propose geometry aware employ metric properties pems perform gradient descent steps gsgd curvature properties pems explore convergence properties g-sgd. explore metric curvature properties pems next theorem. convolution kernel resides admit metric negative sectional curvature therefore geometric properties pems composed non-identical component submanifolds crucially affect convergence methods training cnns using metrics identiﬁed component manifolds employing given lemma addition riemannian curvature sectional curvature given lemma analyze convergence proposed g-sgd compute adaptive step size. lemma. instance suppose unit twocomputed unit curvature along two-dimensional subspaces tangent spaces called two-planes. hand zero curvature along two-planes spanning exactly distinct spheres. therefore learning rates need computed adaptively according sectional curvatures layer epoch g-sgd kernel manifold algorithmic description proposed geometry-aware given algorithm initialization gsgd identify component embedded kernel submanifolds according constraints applied kernels instance employ orthonormalization construct ensembles pems using sphere oblique stiefel manifolds. also kernels residing ambient euclidean space embedded kernel submanifolds order preserve task structure employed layers considering space images ensembles employ using kernel splitting scheme. split kernel subsets belonging sake simplicity analyses split kernel subsets size proposed schemes enable construct kernel sets varying size. implementation details g-sgd different ensembles resnets data preprocessing details benchmark datasets additional results given supp. mat. considering nonlinear geometry manifolds. g-sgd perform optimization pems ensemble according sets component manifolds well pems ensemble. methods studied literature assurance convergence applied optimization ensembles pems. employment line retractions line essential assurance convergence explained next. machine learning tasks clustering geodesic distance computed closed form. however closed form solution computed using cnns challenge computation local minima. therefore provide asymptotic convergence property algorithm next theorem. theorem assures convergence g-sgd minima. implementation g-sgd result given lemma pems employ sectional curvatures. although sectional curvatures non-identical embedded kernel submanifolds different lemma assures existence zero sectional curvature pems along tangent spaces. next theorem provide example computation step experimental analyses different step size functions analyze convergence properties performance cnns trained using g-sgd relaxing assumptions theorem corollary different architectures benchmark image classiﬁcation datasets. examine proposed g-sgd method training state-ofthe-art cnns called residual networks equipped different number layers kernels. three benchmark image classiﬁcation datasets namely cifar- cifar imagenet cifar- cifar- datasets analyze classiﬁcation performance cnns trained using gsgd benchmark cifar- cifar- imagenet datasets. order construct ensembles kernels belonging euc. using increase number kernels used cnns multiples hyperparameters cnns suggested depict performance implementation cnns baseline geometries marker tables. examine classiﬁcation performance resnets layers layers cifar- data augmentation imagenet table table respectively. results show performance cnns boosted employing ensembles pems using g-sgd compared employment baseline euc. observe pems component submanifolds identical geometry ensembles provide better performance compared employment component submanifolds instance obtain error using pems table respectively. however error obtained using respectively. addition obtain boost performance ensemble euc. experiments cifar imagenet datasets using scheme table table respectively. moreover observe construction ensembles using performs better compared instance observe pems provides pems provides table table respectively. associate result observation kernels belonging used feature selection modeling texture patterns high performance however ensembles perform better compared kernels employed output channels. also observed performs better experiments. observe boost construction ensemble four manifolds using scheme table table respectively. words ensemble methods boost performance large-scale cnns large-scale datasets consisting larger number samples classes compared performance smaller cnns employed smaller datasets result attributed enhancement sets features learned using multiple constraints kernels. analyze observation examining performance larger cnns consisting layers cifar- cifar datasets without using table results show employment pems boost performance cnns component submanifolds larger networks compared smaller networks moreover employment pems sp+ob+st+euc. boosts performance cnns euc. cifar- compared performance obtained cifar- addition observe ensembles boost performance cnns methods compared performance cnns without using method fundamentally differs network ensembles. order analyze results network ensembles cnns employed ensemble method voting decisions resnet cifar cnns trained individual ensembled using voting obtained errors analyses ensembles contains kernels number kernels used layer number pems. ensemble trained using individual manifold contains kernels obtained errors. thus proposed methods outperform ensembles constructed voting. additional results given supplemental material. introduced elucidated problem training cnns using multiple constraints employed convolution kernels convergence properties. following theoretical results proposed g-sgd algorithm adaptive step size estimation methods optimization ensembles pems identiﬁed constraints. experimental results show proposed methods improve convergence properties classiﬁcation performance cnns. overall results show employment ensembles pems using g-sgd boost performance larger cnns large scale datasets compared performance small medium scale networks employed smaller datasets future work plan extend proposed framework development ensemble schemes perform various tasks machine translation video recognition using cnns recurrent neural networks addition proposed methods applied stochastic optimization methods adam trust region methods. believe proposed framework useful researchers study geometric properties parameter spaces deep networks improve understanding deep feature representations. joint diagonalization oblique manifold independent component analysis. proc. ieee int. conf. acoust. speech signal process volume pages toulouse france salimans kingma. weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems arpit zhou kota govindaraju. normalization propagation parametric technique removing internal covariate shift deep networks. proc. int. conf. mach. learn. icml pages york city june cisse bojanowski grave dauphin usunier. parseval networks improving robustness adversarial examples. precup editors proceedings international conference machine learning volume proceedings machine learning research pages international convention centre sydney australia pmlr. zhang sun. deep residual learning image recognition. ieee int. conf. comp. vis. patt. recog. henaff szlam lecun. recurrent orthogonal networks long-memory tasks. proc. int. conf. mach. learn. icml’ pages huang wang shan chen. log-euclidean metric learning symmetric positive deﬁnite manifold application image classiﬁcation. proc. int. conf. mach. learn. pages", "year": 2017}