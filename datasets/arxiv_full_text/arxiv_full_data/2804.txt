{"title": "Cross-Domain Transfer in Reinforcement Learning using Target Apprentice", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we present a new approach to Transfer Learning (TL) in Reinforcement Learning (RL) for cross-domain tasks. Many of the available techniques approach the transfer architecture as a method of speeding up the target task learning. We propose to adapt and reuse the mapped source task optimal-policy directly in related domains. We show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and source. The main benefit of this policy augmentation is generalizing policies across multiple related domains without having to re-learn the new tasks. Our results show that this architecture leads to better sample efficiency in the transfer, reducing sample complexity of target task learning to target apprentice learning.", "text": "action applied direction fall pendulum bicycle. similarity dynamical behavior systems makes learning cart-pole domain relevant bike balancing problem. another cross-domain transfer results present problem transferring skills learned mountain inverted pendulum mountain agent learns optimal policy make underpowered climb mountain. hand pendulum domain agent learns balance pendulum upright initial position. cases common physical principle agent must learn exploit principle energy exchange. pendulum must swung upright position creating enough angular momentum smaller oscillations similarly must made climb steeper slope using energy exchanged moving slope mountain. principle good agent would easier balance pendulum learned related task make underpowered climb mountain. humans capable efﬁciently quickly generalizing learned skills related tasks. however algorithms capable performing efﬁcient transfer policies without learning domain reported. address main contribution fig. cross domain transfer source task cart-pole target taskbicycle vrml matlab used simulation environment demonstrate cross domain transfer. algorithm enables cross-domain transfer setting. leveraging notions apprenticeship learning adaptive control propose algorithm directly transfer learned policy source target task. abstract— paper present approach transfer learning reinforcement learning cross-domain tasks. many available techniques approach transfer architecture method speeding learning target task. propose adapt reuse mapped source task optimal-policy directly related domains. show optimal policy related source task near optimal target domain provided adaptive policy accounts model error target source. main beneﬁt policy augmentation generalizing policies across multiple related domains without re-learn tasks. results show architecture leads better sample efﬁciency transfer reducing sample complexity target task learning target apprentice learning. reinforcement learning machine learning paradigm robotic agent learns optimal policy performing sequential decision making without complete knowledge environment. recent successes deep reinforcement learning enabled agents solve complex problems balancing inverted pendulums playing atari games despite recent successes understand efﬁciently transfer learned policies task another particular level success achieved transferring policies state-space domains problem efﬁcient cross-domain skill transfer still quite open. consider term similar source target tasks sense exploit underlying physical principles state spaces entirely different. example primary results consider problem knowledge transfer balancing cart-pole bicycle balancing. system share common dimensionality state action spaces span across different coordinate frame. cart-pole deﬁned states cart pendulum action space lateral force cart whereas bicycle dynamics modelled handlebar rotation bicycle roll angle action space torque applied rider handlebar dynamics domain state space processes might completely different share commonality underlying physical principles. systems exhibit non-minimum phase dynamics also nature control policy same control signiﬁcant body literature transfer learning focused using learned source policy initial policy target task examples include transfer scenarios source target task similar mapping state space needed transfer human demonstrations however source target task different state-action spaces policy source cannot directly used target task. case mapping required stateaction space corresponding source target tasks enable knowledge transfer inter-task mapping supervised; provided agent hand coded using semantics state features unsupervised using manifold alignment sparse coding algorithm aforementioned methods accelerate learning minimize regret compared stand alone target. however inter-task mapping simple initializing target task learning transferred source policy lead sample efﬁciency transfer. particular approaches leverage fact tasks exploit physical principle possibility reusing source policy target domain. paper take different approach using source policy initialize target. inspired literature model reference adaptive control propose algorithm adapts source policy target task. also unlike mrac literature extend method probabilistic mdps discrete state-action spaces. argue optimal policies retain optimality across domains leverage physical principle different state-spaces. augment transferred policy policy adjustment term adapts difference dynamics tasks target space. adaptive policy could design match target model projected source model demonstrate adapted projected policy \u0001-optimal target task. adaptive policy termed designed accommodate difference transition dynamics projected source target task. beneﬁt method obviates need learn policy target space leading high sample efﬁcient transfer. section proposes novel transfer learning algorithm capable cross-domain transfer related dissimilar tasks. presented architecture applies continuous discrete state action space systems. unlike available state algorithms mainly concentrates policy initialization target task propose source policy directly optimal policy related target model. achieve step transfer online correction transferred policy adaptive policy derived based model transition error. presented approach three distinct phases phase involves ﬁnding optimal policy source task. purpose fitted q-iteration solve source mdp. since source task much simpler smaller problem compared target tasks assume always discover optimal policy source task. phase involves discovering mutual mapping state action space source target using unsupervised manifold alignment phase transfer adaptation mapped source optimal policy policy augmentation target domain. possible drawback proposed method could suboptimal transfer adaptive policy fails account total model error projected source target model. given small residual model error adaptive policy results suboptimal behavior target task. advantage high sample efﬁciency proposed technique near-optimal behavior target acceptable. noted; engage exploration target space transfer exploit projected source policy target space achieve nearoptimal behavior. nevertheless exploration improve upon adapted transferred policy achieve optimal solution left follow-on work. fitted q-iteration used learn optimal policy source task. policy search limited q-learning extended optimal policy generation methods. single layer shallow network used approximate function. tasks considered paper shallow networks found sufﬁcient complex tasks deep architecture multiple layers used. exercise underway left follow-on work. transfer setting source target task different representation state action spaces. crossdomain transfer requires inter-task mapping facilitate meaningful transfer. state space belonging different manifold cannot directly compared. unsupervised manifold alignment technique helps discover alignment data sets provide onto inter-task mapping. using inter-task mapping allows build step optimal policy target task. important note consider cardinality analogous action spaces analysis experiments ease exposition proposed transfer architecture. problems distinct nonuniform action spaces classiﬁcation methods correspondence action spaces transfer achieved augmenting transferred policy adaptive policy learned target model. proposed policy transfer adaptation method reuse source policy target space resulting near-optimal behavior. details inter-state mapping provided reference therein. phase transfer learning policy adaptation section presents transfer algorithm pair tasks continuous/discrete state action spaces. algorithm details policy adaptation using apprentice model. empirically show presented method sample efﬁcient compared methods transfer; since sample complexity learning optimal policy initialized target task reduced sample complexity local apprentice model learning. algorithm- leverages intertask mapping detailed subsection ii-b move back forth source target space knowledge transfer adaptive policy learning. performance policy transfer depends quality manifold alignment source target tasks. assume provides onto correspondence source target state spaces efﬁcient transfer. algorithm- provides pseudo-code using target apprentice. steps provide architecture cross-domain policy transfer step details policy adaptation target apprentice learning. assume underlying problem deﬁned markov decision process deﬁned tuple ﬁnite states; actions. markovian state transition model probability making transition upon taking action state solution horizon terminates steps. distribution initial states chosen reward function measuring performance agent assumed bounded rmax. total return states deﬁned discounted reward γi−tr discount factor. policy mapping states probability distribution actions agent’s goal policy maximize total return. formalize underlying transfer problem considering source target state space action space transition model respectively. general state space completely different domains. regarding action space domains assume assumption cardinality discrete action space source target task assume invertible mapping provides correspondence state space source target model. denote corresponding projected states time target source spaces respectively. transition probabilities also differ. however assume physics problem share similarities underlying principles. target transition probabilities modeled online using state-action-state triplets collected along trajectories generated random exploration policy. call approximate model apprentice target target space. assumption- every selected action source task greedy correspondence target task. using equivalence actions every selected action source task equivalent action target task selected selected action target task augmented derived adaptive policy adaptive action space total transferred policy solving related target task proposed linear combination mapped optimal policy adaptive policy follows proof analyze admissibility augmented policy target space. target model assumed nonlinear control afﬁne system source model nonlinear system. discrete transition model source target model considered follows target apprentice approximation target model. retain control afﬁne property target model using appropriate basis single layer neural network model target dynamics. approximate apprentice model target written function network weights basis expression demonstrates implementation modiﬁed optimal policy target task equivalent projecting source optimal trajectories target space. assuming existence unique correspondence between source target task space prove policy leads \u0001-optimal solution target model. target apprentice approximate model target task. paper consider target model apprenticeship learning using random policy explores randomly target domain re-use dataset state-actionstate triplets generated random policy manifold alignment target apprentice learning. data reuse leads saving time processing sample generation apprentice learning. using random policy explore target model collect state-action-state triplets learn target apprentice enable transfer learning using apprentice model. trials target task random policy steps. save state trajectories experienced. using accumulated data state-action-state triplets estimate system dynamics using least square linear regression linearly parametrized model store system parameters present results experiments evaluate proposed transfer learning framework. ﬁrst experiments consider transfer learning domains different transition models action spaces. ﬁrst problem discrete state action space second problem continuous state space nonstationary transition model target task. third fourth experiment focuses cross-domain transfer policy cart-pole mountain transferred bicycle problem inverted pendulum domains respectively. also demonstrate presented approach robust negative transfer ﬁnal experiment. compare presented target apprentice existing state transfer unsupervised manifold alignment transfer learn optimal policy source task using fqi. problem distinction environment/system parameters makes source target tasks different. target source domains state-space different transition models action spaces. also need target reward model similar source task proposed algorithm directly adapts policy source task engage target domain. grid world windy grid world source task experiment non-windy grid world. state variables describing system grid positions. objective navigate agent obstacles start goal position optimally. admissible actions right left reward function reaching goal position hitting obstacles everywhere else. target domain source added wind affects transition model parts state-space optimal policy source task learned using q-iteration. need inter-task mapping source target state spaces identical. start randomly sampled starting position execute policy target domain collect samples apprentice model learning. empirically show proposed method provides sample efﬁcient algorithm compared transfer techniques. figure shows results domain transfer grid world demonstrating ta-tl achieving successful transfer navigating grid obstacles wind bias. figure shows quality transfer faster convergence average maximum reward lesser training samples compared uma-tl methods. presented algorithm attain maximum average reward reaching goal position steps. uma-tl algorithm achieve similar performance steps respectively nearly order higher compared proposed ta-tl. inverted pendulum time-varying demonstrate approach continuous state domain inverted pendulum swing-up balance. source task conventional domain target task nonstationary inverted pendulum length mass pendulum continuously time varying function .cos. objective swing-up balance pendulum upright reward function selected −|θ| yields maximum value upright position minimum down-most position. action space full throttle right full throttle left zero throttle. note domain tricky since full throttle action assumed generate enough torque able lift pendulum upright position hence agent must learn swing pendulum generates oscillations leverages angular momentum upright position. target task differs source task transition model. source task learning single layer radial basis functions network. function modeled linear combination weights basis bases value function approximation bandwidth centers spanning space network learning rate iterations. figure shows quality transfer faster convergence average maximum reward lesser training samples proposed ta-tl method compared uma-tl methods. next consider even challenging transfer setting cross-domain transfer. problem setup similar domain transfer notable distinction state spaces different source target tasks. task agent learns ride bicycle. consider problem learning balance; concentrate navigation problem goal position. since navigation trajectory optimization problem i.e. agent learn focus maneuvering towards target learned balance bicycle upright. balancing interesting problem bicycle critical velocity. usually critically velocity bicycle self stabilizing approximately m/s. trying learn balance unstable bicycle forward velocity i.e. .m/s. also simulate imperfect balance inducing random noise displacement rider zero position. every time step agent receives information state bicycle angle angular velocity handlebar bike vertical respectively. given state agent chooses action applying torque handlebar trying keep bike upright. details bicycle dynamics beyond scope paper interested readers referred references therein. cart-pole source task learning balance bicycle. bicycle balance problem different cart pole cases objective keep unstable system upright. objective balance achieved systems moving direction fall. however control cart pole affects directly angle pole i.e. move cart always pole. bicycle control move handlebar direction fall. however balancing bike simple turn bike itself must ﬁrst steer direction called counter steering observe cart pole bicycle commonality dynamical behavior system non-minimum phase presence unstable zero. tend move initially direction opposite applied control. similarity qualiﬁes cart-pole system appropriate choice source model bicycle balance task. cart pole characterized state vector i.e. position velocity cart angle angular velocity pendulum. action space force applied cart cross-domain transfer requires correspondence inter-task space manifold mapping learned policy source transition model source target space back. discover correspondence state space bicycle cart pole model. solve optimal policy source cart pole model. linear network cart pole states used basis vector approximate action value function network learning rate figure shows average reward accumulated tatl uma-tl learning balance bicycle. typical learning process ta-tl performs transfer method converges maximum average reward episodes. episode simulation fig. windy windy grid world transfer agent navigating grid world source target domain average rewards training length comparing quality transfer ta-tl uma-tl convergence rate average reward training length fig. transfer cart-pole bike balancing task average rewards total simulation time agent balance bike negative transfer inverted pendulum average reward training length policy balance bicycle without toppling; episode ends bicycle falls time reached. figure shows total time bicycle balanced upright method. bike balancing time ta-tl methods highest times compared uma-tl method. mountain inverted pendulum tested cross-domain transfer mountain inverted pendulum. source target task characterized different state action space. source task benchmark problem driving underpowered hill. dynamics described continues state variables input action takes three distinct values full throttle right full throttle left throttle. reward function proportional negative utilize obtain mapping described training time section ii-b. report learn intertask mapping since common ta-tl uma-tl methods. used random policy generate samples manifold alignment target apprentice learning. source task uses learning single layer network optimal policy generation. source q-function modeled using function approximation using bases bandwidth diag centers spanning space network learning rate results training length involved ta-tl method figure sample lengths target apprentice learning. compare ta-tl uma-tl generic-rl target task. examine efﬁciency effectiveness transfer methods based sample efﬁciency learning target task speed convergence maximum average reward. similar domain transfer figure shows quality transfer ta-tl faster convergence average maximum reward lesser training samples compared uma-tl methods. proposed transfer robust negative transfers. given target model effectiveness transfer depends relevance source task target task. relationship strong transfer method take advantage signiﬁcantly improving performance target task transfer. however source target sufﬁciently related features source task correspond target transfer improve even decrease performance target task leading negative transfer. show uma-tl suffers negative transfer results performance presented tatl much superior compared uma-tl demonstrate inverted pendulum upright balance task. inverted pendulum model source target systems. target different source model sign control action. exactly dynamics source target model sign ﬂipped control effective term target observed initialized target task learning suffers negative transfer. indifferent sign change learns policy scratch. whereas tatl method since learn apprentice model target learn sign associated action well. thereby policy modiﬁcation term sign ﬂipped accordingly policy transfer performance achieved irrespective control sign change. figure shows quality transfer faster convergence average maximum reward lesser training samples proposed ta-tl method compared uma-tl methods. observed umatl method converges much lower average reward gets stuck local minima never achieves upright balance pendulum. also samples observed umatl learning task much higher compared transfer proposed ta-tl methods. introduced transfer learning technique leads sample efﬁcient transfer source target tasks. presented approach demonstrates nearoptimality transferred policy target domain augmenting adaptive policy; accounts model error target projected source. sample complexity transfer reduced target apprentice learning demonstrated empirically leads volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature pieter abbeel andrew exploration apprenticeship learning reinforcement international conference machine learning pages karl ˚astr¨om bj¨orn wittenmark. adaptive control. courier girish chowdhary tongbin mark cutler jonathan how. rapid transfer controllers uavs using learning-based robotics automation ieee adaptive control. international conference pages ieee matthew taylor peter stone yaxin liu. value functions rl-based behavior transfer comparative study. proceedings national conference artiﬁcial intelligence volume page menlo park cambridge london; aaai press; press; haitham ammar karl tuyls matthew taylor kurt driessens learning transfer sparse gerhard weiss. international conference coding. autonomous agents multiagent systems-volume pages international foundation autonomous agents multiagent systems haitham ammar eric eaton paul ruvolo matthew taylor. unsupervised cross-domain transfer policy gradient reinforcement learning manifold alignment. proc. aaai lisa torrey jude shavlik trevor walker richard maclin. reinternalational macros transfer reinforcement learning. tional conference inductive logic programming pages springer yaxin peter stone. value-function-based transfer reinproceedings forcement learning using structure mapping. national conference artiﬁcial intelligence volume page menlo park cambridge london; aaai press; press; jette randlov preben alstrom. learning drive bicycle using reinforcement learning shaping. proceedings fifteenth international conference machine learning pages karl ˚astr¨om richard klein anders lennartsson. bicycle dynamics control. ieee control systems magazine", "year": 2018}