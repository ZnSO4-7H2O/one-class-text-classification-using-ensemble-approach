{"title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR", "tag": ["cs.CL", "cs.NE"], "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.", "text": "convolutional neural networks standard component many current state-of-the-art large vocabulary continuous speech recognition systems. however cnns lvcsr kept pace recent advances domains deeper neural networks provide superior performance. paper propose number architectural advances cnns lvcsr. first introduce deep convolutional network architecture weight layers. multiple convolutional layers pooling layer small kernels inspired imagenet architecture. then introduce multilingual cnns multiple untied layers. finally introduce multi-scale input features aimed exploiting context negligible computational cost. evaluate improvements ﬁrst babel task resource speech recognition obtaining absolute improvement baseline training combined data different languages. evaluate deep cnns hub’ benchmark achieving word error rate cross-entropy training improvement best published result far. convolutional neural networks recently pushed state large-scale tasks many domains dealing natural data notably computer vision tasks like image classiﬁcation object detection object localization segmentation early applications neural nets speech recognition used time-delay neural nets seen simple forms cnns without pooling subsampling. full-ﬂedged cnns pooling subsampling soon applied speech recognition combined dynamic time warping globally-trained combination neural nets hmms speech handwriting goes back recent developments hmm/dnn hybrid modeling became dominant asr. context hybrid models cnns relatively recent cnns shown achieve state performance benchmark datasets broadcast news switchboard however contrast trend domains deeper architectures often shown gain performance classical architecture lvcsr convolutional layers. net) obtained second place classiﬁcation section imagenet competition. central idea replace large convolutional kernels stack kernels relu nonlinearities without pooling layers; authors argue advantage twofold additional nonlinearity hence expressive power reduced number parameters. using principles deep networks trained weight layers contrast classical cnns deployed lvcsr typically convolutional layers large kernels ﬁrst layer sigmoid activation functions. ﬁrst goal work adapt architecture lvcsr. closely related also uses net-inspired cnns lvcsr contrast work architectures investigated quite different paper provides results training non-standard switchboardh dataset close state performance hub’. context low-resource language tasks crucial leverage training data languages target language. therefore trained multilingual deep cnns describe section related multilingual neural networks hybrid nn-hmm systems extended multilingual bottleneck architectures tandem models proven valuable spoken term detection knowledge work published extends multilingual setup cnns. multi-scale features described section exploiting context computational cost. inspired recent success combining information multiple scales tasks like trafﬁc sign recognition semantic segmentation depth prediction lvcsr multi-scale idea explored tandem systems cldnn architecture training becomes challenging increasing depth used recently proposed optimization algorithms adadelta adam algorithms ﬁrst order gradient-based optimization methods keep track estimate ﬁrst second order moment gradient tune step size weight separately. rest paper organized follows. section introduce novel aspects work deep architectures multilingual training multi-scale features training details show experimental results babel switchboard table conﬁgurations deep cnns lvcsr. classic convnet convolutional layers kernels thus kernel size omitted. depth networks increases left right. deepest conﬁguration convolutional fully connected layers. leftmost column indicates number output feature maps layer. optional means four fully connected layers instead three deep convolutional networks describe adaptations architecture lvcsr domain networks convolutional layers dominated table shows conﬁgurations deep cnns. deepest conﬁguration weight layers convolutional fully connected. omit rectiﬁed linear unit layers following every convolutional fully connected layer. convolutional layers written conv kernel understood size pooling layers written stride equal pool size. architectures apply zero padding size every side every convolution architecture convolutions reduce size feature maps hence higher layers padding applied. contrast reinitialize deeper models shallower models. model trained scratch random initialization uniform distribution range follows argument initialize weights variance activations layer explode vanish forward pass. figure shows multilingual network used babel experiments. similar previous multilingual deep neural networks main difference shared lower layers network convolutional. fig. multi-scale feature maps context strides ﬁnal size feature along time dimension three input feature maps stacked input similar channels form input feature maps image. meaning weights biases multiple fully connected layers different language. since output dimension convolutional stages typically large using large context windows weights ﬁrst fully connected layer acts ﬂattened output convolutional stages. argument share large ﬁrst fully connected layer across languages. experimentally conﬁrmed architectures untying fully connected layers except lowest gives optimal performance strong degradation ﬁrst fully connected layer also untied. untying corresponds view shared layers ﬁrst fully connected layer shared multilingual feature extractor fully connected layers higher form classiﬁer. multilingual trained round-robin fashion process mini-batch language making update weights. shared part network gradients mini-batches accumulated weight updates. main goal constructing multi-scale feature maps context without increasing computational cost. figure illustrates concept multi-scale feature maps additional input feature maps contain larger view context frame downsampling larger context windows different strides. kernels ﬁrst convolutional layer able combine information multiple scales i.e. different distances central frame. difference convnet conﬁguration ﬁrst convolutional layer feature maps additional computational cost number parameters small. found style multi-scale training give small gains. increasing context size stronger positive impact though expense increased computational cost. adadelta adam initial training deep cnns. using adadelta main advantages. firstly experience optimization problem converges much faster sgd; babel experiments typically convergence million frames using hours babel training data secondly optimal working point adadelta’s hyperparameters stable across architectures always giving optimal performance. crucial order explore architectural variations. initial training adadelta tune using small learning rate. another aspect training improved results data balancing construct batches sampling target probability related frequency context dependent state sampling sample uniformly across frames target. exponent takes values balanced training unbalanced training using natural frequencies experiments babel proved optimal start raise training ﬁnal value experiments switchboard varied typically decoded priors adjusted match ﬁnal distribution. table babel different model architectures. left right increasing depth. bottom shows absolute improvement baseline. note adding fully connected layer -layer convolutional model degrades performance. table babel monolingual versus multilingual trained single language classical architecture slightly worse baseline dnn. however architecture gives average improvement even trained language. expected models training multilingual gives strong performance boost. ﬁrst experiments babel focuses multilingual multi-scale aspects work. iarpa babel program aimed developing robust keyword search technology resource languages. though word error rates reported high useful simple speech text applications useful keyword search systems still built based models. training data combination languages hours training data language. languages used training languages second option period babel program i.e. kurmanji pisin cebuano kazakh telugu lithuanian features used experiments standard log-mel features standardized global mean variance shared across speakers langauges. unless explicitly mentioned multi-scale features context babel experiments. report results crossentropy training adadelta varying trained multilingual deep architecture babel languages using alignments baseline speaker independent hmm/dnn systems using features context dependent states. context dependent states speciﬁc language. baseline system cross-entropy trained single language hours data. report cnns table multi-scale training different context windows. stands three scales context strides stride i.e. regular input features. multi-scale features provide modest gain. using larger context size gives better improvement however comes cost extra computation proportional context size convolutional layers. compared baseline summarize average absolute improvement baseline gives number compare different models. improvements baseline fairly consistent across languages. tables show results outlining performance gains different architectural improvements discussed section respectively. table note even monolingual case architecture outperforms classical baseline dnn. table results hub’ training -hour swb- dataset. obtain relative improvement baseline adaptation classical relative improvement means adadelta ﬁnetuning. means model trained random initialization using sgd. last column gives number frames convergence. evaluate deep architecture training hour swb- training data report word error rates hub’ switchboard experiments focus deep aspect work. apart involving multilingual training multi-scale features switchboard experiments speaker-dependent vtln deltas double deltas shown help performance classical cnns ﬁnetuning. typically achieve good performance minimal time. second strategy training scratch using requires training however performance slightly superior. classical momentum yielded gains sometimes slight degradation plain sgd. provide results total number frames convergence. note ﬁrst strategy achieve frames i.e. passes dataset using achieve passes data. present results cross-entropy training compare best published cross-entropy trained cnns. baseline work soltau using classical cnns feature maps convolutional layers. second baseline work saon introduces annealed dropout maxcnn’s large number states achieving cross-entropy training note improvements could readily integrated deep architectures. paper proposed number architectural advances cnns lvcsr. introduced deep convolutional network architecture small kernels multiple convolutional layers pooling layer inspired imagenet architecture. best performing model weight layers. also introduced multilingual cnns proved valuable context resource speech recognition. introduced multi-scale input features aimed exploiting acoustic context minimal computational increase. showed improvement standard baseline using hours data improvement combining languages train hours data. showed results hub’ training hours swb- data improvement baseline improvement best result published classical cnns cross-entropy training effort uses limited language packs iarpa babel program language collections iarpa-babelb-v.a iarpababelb-v.e iarpa-babelbv.a iarpa-babelb-v.a iarpa-babelb-v.b. work supported intelligence advanced research projects activity department defense u.s. army research laboratory contract number wnf--c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. gratefully acknowledge support nvidia corporation. authors would like thank pierre sermanet initial code base george saon vaibhava goel etienne marcheret xiaodong valuable discussions comments. bottou souli´e blanchet li´enard experiments time delay networks dynamic time warping speaker independent isolated digits recognition proc. eurospeech bottou souli´e blanchet li´enard speaker-independent isolated digit recognition multilayer perceptrons dynamic time warping neural networks vol. a.-r. mohamed sainath dahl ramabhadran hinton michael deep belief networks using discriminative features phone recognition proc. icassp. ieee", "year": 2015}