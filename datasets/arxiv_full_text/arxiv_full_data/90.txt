{"title": "Integrating planning for task-completion dialogue policy learning", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Training a task-completion dialogue agent with real users via reinforcement learning (RL) could be prohibitively expensive, because it requires many interactions with users. One alternative is to resort to a user simulator, while the discrepancy of between simulated and real users makes the learned policy unreliable in practice. This paper addresses these challenges by integrating planning into the dialogue policy learning based on Dyna-Q framework, and provides a more sample-efficient approach to learn the dialogue polices. The proposed agent consists of a planner trained on-line with limited real user experience that can generate large amounts of simulated experience to supplement with limited real user experience, and a policy model trained on these hybrid experiences. The effectiveness of our approach is validated on a movie-booking task in both a simulation setting and a human-in-the-loop setting.", "text": "training task-completion dialogue agent real users reinforcement learning could prohibitively expensive because requires many interactions users. alternative resort user simulator discrepancy between simulated real users makes learned policy unreliable practice. paper addresses challenges integrating planning dialogue policy learning based dyna-q framework provides sample-efﬁcient approach learn dialogue polices. proposed agent consists planner trained on-line limited real user experience generate large amounts simulated experience supplement limited real user experience policy model trained hybrid experiences. effectiveness approach validated movie-booking task simulation setting human-in-theloop setting. dialogue policy learning task-completion dialogue formulated sequential decision problem. reinforcement learning widely explored leverage user interactions improve dialogue agents. typically reinforcement learners need environment operate dialogue agent learn interactions real users on-line fashion improve adapt time. however best knowledge prior work learning dialogue policies algorithms real users online manner. biggest challenge high sample complexity algorithms demands large number samples environment dialogue policy learning real humans restricted trial-and-error setting. since expensive laborious acquire large amount dialogue examples real users learning scratch real users rendered impractical limited training data. tackle challenge popular approach build user simulator based corpus example dialogues. then reinforcement learning agents trained online fashion interacting simulated users. recently studies developing dialogue agents task-completion dialogue user simulators using deep neural networks conjunction model-free reinforcement learning studies showed dialogue agents trained user simulators serve effective starting point later deployed real humans improve reinforcement learning. though user simulator alleviates high sample complexity issue algorithms raises another issue discrepancy simulated users real users. ideally user simulator provide unlimited dialogue examples without expensive cost allows agent explore much larger policy space. however simulation-based approach still controversial issue dialogue research community because evaluation user simulator remains open problem universally accepted metric therefore best knowledge standard build user simulator. discrepancy simulated users real users make learned policy unreliable practice hard deployed reality hinder deployment dialogue systems real world settings. given challenges work integrate planning task-completion dialogue policy learning propose dialogue agent planner based dyna-q framework first agent equipped planner trained on-line limited real user experience mimic behavior users. internal simulated user planner agent plan multiple steps reason future generate large amounts simulated experience supplement limited real user experience avoiding sequences trailand-error real environment leads greater data efﬁciency. then policy model trained hybrid experiences simulation experiments movie-booking task show proposed agent signiﬁcantly improves sample efﬁciency learn dialogue policy. human-in-the-loop experiments show data-efﬁcient agent enables viable approach train dialogue policy reinforcement learning on-line real humans scalable deployed real-world dialogue systems. main contributions three-fold sample-efﬁciency proposed agent enables viable approach learn dialogue policy human-in-the-loop reinforcement learning on-line fashion task-completion dialogue systems. recently task-completion dialogue systems attracted numerous research efforts growing interest leveraging reinforcement learning dialogue policy learning. line research working single-domain taskcompletion dialogues deep reinforcement learning algorithms actor-critic policy gradients another line research work addresses complex setting multi-domain dialogues dialogue agents trained simulated users model-free reinforcement learning. despite widespread interest exploiting reinforcement learning task-completion dialogue systems still impractical learn dialogue policy scratch real users learners need environment interact with algorithms typically need many samples environment. while dialogue policy learning real humans restricted trial-and-error setting quite costly acquire large amounts dialogue examples real users. overcome limitation popular approach dialogue research community train agents using simulated users purpose many models introduced user modeling different dialogue systems. given reliance research community user simulations seems important assess quality simulator. best assess user simulator remains open issue universally accepted metric important feature good user simulator requires coherent behavior throughdialogue; ideally good metric measure correlation user simulation real human behaviors hard widely accepted metric. therefore best knowledge standard build user simulator. rule thumb trained agents evaluated real human well. controversial issue user simulators also brings uncertainty inconsistency dialogue agents trained simulated users. dhingra trained dialogue agent information access gains highest sucdialogue policy either pre-trained supervised learning based annotated humanhuman human-machine conversation data trained reinforcement learning simulated users dialogue agent serve effective starting point deploy real humans improve reinforcement learning. however discrepancy simulated users real users make learned policy unreliable practice hard deployed reality hinder deployment dialogue systems real world settings. work directly learn dialogue policy algorithms on-line manner real users task-completion dialogue systems. ﬁrstly trained agent interact teacher improve question-answering ability feedback teachers reinforcement learning. presented on-line reward learning optimize dialogue policy gaussian process model. deep reinforcement learning especially modelfree based methods witnessed remarkable success various tasks however model-free methods tend slower high sample complexity. model-based methods considered efﬁcient. alphago world champion computer system trains policy decide expand search tree using known environment world dynamic. also various research efforts tasks environment dynamics easy get. classic dyna algorithm learns model uses pseudo environment train policy. value iteration network implicitly incorporates planning policy learning iterative rollouts. predictron learns abstract model reward process applied evaluate proposed action model. proposed locally linear models local on-policy imagination rollouts accelerate model-free continuous q-learning robotic tasks. imagination-augmented agents improves policy learning aggregating rollouts information provided model. nevertheless performance relies pretrained observation-space model might able scale complex dynamic environment like dialogues. illustrated figure task-completion dialogue system mainly comprises three modules language understanding transforms natural language users system-readable semantic frames; natural language generation converts system actions natural language user; last important module dialogue manager keeps tracking dialogue stats controls dialogue policy. paper focus dialogue policy learning formulated markov decision process goal maximizing long-term objective associated reward function. extend dyna-q framework propose planner agent dialogue policy learning. similarly planner agent planner learnable model mimic behavior real user model-free policy model learns dialogue policy interacting real user planner. work planner model represented neural networks mimic behavior users while original dyna framework planner tabular method restrict planning capability previous known states actions fails generalize well unseen state action. real experience generated interaction real user roles used directly improve policy model update planner. denotes reward provided user planner time discount factor. policy optimized variety modelfree algorithms deep q-networks actor-critic algorithms. choose policy model showed remarkable performance task-completion dialogue system planning aforementioned planning process agent uses learned world model improve policy. paper refer learned world model planner aims mimic behavior users. takes dialogue stats agent action aagent input maps consequent user action auser scalar rewards binary termination formulate multi-task learning problem classiﬁcation tasks regression task. several different modeling choice applied purpose e.g. convolutional neural network recurrent neural network paper simply multi-layer perceptron planner model. learning process also termed direct method simpler achieved well performance various tasks. however requires tons interactions environment. planning process named indirect methods able make full real experience better policy less environment interactions. proposed planner agent combined processes improve dialogue policy learning much fewer interactions user. illustrated figure dialogue agent alternatively chooses either session learning agent learning planning. produces action passed user response termed direct update. otherwise planning agent produces action executed planner termed planning update. learning goal policy model learn policy controls dialogue system states actions order maximize expected reward possible dialogue trajectories accordance reward function expected reward deﬁned interaction procedure planner agent follows dialogue session users planner alternatively interact policy model. firstly users interact policy model accumulate real dialogue experience tuples used train policy model planner. afterwards planner utilized interact policy model number sessions generate times simulated dialogue experience tuples tuples used update policy model. planner agent combines learning planning dialogue policy learning. better exploits real dialogue experience helps exploration states previously experiences since planner model represented neural networks generalize unseen states actions. detailed summary training planner agent found algorithm work consider task-completion dialogue system attempts assist users book movie tickets. give systematic analysis sample efﬁciency proposed planner agent conduct intensive experiments simulated users since cheaper repeatable experiments. dataset conversational data movie-ticket booking task collected amazon mechanical turk annotations provided domain experts. total labeled dialogues average number turns dialogue approximately annotated data includes dialogue acts slots slots informable slots users constrain search requestable slots users values agent. perform action according policy receive reward switch state user store buffer duser sample random minibatch duser perform gradient descent loss equation perform action according policy receive reward switch state user store buffer dplanner sample random minibatch transition dplanner perform gradient descent loss equation example numberofpeople cannot requestable slot since arguably user knows many tickets wants buy. detailed data schema information found appendix adapted publicly available user simulator task-completion dialogue setting dataset described section during training simulator provides agent reward signal dialogue. dialogue considered successful movie ticket booked successfully information provided agent satisﬁes user’s constraints. dialogue agent receives positive reward turn success negative reward −max turn failure; experiments turn furthermore turn agent receives reward shorter dialogues encouraged. details user simulator found appendix standard model-free reinforcement learning model equipped learnable world model planner also hypothetical experiences generated planner real experience replay buffer; world model agent trained scratch. simulation experimented planning steps planner-ground agent agent extra times groundtruth dialogues experience added experience reply buffer serves upper bound planner agent boosted-planner agent. simulation experimented planning steps agents size hidden layer policy model planner. rmsprop applied optimize parameters. batch size policy model planner. training used \u0001-greedy strategy exploration simulation epoch simulated dialogue stored real state transition tuples experience replay buffer. planner agents times simulated dialogue experiences generated experience replay buffer. simulation epoch policy model updated transition tuples buffer batch manner. process training planner real dialogue experience tuples. cumulate experience replay buffer. procedure viewed implicit imitation learning initialize agent and/or planner. afterwards agent accumulates states transition tuples ﬂushes replay buffer after agent reaches success rate threshold e.g. equivalent performance rule agent. experiments planners investigate potential improvement space proposed planner agent baseline agent trained based real experience replay buffer serves lower bound performance planner-ground agent trained extra times real dialogue experience reply buffer viewed upper bound performance. figure shows performance curves three agents planning steps huge agent plannerground agent shows agent needs acquire real dialogue examples close reality quite costly obtain many real dialogue experiences. however proposed planner agent make number real dialogue examples agent planner agent provide extra times simulated dialogue experience train agent policy model. signiﬁcantly improves sample efﬁcient model moves close upper bound planner-ground agent. perfect real world early stage learning generate negative experiences misguide policy model agent. mitigate issue planner model agent pre-trained labeled dataset continue trained on-line interaction users adapt user behaviors. experiments called agent boostedplanner agent. figure shows performance curves four agents different planner models. agent planner model planner model planner agent learned scratch boosted-planner agents equipped pretrained planner model planner model boosted-planner agent ﬁxed without training interaction users planner model boostedplanner agent trained adapt users interactions. planner agent boosted-planner agents planning steps. boosted-planner agent learning faster planner agent. while different observation though boostedplanner learning faster planner agent early stage cannot adapt user behaviors since planner model trained interactions users performance boostedplanner agent cannot reach level planner agent. furthermore investigate impact different planning steps sample efﬁciency issue. figure shows performance curves four agents planning steps case involves planning trained time real dialogue experience reply buffer baseline agent. still gaps among different planning steps larger planning steps needs fewer dialogue examples reach ﬁxed success rate early stage much sample-efﬁcient important beneﬁt on-line humanin-the-loop dialogue policy learning much larger increase amount internal simulation also risk generate negative examples planner model good enough lead policy model local minimas. table shows test performance agents chosen three different training epochs number averaged runs generated simulated dialogues. training epochs planner agent better agent boosted-planner agent better planner agent agent larger planning steps help. appendix shows sample dialogue generated planner boosted-planner agent interacting policy model. verify sample efﬁciency proposed planner agent real world setting compare model-free agent on-line human experiment dialogue policy learning real users. benchmarked three agents planner boosted-planner experiment settings results human experiments agent randomly selected interact user avoid systematic bias user aware agent selected. beginning dialogue session user presented goal sampled user-goal corpus instructed converse agent complete given task. course conversation dialogue agents gather information customer’s desires ultimately books movie tickets. environment assesses binary outcome conversation based whether movie booked whether movie satisﬁes users constraints. three agents trained real users recruited authors’ afﬁliation curve averaged runs trained dialogues. total collected dialogue sessions. agents beginning training epoch user interacts agents generate real training dialogue example; planner agent boosted-planner agent planner model roll-out times simulated dialogue examples experience reply buffer agents trained based experience buffer epoch. figure shows learning curves agents trained real users terms success rate that planner agent learning much faster agent indicting planner agent much sample efﬁcient signiﬁcantly reduce number dialogues train good dialogue agent real world. paper propose integrate planning dialogue policy learning based dyna-q framework present planner agent consists planner policy learner planner trained limited real user experience endows agent learned simulated user plan multiple steps reason future generate large amounts simulated experience supplement limited real user experience avoiding sequences trailand-error real environment policy learner train dialogue policy model based hybrid experience. human-in-the-loop experiments show data-efﬁcient agent enables directly learn dialogue policy real users on-line fashion scalable deployed real-world dialogue scenarios. intensive analysis simulation boosted planner model reasonable larger planning steps improve sample efﬁciency agent. promising results suggest several interesting directions future research. first efﬁciency on-line human-in-the-loop dialogue policy learning single-domain task-completion dialogue system motivates investigate effectiveness complex multi-domain taskcompletion dialogue setting action state spaces much larger complicate planner model. second would valuable explore equip planning hierarchical reinforcement learning algorithms. third boosted planner model agent demonstrates strong adaptation ability tailor dialogue policy learning different users motivates systematically investigate dialogue personalization. pawel budzianowski stefan ultes pei-hao nikola mrksic tsung-hsien inigo casanueva lina rojas-barahona milica gasic. subdomain modelling dialogue management hierarchical reinforcement learning. arxiv preprint arxiv. heriberto cuay´ahuitl steve renals oliver lemon hiroshi shimodaira. human-computer dialogue simulation using hidden markov models. ieee workshop automatic speech recognition understanding. ieee. bhuwan dhingra lihong xiujun jianfeng yun-nung chen faisal ahmed deng. towards end-to-end reinforcement learning diaproceedlogue agents information access. ings annual meeting association wieland eckert esther levin roberto pieraccini. user modeling spoken dialogue sysautomatic speech recognition evaluation. understanding proceedings. ieee workshop ieee pages matthew frampton oliver lemon. learning effective dialogue strategies using limited dialogue move features. acl. association computational linguistics. gaˇsi´c mrkˇsi´c pei-hao david vandyke tsung-hsien steve young. policy committee adaptation multi-domain spoken dialogue systems. asru ieee pages milica gaˇsi´c dongho pirros tsiakoulis steve young. distributed dialogue policies multi-domain statistical dialogue management. icassp pages shixiang timothy lillicrap ilya sutskever sergey levine. continuous deep q-learning international model-based acceleration. conference machine learning. pages esther levin roberto pieraccini wieland eckert. stochastic model human-machine interaction learning dialog strategies. ieee transactions speech audio processing zachary lipton jianfeng lihong xiujun faisal ahmed deng. efﬁcient exploration dialogue policy learning networks replay buffer spiking. arxiv preprint arxiv. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton mastering game withhuman knowledge. nature david silver hado hasselt matteo hessel schaul arthur guez harley gabriel dulacarnold david reichert neil rabinowitz andre barreto predictron endarxiv preprint to-end learning planning. arxiv. pei-hao milica gasic nikola mrksic lina rojasbarahona stefan ultes david vandyke tsunghsien steve young. continuously learning neural dialogue management. arxiv preprint arxiv. pei-hao milica gasic nikola mrksic lina rojasbarahona stefan ultes david vandyke tsunghsien steve young. on-line active reward learning policy optimisation spoken dialogue systems. arxiv preprint arxiv. integrated architectures learning planning reacting based approxproceedings imating dynamic programming. seventh international conference machine learning. pages jason williams kavosh asadi geoffrey zweig. hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning. proceedings annual meeting association computational linguistics tiancheng zhao maxine eskenazi. towards end-to-end learning dialog state tracking management using deep reinforcement learning. arxiv preprint arxiv. bing gokhan dilek hakkani-tur pararth end-to-end opshah larry heck. timization task-oriented dialogue model arxiv preprint deep reinforcement arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature baolin peng xiujun jianfeng jingjing yun-nung chen kam-fai wong. adversarial advantage actor-critic model taskcompletion dialogue policy learning. arxiv preprint arxiv. baolin peng xiujun lihong jianfeng asli celikyilmaz sungjin kam-fai wong. composite task-completion dialogue policy learning hierarchical deep reinforcement learning. proceedings conference empirical methods natural language processing. pages olivier pietquin. consistent goal-directed user model realisitc man-machine task-oriented ieee internaken dialogue simulation. tional conference multimedia expo. ieee. olivier pietquin thierry dutoit. probabilistic framework dialog simulation optimal strategy learning. ieee transactions audio speech language processing s´ebastien racani`ere theophane weber david reichert lars buesing arthur guez danilo jimenez rezende adri`a puigdom`enech badia oriol vinyals nicolas heess yujia razvan pascanu peter battaglia demis hassabis david silver imagination-augmented daan wierstra. nips agents deep reinforcement learning. long beach usa. pages jost schatzmann blaise thomson karl weilhammer steve young. agenda-based user simulation bootstrapping pomdp dialogue sysnaacl companion volume short tem. papers. association computational linguistics pages jost schatzmann karl weilhammer matt stuttle steve young. survey statistical user simulation techniques reinforcement-learning dialogue management strategies. knowledge engineering review request inform deny conﬁrm question conﬁrm answer greeting closing sure multiple choice thanks welcome actor actress city closing critic rating date description distanceconstraints greeting implicit value movie series moviename mpaa rating numberofpeople numberofkids taskcomplete other price seating starttime state theater theater chain video format result ticket list task-completion dialogue setting entire conversation around user goal implicitly agent knows nothing user goal explicitly objective help user accomplish goal. generally deﬁnition user goal contains parts inform slots contain number slot-value pairs serve constraints user. request slots contain slots user information values wants values agent durticket default slot conversation. always appears request slots part user goal. make user goal realistic constraints user goal slots split groups. slots must appear user goal called elements required slots. movie-booking scenario includes moviename theater starttime date numberofpeople; rest slots optional slots example actor actress video format theater chain etc. generated user goals labeled dataset mentioned section using mechanisms. mechanism extract slots ﬁrst user turns data since usually ﬁrst turn contains required information user. mechanism extract slots ﬁrst appear user turns aggregate user goal.", "year": 2018}