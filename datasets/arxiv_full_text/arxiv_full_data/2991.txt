{"title": "Unsupervised Learning for Physical Interaction through Video Prediction", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abstract": "A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.", "text": "core challenge agent learning interact world predict actions affect objects environment. many existing methods learning dynamics physical interactions require labeled object information. however scale real-world interaction learning variety scenes objects acquiring labeled data becomes increasingly impractical. learn physical object motion without labels develop action-conditioned video prediction model explicitly models pixel motion predicting distribution pixel motion previous frames. model explicitly predicts motion partially invariant object appearance enabling generalize previously unseen objects. explore video prediction real-world interactive agents also introduce dataset robot interactions involving pushing motions including test novel objects. dataset accurate prediction videos conditioned robot’s future actions amounts learning visual imagination different futures based different courses action. experiments show proposed method produces accurate video predictions quantitatively qualitatively compared prior methods. object detection tracking motion prediction fundamental problems computer vision predicting effect physical interactions critical challenge learning agents acting world robots autonomous cars drones. existing techniques learning predict physics rely large manually labeled datasets however interactive agents unlabeled video data learn physical interaction autonomously collect virtually unlimited experience exploration. learning representation predict future video without labels applications action recognition prediction conditioned action agent amounts learning predictive model used planning decision making. however learning predict physical phenomena poses many challenges since real-world physical interactions tend complex stochastic learning video requires handling high dimensionality image pixels partial observability object motion videos. prior video prediction methods typically considered short-range prediction small image patches synthetic images models follow paradigm reconstructing future frames internal state model. approach propose method require model store object background appearance. appearance information directly available previous frame. develop predictive model merges appearance information previous frames motion predicted model. result model better able predict future video sequences multiple steps even involving objects seen training time. merge appearance predicted motion output motion pixels relative previous image. applying motion previous image forms next frame. present evaluate three motion prediction modules. ﬁrst refer dynamic neural advection outputs distribution locations previous frame pixel frame. predicted pixel value computed expectation distribution. variant approach call convolutional dynamic neural advection outputs parameters multiple normalized convolution kernels apply previous image compute pixel values. last approach call spatial transformer predictors outputs parameters multiple afﬁne transformations apply previous image akin spatial transformer network previously proposed supervised learning case latter methods predicted transformation meant handle separate objects. combine predictions single image model also predicts compositing mask transformations. cdna simpler easier implement models achieve comparable performance object-centric cdna models also provide interpretable internal representations. main contribution method making long-range predictions real-world videos predicting pixel motion. conditioned actions taken agent model learn imagine different futures different actions. learn physical interaction videos need large dataset complex object interactions. collected dataset robot pushing motions consisting million frames corresponding actions time step. experiments using robotic pushing dataset using human motion video dataset show models explicitly transform pixels previous frames better capture object motion produce accurate video predictions compared prior state-of-the-art methods. dataset video results code available online sites.google.com/site/robotprediction. video prediction prior work video prediction tackled synthetic videos short-term prediction real videos. yuan used nearest neighbor approach construct predictions similar videos dataset. ranzato proposed baseline video prediction inspired language models lstm models adapted video prediction patches actionconditioned atari frame predictions precipitation nowcasting mathieu proposed loss functions sharper frame predictions prior methods generally reconstruct frames internal state model predict internal state directly without producing images method instead transforms pixels previous frames explicitly modeling motion case cdna models decomposing image segments. found experiments three models produce substantially better predictions advecting pixels previous frame compositing onto image rather constructing images scratch. approach differs recent work optic prediction predicts pixels move using direct optical supervision. boots predict future images robot using nonparametric kernel-based methods contrast work approach uses ﬂexible parametric models effectively predicts interactions objects including objects seen training. knowledge previous video prediction method applied predict real images novel object interactions beyond time steps future. number promising methods frame prediction developed concurrently work vondrick combine adversarial objective multiscale feedforward architecture foreground/background mask similar masking scheme proposed here. brabandere propose method similar model softmax sharper distributions. probabilistic model proposed predicts transformations applied latent feature maps rather image itself demonstrates single frame prediction. learning physics several works explicitly addressed prediction physical interactions including predicting ball motion block falling effects forces future human interactions future trajectories methods require ground truth object pose information segmentation masks camera viewpoint image patch trackers. domain reinforcement learning model-based methods proposed learn prediction images either used synthetic images instance-level models demonstrated generalization novel objects accurate prediction real-world videos. shown figure architecture cdna model three proposed pixel advection models. convolutional lstms process image outputting normalized transformation kernels smallest middle layer network -channel compositing mask last layer kernels applied transform previous image different transformed images composited according masks. masks pixel channel-wise softmax. yellow arrows denote skip connections. comparison lstm-based prediction designed atari frames models work well synthetic domains necessarily succeed real images. video datasets existing video datasets capture youtube clips human motion synthetic video game frames driving however investigate learning visual physics prediction need data exhibits rich object motion collisions interaction information. propose large dataset consisting real-world videos robot-object interactions including complex physical phenomena realistic occlusions clear use-case interactive robot learning. order learn object motion remaining invariant appearance introduce class video prediction models directly appearance information previous frames construct pixel predictions. model computes next frame ﬁrst predicting motions image segments merges predictions masking. section discuss novel pixel transformation models propose effectively merge predicted motion multiple segments single next image prediction. architecture cdna model shown figure diagrams models appendix core models motion prediction module predicts objects’ motion without attempting reconstruct appearance. module therefore partially invariant appearance generalize effectively previously unseen objects. propose three motion prediction modules dynamic neural advection approach predict distribution locations previous frame pixel frame. predicted pixel value computed expectation distribution. constrain pixel movement local region regularizing assumption pixels move large distances. keeps dimensionality prediction low. approach ﬂexible proposed approaches. higher-dimensional transformation parameters outputted last layer instead lstm layer used cdna model. convolutional dynamic neural advection assumption mechanisms used predict motions different objects different regions image consider object-centric approach predicting motion. instead predicting different distribution pixel model predicts multiple discrete distributions applied entire image convolution computes expected value motion distribution every pixel. idea pixels rigid object move together therefore share transformation. formally predicted object transformation applied previous image produces image pixel follows spatial size normalized predicted convolution kernel multiple transformations output images applied previous image ˆit− form multiple images combined single prediction described next section show figure spatial transformer predictors approach model produces multiple sets parameters afﬁne image transformations applies transformations using bilinear sampling kernel formally afﬁne parameters produces warping grid previous image pixels generated image pixels image width height. type operator applied previously supervised learning tasks well-suited video prediction. multiple transfor} mations applied previous image ˆit− form multiple images composited based masks. architecture matches diagram figure instead outputting cdna kernels lstm layer model outputs parameters models focus learning physics rather object appearance. experiments show models better able generalize unseen objects compared models reconstruct pixels directly predict difference previous frame. cdna produce multiple object motion predictions need combined single image. composition predicted images modulated mask deﬁnes denotes channel mask element-wise multiplication pixels. obtain mask apply channel-wise softmax ﬁnal convolutional layer model ensures channels mask pixel position. practice experiments show cdna models learn mask objects moving consistent directions. beneﬁt approach two-fold ﬁrst predicted motion transformations reused multiple pixels image second model naturally extracts object centric representation unsupervised fashion desirable property agent learning interact objects. model lacks beneﬁts instead ﬂexible produce independent motions every pixel image. model including also include background mask allow models copy pixels directly previous frame. besides improving performance also produces interpretable background masks visualize section additionally previously occluded regions well represented nearby pixels allowed models generate pixels image included ﬁnal masking step. existing physics video prediction models feedforward architectures feedforward encodings image generate motion predictions discussed above employ stacked convolutional lstms recurrence convolutions natural multi-step video prediction takes advantage spatial invariance image representations laws physics mostly consistent across space. result models convolutional recurrence require signiﬁcantly fewer parameters parameters efﬁciently. model architecture displayed figure detailed appendix interactive setting agent’s actions internal state inﬂuence next image. integrate model spatially tiling concatenated state action vector across feature concatenating result channels lowest-dimensional activation map. note though agent’s internal state input network beginning must predicted actions future timesteps. trained networks using reconstruction loss. alternative losses presented could complement method. application action-conditioned video prediction learned model decision making visionbased robotic control tasks. unsupervised learning video enable agents learn world without human involvement critical requirement scaling interactive learning. order investigate action-conditioned video prediction robotic tasks need dataset real-world physical object interactions. collected dataset using robotic arms shown figure pushing hundreds objects bins amounting interaction sequences million video frames. test sets recorded motions also collected. ﬁrst test used different subsets objects pushed training. second test involved subsets objects none used training. addition images also record corresponding gripper poses refer internal state actions corresponded commanded gripper pose. dataset publically available. details data collection procedure provided appendix evaluate method using dataset section well videos human motion human.m dataset settings evaluate three models described section well prior models cdna used transformers. show stills predicted videos ﬁgures qualitative results easiest compare predicted videos viewed side-by-side. reason encourage reader examine video results supplemental website. code training model also available website. training details trained models using tensorflow library optimizing convergence using adam suggested hyperparameters. trained recurrent models without scheduled sampling report performance model best validation error. found scheduled sampling improved performance models substantially affect performance ablation baseline models model pixel motion. primary evaluation video prediction using robotic interaction dataset conditioned future actions taken robot. setting pass initial images well initial robot state actions sequentially roll model passing future actions model’s image state prediction previous time step. trained future time steps recurrent models test time steps. held training validation. quantitatively evaluate predictions measure average psnr ssim proposed unlike measure metrics entire image. evaluate test sets described section objects seen training time previously unseen objects. figure illustrates performance models compared prior methods. report performance feedforward multiscale model using l+gdl loss best performing model experiments full results multi-scale models appendix methods signiﬁcantly outperform prior video prediction methods metrics. lstm model reconstructs background lacks representational power reconstruct objects bin. feedforward multiscale model performs well -step prediction performance quickly drops time trained -step prediction. worth noting models signiﬁcantly parameter efﬁcient despite recurrent contain million parameters slightly less feedforward model million parameters signiﬁcantly less lstm model million parameters. found none models suffered signiﬁcant overﬁtting dataset. also report baseline performance simply copying last observed ground truth frame. figure compare models stacked convolutional lstm architecture predict pixel values difference previous current frames. explicitly modeling pixel motion method outperforms ablations. note model without skip connections representative model xingjian show second ablation figure illustrating beneﬁt training longer horizons conditioning action robot. lastly show qualitative results figure changing action examine model’s predictions possible futures. models prediction quality degrades time uncertainty increases future. mean-squared error objective optimizes mean pixel values. figure quantitative comparison models reconstruct rather predict motion. notice novel objects test larger models predict motion reconstruct appearance. model thus encodes uncertainty blur. modeling uncertainty directly through example stochastic neural networks interesting direction future work. note prior video prediction methods largely focused single-frame prediction demonstrated prediction multiple real-world video frames sequence. action-conditioned multi-frame prediction crucial ingredient model-based planning robot could mentally test outcomes various actions picking best given task. addition action-conditioned prediction also evaluate model predicting future video without actions. chose human.m dataset consists human actors performing various actions room. trained models human subjects held subject validation held different subject evaluations presented here. thus models never seen particular human subject subject wearing clothes. subsampled video noticeable motion videos within reasonable time frames. since model longer conditioned actions video frames trained network produce next frames corresponding second each. evaluation measures performance timesteps future. results figure show motion-predictive models quantitatively outperform prior methods qualitatively produce plausible motions least timesteps start degrade thereafter. also show masks predicted internally model masking previous figure cdna predictions starting image different future actions objects seen training set. images show predicted future zero action original action action larger original. note prediction shows motion zero action larger action predicts motion including object motion. work develop action-conditioned video prediction model interaction incorporates appearance information previous frames motion predicted model. study unsupervised learning interaction also present video dataset real robot interactions million video frames. experiments show that learning transform pixels initial frame model produce plausible video sequences time steps future corresponds second. comparisons prior methods method achieves best results number previous proposed metrics. predicting future object motion context physical interaction building block intelligent interactive system. kind action-conditioned prediction future video frames demonstrate allow interactive agent robot imagine different futures based available actions. mechanism used plan actions accomplish particular goal anticipate possible future problems recognize interesting phenomena context exploration. model directly predicts motion image pixels naturally groups together pixels belong object move together explicitly extract internal object-centric representation learning representation would promising future direction particularly applying efﬁcient reinforcement learning algorithms might beneﬁt concise state representations. would like thank vincent vanhoucke mrinal kalakrishnan barron deirdre quillen anonymous reviewers helpful feedback discussions. would also like thank peter pastor technical support robots.", "year": 2016}