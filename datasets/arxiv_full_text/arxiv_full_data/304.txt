{"title": "Answerer in Questioner's Mind for Goal-Oriented Visual Dialogue", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Goal-oriented dialogue has been paid attention for its numerous applications in artificial intelligence. To solve this task, deep learning and reinforcement learning have recently been applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose \"Answerer in Questioner's Mind\" (AQM), a novel algorithm for goal-oriented dialogue. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intent via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialogue tasks: \"MNIST Counting Dialog\" and \"GuessWhat?!.\" In our experiments, AQM outperforms comparative algorithms and makes human-like dialogue. We further use AQM as a tool for analyzing the mechanism of deep reinforcement learning approach and discuss the future direction of practical goal-oriented neural dialogue systems.", "text": "research recent studies goaloriented dialogues utilized deep learning using massive data train neural networks self-play environments. many researchers attempted solve goaloriented dialogue tasks using deep supervised learning approach based seqseq models deep reinforcement learning approach utilizing rewards obtained result dialogue however methods struggle correct model uses back-propagation owing complexity learning series sentences. algorithms tend generate redundant sentences making generated dialogues inefﬁcient idea deal goal-oriented dialogue introduce opponent modeling. approach agent considers opponent respond using explicit approximated model opponent. study motivated theory mind ability attribute mental states others understand mental states different wishes efﬁciently convey information opponent best converse maximizes opponent’s understanding method consider mind beyond part mental states goal-oriented dialogue paid attention numerous applications artiﬁcial intelligence. solve task deep learning reinforcement learning recently applied. however approaches struggle competent recurrent neural questioner owing complexity learning series sentences. motivated theory mind propose answerer questioner’s mind novel algorithm goal-oriented dialogue. questioner asks infers based approximated probabilistic model answerer. questioner ﬁgures answerer’s intent selecting plausible question explicitly calculating information gain candidate intentions possible answers question. test framework goal-oriented visual dialogue tasks mnist counting dialog guesswhat?. experiments outperforms comparative algorithms makes human-like dialogue. tool analyzing mechanism deep reinforcement learning approach discuss future direction practical goal-oriented neural dialogue systems. goal-oriented dialogue classical artiﬁcial intelligence problem including digital personal assistants order-byphone tools online customer service centers. goaloriented dialogue occurs questioner asks actionoriented question answerer responds intent letting questioner know correct action take. signiﬁcant research goal-oriented dialogue tackled problem though good solution provided propose answerer questioner’s mind’’ algorithm allow agent appropriate consecutive questions goal-oriented dialogue aqm’s questioner explicitly possesses approximated model answerer. questioner utilizes approximated model calculate information gain candidate answerer’s intentions answers question. experiment aqm’s question generator extracts proper questions training data really generates. test mainly goal-oriented visual dialogue tasks. several kinds visual-language tasks including image captioning visual question answering recent research goes propose multiturn visual dialogue tasks main experiment conducted guesswhat? cooperative two-player guessing game image achieves accuracy turns turns outperforming deep deep algorithms. though demonstrate performance models visual dialogue tasks approach directly applied general goal-oriented dialogue non-visual context. discussion section aside experimental success leverage tool analyzing deep approach goal-oriented dialogue tasks perspective theory mind. according argument training agents make plausible dialogues rewards self-play adaptable service scenario. enable agent converse human opponent agent self-play model human much possible. prove similar objective function objective function replaced direct objective function question instead reward assigned dialogue. series arguments discuss future directions building practical goal-oriented dialogue systems. section introduce series studies considering opponent’s uncertainty opponent model explicitly. refer methods theory mind approach. opponent modeling studies opponent modeling treated simple games multi-agent environment agent competed study foerster agent model opponent updates assuming opponent updated gradient descent argued modeling opponent could applied track non-stationary behavior opponent agent. model outperformed classical methods simple games tic-tac-toe rock-paper-scissors. hand applied opponent modeling cooperative multi-agent setting. believe opponent modeling also applied dialogue systems agents partially cooperative partially competitive. obverter recent study applied obverter technique motivated theory mind imagedescription-match classiﬁcation task study language emergence experiments choi agent transmitted sentence describing artiﬁcial images opponent agent. study obverter technique understood agent play questioner answerer maximizing consistency visual language modules. experimental results showed obverter technique generated word corresponding speciﬁc object argued method could alternative rl-based language emergence systems. compared model however uses real images creates multi-turn dialogue used general goal-oriented dialogue tasks. information gain aqm’s question-generator optimizes information gain using approximated opponent model. however concept utilizing information gain dialogue task classical rule-based approach. polifroni walker used information gain build dialogue system restaurant recommendations made decision tree using information gain asked series informative questions restaurant preferences. rothe applied similar method generate questions simple battleship game experiment noticeable used pre-deﬁned logic generate questions information gain criteria make novel human-like questions. unlike previous studies makes decision tree every context also considers uncertainty using multi-modal deep learning module require hand-made domain-speciﬁc rules. theory ai’s mind chandrasekaran argued human-machine collaboration could improved humans understand properties behavior opponent machine referred kind human understanding theory ai’s mind.’’ human experiment visual question answering team consisting human-questioner machine-answerer performed better human knew whether opponent could answer correctly. guesswhat? guesswhat? cooperative two-player guessing game proposed vries guesswhat? received attention ﬁeld deep learning artiﬁcial intelligence testbed research interplay computer vision dialogue systems. goal locate hidden object rich image scene asking sequence questions. participant called oracle answerer paper randomly assigned object image. participant questioner guesses object assigned answerer. questioner asks series questions answerer responds n/a. questioner decides guess correct object list candidate objects revealed. occurs questioner picks correct object. guesswhat? dataset contains mscoco images games question-answer pairs. visual dialog visual dialog agents also communicate questioning answering given mscoco image. unlike guesswhat? answer sentence restriction visual dialogue answerer. used dataset make goal-oriented dialogue task questioner guesses target image candidates test dataset dataset includes true caption image achieving percentile ranks around self-play experiments adding information dialogue improved percentile ranks around questioner answerer trained deep deep methods. means models predict correct image exact rest images test dataset. answerer questioner’s mind preliminary experimental setting machine players questioner answerer communicate natural dialogue. speciﬁcally exists class answerer’s intention goal-action questioner perform. answerer knows class whereas questioner not. goal dialogue questioner correct class asking series questions answerer. answerer responds correct answer given question. treat random variables class t-th question t-th answer respectively. becomes single instance. restaurant scenario example would like order? what you? coffees please. what’s password wi-fi? receive order americanos. customer know wi-fi password. aqm’s claim problem setting answerer requires module answer-generator questioner needs modules question-generator guesser. objective function three modules follows. information gain mutual information class current answer previous history current question given note maximizing information gain understood minimizing conditional entropy class given current answer answer-generator answergenerator module neural network trained minimizing cross-entropy answer distribution like deep approach. hand questioner’s question-generator guesser module possess approximated answer distribution simply likelihood likelihood obtained learning training data answer-generator distilling answer-generator module directly fergus explainable clear question selected. asking series questions corresponds constructing efﬁcient decision tree classiﬁer. introduce properties related explainability. aqm’s property performance aqm’s questioner optimal likelihood equivalent answering distribution opponent guesser module performance guesser posterior optimal performance questionˆ generator also increases becomes similar opponent ﬁxed. aqm’s property contexts history questioner requires posterior history used input likelihood comparative deep learning methods hidden neurons expected track context history; though known kinds information tracked. question asked independent previous questions context track posterior case posterior yellow figure corresponds hidden vector comparative dialogue studies. clearly explain mechanism introduce mnist counting dialog task goaloriented visual dialogue problem illustrated figure task utilizes concept mnist dialog dataset suggested like mnist dialog image mnist counting dialog contains digits four randomly assigned properties color {red blue green purple brown} bgcolor {cyan yellow white silver salmon} number style {ﬂatstroke}. unlike mnist dialog mnist counting dialog asks counting questions. type require information previous questions answers. term likelihood prior posterior perspective questioner classiﬁes class answerer’s answer distribution ﬁxed questioner achieves ideal performance likelihood question-generator aqm’s question-generator module selects maximum information gain simply calculate information gain question-generator module uses likelihood posterior algorithm figure explain question-generator module procedure. question-generator requires ˆpmodel prior ˜p-model likelihood q-sampler candidate questions. explainable explainable machine learning recently received much attention example figure asking number digits digits classiﬁes target image perfectly recognition accuracy digit image asking number digits help classify images zero. recognition accuracy less asking number digits better asking number digits variance number digits larger digits. mnist counting dialog task neural networks modeling questioner answerer. questioner’s answering model count-based property value trained training data. randomness uncertainty task ratio recognition accuracy property answerer. recognition accuracy digit decreases answering accuracy image decreased much. recognition accuracy color answering accuracy average. questions stroke used candidate questions. figure shows nearly always chooses true target image candidates turns recognition accuracy however also chooses correctly probability turns recognition accuracy respectively. random denotes questioner random question-generator module aqm’s guesser module. guesswhat? p-model prior questioner know list candidate objects asking questions. makes guesswhat? task difﬁcult although number candidates around yolo real-time object detection algorithm estimate candidate objects prior number extracted objects. p-model likelihood answerer model previous guesswhat? research inputs answer-generator module consist feature given context image feature cropped object context image spatial categorical information target object question time step answer-generator module assumes answer distribution independent history strategy make questioner’s answer distribution approximate answerer’s answer distribution ﬁrst inda trained separately training data. second depa trained answer question image also sampled training data. q-sampler candidate question compare q-samplers. ﬁrst randq’’ samples questions randomly training data. second countq’’ causes every question less dependent other. countq checks dependency questions following rule probability consecutive questions’ answers derived count pair answers questions training data. made inda used countq. size experimental results figure table shows experimental results. figure illustrates generated dialogue. best algorithm aqm-countq-depa achieved three turns outperforming deep deep algorithms. allowing questions algorithms achieved reached near-human performance. answerer’s answer distribution directly used questioner’s answer distribution aqm-countq achieved three turns turns. depa remarkably improved score self-play increased quality generated dialogue signiﬁcantly. hand countq boost score much increased quality generated dialogue. comparative deep method used question-generator hierarchical recurrent encoder-decoder achieving accuracy turns however random-randq-depa achieved turns competitive result deep model. comparative deep method applied reinforcement learning long short-term memory achieving turns many researchers studied dialogue systems cooperative games using increase score self-play environments discussion section discuss various issues research goal-oriented dialogue mainly based case goal-oriented visual dialogue studies primary argument considering opponent’s section discusses three objectives goal-oriented visual dialogue research score self-play service language emergence. section introduces three properties algorithms goal-oriented dialogue tasks using tool. section discusses direction future works make questioner agent applicable service. score goal-oriented visual dialogue research score used main measurements dialogue efﬁciency. however high score achieved optimization objective function particularly agent achieve high score probability distribution questioner answerer bound human’s distribution even human-like dialogue generated. example guesswhat? showed pre-deﬁned questions location provide accuracy turns methods divided image evenly three parts using vertical horizontal lines three cases answer {yes n/a}. natural language-based protocol also created using size color category major properties object. service ultimate objectives goal-oriented dialogue research create agent used real service however successful reports limited. chattopadhyay reported human-machine performance deep method study visual dialog method questioner answerer ﬁne-tuned thus answerer’s answering distribution differed training data. method also used objective function deep regularizer conserving generated dialogue human-like. nevertheless algorithm deteriorated score game human compared deep authors also assessed measures generated dialogue quality accuracy consistency image understanding detail question understanding ﬂuency. however human subjects reported deep algorithm performed worse deep algorithm measures except detail. language emergence plenty research recently published language emergence multiagent environment. studied artiﬁcial language whereas others attempted improve quality generated natural dialogue. best progress found latter study improvement quality series questions multi-turn vqa. deep applied questioner generates fewer redundant questions deep understood questioner methods optimized questioners improved answerers answerer gets objective function directly answer whereas questioner not. third line bayes rule used. assumption second line alleviated multi-step uses objective function question-generator module. multi-step optimal question cannot selected greedy unlike aqm. multi-step needs search tree structure; monte carlo tree search used reasonable solution. question-generator closely related discriminative-generative pair classiﬁers aqm’s question-generator guesser module explicitly likelihood whereas rl’s modules explicitly. properties example including optimal conditions sentence dynamics extended complexity rl’s question-generator decomposed tracking class posteriors rl’s property optimizing questioner answerer rewards makes agent’s performance human worse. true even process improves score self-play uses tricks maintain humanlike language generation. property self-play cooperative goal-oriented dialogue task different case alphago defeated human champion example guesswhat? reversed response answerer preserve score self-play makes score human-machine game near according aqm’s property play machine questioner human answerer performance optimal approximated answerer’s distribution aqm’s questioner human’s answering distribution. according rl’s property shares optimality condition distribution opponent. fine-tuning answerer agent reward makes distribution agent different human’s. therefore ﬁne-tuning agents decreases performance service situations. experiment human studied chattopadhyay explained section empirically demonstrates claim. rl’s property alternative objective function exists directly applicable question. reward applied game ﬁnished. goal training make language emergence make agent service agents communicate question-answering back-propagation including sharing attention image cross-entropy guesser round considered replace reward. information gain also used also alternative objectives backpropagation. theory mind methods enhanced service scenario considering answering distribution human. advantageous machine questioner questions human answer predictable words question high accuracy preferred. model uncertainty questioner also measured utilized recent studies bayesian neural networks uncertainty measures questioner initiative dialogue questioner need necessarily learn entire distribution human conservation. question questioner uses frequently self-play asked human. then obtained question-answer pairs used improving answerer like active learning. opponent modeling test-phase perspective making agent service self-play understood opponent modeling training-phase; opponent model self-play approximated model true opponent human. hand understood opponent modeling test-phase especially approximated model agent opponent model self-play. opponent modeling test-phase make easier handle non-stationary environment multi-domain problem. models also easily trained research training answerer model tasks progressed training questioner. however testphase opponent modeling difﬁcult back-propagate end-to-end way. thus requires additional techniques. example guesswhat q-sampler experiment replaced seqseq trained deep method would generate question optimizing sentence probability seqseq model information gain q-sampler corresponds regularizing deep approach. contributions two-fold. first proposed practical explainable goal-oriented dialogue system. algorithm scaled-up version theory mind methods realistic problem. self-play experiment outperformed comparative deep methods. second used tool analyzing deep research. argued approach shared objective function. discussion optimizing agents reward self-play makes service performance worse; reward replaced alternative objective function directly applicable question. limitation question-generator retrieves question training data generating question. future directions would include generating questions perspective discussed paper. however generating novel questions produced simple rule-based program replacing q-sampler seqseq model. primary interest make agent talk human practical service. future work make algorithm strengths theory mind deep approach. references antol stanislaw agrawal aishwarya jiasen mitchell margaret batra dhruv lawrence zitnick parikh devi. visual question answering. proceedings ieee international conference computer vision chandrasekaran arjun yadav deshraj chattopadhyay prithvijit prabhu viraj parikh devi. takes tango towards theory ai’s mind. arxiv preprint arxiv. chattopadhyay prithvijit yadav deshraj prabhu viraj chandrasekaran arjun abhishek stefan batra dhruv parikh devi. evaluating visual conversational agents cooperative human-ai games. arxiv preprint arxiv. kyunghyun merrienboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing abhishek kottur satwik gupta khushi singh yadav deshraj moura jos´e parikh devi batra dhruv. visual dialog. proceedings ieee conference computer vision pattern recognition abhishek kottur satwik moura jos´e stefan batra dhruv. learning cooperative visual dialog agents deep reinforcement learning. arxiv preprint arxiv. vries harm strub florian chandar sarath pietquin olivier larochelle hugo courville aaron. guesswhat? visual object discovery multi-modal dialogue. proceedings ieee conference computer vision pattern recognition foerster jakob chen richard al-shedivat maruan whiteson shimon abbeel pieter mordatch igor. arxiv learning opponent-learning awareness. preprint arxiv. cheolho sang-woo yujung kang wooyoung jaehyun zhang byoung-tak. criteria human-compatible two-player vision-language tasks. ijcai workshop linguistic cognitive approaches dialogue agents hernandez-leal pablo kaisers michael. learning sequential opponents repeated stochastic games. multi-disciplinary conference reinforcement learning decision making arbor kottur satwik moura jos´e stefan batra dhruv. natural language emerge ‘naturally’in multiagent dialog. proceedings conference empirical methods natural language processing lazaridou angeliki hermann karl moritz hermann tuyls karl clark stephen. emergence linguistic communication referential games symbolic pixel input. iclr lemon oliver georgila kallirroi henderson james stuttle matthew. dialogue system exhibiting reinforcement learning dialogue policies generic slotﬁlling talk in-car system. proceedings eleventh conference european chapter association computational linguistics posters demonstrations association computational linguistics tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. european conference computer vision springer junhua huang jonathan toshev alexander camburu oana yuille alan murphy kevin. generation comprehension unambiguous object descriptions. proceedings ieee conference computer vision pattern recognition andrew jordan michael discriminative generative classiﬁers comparison logistic regression naive bayes. advances neural information processing systems polifroni joseph walker marilyn. learning database content spoken dialogue system design. international conference language resources evaluation ribeiro marco tulio singh sameer guestrin carlos. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining paul hongsuck lehrmann andreas bohyung sigal leonid. visual reference resolution using attention memory visual dialog. advances neural information processing systems serban iulian sordoni alessandro bengio yoshua courville aaron pineau joelle. hierarchical neural network generative models movie dialogues. arxiv preprint arxiv. silver david schrittwieser julian simonyan karen antonoglou ioannis huang guez arthur hubert thomas baker lucas matthew bolton adrian mastering game without human knowledge. nature strub florian vries harm mary jeremie piot bilal courville aaron pietquin olivier. end-to-end optimization goal-driven visually grounded dialogue systems. arxiv preprint arxiv. vinyals oriol toshev alexander bengio samy erhan dumitru. show tell neural image caption proceedings ieee conference generator. computer vision pattern recognition tsung-hsien vandyke david mrksic nikola gasic milica rojas-barahona lina pei-hao ultes stefan young steve. network-based end-to-end trainable task-oriented dialogue system. arxiv preprint arxiv. zhao tiancheng eskenazi maxine. towards endto-end learning dialog state tracking management using deep reinforcement learning. arxiv preprint arxiv.", "year": 2018}