{"title": "Survey of the State of the Art in Natural Language Generation: Core  tasks, applications and evaluation", "tag": ["cs.CL", "cs.AI", "cs.NE", "I.2.7; H.5"], "abstract": "This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.", "text": "paper surveys current state natural language generation deﬁned task generating text speech non-linguistic input. survey timely view changes ﬁeld undergone past decades especially relation methods well applications technology. survey therefore aims give up-to-date synthesis research core tasks architectures adopted tasks organised; highlight number recent research topics arisen partly result growing synergies areas artiﬁcial intelligence; draw attention challenges evaluation relating similar challenges faced areas emphasis diﬀerent evaluation methods relationships them. content determination text structuring sentence aggregation lexicalisation referring expression generation linguistic realisation templates hand-coded grammar-based systems statistical approaches discussion stochastic planning uncertainty using reinforcement learning stochastic approaches acquiring data sequential stochastic process classiﬁcation optimisation ‘parsing’ deep learning methods encoder-decoder architectures conditioned language models discussion generating style textual variation personality generating feeling aﬀect politeness stylistic control challenge neural style aﬀect concluding remarks extrinsic evaluation methods black versus glass evaluation relationship evaluation methods metrics versus human judgements using controlled experiments evaluation concluding remarks", "year": 2017}