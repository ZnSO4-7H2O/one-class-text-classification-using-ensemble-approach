{"title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label  Classification", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.", "text": "andr´e martins† ram´on astudillo† †unbabel visconde santar´em lisboa portugal instituto telecomunicac¸ ˜oes instituto superior t´ecnico rovisco pais lisboa portugal instituto engenharia sistemas computadores alves redol lisboa portugal propose sparsemax activation function similar traditional softmax able output sparse probabilities. deriving properties show jacobian efﬁciently computed enabling network trained backpropagation. then propose smooth convex loss function sparsemax analogue logistic loss. reveal unexpected connection loss huber classiﬁcation loss. obtain promising empirical results multi-label classiﬁcation problems attention-based neural networks natural language inference. latter achieve similar performance traditional softmax selective compact attention focus. softmax transformation component several statistical learning models encompassing multinomial logistic regression action selection reinforcement learning neural networks multi-class classiﬁcation recently also used design attention mechanisms neural networks important achievements machine translation image caption generation speech recognition memory networks various tasks natural language understanding computation learning log-likelihood loss function taking logarithm output. alternatives proposed literature bradley-terry model multinomial probit spherical softmax softmax approximations theoretically computationally advantageous certain scenarios lack convenient properties softmax. paper propose sparsemax transformation. sparsemax distinctive feature return sparse posterior distributions assign exactly zero probability output variables. property makes appealing used ﬁlter large output spaces predict multiple labels component identify group variables potentially relevant decision making model interpretable. crucially done preserving attractive properties softmax show sparsemax also simple evaluate even cheaper differentiate turned convex loss function. propose sparsemax loss loss function sparsemax analogue logistic regression show convex everywhere differentiable regarded multi-class generalization huber classiﬁcation loss important tool robust statistics finally devise neural selective attention mechanism using sparsemax transformation evaluating performance natural language inference problem encouraging results sparsemax transformation deﬁnition -dimensional simplex. interested functions vectors probability distributions ∆k−. functions useful converting vector real weights probability distribution classical example softmax function deﬁned componentwise limitation softmax transformation resulting probability distribution always full support i.e. softmaxi every disadvantage applications sparse probability distribution desired case common deﬁne threshold small probability values truncated zero. essence prop. states need evaluating sparsemax transformation compute threshold coordinates threshold shifted amount others truncated zero. call threshold function. piecewise linear function play important role sequel. alg. illustrates na¨ıve algorithm uses prop. evaluating sparsemax. words sparsemax returns euclidean projection input vector onto probability simplex. projection likely boundary simplex case sparsemax becomes sparse. sparsemax retains important properties softmax addition ability producing sparse distributions. projecting onto simplex well studied problem linear-time algorithms available start recalling well-known result projections correspond soft-thresholding operation. below notation max{ proposition solution form highlight properties common softmax sparsemax. maxk denote maximal components deﬁne indicator vector whose component otherwise. denote maxk maximal components second largest. vectors zeros ones respectively. proposition following properties hold {softmax sparsemax}. interpreting temperature parameter ﬁrst part prop. shows sparsemax zerotemperature limit behaviour softmax without need making temperature arbitrarily small. prop. reassuring since shows sparsemax transformation despite deﬁned differently softmax similar behaviour preserves invariances. note properties satisﬁed proposed replacements softmax example spherical softmax deﬁned two-class case well known softmax activation becomes logistic function. precisely softmax next show analogous fact sparsemax hard version sigmoid. using prop. that fig. provides illustration threedimensional cases. latter parameterize plot softmax sparsemax function sparsemax piecewise linear asymptotically similar softmax. jacobian matrix transformation /∂zj]ij importance train models gradient-based optimization. next derive jacobian sparsemax activation recall jacobian softmax looks like. instructive compare eqs. regard jacobian sparsemax laplacian graph whose elements fully connected. compute need obtained time algorithm evaluates sparsemax. deﬁned sparsemax transformation established main properties show transformation design loss function resembles logistic loss yield sparse posterior distributions. later apply loss label proportion estimation multi-label classiﬁcation. denotes delta distribution otherwise. well-known result; plugged gradient-based optimizer leads updates move probability mass distribution predicted current model gold label something similar sparsemax? nice aspect log-likelihood adding loss terms several examples assumed i.i.d obtain log-probability full training data. unfortunately idea cannot carried sparsemax labels exactly probability zero model assigns zero probability gold label would zero probability entire training sample. course highly undesirable. possible workaround deﬁne another possibility explore here construct alternative loss function whose gradient resembles note gradient particularly important since directly involved model updates typical optimization algorithms. formally want lsparsemax differentiable function ∇zlsparsemax sparsemax. generally consider problem estimating sparse label proportions target probability distribution assume training dataset input vector target distribution outputs assumed sparse. subsumes single-label classiﬁcation delta distributions concentrated single class. generalization multinomial logistic loss setting lsoftmax softmax) kl.) denote kullback-leibler divergence shannon entropy respectively. note that constant loss equivalent standard logistic regression soft labels. gradient loss note ﬁrst four properties prop. also satisﬁed logistic loss except gradient given ﬁfth property particularly interesting since satisﬁed hinge loss support vector machines. however unlike hinge loss lsparsemax everywhere differentiable hence amenable smooth optimization methods l-bfgs accelerated gradient descent coincidentally next show sparsemax loss binary case reduces huber classiﬁcation loss important loss function robust statistics note ﬁrst that have sorted components note second expression equals ﬁrst asserts continuity loss even though non-continuous two-class case otherwise. assume without loss generality correct label deﬁne eqs. dataset lewis datasets removed examples without labels reuters dataset normalized features zero mean unit variance. statistics datasets presented table recent work investigated consistency multi-label classiﬁers various micro macro-averaged metrics among plug-in classiﬁer trains independent binary logistic regressors label tunes probability threshold validation data. test time labels whose posteriors threshold predicted used procedure baseline comparison. second baseline multinomial logistic regressor using loss function target distribution uniform active labels. similar probability threshold used prediction label predicted compare systems sparsemax loss function found beneﬁcial scale label scores constant test time applying sparsemax transformation make resulting distribution sparsemax sparse. predict label optimized three losses l-bfgs tuning hyperparameters heldvalidation -fold cross-validation hyperparameters regularization constant probability thresholds logistic show simulation results sparse label proportion estimation synthetic data. since sparsemax predict sparse distributions expect superiority task. generated datasets training test examples. example emulates multi-labeled document variable-length sequence word symbols assigned multiple topics pick number labels sampling poisson distribution rejection sampling draw labels multinomial. then pick document length poisson repeatedly sample words mixture label-speciﬁc multinomials. experimented settings uniform mixtures random mixtures vocabulary size equal number labels varied average document length words. trained models optimizing {lsoftmax lsparsemax} picked regularization constant results shown fig. report mean squared error test respectively target predicted label posteriors) jensen-shannon divergence observe losses perform similarly small document lengths average document length exceeds sparsemax loss starts outperforming logistic loss consistently. stronger signal sparsemax estimator manages identify correctly support label proportions contributing reduce mean squared error divergence. occurs uniform random mixtures. figure simulation results estimation label posteriors uniform random mixtures shown mean squared error jensen-shannon divergence function document length logistic sparsemax estimators. {n/k} results shown table overall performances three losses similar slight advantage sparsemax attained highest results experiments logistic softmax times each. particular sparsemax appears better suited problems larger numbers labels. experiments task natural language inference using recently released snli corpus collection human-written english sentence pairs. pair consists premise hypothesis manually labeled labels entailment contradiction neutral. used provided training development test splits. architecture system shown fig. proposed rockt¨aschel compare performance four systems noattention rnn-based system similar bowman logisticattention attention-based system independent logistic activations; softattention near-reproduction rockt¨aschel attention-based system; sparseattention replaces latter softmax-activated attention mechanism sparsemax activation. figure network diagram inference problem. premise hypothesis rnns. noattention system replaces attention part direct connection last premise state output logisticattention softattention sparseattention systems respectively independent logistics softmax sparsemax-activated attention mechanism. example denote respectively projected premise hypothesis word vectors. sequences recurrent networks instead long short-term memories rockt¨aschel used gated recurrent units behave similarly fewer parameters. premise generates state sequence rd×l follows table examples sparse attention natural language inference task. nonzero attention coefﬁcients marked bold. system classiﬁed four examples correctly. examples picked rockt¨aschel model parameters w{xzxrxhhzhrhh} rd×d {zrh} likewise hypothesis generates state sequence initialized last state premise noattention system computes ﬁnal state based last states premise hypothesis follows optimized systems adam using default parameters setting learning rate tuned -regularization coefﬁcient rockt¨aschel dropout probability inputs outputs network. results shown table observe soft sparse-activated attention systems perform similarly latter slightly accurate test outperform noattention logisticattention systems. rockt¨aschel report scores slightly ours reached test accuracy implementation softattention best system elaborate word-by-word attention model. differences former case distinct word vectors lstms instead grus. table shows examples sentence pairs highlighting premise words selected sparseattention mechanism. that examples small number words selected making ﬁnal decision. compared softmax-activated mechanism provides dense distribution words sparsemax activation yields compact interpretable selection particularly useful long sentences bottom row. introduced sparsemax transformation similar properties traditional softmax able output sparse probability distributions. derived closed-form expression jacobian needed backpropagation algorithm proposed novel sparsemax loss function sparse analogue logistic loss smooth convex. empirical results multi-label classiﬁcation attention networks natural language inference attest validity approach. connection sparse modeling interpretability signal processing approach distinctive model assumed sparse label posteriors model parametrizes. sparsity also desirable property neural networks present rectiﬁed units maxout nets several avenues future research. ability sparsemax-activated attention select variables attend makes potentially relevant neural architectures random access memory since offers compromise soft hard operations maintaining differentiability. fact harder forms attention often useful arising word alignments machine translation pipelines latent variables sparsemax also appealing hierarchical attention deﬁne top-down product distributions along hierarchy sparse distributions produced sparsemax automatically prune hierarchy leading computational savings. possible disadvantage sparsemax softmax seems less gpu-friendly since requires sort operations linear-selection algorithms. however recent work providing efﬁcient implementations algorithms gpus would like thank rockt¨aschel answering various implementation questions m´ario figueiredo chris dyer helpful comments draft report. work partially supported fundac¸˜ao para ciˆencia tecnologia contracts uid/eea// uid/cec//. alabi tolu blanchard jeffrey gordon bradley fast k-selection algorithms steinbach russel. graphics processing units. journal experimental algorithmics astudillo ramon amir silvio wang silva m´ario trancoso isabel. learning word representations scarce noisy data embedding subproc. association computational spaces. linguistics beijing china bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. international conference learning representations bowman samuel angeli gabor potts christopher manning christopher large annotated corpus learning natural language inference. proc. empirical methods natural language processing bridle john probabilistic interpretation feedforward classiﬁcation network outputs relationships statistical pattern recognition. neurocomputing springer kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. proc. empirical methods natural language processing chorowski bahdanau dzmitry serdyuk dmitriy kyunghyun bengio yoshua. attention-based models speech recognition. advances neural information processing systems duchi shalev-shwartz singer chandra efﬁcient projections onto l-ball learning high dimensions. proc. international conference machine learning grefenstette edward hermann karl moritz suleyman mustafa blunsom phil. learning transduce unbounded memory. advances neural information processing systems hermann karl moritz kocisky tomas grefenstette edward espeholt lasse will suleyman mustafa blunsom phil. teaching machines read comprehend. advances neural information processing systems koyejo sanmi natarajan nagarajan ravikumar pradeep dhillon inderjit consistent multilabel classiﬁcation. advances neural information processing systems jeffrey socher richard manning christopher glove global vectors word representation. proceedings empiricial methods natural language processing penot jean-paul. conciliating generalized derivatives. demyanov vladimir pardalos panos batsyn mikhail constructive nonsmooth analysis related topics springer rockt¨aschel grefenstette edward hermann karl moritz koˇcisk`y tom´aˇs blunsom phil. reasoning entailment neural attention. arxiv preprint arxiv. rush alexander chopra sumit weston jason. neural attention model abstractive sentence summarization. proc. empirical methods natural language processing kelvin jimmy kiros ryan courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. international conference machine learning ezk; sparsemax since equals plus constant finally turn fourth property. ﬁrst inequality states softmax case follows trivially fact exponential function increasing. sparsemax proof contradiction. suppose sparsemaxi sparsemaxj. deﬁnition must sparsemax ∆k−. leads contradiction choose sparsemaxk sparsemaxj sparsemaxi. prove second inequality sufﬁces consider binary case i.e. need prove tanh/) tanh comes tanh tanh tanh sparsemax given coordinates three things happen thresholded case smaller truncated case truncated case ﬁrst claims minima lsparsemax zero gradient i.e. satisfy equation sparsemax furthemore prop. sparsemax never increases distance coordinates i.e. sparsemaxk− sparsemaxj therefore sparsemax implies maxj=k prove converse statement note distance decreased smallest coordinate truncated zero. establishes equivalence ﬁfth claim. finally minimum loss value achieved case leading", "year": 2016}