{"title": "Optimizing the Latent Space of Generative Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images. GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs. To this end we introduce and study Generative Latent Optimization (GLO), a framework to train a generator without the need to learn a discriminator, thus avoiding challenging adversarial optimization problems. We show experimentally that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.", "text": "generative adversarial networks shown able sample impressively realistic images. training consists saddle point optimization problem thought adversarial game generator produces images discriminator judges images real. generator discriminator commonly parametrized deep convolutional neural networks. goal paper disentangle contribution optimization procedure network parametrization success gans. introduce study generative latent optimization framework train generator without need learn discriminator thus avoiding challenging adversarial optimization problems. show experimentally enjoys many desirable properties gans learning large data synthesizing visually-appealing samples interpolating meaningfully samples performing linear arithmetic noise vectors. generative adversarial networks powerful framework learn generative models natural images. gans learn generative models setting adversarial game learning machines. first generator plays transform noise vectors fake samples resemble real samples drawn distribution natural images. second discriminator plays distinguish real fake samples. training generator discriminator learn turns. first discriminator learns assign high scores real samples scores fake samples. second generator learns increase scores fake samples fool discriminator. proper training generator able produce visually-appealing samples noise vectors. recently gans used produce samples remarkable quality distributions natural images handwritten digits human faces house interiors furthermore gans exhibit three strong signs generalization. first generator translates linear interpolations noise space semantic interpolations image space. words linear interpolation noise space generate smooth interpolation visually-appealing images. second generator allows linear arithmetic noise space. similarly word embeddings linear arithmetic indicates generator organizes noise space disentangle nonlinear factors variation natural images linear statistics. third generator able synthesize visually-appealing samples. allows applications image inpainting super-resolution despite successes training evaluating gans notoriously difﬁcult. adversarial optimization problem implemented gans sensitive random initialization architectural choices hyper-parameter settings. many cases fair amount human care necessary correct conﬁguration train particular dataset. common observe generators similar architectures hyper-parameters exhibit dramatically different behaviors. even properly trained resulting generator synthesize samples resemble localized regions data distribution several advances made stabilize training gans task remains science. difﬁculty training gans aggravated challenges evaluation since likelihood respect data intractable current gold standard evaluate quality model eyeball samples produced generator. evaluation discriminator also simple since visual features learned discriminator always transfer well supervised tasks finally application gans non-image data relatively limited. research question model natural images gans generator discriminator often parametrized deep convolutional networks therefore reasonable hypothesize reasons success gans modeling natural images come complementary sources contribution. investigate importance propose drastic simpliﬁcation training approach called generative latent optimization maps learnable noise vector images dataset minimizing simple reconstruction loss. since predicting images learnable noise borrows inspiration recent methods predict learnable noise images alternatively understand auto-encoder latent representation produced parametric encoder learned freely. contrast gans track correspondence learned noise vector image represents. hence goal meaningful organization noise vectors mapped target images. turn generative model observe sufﬁces draw vectors span learned noise vectors. experiments show inherits many appealing features gans enjoying much simpler training. particular study efﬁcacy compress decompress dataset images generate samples perform linear interpolations extrapolations noise space perform linear arithmetics super-resolution experiments provide quantitative qualitative comparisons principal component analysis deep convolutional autoencoders gans. focus mnist cifar- stl- celeba lsunbedroom datasets. conclude exposition section generative latent optimization model first consider large images image dimensions w×h. second initialize d-dimensional random vectors third pair dataset images random vectors obtaining dataset finally jointly learn parameters generator optimal noise vector image solving learnable distinctive features joint optimization random inputs model parameter contrast autoencoders assume parametric model usually referred encoder compute vector samples minimize reconstruction loss since vector free parameter model recover solutions found autoencoder reach others. nutshell viewed encoder-less autoencoder discriminator-less gan. choice representation space encapsulate prior knowledge data xn}. instance sparse coding literature common practice representation space closed unit ball since interested matching properties gans make similar choices comes representation space common choices representation space gans either uniform distribution hypercube normal distribution since gaussian distributions lead stable training distribution design representation space. since random vectors drawn d-dimensional normal distribution unlikely land outside ball rescale random vectors drawn d-dimensional normal distribution representation space choice loss function. hand squared-loss function simple choice leads blurry reconstructions natural images. hand gans convnet loss function. since early layers convnets focus edges samples sharper. therefore experiments provide quantitative qualitative comparisons loss laplacian pyramid loss loss corresponds small convnet estimates earth mover’s distance pixel space using euclidean ground metric. mathematically j-th level laplacian pyramid representation therefore loss weights details multiple scales equally particular penalize small shifts locations edges strongly euclidean metric. optimization. choice differentiable generator objective differentiable respect therefore learn stochastic gradient descent gradient respect obtained backpropagating gradients generator function project back representation space update. case generative adversarial networks. gans introduced goodfellow reﬁned multiple recent works described section gans construct generative model probability distribution setting adversarial game generator discriminator practice applications gans concern modeling distributions natural images. cases generator discriminator parametrized deep convnets among multiple architectural variations explored literature prominent deep convolutional generative adversarial network therefore paper speciﬁcation generator function dcgan construct generator across experiments. autoencoders. simplest form auto-encoder pair neural networks formed encoder decoder role autoencoder compress data representation using encoder decompress using decoder therefore autoencoders minimize ex∼p simple loss function mean squared error. vast literature autoencoders spanning three decades conception renaissance recent probabilistic extensions main messages paper deep convolutional auto-encoders autoencoders encoder decoder deep convnets exhibit many attractive features gans like allow much simpler training. several works combined gans aes. instance zhao replace discriminator ulyanov replace decoder generator gan. similar works suggest combination standard pipelines lead good generative models. work attempt step further explore learning generator alone possible. inverting generators. several works attempt recovering latent representation image respect generator. particular lipton tripathi show possible recover generated sample. similarly creswell bharath show possible learn inverse transformation generator. works similar gradients particular feature convnet back-propagated pixel space order visualize feature stands for. theoretical perspective bruna explore theoretical conditions network invertible. inverting efforts instances pre-image problem bora recently showed possible recover trained generator compressed sensing. similiar work loss backpropagate gradient rank distribution. however train generator simulatenously. jointly learning representation training generator allows extend ﬁndings. santurkar also generative models compress images. several works used optimization latent representation express purpose generating realistic images e.g. works total loss function optimized generate trained separately optimization latent representation work train latent representations generator together scratch; show test time sample latent either simple parametric distributions interpolation latent space. learning representations. arguably problem learning representations data unsupervised manner long-standing problems machine learning earliest algorithms used achieve goal principal component analysis instance used learn low-dimensional representations human faces produce hierarchy features nonlinear extension autoencoder turn extended algorithms learn low-dimensional representations data. similar algorithms learn low-dimensional representations data certain structure. instance sparse coding representation image linear combination elements dictionary features. recently zhang realized capability deep neural networks large collections images noise vectors bojanowski joulin exploited similar procedure learn visual features unsupervisedly. similarly bojanowski joulin allow noise vectors move order better learn mapping images noise vectors. proposed analogous works opposite direction learn noise vectors images. finally idea mapping images noise learn generative models well known technique organized experiments follows. first section describes generative models compare along implementation details. section reviews image datasets used experiments. section discusses results experiments including compression datasets generation samples interpolation samples arithmetic noise vectors dcae deep convolutional autoencoder minimizing mean-squared error. encoder follows architecture dcgan discriminator radford removing tanh layer letting last layer return feature maps instead decoder follows architecture dcgan generator. impose constraint representation space output encoder. adam optimizer default parameters. dcgan since gans come mechanism retrieve random vector associated image estimate random vector instantiating random vector computing updates backpropagation convergence. obviously experiments figure percent representation space explained function number singular vectors used reconstruct glo. curve evenly distributed energy across spectrum. models curves allotted space store information. train model generator follows architecture generator dcgan. stochastic gradient descent optimize setting learning rate learning rate representation space ease optimization initialize noise vectors solution provided pca. evaluate models datasets natural images. unless speciﬁed otherwise prescribed training splits train generative models. images rescaled three channels pixels short side center-cropped normalized pixel values mnist handwritten digits. original training following compare methods described section applied datasets described section particular evaluate performance methods tasks compressing dataset generating samples performing sample interpolation sample arithmetic. dataset compression start measuring reconstruction error terms mean-squared loss loss table shows reconstruction error models datasets losses. despite much simpler gans obtains better reconstruction error even using loss. figure shows reconstruction examples obtained different sizes latent space glo. figure show quantity representation space explained function number eigenvectors used reconstruct approach efﬁciently representation space spread information around concentrates information directions. figure shows samples dcae dcgan models datasets. case gaussian distribution full covariance representation space sample distribution generate samples. samples quite good even simple model space; leave careful modeling space future work. figure shows examples interpolations different images form celeba dataset lsun bedroom. compare interpolation z-space linearly interpolate forward model linear interpolation image space. interpolation different image space showing generator learns non-linear mapping noise image. example faces slowly rotating. difference type interpolations clearer middle interpolation images columns. non-linear transformation observed bedrooms. finally figure show difference loss laplacian loss high quality model i.e. space dimensions. sparse loss creates sharper interpolations sqaure loss. figure conditional generation model. image left maps pooling operator corresponding image right. model trained normal sampling time variable intialized independently measure loss targets pooling. note variety hallucinations generated. although conditioning perfect quite good; average square error projected generated image projected target square norm projected target. spirit radford showcase effect simple arithmetic operations noise space model. precisely average noise vector three images wearing sunglasses remove average noise vector three images wearing sunglasses average noise vector three images women wearing sunglasses. resulting image resembles woman wearing sunglasses glasses shown figure model also used conditional generation. this take fully trained model measure loss conditioned variables. example hallucinate upsamplings image backpropagating variable measure error output pooling operation applied generated image. small image condition pooling operator takes large image maps small image minimize instead x||. similar condition right side pixels matching target left side free etc. optimization usually lead error conditioning exact; error often made quite small. experimental results presented work suggests that image domain recover many properties generative models trained gans convnets trained reconstruction losses. invalidate promise gans generic models uncertainty methods building generative models suggest order fully test adversarial construction researchers need move beyond images convnets. hand practitioners care generating images particular application parameterized discriminator improve results reconstruction losses model searches alleviating instability training. results promising level latest results obtained recent works generate images larger images generated here. much work done gap. particular exploring loss functions model architectures adding structure space using sophisticated sampling methods training model improve results obtained method. ledig theis husz´ar caballero cunningham acosta aitken tejani totz wang photo-realistic single image super-resolution using generative adversarial network. arxiv preprint arxiv. nguyen yosinski bengio dosovitskiy clune. plug play generative networks conditional iterative generation images latent space. ieee conference computer vision pattern recognition turk pentland. face recognition using eigenfaces. computer vision pattern recognition proceedings cvpr’. ieee computer society conference ieee vincent larochelle bengio p.-a. manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning.", "year": 2017}