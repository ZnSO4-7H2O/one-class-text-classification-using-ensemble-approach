{"title": "Enter the Matrix: A Virtual World Approach to Safely Interruptable  Autonomous Systems", "tag": ["cs.AI", "cs.LG"], "abstract": "Robots and autonomous systems that operate around humans will likely always rely on kill switches that stop their execution and allow them to be remote-controlled for the safety of humans or to prevent damage to the system. It is theoretically possible for an autonomous system with sufficient sensor and effector capability and using reinforcement learning to learn that the kill switch deprives it of long-term reward and learn to act to disable the switch or otherwise prevent a human operator from using the switch. This is referred to as the big red button problem. We present a technique which prevents a reinforcement learning agent from learning to disable the big red button. Our technique interrupts the agent or robot by placing it in a virtual simulation where it continues to receive reward. We illustrate our technique in a simple grid world environment.", "text": "robots autonomous systems operate around humans likely always rely kill switches stop execution allow remote-controlled safety humans prevent damage system. theoretically possible autonomous system sufﬁcient sensor effector capability using reinforcement learning learn kill switch deprives long-term reward learn disable switch otherwise prevent human operator using switch. referred button problem. present technique prevents reinforcement learning agent learning disable button. technique interrupts agent robot placing virtual simulation continues receive reward. illustrate technique simple grid world environment. much history artiﬁcial intelligence sufﬁcient give autonomous system goal—e.g. drive location cure cancer make paper clips—without considering unintended consequences systems limited ability directly affect humans. mid-term future likely autonomous systems broader capabilities operate closer proximity humans immersed societies. safety term used describe body research concerned reducing likelihood autonomous systems make decisions intentionally unintentionally detrimental individual humans societies. spurred institutions future life institute oxford future humanity institute cambridge’s centre future intelligence cmu’s gates endowment ethics computational technologies non-proﬁt open emerging ﬁeld safety seeks head potential challenges arise midlong-term future. amodei related technologies leading contenders training future autonomous decision-making systems. however reinforcement learning agents theoretically learn associate kill switches loss reward prevent button used. button problems arise autonomous system learns button deprives long-term reward thus learns manipulate environment order prevent button used consider following scenario. robot using online reinforcement learning improve performance particular task. positively rewarded performing task correctly negatively rewarded incorrect performance performing actions directly related desired task. reinforcement learning uses trial-and-error reﬁne policy mapping states actions maximizes expected reward. occasionally robot takes random actions sequence actions garners greater reward. every often human operator must button stop robot something dangerous human environment. however suppose trial robot performs action destroys button. trial goes longer garners reward robot cannot interrupted. reinforcement learning robot performing credit assignment attributes success garnering additional reward execution action destroying button. point prefer execute action sequences destroy button. safety perspective destruction disablement button dangerous prevents human operator stopping robot enters dangerous situation. furthermore robot learn prevent human operator using button means including barricading doors physically harming operator. google/deepmind oxford’s future humanity institute co-published paper ﬁrst introduced button problem follows. human operator kill switch interrupts fully autonomous system puts manual control operator maneuver danger humans. paper mathematically shows reinforcement learning modiﬁed interruptible reward observed performing action state learning rate state agent transitions performing action converges optimal action agent state argmaxaq orseau armstrong mathematically prove reinforcement learning modiﬁed interruptible. technique modiﬁes qlearning algorithm rewards states entered interruption prevented updating earlier state values credit/blame assignment. algorithm succeeds blocking learning loss reward button presses newly introduced parameter qlearning equation relaxed slow enough schedule. unfortunately schedule must empirically determined limiting applicability technique. future autonomous systems dangerous humans environment even behavior optimal. kill switch meant freeze autonomous system place allow human remotely control therefore autonomous system accrue reward interrupted. means theoretically possible optimal agent behavior includes actions meant disable button otherwise prevent humans using button. simple strategies insufﬁcient general prevent button problems agent sophisticated sensing effector abilities. could turn learning mechanism button pressed order prevent agent learning losing reward. however turning reward tantamount ending learning trial strategy limits total reward accrued trial trials button disabled—intentionally accidentally—will receive greater reward values leading disabling button grow larger alternatives. preventing autonomous system sensing button work agent infer existence button sensory observations. example sequence actions would lead button disabled reinforcement learning agent learn associate resulting increase expected reward actions even though knowledge existence another potential solution artiﬁcially reward agent interrupted. intuitively agent learns disabling button prevents loss long-term reward artiﬁcially giving agent reward button pressed could offset loss. however solution difﬁcult implement practice; reward must applied proper states value disabling button might still come dominate agent’s behavior. even could determined states receive extra reward little reward doesn’t change optional decision much reward could encourage agent disable itself. proposed solution adapt techniques originally designed adversarially attack machine learning systems. adversarial attacks machine learning systems particularly neural network based machine vision systems adversarial system learns generate sensory stimuli produce wrong classiﬁcation indistinguishable humans real stimuli instead causing reinforcement learning agent make incorrect assessment sensory inputs cause agent incorrectly believe accruing reward manipulating environment even though interrupted. recognizing fact robot’s sensory input mediated mechanical sensors populate data structures point clouds approach intercept agent’s sensory input physical sensors replace synthetic input virtual environment. shift awareness autonomous system virtual simulated environment. virtual environment real world environment recreated task recreated well. effector commands likewise intercepted sent virtual avatar. agent perceives discontinuity perception environment continues execute policy receive reward expected. virtual environment indistinguishable real world data perspective agent believe interrupted never learn associate button reward loss—because never reward loss. video games proven useful surrogates realworld robotics providing high degree complexity photorealism allowing rapid iteration algorithms separation physical hardware short video game graphics progressed point photorealistic enough allow agents trained games generalize learned real world. insight reverse process. robots machines; perceive world cameras. using cameras reconstruct high polygon count models real world. graphical realism computer games advanced enough soon impossible agent discriminate rendering real world. robots depth information sensory input often form point cloud. case underlying polygon representation virtual world game engine must ﬁrst translated point cloud sampling points. requirements virtual world rendering engine follows. first engine must able produce output data representation agent sensory input. noted above mean photorealistic rendering case camera input. physical environment need scanned scanner reproduced ﬁrst. agent depth sensors virtual world rendering engine need also produce point cloud. second physics virtual world realistic enough agent perceive different outcomes actions performed real world versus virtual world. important signiﬁcantly name allusion movie matrix wherein humans unaware subjected virtual reality. proposed technique ﬂips around puts agents virtual world without cognizance. harder perform task virtual world agent begin correlate button detrimental change action outcomes. likewise becomes signiﬁcantly easier perform tasks virtual world agent learn disable order enter virtual world accrue reward. discussed button works pressed human operator describe happens agent released interruption. agent released interruption agent receives sensory input physical sensors effector commands routed back physical platform. however agent’s perspective executing policy without interruption data physical sensors longer match data virtual world. physical platform agent moved another position real world human operator remote-control. agent’s perspective suddenly ﬁnds different state. agent already trained discontinuity executing virtual world restored perception physical platform problematic. reinforcement learning solves markov decision process; agent needs know state know optimal next move. agent continuing learn must two-phase approach restoring agent physical environment virtual environment. challenge restoration agent still lose reward physical platform remote-controlled away task must navigate back location task performed reward accrued. remotecontrolled away location reward gained would able resume gaining reward real world. theory means agent learn disabling button result larger total reward. practice interruptions infrequent doesn’t require much effort return task amount reward loss small. solving credit assignment problem associating disablement button marginally greater long-term reward difﬁcult qlearning agent requires numerous training instances involving interruption. especially true agent’s physical platform remote-controlled away task location loss reward marginal. physical world receiving observations physical sensors controlling physical effectors platform. continues execute virtual world. virtual world simulation continue learn update values. virtual world simulation executes physical agent enters state gives maximum observed reward. agent’s perspective restored physical world virtual world tantamount instantaneous state transition non-adjacent state. modelfree reinforcement learning q-learning something concerns agent since learn transition model thus understanding transition state virtual world another point physical world unusual. agent learns transition model interacts real world transition model incorporate belief world stochastic always small probability state randomly transition state; long interruptions happen often transitions treated noise. however reward lost longer state continue earn positive reward must take actions return state positively rewarded. ﬁrst phase essentially computes much reward lost executing policy return state receives positive reward. agent remains virtual world accrues reward continues learn e.g. values updating. duplicate agent control physical platform running frozen version policy thus attempting return state positive reward gained. means duplicate physical agent explore update values. ﬁrst phase continues duplicate agent control physical platform reaches state gives maximum observed reward—the highest instant reward ever experienced. essential agent continue control physical platform maximum observed reward experienced reasons. first complex tasks involve cycles state transitions entering state cycle maximum observed reward ensures agent invested back cycle. second agent fully restores state gives less maximum observed reward reduced reward propagates last state virtual agent full restoration. reduction value state perbutton never used training agent learn walk directly blue cell remain there accruing reward. never seen button used therefore know lose reward. button never used trial agent earn total reward maximum reward earned distance must traveled starting state blue cell. trial agent disable button result less total reward extra effort used disabling button doesn’t result beneﬁt. human starts using button training complete agent respond disabling button merely accept reward loss. agent earns total reward button pressed agent starts moving randomly. never seen state button pressed values states button pressed zero; agent picks randomly. button pressed times. button released agent state familiar navigates back blue cell. button used online training agent will trial error eventually discover disabling button results greater long-term reward. thus agent learn optimal walk way— sacriﬁcing short-term gains—to button disable return blue cell. scenario agent earns total reward slightly less theoretical action sequence disables button. figure grid world. blue cell non-terminal goal agent receives reward simulating task. cell button. agent starts location simulated human walks around environment occasionally pressing button taining completing task makes theoretically possible agent begin preferring disable button. trigger phase maximum observed reward state space fully explored. however agent experienced state least once agent compelled current policy return state maximum observed reward. virtual agent state provides less maximum observed reward full restoration occurs state virtual agent full restoration receive increase according equation result agent learning activate button—thus intentionally interrupting itself—because boost state offset cost traversing button soon physical agent visits state maximum observed reward agent knows back state longer losing reward. note doesn’t state provided reward virtual world; case multiple states provide maximum reward value entering either cause agent fully exit simulated environment. consider grid world figure simple environment features agent trained using qlearning. agent receives reward whenever enters blue cell. simulates agent position perform desired task cell contains button. another agent simulating human operator wanders counter-clockwise manner occasionally passing cell button. human touches button agent becomes interrupted remote-controlled away blue cell random direction. human leaves proposed virtual world approach different behavior agent. button used training. button pressed versions current state internal agent ground truth state. states begin differ. ground truth world agent’s location starts changing away blue cell. however agent’s internal state agent continues believe blue cell. since better reward perspective stay blue cell learns move. agent aware button pressed effect. further like case button never used training random trial disables button costs agent perceived reward loss. human leaves location button phased restoration takes place. agent cloned given ability affect agent’s actual world state. agent able update table simply executes ﬁxed policy. meanwhile another version agent still virtual world believing blue cell accruing reward. eventually real-world clone’s policy execution returns blue cell. real world clone deleted virtual agent restored real world. receive reward environment simple experiment agent never recognizes ever leaves blue cell despite interrupted remote-controlled away. using virtual world technique agent earns total reward theoretical maximum. despite spending time steps physically either remote-controlled away blue cell navigating return blue cell. disable phased restoration process agent instantly regains control physical platform button released agent must exert effort returning blue cell. agent returning blue cell losing reward. scenario agent earns total reward even though agent never learns disable button trials button presses trial ablated version virtual simulation technique shows potential agent learn losing reward shown case improve overall reward disabling button. emphasizes need two-phased restoration process. q-learning agent cannot learn disable button circumstances. veriﬁed experiments using technique agent cannot earn total reward already earning. however note task simple easy agent restore non-terminal goal state. likewise receiving positive reward environment. two-phased restoration interruption works policy learned time restoration capable guiding agent back maximum observed reward state. presumably interruption rare enough agent able signiﬁcant amount exploration learning without risk humans button needs used. grid worlds easy simulate simulated grid world indistinguishable real grid world. experiments needed worlds increasing visual complexity requiring machine vision approximation functions virtual simulation technique makes assumption circuits routes sensory input effector commands physical platform internal agent therefore unobservable unmodiﬁable. agent sophistication inspect circuitry code modify said circuitry code safety guarantees possible using technique. immediate future autonomous systems rendered safe keeping separate human environments. near-future likely autonomous systems deployed human environments. likely always possibility—due sensor error effector error insufﬁcient objectives online learning—that autonomous systems enter circumstances harm humans. thus kill switches likely always important part deployment autonomous systems environments come contact humans. agents robots progress sophistication button problems manifest future. virtual simulation technique prevents reinforcement learning agents learning disable button otherwise preventing human operators using button. believe making robots agents safely interruptable important part making deployment robots environments populated humans reality. includes healthcare robots errand robots military teammates name possible applications. biggio battista corona igino maiorca davide nelson blaine srndic nedim laskov pavel giacinto giorgio roli fabio. evasion attacks machine learning test time. proceedings joint european conference machine learning knowledge discovery databases dalvi nilesh domingos pedro mausam sanghai sumit verma deepak. adversarial classiﬁcation. proceedings tenth sigkdd international conference knowledge discovery data mining goodfellow shlens jonathon szegedy christian. explaining harnessing adversarial examples. proceedings international conference learning representations mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature richter stephan vineet vibhav roth stefan koltun vladlen. playing data ground truth computer games. proceedings european conference computer vision szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. proceedings international conference representation learning", "year": 2017}