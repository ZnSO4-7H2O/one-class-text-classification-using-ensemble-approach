{"title": "Integrated Inference and Learning of Neural Factors in Structural  Support Vector Machines", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors.", "text": "tackling pattern recognition problems areas computer vision bioinformatics speech text recognition often done best taking account task-speciﬁc statistical relations output variables. structured prediction internal structure used predict multiple outputs simultaneously leading accurate coherent predictions. structural support vector machines nonprobabilistic models optimize joint input-output function margin-based learning. ssvms generally disregard interplay unary interaction factors training phase ﬁnal parameters suboptimal. moreover factors often restricted linear combinations input features limiting generalization power. improve prediction accuracy paper proposes joint inference learning integration back-propagation loss-augmented inference ssvm subgradient descent; extending ssvm factors neural networks form highly nonlinear functions input features. image segmentation benchmark results demonstrate improvements conventional ssvm training methods terms accuracy highlighting feasibility end-to-end ssvm training neural factors. traditional machine learning output consists single scalar whereas structured prediction output arbitrarily structured. models proven useful tasks output interactions play important role. examples image segmentation partof-speech tagging optical character recognition taking account contextual cues predicting output variables beneﬁcial. widely used framework conditional random ﬁeld models statistical conditional dependencies input output variables well output variables mutually. however many tasks require ‘most-likely’ predictions rise nonprobabilistic approaches. rather optimizing bayes’ risk models minimize structured loss allowing optimization performance indicators directly model structural support vector machine generalization hinge loss multiclass multilabel prediction used. output dependencies) trained sequentially. unary classiﬁcation model optimized interactions trained post-hoc. however two-phase approach suboptimal errors made training interaction factors cannot accounted during training unary classiﬁer. another limitation ssvm factors linear feature combinations restricting ssvm’s generalization power. propose extend linearities highly nonlinear functions means multilayer neural networks refer neural factors. towards goal subgradient descent extended combining loss-augmented inference back-propagation ssvm objective error unary interaction neural factors. leads better generalization synergy ssvm factor types resulting accurate coherent predictions. model empirically validated means complex structured prediction task image segmentation msrc- kitti sift flow benchmarks. results demonstrate integrated inference learning and/or using neural factors improves prediction accuracy conventional ssvm training methods -slack cutting plane subgradient descent optimization furthermore demonstrate model able perform current state-of-the-art segmentation models msrc- benchmark. although combination neural networks structured probabilistic graphical models dates back early interest topic resurging. several recent works introduce nonlinear unary factors/potentials structured models. task image segmentation chen train convolutional neural network unary classiﬁer followed training dense random ﬁeld input pixels. similarly farabet combine output maps convolutional network image segmentation zemel propose semisupervised maxmargin learning nonlinear unary potentials. contrary works trade bifurcated training approach integrated inference training unary interactions factors. several works focus linear-chain graphs using independently trained deep learning model whose output serves unary input features. contrary works focus general graphs. works suggest kernels towards nonlinear ssvms approach nonlinearity representing ssvm factors arbitrarily deep neural networks. arti`eres propose potentials represented multilayer networks. performance linear-chain probabilistic model demonstrated optical character speech recognition using two-hidden-layer neural network outputs unary potentials. furthermore joint inference learning linearchain models also proposed peng however application general graphs remains open problem contrary works popose nonprobabilistic approach general graphs also modeling nonlinear interaction factors. recently schwing urtasun train convolutional network unary classiﬁer jointly fully-connected task image segmentation similar chen advocate joint learning reasoning approach structured model probabilistically trained using loopy belief propagation task optical character recognition image tagging. related work includes domke uses relaxations combined messagepassing learning. related work aiming improve conventional ssvms works wang hierarchical part-based model proposed multiclass object recognition shape detection focusing model reconﬁgurability compositional alternatives and-or graphs. liang propose convolutional neural networks model end-toend relation input images structured outputs active template regression. propose learning structured model multilayer deformable parts action understanding propose hierarchical structured model action segmentation. contrast paper takes nonprobabilistic approach wherein ssvm optimized subgradient descent. algorithm altered back-propagate ssvm loss errors based ground truth loss-augmented prediction factors. moreover factors nonlinear functions allowing learning complex patterns originate interaction features. section essential ssvm background introduced integrated inference backpropagation explained nonlinear unary factors. finally notion generalized ssvm model using neural factors optimized alteration subgradient descent. tion function outputs scalar. contrast whose output arbitrarily structured. class labels. structured models employ compatibility function parametrized prediction done solving called inference i.e. obtaining most-likely assignment labels similar maximum-aposteriori inference probabilistic models. because combinatorial complexity output space hence important impose kind regularity exploited inference. done ensuring corresponds nonprobabilistic factor graph eﬃcient inference techniques exist general linearly parametrized product weight action factors functions sums individual joint input-output features nodes interactions corresponding factor graph example case section nodes image regions interactions connections regions joint feature vector. data samples conform graphical structure i.e. composed unary features interaction features maximizing corresponds minimizing state nonprobabilistic factor graph factorizes product factors. however operating log-domain state decomposes factors. traditional ssvm training methods optimize joint parameter vector unary interaction factors. however restrict parameters linear combinations input features allow limited nonlinearity addition kernels. objective function case arbitrary nonlinear factors often hard optimize many numerical optimization methods require convex objective function formulation. example -slack cutting plane training requires conversion constraints quadratic programming procedure block-coordinate frank-wolfe ssvm optimization assumes linear input dependencies; structured perceptron similarly assumes linear parametrization dual coordinate descent focuses solving dual linear l-loss ssvms subgradient descent minimization described ﬂexible tool optimizing naturally allows error back-propagation. algorithm alternates steps. first calculated training samples called loss-augmented inference prediction step derived paper general inference determining approximated α-expansion algorithm whose eﬀectiveness validated extensive experiments loss-augmented prediction incorporated procedure adding loss term classiﬁer trained upfront based diﬀerent unary inputs corresponding node underlying factor graph. linear deﬁnition ssvm model learning linear combinations classiﬁer outputs unary factors. unary features corresponding node interaction features corresponding interaction similarly higher-order interaction features incorporated extending matrix higherorder combinations nodes according interactions. experiments paper unary features bagof-words features corresponding superpixel. interaction features also bag-of-words time corresponding connected superpixels. nodes interactions. paper make l-regularization hence line image segmentation case section loss function class-weighted hamming distance label assignments n-th training sample. contrary maximum likelihood approaches hamming distance allows directly maximize performance metrics regarding accuracy. setting focus node-wise yi])− allows accuracy setting inverse regularization strength training samples output optimized parameters initialize according output layer weights initialized prediction back-propagated underlying network adjust element altered subgradient descent method shown algorithm herein represents objective function n-th training sample i.e. g)]. loss terms longer aﬃne input transformations introduced nonlinearities neural network longer assume convex case conventional ssvms. although theoretical guarantees made convergence gradient methods convex functions particular classes nonconvex functions guarantees made arbitrary nonconvex functions problem optimizing highly nonconvex functions studied extensively neural network gradient descent literature. however demonstrated nonconvex objectives minimized eﬀectively high dimensionality neural network parameter space dauphin show saddle points propose replace pretraining nonlinear unary classiﬁer transformation outputs linear factors direct optimization nonlinear unary factors. particular unary part represented outputs adapted neural network models factor values. achieve this loss-augmented prediction step deﬁned altered compatibility function thus becomes originally deﬁned subderivative objective function remains unaltered. however longer assume conforms deﬁnition subgradient nonconvexity. however calculate much likelier local minima multilayer neural network objective landscapes. particular ratio saddle points local minima increases exponentially parameter dimensionality. several methods exists avoid saddle points e.g. momentum furthermore dauphin show based random matrix theory existing local minima close global minimum objective function. understood intuitively probability directions surrounding local minimum lead upwards small making local minima issue general. empirical results presented section reinforce believe demonstrating regularized objective function still minimized eﬀectively achieve accurate predictions. described algorithm gradient deﬁned whole data samples consist multiple nodes. thus models unary part compatibility function unary factors. therefore function decomposes neural unary factors unary features nonlinear function r|l| multiclass multilayer neural network parametrized whose inputs features corresponding diﬀerent nodes. forms template neural unary factors. network softmax-function removed output layer matches unary factor range r|l|. argument joint feature function used index select particular output unit. forms template interaction factors. herein depends interaction order e.g. section case connections nodes edges. interaction factors generally trained upfront. however neural interaction factors useful extract complexer interaction patterns thus transcend limited generalization power linear combinations. image segmentation example interaction features consisting vertical gradients ◦-angle indicate connected nodes belong class. loss-augmented inference step adapted multiclass multilayer neural network softmax-function removed shared among interaction factors. output layer dimension matches number interaction label combinations |l|q general case. example image segmentation problem symmetric edge features number out|l| represent diﬀerent units states particular interaction factor consists resulting structured predictor longer requires two-phase training linear interaction factors combined upfront training unary classiﬁer whose output transformed linearly unary factor values. makes highly nonlinear functions ssvm factors multilayer neural networks using integration loss-augmented inference backpropagation subgradient descent framework. allows factors generalize strongly able mutually adapt other’s parameter updates leading accurate predictions. section model analyzed task image segmentation. herein goal label diﬀerent image regions correct class label. cast structured prediction problem predicting image region class labels simultaneously. unary factor underlying ssvm graphical structure every image region interactions represent edges neighboring regions. first model analyzed different variants compared conventional ssvm training schemes. second best performing variant compared state-of-the-art segmentation approaches. model implemented extension pystruct using theano gpu-accelerated neural factor optimization. model analysis experiments executed widely-used msrc- benchmark consists training validation testing images. benchmark suﬃciently complex classes noisy labels focuses object delineation well irregular background recognition. furthermore experiments executed kitti benchmark consisting training testing images augmented training images kundu latter benchmark consists classes drop least frequently-occurring ones insuﬃciently represented dataset. finally experiment slic superpixel algorithm. region gradient color features densely extracted. features transformed times separate bags-of-words minibatch k-means clustering unary input vectors form concatenations ﬁrst bags-of-words. model’s connectivity structure links together neighboring regions edges. edge/interaction input vectors based concatenations second bags-of-words. input vectors edge’s incident regions factors trained momentum using parameters learning rate curve current training iteration number used algorithms regularization learning rate momentum hyperparameter values tuned using validation means coarseﬁne-grained grid search parameter spaces yielding separate settings unary pairwise factors. linear parameters initialized neural factor parameters initialized according except layer weights class weights correct class imbalance. model trained using cpu-parallelized loss-augmented prediction neural factors trained using parallelism. unary-only -slack cutting plane training delayed constraint generation subgradient descent integrated training neural unary linear interaction factors bifurcated training neural interaction factors integrated training neural unary neural interaction factors multiclass logistic regression used unary classiﬁer trained gradient descent cross-entropy optimization. unary neural factors contain single hidden layer tanh-units direct comparison integrated learning upfront logistic regression training. interaction neural factors contain single hidden layer tanh-units elucidate beneﬁt nonlinear factors without overly increasing model’s capacity. experiment highlight beneﬁt integrated learning restricting unary factors features insuﬃciently discriminative own. deliberately leads noisy unary classiﬁcation forcing model rely contextual relations accurate prediction. interaction factors encode information incident region figure illustrative examples performance int+nrl several msrc- test images. integrated training neural factors improves classiﬁcation accuracy subgradient descent. last column presents case model fails outperform sgd. figure illustrative examples performance int+nrl several kitti test images. integrated training neural factors improves classiﬁcation accuracy subgradient descent. last column presents case model fails outperform sgd. figure illustrative examples performance int+nrl several sift flow test images. integrated training neural factors improves classiﬁcation accuracy subgradient descent. last column presents case model fails outperform sgd. feature vectors allow neural factors extract meaningful patterns gradient/color combinations. deliberately encoded less information interaction features model cannot solely rely interaction factors accurate coherent predictions. accuracy results msrc- test images presented table figure shows handful illustrative examples compare segmentations attained int+nrl. results experiment kitti benchmark augmented additional training images kundu shown table figure qualitative results sift flow dataset shown figure accuracy results shown table unary features suﬃciently distinctive allow diﬀerentiation classes dimensionality. accurate predictions possible taking account contextual output relations demonstrated increased accuracy well structured predictors learn linear relations image regions allows correct errors originating underlying unary classiﬁer. however unary factor’s linear weights limited capability error correction opposite direction fact ssvm cannot alter unary classiﬁer parameters post-hoc. using integrated training approach int+lin ssvm trained end-to-end improves accuracy bifurcated procedures sgd. although neither unary interaction features distinctive integrated procedure upfigure visualization synergy unary interaction factors. bifurcated training interactions make unary factors redundant cannot adapt errors made interactions. integrated training combining factor types leads higher accuracy mutually adapt other’s weight updates. dates parameters factor types unique discriminative focus. synergistic relationship ultimately results higher accuracy. better compare int+lin also depict accuracy model unary hidden units kitti msrc sift flow dataset rather units. ./.% ./.% ./.% increases accuracy illustrates beneﬁt integrated learning inference conventional bifurcated ssvm training. another insight gained results accuracy increases replacing linear interaction factors conventional ssvms neural factors i.e. int+nrl bif+nrl outperform int+lin respectively. increase attributed higher number parameters well added nonlinearities combination correct regularization. model greater generalization power allowing factors extract complex meaningful interaction patterns. neural factors oﬀer great ﬂexibility stacked arbitrary depths. leads even higher generalization indicated increased accuracy deeper -layer model. herein unary interaction factors -hidden-layer neural networks consisting units layer respectively. model thus easily extended example letting neural factors represent fullyconnected layer convolutional neural networks. such serves foundation complex structured models. methods converge within epochs epoch taking approximately seconds msrc- dataset seconds kitti dataset seconds sift flow dataset int+nrl algorithm. since implementation algorithm optimized speed values reduced better exploitation parallelism. figure illustrates synergy unary interaction factors achieved integrated bifurcated training exercised msrc- dataset. bars depict model test accuracy using unary pairwise factors setting either pairwise unary factors respectively zero factor value thus unary factors alone perform well bifurcated training nearly accuracy attributed interactions. possible explanation types essentially learn information. interactions correct errors underlying classiﬁer ultimately make unary factors redundant. integrated training neither unary interaction factors alone attain high accuracy combination does. explain synergistic relationship example unary factors assign region class secondto-highest factor value class highest value class value class interactions also assign second-to-highest value class highest value class value class independently factors incorrectly predict region class belonging class class however combined correctly assign highest value class ﬁgure bifurcated training shows limited signs factor synergy optimization procedure insuﬃciently able steer unary pairwise parameters diﬀerent directions causes similar discriminative focus. observation leads believe integrated learning inference results higher accuracy synergistic unary/interaction factor optimization. factor types longer optimized independent accuracy mutually adapt other’s parameter updates results enhanced predictive power. addition previous experiments viability neural factor model shown comparison closely related work msrc- dataset. make features extracted square regions varying size around superpixel means pretrained convolutional neural network. compare model using overfeat features similar fashion trained individual regions. furthermore model settings altered respect previous experiments. speciﬁcally slic superpixels utilized over-segmentation preprocessing step enforcing superpixel connectivity merging superpixel surface area particular threshold. daisy gradient color features extracted according regular lattice clustered minibatch k-means clustering. next type features extracted individual pixel leading unary pairwise factor feature vectors. moreover -position superpixel center included unary feature vectors distance angle superpixel centers encoded interaction feature vectors. neural factors represented multilayer neural networks using tanh-units trained according algorithm using conventional momentum single image-sized batches gradient update. classes balanced weighing inverse class frequency. results presented table indicate model capable performing current state-of-practice used conjunction advanced methods e.g. overfeat features. moreover similar compared model less closely related methods completeness results shown horizontal line table structured prediction model integrates backpropagation loss-augmented inference subgradient descent training structural support vector machines proposed. model departs traditional bifurcated approach unary classiﬁer trained independently structured predictor. furthermore ssvm factors extended neural factors allows unary interaction factors highly nonlinear functions input features. results complex image segmentation task show end-toend ssvm training and/or using neural factors leads accurate predictions conventional subgradient descent -slack cutting plane training. results show model serves foundation advanced structured models e.g. using latent variables learned feature representations complexer connectivity structures.", "year": 2015}