{"title": "A Generic Approach for Escaping Saddle points", "tag": ["cs.LG", "cs.AI"], "abstract": "A central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points. First-order methods often get stuck at saddle points, greatly deteriorating their performance. Typically, to escape from saddles one has to use second-order methods. However, most works on second-order methods rely extensively on expensive Hessian-based computations, making them impractical in large-scale settings. To tackle this challenge, we introduce a generic framework that minimizes Hessian based computations while at the same time provably converging to second-order critical points. Our framework carefully alternates between a first-order and a second-order subroutine, using the latter only close to saddle points, and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical performance.", "text": "central challenge using ﬁrst-order methods optimizing nonconvex problems presence saddle points. first-order methods often stuck saddle points greatly deteriorating performance. typically escape saddles second-order methods. however works second-order methods rely extensively expensive hessian-based computations making impractical large-scale settings. tackle challenge introduce generic framework minimizes hessian based computations time provably converging second-order critical points. framework carefully alternates ﬁrstorder second-order subroutine using latter close saddle points yields convergence results competitive state-of-the-art. empirical results suggest strategy also enjoys good practical performance. neither individual functions necessarily convex. operate general nonconvex setting except smoothness assumptions like lipschitz continuity gradient hessian. optimization problems form arise naturally machine learning statistics empirical risk minimization m-estimation respectively. large-scale settings algorithms based ﬁrst-order information functions typically favored relatively inexpensive scale seamlessly. algorithm widely used practice stochastic gradient descent iterative update figure first order methods like potentially stuck saddle points. second-order methods escape iterations cost expensive hessian based iterations proposed framework novel strategies escape saddle points faster time carefully trading computation iteration complexity. randomly chosen index learning rate. suitable selection learning rate show converges point that expectation satisﬁes stationarity condition iterations result critical weaknesses ensure convergence local optima second-order critical points; rate convergence algorithm slow. general nonconvex problems settle modest goal sub-optimality ﬁnding global minimizer ﬁnite-sum nonconvex problem general intractably hard. unfortunately even ensure second-order critical conditions local optimality since stuck saddle points. issue recently received considerable attention community especially context deep learning works argue saddle points highly prevalent optimization paths primary obstacle training large deep networks. tackle issue achieve second-order critical point need algorithms either hessian explicitly exploit structure. work explicitly uses hessians obtain faster convergence rates cubic regularization method particular nesterov polyak showed requires iterations achieve second-order critical conditions. however iteration expensive requires computing hessian solving multiple linear systems complexity thus undermining beneﬁt faster convergence. recently agarwal designed algorithm solve efﬁciently however still exhibits slower convergence practice compared ﬁrst-order methods. approaches hessian based optimization iteration make slow practice. second line work focuses using hessian information whenever method gets stuck stationary points second-order critical. knowledge ﬁrst work line shows class functions satisfy special property called strict-saddle property noisy variant converge point close local minimum. class functions points close saddle points hessian large negative eigenvalue proves instrumental escaping saddle points using isotropic noise. noisebased method appealing uses ﬁrst-order information dependence dimension furthermore result holds strict-saddle property satisﬁed recently carmon presented faster algorithm alternates ﬁrst-order second-order subroutines. however algorithm designed simple case hence expensive practice. inspired line work develop general framework ﬁnding second-order critical points. idea framework ﬁrst-order information part optimization process invoke hessian information stuck stationary points second-order critical. summarize idea main contributions paper below. main contributions develop algorithmic framework converging second-order critical points provide convergence analysis framework carefully alternates subroutines gradient hessian information respectively ensures second-order criticality. furthermore present instantiations framework provide convergence rates them. particular show simple instance framework based svrg achieves convergence rates competitive current state-of-the-art methods; thus highlighting simplicity applicability framework. finally demonstrate empirical performance algorithms encapsulated framework show superior performance. related work. vast literature algorithms solving optimization problems form classical approach solving optimization problems dates back least seminal work since then subject extensive research especially convex setting recently faster methods called variance reduced methods proposed convex ﬁnite-sum problems. methods attain faster convergence reducing variance stochastic updates e.g. accelerated variants methods achieve lower bounds proved thereby settling question optimality. furthermore developed asynchronous framework methods demonstrated beneﬁts parallel environments. aforementioned prior works study stochastic methods convex specialized nonconvex settings admit theoretical guarantees sub-optimality. general nonconvex setting recently non-asymptotic convergence rate analysis variants obtained showed ensures iterations. similar rate parallel distributed shown problems reddi proved faster convergence rates ensure optimality criteria order faster methods ensure convergence stationary points faster rate question convergence local minima addressed. knowledge convergence rates second-order critical points general nonconvex functions ﬁrst studied however iteration algorithm prohibitively expensive since requires eigenvalue decompositions hence unsuitable large-scale high-dimensional problems. recently agarwal carmon presented algorithms ﬁnding second-order critical points tackling practical issues arise however algorithms either applicable restricted setting heavily hessian based computations making unappealing practical standpoint. noisy variants ﬁrst-order methods also shown escape saddle points however methods strong dependence either undesirable. condition typically necessary ensure convergence algorithms second-order critical points addition smoothness conditions also assume function bounded below i.e. order measure stationarity iterate similar condition paper interested convergence second-order critical points. thus addition stationarity also require solution satisfy hessian condition iterative algorithms require number iterations saddle points non-degenerate condition implies convergence local optimum. deﬁnition algorithm said obtain point -second order critical point expectation randomness must exercise caution interpreting results pertaining -second order critical points. points need close local minima either objective function value domain algorithms incremental first-order oracle incremental second-order oracle deﬁned below. deﬁnition takes index point returns pair ∇fi). takes index point vector returns vector ∇fiv. calls typically cheap call relatively expensive. many practical settings arise machine learning time complexity oracle calls linear clarity clean comparison dependence time complexity lipschitz constant initial point polylog factors results hidden. generic framework section propose generic framework escaping saddle points solving nonconvex problems form primary difﬁculties reaching second-order critical point presence saddle points. evade points needs properties gradients hessians. framework based core subroutines gradient-focused-optimizer hessian-focused-optimizer. idea subroutines focused different aspects optimization procedure. gradient-focused-optimizer focuses using gradient information decreasing function. gradient-focused-optimizer might converge local minimizer since stuck saddle point. hence require subroutine hessian-focused-optimizer help avoid saddle points. natural idea interleave subroutines obtain second-order critical point. even clear procedure even converges. propose carefully designed procedure effectively balances subroutines provides meaningful theoretical guarantees remarkably also translates strong empirical gains practice. algorithm provides pseudocode framework. observe algorithm still abstract since specify subroutines gradient-focused-optimizer hessian-focused-optimizer. subroutines determine crucial update mechanism algorithm. present speciﬁc instance subroutines next section assume following properties hold subroutines. gradient-focused-optimizer suppose gradient-focused-optimizer exists positive function outputs expectation conditions randomness part subroutine. function critical overall rate algorithm typically gradient-focused-optimizer ﬁrst-order method since primary subroutine focus gradient based optimization. hessian-focused-optimizer suppose hessian-focused-optimizer {∅}. -second order critical point probability least otherwise satisﬁes following condition λmin) function expectation randomness subroutine hessian-focused-optimizer. conditions ensure objective function value expectation never increases furthermore decreases certain rate λmin) general subroutine utilizes hessian properties minimizing objective function. typically expensive part algorithm hence needs invoked judiciously. aspect subroutines they expectation never increase objective function value. functions determine convergence rate algorithm order provide concrete implementation need specify aforementioned subroutines. delve details provide generic convergence analysis algorithm convergence analysis theorem min\u0001g also output algorithm gradient-focused-optimizer satisfying hessian-focused-optimizer satisfying furthermore ∆/θ. suppose multiset ...ik} indices selected independently uniformly randomly |γ|}. following holds indices -critical point probability least min) log)) least probability least proof result presented appendix point regarding result overall convergence rate depends magnitude functions theorem shows slowest amongst subroutines gradient-focused-optimizer hessian-focused-optimizer governs overall rate algorithm thus important ensure procedures good convergence. also note optimal setting based result satisﬁes defer discussion convergence next section present speciﬁc convergence rate analysis. present speciﬁc instantiations framework section. state results discuss important subroutine used gradient-focused-optimizer rest paper svrg. give brief description algorithm section show meets conditions required gradient-focused-optimizer. svrg stochastic algorithm recently shown effective reducing variance ﬁnite-sum problems. seek understand beneﬁts nonconvex optimization particular focus issue escaping saddle points. algorithm presents svrg’s pseudocode. observe algorithm epoch-based algorithm. start epoch full gradient calculated point requiring calls ifo. within inner loop svrg performs stochastic updates. suppose chosen total calls epoch strong convergence rates proved algorithm context convex nonconvex optimization following result shows svrg meets requirements gradient-focused-optimizer. lemma suppose /ln/ depends algorithm gradient-focused-optimizer t\u0001/ln/. rest section discuss approaches using svrg gradient-focused-optimizer. particular propose provide convergence analysis different methods different hessian-focused-optimizer svrg gradient-focused-optimizer. ﬁrst approach based directly using eigenvector corresponding smallest eigenvalue hessian-focused-optimizer. speciﬁcally smallest eigenvalue hessian negative reasonably large magnitude hessian information used ensure descent objective function value. pseudo-code algorithm given algorithm idea utilize minimum eigenvalue information order make descent step. λmin) idea information take descent step. note subroutine designed fashion objective function value never increases. thus naturally satisﬁes requirement hessian-focused-optimizer. following result shows hessiandescent hessian-focused-optimizer. lemma hessiandescent hessian-focused-optimizer proof result presented appendix svrg gradient-focused-optimizer hessiandescent hessian-focused-optimizer show following result theorem suppose svrg /ln/ ln//\u0001/ used gradient-focused-optimizer hessiandescent used hessian-focused-optimizer algorithm ﬁnds -second order critical point o\u0001/) probability least result directly follows using lemma theorem result shows iteration complexity algorithm case thus overall complexity svrg algorithm since call takes time overall time complexity gradient-focused-optimizer steps understand time complexity hessiandescent need following result preposition time complexity ﬁnding probability least following inequality holds λmin) note iteration algorithm case linear dependence since total number hessiandescent iterations o\u0001/) iteration complexity using remark obtain overall time complexity hessiandescent combining time complexity svrg following result. corollary overall running time algorithm note dependence much better comparison noisy used furthermore results competitive respective settings much simpler algorithm analysis. also note algorithm faster proposed time complexity section show cubic regularization method used hessian-focused-optimizer. speciﬁcally hessian-focused-optimizer approximately solves following optimization problem returns output. following result proved approach. theorem suppose svrg used gradient-focused-optimizer cubicdescent used hessian-focused-optimizer algorithm ﬁnds however practice would expect gradient-focused-optimizer perform optimization hessian-focused-optimizer used fewer iterations. using method developed solving cubicdescent obtain following corollary. corollary overall running time algorithm matrix multiplication constant. dependence weaker comparison corollary however iteration cubicdescent expensive thus high dimensional settings typically encountered machine learning approach expensive comparison hessiandescent. focus section demonstrate wide applicability framework; wherein using simple instantiation framework could achieve algorithms fast convergence rates. achieve good empirical performance slightly modify procedures. hessian-focused-optimizer found stochastic adaptive inexact approaches solving hessiandescent cubicdescent work well practice. lack space exact description modiﬁcations deferred appendix furthermore context deep learning empirical evidence suggests ﬁrst-order methods like adam exhibit behavior congruence properties theoretical analysis setting adam used gradient-focused-optimizer still unresolved nevertheless demonstrate performance empirical results following section. present empirical results saddle point avoidance technique highlight three aspects framework successfully escapes non-degenerate saddle points framework fast framework practical large-scale problems. algorithms implemented tensorflow case deep networks hessian-vector product evaluated using trick presented experiments commodity machine intel xeon nvidia titan gpu. synthetic problem demonstrate fast escape saddle point proposed method consider following simple nonconvex ﬁnite-sum problem matrix exactly negative eigenvalue eigenvalues randomly chosen interval total number examples hard problem nondegenerate saddle point origin. allows explore behaviour different optimization algorithms vicinity saddle point. experiment compare svrg hessiandescent adam svrg figure comparison various methods curves mnist deep autoencoder. approach converges faster baseline methods uses relatively calls comparison approxcubicdescent. cubicdescent. parameter algorithms chosen grid search gives best performance. subproblem cubicdescent solved gradient descent gradient norm subproblem reduced study progress optimization i.e. decrease function value wall clock time calls calls. algorithms initialized starting point close origin. results presented figure shows proposed framework fastest escape saddle point terms wall clock time. observe performance ﬁrst order methods suffered severely saddle point. note eventually escaped saddle point inherent noise mini-batch gradient. cubicdescent second-order method escaped saddle point faster terms iterations using hessian information. operating hessian information expensive result method slow terms wall clock time. proposed framework strategies inherits best worlds using cheap gradient information time reducing relatively expensive hessian information resulted faster escape saddle point terms wall clock time. deep networks investigate practical performance framework deep learning problems applied deep autoencoder optimization problems called curves mnist. high difﬁculty performance problems become standard benchmark neural network optimization methods e.g. curves autoencoder consists encoder layers size ----- symmetric decoder totaling parameters. units code layer linear units logistic. network trained images tested images. data contains images curves generated three randomly chosen points dimensions. mnist autoencoder consists encoder layers size ---- symmetric decoder totaling parameters. thirty units code layer linear units logistic. network trained images tested images. data contains images handwritten digits pixel intensities normalized instantiation framework adam popular deep learning community approxcubicdescent practical reasons mentioned section method adam approxcubicdescent. parameters algorithms chosen produce best generalization held test set. regularization parameter chosen smallest value function value ﬂuctuate ﬁrst epochs. initialization suggested mini-batch size algorithms. report objective function value wall clock time calls. results presented figure shows proposed framework fastest escape saddle point terms wall clock time. adam took considerably time escape saddle point especially case mnist. approxcubicdescent escaped saddle point relatively fewer iterations iteration required considerably large number calls; result method extremely slow terms wall clock time despite efforts improve approximations code optimizations. hand proposed framework seamlessly balances methods thereby resulting fast decrease training loss. discussion paper examined generic strategy escape saddle points nonconvex ﬁnite-sum problems presented convergence analysis. intuition alternate ﬁrst-order second-order based optimizers; latter mainly intended escape points stationary second-order critical points. presented different instantiations framework provided detailed convergence analysis. methods explicity hessian information also noisy ﬁrst-order methods hessian-focused-optimizer scenario exploit negative eigenvalues hessian escape saddle points using isotropic noise explicitly iso. methods strict-saddle point property show convergence local optima within framework. primarily focused obtaining second-order critical points nonconvex ﬁnite-sums necessarily imply test error good generalization capabilities. thus careful interpreting results presented paper. detailed discussion analysis issues scope paper. prior works argue convergence local optima exact connection generalization local optima well understood interesting open problem. nevertheless believe techniques presented paper used alongside optimization tools faster better nonconvex optimization. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. cartis scheinberg. global convergence rate analysis unconstrained optimization methods based probabilistic models. mathematical programming pages issn ./s---. yann dauphin harm vries yoshua bengio. equilibrated adaptive learning rates nonconvex optimization. cortes lawrence sugiyama garnett editors advances neural information processing systems pages curran associates inc. yann dauphin razvan pascanu caglar gulcehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional nonconvex optimization. proceedings international conference neural information processing systems nips’ pages aaron defazio francis bach simon lacoste-julien. saga fast incremental gradient method support non-strongly convex composite objectives. nips pages james martens roger grosse. optimizing neural networks kronecker-factored approximate curvature. international conference machine learning pages sashank reddi ahmed hefny suvrit barnabas poczos alex smola. variance reduction stochastic gradient descent asynchronous variants. nips pages sashank reddi ahmed hefny suvrit barnabás póczos alexander smola. stochastic variance reduction nonconvex optimization. proceedings international conference machine learning icml york city june pages ilya sutskever james martens george dahl geoffrey hinton. importance initialization momentum deep learning. international conference machine learning pages case handled straightforward manner focus case split analysis cases analyzing change objective function value depending second-order criticality start case gradient condition second-order critical point violated proceed case hessian condition violated. case ﬁrst observe following follows straightforward application jensen’s inequality. inequality following furthermore property non-increasing nature gradient-focused-optimizer also focus hessian-focused-optimizer subroutine. property hessian-focused-optimizer objective function value non-increasing therefore combining inequality ﬁrst equality deﬁnition algorithm therefore gradient condition violated irrespective whether λmin) objective function value always decreases least case λmin) case ﬁrst note hessian-focused-optimizer λmin) observe hessian-focused-optimizer. therefore λmin) second inequality non-increasing property gradient-focused-optimizer. hand hand e|ut non-increasing property hessian-focused-optimizer. combining inequalities using total expectation second inequality non-increasing property gradient-focused-optimizer. therefore hessian condition violated objective function value always decreases least case favorable case algorithm. condition note objective function value non-increasing case too. again non-increasing properties subroutines gradient-focused-optimizer hessian-focused-optimizer. general greater occurrence case course algorithm higher probability output algorithm satisﬁes desired property. observation case cannot occur large number times since cases strictly decreases objective function value. particular equation easy occurrence case following holds min\u0001g ph). furthermore function lower bounded thus case cannot occur b)/θ times. therefore probability occurrence case least completes ﬁrst part proof. second part proof simply follows ﬁrst part. seen above probability case b)/t therefore probability element falls case least b)/t gives required result second part. ﬁrst inequality l-smoothness function second inequality simply follows unbiasedness svrg update algorithm analysis algorithm need following lyapunov function second equality unbiasedness update svrg. last inequality follows simple application cauchy-schwarz young’s inequality. substituting equation deﬁnition output algorithm i.e. iterate chosen uniformly random {{xs+ clear algorithm satisﬁes requirement gradient-focused-optimizer t\u0001/ln/. since satisiﬁed algorithm conclude svrg gradient-focused-optimizer. proof. ﬁrst important observation function value never increases minz∈{ux} i.e. thus satisfying hessian-focused-optimizer. analyze scenario λmin) consider event obtain ﬁrst inequality follows m-lipschitz continuity hessain ﬁrst equality follows update rule hessiandescent. second inequality obtained dropping negative term using fact second equality obtained substituting scenario first note cubic method descent method thus trivially satisﬁed. furthermore cubic descent hessian-focused-optimizer this again follows theorem result easily follows aforementioned observations. cubic regularization method designed operate full batch i.e. exploit ﬁnite-sum structure problem requires computation gradient hessian entire dataset make update. however full-batch methods scale gracefully size data become prohibitively expensive large datasets. overcome challenge devised approximate cubic regularization method described below found mini-batch training strategy requires computation gradient hessian small subset dataset work well datasets similar method analysed furthermore many deep-networks adaptive per-parameter learning rate helps immensely possible explanation scale gradients layer network often differ several orders magnitude. well-suited optimization method take account. reason popularity methods like adam rmsprop deep learning community. similar lines account different per-parameter behaviour cubic regularization modify sub-problem adding diagonal matrix addition scalar regularization coefﬁcient i.e. also devised adaptive rule obtain diagonal matrix diag/) maintained moving average third order polynomial mini-batch gradient fashion similar rmsprop adam respectively vectors |gi| experiments reported curves mnist paper utilizes modiﬁcations cubic regularization refer modiﬁed procedure acubic results. scalar step-size determined grid search. adam performed grid search parameters adam tied together i.e. svrg scalar step-size determined grid search. cubicdescent regularization parameter chosen grid search. sub-problem solved gradient descent step-size solver till gradient norm sub-problem reduced observations results presented figure ﬁrst order methods like adam higher noise could escape relatively faster whereas svrg reduced noise stayed stuck saddle point. approxcubicdescent regularization parameter chosen largest value function value jump ﬁrst epochs. found curves mnist. sub-problem solved gradient descent step-size solver till gradient norm sub-problem reduced figure comparison various methods deep autoencoder curves mnist approach converges faster baseline methods uses relatively calls comparison approxcubicdescent", "year": 2017}