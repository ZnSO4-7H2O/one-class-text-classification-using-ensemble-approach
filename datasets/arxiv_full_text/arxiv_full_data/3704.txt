{"title": "Group invariance principles for causal generative models", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH", "I.2.6; I.2.10; G.3; I.5.3"], "abstract": "The postulate of independence of cause and mechanism (ICM) has recently led to several new causal discovery algorithms. The interpretation of independence and the way it is utilized, however, varies across these methods. Our aim in this paper is to propose a group theoretic framework for ICM to unify and generalize these approaches. In our setting, the cause-mechanism relationship is assessed by comparing it against a null hypothesis through the application of random generic group transformations. We show that the group theoretic view provides a very general tool to study the structure of data generating mechanisms with direct applications to machine learning.", "text": "postulate independence cause mechanism recently several causal discovery algorithms. interpretation independence utilized however varies across methods. paper propose group theoretic framework unify generalize approaches. setting cause-mechanism relationship assessed comparing null hypothesis application random generic group transformations. show group theoretic view provides general tool study structure data generating mechanisms direct applications machine learning. inferring causal relationships empirical data challenging problem major applications. problem inferring relations arbitrarily many random variables extensively addressed conditional statistical independences graphical models several limitations framework motivated search perspectives causal inference. major contribution line research postulate independence cause mechanism assumes causes mechanisms chosen independently nature thus contain information other. here information either meant sense algorithmic independence sense semi-supervised learning work major interest framework development several causal inference algorithms cause-effect pairs however results sch¨olkopf also suggest exploited broader settings providing guiding principle study learning algorithms. methods addresses causal inference problem speciﬁc models thus usable restricted applications. principled ways generalize address problems unknown. particular unclear notion independence deﬁned given domain could impact results. conceptual difﬁculty framework independence assessed between objects different nature input mechanism; moreover appropriate notion independence usual statistical independence rvs. paper suggest group theoretic framework unify icm-based approaches provide useful tools study generative models general. involves deﬁning group generic transformations perturb relationship mechanisms causes well appropriate contrast function assess genericity cause-mechanism relationship. show framework encompasses previous approaches addition previous methods based focused cause-effect pairs present paper shows group theoretic view provides framework study causal generative models general setting includes latent variable models. framework full generality. section shows previous causal inference approaches framework. finally section shows applied analyze improve unsupervised learning algorithms. proofs provided appendix. section illustrate fundamental properties group theoretic approach studying simpliﬁed version basic inference problem visual perception identiﬁcation partially occluded objects. dimensional naturalistic visual scenes object partially mask objects standing behind scene. phenomenon usually well identiﬁed human visual system remains major challenge robust object detection computer vision. interestingly even human perception occlusion misled speciﬁc examples illusory contours. case well know kanizsa’s triangle shown fig. important illusory contour emerge precise alignment edges pac-man-shaped inducers instigating completion aligned segment pair larger edge. describe speciﬁcity ﬁgure count number lines carrying straight edges three pac-man shapes ﬁgure three lines atypically small ﬁgure made three objects totaling straight edges. idea conﬁguration atypical lies heart causal inference framework indeed formulate scene understanding tasks related object occlusion causal inference problems. state following scene understanding problem polygonal objects appear visual scene occluding other want infer object partially occludes one. example scene represented fig. particular example likely interpretation scene triangle appears front yellow square. however could imagine contrary yellow object foreground occludes object picking objects shown fig. example. conﬁguration however intuitively unlikely precise positions orientations objects tuned lead scene shown fig. would give illusion yellow square occluded. considerations vision scientists formulate generic viewpoint assumption order perform inferspecify precisely generative model visual scene large collection polygons ﬁrst object selected scene position orientation second object selected collection ﬁrst position orientation generative model conﬁgurations possible. order determine conﬁguration likely resort additional postulate independence cause mechanism. cause pair objects mechanism being positions orientations scene assume last parameters picked independently geometry chosen objects. consequence apply random rotation objects expect cases global properties image preserved original scene qualiﬁed typical. following number lines scene simple useful global property. indeed apply random rotation yellow object hypothesis triangle object front obtain modiﬁed scene fig. respect similar original ﬁgure. particular total number lines carrying objects’ edges cases contrary hypothesis yellow object front typically conﬁguration like fig. leads larger number lines scene summarize experiment assume object front number straight lines scene typical since arbitrary rotation object typically lead number edges. contrary assume yellow object front number lines scene suspiciously respect becomes modifying generative model arbitrary rotation applied object. differently assumption orientations chosen random independently shape objects scene conﬁgurations typical number lines occur high probability much likely object lies front. figure kanizsa’s triangle. scene yellow square occluded triangle. example objects leading scene occludes yellow generative model scene resulting arbitrary rotation yellow object dashed lines indicate straight lines carrying edges objects. case presented text explanations. reasoning abstracted order better understand critical elements used causal inference. first generative model introduced given polygonal objects scene generated according function mpθpθ represents mechanisms puts ﬁrst scene position orientation puts position orientation front seen important characteristic scene number lines carrying edges. lets call number given scene inferred whether typical observed scene given assumption object stands front scene. assume ﬁrst object stands front yellow generative model typical choices happens case example rotate object around center preserves occlusion. consequence typical reﬂect invariance value respect group transformations rotations. therefore model described atypical witnessed lack invariance values thus less likely model explain scene. elements used causal inference thus generative model observed data group generic transformations applied model simulate typical conﬁgurations generative model contrast evaluated observed data typical conﬁgurations model. invariance contrast generic transformations indicates observations typical. elements used deﬁning general framework causal inference section many machine learning approaches rely statistical models order approximate observed data ranging simple linear regression models recently introduced generative adversarial networks order models serve purpose represent observations faithfully possible. property evaluated purely statistical sense testing whether probability distribution model close possible empirical distribution data however inferring causal model goes beyond statistical criterion imposing ﬁtted model sense capture structure true data generating process. concepts pertaining causality well formulated using structural equation models structural causal models describe relationship different variables structural equations taking form equations represent algebraic dependencies variables indicated asymmetry colon-equals symbol suggests left-hand side variable deﬁned based right-hand side variables. broadly construed means relation would still hold external agent intervene several right-hand side variables alternatively formulate counterfactuals what would happened right-hand side variable different overview). consequence properly inferred causal generative model based structural equation offers robustness changes environment purely statistical models. includes important cases related transfer leaning covariate-shift changes input distribution making causal generative models highly relevant machine learning. addition correct causal generative model offers formalism describe understand actual mechanisms underlying observed data thus central goal experimental sciences. section deduce framework virtually probes structural equations counterfactual reasoning could stated what would happen apply generic transformation given variable mechanism scm. virtual intervention represented fig. single cause mechanism. applied transformation generic sense sampled random large transformations turns variable/mechanism another likely occur similar scm. outcome virtual intervention tested quantifying whether counterfactual outcome qualitatively different observed outcome generic transformations. observations would atypical considered causal model unlikely causal generative model generated observations. justiﬁed assumption order generated particular observation unlikely precise mechanism would selected nature independently input; contrary mechanism would likely need chosen speciﬁcally input generate observed outcome. framework generic transformations chosen group. refer appendix relevant deﬁnitions regarding group theory. readers however assume compact group invertible transformations applied causes equipped uniform probability measure. choice particular structure motivated fact group actions combine well general structural equation framework. although extension structures prove useful group setting describe large variety causal generative models. state framework ﬁrst full generality assume plausible causes effects need belong sets type object. importantly causes effects deterministic random. addition assume invertibility cause effect relationship. allow quantitative analysis system necessary characterize variables mathematical object call attribute. covariance matrix example attribute multivariate random variable typically attribute random variable function probability distribution. generally function cause effect potentially considered attribute. speaking formally given effect generated cause mechanism described fig. measure attributes cause effect using functions codomain respectively. allow less formal presentation abusively consider mechanism acting directly attribute space indicate indistinctly cause effect attribute. figure principle group theoretic framework generic transformation introduced cause mechanism. introduction concept attribute describe structural causal model. satisﬁed exactly practice justiﬁed assuming appropriate contexts generic distribution concentrates around mean example). concentration measure results hard obtain general leave work focus properties genericity equation paper. genericity formulated rigorously statistical test assessing whether likely sampled null hypothesis whose distribution generated interesting probabilistic interpretation concept genericity. given generative model cause single sample drawn distribution estimate genericity irrespective possible values consider generic ratio c/cmx quantity close large probability order satisfy assumptions several cases leads scale invariant measure genericity. assume g-invariant distribution mild assumptions parametrized independent show section group invariance framework encompasses previous causal inference methods proposed literature solve pairwise case given observables decide alternatives causes causes tells postulate genericity true least average generative model. contrary average would different happen non-invariant distributions method unlikely succeed. represented fig. reasoning applied sampling mechanism invariant distribution. reasoning generalized complex causal structures involving multiple mechanisms variables. note also present elsewhere many causal inference approaches formulated present group theoretic framework provide appendix additional example spectral independence criterion approach shajarisales context time series. section develops idea unsupervised learning ﬁeld application principles based group invariance. many unsupervised learning algorithms indeed thought generative models propose causal perspective order improve characterization inference. clustering using gaussian mixture models experimental scientists cluster dataset expect resulting clusters reﬂect reliable structure data. explicitly expect clusters robust moderate changes data generating mechanism another experimenter replicating experiment able similar clusters. required property although commonly explicitly stated puts clustering task causal inference perspective. like causal inference problem ﬁnding plausible causal generative models require assumptions data generating mechanism. example exploit postulate learn structure generative models causal perspective. suggested many real world datasets intuitive underlying causal structure exploit improve learning algorithms. instance character recognition datasets mnist character human intents write cause observed hand-written character image. section assume setting fig. observations generated latent variables trough possibly unknown mechanism. postulate holds latent causes mechanism. apply strategy speciﬁc unsupervised learning algorithms used wide range areas non-negative matrix factorization classical gaussian mixture model clustering. finally draw connection framework gans. classically generative models modeling probability distribution observations. however often expect model capture information true generative process order better understand underlying mechanisms. take example case holds approximately. assume corresponds observations generated generative model latent variables representing sources stored linearly mixed unknown linear process whose coefﬁcients stored interpretation model terms generative mechanism application dependent motivation using method since early work problem solved using several algorithms being np-hard general difﬁcult guaranties estimated factors close true generative model. assume matrices selected permutation invariant distributions sense permuting columns columns independently leads causal model likely occur nature. implies particular relationship given column corresponding column right hand side equation indeed shows similarity matrices compared using trace product scalar product matrices. orderings columns unrelated expects eigenvectors generic orientation respect eigenvectors leading average trace values. make precise introduce notations. symmetric group abusively identify permutation matrices. thus abusively denote corresponding haar measure group. group ﬁnite contains group elements assigns probability element. order assess genericity trace value evaluate order assess whether group invariance framework useful better infer generative process simulated model generating data matrices using components matrices generated i.i.d. coefﬁcients uniformly distributed unit interval sparse matrices selecting non-zero coefﬁcients sampling i.i.d. bernoulli variables probability value selected non-vanishing coefﬁcients drawn i.i.d. uniform distributions unit interval. small i.i.d. uniform additive noise added data matrix. first assumed number components known quantiﬁed performance algorithm computing average cosine similarity columns ground truth corresponding best matching columns estimated algorithms. simultaneously estimated generic ratio ground truth model estimated models. results different algorithms simulations provided fig. show algorithms tend introduce dependencies estimated latent variables mechanism generic ratio tends larger ’mult’ ratio smaller ’als’. addition ’mult’ performance tends less variable across trials leads generic ratios close ’als’ algorithm fails frequently leads particularly values generic ratio. next assumed number components unknown fig. show evolution performance generic ratio depending number estimated components. overestimation number components leads generic ratio progressively departs suggesting used indicate misspeciﬁcation model. overall results suggest generic ratio used detect failures algorithms assuming generic causal model respects postulate. addition behavior generic ratio algorithm speciﬁc introducing perturbations estimated parameters quantiﬁed exploited improve algorithm. insight form genericity relevant generative model imagine collected data reﬂects phenotype different subspecies plants cluster mean reﬂects average characteristics species covariance matrices express variations characteristics across subpopulation. assume subspecies emerged independently never interacted suggest variability within subspecies unrelated variations across species. consequence could imagine randomizing properties µk’s keeping σk’s constant would lead model likely generated nature observed dataset. choose generic group pdimensional orthogonal matrices group mean vectors left multiplication variable added. application generic transformation results clusters intra-cluster variability original data whose locations feature space randomized illustrated figs. -dimensional feature space clusters. illustration structure observations seem affected transformation suggesting original data typical sense. makes sense mean covariance parameters drawn independently random. however simple pathological examples clustering algorithm fail capture underlying structure data generate atypical dependency means covariances. assume example that focusing single gaussian cluster clustering algorithm fails identify single cluster instead cuts clusters. situation illustrated fig. shows interesting dependency centroids clusters within cluster empirical covariance matrices difference centroids figure cluster data generated random parameters data generic transformation suspicious dependency cluster means covariances case mispeciﬁed number clusters test performance clustering algorithm kmeans expectation minimization algorithm based simulated gaussian mixture model scatter plot shown fig. suggests generic ratios broadly distributed interval algorithms reach good estimation original clusters. comparison distributions generic ratio case success failure clustering shown fig. shows much concentrated distribution clusters correctly retrieved. suggests generic ratio indeed witnesses failure algorithm cluster data properly could exploited improve performance clustering algorithms. recent interest learning complex generative models data using deep neural networks. design generative adversarial networks able produce impressively realistic synthetic images using several image datasets. principle gans oppose generative model adversarial discriminative model goal differentiate real data data generated goal mislead mistakenly considered generated images real. argue intriguing connection gans group theoretic framework. first introduce example formally. generator’s output random variable generated applying parametric mapping multivariate input noise variable using function typically implemented deep neural network. distribution ﬁxed consists i.i.d. uniformly distributed variables parameters mapping generic contrast obtained random orthogonal transformation applied cluster means c−cµσ indeed result shows differences contrast data quantify alignment cluster means principal axes covariance matrices test empirically whether resulting generic ratio indicate suspicious dependencies solution clustering algorithms. test approach detect clustering simulated dataset. generate random clusters dimensional space. cluster means drawn random isotropic gaussian distribution standard deviation cluster covariances generated random axes eigenvalues. figure clustering performance ’gm-em’ ’kmeans’ algorithms generic ratio simulations; yellow ground truth points right hand-side indicate ground truth generic ratio concentrates distribution generic ratios algorithms case successful trials failed trials sisting synaptic weights deep neural network) optimized. optimization done simultaneously parameters discriminator outputs probability sample came form data rather interestingly last equation interpreted averaged genericity equation. indeed coefﬁcients uniform i.i.d. considered generative model whose cause invariant distribution unit hypercube generic group group combining translations consequence goal optimization interpreted building generative model satisﬁes genericity equation based contrast deﬁned discriminator overall training considered applying group theoretic framework robust generative model applying generic transformation cause lead corresponding output still look typical. case face generator example want applying generic transformation input generates true face image output still looks like face afterwards. characteristic looks like face quantiﬁed contrast implemented discriminator consequence seen group theoretic framework causal generative model whose structure corresponding contrast tuned based dataset match postulate. interpretation together empirical success gans suggests principle genericity used learn robust generative models. robustness enforced applying principle input deep neural network implementing generative model also level intermediate representations network. might", "year": 2017}