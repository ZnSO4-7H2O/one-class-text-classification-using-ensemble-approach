{"title": "Learning from Logged Implicit Exploration Data", "tag": ["cs.LG", "cs.AI"], "abstract": "We provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in \"contextual bandit\" or \"partially labeled\" settings where only the value of a chosen action is learned.  The primary challenge in a variety of settings is that the exploration policy, in which \"offline\" data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged.  We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.", "text": "arbitrary input space actions. instance contextual bandit problem speciﬁed distribution tuples input vector rewards events occur round round basis round standard goal setting maximize rewards rounds interaction. order well essential previously recorded events form good policy ﬁrst round interaction. known warm start problem subject paper. formally given dataset form generated interaction uncontrolled logging policy want construct policy maximizing primary challenge variety settings exploration policy ofﬂine data logged explicitly known. prior solutions require either control actions learning process recorded random exploration actions chosen obliviously repeated manner. techniques reported lift restrictions allowing learning policy choosing actions given features historical data randomization occurred logged. consider advertisement display problem search engine company chooses display intended interest user. revenue typically provided search engine advertiser user clicks displayed problem intrinsic economic interest resulting substantial fraction income several well known companies google yahoo facebook. further. supervised learning. could learn regressor trained predict reward observed events conditioned action information regressor policy derived according argmaxa∈a approach argmax extend choices included training data hence generalize veriﬁed considering extreme cases. suppose actions action occurring times action occuring times. since action occurs fraction time learning algorithm forced trade predicting expected value overwhelmingly prefers estimate well expense accurate estimation application action chosen argmax. problem worse action occurs zero times might commonly occur exploration situations. bandit approaches. standard setting approaches suﬀer curse dimensionality must applied conditioned particular applying requires data linear extraordinarily wasteful. essence failure take advantage generalization. contextual bandits. existing approaches contextual bandits epoch greedy require either interaction gather data require knowledge probability logging policy chose action case probability unknown fact always exploration scavenging. possible recover exploration information action visitation frequency logging policy chooses actions independent input doesn’t setting logging policy surely dependent query. controlled contextual bandit event according parameter. fourth element tuple max{ˆπ importance weight speciﬁes important current event training. parameter appear mysterious ﬁrst critical numeric stability. apply oﬄine contextual bandit algorithm synthetic contextual bandit events. experimental results variant argmax regressor used critical modiﬁcations mean given logging policy deterministically choosing action given features essential observation policy deterministically chooses action deterministically chooses action treated randomizing actions probability number events events iid. thus estimate expected frequency action would displayed given features timespan logged events. section show approach sound sense expectation provides unbiased estimate value policy. inevitable errors inﬂuence process? turns eﬀect dependent small values estimates must extremely accurate yield good performance larger values less accuracy required. section prove robustness property. inﬂuence parameter ﬁnal result? creating bias estimation process turns form bias mild relatively reasonable—actions displayed frequency conditioned eﬀectively underestimated value. exactly expected limit actions frequency. section prove this. policies where function mapping input distribution learning algorithm given dataset samples form drawn described section action chosen according policy. denote random process similarly interaction policies results sequence samples denote learner given prior knowledge logging policy second policy optimization step utilize estimated logging policy main result theorem provides generalization bound addressing issue estimation optimization error contribute total error. logging policy deterministic implying conventional approaches relying randomization logging policy applicable. show next world policy varies actions. eﬀectively substitute standard approach randomization algorithm randomization world. unif denotes uniform distribution. stochastic policy chooses action uniformly random policies ﬁrst result expected value estimator world chooses actions according either sequence policies although result proof straight-forward forms basis rest results paper. note policies arbitrary assumed depend data used evaluation. allowing oﬄine evaluation policies using data trained important open problem. lemma bounds bias estimate sources bias—one error estimating threshold ﬁrst source it’s crucial analyze result terms squared loss rather loss reasonable sample complexity bounds regret squared loss estimates achievable. section show suitable choice estimator suﬃciently accurate evaluating policies aggressively simpliﬁcation previous section shows think data generated ﬁxed stochastic policy i.e. first suppose π|x) then less equal zero operation denominator fact rewards positive. thus expectation taken π|x) aside note |δx| magnitude large words situation estimator drastically underestimate value policy never overestimate ﬁrst result estimator consistent. following theorem statement denotes indicator function probability logging policy chooses action input estimator deﬁned equation based parameter theorem function distributions actions deterministic policies. deﬁne π|x) argmaxh∈ ˜h{v argmaxh∈h{ hypothesis maximizes empirical value estimator deﬁned equation then probability least holds probability least ﬁxed policy important note independent identical since action time chosen according policy previous argument made hold replacing δ/|h| applying union bound. distribution sequences samples generated executing logging policies sequence described section distribution samples form samples used estimator obtained single draw lemma shows expected value estimate policy approximation lower bound true value policy approximation errors estimate lower bound threshold statement lemma simpliﬁes thus perfect predictor expected value estimator guaranteed lower bound true value policy however left-handside statement suggests loose bound especially action chosen often small probability chosen dependence lemma somewhat unsettling unavoidable. consider instance bandit problem single input actions suppose positive policy evaluating. suppose rewards always then estimator satisﬁes π/ˆπ thus expected error estimate regret previous section proves eﬀectively evaluate policy observing stochastic policy long actions chosen adequate support speciﬁcally π|x) inputs however often interested choosing best policy policies observing logged data. furthermore described section logged data generated ﬁxed possibly deterministic policies described section rather single stochastic policy. section deﬁne stochastic policy equation logs events event represents visit user particular page pages large advertisements commercial system chooses single topmost prominent position. also chooses additional display ignored test. output indicator whether user clicked not. total number data approximately training data consist million events. test data contain million events occurring events training data. total number distinct pages approximately million. trained policy choose based current page maximize probability click. purposes learning page represented internally sparse high-dimensional feature vector. features correspond words appear page weighted frequency appear. contains average features page approximately page features. particular form linear features input sparse high-dimensional feature vector representing combination page instance every pair possible words corresponding feature. example given words apple ipod corresponding feature apple-ipod value ﬁrst word apple appeared page frequency second word ipod appeared frequency training data only. since changes time many appearing test data occur training data. consequently reliably predicting performance test data problematic. ﬁrst step follows lemma second fact regret always non-negative. third application jensen’s inequality. forth eighth application theorem ﬁfth seventh application hoeﬀding’s bound detailed above. sixth definition ﬁnal step follows corollary observing proof theorem relies lower-bound property estimator words contains good policy little support under able detect estimator. hand estimation safe sense never drastically overestimate value policy underestimate don’t overestimate property critical application optimization techniques implies unrestrained learning algorithm derive warm start policy. evaluated method real-world internet advertising dataset. obtained proprietary data online advertising company covering period approximately month. data comprised interval column computed using relative entropy form chernoﬀ bound holds assumption variables case samples used computation estimator iid. note computation slightly complicated range variables rather typical. handled rescaling applying bound rescaling results naive policy corresponds theoretically ﬂawed supervised learning approach detailed introduction. evaluation policy quite expensive requiring evaluation example size test reduced examples click reduces signiﬁcance results. bias results towards naive policy choosing chronologically ﬁrst events test nevertheless naive policy receives reward signiﬁcantly less approaches. possible fear evaluation naive policy always ﬁnding good simply weren’t explored. quick check shows correct– naive argmax simply makes implausible choices. data. approach means policies must generally select larger available moment time live system implying policy evaluation generally pessimistic. note logging policy contrast optimistically evaluated test-time available smaller available test-time traintime implying frequency estimates test-time train+test dataset generally smaller estimate using test-time ads. smaller-than-necessary frequency estimates imply logging policy evaluation optimistic since events weighted inverse frequency. consequently choice provides conservative estimate policies optimistic choice older policy. particular policy optimized argmax form argmaxa∈c{f crucial distinction previous approaches trained. regression function trained estimate probability click feasible ads. extreme example suppose data days ﬁrst training second testing. suppose single appears train single appears test fact budget ﬁrst day. empirical estimate test used denominator estimator fact true probability test thus value logging policy estimated factor suppose indeed better evaluation policy always chooses better using equation zero drastic underestimate true value. logging policy chooses actions independently input. introduced method works assumption violated. hand required logging policy sequence ﬁxed possibly deterministic policies whereas exploration scavenging paper allowed logging policies learn adapt time. interesting situation occurs allow depend history time setting policy adapt choose actions dependent current input. oﬄine policy estimator work setting? generic answer exist natural constraint encapsulates approach discussed here well earlier paper.", "year": 2010}