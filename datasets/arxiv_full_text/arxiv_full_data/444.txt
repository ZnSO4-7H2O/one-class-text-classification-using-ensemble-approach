{"title": "Teaching Machines to Read and Comprehend", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "text": "teaching machines read natural language documents remains elusive challenge. machine reading systems tested ability answer questions posed contents documents seen large scale training test datasets missing type evaluation. work deﬁne methodology resolves bottleneck provides large scale supervised reading comprehension data. allows develop class attention based deep neural networks learn read real documents answer complex questions minimal prior knowledge language structure. progress path shallow bag-of-words information retrieval algorithms machines capable reading understanding documents slow. traditional approaches machine reading comprehension based either hand engineered grammars information extraction methods detecting predicate argument triples later queried relational database supervised machine learning approaches largely absent space lack large scale training datasets difﬁculty structuring statistical models ﬂexible enough learn exploit document structure. obtaining supervised natural language reading comprehension data proved difﬁcult researchers explored generating synthetic narratives queries approaches allow generation almost unlimited amounts supervised data enable researchers isolate performance algorithms individual simulated phenomena. work data shown neural network based models hold promise modelling reading comprehension something build upon here. historically however many similar approaches computational linguistics failed manage transition synthetic data real environments closed worlds inevitably fail capture complexity richness noise natural language work seek directly address lack real natural language training data introducing novel approach building supervised reading comprehension data set. observe summary paraphrase sentences associated documents readily converted context–query–answer triples using simple entity detection anonymisation algorithms. using approach collected corpora roughly million news stories associated queries daily mail websites. demonstrate efﬁcacy corpora building novel deep learning models reading comprehension. models draw recent developments incorporating attention mechanisms recurrent neural network architectures allows model focus aspects document believes help answer question also allows visualises inference process. compare neural models range baselines heuristic benchmarks based upon traditional frame semantic analysis provided state-of-the-art natural language processing table corpus statistics. articles collected starting april june daily mail april validation data march test data april articles tokens queries whose answer entity appear context ﬁltered out. reading comprehension task naturally lends formulation supervised learning problem. speciﬁcally seek estimate conditional probability context document query relating document answer query. focused evaluation wish able exclude additional information world knowledge gained co-occurrence statistics order test model’s core capability detect understand linguistic relationships entities context document. approach requires large training corpus document–query–answer triples corpora limited hundreds examples thus mostly testing limitation meant work area taken form unsupervised approaches templates syntactic/semantic analysers extract relation tuples document form knowledge graph queried. propose methodology creating real-world large scale supervised training data learning reading comprehension models. inspired work summarisation create machine reading corpora exploiting online newspaper articles matching summaries. collected articles articles daily mail websites. news providers supplement articles number bullet points summarising aspects information contained article. importance summary points abstractive simply copy sentences documents. construct corpus document–query– answer triples turning bullet points cloze style questions replacing entity time placeholder. results combined corpus roughly data points code replicate datasets—and apply method sources—is available online. note focus paper provide corpus evaluating model’s ability read comprehend single document world knowledge co-occurrence. understand distinction consider instance following cloze form queries hi-tech helps beat breast could saccharin help beat oils help ﬁght prostate ngram language model trained daily mail would easily correctly predict regardless contents context document simply frequently cured entity daily mail corpus. producer allegedly struck jeremy clarkson press charges gear host lawyer said friday. clarkson hosted most-watched television shows world dropped wednesday internal investigation british broadcaster found subjected producer oisin tymon unprovoked physical verbal attack. producer allegedly struck press charges host lawyer said friday hosted watched television shows world dropped wednesday internal investigation broadcaster found subjected producer unprovoked physical verbal attack prevent degenerate solutions create focused task anonymise randomise corpora following procedure coreference system establish coreferents data point; replace entities abstract entity markers according coreference; randomly permute entity markers whenever data point loaded. compare original anonymised version example table clearly human reader answer queries correctly. however anonymised setup context document required answering query whereas original version could also answered someone requisite background knowledge. therefore following procedure remaining strategy answering questions exploiting context presented question. thus performance corpora truly measures reading comprehension capability. naturally production system would beneﬁt using available information sources clues language co-occurrence statistics. table gives indication difﬁculty task showing frequent correct answer contained entity markers given document. note models don’t distinguish entity markers regular words. makes task harder models general. motivated need better datasets tasks evaluate capabilities machine reading models. proceed describing number baselines benchmarks models evaluate paradigm. deﬁne simple baselines majority baseline picks entity frequently observed context document whereas exclusive majority chooses entity frequently observed context observed query. idea behind exclusion placeholder unlikely mentioned twice single cloze form query. traditionally pipeline models used attempting question answering models make heavy linguistic annotation structured world knowledge semantic parsing similar pipeline outputs. building approaches deﬁne number nlp-centric models machine reading task. frame-semantic parsing frame-semantic parsing attempts identify predicates arguments allowing models access information whom. naturally kind annotation lends exploited question answering. develop benchmark makes frame-semantic annotations obtained parsing model state-ofthe-art frame-semantic parser parser makes extensive linguistic information benchmarks unanonymised version corpora. signiﬁcant advantage frame-semantic approach used possess capability generalise language model beyond exploiting parsing phase. thus objective evaluating machine comprehension abilities maintained. extracting entity-predicate triples— denoted —from query context document attempt resolve queries using number rules increasing recall/precision trade-off follows table resolution strategies using propbank triples. denotes entity proposed answer fully qualiﬁed propbank frame strategies ordered precedence answers determined accordingly. heuristic algorithm iteratively tuned validation data set. reasons clarity pretend propbank triples form practice take argument numberings parser account compare like like except cases permuted frame rule ordering relaxed. case multiple possible answers single rule randomly choose one. word distance benchmark consider another baseline relies word distance measurements. here align placeholder cloze form question possible entity context document calculate distance measure question context around aligned entity. score calculated summing distances every word nearest aligned word alignment deﬁned matching words either directly aligned coreference system. tune maximum penalty word validation data. neural networks successfully applied range tasks nlp. includes classiﬁcation tasks sentiment analysis tagging well generative problems language modelling machine translation propose three neural models estimating probability word type document answering query vocabulary indexes weight matrix slight abuse notation word types double indexes. note privilege entities variables model must learn differentiate input sequence. function returns vector embedding document query pair. deep lstm reader long short-term memory networks recently seen considerable success tasks machine translation language modelling used translation deep lstms shown remarkable ability embed long sequences vector representation contains enough information generate full translation another language. ﬁrst neural model reading comprehension tests ability deep lstm encoders handle signiﬁcantly longer sequences. feed documents word time deep lstm encoder delimiter also feed query encoder. alternatively also experiment processing query document. result model processes document query pair single long sequence. given embedded document query network predicts token document answers query. indicates vector concatenation hidden state layer time input forget output gates respectively. thus deep lstm reader deﬁned glstm input concatenation separated delimiter |||. attentive reader deep lstm reader must propagate dependencies long distances order connect queries answers. ﬁxed width hidden vector forms bottleneck information propose circumvent using attention mechanism inspired recent results translation image recognition attention model ﬁrst encodes document query using separate bidirectional single layer lstms denote outputs forward backward lstms respectively. encoding query length formed concatenation ﬁnal forward backward outputs −→yq ←−yq document composite output token position −→yd ←−yd. attentive reader viewed generalisation application memory networks question answering model employs attention mechanism sentence level sentence represented embeddings. attentive reader employs ﬁner grained token level attention mechanism tokens embedded given entire future past context input document. impatient reader attentive reader able focus passages context document likely inform answer query. equipping model ability reread document query token read. token query model computes document representation vector using bidirectional result attention mechanism allows model recurrently accumulate information document sees query token ultimately outputting ﬁnal joint document query representation answer prediction described number models previous section next evaluate models reading comprehension corpora. hypothesis neural models principle well suited task. however argued simple recurrent models lstm probably insufﬁcient expressive power solving tasks require complex inference. expect attention-based models would therefore outperform pure lstm-based approaches. considering second dimension investigation comparison traditional versus neural approaches strong prior favouring approach other. numerous publications past years demonstrated neural models outperforming classical methods remains unclear much side-effect language modelling capabilities intrinsic neural model nlp. entity anonymisation permutation aspect task presented levelling playing ﬁeld regard favouring models capable dealing syntax rather semantics. considerations mind experimental part paper designed threefold aim. first want establish difﬁculty machine reading task applying wide range models second compare performance parse-based methods versus neural models. third within group neural models examined want determine component contributes performance; want analyse extent lstm solve task extent various attention mechanisms impact performance. model hyperparameters tuned respective validation sets corpora. experimental results table attentive impatient readers performing best across datasets. deep lstm reader consider hidden layer sizes depths initial learning rates batch sizes dropout evaluate types feeds. setup feed ﬁrst context document subsequently question encoder model starts feeding question followed context document. report results best model attention models consider hidden layer sizes single layer initial learning rates batch sizes dropout models used asynchronous rmsprop momentum decay appendix details experimental setup. valid maximum frequency exclusive frequency frame-semantic model word distance model deep lstm reader uniform reader attentive reader impatient reader frame-semantic benchmark frame-semantic model proposed paper clearly simpliﬁcation could achieved annotations pipeline highlight difﬁculty task approached symbolic perspective. issues stand analysing results detail. first frame-semantic pipeline poor degree coverage many relations picked propbank parser adhere default predicate-argument structure. effect exacerbated type language used highlights form basis datasets. second issue frame-semantic approach trivially scale situations several sentences thus frames required answer query. true majority queries dataset. word distance benchmark surprising perhaps relatively strong performance word distance benchmark particularly relative frame-semantic benchmark expected perform better. here again nature datasets used explain aspects result. frame-semantic model suffered language used highlights word distance model beneﬁted. particularly case daily mail dataset highlights frequently signiﬁcant lexical overlap passages accompanying article makes easy word distance benchmark. instance query hanks friends manager scooter brown phrase turns good friends scooter brown manager carly jepson context. word distance benchmark correctly aligns frame-semantic approach fails pickup friendship management relations parsing query. expect types machine reading data questions rather cloze queries used particular model would perform signiﬁcantly worse. neural models within group neural models explored here results paint clear picture impatient attentive readers outperforming models. consistent hypothesis attention ingredient machine reading question answering need propagate information long distances. deep lstm reader performs surprisingly well demonstrating simple sequential architecture reasonable learning abstract long sequences even thousand tokens length. however model fail match performance attention based models even though single layer lstms. poor results uniform reader support hypothesis signiﬁcance attention mechanism attentive model’s performance difference models attention variables ignored uniform reader. precisionrecall statistics figure highlight strength attentive approach. visualise attention mechanism heatmap context document gain insight models’ performance. highlighted words show tokens document attended model. addition must also take account vectors figure attention heat maps attentive reader correctly answered validation queries examples require signiﬁcant lexical generalisation co-reference resolution order answered correctly given model. token integrate long range contextual information bidirectional lstm encoders. figure depicts heat maps queries correctly answered attentive reader. cases conﬁdently arriving correct answer requires model perform signiﬁcant lexical generalsiation e.g. ‘killed’ ‘deceased’ co-reference anaphora resolution e.g. ‘ent killed’ identiﬁed.’ however also clear model able integrate signals rough heuristic indicators proximity query words candidate answer. supervised paradigm training machine reading comprehension models provides promising avenue making progress path building full natural language understanding systems. demonstrated methodology obtaining large number document-queryanswer triples shown recurrent attention based neural networks provide effective modelling framework task. analysis indicates attentive impatient readers able propagate integrate semantic information long distances. particular believe incorporation attention mechanism contributor results. attention mechanism employed instantiation general idea exploited. however incorporation world knowledge multi-document queries also require development attention embedding mechanisms whose complexity query scale linearly data size. still many queries requiring complex inference long range reference resolution models able answer. data provides scalable challenge support research future. further signiﬁcantly bigger training data sets acquired using techniques described undoubtedly allowing train expressive accurate models.", "year": 2015}