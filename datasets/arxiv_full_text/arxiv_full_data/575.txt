{"title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker  Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.", "text": "recent research deep learning offers idea ‘feature learning’. shown deep neural network task-oriented features learned layer layer input. example automatic speech recognition phone-discriminative features learned spectra ﬁlter bank energies learned features powerful defeated mfcc dominated several decades capability dnns learning task-oriented features utilized learn speaker-discriminative features well. recent study shows possible least text-dependent tasks authors reported reasonable performance achieved dnn-learned feature additional performance gains obtained combining dnnbased approach i-vector approach. although dnn-based feature learning shows great potential existing implementation still compete i-vector baseline. least drawbacks current implementation first model information phone content leads difﬁculty inferring speaker-discriminative features; second evaluation based speaker vectors derived averaging frame-wise features. simple average ignores temporal constraint highly important text-dependent tasks. note tasks ﬁxed test phrase drawbacks closely linked other. paper follows work provides enhancements dnn-based feature learning first phone posteriors involved input speakerdiscriminative features learned easier alleviating impact phone variation; second scoring methods consider temporal constraint proposed segmentation pooling dynamic time warping rest paper organized follows. section describes related work section presents dnn-based feature learning. methods proposed section experiments presented section finally section concludes paper discusses future work. abstract—a deep learning approach proposed recently derive speaker identiﬁes deep neural network approach applied text-dependent speaker recognition tasks shows reasonable performance gains combined conventional i-vector approach. although promising existing d-vector implementation still compete i-vector baseline. paper presents improvements deep learning approach phonedependent structure normalize phone variation scoring approach based dynamic time warping experiments text-dependent speaker recognition task demonstrated proposed methods provide considerable performance improvement existing d-vector implementation. index terms d-vector time dynamic warping speaker recognition modern speaker recognition systems based human-crafted acoustic features example frequency cepstral coefﬁcients problem mfcc involves plethora information besides speaker identity phone content channels noises etc. heterogeneous noisy information convolve together making difﬁcult used either speech recognition speaker recognition. modern speaker recognition systems rely statical model ‘purify’ desired speaker information. example famous gaussian mixture model-universal background model framework acoustic space divided subspaces form gaussian components subspace roughly represents phone. formulating speaker recognition task subtasks phone subspaces gmm-ubm model largely eliminate impact phone content acoustic factors. idea shared many advanced techniques derived gmm-ubm including joint factor analysis i-vector model spite great success gmm-ubm approach related methods still limited lack discriminative capability acoustic features. researchers proposed solutions based discriminative models. example approach gmm-ubms plda approach i-vectors discriminative methods achieved remarkable success. another direction look task-oriented features i.e. features discriminative speaker recognition although seems straightforward ‘feature engineering’ turns paper follows work provides several extensions. particularly speaker identity represented d-vector derived average pooling quite neat efﬁcient loses much information test signal distributional property temporal constraint. main contribution paper investigate utilize temporal constraint dnn-based approach. model studied speaker recognition several ways. example dnns trained used replace model derive acoustic statistics i-vector models. used replace plda improve discriminative capability i-vectors. methods rely generative framework i.e. i-vector model. dnn-based feature learning presented paper purely discriminative without generative model involved. well-known dnns learn task-oriented features input layer layer. property employed phone-discriminative features learned low-level features fbanks even spectra shown well-trained variations irrelevant learning task gradually eliminated feature propagates structure layer layer. feature learning powerful primary fbank feature defeated mfcc feature carefully designed people dominated several decades. property also employed learn speakerdiscriminative features. actually researchers much effort searching features discriminative speakers effort mostly vain mfcc still popular choice. success dnns suggests direction speaker-discriminative features learned data instead crafted hand. learning easily done process rather similar difference speaker recognition learning goal discriminate different speakers. fig. presents structure used work speaker-discriminative feature learning. following convention input layer involves window dimensional fbanks. window size found optimal work. structure involves hidden layers consists units. units output layer correspond speakers training data number experiment. -hot encoding scheme used label target training criterion cross entropy. learning rate beginning halved whenever improvement cross-validation found. training process stops learning rate small improvement marginal. trained successfully speaker-discriminative features read last hidden layer. test phase features extracted frames given utterance. derive utterance-based representations average pooling approach used framelevel features averaged resultant vector used represent speaker. vector called ‘d-vector’ kinds speaker vectors d-vector ivector fundamentally different. i-vectors based linear gaussian model learning unsupervised learning criterion maximum likelihood acoustic features; contrast d-vectors based neural networks learning supervised learning criterion maximum discrimination speakers. difference leads several advantages d-vectors first ‘discriminative’ vector represents speakers removing speakerirrelevant variance sensitive speakers invariant disturbance; second ‘local’ speaker description uses local context inferred short utterances; third relies ‘universal’ data learn model makes possible learn large amounts data task-independent. several limitations implementation feature learning paradigm presented previous section. first involve prior knowledge model training example phone identities. second simple average pooling consider temporal information particularly important text-dependent recognition tasks. several approaches proposed section address problems. potential problem dnn-based feature learning described previous section ‘blind learning’ i.e. features learned data without prior knowledge. means learning purely relies complex deep structure model large amount data discover speaker-discriminative patterns. training data abundant often problem; theoretical treatment based dynamic time warping algorithm principle measure similarities variable-length temporal sequences. simple sense searches optimal path matches sequences lowest cost employing dynamic programming method reduce search complexity. task dnn-extracted features utterance treated temporal sequence. test sequence derived enrollment utterance sequence derived test utterance matched cosine distance used measure similarity frame-level features. principally segment pooling regarded special case sequences length matching sequences piece-wise. however tasks limited amount data instance text-dependent task hand blind learning tends difﬁcult many speaker-irrelevant variations involved data particularly phone contents. possible solution supply model extra information phone spoken frame. simply achieved adding phone indicator input. however often easy phone alignment practice. alternative supply vector phone posterior probabilities frame ‘soft’ phone alignment easily obtained phone-discriminative model. work choose model trained produce phone posteriors. fig. illustrates phone posteriors involved structure. training process change structure. text-dependent speaker recognition essentially sequential pattern matching problem current d-vector approach derives speaker identities single vectors average pooling formulates speaker recognition vector matching. certainly ideal temporal constraint totally ignored deriving speaker vector. possible solution segment enrollment/test utterance several pieces derive speaker vector piece. speaker identity utterance represented sequence piece-wise speaker vectors speaker matching conducted matching corresponding vector sequences. paper adopts simple sequence matching approach sequences assumed identical length matching conducted piece piece independently. finally matching score takes average scores pieces. segmentation method feasible since i-vectors inferred feature distributions piece-wised solution simply degrades quality i-vector piece. d-vectors approach totally inferred local context experiments performed database involves limited short phrases. entire database contains recordings short phrases speakers phrase contains chinese characters. speaker every phrase recorded times amounting utterances speaker. training involves randomly selected speakers results utterances total. prevent overﬁtting cross-validation containing utterances selected training data remaining utterances used model training including model d-vector approach matrix plda model i-vector approach. evaluation consists remaining speakers. evaluation performed particular phrase. phrase trails including target trails non-target trials. sake neat presentation report results short phrases ‘pn’ denote n-th phrase. conclusions obtained generalize well phrases. baseline baseline systems built based i-vectors based d-vectors. acoustic features i-vector system -dimensional mfccs consist static components ﬁrstsecondorder derivatives. number gaussian components dimension i-vector d-vector baseline uses structure shown fig. average pooling used derive d-vectors. acoustic features -dimensional fbanks left right frames concatenated together. frame-level features extracted last hidden layer dimension table presents results terms equal error rate seen i-vector system generally outperforms d-vector system signiﬁcant way. particularly discriminative methods clearly improves i-vector system however d-vector system improvement found methods. surprising since d-vectors discriminative themselves. reason plda considered d-vectors following experiments. experiment phone posteriors included input shown fig. phone posteriors produced model trained chinese database consisting hours speech data. phone consists toneless initial ﬁnals chinese plus silence phone. results shown second table denoted ‘dnn+pt’. seen phone-dependent training leads marginal consistent performance improvement d-vector system. presented section segment pooling approach segments input utterance pieces derives dvector piece. scoring conducted piecewise d-vectors independently average scores pieces taken utterance-level score. results shown fig. seg-n means utterance segmented pieces. seen segment pooling offers clear performance improvement. following combine best i-vector system best d-vector system combination simply done interpolating scores obtained systems αsiv scores i-vector d-vector systems respectively interpolation factor. results optimal shown table iii. seen combination leads best performance obtain far. paper presented several enhancements dnnbased feature learning approach speaker recognition. presented phone-dependent model supply phonetic information learning speaker features proposed scoring methods based segment pooling respectively leverage temporal constraints. extensions signiﬁcantly improved performance d-vector system. future work involves investigating complicated statistical models d-vectors large amounts data learn powerful speaker-discriminative features. kenny boulianne ouellet dumouchel joint factor analysis versus eigenchannels speaker recognition ieee transactions audio speech language processing vol. speaker session variability gmm-based speaker veriﬁcation ieee transactions audio speech language processing vol. ehsan erik ignacio g.-d. javier deep neural networks small footprint text-dependent speaker veriﬁcation ieee international conference acoustic speech signal processing vol.", "year": 2015}