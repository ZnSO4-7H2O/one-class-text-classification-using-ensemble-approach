{"title": "Tensor2Tensor for Neural Machine Translation", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.", "text": "ashish vaswani samy bengio eugene brevdo francois chollet aidan gomez stephan gouws llion jones łukasz kaiser kalchbrenner niki parmar ryan sepassi noam shazeer jakob uszkoreit tensortensor library deep learning models well-suited neural machine translation includes reference implementation state-of-the-art transformer model. machine translation using deep neural networks achieved great success sequence-tosequence models used recurrent neural networks lstm cells basic sequence-to-sequence architecture composed encoder reads source sentence token time transforms ﬁxed-sized state vector. followed decoder generates target sentence token time state vector. pure sequence-to-sequence recurrent neural network already obtain good translation results suffers fact whole input sentence needs encoded single ﬁxed-size vector. clearly manifests degradation translation quality longer sentences partially overcome bahdanau using neural model attention. convolutional architectures used obtain good results word-level neural machine translation starting kalchbrenner blunsom later meng early models used standard convolution generate output creates bottleneck hurts performance. fully convolutional neural machine translation without bottleneck ﬁrst achieved kaiser bengio kalchbrenner extended neural model used recurrent stack gated convolutional layers bytenet model away recursion used left-padded convolutions decoder. idea introduced wavenet signiﬁcantly improves efﬁciency model. technique improved number neural translation models recently including gehring kaiser instead convolutions stacked self-attention layers. introduced transformer model signiﬁcantly improved state-of-the-art machine translation language modeling also improving speed training. research continues applying model domains exploring space self-attention mechanisms. clear self-attention powerful tool general-purpose sequence modeling. rnns represent sequence history hidden state transformer ﬁxed-size bottleneck. instead timestep full direct access history dot-product attention mechanism. effect enabling model learn distant temporal relationships well speeding training need wait hidden state propagate across time. comes cost memory usage attention mechanism scales length sequence. future work reduce scaling factor. transformer model illustrated figure uses stacked self-attention pointwise fully connected layers encoder decoder shown left right halves figure respectively. encoder encoder composed stack identical layers. layer sub-layers. ﬁrst multi-head self-attention mechanism second simple positionwise fully connected feed-forward network. decoder decoder also composed stack identical layers. addition sub-layers encoder layer decoder inserts third sub-layer performs multi-head attention output encoder stack. table maximum path lengths per-layer complexity minimum number sequential operations different layer types. sequence length representation dimension kernel size convolutions size neighborhood restricted self-attention. computational performance noted table self-attention layer connects positions constant number sequentially executed operations whereas recurrent layer requires sequential operations. terms computational complexity self-attention layers faster recurrent layers sequence length smaller representation dimensionality often case sentence representations used state-of-the-art models machine translations word-piece byte-pair representations. single convolutional layer kernel width connect pairs input output positions. requires stack convolutional layers case contiguous kernels case dilated convolutions increasing length longest paths positions network. convolutional layers generally expensive recurrent layers factor separable convolutions however decrease complexity considerably even however complexity separable convolution equal combination self-attention layer point-wise feed-forward layer approach take model. self-attention also yield interpretable models. tensortensor visualize attention distributions models individual layer head. observing closely models learn perform different tasks many appear exhibit behavior related syntactic semantic structure sentences. english-to-german translation task transformer model table outperforms best previously reported models bleu establishing state-of-the-art bleu score training took days gpus. even base model surpasses previously published models ensembles fraction training cost competitive models. english-to-french translation task model achieves bleu score outperforming previously published single models less training cost previous state-of-the-art model. base models used single model obtained averaging last checkpoints written -minute intervals. models averaged last checkpoints. used beam search beam size length penalty library deep learning models datasets designed make deep learning research faster accessible. uses tensorflow throughstrong focus performance well usability. tensorflow various tt-speciﬁc abstractions researchers train models locally cloud usually minimal device-speciﬁc code conﬁguration. development began focused neural machine translation tensortensor includes many successful models standard datasets. since added support task types well across multiple media number models datasets grown signiﬁcantly. usage standardized across models problems makes easy model multiple problems multiple models single problem. example usage usability beneﬁts standardization commands uniﬁcation datasets models training evaluation decoding procedures. datasets problem class encapsulate everything particular dataset. problem generate dataset scratch usually downloading data public source building vocabulary writing encoded samples disk. problems also produce input pipelines training evaluation well necessary additional information feature device conﬁguration type number location devices. tensorflow tensortensor currently support single multi-device conﬁgurations. tensortensor also supports synchronous asynchronous data-parallel training. model model ties together preceding components instantiate parameterized transformation inputs targets compute loss evaluation metrics construct optimization procedure. estimator experiment classes part tensorflow handle instantiating runtime running training loop executing basic support services like model checkpointing logging alternation training evaluation. abstractions enable users focus attention component they’re interested experimenting with. users wish models problem usually deﬁne problem. users wish create modify models create model edit hyperparameters. components remain untouched available reduces mental load allows users quickly iterate ideas scale. tensortensor provides vehicle research ideas quickly tried shared. components prove useful committed widely-used libraries like tensorflow contains many standard layers optimizers higher-level components. tensortensor supports library usage well script usage users reuse speciﬁc components model system. example multiple researchers continuing work extensions variations attention-based transformer model availability attention building blocks enables work. adafactor optimizer signiﬁcantly reduces memory requirements second-moment estimates developed within tensortensor tried various models problems. tf.contrib.data.bucket sequence length enables efﬁcient processing sequence inputs gpus tf.data.dataset input pipeline api. ﬁrst implemented exercised tensortensor. continuing development machine learning codebase maintaining quality models difﬁcult task expense randomness model training. freezing codebase maintain certain conﬁguration moving append-only process enormous usability development costs.", "year": 2018}