{"title": "Author Identification using Multi-headed Recurrent Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE", "68T50"], "abstract": "Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author's style. The method proves competitive, ranking first in two of the four languages.", "text": "abstract. recurrent neural network used model language several authors concurrently author‘s text represented separate outputs rely shared recurrent state. allows recurrent layer model language whole without over-fitting even small corpora. method developed author identification task performed well coming first overall average greater author identification task looks problem deciding whether document unknown authorship written author small documents. known unknown documents vary topics genres spread across four languages—dutch english greek spanish. training corpus containing problems language provided. software tested using tira evaluation-as-a-service platform unseen evaluation corpus. although task artificial designed proxy common author verification problems. recurrent neural network feed forward neural network shares parameters across time. time step simple hidden vector derived input vector previous hidden state hidden vector usually obtained affine transform offset bias vector followed non-linear activation function operates point-wise manner giving formula wxhxt bh). output vector similarly obtained hidden state though non-linear function often point-wise. discrete probability distribution desired softmax function self-referential hidden state allows network model complex time series processes. training involves iteratively adjusting weights biases usually using form gradient descent back-propagation time sake brevity details algorithms elided. tomáš mikolov’s thesis offers good introduction algorithms simple recurrent neural networks language modelling. language model predicts flow text symbol time estimating probability distribution i-th symbol given predecessors symbols belong predetermined vocabulary. practice language model limited horizon basing predictions based primarily recent symbols. explicit case n-gram based models recurrent neural networks adapt scope attention suit context. fig. diagram left tries show output hidden nodes previous time step flowing round back hidden node inputs better looking right network unfolded time. layers fully connected. time point hidden state summarises entire history input data. language models often word-based vocabulary large inevitably contains gaps. alphabetic languages character-based models also possible although don’t perform well general robust face novel words require less text time train. figure shows character-based language model trying navigate character sequence cat. accuracy language model document measured bits cross entropy mean negative binary probability model assigns cross-entropy thought measure information model fails model. language model predicts every character document probability result zero cross-entropy; hand assigning probability occurring character costly. fig. example character based language model work. model sees predicts chance. cross entropy single step seeing gives chance cross entropy steps mean supposing training effective language model better predict flow documents similar ones trained on—if similarity capturable model—and show reduced cross entropy relative unrelated documents. author identification problem consists several mini-corpora various languages mini-corpus contains documents known single author. documents amount thousand characters—generally fewer paper—and task decide whether another short document author. hypothesis underlying work character-level language model trained author’s known output match author’s unknown output closely matches text written arbitrary others. unfortunately conventional language models trained millions characters would severely over-fit corpus thousand—effectively learning recite training text verbatim. combat this multi-headed character-level language model introduced shares common recurrent state multiple independent softmax output groups. softmax group trained predominantly author’s corpus causing recurrent layer model combination author’s texts approximating language whole. output groups learn weight recurrent values best reflects authors’ tendencies. figure attempts illustrate this. fig. multi-headed language model multiple sets output probabilities—one author—each making slightly different predictions. cross entropies thus differ relatively cross entropy supporting hypothesis corresponding author wrote text examined. relative cross-entropy scores insufficient decide questions authorship brought evidence sway pre-existing authorship question. specific format panclef author identification challenge shapes interpretation scores paper. understood methods designed perform well specific artificial task useful general. therefore necessary cover task parameters guide work. task split four corpora based different language varying levels consistency topic genre varying numbers sizes documents. corpus contains questions documents known author document unknown origin. task decide whether unknown document written corresponding known author. note task symmetrical cases known document question posed labels swapped. known exactly half questions answered half looked independently question probability looked together probabilities usefully correlated. limited supply answer confident question diminishes chance another. solution ends imbalance positive negative answers guaranteed wrong. answer takes form score zero interpreted somewhat complicated fashion. score language product area curve general measure binary classifier efficacy measure accuracy rewards admissions uncertainty circumstances. precisely number problems number correct answers number explicitly uncertain answers purposes context score greater regarded score less exactly indicates uncertainty. given prior knowledge distribution answers solution never half scores either side measure oblivious score transforms preserve monotonicity shifting scores suit requirements preserving possible multiple uncertain answers required. system assigns scores uniform random distribution tend produce scores hence overall score easy devise system produces degenerate scores requires classifier combines cleverness perversity. known unknown texts mapped smaller character reduce computational complexity remove overwhelming self-importance extremely rare characters. separate mapping used language. text first converted nfkd unicode normal form decomposes accented letters letter followed combining accent becomes <u+><u+> capital letters decomposed uppercase marker followed corresponding lowercase letter. thus becomes triplet <right-combining uppercase modifier> <left-combining grave accent>. various rare characters seem largely equivalent mapped together; example en-dash em-dash rare appear used interchangeably practice mapped together. punctuation marks—such various forms apostrophes single quotation marks—are likewise collapsed canonical forms. likely discards usable information—indications finger habits choice software—but avoidance extremely rare characters important. known author corpus uses character assigned excessive weight. greek text latin characters mapped letter arbitrarily chosen doesn’t resemble greek character. rationale foreign quotations references appear rarely content valuable attempt model processing character frequency lower discarded. characters occurring text resultant alphabet ignored—there unknown token. alphabet sizes english dutch greek spanish. finally runs identical characters truncated mainly aimed latin stretches greek text; little value model guess exact word length language can’t directly see. example following greek text superscript represents capital marker attaching next character superscript acute belonging previous character. case transformation reveals author spelt this uppercase instead visually identical uppercase multi-headed recurrent neural network language model concept briefly introduced section single recurrent neural network trained predict flow text many authors sharing collective model complete language. output layer softmax group models probability distribution expected author sometimes another control corpus—a large body text intended help prevent over-fitting recurrent layer. convenience training used control corpora. turns overlap training sets control corpora completely independent. empirically seem small positive effect. otherwise terms non-zero part follows model widely used rectified linear unit defined max) output derivative zero non-positive numbers offers performance propagative benefits allows neurons opining areas outside speciality. relu difficult recurrent neural networks lack inherent scale means slip explosive cycle amplification easily conventional squashing activations like tanh. resqrt steers middle course empirically works well character based language models. resqrt activation described before. training uses form adagrad learning rate weight change inversely proportional distance already changed. amounts monotonically decreasing per-weight learning rate. training structured number sub-epochs. sub-epoch author provides training text. text every author sub-epoch conventional epoch; otherwise author’s texts used cyclical way. runs texts author concatenated—in cases sub-epoch also true epoch. another mode sub-epoch balanced documents drawn author trained around amount. training step chance training example leaking affecting authors also made particular choice character. calling authors problems character text normal case back-propagated error authors aj̸=i zero; example leaks training error back-propagated. effect blaming author actual text small part others serves proxy overall possibilities language. avoids extreme overfitting. initial leakage rate order decays exponentially sub-epoch. towards training leakage rate author’s sub-model trained almost entirely text. parameters thus roughly early training high leakage high learning rate refined specialise author’s style. mini-batch size used meaning weights modified accumulated deltas every characters. training gradient first characters text ignored. back-propagation time truncated depth sooner gradient diminishes adaptive threshold. hidden layer quite small; experiments larger sizes showed benefit. single known text significantly shorter unknown text swapped around trained unknown text. insufficient training text causes over-fitting worse insufficient test text increases uncertainty. ensemble variations final results combine several runs together using variety configurations metaparameters. approximate ranges shown table consists training phase lasting several minutes hours followed calculation cross entropies unknown documents takes less second. meta-parameter initial adagrad learning scale initial leakage classes leakage decay hidden neurons presynaptic noise sub-epochs text direction text handling initialisation control corpus sets seemingly reasonable meta-parameters chosen per-language haphazard search random selections configurations used evaluation ensembles. evaluations time-out; number hours test scores obtained averaged together. length time dedicated ensemble determined available time submission due. interpretation predictive output clusters representing known authors problem corpus system produces cross entropy scores document. training https//github.com/douglasbagnall/recur https//github.com/pan-webis-de/caravel data problems hence scores language; evaluation numbers vary languages. problems share known authors reality number authors smaller. scores directly comparable random variation various sub-models’ performance inherent variation texts’ predictability. texts difficult models. scores normalised subtracting mean cross-entropy text resultant zero-centred relative entropies scaled sub-model scores similar range. author’s sub-model ranking score unknown text determines probability author wrote text. example text ranks first likely author’s work; ranks last probably not. system likely optimal given ignores nuances evidence simple implement works well enough. final score must take form probability special significance calculation. discussed section task designed exactly half cases unknown documents known author correct align point take median. documents ranking better median receive score linearly related ranking ranking worse median receive lower score. spanish greek english tasks small radius around median also assigned score. training corpus little effect slightly increased component. table selected results author identification evaluation taken score product area curve score using indicate non-decision described section results sorted macro-average. micro-average differs macro-average different numbers problems languages english dutch spanish greek. table details paper’s results showing constituent portions margins relative highest scores teams. positive margin indicates software’s score ahead nearest team; negative margin shows behind leader. consistently results dutch task seem reflect drastic genre difference known unknown texts. many teams better task presumably concentrating local flow characters instead using linguistic knowledge otherwise deriving longer range features. hand genre topic seem closer model performs well despite using specialist linguistic natural language processing techniques. ought extrapolate well languages work well concert unrelated methods. performance english task notable exceeding teams’ results significant margin. possibly larger corpus increasing breadth training perhaps smoothing noise ranking procedure. without active recurrent layer model falls predicting unigram character probabilities author output bias vector. cross entropy case amounts weighting frequencies characters text training score english spanish; english best training results. humiliating confirms validity training scoring methods. leakage authors allows sub-model learn background probabilities language whole presumably better common technique giving unseen symbols fixed chance. entirely additive cross entropy sidesteps curse dimensionality. also eschews possibility moment involving strong positive evidence—positive evidence typically accumulated fraction time course entire document. contrasts typical human approach often involves identifying idiosyncratic usages ignoring boring bits directly reported authorship probability distribution conceivable unlikely easy train. character level language model learns concentrate text; direct approach likely distracted. tirelessly trying understand text strength method. contemporary research recurrent neural networks focuses sophisticated models using long short term memory gated recurrent units deeper wider architectures seen here. allow rnns discover longer range complex dependencies. whether would help task open question—a long range view much training character text. resqrt activation used work well precisely learn much instead relies surface detail writing style. computational cost sophisticated networks would also problematic context competition limited hardware. nevertheless work demonstrates even small recurrent neural networks useful tool authorship identification. approach owes information theory traditional clustering happily avoids quagmires feature selection excessive dimensionality. results training size recurrent layer mistakenly reduced zero suggest interpretive approach important predictive ability network. thank recently deceased david mackay whose clear writing topics related inference guided work much else.", "year": 2015}