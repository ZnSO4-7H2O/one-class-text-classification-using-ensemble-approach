{"title": "Unsupervised Learning Layers for Video Analysis", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "This paper presents two unsupervised learning layers (UL layers) for label-free video analysis: one for fully connected layers, and the other for convolutional ones. The proposed UL layers can play two roles: they can be the cost function layer for providing global training signal; meanwhile they can be added to any regular neural network layers for providing local training signals and combined with the training signals backpropagated from upper layers for extracting both slow and fast changing features at layers of different depths. Therefore, the UL layers can be used in either pure unsupervised or semi-supervised settings. Both a closed-form solution and an online learning algorithm for two UL layers are provided. Experiments with unlabeled synthetic and real-world videos demonstrated that the neural networks equipped with UL layers and trained with the proposed online learning algorithm can extract shape and motion information from video sequences of moving objects. The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization.", "text": "paper presents unsupervised learning layers label-free video analysis fully connected layers convolutional ones. proposed layers play roles cost function layer providing global training signal; meanwhile added regular neural network layers providing local training signals combined training signals backpropagated upper layers extracting slow fast changing features layers different depths. therefore layers used either pure unsupervised semi-supervised settings. closedform solution online learning algorithm layers provided. experiments unlabeled synthetic real-world videos demonstrated neural networks equipped layers trained proposed online learning algorithm extract shape motion information video sequences moving objects. experiments demonstrated potential applications layers online learning algorithm head orientation estimation moving object localization. deep neural networks powerful ﬂexible models extract hierarchical discriminative features large amounts data. despite success many supervised tasks image classiﬁcation speech recognition machine translation dnns data hungry limits applications many domains abundant annotated data available. motivates explore almost inﬁnite amount unlabeled data obtaining good representations generalizes across tasks. learning unlabeled data often named unsupervised learning divided four levels learning unlabeled images unlabeled videos virtual environment real environment directly. paper focus unsupervised learning label-free videos. main challenges unsupervised learning derive good training signals. supervised learning difference prediction annotated ground truth training signal. unsupervised learning ways obtain signal design auxiliary tasks reconstructing input image predicting next fewer frames given ﬁrst fewer frames video reordering shufﬂed video frames generating fake images/videos generative adversarial network settings etc; another provide constraints describe desired structure output dnn. work belongs latter one. inspired human visual system learn invariant representations structures objects temporal experiences design objective function constrains output temporally consistent meanwhile avoiding degenerated cases. evaluate proposed algorithm synthetic natural video settings. experiments end-to-end training unlabeled-videos applications head orientation estimation moving object localization demonstrated effectiveness proposed algorithm. contributions paper following design unsupervised learning layers label-free video analysis fully connected layers convolutional layers. proposed layers play roles cost function layer providing global training signal; meanwhile added regular neural network layers providing local training signals combined training signals backpropagated upper layers extracting slow fast changing features layers different depths. layers applied neural network architectures used either pure unsupervised semi-supervised settings. evaluated proposed algorithm synthetic real-world videos show potential application head orientation estimation moving object localization. rest paper organized follows. brieﬂy review related work section detailed description proposed methods given section followed experimental results section conclusion section work closely related representation learning unsupervised learning application label-free video analysis. here brieﬂy review related work areas. representation learning learning good data representations main goals machine learning algorithms matter whether supervised unsupervised one. unsupervised learning want learned representations data makes easier build classiﬁers predictors upon them. question evaluate quality learned representations. goodfellow proposed number empirical tests directly measuring degree learned representations invariant different transformations. higgins devised protocol quantify degree disentanglement learned different models. bengio list properties/attributes good representation possess sparsity distributed among multiple explanatory factors hierarchical organization abstract invariant concepts higher hierarchy temporal spatial coherence. listed properties provide guide design appropriate objective functions learning representations. unsupervised learning unsupervised learning visual representations broad area rich history large volume work ranging classical k-means clustering dimensionality reduction sparse coding rbms autoencoders single-layer analysis recent work vaes gans pixelcnns pixelrnns increase computational capabilities also makes large-scale deep unsupervised learning possible. work closely related unsupervised learning label-free video. videos provide import temporal constraints machine learning algorithms abundant websites. recent work employ video prediction auxiliary task learning useful representations. argument algorithm predict next fewer frames well learn features related objects’ shape motion etc. algorithms usually predict next frames pixel interstate object location levels using language models motion transformation lstm probabilistic models unlike work relies auxiliary tasks unsupervised learning work belongs constraints-based unsupervised learning. design objective function constrains output temporally consistent meanwhile avoiding degenerated cases. related work. authors propose employ laws physics constraints supervising neural network training. tracking object free fall constrain outputs neural network encodes object’s height form parabola; tracking position walking outputs satisfy constant velocity constraint. therefore design different loss functions different types object motion. contrast objective function general used analyze videos smoothly moving objects. authors propose slow feature analysis algorithm learning invariant slowly varying features demonstrate learned functions good match complex cell properties. similar closed-form solution method objective function also constrains output vary slowly possible keep unit variance. like work also proposed neural network implementations algorithm. difference designed unsupervised learning layers implementing online algorithms. layers integrated neural networks seamlessly. problem formulation label-free learning setting training training video sequences. sequence xim} consists image frames. goal learn internal representation function although closed-form solution appealing computationally expensive long video sequences practical real world situations frames captured time. therefore derive online learning algorithm applied deep neural network current output neural network short-term moving average long-term moving average. updated follows implemented online learning algorithm unsupervised learning layers fully connected layers convolutional layers. output fully connected layers covariances calculated respect whole vector; output convolutional layers covariances calculated respect feature channel. layers added different deep learning models different layers providing directly improve feature representation layer unsupervised learning manner. weight term determined based distance unsupervised layer input layer. closer input larger lower level receptive ﬁeld node smaller corresponding input signal tends change faster. therefore proposed unsupervised learning layers capture fast slow features different depths dnn. unlike regular layers gradients calculated backward stages unsupervised layers partial derivatives calculated forward stages combined gradient upper layers backward stage. algorithm shows summary algorithm. summary proposed layers play roles cost function layer providing global training signal; meanwhile added regular neural network layers providing local training signals combined training signals backpropagated upper layers extracting features different changing paces. therefore layers used either pure unsupervised semi-supervised settings supervised cost function layer several layers following regular hidden layers. evaluate convergence speed robustness proposed algorithm noise generated synthetic video sequences consisting randomly selected points rotating around centroid sampled frame degrees training shown fig. network consists fully connected layer followed unsupervised layer. input size output hyperparameters learning rate=. momentum=. weight decay rate=. fig. visualize weight matrices learned fully connected layer corresponds major eigenvectors input sequence. fig. indicates output encodes rotation angle frame sequence. demonstrates neural network trained proposed algorithm encode input shape weights motion output. decoded output using linear regression respect function rotation angles calculated total absolute errors output ground truth. fig. illustrates algorithm converge within epochs training test set. evaluate noise sensitivity proposed algorithm tested algorithm noise levels ranging fig. demonstrates proposed algorithm exhibits smooth degradation noise level increased gain understanding representations learned convolutional layers trained neural network consisting regular convolutional fully connected layers followed unsupervised layer predicting head orientation video sequence shows input image frame fig. shows learned four convolution kernels fig. shows output convolutional layers demonstrates learned representations captured low-level features edges corners circles. also trained network video sequences ﬁrst task predict location pillow tossed person shown fig. dataset consists sequences pillow ﬂight totaling images. images resized pixels. comparison purpose employ architecture consists three convolutional layers followed fully connected layers unsupervised learning layer. fig. shows qualitative results network training sequences epochs. since author provide ground truth dataset perform quantitative performance evaluation dataset. second task predict mask moving object video shown fig. dataset consisted color images pixel size sequences training sequences validation. fig. shows recovered masks centroids masks locate walking person correctly. fig. shows comparison prediction ground-truth. estimate presents unsupervised learning layers label-free video analysis fully connected layers convolutional layers. proposed layers play roles cost function layer providing global training signal; meanwhile added regular neural network layers providing local training signals combined training signals backpropagated upper layers extracting slow fast changing features layers different depths. closed-form solution online learning algorithm implemented layers provided. experiments training unlabeled synthetic natural videos demonstrated effectiveness proposed algorithm potential applications head orientation estimation moving object localization. several parts presented approach could tuned studied detail. proposed unsupervised layers could used semi-supervised setting. temporal consistency constraint could extended spacial space. leave questions future research. consider present work ﬁrst step making agent perform unsupervised learning real environment. thank members haonan zhuoyuan chen haichao zhang tianbing yuanpeng peng wang jianyu wang xiaochen lian interns qing yaming wang zhenheng yang zihang encouragement. therefore order minimize eigenvalues smallest ones except generalized eigenvector eigenvalue note vector whose elements also generalized eigenvector eigenvalue therefore hence denote short-term long-term covariances output neural network respectively. ﬁrst term minimizes entropy short term distribution second term maximizes entropy long term distribution. therefore closer input frames closer corresponding output representations would farther away inputs farther away corresponding output representations short-term moving average long-term moving average.", "year": 2017}