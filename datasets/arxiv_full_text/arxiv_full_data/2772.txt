{"title": "The Promise and Peril of Human Evaluation for Model Interpretability", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications. This alone presents a challenge in many areas of artificial intelligence. In this position paper, we propose a distinction between descriptive and persuasive explanations. We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences. If this is indeed the case, evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency. Finally, we propose two potential research directions to disambiguate cognitive function and explanation models, retaining control over the tradeoff between accuracy and interpretability.", "text": "transparency user trust human comprehension popular ethical motivations interpretable machine learning. support goals researchers evaluate model explanation performance using humans real world applications. alone presents challenge many areas artiﬁcial intelligence. position paper propose distinction descriptive persuasive explanations. discuss reasoning suggesting functional interpretability correlated cognitive function user preferences. indeed case evaluation optimization using functional metrics could perpetuate implicit cognitive bias explanations threaten transparency. finally propose potential research directions disambiguate cognitive function explanation models retaining control tradeoff accuracy interpretability. interpretable explanations present direct conﬂict explanations best describe underlying model explanations tailored user. features important however transparency ethical motivation regarding former latter feature matter user trust human comprehension. interpretable machine learning community championed evaluation functional interpretability using human user studies. little work focused inherent problems bias human evaluation cases hope balance accuracy complexity. discussion regarding tradeoff accuracy complexity models long preceded ﬁeld interpretable machine learning. statistics discuss \"parsimony\" problem also discussed philosophy science \"scientiﬁc realism\" \"simpliﬁcation\" majority methods found interpretable machine learning community optimize model complexity straying away complex measures interpretability. fact ribeiro explicitly mentions optimization using model complexity proxy interpretability seminal paper. evaluation explanation systems conducted using complex deﬁnitions interpretability. difference explanation systems optimized explanation systems evaluated introduce bias explanation vehicles strategies themselves. control tradeoff accuracy simplicity risk pursuit better performance real world evaluation metrics. section deﬁne terminology relevant interpretable model explanations human cognitive function affect functional interpretability. provide deﬁnitions distinguish explanations emphasize accuracy explanations balance accuracy human-related metrics section motivation discussion concerning human evaluation resulting biases discussed section present potential research directions mitigate negative effects human evaluation interpretable machine learning section interpretable explanation explanation simple model visualization text description lies interpretable feature space approximates complex model. work focus form statistical models algorithms rules referring explanation models. model complexity measure amount information contained model. paper often refer model complexity explanatory model explanation complexity. introduce terms clarify different components interpretable explanation model systems. explanation vehicle model class explanation decision trees generalized additive models falling rule lists expand deﬁnition pairings model class ﬁxed explanation complexity constraint e.g. decision trees depth less deﬁne explanation strategy combination explanation vehicle objective function constraints hyperparameters required generate interpretable model explanation. finally consider particular implementation explanation strategy underlying machine learning model explanation model system explanation system short. high-ﬁdelity explanations also referred faithful strong correspondence explanation model underlying machine learning model deﬁne descriptive explanation strategy generates explanations maximum model ﬁdelity particular explanation vehicle underlying machine learning model. optimizing descriptive explanation often involves traditional accuracy metric mean squared error calculated underlying explanatory models. descriptive explanations best satisfy ethical goal transparency. explanations faithful underlying machine learning model humans information inner workings system. value societal importance studied science technology studies community well public policy community. contrast descriptive explanations persuasive explanation strategiesdo achieve maximum model ﬁdelity often incorporate user preferences knowledge characteristics. much like persuasive argument explanations balance accuracy convincing user. less faithful underlying model descriptive explanations tradeoff freedom explanation complexity structure parameters. freedom permits explanations better tailored human cognitive function making functionally interpretable. model complexity practiced persuasive treatments. natural optimal model complexity explanation dependent user expectations expertise. researchers often must limit explanation complexity facilitate trust understanding humans. make explanation simple enough contemplated human once trait lipton calls simulatability. user trust understanding popular ethical motivations interpretability. consider goals persuasive rely human cognitive function preferences. framing ethical goals respect persuasive explanation strategies highlights ethical dilemmas plagued model interpretability research it’s currently practiced doshi-velez review evaluation within interpretable machine learning describing human evaluation practical applications functional evaluation using quantitative metrics. after proposing taxonomy evaluation methods hypothesize common latent factors inform understanding explanation vehicles applications applied. believe important analysis pursue. however ﬁnding latent factors amongst heavily inﬂuenced decisions explanation vehicles application domains performance metrics lead biased output unless representation across choice balanced. human cognitive attributes user expectations indeed predictive user trust comprehension major source bias. expectations expertise explanations compared differ greatly across users applications. also characteristics shared across human users affect performance explanation system. refer implicit human cognitive bias. explanation systems likely overﬁt attributes researchers produce systems. effect ampliﬁed publication bias. ﬁeld matures explanation vehicles strategies achieve results performance published visible community. researchers favor mimic performing explanation strategies basis future work. methods incorporate human cognitive function preferences score highly functional evaluation metrics future work based research capture characteristics higher functional performance. even understand cognitive attributes explanation strategy methods like doshi-velez fail allow ﬁne-tuned control accuracy interpretability. ﬁelds within artiﬁcial intelligence base performance human evaluation also likely human cognitive bias. examples include machine translation information retrieval user modeling adaptation personalization communities. perils human evaluation warrant study areas. however communities favor results best match human performance outright success deﬁned ability replicate human performance. deﬁnition success would make research ﬁelds tolerant human cognitive bias machine learning interpretability community. implicit human cognitive bias problematic within interpretable machine learning directly limits ability provide purely descriptive explanations accurate transparent. bias becomes sufﬁciently large fail satisfy ethical goals related transparency underlying machine learning model. present research directions potential reduce implicit cognitive bias interpretable explanation strategies still prioritizing functional human interpretability trust. methods allow control tradeoff transparency interpretability. direction involves separating descriptive persuasive explanation generation tasks. case treat explanation complexity persuasive strategy attribute. ﬁrst step create fully descriptive explanation within interpretable feature space particular explanation vehicle. efforts simplify explanation made beyond projection given interpretable feature space. explanation altered become persuasive second ﬁnal step. incorporate human cognitive function user preferences expertise explanation. process already treated separate steps number interpretable explanation systems regard model complexity. reducing model complexity researchers choose fully descriptive model truncating reduce complexity. example using standard cart algorithm produce decision tree interpretable feature space depth model assumed complex human understanding remove nodes depth greater elicit altered explanation persuasive interpretable. separation concern encourages rapid innovation reduces cost evaluation. separated descriptive step evaluated using functional metrics. researchers developing methods project uninterpretable models interpretable feature space could themselves expensive delicate human evaluation tasks. persuasive step transformed task altering model speciﬁc explanation vehicle balance accuracy interpretability. researchers make progress evaluation explanations across different users applications without conﬂating choice explanation vehicle. second research direction consider extending objective function include cognitive attributes expertise inﬂuence functional measures interpretability. incomplete coverage cognitive features important model user trust measures functional interpretability explicitly included constraint. feature missing explanation strategy’s loss function contribute implicit cognitive bias. relevant cognitive features differ across applications evaluation metrics. knowledge representation user expertise expected important factor functional interpretability explanation. however collecting knowledge graph individual group expensive well incomplete multi-objective loss functions optimization multi-objective loss function becomes difﬁcult nontrivial proposes solution problem using dataset constraints ramp penalty. increased complexity optimization pose difﬁculty ﬁtting reasonable explanations. present user conviction example explicit attribute human cognition. this like cognitive attributes function users’ personal expertise model. represents fairly difﬁcult attribute differs individual users requires capture user knowledge. brey describes conﬂict user trusting one’s judgments intuitions trusting potentially intelligent technology. comment regard intelligence-enabled devices conﬂict comparable users evaluating trust machine learning explanations. paper deﬁne user conviction propensity individual group trust judgment classiﬁcation model interpretable explanation. point time treat user conviction cuser single parameter across entire user expectation model cuser cuser concept user conviction extended capture feature-level decision-level trust ﬁne-grained evaluation. presented research directions mitigate negative consequences implicit human cognitive bias. believe research studied better characterize risks presented human evaluation functional interpretability. project funded generous grants escience institute gordon betty moore foundation alfred sloan foundation university washington escience institute. thank valentina staneva zachary jones bill howe fruitful discussion. large thank goes michael esveldt katie kuhl university washington libraries writing editing support.", "year": 2017}