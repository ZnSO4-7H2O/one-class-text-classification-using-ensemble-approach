{"title": "The information bottleneck and geometric clustering", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "The information bottleneck (IB) approach to clustering takes a joint distribution $P\\!\\left(X,Y\\right)$ and maps the data $X$ to cluster labels $T$ which retain maximal information about $Y$ (Tishby et al., 1999). This objective results in an algorithm that clusters data points based upon the similarity of their conditional distributions $P\\!\\left(Y\\mid X\\right)$. This is in contrast to classic \"geometric clustering\" algorithms such as $k$-means and gaussian mixture models (GMMs) which take a set of observed data points $\\left\\{ \\mathbf{x}_{i}\\right\\}_{i=1:N}$ and cluster them based upon their geometric (typically Euclidean) distance from one another. Here, we show how to use the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017), a variant of IB, to perform geometric clustering, by choosing cluster labels that preserve information about data point location on a smoothed dataset. We also introduce a novel intuitive method to choose the number of clusters, via kinks in the information curve. We apply this approach to a variety of simple clustering problems, showing that DIB with our model selection procedure recovers the generative cluster labels. We also show that, for one simple case, DIB interpolates between the cluster boundaries of GMMs and $k$-means in the large data limit. Thus, our IB approach to clustering also provides an information-theoretic perspective on these classic algorithms.", "text": "information bottleneck approach clustering takes joint distribution maps data cluster labels retain maximal information objective results algorithm clusters data points based upon similarity conditional distributions contrast classic geometric clustering algorithms k-means gaussian mixture models take observed data points {xi}i=n cluster based upon geometric distance another. here show deterministic information bottleneck variant perform geometric clustering choosing cluster labels preserve information data point location smoothed dataset. also introduce novel intuitive method choose number clusters kinks information curve. apply approach variety simple clustering problems showing model selection procedure recovers generative cluster labels. also show that simple case interpolates cluster boundaries gmms k-means large data limit. thus approach clustering also provides information-theoretic perspective classic algorithms. unsupervised learning crucial component building intelligent systems since systems need able leverage experience improve performance even absence feedback. aspect discovering discrete structure data problem known clustering typical setup handed data points {xi}n asked return mapping data point label ﬁnite cluster labels basic approaches include k-means gaussian mixture models gmms cluster data based maximum likelihood ﬁtting probabilistic generative model. k-means either thought directly clustering data based geometric distances data points special case gmms assumptions evenly sampled symmetric equal variance components. information bottleneck information-theoretic approach clustering data optimizes cluster labels preserve information third target variable interest resulting clustering groups data points based similarity conditional distributions target variable divergence clustering problem fully speciﬁed joint distribution tradeoﬀ parameter quantifying relative preference fewer clusters informative ones. ﬁrst glance obvious approach cluster geometric data input data point locations {xi}n example target variable clusters retain information about? choose tradeoﬀ parameter still ﬁrst attempt geometric clustering claimed equivalence k-means. unfortunately much approach correct contained fundamental errors nullify main results. next section describe errors correct them. essentially approach properly translate geometric information form could used correctly information-theoretic algorithm. addition ﬁxing issue also choose recently introduced variant information bottleneck called deterministic information bottleneck make choice diﬀerent number clusters provided them. known clusters access thus clustering requires search number clusters well parsimony-informativeness tradeoﬀ parameter hand built-in preference using clusters thus requires parameter search moreover dib’s ability select number clusters given leads intuitive model selection heuristic based robustness clustering result across show recover generative number clusters many cases. next section formally deﬁne geometric clustering problem approach still approach. section show approach geometric clustering behaves intuitively able recover generative number clusters single free parameter section discuss relationship approach gmms k-means proving least simple case interpolates k-means cluster boundaries varying data smoothing scale approach thus provides novel information-theoretic approach geometric clustering well information-theoretic perspective classic clustering methods. geometric clustering problem given observed data points {xi}i=n asked provide weighting categorizes data points clusters data points near another cluster. deﬁnition near varies algorithm k-means example points cluster closer cluster mean cluster mean. information bottleneck problem given joint distribution asked provide mapping contains relevant information predicting goal embodied information-theoretic optimization problem subject markov constraint free parameter allows setting desired balance compression encouraged ﬁrst term relevance encouraged second; small values throw away favor succint representation large values retain nearly information approach squeezing information latent variable bottleneck might remind readers variational autoencoder indeed close relationship vaes. pointed alemi variational version essentially seen supervised generalization typically unsupervised algorithm. interested performing geometric clustering information bottleneck. purposes paper focus recent alternative formulation called deterministic information bottleneck dib’s cost function directly encourages clusters possible initialized nmax clusters typically converge solution fewer. thus form model selection built prove useful geometric hand tend nmax clustering clusters thus requires additional search parameter also diﬀers leads hard clustering instead soft clustering. seen replacing argmax exponential soft max. referred distributional clustering algorithm divergence term seen measuring similar data point conditional distribution cluster assigned cluster based upon similar conditional candidate point thus cluster data points based upon conditionals apply geometric clustering problem must choose geometric clustering dataset {xi}i=n appropriate dataset first since data clustered we’ll choose data point index target variable wish maintain information about seems reasonable choose data point location thus want cluster data indices cluster indices maintains much possible info location possible choose joint distribution ﬁrst glance might choose δxxi since data point observed location reason lies fact distributional clustering algorithm discussed paragraphs above. data points compared another conditionals choice delta function overlap unless data points another. choosing δxxi leads divergence either inﬁnite data points diﬀerent locations zero data points exactly another i.e. δxixj. trivially resulting clustering would assign data point cluster grouping data points identical. another relational information problem lies joint distribution wants perform geometric clustering approach geometric information must somehow injected joint distribution series delta functions that. previous attempt linking kmeans made mistake subsequent algebraic errors tantamount incorrectly introducting geometric information precisely geometric information appears k-means resulting algorithm describe errors detail appendix figure illustration data smoothing procedure. example dataset symmetric skew cluster. scatterplot data points smoothed probability distribution overlaid. bottom heat joint distribution dib. spatial dimensions binned concatenated single dimension bottom source striations. gaussian corresponds gaussian smoothing data. case obvious choice marginal number data points unless reason priori favor certain data points others. choices determine completely dataset figure contains illustration data smoothing procedure. explore eﬀect choice smoothing scale throughout paper. note solution contains free parameter. discussed above allows preference solutions fewer clusters retain spatial information. common literature algorithm multiple values plot collection solutions information plane relevance term y-axis compression term x-axis natural plane relevance term y-axis compression term x-axis curve drawn solutions information plane viewed pareto-optimal boundary much relevant information extracted given ﬁxed amount information representational capacity solutions lying curve course suboptimal priori formalism doesn’t tell select single solution family solutions lying boundary. intuively however faced boundary pareto-optimality must pick solution best choose knee curve. quantitatively knee curve point curve maximum magnitude second derivative. extreme case second derivative inﬁnite kink curve thus largest kinks might correspond solutions particular interest. case since slope curve given solution kinks indicate solutions valid wide range large kinks also correspond robust solutions sense optimize wide range tradeoﬀs. quantitatively measure size kink angle discontinuity causes slope curve; ﬁgure details. show next section searches solutions large result recovering generative cluster labels geometric data including correct number clusters. note model selection procedure would possible chosen instead dib. uses clusters available regardless choice thus solutions curve would number clusters anyway knees kinks cannot used select number clusters. figure kinks information curve model selection. βmin βmax smallest largest solution kink valid. thus arctan slopes upper lower dotted lines. kink angle arctan. measure robust solution choice thus high values indicate solutions particular interest. described four geometric clustering datasets varying smoothing width tradeoﬀ parameter measured solution number clusters used fraction spatial information extracted well kink angle iterated equations strouse schwab diﬀerence. iterating greedily initialization lead local minima help overcome suboptimal solutions upon convergence checked whether merging clusters would improve value cost functional chose merging highest reduction began iterative equations again. repeated procedure algorithm converged merging reduced value found non-local steps worked well combination greedy local improvements iterative equations. essential function improvement performance produced cleaner information curves less noise caused convergence local minima. results shown ﬁgure large represents diﬀerent dataset. left column shows fractional spatial information versus number clusters used stacked smoothing width center column shows kink angle cluster number stacked smoothing width finally right column shows example solutions. general note increase move right along plots left column towards higher number clusters spatial information values present varying implicit parameter note upper bound data processing inequality indeed fraction potential geometric information extracted smoothed note information plane curve ﬁgure y-axes figure results model selection clustering dib. results four datasets. represents diﬀerent dataset. left column fraction spatial information extracted versus number clusters used across variety smoothing scales center column kink angle curve) versus number clusters used across variety smoothing scales right column example resulting clusters. necessarily choose possible cluster numbers. example small smoothing width points won’t enough overlap neighbors support solutions clusters large smoothing width local spatial information thrown solutions clusters possible. interestingly retain drop solutions based well match structure data discuss dataset below. additionally solutions match well structure data tend especially robust large kink angle thus used perform model selection. datasets structure multiple scales kink angle select diﬀerent solutions diﬀerent values smoothing width allows investigate structure dataset particular scale choosing. turn individual datasets. ﬁrst dataset consists equally spaced equally sampled symmetric gaussian clusters -cluster solution stands several ways. first robust spatial scale second -cluster solution extract nearly available spatial information; solutions extract little extra third perhaps salient -cluster solution largest value kink angle across wide range smoothing scales. right column show examples -cluster solutions. note -cluster solutions look exactly like -cluster solutions vary chop true cluster two. second dataset consists equally sampled symmetric gaussian clusters time equally spaced; much closer another third. dataset multiple scales present thus expect number clusters picked model selection procedure e.g. kink angle depend spatial scale interest. indeed true. -cluster solution present smoothing widths shown selected best solution kink angle intermediate smoothing widths large smoothing widths -cluster solution chosen best. smoothing widths -cluster solutions roughly equally valid. terms spatial information -cluster solutions also prominent transitions providing signiﬁcant improvement third dataset features even multi-scale structure symmetric equally sampled gaussians unequal spacing. sensible solutions exist seen gradual rise fractional spatial information regime. also transition model selection -cluster solution small smoothing widths -cluster solution larger smoothing widths intermediate favoring intermediate solutions. example clusters shown right. finally wanted ensure model selection procedure would halluscinate structure none applied single gaussian blob hope solution would stand prove robust seen fourth ﬁgure indeed true. solution smoothing width particuarly high kink angle solution remained knee versus curve across wide range smoothing widths. show limit inﬁnite data small smoothing scale behavior intimately related hard cluster boundaries implied gmms. assume gaussian cluster centered covariance diag second gaussian cluster centered covariance diag. mixture model weights hard maximum likelihood boundary clusters plane given notice small smoothing width either evenly sampled clusters identical hard boundary implied gmm. small smoothing width that compared encourages capturing information spatial location expense using clusters equally. reduced pulling closer another eﬀect cluster prior term eﬀect larger smoothing widths compared numerically calculated k-means cluster boundaries true assignments range smoothing widths data consisted points sampled equally isotropic skew gaussian shown. small smoothing widths boundary indeed approaches gmm. larger smoothing widths eﬀect shape clusters muted boundary approaches k-means. note particular example however need approach k-means large limit general. figure cluster boundaries diﬀerent algorithms. colored lines show boundaries separating clusters diﬀerent algorithms k-means gmms diﬀerent levels smoothing. dataset points drawn equally single symmetric gaussian single skew gaussian. black points show data. here shown formalism information bottleneck perform geometric clustering. previous paper claimed contribute similarly however reasons discussed sections approach contained fundamental ﬂaws. amend improve upon paper four ways. first show errors made problem setup second argue using setting preference using clusters can. third introduce novel form model selection number clusters based discontinuities slope curve indicate solutions robust across tradeoﬀ parameter show information-based model selection criterion allows correctly recover generative structure data multiple spatial scales. finally compare resulting clustering algorithm k-means gaussian mixture models found large smoothing width performance method seems behave similarly k-means. interestingly found small smoothing width method behaves generalization tunable tradeoﬀ compression ﬁdelity representation. introduced geometric clustering information bottleneck think opens avenues ways well. first uniform smoothing perform could generalized number ways better exploit local geometry better estimate true generative distribution data. example could gaussian smoothing mean centered data point covariance estimated sample covariance neighboring data points around mean. indeed early experiments alternative suggest useful certain datasets. second choosing spatial location relevant variable preserve information seems obvious ﬁrst choice investigate options might prove interesting. example preserving information identity neighbors carefully formulated might make fewer implicit assumptions shape generative distribution enable extension approach wider range datasets. scaling approach introduced higher-dimensional datasets non-trivial tabular representation used original algorithms leads exponential scaling number dimensions. recently however alemi introduced variational version parameterizes encoder function approximator e.g. deep neural network. advantage allowing scaling much larger datasets. moreover choice parameterization often implies smoothness constraint data relieving problem encountered needing smooth data. would interesting develop variational version could used perform information-theoretic clustering done here larger problems perhaps need data smoothing. previous attempt made draw connection k-means even reviewing algebraic errors lead result break down intuitive reasons claim unlikely true. first soft clustering algorithm k-means hard clustering algorithm. second authors made choice smooth data δxxi. discussed section clusters data points based conditionals delta functions trivially overlap identical. primary algebraic mistake appears claim might wonder mistakes authors still obtain algorithm looks performs like k-means. reason sequence mistakes leads result eﬀectively assumes access geometric information namely cluster centers step since exactly k-means uses assign points clusters surprising behavior resembles k-means.", "year": 2017}