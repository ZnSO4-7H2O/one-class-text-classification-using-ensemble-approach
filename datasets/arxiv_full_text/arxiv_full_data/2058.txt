{"title": "Learning Determinantal Point Processes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Determinantal point processes (DPPs), which arise in random matrix theory and quantum physics, are natural models for subset selection problems where diversity is preferred. Among many remarkable properties, DPPs offer tractable algorithms for exact inference, including computing marginal probabilities and sampling; however, an important open question has been how to learn a DPP from labeled training data. In this paper we propose a natural feature-based parameterization of conditional DPPs, and show how it leads to a convex and efficient learning formulation. We analyze the relationship between our model and binary Markov random fields with repulsive potentials, which are qualitatively similar but computationally intractable. Finally, we apply our approach to the task of extractive summarization, where the goal is to choose a small subset of sentences conveying the most important information from a set of documents. In this task there is a fundamental tradeoff between sentences that are highly relevant to the collection as a whole, and sentences that are diverse and not repetitive. Our parameterization allows us to naturally balance these two characteristics. We evaluate our system on data from the DUC 2003/04 multi-document summarization task, achieving state-of-the-art results.", "text": "determinantal arise random matrix theory quantum physics natural models subset selection problems diversity preferred. among many remarkable properties dpps oﬀer tractable algorithms exact including computing marginal probabilities sampling; however important open question learn labeled training paper propose natural data. feature-based parameterization conditional dpps show leads convex eﬃcient learning formulation. analyze relationship model binary markov random ﬁelds repulsive potentials qualitatively similar computationally intractable. finally apply approach task extractive summarization goal choose small subset sentences conveying important information documents. task fundamental tradeoﬀ sentences highly relevant collection whole sentences diverse repetitive. parameterization allows naturally balance characteristics. evaluate system data multidocument summarization task achieving state-of-the-art results. extractive multi-document summarization system chooses among sentences collection documents subset summarizes important information. selection problem balancing hand selected sentence relevant sharing signiﬁcant information collection whole; other selected sentences diverse group summary repetitive informative possible given length. modeling learning problems challenging diversity requirement implies strong repulsive interactions variables. standard models like markov random ﬁelds often involve notoriously intractable inference problems negative correlations present. determinantal point processes arise quantum physics studied extensively probabilists probabilistic models give likelihood selecting subset items determinant kernel matrix. viewed joint distributions binary variables corresponding item selection dpps capture negative correlations. makes signiﬁcantly less general markov random ﬁelds. however natural problems described above; furthermore unlike markov random ﬁelds negative interactions dpps oﬀer remarkably tractable inference computing marginals computing certain conditional probabilities sampling done exactly polynomial time. kulesza taskar recently used dpps select multiple poses people image however question learning dpps data addressed. work propose conditional version show parameterized allow convex eﬃcient maximum-likelihood learning. computation feature expectations important step oﬀ-diagonal elements determine correlations pairs elements large values imply tend co-occur. might therefore used model diverse sets items example sentences summary. note dpps canrepresent distributions elements likely co-occur independent correlations always negative. figure shows diﬀerence sampling points plane using leads widely spread good coverage sampling points independently points exhibit random clumping. determinantal point processes introduced model fermions also arise studies non-intersecting random paths random spanning trees eigenvalues random matrices training show done exactly eﬃciently. also describe algorithm approximating test time maximum posteriori prediction. demonstrate utility model extractive multi-document summarization task. paper proceeds follows. section deﬁne dpps give intuition behind unique characteristics show compute useful quantities. section compare distributions realizable dpps mrfs give sense relative expressiveness. section describe conditional dpps suitable feature-based parameterization learning. section derive convex maximum-likelihood estimation approach learning parameters model discuss inference section section show state-of-the-art performance trained model extractive summarization data. determinantal processes point process discrete probability measure subsets called determinantal point process random drawn according have every tion entries indexed elements adopt refer marginal kernel contains information needed compute probability subset being included simple observations follow equation parallelepiped spanned vectors representing items figure illustrates cardinalities increase quality item easy probability containing also increase. hand become similar another probability containing pair decrease. thus model naturally balances objectives high quality high diversity. addition computing marginals normalizing constant surprising number inference operations also efﬁcient despite fact modeling exponential number possible subsets given tractability dpps natural question representational power dpps compares mrfs repulsive potentials generally intractable. indicator variable selected otherwise. closely resembling items pairwise log-linear model negative interaction terms parameters also capture negative correlations variables representational diﬀerence that individually constrained less positive semi-deﬁnite constraint kernel global. particular similarities must satisfy kind transitivity consequence triangle inequality figure geometric view dpps vector corresponds item. probability selected square volume spanned included item vectors. quality item increases probabilities sets containing item. similarity items increases probabilities sets containing decrease. gives rise marginal probabilities l-ensembles directly model probabilities atomic events oﬀering convenient representation optimization. furthermore need positive semideﬁnite eigenvalues bounded above. reasons focus modeling eﬀorts dpps represented l-ensembles. decomposition serves purposes. first implicitly enforces constraint must positive semideﬁnite simpliﬁes learning. second allows independently model quality similarity combine uniﬁed model. particular have entry large negative correlations weak models give rise qualitatively similar distributions. value entry shrinks zero however surfaces become quite diﬀerent. mrfs example describe distribution ﬁrst item strongly anti-correlated others second third anticorrelated other. transitive nature makes impossible. third panel shows overlaid entry even relatively strong interactions models fairly close. conjecture story larger follows similar pattern. purposes modeling real data discriminative learning propose parameterized conditional dpp. concreteness consider extractive document summarization. setting input cluster documents given topic diﬀerences become apparent. setting parameters place models equal footing represent factor graph unnormalized node potentials single unnormalized ternary factor node potentials ewiyi figure shows values ternary factors conﬁgurations. last four entries determined respectively three edge parameters three parameters sets realizable ternary factors form manifolds. attempt visualize manifolds showing slices various values last entry figure improve visibility compute expected feature counts know marginal probability inclusion sentence recall dpps probabilities given diagonal marginal kernel computed eigendecomposition kernel thus eﬃciently compute gradient described algorithm following previous work experiment data document understanding conference multi-document summarization task task generate event-focused summaries document clusters nist collection. cluster contains approximately articles york times newswires covers single topic short time span. clusters mean length approximately sentences words. task training contains clusters task test contains clusters. summaries required bytes length including spaces; forms budget constraint. cluster comes four reference human summaries evaluation purposes. figure depicts sample cluster test set. measure performance rouge automatic evaluation metric summarization rouge measures n-gram overlap statistics human references summary scored combines produce various sub-metrics. rouge- simple unigram recall measure shown correlate quite well human judgments therefore follow previous work rouge’s unigram f-measure primary metric. refer measure rouge-f. also report rouge-p rouge-r well rouge-f rouge-suf include bigram match statistics also shown correlate well human judgments. implementation uses rouge version stemming turned without stopword removal. settings correspond used actual competitions human reference summaries provided data high-quality train model need target extractive summaries. obtain highquality extractive oracle summaries training employ simple greedy algorithm. intuitively idea choose round sentence achieves maximal mean unigram f-measure human references summary. since high fmeasure requires high precision well recall update references removing words covered newly selected sentence proceed next round. input option sample conditional distribution done exactly dpps cubic time however found obtained better performance maximum posteriori conditioned budget constraint cost measure cost item limit total cost. computing exactly np-hard consider approximations. first obtain brute-force estimate sampling large number sets choosing sample highest probability among satisfying budget constraint. second practically note optimization equation submodular approximate simple greedy algorithm. algorithm closely related given bilmes krause guestrin monotone submodular problems algorithms type formal approximation guarantees. problem generally monotone; nonetheless algorithm seems work well practice. though preprocessing also used improve candidate given predicted sentences follow previous work construct ﬁnal summary placing sentences order appeared original documents data achieve rouge scores shown table compare scores oracle extractive summaries best automatic system competition well actual human references. note that third column human summary evaluated also four references used compute rouge; hence scores probably signiﬁcantly higher could achieved practice. furthermore shown extractive summaries even generated optimally nature limited quality compared unconstrained summaries thus believe oracle summaries excellent targets training. similarity features standard normalized tf-idf vectors scores computed across documents clusters training set. thus similarity sentences cosine distance. stop word removal stemming. augment tfidf vectors additional constant feature taking following quality features features make cosine distances; computed using tf-idf vectors similarity features. feature intrinsically realvalued produce series binary features binning. boundaries determined either globally locally. global bins evenly spaced quantiles feature values across sentences training local bins quantiles feature values current cluster. below indicate types bins used case. sentence original document generate binary features indicating positions plus sixth binary feature indicating positions. expect that newswire text sentences appear earlier article likely useful summarization. scores ﬁnding principal eigenvector row-normalized cosine similarity matrix. details.) provides alternative measure centrality. score global bins local bins. train model standard l-bfgs optimization algorithm. place zero-mean gaussian prior parameters variance optimize rouge-f development subset data. learn parameters corpus test using data. test model inference algorithms described section ﬁrst bruteforce estimate optimal summary obtained taking samples model choosing highest-probability sample length characters. process computationally expensive provides point comparison much eﬃcient greedy procedure. test additional baselines rely quality scores trained logistic regression sentence unique instance classiﬁed included included using labels training oracle features described above. baseline lr+mmr quality scores used relevance portion well-known maximum marginal relevance algorithm sentences chosen iteratively according sentences already selected learned quality score cosine similarity sentences tradeoﬀ optimized development sentences added budget full. baseline lr+dpp logistic regression quality scores sophisticated baselines compare three highest-scoring systems peers well submodular graph-based approach recently described bilmes refer submod. code available latter system implemented ourselves striving replicate results. version fails reproduce exact numbers given prior work; however suspect diﬀerences text processing pipeline including detagging sentence splitting computation tf-idf scores. model relies upon tf-idf scores submod change preprocessing likely impact performance systems. hyperparameters submod values reported showed determinantal point processes applied subset selection tasks like extractive summarization negative interactions traditional models like markov random ﬁelds computationally problematic. dpps give rise intuitive tradeoﬀ high-quality items like sentences concisely represent text summarized diverse items like sentences reduce redundancy increase coverage. showed parameterize model allow convex training eﬃciently maximizing likelihood training despite exponential number possible outputs. also showed perform eﬃcient approximate test-time inference. experiments multi-document summarization task demonstrate state-of-the-art performance. believe learning dpps many applications beyond summarization. example diversity important consideration many search applications would like return results relevant user also cover multiple possible interpretations ambiguous queries. search apple example might request information fruit computers record companies etc.; don’t know user after might preferable return results topic rather future work summarization dpps includes combining extractive approach internal sentence compression achieve informative summaries approach requires treating sentence structure rather simple binary in-or-out decision. recent work shown dpps extended eﬃciently deal structures many cases generally important open question regarding dpps whether might possible also parameterize similarity matrix permits efﬁcient learning. would enable wider variety expressive models analogous advantages oﬀered kernel matrix learning svms. additionally illustrated relationship mrfs dpps extending observations larger might lead improved inference algorithms subclass dpp-like mrfs; e.g. using variational methods.", "year": 2012}