{"title": "Online Reinforcement Learning for Real-Time Exploration in Continuous  State and Action Markov Decision Processes", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper presents a new method to learn online policies in continuous state, continuous action, model-free Markov decision processes, with two properties that are crucial for practical applications. First, the policies are implementable with a very low computational cost: once the policy is computed, the action corresponding to a given state is obtained in logarithmic time with respect to the number of samples used. Second, our method is versatile: it does not rely on any a priori knowledge of the structure of optimal policies. We build upon the Fitted Q-iteration algorithm which represents the $Q$-value as the average of several regression trees. Our algorithm, the Fitted Policy Forest algorithm (FPF), computes a regression forest representing the Q-value and transforms it into a single tree representing the policy, while keeping control on the size of the policy using resampling and leaf merging. We introduce an adaptation of Multi-Resolution Exploration (MRE) which is particularly suited to FPF. We assess the performance of FPF on three classical benchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double Integrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other algorithms, although these algorithms rely on the use of particular representations of the policies, especially chosen in order to fit each of the three problems. Finally, we exhibit that the combination of FPF and MRE allows to find nearly optimal solutions in problems where $\\epsilon$-greedy approaches would fail.", "text": "generally control physical systems naturally leads models continous-action spaces since typically controls position acceleration object torque sent joint. policy gradients methods used successfully learn highly dynamical tasks hitting baseball anthropomorphic algorithms suited learning low-cost robots need provide motor primitive able estimate gradient reward respect motor primitive parameters. model-based control difﬁcult apply robots hand-tuned open-loop behaviors proven effective therefore model-free learning csa-mdp appears promising approach learn behaviors. paper presents method learn online policies continuous state continuous action model-free markov decision processes properties crucial practical applications. first policies implementable computational cost policy computed action corresponding given state obtained logarithmic time respect number samples used. second method versatile rely priori knowledge structure optimal policies. build upon fitted q-iteration algorithm represents q-value average several regression trees. algorithm fitted policy forest algorithm computes regression forest representing q-value transforms single tree representing policy keeping control size policy using resampling leaf merging. introduce adaptation multi-resolution exploration particularly suited fpf. assess performance three classical benchmarks reinforcement learning inverted pendulum double integrator hill show equals outperforms algorithms although algorithms rely particular representations policies especially chosen order three problems. finally exhibit combination allows nearly optimal solutions problems \u0001-greedy approaches would fail. initial motivation research presented paper optimization closed-loop control humanoid robots autonomously playing soccer annual robocup competition speciﬁcally target learn behaviors grosban robot presented figure requires computation policies markov decision processes state space continous action space continous transition function known. additionally order provide real-time closed-loop control policy allow retrieve nearly optimal-action computational-cost. consider transition function known small low-cost humanoid tree following idea extremely randomized trees introduce randomness split thus allowing grow forest order increase smoothness knownness function. moreover changing update rule q-value reduce attracting power local maxima. viability demonstrated performance comparison results proposed three classical benchmark inverted pendulum stabilization double integrator hill. experimental results show drastically reduce computation time improving performance. illustrate gain obtained using version inverted pendulum stabilization problem ﬁnally present results obtained inverted pendulum swing-up using underactuated angular joint. last experiment using gazebo simulator place analytical model. paper organized follows section introduces notations used markov decision processes regression forests section presents original version fitted q-iteration classical methods batch mode continuous action space section proposes algorithms extract informations regression forest section introduces core algorithm. section presents exploration algorithm used. efﬁciency demonstrated series experiments classical benchmarks section meaning experimental results discussed section markov-decision process markov-decision process short -tuple states actions reward function denotes expected reward taking action state transition function denotes probability reaching using denotes action choice state thereafter policy implicitely refer deterministic policy. qvalue couple policy horizon denoted deﬁned expected cumulative discounted reward applying state choosing actions according abreviate short. greedy policy respect denoted always selects action highest q-value; i.e. argmax considering action space bounded interval limit exists although necessarily unique. respect πq∗. ploitation collected samples required sufﬁcient. smart exploration necessary problems nearly-optimal strategies requires succession actions unlikely occur using uniformous random actions. extreme cases might even lead situation reward ever seen probability reaching state carrying reward following random policy almost problem known combinatory lock problem appears discrete case continuous problems control problems action discrete large already existing efﬁcient algorithms tackle problem producing efﬁcient policy result previous experiments. course algorithms used continous action space case discretization action sets. however naive approach often leads computational costs high practical applications stated speciﬁcity continuous action space also adressed speciﬁc methods particularly encouraging empirical results obtained thanks example binary action search approach also methods require design functional basis used represent qvalue function prefer avoid order obtain versatile algorithms. recent major-breakthrough ﬁeld solving csamdp symbolic dynamic programming allows exact solutions using extended algebraic decision diagrams also however algorithms requires model rely several assumptions concerning shape transition function reward function. additionally methods suited close horizon therefore suited application. local planning allows achieve outstanding control high-dimensionnal problems humanoid locomotion computational cost online planning burden real-time application. particularly relevant robotics processing units light small order embedded. therefore global planning policy computed ofﬂine loaded robot. learning algorithms based fitted iteration algorithm represents q-value average several regression trees. ﬁrst present method allowing extract approximately optimal continuous action q-value forest. introduce algorithm fitted policy forest learn approximation policy function using regression forests. representation policy allows retrieve nearly optimal action computational cost therefore allowing embedded systems. kd-trees kd-trees data structure allows store points size providing access leaf tree several points non-leaf node orthogonal split. space kdtree deﬁned every exist single path root kd-tree leaf would leaf denoted leaf deﬁned space leaf contains points noted points concerns hyperrectangle space. procedure yields satisfying results action space discrete computational complexity part equation using regression forest makes become quickly inefﬁcient. therefore action spaces always discretized compute equation thus leading inappropriate action optimal control requires discretization. lagoudakis proposes generical approach allowing avoid computation part equation results presented show binary action search strongly outperforms method ﬁnite number actions problems rewards including cost depending square action used inverted pendulum stabilization double integrator. hand binary action search yields unsatisfying results hill problem optimal strategy known bang-bang part propose methods extract information regression forest choosing trade-off accuracy computational cost. first introduce algorithm grow regression forest. present algorithm project regression tree given subspace. finally propose method allowing average whole regression forest single regression tree whose number leaf bounded. given complete ﬁnite standard algorithms exists ﬁnding optimal policy including value iteration policy iteration linear programming. however transition function reward function unknown necessary samples learn approximation solving ofﬂine direct access transition function necessary gathered samples. samples deﬁned -tuples form starting state action used reward received successor state. regression forests regression tree representation approximation function decision tree structure every non-leaf node function mapping children every leaf basic function simple regression tree piecewise constant approximation presented figure several algorithms exist extract regression trees training complete introduction refer predicting output entry requires leaf corresponding compute basic function found leaf corresponding refer value predicted tree input short. algorithms uses oblique split algorithms presented valid orthogonal splits note lower upper children node concerning respectively. deﬁne space hyperrectangle leaf concern different part refer minimun maximum value along dimension respectively. deﬁne size hyperrectangle him. abusive notation norm place him. regression forest regression trees tm}. exhibited using multipe trees represent function leads accurate prediction. value predicted forest extra-trees several methods exists build regression forests training samples implementation based extratrees algorithm produces satisfying approximation moderate computational cost. main characteristic extra-trees split dimensions chosen randomly chosen split dimension position split picked randomly uniformous distribution minimal maximal value dimension along samples split. finally best random splits used; criteria used rank splits variance gain brought split. original training splitted terminal condition reached. ﬁrst terminal condition number samples remaining smaller nmin nmin parameter allowing control overﬁtting. terminal conditions inputs samples identical output value constant. provide improvements extra-trees order remedy problems. first terminal conditions large trees grown parts space q-value almost constant q-value strictly constant terminal condition number samples lower nmin. remedy problem help parameter vmin speciﬁes minimal variance prediction measure necessary allow splitting. naive implementation extra-trees leads second problem generate nodes samples paves overﬁtting linear interpolation. therefore changed choice split values. instead choosing uniformly minimum maximum samples algorithm choose uniformly nmin-th smallest highest values guarantees node split tree contains least nmin samples. projection regression tree consider tree deﬁne projection tree state another tree since known contain split depending value therefore contains splits related action space. easy create hyperrectangle corresponding state simple scheme computing would root replicate leaf however would lead overgrown tree containing various unreachable nodes. example split predicate could perfectly appear lower child another node whose predicate therefore designed algorithm merges trees walking simultaneously trees form root leaves performing on-the-ﬂy optimizations. algorithm pseudo-code shown algorithm example input output algorithm shown figure also tend keep original aspect regression tree top-most nodes carry important splits pruning trees although merging procedure helps reduce size ﬁnal trees combination trees might still lead tree size therefore developed pruning algorithm aims removing split nodes bring smallest change prediction function. nodes algorithms allowed remove nodes parent leafs. deﬁne loss prediction function node concerning hyperrectangle prediction function given equation weighted average prediction functions children weighted size space concerned one. choice reduces impact prediction leaf merged bigger leaf. deﬁnition loss equation also considers size spaces since compute integral. main interest method reduce average error whole tree weighting cost error size space. prunning procedures litterature centered reducing risk overﬁtting algorithm cares reducing size tree ensuring complexity representation given threshold. since procedure based training used grow forest necessary access training order prune tree. merging trees forest crucial prune tree resulting merge applying another merge. section propose three methods used choose optimal action given state based estimation q-value regression forest. learning policy computationally demanding since performed ofﬂine crucial obtain descriptions policies allow quick computation action given current state. q-value regression forest need solve following equationπ∗ argmax given straightforward compute denotedt imposing limit number leafs usactions simply iterating leafs computing maximum function leaf interval. solution provide exact policy would induced provides roughly good approximation. refer method fitted q-iteration short. computationally expensive used online situation computation single action requires exploring potentially large number leaves. therefore order provide quick access optimal action given state propose scheme. decomposing policy function several functions dimension action space easily generate samples train regression forests provide estimates policy dimension. named process fitted policy forest abreviate fpf. variants using piecewise constant model nodes short another using piecewise linear model nodes short. refer methods fpfpwc fpfpwl respectively. policies resulting algorithm provides quick access. policy composed trees maximal number nodes complexity getting action since values used need high provide good approximation complexity makes perfectly suited real-time applications online computational ressources limited robotics. provide strong basis build exploration algorithm found performance strongly improved bringing three modiﬁcations. first change equation used compute knownness second bagging technic improve estimation knownness third modify rule used q-value update. original deﬁnition multi resolution exploration propose generic algorithm allowing balance exploration exploitation samples. main idea build function estimate degree knowledge couple execution algorithm action taken state point inserted kd-tree called knownness-tree. then knownness value according knownness-tree point computed using following equation maximal number points leaf number points inside whole tree points) space). crucial point equation fact knownness value depends three main aspects size cell number points inside cell number points inside whole tree. therefore ratio number points contained cell size evolve knownness value decrease. insertion points inside kd-tree follows rule adding point corresponding leaf would lead number points greater leaf splitted leafs size dimension chosen using round-robin. points stored attributed depending value. rmax maximal reward awarded single step result obtained equation update seen adding transition ﬁctive state containing self-loop leading maximal reward every step. transition occurs probability computation knownness value initial deﬁnition knownness given equation since deﬁnition depend biggest dimension following. consider leaf knownness adding point result creating leafs respective knowledge leads unnatural fact adding point middle points decrease knowledge points. total number points inside tree. deﬁnition leads fact anytime least leaf knownness equal also easy least leaf knownness strictly lower except cells density. knownness tree knownness forest order increase smoothness knownness function decided aggregate several kd-trees grow forest following core idea extra-trees however order grow different kd-trees input splitting process needs stochastic. therefore implemented another splitting scheme based extra-trees. splitting process follows every dimension choose uniformous random split ﬁrst sample last sample. thus ensure every leaf contains least point. heuristic choose best split. modiﬁcation q-value update q-value update rule proposed improve search speed however major drawback. since alters training used grow regression forest knownness information state action combination tried. therefore even state action might inﬂuence all. order solve issue decided avoid modiﬁcation training creation thus using equation place modifying samples simply update regression forest applying following modiﬁcator every leaf every tree present experimental results different learning setup. first results obtained batch reinforcement learning second performances obtained combining online learning. batch reinforcement learning used three benchmark problems classical evaluate perfomances algorithms. methods share parameters computing qvalue forest tuned speciﬁcally parameters concerning approximation policy using q-value forest. compared results presented however access numerical data rely graphical representation datas. thus graphical lines shown approximative drawn thicker highlight noise measurement. present result separately three benchmarks discussing results speciﬁc problem well global results. problems performances fpfpwl better least equivalent achieved remarkable uses basic functions speciﬁcally chosen problem method generic problems. computation cost retrieving actions policy calculated appears negligeable therefore conﬁrms approach perfectly suited high-frequency control embedded systems. inverted pendulum stabilization inverted pendulum stabilization problem consists balancing pendulum unknown length mass applying force cart attached description problem given state space composed angular position pendulum angular speed pendulum action space newtons uniform noise newtons added. goal keep pendulum perpendicular ground reward formulated following constant gravity mass pendulum mass cart length pendulum ﬁnal action applied. used control step integration step reward used description problem ensure policies leading smoothness motion training sets obtained simulating episodes using random policy maximal number steps episode performances policies evaluated testing episodes maximal length computing cumulative reward. order provide accurate estimate performance algorithms computed different policies point displayed figure average cumulative reward parameters used produce policies shown table learning policy q-value tree clearly outperform direct problem approximations outperform approximations. results rank systematically lower methods. huge difference learning speed between suggests using regression forest learn policy q-value lead drastical improvements. problem optimal policy requires choice action surprising using linear models represent policy provide higher results constant models. best value nmin minimal number samples leaf pretty high understanding phenomena q-value tree tend slightly overﬁt data additionally uses approximation. therefore using directly lead important quantization noise. using large value nmin might seen applying smoothing considered necessary regression trees sampling stochastic function according need large number samples increased fpfpwl because providing accurate linear interpolation noisy application requires samples. double integrator order provide meaningful comparison stick description problem given control step increased original version presented double integrator linear dynamics system controller reduce negative quadratic costs. continuous state space consist position velocity car. goal bring equilibrium state controlling acceleration car. constraints case constraint violated penalty received experiment ends. case cost state control step used integration step discount factor training sets obtained simulating episodes using random policy maximal number steps episode performances policies evaluated testing episodes maximal length computing cumulative reward. order provide accurate estimate performance algorithms computed different policies point displayed figure average results. parameters used learning policy shown table problem although none proposed methods reach performance learning episodes fpfpwl learns quicker small number episodes. important note basic function approximator constant polynome used least-square policy iteration ﬁtting fact optimal policy known linear-quadratic regulator problem underactuated must reach hill. state space composed position speed action space acceleration violate constraints receives negative reward reaches state without breaking constraint receive reward states reward need move away target ﬁrst order momentum. well known solution problem bangbang strategy i.e. nearly optimal strategy exists uses actions stated problem worst case reinforcement learning continuous action space since requires learn binary strategy composed actions sampled frequently. shown introducing actions usually reduce performance controller. therefore hope reach performance comparable achieved binary choice. benchmark aimed assess performance algorithms worst case. sample previous algorithms based episodes generated starting point samples used hill problem generate sampling uniformly state action spaces. procedure used highly improbable random policy could manage positive reward problem. evaluation performed observing repartition number steps required reach hill initial state show histogram number steps required method figure method different strategies computed tested. signiﬁcant difference number steps required reach hill different methods. method least computed policies number step interval thus consider controller take steps average mentioned controller requires steps average. hundred experiments gathered across three different methods maximal number steps measured therefore consider results strongly outperforms results. experienced signiﬁcant difference fqi. since main advantage approach reduce quantization noise method result logical. although number steps required reduced approach online cost still reduced around orders magnitude. therefore afﬁrm highly preferable problem. computational cost mentioned previously quick access optimal action given state crucial realtime applications. present average time spent retrieve actions different methods figure average time spent learning policies experiments runned using opteron processor running running debian computer running experiments processors experiment used single core. using reduces average time orders magnitude. moreover fpfpwl presents lower online cost fpfpwc perfectly logical since representing model using linear approximation instead constant approximations requires less nodes. results displayed double integrator problem lack space similar results observed problems. inverted pendulum stabilization problem exactly deﬁned section used context online reinforcement learning. result presented section represent trials episodes. trial used generate different policies every policy evaluated episodes steps. thus results concerns total evaluations episodes. repartion reward presented figure reward obtained best worst policy shown thin vertical lines average reward represented thick vertical line. thus easy huge best worst policy. episodes average reward minimum maximal reward batch mode settings number episodes fpf-pwl obtained average reward minimal reward maximal reward average reward signiﬁcantly improve dispersion reward largely increased cases thus leading better also worst policy. might perceived weakness generating several policies computed q-value computationally cheap. then episodes might used select best policy. density reward presented figure obvious removing worst policies average reward would greatly improve. another point keep mind fact parameters optimized problem setup hand-tuned batch setup. therefore reaching comparable performance withparameter tuning already improvement. inverted pendulum swing-up problem instead using mathematical model decided simulator gazebo control using ros. since tools widely accepted robotic community believe exhibiting reinforcement learning experiments based contribute democratization methods robotics. developed simple model composed support pendulum bounded represents entire episode simulation thus contains action access simulation steps. therefore safe assume average time needed retrieve action fpfpwc fpfpwl inferior even used orders magnitude slower used experiment still possible include action access additional ofﬂine cost computing polices required lower cost computing qvalue using number training episode grows presented figure therefore possible also possible without increasing much ofﬂine cost. evaluated performance combination different problems. first present experimental results inverted pendulum stabilization problem compare results obtained random exploration. second exhibit results inverted pendulum swing-up problem. since online learning robots expensive time resources allow early phase parameter tuning used simple rules parameters problems. problems policy updated episode order ensure system controlled real-time. section denote trial whole execution algorithm problem. angular joint. angular joint controled torque underactuated i.e. available torque sufﬁcient maintain pendulum horizontal state. main parameters following mass pendulum length pendulum damping coefﬁcient friction coefﬁcient maximal torque τmax maximal angular speed ˙θmax control frequency reward function used following system involves state dimensions action dimension presents main difﬁculties ﬁrst random exploration unlikely produce samples target second requires whole scale action large actions order inject energy system action order stabilize system. result presented section represent trials episodes. trial used generate different policies every policy evaluated episodes steps. thus total evaluation episodes. present repartition reward figure average reward represented thick vertical line best worst policies rewards shown thin vertical lines. again notice large difference best worst policy. exhibit trajectory best worst evaluation episode figure worst episode cumulated reward worst policy average reward according repartition reward expect policies lead unsatisfying results thus ensuring reliability learning process multiple policies generated gathered samples episodes used discard worst policy. results show using allow drastically reduce online computational cost also tend outperforms especially transition function stochastic inverted pendulum stabilization problem. although using piecewise linear function represent q-value often leads divergence mentioned problem appear three presented problems. three presented benchmarks fpfpwl yields signiﬁcantly better results fpfpwc last problem results similar method. possibility using approximations representation policy holds fact approximation process performed once. another advantage fact experiments combination showed obtain satisfying results without parameter-tuning phase. results also show strong variability generated policies thus leading natural strategy generating multiple policies selecting best validation phase. article introduces fitted policy forest algorithm extracting policy regression forest representing qvalue. presents several advantages extremely computational cost access optimal action require expert knowledge problem particularly successful solving problems requiring actions stochastic problems used algorithm producing regression forests. effectiveness algorithm batch setup demonstrated three different benchmarks. online reinforcement learning also discussed assessed using exploration strategy. experimental results suggest exploration lead satisfying results without requiring tuning parameters. future also would like apply approach closed-loop control robocup humanoid robots.", "year": 2016}