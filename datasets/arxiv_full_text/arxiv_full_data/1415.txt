{"title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain  Surgeon", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.", "text": "develop slim accurate deep neural networks become crucial realworld applications especially employed embedded systems. though previous work along research line shown promising results existing methods either fail signiﬁcantly compress well-trained deep network require heavy retraining process pruned deep network re-boost prediction performance. paper propose layer-wise pruning method deep neural networks. proposed method parameters individual layer pruned independently based second order derivatives layer-wise error function respect corresponding parameters. prove ﬁnal prediction performance drop pruning bounded linear combination reconstructed errors caused layer. controlling layer-wise errors properly needs perform light retraining process pruned network resume original prediction performance. conduct extensive experiments benchmark datasets demonstrate effectiveness pruning method compared several state-of-the-art baseline methods. codes work released https//github.com/csyhhu/l-obs. intuitively deep neural networks approximate predictive functions arbitrary complexity well huge amount parameters i.e. layers neurons. practice size deep neural networks tremendously increased lenet- less parameters vgg- parameters large number parameters make deep models memory intensive computationally expensive also urge researchers redundancy deep neural networks. hand neuroscience recent studies point signiﬁcant redundant neurons human brain memory relation vanishment speciﬁc synapses hand machine learning theoretical analysis empirical experiments shown evidence redundancy several deep models therefore possible compress deep neural networks without little loss prediction pruning parameters carefully designed criteria. however ﬁnding optimal pruning solution np-hard search space pruning exponential terms parameter size. recent work mainly focuses developing efﬁcient algorithms obtain near-optimal pruning solution common idea behind exiting approaches select parameters pruning based certain criteria increase training error magnitude parameter values etc. existing pruning criteria designed heuristically guarantee prediction performance deep neural network preserved pruning. therefore time-consuming retraining process usually needed boost performance trimmed neural network. instead consuming efforts whole deep network layer-wise pruning method net-trim proposed learn sparse parameters minimizing reconstructed error individual layer theoretical analysis provided overall performance drop deep network bounded reconstructed errors layer. pruned deep network theoretical guarantee error. however net-trim adopts -norm induce sparsity pruning fails obtain high compression ratio compared methods paper propose layer-wise pruning method deep neural networks aiming achieve following three goals layer parameters highly compressed pruning reconstructed error small. theoretical guarantee overall prediction performance pruned deep neural network terms reconstructed errors layer. deep network pruned light retraining process required resume original prediction performance. achieve ﬁrst goal borrow idea classic pruning approaches shallow neural networks optimal brain damage optimal brain surgeon classic methods approximate change error function functional taylor series identify unimportant weights based second order derivatives. though approaches proven effective shallow neural networks remains challenging extend deep neural networks high computational cost computing second order derivatives i.e. inverse hessian matrix parameters. work restrict computation second order derivatives w.r.t. parameters individual layer only i.e. hessian matrix parameters speciﬁc layer computation becomes tractable. moreover utilize characteristics back-propagation fully-connected layers well-trained deep networks reduce computational complexity inverse operation hessian matrix. achieve second goal based theoretical results provide proof bound performance drop pruning terms reconstructed errors layer. layer-wise pruning framework using second-order derivatives trimming parameters layer empirically show signiﬁcantly pruning parameters little drop prediction performance compared pruning. therefore light retraining process needed resume performance achieves third goal. contributions paper summarized follows. propose layer-wise pruning method deep neural networks able signiﬁcantly trim networks preserve prediction performance networks pruning theoretical guarantee. addition proposed method time-consuming retraining process re-boosting performance pruned network waived. conduct extensive experiments verify effectiveness proposed method compared several state-of-the-art approaches. pruning methods widely used model compression early neural networks modern deep neural networks past relatively small size training data pruning crucial avoid overﬁtting. classical methods include obs. methods prune parameters least increase error approximated second order derivatives. however computation hessian inverse parameters expensive. hessian matrix restricted diagonal matrix make computationally tractable. however approach implicitly assumes parameters interactions hurt pruning performance. different makes full hessian matrix pruning. obtains better performance much computationally expensive even using woodbury matrix identity iterative method compute hessian inverse. example using vgg- naturally requires compute inverse hessian matrix size regarding pruning modern deep models proposed delete unimportant parameters based magnitude absolute values retrain remaining ones recover original prediction performance. method achieves considerable compression ratio practice. however pointed pioneer research work parameters magnitude absolute values necessary error. therefore magnitude-based approaches eliminate wrong parameters resulting prediction performance drop right pruning poor robustness retraining though variants tried better magnitude-based criteria signiﬁcant drop prediction performance pruning still remains. avoid pruning wrong parameters introduced mask matrix indicate state network connection dynamically pruning gradient decent step. proposed iterative hard thresholding approach re-activate pruned parameters pruning phase. besides net-trim layer-wise pruning method discussed previous section work proposed induce sparsity low-rank approximation certain layers pruning however -norm -norm sparsity-induced regularization term increases difﬁculty optimization pruned deep neural networks using methods either obtain much smaller compression ratio compared direct pruning methods require retraining whole network prevent accumulation errors optimal brain surgeon proposed layer-wise pruning method extension deep neural networks brieﬂy review basic here. consider network terms parameters trained local minimum error. functional taylor series error w.r.t. variable ∂e/∂w rm×m hessian matrix number parameters oδθl) third higher order terms. network trained local minimum error ﬁrst term vanishes term oδθl) ignored. goal parameters zero denoted minimize pruning iteration. resultant optimization problem written follows unit selecting vector whose q-th element otherwise shown optimization problem solved lagrange multipliers method. note computation bottleneck calculate store non-diagonal hesssian matrix inverse makes impractical pruning deep models usually huge number parameters. given training instances well-trained deep neural network layers denote input output whole deep neural network ∈rd×n y∈rn× respectively. layer denote input output layer ∈rml×n respectively considered representation layer using wl∈rml−×ml forward-pass step matrix parameters layer activation function. convenience presentation proof deﬁne activation function rectiﬁed linear unit denote θl∈rml−ml× vectorization well-trained neural network ﬁxed matrixes contain information neural network. goal pruning values elements zero. layer-wise error layer-wise pruning layer input ﬁxed well-trained network. suppose q-th element denoted zero parameter vector denoted ˆθl. obtain output layer denoted ˆyl. consider root frobenius norm. note single parameter pruning compute ml−ml pruning criterion. idea adopted error existing methods however parameter layer pass whole training data compute error measure computationally expensive. efﬁcient approach make second order derivatives error function help identify importance parameter. ﬁrst deﬁne error function outcome weighted operation right performing activation function layer well-trained neural network outcome weighted operation pruning layer note considered desired output layer activation. following lemma shows layer-wise error bounded error deﬁned therefore parameters whose deletion minimizes translated parameters deletion minimizes error function following error function approximated functional taylor series follows hessian matrix denotes perturbation corresponding variable ∂el/∂θl w.r.t. oδθl) third higher order terms. proven error function deﬁned ﬁrst term unit selecting vector whose q-th element otherwise using lagrange multipliers method suggested obtain closed-form solutions optimal parameter pruning resultant minimal change error function follows referred sensitivity parameter select parameters prune based sensitivity scores instead magnitudes. mentioned section magnitude-based criteria merely consider numerator poor estimation sensitivity parameters. moreover inverse hessian matrix training data involved able capture data distribution measuring sensitivities parameters. pruning parameter smallest sensitivity parameter vector updated +δθl. lemma layer-wise error layer bounded shown prune parameters layer estimate introduced errors independently. however control consistence network’s ﬁnal output pruning. this following show layer-wise errors propagate ﬁnal output layer accumulated error multiple layers explode. theorem given pruned deep network layer-wise pruning introduced section layer layer-wise error accumulated error ultimate network output theorem shows that layer-wise error layer scaled continued multiplication parameters’ frobenius norm following layers propagates ﬁnal output i.e. layers l-th layer; ﬁnal error ultimate network output bounded weighted layer-wise errors. proof theorem found appendix. consider general case parameter smallest sensitivity layer k=l+ ˆθkf√δel ultimate network output error. worth mention although seems layer-wise error scaled k=l+ ˆθkf propagates ﬁnal layer scaling still tractable practice ultimate network output also scaled product factor compared output layer example easily estimate norm ultimate network output pruning operation layer causes layer-wise error relative ultimate output error thus even quite large relative ultimate output error would still controllable practice especially modern deep networks adopt maxout layer ultimate output. actually called network gain representing ratio magnitude network output magnitude network input. selectively prune parameters approach needs compute inverse hessian matrix layer measure sensitivities parameter layer still computationally expensive though tractable. section present efﬁcient algorithm reduce size hessian matrix thus speed computation inverse. layer according deﬁnition error function used lemma ﬁrst derivative error function respect j-th columns matrices respectively hessian matrix deﬁned figure illustration shape hessian. feed-forward neural networks unit gets activation forward propagation illustrated ﬁgure elements zero except corresponding denoted similar. importantly diag result needs compute layer output activation function gradient simply calculate importantly output units’s gradients equal layer input illustrated example shown figure ignore scripts simplicity presentation. shown block diagonal square matrix equal block diagonal square matrix diagonal blocks reduced computational complexity calculating make estimated minimal change error function optimal layer-wise hessian matrices need exact. since layer-wise hessian matrices depend corresponding layer inputs always able exact even several pruning operations. parameter need control layer-wise error note pruning inﬂection point layer-wise error would drop dramatically. practice user incrementally increase size pruned parameters based sensitivity make trade-off pruning ratio performance drop proper tolerable error threshold pruning ratio. procedure pruning algorithm fully-connected layer summarized follows. step layer input well-trained deep network. step calculate hessian matrix hlii pseudo-inverse dataset straightforward generalize method convolutional layer variants vectorize ﬁlters channel consider special fully-connected layers multiple inputs single instance. consider vectorized ﬁlter channel acts similarly parameters connected output unit fully-connected layer. however difference single input instance every ﬁlter step sliding window across extract patch input volume. similarly pixel -dimensional activation gives response patch corresponds output unit fully-connected layer. hence convolutional layers generalized block diagonal square matrix whose diagonal blocks same. then slightly revise computation hessian matrix extend algorithm fully-connected layers convolutional layers. note accumulated error ultimate network output linearly bounded layer-wise error long model feed-forward. thus l-obs general pruning method friendly feed-forward neural networks whose layer-wise hessian computed expediently slight modiﬁcations. however models sizable layers like resnet- l-obs economical computational cost hessian studied future work. section verify effectiveness proposed layer-wise using various architectures deep neural networks terms compression ratio error rate retraining number iterations required retraining resume satisfactory performance. deﬁned ratio number preserved parameters original parameters lower better. conduct comparison results l-obs following pruning approaches randomly pruning net-trim deep architectures used experiments include lenet-- lenet- mnist dataset cifar-net cifar- dataset alexnet vgg- imagenet ilsvrc- dataset. experiments ﬁrst well-train networks apply various pruning approaches networks evaluate performance. retraining batch size crop method hyper-parameters setting used lwc. note make comparisons fair adopt pruning related methods like dropout sparse regularizers mnist. practice l-obs work well along techniques shown cifar- imagenet. overall comparison results shown table ﬁrst experiments prune layer well-trained lenet-- compression ratios achieving slightly better overall compression ratio comparable compression ratio l-obs quite less drop performance lighter retraining compared whose performance almost ruined pruning. classic pruning approach also compared though observe hessian matrices modern deep models strongly non-diagonal practice. besides relative heavy cost obtain second derivatives chain rule suffers drastic drop performance directly applied modern deep models. properly prune layer lenet- increase tolerable error threshold relative small initial value incrementally prune parameters monitor model performance stop pruning encounter pruning inﬂection point mentioned section practice prune layer lenet- compression ratio retrain pruned model much fewer iterations compared methods retrains pruned network every pruning operation able report error rate pruned network retraining. however seen similar total number iterations used rebooting network large compared l-obs. results retraining iterations reported experiments implemented based tensorflow addition scenario requiring high pruning ratio l-obs quite ﬂexibly adopted iterative version performs pruning light retraining alternatively obtain higher pruning ratio relative higher cost pruning. iterations pruning retraining l-obs able achieve pruning ratio much lighter total retraining iterations lenet-- iterations lenet-. regarding comparison experiments cifar-net ﬁrst well-train achieve testing error dropout batch-normalization. prune well-trained network l-obs similar results network architectures. also observe retraining-required methods always require much smaller learning rate retraining. representation capability pruned networks much fewer parameters damaged pruning based principle number parameters important factor representation capability. however l-obs still adopt original learning rate retrain pruned networks. consideration l-obs ensures warm-start retraining also ﬁnds important connections preserve capability representation pruned network instead ruining model pruning. regarding alexnet l-obs achieves overall compression ratio without loss accuracy hours intel xeon compute hessians hours nvidia tian retrain pruned model computation cost hessian inverse l-obs negligible compared heavy retraining methods. claim also supported analysis time complexity. mentioned section time complexity calculating approximate time complexity retraining size mini-batch total numbers parameters iterations respectively. considering shown experiments complexity calculating hessian l-obs quite economic. interestingly trade-off compression ratio pruning cost. compared methods l-obs able provide fast-compression prune alexnet original size without substantively impacting accuracy even without retraining. apply l-obs vgg- parameters. achieve promising compression ratio perform pruning retraining alteratively twice. seen table l-obs achieves overall compression ratio without loss accuracy taking hours total intel xeon compute hessian inverses iterations retrain pruned model. also apply l-obs resnet- best knowledge ﬁrst work perform pruning resnet. perform pruning layers layers share compression ratio change compression ratio experiments. results shown figure l-obs able maintain resnet’s accuracy compression ratio larger equal yl−) corresponds rylf l-obs. memory limitation net-trim prune middle layer lenet-- l-obs net-trim setting. shown table pruned error rate l-obs outnumbers net-trim times. addition net-trim encounters explosion memory time large-scale datasets large-size parameters. speciﬁcally space complexity positive semideﬁnite matrix proposed novel l-obs pruning framework prune parameters based second order derivatives information layer-wise error function provided theoretical guarantee overall error terms reconstructed errors layer. proposed l-obs prune considerable number parameters tiny drop performance reduce even omit retraining. importantly identiﬁes preserves real important part networks pruning compared previous methods help dive nature neural networks. acknowledgements", "year": 2017}