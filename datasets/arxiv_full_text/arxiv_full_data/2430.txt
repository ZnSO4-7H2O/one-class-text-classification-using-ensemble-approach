{"title": "Learning Bayesian Networks with Local Structure", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper we examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability tables (CPTs), that quantify these networks. This increases the space of possible models, enabling the representation of CPTs with a variable number of parameters that depends on the learned local structures. The resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures, as well as an empirical evaluation of the proposed method. This evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure. Our results also show that networks learned with local structure tend to be more complex (in terms of arcs), yet require less parameters.", "text": "paper examine novel addition known methods learning bayesian networks data improves quality learned networks. learns local structure bility tables quantify networks. increases abling representation number parameters local structures. capable inducing models better emulate real complexity data. describe practical well empirical method. evaluation curves characterizing local structure converge faster standard procedure. results also show networks learned local structure tend complex require less parameters. introduction recent years growing number interest­ bayesian networks results literature data. results global network; structure acyclic graph describes network. structure parameters conditional usually solved estimating parameters data. paper propose local structures introduce structures using structures complexity considerably works. whose parameters node network associated conditional probability distribution different values parents. encoded using tabular representation locally exponential assignment specification thus example consider nodes correspond \"alarm armed\" \"burglary\" sound\" respectively. naive tabular representation parameters possible possible quantification first resenta�wns usually estimation ples account thus robust. global structure approximation local structure explore penalty eration. last point finding glob�l stru�ture makes unrealistic sumptions thus crucial global structure. validate sec­ tion show local representations significant translates data. words fewer data _samples order induce network better proxtmates �any parameters. nstances procedure original paper twofold. representation parameters. firs� formulation introduced ever network parents whtch u�covers local rep­ thus small train­ would require parameters. �se�tatwn cpts. second empirical network preferred even though ttgatwn validates process g�ores effect point example also derive illustrate algorithms rame�ers learn�ng procedure tion discuss metnc describes another possible tation based describes encode possible tree captures learning bayesian networks nite discrete random vari able variable take values nite apital lette names lowe lette denote variabl specific values taken variables. value attain denote ardinality denote lxii vali. variables denote dface apital lette assignments variables sets denoted boldface letters lxii obviou way) oint probability distr ibuti subse variabl andy conditionally valy whene network direct dependencies variables. graph struc­ ture odes followin independence assump­ tions given nent pair eters uant ifies network ontai parameter follows. given training stances work best matches ormalize goodness network respect data norm ally introduce scorin metric solve optim izat problem usually rely heurist searc ique space sible network several iffere met­ propose rature pape focus attention minimal description length score score intuitiv prove quite effective practice. another scorin metric receive much tention recentl ayesian scorin metric defe disc sion metric tivation unive rsal oding. suppose give instances would like store keep ords. urally would onse space save presse version need suitable odel ford oder take produce pact image moreover able also store version odel encode press desc ript length data odel using particula encode length ssed plus representation size odel itse princi dictate optimal odel minimize total work network odel data oder produced pressed version follows network assign probability instance probabilitie onstruc efficient ode. particular huffman whic assigns frequent nstances. nefi using scor­ metric best network imal ance plexity network degree accurac whic network represents quencies describe detail repre ntation using probability huffman code instances exact length codeword depends probability assigned description thomas choose longer coding blocks approximate encoding description length used simplistic param­ cpts eters assumed usual representation requiring node assumed need precise encode iiiixiiciix;iihow­ ever relation benign represent example figure information encoded four parameters figure opposed eight parameters required naive tabular representation tations node parents resentation. metric tradeoffs curacy representation. networks encoding large number parents. hand take advantage node parents allow explo­ ration networks fitness default tables default table similar standard sentation values parents rows table. values parents explicitly special probability parents need represent entries. represented rameters dldej decision trees context node annotated particular variable leaves process value parents traverse node choose subtree value parent outgoing would like know tree shown figure edge edge annotated right subtree value similarly left edge immediate sub-trees. pends position test variable along single smaller eral node levels description length parameters simply number leaves multiplied ill) noted encoding lace patrick suboptimal factor. since description length parameters sample size similar bookkeeping penalty tree compared more penalty little tree grows near full tree. learning locaj structures section approach structures {default global structure applied independently assume given variable parents objective structure procedures network candidate. important aspect previous section example representation lengths scoring data using entropy also decomposable. shown case cpts section follows easily cases default tables trees. incremental learning subtree children decide parent perform split compute score i.e. dlree defined above tree associated induces best scoring tree. computed instances path root procedure instances errors graphs roughly resent boundaries three converge search method finding candidate intersect point search empty network. consider however three possible general rule edge addition measure gtab gdef gtree· lines constant step procedure error clearly applies samples gtab needs reach approximation includes gctef cpts modified repeated surprising candidate's method likely global one. however reasonable practice hill-climbing search default clearly sample sizes. suspect bookkeeping greedy learning algorithm note however trees perform better sample sizes. example gtree performance improves sample size grows fact many cpts represented noisy-or better approximated another possible default trees handle multi-valued attribute subtrees fault tables values multi -attribute variables future work plan address complexity using trees using default tables depends representation correspond values leaves decision table. since mutually disjoint exhaus­ tive thus grows larger parameters evaluated risk less robust. hence general fixed training data size fewer parameters model reliable ways. chosen measure using number parameters usual parameters exponential size family measure estimates network. measure exact counting edges since parent variables. network edge directed variable values representing from. variable cost terms number figure meth­ complexity networks ods. note learns default paramet bles learns models complex. sam� ti�e induces reduces estimated variance thts combmatwn representa tion �eters produces real distribution m)dependencies cross-entropy proves gtree learns grab produces less parameters complex networks. usually learns rameters gctef· complex discussion main contribution paper repr.esen�ations structur denttfi benefits cess sentatw and. mentioned number parameters needed sider efficient edge exiii multiplie learning. best knowledge �onsider demonst complexity. tations network. distinguish important addition representat noisy­ ions examine paper logistic examined sion models regres th.e li�erature. noisy-or regression logistic attempt wtth fixed number timate undoubtedly number usually linear number parents cpt. cases target satisf distribution models assumptions embodied estimates cpts produced arbitrarily diverge methods target distribution. hand local gained tions learning representa involve structure iearmng lean structure range param� following fewer �earmng performed number full structure eters exponential took expenment. accord ions scale thus rameters. representat learned best parameters holding structure complexity data. ensures training that usmg given enough asymptotical correct theory found tables note samples approximation construct global target lo�al structures learning obtain investigations simple cpts. observing cross­ tables. tions-trees structured entr.opy tables row. traverse representa certai tion cpts addition example formu­ decision based lead additional improvements approximations. goldszmidt etc. larger since increases error complex) structures table learning learned sample sets size network sample sets size part learned network.", "year": 2013}