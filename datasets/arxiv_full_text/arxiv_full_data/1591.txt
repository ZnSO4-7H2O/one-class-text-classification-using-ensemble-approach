{"title": "Improved Relation Extraction with Feature-Rich Compositional Embedding  Models", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.", "text": "compositional embedding models build representation linguistic structure based component word embeddings. propose feature-rich compositional embedding model relation extraction expressive generalizes domains easy-to-implement. idea combine handcrafted features learned word embeddings. model able directly tackle difﬁculties traditional compositional embeddings models handling arbitrary types sentence annotations utilizing global information composition. test proposed model relation extraction tasks demonstrate model outperforms previous compositional models traditional feature rich models relation extraction task semeval relation classiﬁcation task. combination model loglinear classiﬁer hand-crafted features gives state-of-the-art results. made implementation available general use. common feature types lexical properties words unlexicalized linguistic/structural interactions words. prior work relation extraction extensively studied design features combining discrete lexical properties aspects word’s linguistic context help learning make generalization unseen words difﬁcult. alternative approach capturing lexical information relies continuous word embeddings representative words generalizable words. embedding features improved many tasks including chunking dependency parsing semantic role labeling relation extraction embeddings capture lexical information alone insufﬁcient state-of-the-art systems used alongside features broader linguistic context. paper introduce compositional model combines unlexicalized linguistic context word embeddings relation extraction task contextual feature construction plays major role generalizing unseen data. model allows composition embeddings arbitrary linguistic structure expressed hand crafted features. following sections begin precise construction compositional embeddings using word embeddings conjunction unlexicalized features. various feature sets used prior work capincluding term-document frequency matrices lowdimensional counterparts obtained linear algebra tools brown clusters random projections vector space models. recently neural networks deep learning provided several popular methods obtaining embeddings. table examples word driving strong indicator relation feature depends embedding context word could generalize lexical indicators relation don’t appear training. lexical information alone insufﬁcient; relation extraction requires identiﬁcation lexical roles word appears structurally sentence. word suburbs baghdad suggests ﬁrst entity part second earlier occurrence direction signiﬁcance relation. even ﬁner information expressed word’s role dependency path entities. distinguish word died irrelevant words don’t appear entities. tured special cases construction. adding compositional embeddings directly standard log-linear model yields special case full model. treat word embeddings parameters giving rise powerful efﬁcient easy-to-implement log-bilinear model. model capitalizes arbitrary types linguistic annotations better utilizing features associated substructures annotations including global information. choose features promote different properties distinguish different functions input words. full model involves three stages. first decomposes annotated sentence substructures second extracts features substructure combines word’s embedding form substructure embedding. third substructure embeddings form composed annotated sentence embedding used ﬁnal softmax layer predict output label dredze additionally extended incorporate low-rank embedding features focuses ﬁne-grained relation extraction ere. paper obtains better results low-rank extension coarse-grained relation extraction. relation extraction given sentence input goal identifying pairs entity mentions relation exists them any. pair entity mentions sentence construct instance sentence length expresses relation type entity mentions sequences words associated annotations sentence part-of-speech tags dependency parse named entities. consider directed relations relation type y=rel y=rel different relations. table shows relations strong label bias towards negative examples. also consider task relation classiﬁcation number negative examples artiﬁcially reduced. embedding models word embeddings compositional embedding models successfully applied range tasks however applications embedding models relation extraction still limited. prior work relation classiﬁcation focused short sentences relation sentence relation extraction negative examples abound prior work assumed named entity boundaries types available work assumed order entities relation given relation type unknown standard relation extraction task adopted uses long sentences containing multiple named entities known types unknown relation directions. ﬁrst apply neural language model embeddings task. motivation examples whether word indicative relation depends multiple properties relate context within sentence. example whether word inentities dependency path between them left right provide additional complementary information. illustrative examples given table provide motivation model. next section show develop informative representations capturing semantic information word embeddings contextual information expressing word’s role relative entity mentions. ﬁrst incorporate information once. closest work nguyen grishman loglinear model relation extraction embeddings features entity heads. embedding features insensitive broader contextual information show sufﬁcient elicit word’s role relation. propose general framework construct embedding sentence annotations component words. focus relation extraction task framework applies task beneﬁts embeddings typical hand-engineered lexical features. combining features embeddings begin describing precise method constructing substructure embeddings annotated sentence embeddings existing features embeddings. note embeddings included directly log-linear model features—doing results annotated sentence ﬁrst decomposed substructures. type substructures vary task; relation extraction consider substructure word. substructure sentence hand-crafted feature vector dense embedding vector ewi. represent substructure outer product vectors produce matrix herein called substructure embedding ewi. features based local context annotations include global information annotated sentence. features allow model promote different properties distinguish different functions words. feature engineering task speciﬁc relevant annotations change regards task. work utilize unlexicalized binary features common relation extraction. figure depicts construction sentence’s substructure embeddings. substructure embeddings form annotated sentence embedding hand-crafted features word embeddings treated inputs previously case relation extraction annotated sentence embedding used directly features log-linear model. fact feature sets used prior work many tasks special cases simple construction highlights important connection word embeddings constant constructions substructure annotated sentence embeddings speciﬁc forms polynomial feature combination—hence commonality literature. experimental results suggest construction powerful directly including embeddings model. figure example construction substructure embeddings. substructure word augmented target entity information related information annotation show factorization annotated sentence substructures concatenation substructure embeddings sentence single substructure embedding concatenation annotated sentence embedding would substructure embeddings opposed concatenation. previous subsection. model uses parameters score annotated sentence embedding uses softmax produce output label. call entire model feature-rich compositional embedding model ‘matrix product’ frobenious inner product matrices. normalizing constant sums possible output labels given rameters model word embeddings word type list weight matrix used score label model log-bilinear since recover log-linear model ﬁxing either study full log-bilinear log-linear model obtained ﬁxing word embeddings. discussion model substructure embeddings similar words similar functions sentence similar matrix representations. understand selection outer product consider example fig. word driving indicate relation appears dependency path suppose third feature indicates on-path feature. model learn parameters give third high weight label. words embeddings similar driving appear dependency path mentions similarly receive high weight label. hand embedding similar dependency path weight. thus model generalizes model parameters across words similar embeddings share similar functions sentence. smoothed lexical features another intuition selection outer product actually smoothed version traditional lexical features used classical systems. consider lexical feature conjunction non-lexical property lexical part represent one-hot vector outer product exactly recovers original feature replace one-hot representation word embedding current form fcm. therefore model viewed smoothed version lexical features keeps expressive strength uses embeddings generalize frequency features. time complexity inference much faster cnns rnns requires products average sparse features average number per-word non-zero feature values length sentence dimension word embedding. contrast cnns rnns usually experimental settings features features feature vector word target entities dependency path. indices head words refers cartesian product between sets entity types head words entities stands empty feature. refers conjunction elements. in-between features indicate whether word target entities on-path features indicate whether word dependency path words entities. also target entity type feature. combining basic features results powerful compound features help better distinguish functions word embeddings predicting certain relations. example person vehicle know likely relation. relation introduce corresponding weight vector closer lexical embeddings similar embedding drive. features stanford corenlp since semeval gold entity types obtained wordnet named entity tags using ciaramita altun experiments word embeddings trained portion gigaword corpus wordvec cbow model negative sampling window size remove types occurring less times. log-linear model form ploglin exp) model parameters vector features. integration treats model providing score multiply together. constant ensures normalized distribution. training data word embeddings. optimize objective instance perform stochastic training loss function gradi ents model parameters obtained backpropagation deﬁne vector common deep learning initialize embeddings neural language model ﬁne-tune supervised task. training process hybrid model also easily done backpropagation since sub-model separate parameters. model). log-linear model rich binary feature consists baseline features zhou plus several additional carefully-chosen features highly tuned ace-style relation extraction years research. exclude country gazetteer wordnet features zhou remaining methods hybrid models integrate submodel within log-linear model consider combinations. feature nguyen grishman obtained using embeddings heads entity mentions full model models regularization tuned data. datasets evaluation evaluate relation extraction system english portion corpus domains newswire broadcast conversation broadcast news telephone speech usenet newsgroups weblogs following prior work focus domain adaptation setting train tune hyperparameters domain evaluate remainder assume gold entity spans types available train test. pairs entity mentions yield total relations training set. report precision recall relation extraction. focus completeness include results unknown entity types following plank moschitti semeval task evaluate semeval task dataset compare compositional models highlight advantages fcm. task determine relation type entities sentence. adopt setting socher -fold many relation extraction systems evaluate corpus unfortunately common convention -fold cross validation treating entirety dataset train evaluation data. rather continuing overﬁt data perpetuating cross-validation convention instead focus cross validation training data select hyperparameters regularization early stopping. learning rates with/without ﬁne-tuning respectively. report macro-f compare previously published results. despite fcm’s simple feature competitive log-linear baseline out-of-domain test sets typical gold entity spans types setting plank moschitti nguyen grishman found unable obtain improvements adding embeddings baseline feature sets. contrast domains combination baseline obtains highest signiﬁcantly outperforms baselines yielding best reported results task. found ﬁne-tuning embeddings yield improvements out-of-domain development contrast results semeval. suspect because ﬁne-tuning allows model overﬁt training domain hurts performance unseen test domains. accordingly table shows log-linear model. finally highlight important contrast between log-linear model latter uses feature templates based tagger dependency parser chunker constituency parser. uses dependency parse still obtains better results considered feature sets. found using tags instead wordnet tags helps ﬁne-tuning hurts without. wordnet tags larger making model expressive also introduces parameters. embeddings ﬁxed help better distinguish different functions embeddings. ﬁne-tuning becomes easier over-ﬁt. alleviating over-ﬁtting subject future work either wordnet features achieves better performance mvrnn. features ﬁne-tuning outperforms also features preﬁxes morphological wordnet dependency parse levin classed probank framenet nomlex-plus google n-gram paraphrases textrunner word embedding syntactic parse word embedding syntactic parse wordnet word embedding syntactic parse word embedding syntactic parse wordnet word embedding wordnet word embedding word embedding word embedding word embedding word embedding dependency paths word embedding dependency paths wordnet word embedding dependency parse wordnet word embedding dependency parse word embedding dependency parse wordnet word embedding dependency parse word embedding dependency parse wordnet word embedding dependency parse also compared concurrent work enhancing compositional models taskspeciﬁc information relation classiﬁcation including hashimoto trained task-speciﬁc word embeddings santos proposed task-speciﬁc ranking-based loss function. hybrid methods comparable results theirs. note base compositional model results without task-speciﬁc enhancements i.e. relemb wordvec embeddings cr-cnn log-loss still lower best result. believe also improved task-speciﬁc enhancements e.g. replacing word embeddings taskspeciﬁc ones increases result leave application ranking-based loss future work. finally concurrent work proposes depnn builds representations dependency path entities applying recursive convolutional neural networks successively. compared model achieves comparable results. note relemb also efﬁcient models among compositional models since linear time complexity respect dimension embeddings. effects embedding sub-models next investigate effects different types features using ablation tests focus alone feature templates table additionally show results using head embedding features nguyen grishman surprisingly headonly model performs poorly showing importance rich binary feature set. among features templates removing heademb results largest degradation. second important feature template in-between context features little impact. removing entity type features signiﬁcantly worse full model showing value entity type features. effects word embeddings good word embeddings critical compositional models. section show results embeddings used initialize recent state-of-the-art models. embeddings include baseline embeddings trained english wikipedia task-speciﬁc embeddings relemb paper embeddings cr-cnn paper moreover list best result uses embeddings ours. table shows effects word embeddings provides relative comparisons state-of-the-art models. hyperparameters number iterations table results show using different embeddings initialize improve beyond previous results. also increasing dimension word embeddings necessarily lead better results problem over-ﬁtting initial embeddings usually gets better results without changes hyperparameters competing model conﬁrming advantage model-level discussed table exception depnn model gets better result embeddings. task-speciﬁc embeddings leads best performance observacompositional models sentences order build representation sentence based component word embeddings structural information recent work compositional models designed model structures mimic structure input. example models could take account order words build input tree semantic matching energy function) models work well sentence-level representations nature designs also limits ﬁxed types substructures annotated sentence chains cnns trees rnns. models cannot capture arbitrary combinations linguistic annotations available given task word order dependency tree named entities used relation extraction. moreover approaches ignore differences functions words appearing different roles. suit general substructure labeling tasks e.g. models cannot directly applied relation extraction since output result pair entities sentence. compositional models annotation features tackle problem traditional compositional models socher made model speciﬁc relation extraction tasks working minimal sub-tree spans target entities. however specializations enhancing compositional models features. recent trend enhances compositional models annotation features. approach shown signiﬁcantly improve pure compositional models. example hermann nguyen grishman gave different weights words different syntactic context types entity head words different argument ids. zeng concatenations embeddings features model according positions relative target entity mentions. belinkov enrich embeddings linguistic features feeding forward model. socher hermann blunsom enhanced models reﬁning transformation matrices phrase types super tags. engineering embedding features. different approach combining traditional linguistic features embeddings hand-engineering features word embeddings adding log-linear models. approaches achieved state-of-the-art results many tasks including chunking dependency parsing semantic role labeling relation extraction roth woodsend considered features similar semantic role labeling. prior work approaches able utilize limited information usually property word. different useful properties word contribute performances task. contrast easily utilize features without changing model structures. order better utilize dependency annotations recently work built models according dependency paths share similar motivations usage on-path features work. task-speciﬁc enhancements relation classiﬁcation orthogonal direction improving compositional models relation classiﬁcation enhance models task-speciﬁc information. example hashimoto trained presented compositional model deriving sentence-level substructure embeddings word embeddings. compared existing compositional models easily handle arbitrary types input handle global information composition remaining easy implement. demonstrated alone attains near state-of-the-art performances several relation extraction tasks combination traditional feature based loglinear models obtains state-of-the-art results. next steps improving focus enhancements based task-speciﬁc embeddings loss functions hashimoto moreover model provides general idea representing sentences sub-structures language potential contribute useful components various tasks dependency parsing paraphrasing. also kindly pointed anonymous reviewer applied tac-kbp tasks replacing training objective multi-instance multilabel plan explore applications future. thank anonymous reviewers comments nicholas andrews francis ferraro benjamin durme input. thank kazuma hashimoto c´ıcero nogueira santos bing xiang bowen zhou sharing word embeddings many helpful discussions. supported china scholarship council nsfc references yonatan belinkov regina barzilay amir globerson. exploring compositional architectures word vector representations prepositional phrase attachment. transactions association computational linguistics cicero santos bing xiang bowen zhou. classifying relations ranking convolutional neural networks. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. kazuma hashimoto pontus stenetorp makoto miwa yoshimasa tsuruoka. task-oriented learning word embeddings semantic relation classiﬁcation. arxiv preprint arxiv.. iris hendrickx zornitsa kozareva preslav nakov diarmuid s´eaghdha sebastian pad´o marco pennacchiotti lorenza romano stan szpakowicz. semeval- task multi-way classiﬁcation semantic relations pairs nominals. proceedings semeval- workshop. karl moritz hermann dipanjan jason weston kuzman ganchev. semantic frame identiﬁcation distributed word representations. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. terry xavier carreras michael collins. simple semi-supervised dependency parsing. proceedings acl- pages columbus ohio june. association computational linguistics. incremental joint extraction entity mentions relations. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. yang furu sujian heng ming zhou houfeng wang. dependency-based neural network relation classiﬁcation. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. mingbo liang huang bowen zhou bing xiang. dependency-based convolutional neural networks sentence embedding. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky. stanford corenlp natural language processing toolkit. proceedings annual meeting association computational linguistics system demonstrations pages tomas mikolov ilya sutskever chen greg corrado jeffrey dean. distributed representations words phrases compositionality. arxiv preprint arxiv.. scott miller jethran guinness alex zamanian. name tagging word clusters discriminative training. susan dumais daniel marcu salim roukos editors hlt-naacl main proceedings. association computational linguistics. andriy mnih geoffrey hinton. three graphical models statistical language modelling. proceedings international conference machine learning pages acm. thien nguyen ralph grishman. employing word representations regularization domain adaptation relation extraction. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. thien nguyen ralph grishman. relation extraction perspective convolutional neural networks. proceedings naacl workshop vector space modeling nlp. joseph turian ratinov yoshua bengio. word representations simple general method semi-supervised learning. association computational linguistics pages daojian zeng kang siwei guangyou zhou zhao. relation classiﬁcation convolutional deep neural network. proceedings coling international conference computational linguistics technical papers pages dublin ireland august. dublin city university association computational linguistics. main adaptation case study tree kernelbased method relation extraction. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. barbara plank alessandro moschitti. embedding semantic similarity tree kernels domain adaptation relation extraction. proceedings annual meeting association computational linguistics pages soﬁa bulgaria august. association computational linguistics. bryan rink sanda harabagiu. classifying semantic relations combining lexical semantic resources. proceedings international workshop semantic evaluation pages uppsala sweden july. association computational linguistics. richard socher brody huval christopher manning andrew semantic compositionality recursive matrix-vector spaces. proceedings joint conference empirical methods natural language processing computational natural language learning pages jeju island korea july. association computational linguistics. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. empirical methods natural language processing pages ralph grishman satoshi sekine. semi-supervised relation extraction large-scale word clustering. proceedings annual meeting association computational linguistics human language technologies pages portland oregon june. association computational linguistics. mihai surdeanu julie tibshirani ramesh nallapati christopher manning. multi-instance multi-label learning relation extraction. proceedings joint conference empirical experimental settings comparison prior work generate relation instances pairs entities within sentence three fewer intervening entity mentions—labeling pairs relation negative instances gold entity spans train test time evaluate coarse relation types ignoring subtypes. training total relations annotated non-nil relations. match number tokens reported domains. therefore section report results test domain. leave experiments additional domains future work. models task. entity type features. plank moschitti also brown clusters word vectors learned latent-semantic analysis order make fair comparison method also report result using brown clusters entity heads entity types. furthermore report non-comparable settings using wordnet super-sense tags entity heads types. wordnet features also used paper substitution entity types. toolkit wordnet tags brown clusters results table shows results low-resource setting. entity types available performance model greatly decreases consistent observation ablation tests. baseline model also relies heavily entity types. remove handengineering features contain entity type information performance baseline model drop even lower reduced model. combination baseline model head embeddings greatly improve results. consistent observation nguyen grishman gold entity types unknown information entity heads provided embeddings play important role. combination baseline also achieves improvement signiﬁcantly better baseline headonly. possible explanation becomes less efﬁcient using context word embeddings entity type information unavailable. situation head embeddings provided become dominating contribution baseline model making model similar behavior baseline headonly method. finally brown clusters help entity types unknown. although performance still signiﬁcantly better baseline headonly outperforms results plank moschitti single model source features. wordnet supersense tags improves achieves best reported results low-resource setting. results encouraging since shows useful end-to-end setting predictions entity mentions relation mentions required place predicting relation based gold tags recently nguyen proposed novel applying embeddings tree-kernels. results best single model achieves comparable result best single system combination method slightly better ours. suggests beneﬁt combining usages multiple word representations; investigate future work. table comparison models out-ofdomain test sets low-resource setting gold entity spans known entity types unknown. results reported plank moschitti linear+emb implementation method tree-kernel+emb methods enrichments tree-kernels embeddings proposed nguyen", "year": 2015}