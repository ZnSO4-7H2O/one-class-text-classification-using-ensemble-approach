{"title": "True Online Temporal-Difference Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "The temporal-difference methods TD($\\lambda$) and Sarsa($\\lambda$) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD($\\lambda$) and true online Sarsa($\\lambda$), respectively (van Seijen & Sutton, 2014). These new versions maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD($\\lambda$)/Sarsa($\\lambda$) with regular TD($\\lambda$)/Sarsa($\\lambda$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-depth analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations.", "text": "temporal-diﬀerence methods sarsa form core part modern reinforcement learning. appeal comes good performance computational cost simple interpretation given forward view. recently versions methods introduced called true online true online sarsa respectively algorithmically true online methods make small changes update rules regular methods extra computational cost negligible cases. however follow ideas underlying forward view much closely. particular maintain exact equivalence forward view times whereas traditional versions approximate small step-sizes. hypothesize true online methods better theoretical properties also dominate regular methods empirically. article hypothesis test performing extensive empirical comparison. speciﬁcally compare performance true online td/sarsa regular td/sarsa random mrps real-world myoelectric prosthetic domain arcade learning environment. linear function approximation tabular binary non-binary features. results suggest true online methods indeed dominate regular methods. across domains/representations learning speed true online methods often better never worse regular methods. additional advantage choice traces made true online methods. besides empirical results provide in-depth analysis theory behind true online temporal-diﬀerence learning. addition show true online temporaldiﬀerence methods derived making changes online forward view rewriting update equations. temporal-diﬀerence learning core learning technique modern reinforcement learning main challenges reinforcement learning make predictions initially unknown environment future rewards return based currently observed feature values certain behaviour policy. learning possible learn good estimates expected return quickly bootstrapping expected-return estimates. popular algorithm combines basic learning eligibility traces speed learning. popularity explained simple implementation low-computational complexity conceptually straightforward interpretation given forward view. forward view states estimate time step moved towards update target known λ-return determining fundamental trade-oﬀ bias variance update target. trade-oﬀ large inﬂuence speed learning optimal setting varies domain domain. ability improve trade-oﬀ adjusting value underlies performance advantage eligibility traces. although forward view provides clear intuition closely approximates forward view appropriately small step-sizes. recently considered unfortunate unavoidable part theory behind changed introduction true online computes exactly weight vectors forward view step-size. gives true online full control bias-variance trade-oﬀ. particular true online achieve fully unbiased updates. moreover true online requires small modiﬁcations update equations extra computational cost negligible cases. hypothesize true online control version true online sarsa better theoretical properties regular counterparts also dominate empirically. test hypothesis performing extensive empirical comparison true online regular common variation based replacing traces. addition perform comparisons true online sarsa sarsa domains include random markov reward processes real-world myoelectric prosthetic domain arcade learning environment representations consider range tabular values linear function approximation binary non-binary features. besides empirical study provide in-depth discussion theory behind true online theory based online forward view. traditional forward view based λ-return inherently oﬄine forward view meaning updates occur episode λ-return requires data episode. extend forward view online case updates occur every time step using bounded version λ-return grows time. whereas approximates traditional forward view episode show approximates online forward view time steps. true online equivalent online forward view time steps. prove deriving true online update equations directly online forward view update equations. derivation forms blueprint derivation true online methods. making variations online forward view following derivation true online derive several true online methods. article organized follows. start presenting required background section then present online forward view section followed presentation true online section section presents empirical study. furthermore section present several true online methods. section discuss detail related papers. finally section concludes. here present main learning framework. convention indicate scalar-valued random variables capital letters vectors bold lowercase letters functions non-bold lowercase letters sets calligraphic font described -tuples form indicates states; indicates actions; indicates probability transition state action taken state indicates expected reward transition state state action discount factor speciﬁes future rewards weighted respect immediate reward. actions taken discrete time steps according policy reward received taking action state mdps contain special states called terminal states. reaching terminal state reward obtained state transitions occur. hence terminal state interpreted state action returns reward interaction sequence initial state terminal state called episode. tasks typically associated mdp. first task determining value function given policy second challenging task determining optimal policy deﬁned policy whose corresponding value function highest value state tasks considered condition reward function transition-probability function unknown. hence tasks solved using samples obtained interacting environment. let’s consider task learning estimate value function samples estimated using linear function approximation. inner product feature vector weight vector stationary distribution induced error function minimized using stochastic gradient descent sampling stationary distribution resulting following update rule variance typically high. hence learning full return slow. temporaldiﬀerence learning addresses issue using update targets based value estimates. update target longer unbiased case variance typically much smaller learning much faster. learning uses bellman equations mathematical foundation constructing update targets. equations relate value state values successor states update target called one-step update target based information time step ahead. applying bellman equation multiple times results update targets based information ahead. update targets called multistep update targets. eligibility-trace vector. update shown referred accumulatingtrace update. shorthand refer version ‘accumulate distinguish slightly diﬀerent version discussed below. updates appear deviate general gradient-descent-based update rule given close connection update rule. connection formalized forward view discuss detail next section. algorithm shows pseudocode accumulate accumulate sensitive respect parameters. especially large value combined large value easily cause divergence even simple tasks bounded rewards. reason variant sometimes used robust respect parameters. variant assumes binary features uses diﬀerent trace-update equation traditional forward view relates update equations general update rule shown equation speciﬁcally small step-sizes weight vector episode computed accumulate approximately weight vector resulting sequence equation updates using particular multi-step update target called λ-return λ-return state deﬁned call method updates value visited state episode oﬄine method; call method updates value visited state immediately visit online method. online method. update sequence traditional forward view however corresponds oﬄine method λ-return requires data episode. leaves open question interpret weights episode. section provide answer long-standing open question. introduce bounded version λ-return uses information certain horizon construct online forward view. online forward view approximates weight vectors accumulate time steps instead episode. concept online forward view contains paradox. hand multi-step update targets require data time steps beyond time state visited; hand online aspect requires value visited state updated immediately. solution paradox assign sequence update targets visited state. ﬁrst update target sequence contains data next time step second contains data next time steps third next three time steps given initial weight vector sequence visited states weight vector constructed updating visited state update target contains data current time step. below formalize idea. time increases update targets based data away become available state particular time step weight vector computed performing equation update visited state using interim λ-return horizon starting initial weight vector θinit. hence time step sequence updates occurs. describe sequence mathematically weight vectors indices superscript indicates time step updates performed subscript iteration index sequence example update sequences ﬁrst three time steps update sequence performed online λ-return algorithm time step similar update sequence performed same assumption weights used value estimates same. weights practise exactly same typically small diﬀerence. figure illustrates diﬀerence online oﬄine λ-return algorithm well accumulate showing error random walk task. task consists states laid plus terminal state left. state transitions probability left neighbour probability right neighbour rewards furthermore right-most state initial state. whereas oﬄine λ-return algorithm makes updates episode online λ-return algorithm well accumulate make updates every time step. comparison random walk task shows accumulate behaves similar online λ-return algorithm. fact smaller step-size smaller diﬀerence accumulate online λ-return algorithm. formalized theorem proof theorem found appendix theorem uses term theorem generalizes traditional result arbitrary time steps. traditional result states diﬀerence weight vector episode computed oﬄine λ-return algorithm weight vector episode computed accumulate goes step-size goes accumulate behaves like online λ-return algorithm small step-sizes small step-sizes often result slow learning. hence higher step-sizes desirable. higher step-sizes however behaviour accumulate diﬀerent online λ-return algorithm. show empirical section article diﬀerence almost exclusively favour online λ-return algorithm. section analyze online λ-return algorithm outperform accumulate using one-state example shown left figure right figure shows error ﬁrst episodes one-state example diﬀerent step-sizes small step-sizes accumulate behaves indeed like online λ-return algorithm—as predicted theorem larger step-sizes diﬀerence becomes huge. understand reason this derive analytical expression value episode. first consider accumulate state involved indicate value state simply update episode αet−δt−. example time steps except vt−. time steps except last furthermore accumulate pseudo step-size grow much larger even causing divergence values. reason accumulate sensitive step-size explains optimal step-size accumulate much smaller optimal step-size online λ-return algorithm sensitivity accumulate divergence demonstrated previous subsection known long. fact replace designed deal this. replace much robust respect divergence also limitations. obvious limitation applies binary features generally applicable. even domains replace applied perform poorly. reason replacing previous trace values rather adding reduces multi-step characteristics illustrate this consider two-state example shown left figure easy value left-most state state state representation consists single binary feature states terminal state. single feature state values cannot represented exactly. weight minimizes mean squared error assigns value states resulting error consider graph shown right figure shows asymptotic error diﬀerent values error accumulate converges least mean squares error predicted theory online λ-return algorithm convergence behaviour contrast replace converges value value reason behaviour single feature active time steps multi-step behaviour fully removed matter value hence replace behaves exactly value time steps. result also behaves like asymptotically. two-state example clearly demonstrates price payed replace achieve robustness respect divergence reduction multi-step behaviour. contrast online λ-return algorithm also robust divergence disadvantage. course two-state example well one-state example previous section extreme examples merely meant illustrate wrong. practise domain often characteristics one-state example two-state example negatively impacts performance accumulate replace online λ-return algorithm impractical many domains memory uses well computation required time step increases linearly time. fortunately possible rewrite update equations online λ-return algorithm diﬀerent update equations implemented computational complexity independent time. fact alternative update equations diﬀers update equations accumulate extra terms computed eﬃciently. algorithm implementing equations called true online discussed below. online λ-return algorithm time step sequence updates performed. length sequence hence computation time step increases time. however possible compute weight vector resulting sequence time step directly weight vector resulting sequence time step results following update equations compared accumulate trace update t−φt) td-error time-step correction trace; call term simply δ-correction. algorithm shows pseudocode implements equations. terms computation time true online higher cost extra terms accounted for. computation-time complexity step number features— actual computation time close twice much cases. cases computation time true online fraction accumulate/replace terms memory true online cost accumulate/replace using time-dependent step-size pseudocode section reasons explained section requires modiﬁed trace update. pseudocode pseudocode seijen sutton section number examples shown online λ-return algorithm outperforms accumulate/replace true online simply eﬃcient implementation online λ-return algorithm true online outperform accumulate/replace examples well. cases performance diﬀerence. example follows theorem appropriately small step-sizes used diﬀerence online λ-return algorithm/true online accumulate negligible. section identify factors aﬀect whether performance diﬀerence. focus section performance diﬀerence rather performance advantage experiments show true online performs always least well accumulate replace words experiments suggest whenever performance diﬀerence favour true online close features time steps. case diﬀerent versions perform similarly. follows equation case long time delay time steps feature non-zero value. speciﬁcally always least time steps subsequent times feature summarizing analysis order performance diﬀerence suﬃciently large features non-zero value often within small time frame. based summary address related question type domains performance diﬀerence true online optimized parameters accumulate/replace optimized parameters. ﬁrst conditions suggest domain result relatively large optimal optimal typically case domains relatively variance return. last condition satisﬁed multiple ways. example satisﬁed domains non-sparse feature vectors true online policy evaluation methods. however turned control methods straightforward way. learning perspective main diﬀerence prediction expected return conditioned state action rather state. means estimate action-value function learned rather state-value function another diﬀerence instead ﬁxed policy generates behaviour policy depends action-value estimates. estimates typically improve time policy. control counterpart popular sarsa algorithm. control counterpart true online ‘true online sarsa’. algorithm shows pseudocode true online sarsa. ensure accurate estimates state-action values obtained typically exploration strategy used. simple often suﬃcient strategy \u0001-greedy behaviour policy. given current state probability random action selected probability greedy action selected action-feature vector estimate time step common derive action-feature vector state-feature vector involves action-feature vector size n|a| number state features number actions. action corresponds block features section contains main empirical study comparing well sarsa true online counterparts. method domain scan step-size trace-decay parameter performed optimal performance compared. section discuss results. ﬁrst series experiments used randomly constructed markov reward processes interpreted single action state. consequently policy possible. represent random -tuple consisting number states; branching factor standard deviation reward. constructed follows. potential next states particular state drawn total states random without replacement. transition probabilities states randomized well three diﬀerent mrps small number states larger number states larger number states branching factor stochasticity reward discount factor three mrps. evaluated using three diﬀerent representations. ﬁrst representation consists tabular features state represented unique standard-basis vector dimensions. second representation based binary features. binary representation constructed ﬁrst assigning indices states. then binary encoding state index used feature vector represent state. length feature vector determined total number states length length example binary feature vectors states respectively. finally third representation uses non-binary features. representation state mapped -dimensional feature vector value feature drawn normal distribution zero mean unit variance. feature values state drawn normalized feature vector unit length. generated feature vectors kept ﬁxed state. note replace cannot used representation replacing traces deﬁned binary features experiment performed scan speciﬁcally varied according varying steps steps addition varied steps steps initial weight vector zero vector domains. performance metric used mean-squared error respect solution early learning normalized error dividing initial weight estimate. experiment compared performance true online real-world data-set consisting sensorimotor signals measured human control electromechanical robot arm. source data series manipulation tasks performed participant amputation presented pilarski study amputee participant used signals recorded muscles residual limb control robot multiple degrees-of-freedom interactions kind known myoelectric control consistency comparison results used source data prediction learning architecture published pilarski total signals predicted grip force motor angle signals robot’s hand. speciﬁcally target prediction discounted signal time similar return predictions detail subject experimental setting found hebert general value functions nexting; sutton modayil possible used implementation code base pilarski data experiment consisted time steps recorded sensorimotor information sampled state space consisted tile-coded representation robot gripper’s position velocity recorded gripping force muscle contraction signals human user. standard implementation tile-coding used bins signal eight overlapping tilings single active bias unit. results state space features active given time. hashing used reduce space vector features presented learning system. signals normalized provided function approximation routine. discount factor predictions force angle results presented pilarski parameter sweeps conducted three methods. performance metric mean absolute return error time steps learning normalized dividing error figure shows performance angle well force predictions best value diﬀerent values appendix results values shown. relative performance replace accumulate depends predictive question asked. predicting robot’s grip force signal—a signal small magnitude rapid changes—replace better accumulate values larger however predicting robot’s hand actuator position smoothly ﬁnal experiment compared performance true online sarsa accumulate sarsa replace sarsa domain arcade learning environment called asterix. general testbed provides interface hundreds atari games. asterix domain agent controls yellow avatar collect ‘potion’ objects avoiding ‘harp’ objects potions harps move across screen horizontally. every time agent collects potion receives reward points every time touches harp looses life agent actions right down left combinations directions no-op action resulting actions total. game ends agent lost three lives minutes whichever comes ﬁrst. episode lengths vary hugely constructing fair performance metric non-trivial. example comparing average return ﬁrst episodes methods fair seen roughly amount samples episodes guaranteed domain. hand looking total reward collected ﬁrst samples also good metric negative reward associated dying. resolve this look return episode averaged ﬁrst samples. speciﬁcally metric consists average score episode learning hours addition averaged resulting number independent runs. mined preliminary parameter sweep). used discount factor \u0001-greedy exploration weight vector initialized zero vector. also bellemare take action frames. decreases algorithms running time avoids super-human reﬂexes. results shown figure domain optimal performance three versions sarsa similar. figure summarizes performance diﬀerent versions evaluation domains. speciﬁcally shows error method best settings error normalized dividing error behave lies parameter range optimized over normalized error never higher method/domain normalized error equal means setting higher either eﬀect error gets worse. either case eligibility traces eﬀective method/domain. overall true online clearly better accumulate replace terms optimal performance. speciﬁcally considered domain/representation error true online either smaller equal error accumulate/replace especially impressive given wide variety domains fact computational overhead true online small observed performance diﬀerences correspond well analysis section particular note less states mrps hence chance feature non-zero value within small time frame larger. analysis correctly predicts results larger performance diﬀerences. furthermore less stochastic hence smaller variance return. also here experiments correspond analysis predicts results larger performance diﬀerence. asterix domain performance three sarsa versions similar. accordance evaluation results showed size performance diﬀerence domain dependent. worst case performance true online method similar regular method. optimal performance factor determines good method also matters easy performance. detailed plots appendices reveal parameter sensitivity accumulate much higher true online replace clearly visible well experiments myoelectric prosthetic thing take away experiments. non-binary features replace applicable accumulate ineﬀective. however true online still able obtain considerable performance advantage respect demonstrates true online expands domains/representations eligibility traces eﬀective. could potentially far-reaching consequences. speciﬁcally using non-binary features becomes interesting. replacing traces applicable representations accumulating traces easily result divergence values. true online however non-binary features necessarily challenging binary features. exploring nonbinary representations could potentially improve performance true online domains myoelectic prosthetic asterix domain. appendix shown true online equations derived directly online forward view equations. using diﬀerent online forward views true online methods derived. sometimes small changes forward view like using time-dependent step-size result surprising changes true online equations. section look number variations. using time-dependent step-size base equation forward view deriving update equations following procedure appendix turns slightly diﬀerent trace deﬁnition appears. indicate trace using superscript ﬁxed step-size trace deﬁnition equal considered on-policy methods methods evaluate policy policy generates samples. however true online principle also applied oﬀ-policy methods evaluation policy diﬀerent behaviour policy. simple example consider watkins’s oﬀ-policy method evaluates greedy policy given arbitrary behaviour policy. combining accumulating traces error uses maximum state-action value successor state online forward-view perspective strategy watkins’s method interpreted growing update target stops growing non-greedy action taken. speciﬁcally ﬁrst time step time step non-greedy action algorithm shows pseudocode true online method corresponds update target deﬁnition. problem watkins’s behaviour policy diﬀerent greedy policy traces reset often reducing overall eﬀect traces. section discuss advanced oﬀ-policy methods. tabular features special case linear function approximation. hence update equations true online presented also apply tabular case. however discuss separately simplicity special case provide extra insight. interesting tabular case dutch-trace update reduces particularly simple form. fact tabular case dutch-trace update equal weighted average accumulating-trace update replacing-trace update since ﬁrst publication true online several related papers published extending underlying concepts improving presentation. sections review papers. section discuss variations episode. consequence work regarding equivalence forward view backward view traditionally focused ﬁnal weight vector changed papers introduced online forward view corresponding backward view exact equivalence moment time papers introduced online forward view forward views presented diﬀerent other. diﬀerence forward view introduced seijen sutton on-policy forward view whereas forward view sutton oﬀ-policy forward view. however even fundamental diﬀerence related forward views constructed. particular forward view seijen sutton constructed moment time weight vector interpreted result sequence updates form multi-step error. course diﬀerent forward views also result diﬀerent backward views. whereas backward view sutton uses generalized version accumulating trace backward view seijen sutton introduced completely type trace. advantage forward view based instead results much stable updates. particular sensitivity divergence accumulate general side-eﬀect whereas much robust respect divergence. result true online property exact equivalence online forward view times consistently dominates empirically. strong performance true online motivated hasselt construct oﬀ-policy version forward view true online corresponding backward view resulted algorithm true online empirically outperforms gtd. also introduced term ‘dutch traces’ eligiblity trace. hasselt sutton showed dutch traces useful learning. oﬄine setting bootstrapping using dutch traces result certain computational advantages. understand consider monte carlo algorithm updates state values episode using full return update target. requires storing feature vectors rewards episode memory complexity linear length episode. moreover required computation time distributed unevenly episode almost computation required episode huge peak computation time updates need performed. dutch traces alternative implementation made results ﬁnal weight vector require storing feature vectors required computation time spread evenly time steps. hasselt sutton refer appealing property span-independence task ﬁnding eﬃcient backward view corresponds exactly particular online forward view easy. moreover guarantee exists eﬃcient implementation particular online forward view. often minor changes forward view determine whether eﬃcient backward view constructed. created desire somehow automate process constructing eﬃcient backward view. seijen sutton provide direct derivation backward view update equations; simply proved forward view backward view equations result weight vectors. sutton ﬁrst attempt come general strategy deriving backward view hasselt took approach providing theorem proves equivalence general forward view corresponding general backward view. showed forward/backward view true online special case general forward/backward view. showed oﬀ-policy method introduced—true online gtd. recently mahmood sutton extended theorem proving equivalence even general forward view backward view. furthermore hasselt sutton derived backward views series increasingly complex forward views. derivation true online equations appendix similar derivations. linear update equations online forward view presented section easily extended case non-linear function approximation. unfortunately appears impossible construct eﬃcient backward view exact equivalence case non-linear function approximation. reason derivation appendix makes fact gradient respect value function independent weight vector; hold non-linear function approximation. fortunately seijen shows many beneﬁts true online learning also achieved case non-linear function approximation using alternative forward view alternative forward view fully online implemented eﬃciently. several variations treated article suggested literature. schapire warmuth introduced variation upper lower bounds performance derived proven. konidaris introduced parameter-free alternative based multi-step update target called γ-return. oﬄine algorithm computational cost proportional episode-length. furthermore thomas proposed method based multi-step update target call ω-return. ω-return account correlation diﬀerent length returns something λ-return γ-return cannot. however expensive compute open question whether eﬃcient approximations exist. tested hypothesis true online dominates accumulating well replacing traces performing experiments wide range domains. extensive results support hypothesis. terms learning speed true online often better never worse either accumulating replacing traces across domains/representations tried. analysis showed especially domains non-sparse features relatively variance return large diﬀerence learning speed expected. generally true online advantage replacing traces used non-binary features advantage accumulating traces less sensitive respect parameters. terms computation time slight advantage. worst case true online twice expensive. typical case sparse features fractionally expensive memory requirements same. finally outlined approach deriving true online methods based rewriting equations online forward view. lead interesting methods future. authors thank hado hasselt extensive discussions leading reﬁnement ideas. furthermore authors thank anonymous reviewers valuable suggestions resulting substantially improved presentation. work supported grants alberta innovates technology futures national science engineering research council canada. computing resources provided compute canada westgrid. figure results random mean squared error averaged ﬁrst time steps well runs normalized using initial error. graphs summarize results graphs show error best step-size. figure results prosthetic data single amputee subject described pilarski prediction servo motor angle grip force recorded amputee’s myoelectrically controlled robot grasping task.", "year": 2015}