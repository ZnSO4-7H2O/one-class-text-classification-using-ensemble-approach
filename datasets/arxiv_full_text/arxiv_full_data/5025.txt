{"title": "Eigenoption Discovery through the Deep Successor Representation", "tag": ["cs.LG", "cs.AI"], "abstract": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.", "text": "marlos machado∗ clemens rosenbaum xiaoxiao miao gerald tesauro murray campbell university alberta edmonton canada university massachusetts amherst research yorktown heights options reinforcement learning allow agents hierarchically decompose task subtasks potential speed learning planning. however autonomously learning effective sets options still major challenge ﬁeld. paper focus recently introduced idea using representation learning methods guide option discovery process. speciﬁcally look eigenoptions options obtained representations encode diffusive information environment. extend existing algorithms eigenoption discovery settings stochastic transitions handcrafted features available. propose algorithm discovers eigenoptions learning non-linear state representations pixels. exploits recent successes deep reinforcement learning literature equivalence between proto-value functions successor representation. traditional tabular domains provide intuition approach atari games demonstrate potential. sequential decision making usually involves planning acting learning temporally extended courses actions different time scales. reinforcement learning framework options well-known formalization notion actions extended time; shown speed learning planning appropriately deﬁned spite that autonomously identifying good options still open problem. problem known problem option discovery. option discovery received ample attention many years varied solutions proposed recently machado vezhnevets proposed idea learning options traverse directions latent representation environment. paper explore idea. speciﬁcally focus concept eigenoptions options learned using model diffusive information environment. shown improve agents’ performance reducing expected number time steps uniform random policy needs order traverse state space. eigenoptions deﬁned terms proto-value functions basis functions learned environment’s underlying state-transition graph. pvfs eigenoptions deﬁned thoroughly evaluated tabular case. currently eigenoptions used environments infeasible enumerate states linear representation states known beforehand. paper extend notion eigenoptions stochastic environments non-enumerated states commonly approximated feature representations. despite methods learn representations generally ﬂexible scalable often leading better performance current algorithms eigenoption discovery cannot combined representation learning. introduce algorithm capable discovering eigenoptions learning representations. learned representations implicitly approximate model diffusive information environment. exploiting equivalence pvfs successor representation notably using also start able deal stochastic transitions naturally limitation previous algorithms. evaluate algorithm tabular domain well atari games. tabular domain provide intuition algorithm compare algorithms literature. evaluation atari games provides promising evidence applicability algorithm setting representation agent’s observation learned pixels. section discuss reinforcement learning setting options framework options known eigenoptions. also discuss successor representation main concept used proposed algorithm. consider reinforcement learning problem learning agent interacts unknown environment order maximize reward signal. often formalized markov decision process described -tuple time agent state takes action leads next state according transition probability kernel agent also observes reward generated function agent’s goal learn policy maximizes expected discounted return paper interested class algorithms determine agent’s policy greedy respect estimates value functions; either w.r.t. state value w.r.t. πqπ. notice large problems estimates approximated infeasible learn value state-action pair. generally done parameterizing weights currently neural networks successful parametrization approach ﬁeld better known instantiations idea algorithm called deep q-network uses neural network estimate state-action value functions pixels. options main topic study. temporally extended actions allow represent courses actions. option -tuple πωtω denotes option’s initiation denotes option’s policy denotes option’s termination set. consider call-and-return option execution model meta-policy dictates agent’s behavior agent decides follow option state actions selected according agent reaches state interested learning scratch. denotes feature representation given state denotes eigenvector encoding model speciﬁc timescale. intrinsic reward function deﬁned eigenvector used incentivizes agent traverse different latent dimension state space. tabular case algorithms capable learning eigenoptions encode model combinatorial graph laplacian d−/d−/ graph’s weight matrix diagonal matrix whose entries sums weight matrix figure successor representation respect uniform random policy state example similar dayan’s color represents larger values blue color represents smaller values square matrix ij-th entry represents connection states notice approach naturally deal stochastic unidirectional transitions generally deﬁned symmetric adjacency matrix. importantly eigenvectors also known proto-value functions settings states cannot enumerated model represented matrix transitions encoding transition vector denotes ﬁxed linear feature representation known beforehand machado justiﬁes sampling strategy fact that tabular case every transition sampled once right eigenvectors matrix converge pvfs. transitions added once regardless frequency algorithm well suited stochastic environments. paper introduce algorithm naturally deals stochasticity require known beforehand. algorithm learns environment’s model learning representation environment pixels. successor representation determines state generalization similar successor states are. deﬁned expected future occupancy state given agent’s policy starting state seen deﬁning state similarity terms time. figure example. euclidean distance state state smaller euclidean distance state state however considers gray tiles walls agent state reach state much quicker state captures distinction ensuring state similar state state denote indicator function formally deﬁned directly related several ideas ﬁeld. seen dual approach dynamic programming value-function based methods reinforcement learning moreover eigenvectors generated eigendecomposition equivalent proto-value functions slow feature analysis equivalences play central role algorithm describe next section. also important role neuroscience. stachenfeld recently suggested successor representation encoded hippocampus low-dimensional basis representing encoded enthorhinal cortex. interestingly hippocampus entorhinal cortex believed part brain system responsible spatial memory navigation. order discover eigenoptions ﬁrst need obtain eigenpurposes eigenvectors encoding model environment. currently done pvfs agent obtains either explicitly building environment’s adjacency matrix enumerating environment’s transitions approach fairly effective deterministic settings states enumerated uniquely identiﬁed i.e. tabular case. however obvious extension approach stochastic settings. hard agent explicitly model environment dynamics weight matrix. existent alternative enumerate environment’s transitions large cost. issues become worse states cannot enumerated i.e. function approximation case. existing algorithm applicable function approximation setting requires ﬁxed representation input able learn representation estimating model. paper introduce algorithm addresses aforementioned issues estimating model also introduce neural network capable approximating pixels learning latent representation game screens. learned used discover eigenoptions replacing need knowing combinatorial laplacian. section discuss proposed algorithm tabular case equivalence pvfs algorithm capable estimating eigenoptions pixels. general structure algorithms capable discovering eigenoptions fairly straightforward shown alg. agent learns representation captures model uses eigenvectors representation deﬁne eigenpurposes intrinsic reward functions described equation learn maximize. option’s policy maximizes reward function state deﬁned terminal respect eigenpurpose qei∗ initiation option deﬁned tei. tabular case proposed algorithm also fairly simple. instead assuming matrix given form graph laplacian trying estimate graph laplacian samples stacking vectors corresponding different observed transitions estimate model successor representation idea supported fact that purposes eigenvectors normalized laplacian eigenvectors equivalent. formalize concept discuss implications. show eigenvectors normalized laplacian equal eigenvectors scaled γ−d/. aforementioned equivalence ensures eigenpurposes extraction eigenoption learning steps remain unchanged. still obtain eigenpurposes eigendecomposition matrix still eigenvector deﬁne learning problem agent wants maximize eigenpurpose deﬁned equation importantly addresses limitations previous work deals stochasticity environment agent’s policy naturally; memory cost independent number samples drawn agent; assume every action another action agent take return state before i.e. symmetric. aforementioned pvfs equal eigenvectors successor representation scaled γ−d/. best knowledge equivalence ﬁrst explicitly discussed stachenfeld provide formal statement equivalence eingevalues eigenvectors approaches. proof discuss extent interchangeability. theorem. stachenfeld s.t. denotes matrix encoding d−/d−/ denote matrix corresponding normalized laplacian obtained uniform random policy. i-th eigenvalue j-th eigenvalue normalized laplacian related follows importantly using pvfs ﬁrst interested eigenvectors corresponding smallest eigenvalues smoothest ones. however using interested eigenvectors largest eigenvalues. change variables highlights fact i.e. )γ−]. indices sorted reverse order indices distinction important trying estimate relevant eigenvectors. finding largest eigenvalues/eigenvectors statistically robust noise estimation depend lowest spectrum matrix. moreover notice scaling change direction eigenvectors size action constant across states. often case problems studied. domains. real-world situations number states often large ability generalize recognize similar states essential. section inspired kulkarni al.’s al.’s work propose replacing alg. neural network able estimate successor representation pixels. approach circumvents limitations previous work required linear feature representation provided beforehand. non-enumerated states originally deﬁned function approximation setting states described terms feature vectors. successor features natural extension setting. barreto al.’s deﬁnition successor features denotes successor feature state following policy words encodes discounted expected value i-th feature vector agent starts state follows policy update rule presented naturally extended deﬁnition. temporal-difference error update rule used differentiable loss function allowing estimate successor features neural network. neural network architecture architecture used depicted reconstruction module introduced augmented estimator estimator uses learned latent representation input i.e. output representation learning module. proposed neural network receives pixels input learns estimate successor features lower-dimension representation learned neural network. loss function learn successor features denotes feature vector encoding learned representation state denotes estimated successor features. practice output representation learning module output estimator shown fig. loss function also highlights fact neural networks. represent target network updated slower rate stability purposes. cannot directly estimate successor features pixels using zero ﬁxed points. reason added al.’s reconstruction module proposed network. behaves auxiliary task predicts next state observed given current state action. predicting next state increase likelihood agent learn representation takes consideration pixels control shown good bias problems auxiliary task deﬁned network’s reconstruction error finally ensure interfere learned features zero gradients coming estimator trained model rmsprop followed protocol used initialize network. eigenoption learning alg. function extracteigenpurposes returns eigenpurposes described eigenpurposes deﬁned terms feature representation environment eigenvectors model trained network generate both. trivial obtain output appropriate layer network feature representation. obtain ﬁrst need generate meaningful matrix since network outputs vector successor features instead matrix. agent follow uniform random policy store network outputs correspond network estimate successor features state create matrix corresponds deﬁne right eigenvectors. created eigenpurposes option discovery problem reduced regular problem agent aims maximize cumulative rewards. learning algorithm used that. provide details approach next section. evaluate discovered eigenoptions quantitatively qualitatively section. traditional rooms domain evaluate impact eigenvectors discovered options approximating model atari games demonstrate proposed network discover purposeful options pixels. ﬁrst experiment evaluates impact estimating samples instead assuming model given form normalized laplacian. rooms domain evaluate method. fig. depicts ﬁrst eigenvector obtained fig. depicts corresponding eigenoption. followed uniform random policy episodes learn episodes time steps long. used stepsize estimated eigenvector fairly close true expected obtained eigenvector fairly similar pvfs obtained domain. appendix provide plots true well plots different eigenvectors comparing obtained eigenoptions known improving agent’s ability explore environment. metric diffusion time validate whether ability preserved method. diffusion time seen proxy hard agent reach goal state following uniform random policy. deﬁned expected number decisions agent needs take following uniform random policy navigate randomly chosen states. compared agent’s diffusion time using eigenoptions obtained pvfs diffusion time using eigenoptions obtained estimates eigenoptions obtained help agent explore environment. figure different environments used evaluation well learning curves obtained environments different number options obtained estimated episodes. text details. diffusion time using pvfs using likely different ways dealing corners. implicitly models self-loops states adjacent walls since agent takes action observes move. also evaluated estimates evolve episodes used learning impact diffusion time appendix present results showing local structure graph generally preserved. naturally episodes allow learn accurate estimates global facet environment seen since agent chances explore state space. however seems even learned episodes allow discover useful eigenoptions depicted fig. eigenoptions obtained learned using episodes already capable reducing agent’s diffusion time considerably. finally important stress discovered options randomly selecting subgoal states. random options reduce agent’s diffusion time hundreds added agent’s action finally evaluated discovered eigenoptions maximize reward. experiments agent learned off-policy greedy policy primitive actions following uniform random policy actions eigenoptions used qlearning experiments parameters before episodes time steps long. figure summarizes obtained results comparing performance approach regular q-learning primitive actions. eigenoptions extracted estimates obtained episodes. reported results average independent runs learning runs encoding runs evaluating q-learning. options added following sorting provided eigenvalues. example options denotes agent action used behavior policy composed four primitive actions four eigenoptions generated eigenvalues notice results take sample efﬁciency approach consideration meant showcase eigenoptions discovered speed learning. sample complexity learning options generally justiﬁed lifelong learning settings re-used multiple tasks beyond scope paper. obtained results clearly show eigenoptions capable reducing diffusion time environment also improving agent’s control performance. increasing likelihood agent cover larger part state space given amount time. moreover before seems accurate estimate successor representation necessary eigenoptions useful. similar results obtained different locations start goal states estimates accurate. results seen appendix. figure plots density state visitation eigenoptions discovered three atari games. states visited frequently show darker images avatar. note eigenoption’s overwhelming mass visitations corresponds terminal state disparate options different terminal states. followed protocol described previous section create eigenpurposes. trained network fig. estimate uniform random policy. since network impact policy followed built dataset samples game used dataset optimize network weights. passed shufﬂed dataset times using rmsprop step size done training agent follow uniform random policy steps stored output network observed state matrix deﬁne eigenpurposes maximize right eigenvectors matrix extracted time step network fig. computational constraints approximated ﬁnal eigenoptions. using ale’s internal emulator one-step lookahead greedily respect eigenpurpose ideal options obtain quite limited since deal delayed rewards. however even limiting setting able obtain promising results discuss below. following machado evaluate discovered eigenoptions qualitatively. execute options following procedure described tracking avatar’s position screen. figure summarizes behavior meaningful options discovered. trajectories generated different options represented different colors color’s intensity given location represents often agent location. eigenoptions introduced options generate purposeful behavior help agents explore environment. clearly discovered eigenoptions indeed purposeful. reach speciﬁc location stay there. case agent’s trajectory would much visible. instead actually observe mass visitation concentrated location screen dominating others. location agent spending time fact seen option’s terminal state. constantly state suggests agent arrived myopic local maximum eigenpurpose. three four games algorithm discovers options clearly push agent corners relevant parts state space corroborating intuition eigenoptions also improve exploration. montezuma’s revenge terminal state highlighted options even correspond considered good subgoals game likely additional subgoals found myopic greedy approach. approach also explain algorithm ineffective freeway. avoiding cars impossible without longer-term planning. plot depicting meaningful options discovered game appendix. importantly fact myopic policies able navigate speciﬁc locations stay also suggests that tabular case proposed approach gives rise dense intrinsic rewards informative. another important constrast randomly assigned subgoals approach. randomly assigned subgoals give rise dense rewards. thus argue approach generate useful options also gives rise dense eigenpurposes making easier build policies associated them. important stress algorithm able discover eigenoptions pixels similar obtained algorithms state game feature representation. state game often uses speciﬁc bytes encode important information game position player’s avatar game. algorithm implicitly learn meaningful parts screen. also different previous algorithms approach constrained dimensionality state representation binary features. based discussion consider results promising even though depict options effect initial state games. believe general setting algorithm potential discover even better options. work directly inspired kulkarni ﬁrst propose approximating using neural network. loss function novel architecture. directly using control deﬁne terms states instead state-action pairs. different kulkarni network learn reward model autoencoder learn representation world. tries predict next state agent observe. prediction module used introduced predicts next state implicitly learns representations take consideration parts screen agent’s control. ability recognize features known contingency awareness known potential improve agents’ performance kulkarni suggest deep could used bottleneck states commonly used subgoals options idea explored. importantly jong machado shown options look bottleneck states quite harmful learning process. idea explicitly building hierarchies based learned latent representation state space machado vezhnevets machado proposed concept eigenoptions limited linear function approximation case. vezhnevets explicitly build options initiation termination sets. instead learn hierarchy end-to-end learning system allow easily retrieve options finally kompella proposed slow feature analysis discover options. sprekeler shown that given speciﬁc choice adjacency function pvfs equivalent sfa. however work limited linear function approximation. method also differs deﬁne initiation termination sets. options discover look bottleneck states case. paper introduced algorithm eigenoption discovery algorithm uses successor representation estimate model diffusive information environment leveraging equivalence proto-value functions approach circumvents several limitations previous work builds increasingly accurate estimates using constant-cost update-rule; naturally deals stochastic mdps; depend assumption transition matrix symmetric; depend handcrafted feature representations. ﬁrst three items achieved simply using instead pvfs latter achieved using neural network estimate proposed framework opens multiple possibilities investigation future. would interesting evaluate compositionality eigenoptions transferable similar environments different modes atari games finally fundamental algorithms introduced would interesting investigate whether eigenoptions accumulate rewards instead using exploration. authors would like thank craig sherstan martha white feedback earlier draft kamyar azizzadenesheli marc bellemare michael bowling useful discussions anonymous reviewers feedback suggestions. andr´e barreto dabney r´emi munos jonathan hunt schaul david silver hado hasselt. successor features transfer reinforcement learning. advances neural information processing systems marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research ozg¨ur s¸imsek andrew barto. using relative novelty identify useful temporal abstractions reinforcement learning. proc. international conference machine learning zhaohan daniel philip thomas emma brunskill. using options covariance testing long horizon off-policy policy evaluation. advances neural information processing systems jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. proc. international conference learning representations nicholas jong todd hester peter stone. utility temporal abstraction reinforcement learning. proc. international joint conference autonomous agents multiagent systems varun kompella marijn stollenga matthew luciw j¨urgen schmidhuber. continual curiosity-driven skill acquisition high-dimensional video inputs humanoid robots. artiﬁcial intelligence george konidaris andrew barto. skill discovery continuous reinforcement learning domains using skill chaining. advances neural information processing systems tejas kulkarni karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems marlos machado marc bellemare erik talvitie joel veness matthew hausknecht michael bowling. revisiting arcade learning environment evaluation protocols open problems general agents. journal artiﬁcial intelligence research press sridhar mahadevan mauro maggioni. proto-value functions laplacian framework learning representation control markov decision processes. journal machine learning research mcgovern andrew barto. automatic discovery subgoals reinforcement learning using diverse density. proc. international conference machine learning volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature alec solway carlos diuk natalia c´ordova debbie andrew barto yael matthew botvinick. optimal behavioral hierarchy. plos computational biology kimberly stachenfeld matthew botvinick samuel gershman. design principles hippocampal cognitive map. advances neural information processing systems alexander sasha vezhnevets simon osindero schaul nicolas heess jaderberg david silver koray kavukcuoglu. feudal networks hierarchical reinforcement learning. proc. international conference machine learning wang bowling schuurmans. dual representations dynamic programming reinforcement learning. proc. ieee international symposium approximate dynamic programming reinforcement learning detailed proof theorem main paper theorem. stachenfeld s.t. denotes matrix encoding d−/d−/ denote matrix corresponding normalized laplacian obtained uniform random policy. i-th eigenvalue j-th eigenvalue normalized laplacian related follows section brieﬂy discussed impact estimating successor representation samples instead assuming agent access normalized laplacian. makes much sense successor representation model environment estimate quickly. diffusion time main evidence used section support claim early estimates successor representation useful eigenoption discovery. order concise actually plot eigenvectors estimates successor representation different moments explicitly compared proto-value functions eigenvectors matrix section. figures depict ﬁrst four eigenvectors successor representation rooms domain learned different number episodes also depict corresponding eigenvectors matrix normalized laplacian eigenvectors orientation often arbitrary eigendecomposition matched orientation ease visualization. overall episodes already almost perfect estimate ﬁrst eigenvectors environment; episodes seem enough accurately learn model rooms. however learning successor representation episodes seems enough generate eigenoptions reduce agent’s diffusion time show figure better discuss behavior looking figures depict options generated obtained eigenvectors. exception options generated learning successor representation episodes eigenoptions obtained estimates successor representation already move agent towards correct room. naturally always corners general structure policies clearly seen. also observe eigenoptions obtained proto-value functions shifted tile corners. discussed main paper consequence machado al.’s dealt corners. model selfloops despite fact agent state consecutive steps. successor representation captures naturally. finally figure speculate options learned episodes capable reducing agent’s diffusion time. ﬁrst eigenoption learned agent moves parts state space never reason combination options effective. also suggests incremental methods option discovery exploration promising path future work. section also evaluated agent’s ability accumulate reward eigenoptions learned. analyze topic here. section agent learned off-policy greedy policy primitive actions following uniform random policy actions eigenoptions used q-learning experiments parameters episodes time steps long. figures summarize obtained results comparing performance approach regular q-learning primitive actions four different environments evaluate agent’s performance using eigenoptions extracted estimates obtained episodes well eigenoptions obtained true i.e. reported results average independent runs learning runs encoding runs evaluating q-learning. options added following sorting provided eigenvalues. example options denotes agent action used behavior policy composed four primitive actions four eigenoptions generated eigenvalues eigenoptions capable reducing diffusion time environment also improving agent’s control performance. increasing likelihood agent cover larger part state space given amount time. interestingly eigenoptions seem enough agent. moreover although rough estimates seem enough improve agent’s performance accurate predictions able improve agent’s performance mainly dozens eigenoptions used. ﬁrst eigenoptions accurately estimated larger eigenvalues ones ﬁrst. section analyzed eigenoptions able discover four games arcade learning environment. discuss performance proposed network auxiliary tasks deﬁned. here. figures depict comparison target screen predicted network’s actual prediction time steps game. accurately predicts general structure environment able keep track moving sprites screen. prediction quite noisy different al.’s result. still interesting even underperforming network able learn useful representations algorithm. likely better representations would result better options. figure depicts meaningful eigenoptions able discover game freeway. figure option represented normalized count avatar’s position screen trajectory. trajectories generated different options represented different colors color’s intensity given location represents often agent location. figure final -step predictions game montezuma’s revenge. task predicting next game screen auxiliary task estimating successor representation.", "year": 2017}