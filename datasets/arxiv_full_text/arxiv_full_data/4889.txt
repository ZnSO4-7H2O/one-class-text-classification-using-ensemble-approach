{"title": "Quantile Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.", "text": "paul weng school electronics information technology sysu-cmu joint institute engineering sysu-cmu shunde joint research institute guangzhou china reinforcement learning standard criterion evaluate policies state expectation rewards. however criterion always suitable consider alternative criterion based notion quantiles. case episodic reinforcement learning problems propose algorithm based stochastic approximation timescales. evaluate proposition simple model show wants millionaire. keywords reinforcement learning quantile ordinal decision model two-timescale stochastic approximation markov decision process reinforcement learning powerful frameworks building autonomous agents systems make decisions without human supervision order perform given task. examples systems abound expert backgammon player dialogue systems acrobatic helicopter ﬂight human-level video game player however standard framework assumes rewards numeric scalar additive policies evaluated expectation criterion. practice happen numerical rewards available instance agent interacts human generally gives ordinal feedback besides even numerical information available want optimize criterion diﬀerent expectation instance one-shot decision-making. several works considered case preferences qualitative. markov decision processes ordinal reward investigated diﬀerent ordinal decision criteria proposed context. generally preferencebased reinforcement learning proposed tackle situations available preferential information concerns pairwise comparisons histories. paper propose search policy optimizes quantile instead expectation. intuitively -quantile distribution value probability getting value lower cloud computing services amazon reports optimize .%quantile. fact decisions industry often made based quantiles generally service industry skewed distributions generally want customers satisﬁed average rather customers satisﬁed possible. preferences uncertainty valued scales commensurable preferences actions trajectories expressed purely ordinal scale preferences policies robust standard criterion maxicontributions paper follows. best knowledge ﬁrst propose algorithm learn policy optimal quantile criterion. algorithm based stochastic approximation timescales. present empirical evaluation proposition version wants millionaire. paper organized follows. section presents related work. section recalls necessary background presenting approach. section states problem solve introduce algorithm quantile q-learning. section presents experimental results. finally conclude section great deal research mdps considered decision criteria diﬀerent standard ones instance operations research community white notably considered diﬀerent cases preferences policies depend sums rewards expected utility probabilistic constraints mean-variance formulations. context showed suﬃciency working state space augmented rewards obtained far. filar investigated decision criteria variance-penalized versions standard ones. formulated obtained optimization problem non-linear program. optimized probability total reward becomes higher certain threshold. functional variation value iteration. continuation work gilbert investigated skew-symmetric bilinear utility functions generalization enables intransitive behaviors violation independence axiom decision criteria ﬁnite-horizon mdps. interestingly also encompasses probabilistic dominance decision criterion employed preference-based sequential decision-making theoretical computer science sophisticated decision criteria also studied mdps. instance gimbert proved many decision criteria based expectation admit stationary deterministic optimal policy. bruy`ere considered sophisticated preferences policies amounts searching policies maximize standard criterion ensuring expected rewards higher threshold probability higher ﬁxed value. work also extended multiobjective setting recent work markov decision process reinforcement learning considered conditional value-at-risk criterion related quantile risk measure. b¨auerle proved existence deterministic wealth-markovian policies optimal respect cvar. chow ghavamzadeh proposed gradient-based algorithms cvar optimization. contrast borkar jain used cvar constraints instead objective function. closer work several quantile-based decision models investigated diﬀerent contexts. uncertain mdps parameters transition reward functions imprecisely known delage mannor presented investigated quantile-like criterion capture trade-oﬀ optimistic pessimistic viewpoints uncertain mdp. quantile criterion diﬀerent takes account uncertainty present parameters mdp. mdps ordinal rewards quantile-based decision models proposed compute policies maximize quantile using linear programming. quantiles works deﬁned distributions ordinal rewards quantiles paper deﬁned distributions histories. recently machine learning community quantile-based criteria proposed multi-armed bandit setting special case reinforcement learning. nikolova proposed algorithm pure exploration setting diﬀerent risk measures including value-at-risk. carpentier valko studied problem identifying arms extreme payoﬀs particular case quantiles. finally sz¨or´enyi investigated problems quantile optimized instead mean. algorithm propose based stochastic approximation timescales technique proposed borkar method recently exploited achievability problems context multiobjective mdps learning ssb-optimal policies call t-history succession state-action pairs starting state action choices agent guided policy. formally policy horizon sequence decision rules decision rules prescribe action agent perform given time step. markovian depend current state. besides decision rule either deterministic always selects action given situation randomized prescribes probability distribution possible actions. consequently policy markovian deterministic randomized according type decision rules. lastly policy stationary applies decision rule every time step i.e. policies compared respect diﬀerent decision criteria. usual criterion expected rewards optimal deterministic markovian policy known exist horizon criterion deﬁned follows. first value history described rewards obtained along i.e. expectation cumulated rewards obtained agent performs action state time-step continues follow policy thereafter. higher values better. therefore value functions induce preference relation policies following reinforcement learning setting assumption knowledge environment relaxed dynamics transition function preferences reward function known anymore. interacting environment agent tries learn good policy trial error. make ﬁnite horizon mdps learnable assume decision process repeated inﬁnitely many times. horizon reached assume agent automatically returns initial state problem starts over. standard decision criteria used mdps based expectation reasonable situations. firstly unfortunately many cases reward function known. cases recover reward function human expert however even expert user elicitation reward function reveal burdensome. inverse reinforcement learning expert assumed know optimal policy rarely true practice. interactive settings elicitation process cognitively complex requires balance several criteria complex manner imply large number parameters. paper address problem assuming strict weak ordering histories known. secondly numerous applications expectation cumulated reward used equation appropriate criterion instance case high variance policy known applied times solution given criterion satisfying risk-averse agent. moreover domains decisions performance often based minimal quality possible outcomes. therefore article using quantile decision criterion solve mdp. ﬁnite states ﬁnite maximal horizon call episode history starting ending ﬁnal state assume preference relation deﬁned states write state preferred state without loss generality assume states ordered increasing preference i.e. weak relation denoted note ﬁnite horizon reformulated states state augmentation. although resulting large-sized state space models formally equivalent. focus episodic mdps states simplify presentation approach. deﬁne quantiles distributions states ordered ﬁxed parameter. intuitively -quantile distribution states value probability getting state equal lower getting state equal greater .-quantile also known median seen ordinal counterpart mean. -quantile minimum distribution. generally quantiles axiomatically characterized rostek deﬁne decision criteria nice property requiring numeric valuations order. value. deﬁned construction values equal deﬁned equal them. instance always case continuous settings continuous distributions. however discrete setting could happen values diﬀer shown example quantile criterion diﬃcult optimize even numerical reward function given quality episode deﬁned cumulative rewards received along episode. diﬃculty comes notably related sources non-dynamically consistent preferences debatable adopt consequentialist approach sequence decisions lead dominated results. paper adopt resolute choice point view. leave third approach future work. section ﬁrst state problem solved paper useful properties. then present algorithm called quantile q-learning extension q-learning exploits two-timescale stochastic approximation technique. instead optimal lower quantile otherwise happen cumulative distribution non-optimal policy smaller equal optimal policy optimal). case lower thing cannot happen upper quantile. sense determining optimal policy upper quantile easier. lemma simply given solving amounts minimizing probability ending ﬁnal state strictly less preferred solves equation similarly solving amounts maximizing probability ending ﬁnal state least preferred remaining paper present solve upper quantile. similar order optimal upper approximates probability reaching state whose index least high value smaller means high decreased. otherwise optimal integrate q-learning algorithm instead good policy learned searching correct value two-timescale technique q-learning update parameter concurrently diﬀerent speeds work parameter needs seen popular television game show contestant needs answer maximum multiple-choice questions increasing diﬃculty increasingly large sums roughly doubling question. time step contestant decide walk away money currently won. answers incorrectly winnings lost except earned guarantee point player allowed lifelines used once. used ﬁrst model spanish version game presented perea puerto probability answering correctly function question’s number increased lifelines used plot results obtained diﬀerent learning runs million learning steps million learning steps. θ-updates interleaved q-learning phase using ε-greedy exploration strategy learning rate check equation satisﬁed. optimize upper quantile learning process maintain vector frequencies ﬁnal state attained. deﬁne quantity score cross product vector rewards obtained attaining ﬁnal state given current value another score value non-stationary policy played since beginning learning process. moreover increases oscillations decreasing amplitude ever changing value. figures show evolution score number iterations increases. score converges towards value inferior. exploration q-learning algorithm. lastly plot evolution number iteration increases. value converges towards value presented algorithm learning policy optimal quantile criterion reinforcement learning setting special structure corresponds repeated episodic decision-making problems. based stochastic approximation timescales proposition experimentally validated domain wants millionaire. future work would interesting investigate choose learning rate order ensure fast convergence. moreover approach could extended settings episodic mdps. besides would also interesting explore whether gradient-based algorithms could developed optimization quantiles based fact quantile solution optimization problem objective function piecewise linear", "year": 2016}