{"title": "Approximation Algorithms for Cascading Prediction Models", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost. Applied to state-of-the-art ImageNet classification models, this yields up to a 2x reduction in floating point multiplications, and up to a 6x reduction in average-case memory I/O. The auto-generated cascades exhibit intuitive properties, such as using lower-resolution input for easier images and requiring higher prediction confidence when using a computationally cheaper model.", "text": "present approximation algorithm takes pool pre-trained models input produces cascaded model similar accuracy lower average-case cost. applied state-of-the-art imagenet classiﬁcation models yields reduction ﬂoating point multiplications reduction average-case memory i/o. auto-generated cascades exhibit intuitive properties using lower-resolution input easier images requiring higher prediction conﬁdence using computationally cheaper model. machine learning task examples harder others intuitively able away less computation easier examples. potential reduce serving costs cloud well energy usage on-device important wide variety applications following tremendous empirical success deep learning much recent work focused making deep neural networks adaptive typically end-to-end training approach network learns make exampledependent decisions computations performed inference. time recent work neural architecture search demonstrated optimizing thousands candidate model architectures yield results improve upon state-of-the-art architectures designed humans natural think combining ideas lead even better results best remains open problem. motivations work many problems order-of-magnitude differences cost reasonably accurate model model state-of-the-art accuracy. example accurate nasnet model achieves accuracy imagenet using billion multiplies example mobilenet model achieves accuracy million multiplies example combinations. also allow computations performed stage cascade re-used later stages possible cascade-generation algorithm requires input abstaining models prediction models return don’t know certain examples. classiﬁcation problems models known classiﬁers reject option methods training widely studied section present simple post-processing approach used convert pre-trained model domain abstaining model. assume prediction model function performance judged taking expected value accuracy metric accuracy prediction true label goal create abstaining model returns examples accuracy returns otherwise. toward create model predict given typically model based values intermediate computations performed evaluating train model estimate value accuracy metric seeking achieve return predicted accuracy falls threshold. example multi-class classiﬁcation problem might entropy vector predicted class probabilities feature might one-dimensional isotonic regression predicts top- accuracy function entropy. rule would return examples entropy high. pseudo-code abstaining model given algorithm elsewhere distinguish algorithm’s parameters input variables. specifying values parameters deﬁnes function input variables example conﬁdentmodel denotes abstaining model based prediction model accuracy model threshold created abstaining models must next select sequence abstaining models cascade. goal generate cascade minimizes average cost measured validation subject accuracy constraint possible choices functions discussed sections respectively. presents theoretical results performance greedy algorithm. discusses modify greedy algorithm return adaptive policy tree rather linear cascade discusses integrating algorithm model architecture search. present greedy cascade-generation algorithm. already mentioned goal algorithm produce cascade minimizes cost subject accuracy constraint. high-level idea algorithm abstaining model maximizes number non-⊥ predictions unit cost considering abstaining models satisfy accuracy constraint subset examples return prediction. remove validation examples model returns prediction apply greedy rule choose next abstaining model continuing manner examples remain. greedy algorithm builds earlier approximation algorithms min-sum cover related problems primary difference algorithm must worry maintaining accuracy addition minimizing cost. also consider general notion cost reﬂecting possibility reusing intermediate computations. cascade returned greedy algorithm examples remaining start iteration observe implies disjoint construction holds uses make predictions examples implies holds well. thus accuracy constraint satisﬁed long implies ai). sufﬁcient condition accuracy constraint decomposable captured following deﬁnition. deﬁnition accuracy constraint decomposable disjoint sets example decomposable accuracy constraint minrelativeaccuracy constraint shown algorithm requires average accuracy according metric least times ﬁxed reference model. using constraint cascade returned greedy algorithm guaranteed overall accuracy least times reference model pref. consider circumstances greedy algorithm terminates successfully happens long accurate always non-empty. sufﬁcient condition accuracy constraint satisﬁable deﬁned follows. deﬁnition accuracy constraint satisﬁable respect abstaining model generator validation exists model never abstains always returned satisﬁes note minrelativeaccuracy simply requiring ﬁxed minimum average accuracy rather accuracy required depends reference model’s performance provided subset takes many different values running algorithm constraint requires accuracy qmin generally satisﬁable might contain examples models misclassify. allow computation reuse deﬁne weighted directed graph vertex model plus special vertex edge whose weight cost computing scratch. edge weight indicates computed cost already computed. cost function then {v∅}. example suppose models makes predictions computing output ﬁrst layers ﬁxed neural network case graph linear chain whose edge weight equal cost computing output layer given input. equation also generalized terms hypergraphs allow reuse multiple intermediate results. useful case ensemble models take weighted average models’ predictions. simple approach deﬁning take ﬁxed prediction models return conﬁdentmodel threshold high enough satisfy accuracy constraint illustrated algorithm another lightweight approach ensemble model makes already-computed predictions ﬁrst models. assume abstaining model backing prediction model never returns ﬁxed prediction models enj= βjpj optimized maximize accuracy remaining examples converted conﬁdentmodel manner above. section provide performance guarantees algorithm showing reasonable assumptions produces cascade satisﬁes accuracy constraint cost within factor optimal. also show even special cases problem ﬁnding cascade whose cost within factor optimal np-hard shown greedy algorithm return cascade satisﬁes accuracy constraint provided constraint decomposable satisﬁable. fairly weak assumption satisﬁed minrelativeaccuracy constraint given algorithm consider conditions cost function must satisfy subtle. guarantees hold linear cost functions well certain class functions allow limited form computation reuse. make precise following deﬁnitions. deﬁnition abstaining models dominates sequence {mi}k abstaining models respect cost function conditions hold cost function linear sequence abstaining models dominated corresponding set. deﬁnition cost function admissible respect abstaining models sequence models exists dominates linear cost function always admissible. cost functions form admissible certain conditions. sufﬁcient condition graph deﬁning cost function linear chain edge graph linear chain property make cost function admissible including additional models. speciﬁcally model computes output models order returned model maximum index. singleton dominate sequence composed models mk}. similar arguments apply graphs comprised multiple linear chains. also assume finally models deﬁne ∪m∈ma. following lemma shows that cost function admissible number examples cascade answer unit cost bounded maximum number examples single model answer unit cost. theorem uses inequality bound approximation ratio greedy algorithm. finally consider computational complexity optimization problem algorithm solves. given validation abstaining models accuracy constraint refer problem ﬁnding minimumcost cascade satisﬁes accuracy constraint minimum cost cascade. problem np-hard approximate even special cases summarized theorem theorem np-hard obtain approximation ratio minimum cost cascade. true even special case where cost function always returns accuracy constraint always satisﬁed. proof theorem along lines analysis related greedy algorithm generating task-switching schedule turn built elegant geometric proof technique developed feige theorem greedycascade. models accurate admissible respect accurate deﬁned proof. ﬁrst introduce notation. |ri| denote number examples remaining start iteration greedy algorithm cost abstaining model selected iteration. maximum beneﬁt/cost ratio iteration optimal cascade. proof theorem proved using reduction min-sum cover reduction element min-sum cover instance becomes example validation becomes prediction model iff. cost function minimum cost cascade instance always returns accuracy constraint always satisﬁed. greedy algorithm covering modiﬁed produce adaptive policy similar approach applied algorithm resulting algorithm similar algorithm instead building list abstaining models builds tree node tree labeled abstaining model edge labeled feature parent node’s output function satisﬁes technical condition called adaptive monotone submodularity resulting algorithm approximation guarantee analogous stated theorem respect best adaptive policy rather merely best linear sequence. shown combining proof technique golovin krause proof theorem unfortunately function guaranteed property general. however shown adaptive version algorithm still guarantees described theorem greedycascade algorithm integrated model architecture search multiple ways. would simply take models evaluated architecture search input greedy algorithm. potentially much powerful approach architecture search model generator used greedy algorithm’s inner loop. approach architecture search stage generated cascade. goal search maximize beneﬁt/cost ratio criterion used iteration greedy algorithm search needs consider examples already classiﬁed ﬁrst stages later searches potentially lower training cost. furthermore model make intermediate layers ﬁrst models input features allowing computations reused across stages cascade. section evaluate cascade generation algorithm applying state-of-the-art pre-trained models imagenet classiﬁcation task. ﬁrst examine efﬁcacy abstention rules described evaluate full cascade-generation algorithm. accuracy versus abstention rate discussed decide whether model abstain making prediction training second model predict accuracy given example checking whether predicted accuracy falls threshold. imagenet experiments take top- accuracy accuracy metric predict value based vector features derived model’s predicted class probabilities. features entropy vector maximum predicted class probability ﬁrst second highest predictions logit space. accuracy model using logistic regression validation images. figure illustrates tradeoffs accuracy response rate achieved applying rule inception-v measured second disjoint validation images. horizontal axis fraction examples inception-v returns vertical axis top- accuracy remaining examples. comparison also show feature tradecurve obtained simply thresholding feature value. also show theoretically optimal tradeoff curve would achieved using accuracy model predicts top- accuracy perfectly overall inception-v achieves top- accuracy. however logit cutoff threshold appropriately achieve accuracy examples model conﬁdent perhaps surprisingly using learned accuracy model gives tradeoff curve almost identical obtained simply thresholding logit gap. shown effectiveness abstention rules evaluate cascade-generation algorithm pool state-of-the-art imagenet classiﬁcation models. pool consists models released part tf-slim library pool contains recent nasnet models produced neural architecture search models based inception architecture mobilenet models generate abstaining models thresholding logit value. using algorithm important examples seen training statistics logit distributed differently them. used images ilsvrc validation algorithm report results remaining validation images. figure shows tradeoffs achieve accuracy average cost. relative large nasnet model obtain reduction loss accuracy. relative inception-v cascades obtains cost reduction loss accuracy anobtains cost reduction increase accuracy. relative largest mobilenet model achieve cost reduction accuracy gain. cheaper models require higher conﬁdence. minimum logit required make prediction higher earlier stages reﬂecting fact cheaper models must conﬁdent order achieve sufﬁciently high accuracy. cheaper models handle easier images. although overall model accuracy increases later stages accuracy subset images actually classiﬁed stage strictly decreasing supports idea easier images allocated cheaper models. cessing. techniques include quantization pruning weights channels tensor factorizations references therein discussion). section show techniques used generate larger pool approximated models used input cascade-generation algorithm order achieve cost reductions. also provides make cascade-generation algorithm case single pre-trained model available. experiments focus quantization model parameters compression technique. model number bits generate model quantizing parameters bits. yields pool quantized models input cascade-generation algorithm. cost number bits read memory classifying example. aside changes experiments identical figure shows accuracy function average number bits must fetched memory order classify example. though cascades generated consistently improve average memory cascades approximations reduce factor loss accuracy. perhaps surprisingly cascades approximations also offer improvements average-case number multiplications similar shown figure despite explicit component cost function. within realm computer vision cascaded models received much attention following seminal work viola jones used cascade increasingly expensive features create fast accurate face detector. problems speech recognition machine translation inference typically involves heuristic search large space possible outputs cascade idea used progressively ﬁlter outputs consideration recent work sought apply cascade idea deep networks. research involves training adaptive model end-to-end though end-to-end training appealing sensitive choice model architecture. current approaches image classiﬁcation based resnet architectures achieve results competitive latest nasnet models imagenet. another produce adaptive deep networks apply postprocessing pool pre-trained models done. knowledge previous work taken route bolukbasi also present results imagenet. contrast greedy approximation algorithm presented work approach good performance worst case also requires pre-trained input models arranged directed acyclic graph priori opposed learning structure part optimization process. presented greedy meta-algorithm generate cascaded model given pool pre-trained models input proved algorithm near-optimal worstcase performance suitable assumptions. experimentally showed cascades generated using algorithm signiﬁcantly improve upon state-of-the-art imagenet models terms average-case number multiplications average-case memory i/o. work leaves open several promising directions future research. theoretical side remains open problem come compelling theoretical guarantees adaptive policies opposed linear cascades. empirically would interesting incorporate architecture search inner loop greedy algorithm discussed streeter matthew golovin daniel smith stephen proceedings combining multiple heuristics online. twenty-second national conference artiﬁcial intelligence szegedy christian vanhoucke vincent ioffe sergey shlens wojna zbigniew. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition weiss david taskar benjamin. structured prediction cascades. proceedings thirteenth international conference artiﬁcial intelligence statistics joseph dekel ofer saligrama venkatesh. adaptive neural networks efﬁcient inference. proceedings thirty-fourth international conference machine learning golovin daniel krause andreas. adaptive submodularity theory applications active learning stochastic optimization. journal artiﬁcial intelligence research song huizi dally william deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. howard andrew menglong chen kalenichenko dmitry wang weijun weyand tobias andreetto marco adam hartwig. mobilenets efﬁcient convolutional neural networks mobile vision applications. corr abs/. huang chen danlu tianhong felix maaten laurens weinberger kilian multi-scale dense networks resource efﬁcient image classiﬁcation. corr abs/. munagala kamesh babu shivnath motwani rajeev widom jennifer thomas eiter. pipelined cover problem. icdt proceedings international conference volume springer", "year": 2018}