{"title": "Learning both Weights and Connections for Efficient Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "text": "neural networks computationally intensive memory intensive making difﬁcult deploy embedded systems. also conventional networks architecture training starts; result training cannot improve architecture. address limitations describe method reduce storage computation required neural networks order magnitude without affecting accuracy learning important connections. method prunes redundant connections using three-step method. first train network learn connections important. next prune unimportant connections. finally retrain network tune weights remaining connections. imagenet dataset method reduced number parameters alexnet factor million million without incurring accuracy loss. similar experiments vgg- found total number parameters reduced million million loss accuracy. neural networks become ubiquitous applications ranging computer vision speech recognition natural language processing consider convolutional neural networks used computer vision tasks grown time. lecun designed model lenet- less parameters classify handwritten digits krizhevsky imagenet competition parameters. deepface classiﬁed human faces parameters coates scaled network parameters. large neural networks powerful size consumes considerable storage memory bandwidth computational resources. embedded mobile applications resource demands become prohibitive. figure shows energy cost basic arithmetic memory operations cmos process. data energy connection dominated memory access ranges coefﬁcients on-chip sram coefﬁcients off-chip dram large networks on-chip storage hence require costly dram accesses. running billion connection neural network example would require dram access well beyond power envelope typical mobile device. goal pruning networks reduce energy required large networks real time mobile devices. model size reduction pruning also facilitates storage transmission mobile applications incorporating dnns. achieve goal present method prune network connections manner preserves original accuracy. initial training phase remove connections whose weight lower threshold. pruning converts dense fully-connected layer sparse layer. ﬁrst phase learns topology networks learning connections important removing unimportant connections. retrain sparse network remaining connections compensate connections removed. phases pruning retraining repeated iteratively reduce network complexity. effect training process learns network connectivity addition weights much mammalian brain synapses created ﬁrst months child’s development followed gradual pruning little-used connections falling typical adult values. neural networks typically over-parameterized signiﬁcant redundancy deep learning models results waste computation memory. various proposals remove redundancy vanhoucke explored ﬁxed-point implementation -bit integer activations. denton exploited linear structure neural network ﬁnding appropriate low-rank approximation parameters keeping accuracy within original model. similar accuracy loss gong compressed deep convnets using vector quantization. approximation quantization techniques orthogonal network pruning used together obtain gains attempts reduce number parameters neural networks replacing fully connected layer global average pooling. network network architecture googlenet achieves state-of-the-art results several benchmarks adopting idea. however transfer learning i.e. reusing features learned imagenet dataset applying tasks ﬁne-tuning fully connected layers difﬁcult approach. problem noted szegedy motivates linear layer networks enable transfer learning. network pruning used reduce network complexity reduce over-ﬁtting. early approach pruning biased weight decay optimal brain damage optimal brain surgeon prune networks reduce number connections based hessian loss function suggest pruning accurate magnitude-based pruning weight decay. however second order derivative needs additional computation. hashednets recent technique reduce model sizes using hash function randomly group connection weights hash buckets connections within hash bucket share single parameter value. technique beneﬁt pruning. pointed weinberger sparsity minimize hash collision making feature hashing even effective. hashednets used together pruning give even better parameter savings. pruning method employs three-step process illustrated figure begins learning connectivity normal network training. unlike conventional training however learning ﬁnal values weights rather learning connections important. second step prune low-weight connections. connections weights threshold removed network converting dense network sparse network shown figure ﬁnal step retrains network learn ﬁnal weights remaining sparse connections. step critical. pruned network used without retraining accuracy signiﬁcantly impacted. choosing correct regularization impacts performance pruning retraining. regularization penalizes non-zero parameters resulting parameters near zero. gives better accuracy pruning retraining. however remaining connections good regularization resulting lower accuracy retraining. overall regularization gives best pruning results. discussed experiment section. dropout widely used prevent over-ﬁtting also applies retraining. retraining however dropout ratio must adjusted account change model capacity. dropout parameter probabilistically dropped training come back inference. pruning parameters dropped forever pruning chance come back training inference. parameters sparse classiﬁer select informative predictors thus much less prediction variance reduces over-ﬁtting. pruning already reduced model capacity retraining dropout ratio smaller. quantitatively number connections layer original network network retraining number neurons layer since dropout works neurons varies quadratically according equation thus dropout ratio pruning parameters follow equation represent original dropout rate represent dropout rate retraining. retraining better retain weights initial training phase connections survived pruning re-initialize pruned layers. cnns contain fragile co-adapted features gradient descent able good solution network initially trained re-initializing layers retraining them. retrain pruned layers keep surviving parameters instead re-initializing them. retraining pruned layers starting retained weights requires less computation don’t back propagate entire network. also neural networks prone suffer vanishing gradient problem networks deeper makes pruning errors harder recover deep networks. prevent this parameters conv layers retrain layers pruning layers vice versa. learning right connections iterative process. pruning followed retraining iteration many iterations minimum number connections could found. without loss accuracy method boost pruning rate alexnet compared single-step aggressive pruning. iteration greedy search best connections. also experimented probabilistically pruning parameters based absolute value gave worse results. pruning connections neurons zero input connections zero output connections safely pruned. pruning furthered removing connections pruned neuron. retraining phase automatically arrives result dead neurons zero input connections zero output connections. occurs gradient descent regularization. neuron zero input connections contribution ﬁnal loss leading gradient zero output connection respectively. regularization term push weights zero. thus dead neurons automatically removed retraining. implemented network pruning caffe caffe modiﬁed mask disregards pruned parameters network operation weight tensor. pruning threshold chosen quality parameter multiplied standard deviation layer’s weights. carried experiments nvidia titanx gpus. ﬁrst experimented mnist dataset lenet-- lenet- networks lenet- fully connected network hidden layers neurons each achieves error rate mnist. lenet- convolutional network convolutional layers fully connected layers achieves error rate mnist. pruning network retrained original network’s original learning rate. table shows figure visualization ﬁrst layer’s sparsity pattern lenet--. banded structure repeated times correspond un-pruned parameters center images since digits written center. pruning saves parameters networks. layer network table shows original number weights number ﬂoating point operations compute layer’s activations average percentage activations non-zero percentage non-zero weights pruning percentage actually required ﬂoating point operations. interesting byproduct network pruning detects visual attention regions. figure shows sparsity pattern ﬁrst fully connected layer lenet-- matrix size bands band’s width corresponding input pixels. colored regions ﬁgure indicating non-zero parameters correspond center image. digits written center image important parameters. graph sparse left right corresponding less important regions bottom image. pruning neural network ﬁnds center image important connections peripheral regions heavily pruned. examine performance pruning imagenet ilsvrc- dataset training examples validation examples. alexnet caffe model reference model million parameters across convolutional layers fully connected layers. alexnet caffe model achieved top- accuracy top- accuracy original alexnet took hours train nvidia titan gpu. pruning whole network retrained original network’s initial learning rate. took hours retrain pruned alexnet. pruning used iteratively prototyping model rather used model reduction model ready deployment. thus retraining time less concern. table shows alexnet pruned original size without impacting accuracy amount computation reduced promising results alexnet also looked larger recent network vgg- ilsvrc- dataset. vgg- convolutional layers still three fully-connected layers. following similar methodology aggressively pruned convolutional fully-connected layers realize signiﬁcant reduction number weights shown table used iterations pruning retraining. vgg- results like alexnet promising. network whole reduced original size particular note largest fully-connected layers pruned less original size. reduction critical real time image processing little reuse fully connected layers across images trade-off curve accuracy number parameters shown figure parameters pruned away less accuracy. experimented regularization without retraining together iterative pruning give trade lines. comparing solid dashed lines importance retraining clear without retraining accuracy begins dropping much sooner original connections rather original connections. it’s interesting free lunch reducing connections without losing accuracy even without retraining; retraining ably reduce connections figure trade-off curve parameter reduction loss top- accuracy. regularization performs better learning connections without retraining regularization performs better retraining. iterative pruning gives best result. regularization gives better accuracy directly pruning since pushes parameters closer zero. however comparing yellow green lines shows outperforms retraining since beneﬁt pushing values towards zero. extension regularization pruning retraining beat simply using phases. parameters mode adapt well other. biggest gain comes iterative pruning take pruned retrained network prune retrain again. leftmost curve corresponds point green line pruned there’s accuracy loss accuracy begin drop sharply. green points achieve slightly better accuracy original model. believe accuracy improvement pruning ﬁnding right capacity network hence reducing overﬁtting. conv layers pruned different sensitivity. figure shows sensitivity layer network pruning. ﬁgure shows accuracy drops parameters pruned layer-by-layer basis. conv layers sensitive pruning fully connected layers ﬁrst convolutional layer interacts input image directly sensitive pruning. suspect sensitivity input layer channels thus less redundancy convolutional layers. used sensitivity results layer’s threshold example smallest threshold applied sensitive layer ﬁrst convolutional layer. storing pruned layers sparse matrices storage overhead storing relative rather absolute indices reduces space taken layer indices bits. similarly conv layer indices represented bits. table comparison model reduction methods alexnet. data-free pruning saved parameters much loss accuracy. deep fried convnets worked fully connected layers reduced parameters less reduced parameters inferior accuracy. naively cutting layer size saves parameters suffers loss accuracy. exploited linear structure convnets compressed layer individually model compression single layer incurred accuracy penalty biclustering svd. pruning storage requirements alexnet vggnet small enough weights stored chip instead off-chip dram takes orders magnitude energy access targeting pruning method ﬁxed-function hardware specialized sparse given limitation general purpose hardware sparse computation. figure shows histograms weight distribution pruning. weight ﬁrst fully connected layer alexnet. panels different y-axis scales. original distribution weights centered zero tails dropping quickly. almost parameters pruning large center region removed. network parameters adjust retraining phase. result parameters form bimodal distribution become spread across x-axis presented method improve energy efﬁciency storage neural networks without affecting accuracy ﬁnding right connections. method motivated part learning works mammalian brain operates learning connections important pruning unimportant connections retraining remaining sparse network. highlight experiments alexnet vggnet imagenet showing fully connected layer convolutional layer pruned reducing number connections without loss accuracy. leads smaller memory capacity bandwidth requirements real-time image processing making easier deployed mobile systems.", "year": 2015}