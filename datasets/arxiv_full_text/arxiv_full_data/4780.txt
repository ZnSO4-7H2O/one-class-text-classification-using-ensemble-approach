{"title": "Emphatic Temporal-Difference Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Emphatic algorithms are temporal-difference learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on different time steps. Recent works by Sutton, Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a particular way, these algorithms become stable and convergent under off-policy training with linear function approximation. This paper serves as a unified summary of the available results from both works. In addition, we demonstrate the empirical benefits from the flexibility of emphatic algorithms, including state-dependent discounting, state-dependent bootstrapping, and the user-specified allocation of function approximation resources.", "text": "emphatic algorithms temporal-diﬀerence learning algorithms change effective state distribution selectively emphasizing de-emphasizing updates diﬀerent time steps. recent works sutton mahmood white show varying emphasis particular algorithms become stable convergent oﬀ-policy training linear function approximation. paper serves uniﬁed summary available results works. addition demonstrate empirical beneﬁts ﬂexibility emphatic algorithms including state-dependent discounting state-dependent bootstrapping user-speciﬁed allocation function approximation resources. keywords stability convergence fundamental problem reinforcement learning involves learning sequence long-term predictions dynamical system. problem often formulated learning approximations value functions markov decision processes temporal-diﬀerence learning algorithms lstd provide eﬀective solutions problem. algorithms stand particularly ability learn eﬃciently moment-by-moment basis using memory computational complexity constant time. methods also distinguished ability learn predictions technique known bootstrapping often provides fast accurate answers algorithms conventionally make updates every state visited implicitly giving higher importance terms function-approximation resources states visited frequently. value cannot estimated accurately function approximation valuing states means valuing others less. however interested valuing states others based criteria visitation frequency. conventional updates provide ﬂexibility cannot naively modiﬁed. example case oﬀ-policy updates updating according policy learning another cause divergence paper discuss emphatic principled solution problem selective updating convergence ensured arbitrary interest visited states well oﬀ-policy training. idea emphasize deemphasize state updates user-speciﬁc interest conjunction much states bootstrap state. ﬁrst describe idea simpler case linear function approximation full bootstrapping derive full algorithm general oﬀ-policy learning setting arbitrary bootstrapping. finally brieﬂy summarizing available results stability convergence algorithm discuss potential advantages algorithm using illustrative experiment. start problem selective updating simplest function approximation case linear consider markov decision process ﬁnite states ﬁnite actions discounted total reward criterion discount rate setting agent interacts environment taking action state according policy p{at a|st transitions state receives reward sequence time steps rn×n denote state transition probability matrix expected immediate rewards state value approximate value state linear function features feature vector corresponding state conventional linear learns value function generating sequence parameter vectors state valued increased next time transition occurs even bigger increase value bigger increase transition experienced repeatedly system unstable parameter increases without bound—it diverges. problem arises bootstrapping function approximation entails shared resources among states. tabular representation used instead value state would stored independently divergence would occur. likewise value estimate ﬁrst state updated without bootstrapping second state divergence could avoided. emphatic remedies problem emphasizing update state depending much state bootstrapped conjunction relative interest state. although gives full bootstrapping amount bootstrapping still modulated example bootstrapping occurs even amount emphasis update state time according algorithm value estimate state updated user interested state reachable another state user interested. going back two-state example second state value also updated despite user-speciﬁed interest fact equal states updating exactly equivalent on-policy sampling; hence divergence occur. choices relative interest discount rate eﬀective state distribution diﬀerent on-policy sampling algorithm still converges show later. state-dependent degree discounting; equivalently probability terminating upon arrival state denote particular determines degree learning deﬁne general notion bootstrapped state-dependent degree bootstrapping; bootstrapping upon arriving state notational shorthand return λ-return state-dependent bootstrapping discounting bootstrapping preceding state determined γtρt− probability terminating times probability bootstrapping times degree preceding transition followed target policy. finally also depends emphasis preceding state itself. emphasis state similarly depends preceding states bootstrap state extent. thus total emphasis written trace similar emphatic adapted oﬀ-policy case application according emphasis written simply linear interpolation per-step computational memory complexity original number features. additional cost incurs computation scalar emphasis negligible. discussed motivations ideas design emphasis weighting scheme etd. discuss several salient analytical properties underlying algorithm weighting scheme present stability convergence results obtained algorithm. first formally state conditions needed analysis. assumption target policy exists diagonal matrix state-dependent discount factors diagonal entries. behavior policy induces irreducible markov chain unique invariant distribution counted) rewards −rπ; i.e. unique solution bellman equation associated multistep generalized bellman equation determined bootstrapping parameters also unique solution call states places positive emphasis weights emphasized states. precisely assumption assign expected emphasis weight state according weighting scheme sized states precisely important observe emphasis weights reﬂect occupancy probabilities target policy respect initial distribution proportional rather behavior policy. seen shortly gives desired stability property lacks normally algorithms selective updating. terms approximate value function mild condition approximation architecture given below equation equivalent projected version bellman equation contains linearly independent vectors since states positive interest among emphasized states. assumption easily satisﬁed reinforcement learning without model knowledge. ready discuss important stability property underlying algorithm. making emphasis weights reﬂecting occupancy probabilities target policy discussed earlier weighting scheme algorithm ensures matrix positive deﬁnite almost minimal conditions oﬀ-policy training property shows equation associated unique solution approximate value function unique solution). moreover shows unlike normal selective updating deterministic update parameter space converges suﬃciently small stepsize diminishing stepsizes {αt} used {θ∗} globally asymptotically stable associated mean −aθ+b ready address convergence algorithm. important analytical properties discussed shown also extend case linear independence condition assumption relaxed there acts like positive deﬁnite matrix subspace naturally operates extensions based understanding weighting scheme designed revealed proof properties together convergence results least-squares version convergence theorem stochastic approximation theory establish desired convergence constrained variant mean based proof method. section describe experiment illustrate ﬂexibility beneﬁts learning several oﬀ-policy predictions terms value estimates. experiment used gridworld problem depicted figure call miner problem. miner starting cell continually wandered around gridworld using following actions left right down indicating direction miner’s movement. invalid direction going resulted movement. miner zero reward every transition except arrived cell denoted gold case reward obtained. routes reach gold cell went straight block roundabout block trap could activated cells block chosen randomly. active trap stayed time steps trap active time. trap activation probability miner arrived gold cell fell trap transported next time step. note arriving gold cell trap episode miner wandered around continually. miner followed ﬁxed behavior policy according miner equally likely take four actions block inclined block block inclined left block case probability rest actions equally likely. evaluated three ﬁxed policies diﬀerent behavior policy. call uniform headfirst cautious policies. uniform policy actions equally likely every cell. headfirst policy miner chose block probability actions blocks equally likely. actions blocks chosen equal probability. cautious policy miner inclined right block block block left block case probability rest actions equally likely. states miner entrapped indicate termination target policy discounting states. whenever miner block everywhere else. behavior policy miner diﬀerent three target policies must oﬀ-policy training learn could happen policies. used three instances three diﬀerent predictions using miner block block states. clipped impact extremely large eligibility traces updates. clipping increments shown theoretically sound although discuss subject here. state representation used four features corresponding miner four blocks. miner wandered continually entrapments occurred. figure shows estimates calculated terms weight corresponding block three target policies. curves shown average estimates standard error bands using independent runs. dotted straight lines indicate true state value estimated monte carlo simulation function approximation clipping updates true value could estimated accurately. however estimates three policies appear approach values close true ones preserved relative ordering policies. absence clipping estimates less stable highly volatile occasionally moving away desired value runs. although learning curves still look volatile clipping updates reduced extent considerably. summarized motivations ideas available results emphatic algorithms. furthermore demonstrated used learn many predictions world simultaneously using oﬀ-policy learning ﬂexibility provides state-dependent discounting bootstrapping user-speciﬁed relative interests states. among algorithms per-step linear computational complexity convergent oﬀ-policy training. compared convergent gradient-based algorithms simpler easier use; learned parameter vector step-size parameter. problem high variance common oﬀ-policy learning susceptible well. extension variance-reduction methods weighted importance sampling natural remedy problem. produces diﬀerent algorithm conventional even on-policy case. likely that many cases provides accurate predictions relative interests emphasis. interesting direction future work would characterize cases.", "year": 2015}