{"title": "Value Iteration Networks", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.", "text": "introduce value iteration network fully differentiable neural network ‘planning module’ embedded within. vins learn plan suitable predicting outcomes involve planning-based reasoning policies reinforcement learning. approach novel differentiable approximation value-iteration algorithm represented convolutional neural network trained end-to-end using standard backpropagation. evaluate based policies discrete continuous path-planning domains natural-language based search task. show learning explicit planning computation policies generalize better unseen domains. last decade deep convolutional neural networks revolutionized supervised learning tasks object recognition action recognition semantic segmentation recently cnns applied reinforcement learning tasks visual observations atari games robotic manipulation imitation learning tasks neural network trained represent policy mapping observation system’s state action goal representing control strategy good long-term behavior typically quantiﬁed minimization sequence time-dependent costs. sequential nature decision making inherently different one-step decisions supervised learning general requires form planning however recent deep works employed architectures similar standard networks used supervised learning tasks typically consist cnns feature extraction fully connected layers features probability distribution actions. networks inherently reactive particular lack explicit planning computation. success reactive policies sequential problems learning algorithm essentially trains reactive policy select actions good long-term consequences training domain. understand planning nevertheless important ingredient policy consider grid-world navigation task depicted figure agent observe domain required navigate obstacles target position. hopes training policy solve several instances problem different obstacle conﬁgurations policy would generalize solve different unseen domain figure however show experiments standard cnn-based networks easily trained solve maps generalize well tasks outside understand goal-directed nature behavior. observation suggests computation learned reactive policies different planning required solve task. principle enough training data covers possible task conﬁgurations rich enough policy representation reactive policy learn task optimal policy. practice often expensive offer data-efﬁcient approach exploiting ﬂexible prior planning computation underlying behavior. work propose nn-based policy effectively learn plan. model termed value-iteration network differentiable ‘planning program’ embedded within structure. approach observation classic value-iteration planning algorithm represented speciﬁc figure instances grid-world domain. type cnn. embedding network task move goal obstacles. module inside standard feed-forward classiﬁcation network obtain model learn parameters planning computation yields useful predictions. block differentiable whole network trained using standard backpropagation. makes policy simple train using standard algorithms straightforward integrate perception control. connections planning algorithms recurrent previously explored ilin work builds related ideas results broadly applicable policy representation. approach different model-based requires system identiﬁcation observations dynamics model solved policy. many applications including robotic manipulation locomotion accurate system identiﬁcation difﬁcult modelling errors severely degrade policy performance. domains model-free approach often preferred since policy trained model free without requiring explicit system identiﬁcation. addition effects modelling errors vins mitigated training network end-to-end similarly methods demonstrate effectiveness vins within standard algorithms various problems among require visual perception continuous control also natural language based decision making webnav challenge training policy learns observation planning computation relevant task generate action predictions based resulting plan. demonstrate leads policies generalize better unseen task instances. background section provide background planning value iteration cnns policy representations sequel shall show cnns implement particular form planning computation similar value iteration algorithm used policy value iteration standard model sequential decision making planning markov decision process consists states actions reward function transition kernel encodes probability next state given current state action. policy prescribes action distribution state. goal policy obtains high rewards long term. formally value state policy expected discounted rewards starting state γtr| discount factor executing policy denotes expectation trajectories states actions actions selected according states evolve according transition kernel optimal value function maxπ maximal long-term return possible popular algorithm calculating state. policy said optimal value iteration well known value function converges optimal policy derived maxa convolutional neural networks particular architecture proved useful computer vision among domains comprised stacked convolution max-pooling layers. input convolution layer dimensional signal typically image channels horizontal pixels vertical pixels output l-channel convolution image kernels scalar activation function. max-pooling hlij layer selects channel pixel maximum value among neighbors maxij∈n hlij. typically neighbors chosen image hmaxpool patch around pixel max-pooling image down-sampled constant factor commonly resulting output signal channels horizontal pixels vertical pixels. cnns typically trained using stochastic gradient descent backpropagation computing gradients. reinforcement learning imitation learning mdps state space large continuous transitions rewards known advance planning algorithms cannot applied. cases policy learned either expert supervision trial error learning algorithms cases different policy representations focus work similar. additionally state-of-the-art algorithms agnostic policy representation require differentiable performing gradient descent algorithm-speciﬁc loss function. therefore paper commit speciﬁc learning algorithm consider policy. denote observation state policy speciﬁed parametrized function mapping observations probability actions policy parameters. example policy could represented neural network denoting network weights. goal tune parameters policy behaves well sense optimal policy deﬁned section state actions i=...n generated expert. learning policy becomes instance supervised learning optimal action available instead agent world observe rewards state transitions actions effect. algorithms observations improve value policy. value iteration network model section introduce general policy representation embeds explicit planning module. stated earlier motivation representation natural solution many tasks path planning described above involves planning model domain. denote domain design policy assume unknown optimal plan contains useful information optimal policy original task however emphasize assume know advance. idea equip policy ability learn solve solution element policy hypothesize lead policy automatically learns useful plan denote states actions rewards transitions facilitate connection depend observation namely later learn functions part policy learning process. example grid-world domain described above state action spaces true grid-world reward function image domain high reward goal negative reward near obstacle encode deterministic movements grid-world depend observation. rewards transitions necessarily true rewards transitions task optimal plan still follow trajectory avoids obstacles reaches goal similarly optimal plan speciﬁed standard planning algorithm used obtain value function next section shall show using particular implementation planning advantage differentiable simple implement within framework. section however focus planning result within policy approach based important observations. ﬁrst vector values encodes information optimal plan thus adding vector additional features policy sufﬁcient extracting information optimal plan however additional property optimal decision state depend subset values since max¯a therefore local connectivity structure grid-world example above states small subset terminology form attention sense given label prediction subset input features relevant. attention known improve learning performance reducing effective number network parameters learning. therefore second element network attention module outputs vector values finally vector added additional features reactive policy full network architecture depicted figure returning grid-world example particular state reactive policy needs query values states neighboring order select correct action. thus attention module case could return vector subset neighboring states. figure planning-based models. left general policy representation adds value function features planner reactive policy. right module representation algorithm. denote parameters policy namely parameters note fact function therefore policy written form similarly standard policy form could back-propagate function potentially could train policy using standard algorithms like standard policy representation. easy design functions differentiable back-propagating gradient planning algorithm trivial. following propose novel interpretation approximate algorithm particular form cnn. allows conveniently treat planning module another back-propagating train whole policy end-to-end. module introduce module encodes differentiable planning computation. starting point algorithm main observation iteration seen passing previous value function reward function convolution layer max-pooling layer. analogy channel convolution layer corresponds q-function speciﬁc action convolution kernel weights correspond discounted transition probabilities. thus recurrently applying convolution layer times iterations effectively performed. following idea propose network module depicted figure inputs module ‘reward image’ dimensions here purpose clarity follow formulation explicitly assume state space maps -dimensional grid. however approach extended general discrete state spaces example graph report wikinav experiment section reward convolutional layer rli−ij−j. channel layer corresponds particular action layer max-pooled along actions channel produce next-iteration value function layer ¯vij max¯a next-iteration value function layer stacked reward back convolutional layer max-pooling layer times perform iterations value iteration. module simply architecture capability performing approximate computation. nevertheless representing form makes learning parameters reward function natural backpropagating network similarly standard cnn. modules also composed hierarchically treating value module additional input another module. report idea supplementary material. value iteration networks ingredients differentiable planning-based policy term value iteration network based general planning-based policy deﬁned above module planning algorithm. order implement specify state action spaces planning module reward transition functions attention function; refer design. tasks show experiments relatively straightforward select suitable design tasks require thought. however emphasize important point reward transitions attention deﬁned parametric functions trained whole policy. thus rough design speciﬁed ﬁne-tuned end-to-end training. design chosen implementing straightforward simply form cnn. networks experiments required several lines theano code. next section evaluate policies various domains showing learning plan achieve better generalization capability. experiments section evaluate vins policy representations various domains. additional experiments investigating hierarchical vins well technical implementation details discussed supplementary material. source code available https//github.com/avivt/vin. goal experiments investigate following questions additional goal point several ideas designing vins various tasks. exhaustive list domains hope motivate creative designs future work. grid-world domain ﬁrst experiment domain synthetic grid-world randomly placed obstacles observation includes position agent also image obstacles goal position. figure shows random instances grid-world size conjecture learning optimal policy several instances domain policy would learn planning computation required solve unseen task. simple domain optimal policy easily calculated using exact note however interested evaluating whether policy trained using learn plan. following results policies trained using standard supervised learning demonstrations optimal policy. supplementary material report additional experiments show similar ﬁndings. design task following guidelines described above planning grid-world similar true mdp. reward mapping mapping image input reward grid-world. thus potentially learn discriminate obstacles non-obstacles goal assign suitable reward each. transitions deﬁned convolution kernels block exploiting fact transitions grid-world local. recurrence chosen proportion grid-world size ensure information goal state state. attention module chose trivial approach selects values block current state i.e. ﬁnal reactive policy fully connected network maps probability actions. compare vins following reactive policies network devised cnn-based reactive policy inspired recent impressive results convolution layers fully connected output. network trained predict values network outputs probability actions. terms related since maxa fully convolutional network problem setting domain similar semantic segmentation pixel image assigned semantic label therefore devised inspired state-of-the-art semantic segmentation algorithm convolution layers ﬁrst layer ﬁlter spans whole image properly convey information goal every state. table present average prediction loss model evaluated held-out test-set maps random obstacles goals initial states different problem sizes. addition full trajectory initial state predicted iteratively rolling-out next-states figure grid-world domains random instances synthetic gridworld vin-predicted trajectories ground-truth shortest paths random start goal positions. image mars domain points elevation sharper colored red. points calculated matching image elevation data available learning algorithm. note difﬁculty distinguishing obstacles non-obstacles. vin-predicted shortest-path ground truth trajectories random start goal positions. table performance grid-world domain. comparison reactive policies. domain sizes networks signiﬁcantly outperform standard reactive networks. note performance increases dramatically problem size. predicted network. trajectory said succeed reached goal without hitting obstacles. trajectory succeeded also measured difference length optimal trajectory. average difference average success rate reported table clearly policies generalize domains outside training set. visualization reward mapping shows negative obstacles positive goal small negative constant otherwise. resulting value function gradient pointing towards direction goal around obstacles thus useful planning computation learned. vins also signiﬁcantly outperform reactive networks performance increases dramatically problem size. importantly note prediction loss reactive policies comparable vins although success rate signiﬁcantly worse. shows standard case overﬁtting/underﬁtting reactive policies. rather policies structure focus prediction errors less important parts trajectory reactive policies make distinction learn easily predictable parts trajectory fail complete task. vins effective depth larger depth reactive policies. wonder whether deep enough network would learn plan. principle depth potential perform computation vin. however much parameters requiring much training data. evaluate untying weights recurrent layers vin. results reported supplementary material show untying weights degrades performance stronger effect smaller sizes training data. mars rover navigation experiment show vins learn plan natural image input. demonstrate path-planning overhead terrain images mars landscape. domain represented image patch deﬁned grid-world state considered obstacle terrain corresponding image patch contained elevation angle degrees more evaluated using external elevation data base. example domain terrain image depicted figure shortest-path planning case similar grid-world domain section design similar deeper reward mapping processing image. policy trained predict shortest-path directly terrain image. emphasize elevation data part input must inferred terrain image. training achieved success rate rate context compare best performance achievable without access elevation data make comparison trained classify whether patch obstacle not. classiﬁer trained using image data network labels true obstacle classiﬁcations elevation success rate planner uses obstacle generated classiﬁer image showing obstacle identiﬁcation image indeed challenging. thus success rate trained without obstacle labels ‘ﬁgure out’ planning process quite remarkable. continuous control consider path planning domain continuous states continuous actions cannot solved using therefore cannot naively applied. instead construct perform ‘high-level’ planning discrete coarse grid-world representation continuous domain. shall show learn plan ‘highlevel’ plan also exploit plan within ‘low-level’ continuous control policy. moreover policy results better generalization reactive policy. consider domain figure red-colored particle needs navigated green goal using horizontal vertical forces. gray-colored obstacles randomly positioned domain apply elastic force friction contacted. domain presents non-trivial control problem agent needs plan feasible trajectory obstacles also control particle follow state observation consists particle’s continuous position velocity static downscaled image obstacles goal position domain. principle observation sufﬁcient devise ‘rough plan’ particle follow. previous experiments investigate whether policy trained several instances domain different start state goal obstacle positions would generalize unseen domain. training chose guided policy search algorithm unknown dynamics suitable learning policies continuous dynamics contacts used publicly available code mujoco physical simulation. generated random training instances evaluate performance different test instances distribution. design similar grid-world cases important modiﬁcations attention module selects patch value centered around current position map. ﬁnal reactive policy -layer fully connected network -dimensional continuous output controls. addition limited number training domains pre-trained transition weights correspond discounted grid-world transitions. reasonable prior weights task emphasize even initialization initial value function meaningless since reward learned. compare cnn-based reactive policy inspired state-of-the-art results layers image processing followed -layer fully connected network similar reactive policy. figure shows performance trained policies measured ﬁnal distance target. clearly outperforms test domains. also plot several trajectories policies test domains showing learned sensible generalization task. webnav challenge previous experiments planning aspect task corresponded navigation. consider general domain webnav language based search task graph. webnav agent needs navigate links website towards goal web-page speciﬁed short -sentence query. state agent observe average wordembedding features state possible next states features query based select link follow. search performed wikipedia website. here report experiments ‘wikipedia schools’ website simpliﬁed wikipedia designed children pages links page. nn-based policy proposed ﬁrst learns mapping essence policy reactive relies word embedding features state contain meaningful information path goal. indeed property naturally holds encyclopedic website structured tree categories sub-categories sub-sub-categories etc. sought explore whether planning based lead better performance task intuition plan simpliﬁed model website help guide reactive policy difﬁcult queries. therefore designed plans small subset graph contains level categories word-embedding features. designing requires different approach grid-world vins described earlier challenging aspect deﬁne meaningful mapping nodes true graph nodes smaller graph. reward mapping chose weighted similarity measure query features features nodes small graph thus intuitively nodes similar query high reward. transitions ﬁxed based graph connectivity smaller graph known though different true graph. attention module also based weighted similarity measure features possible next states features node simpliﬁed graph reactive policy part similar policy described above. note training end-to-end effectively learning exploit small graph better planning true large graph. policy baseline reactive policy trained supervised learning random trajectories start root node graph. similarly policy said succeed query correct predictions along path within top- predictions. training policy performed mildly better baseline held-out test queries starting root node achieving successful runs baseline. however tested policies harder task starting random position graph vins signiﬁcantly outperformed baseline achieving successful runs baseline test queries. results conﬁrm indeed navigating tree categories root features state contain meaningful information path goal making reactive policy sufﬁcient. however starting navigation different state reactive policy fail understand needs ﬁrst back root switch different branch tree. results indicate strategy better represented vin. remark still room improvements webnav results e.g. better models reward attention functions better word-embedding representations text. conclusion outlook introduction powerful scalable methods opened range problems deep learning. however recent works investigate policy architectures speciﬁcally tailored planning uncertainty current theory benchmarks rarely investigate generalization properties trained policy work takes step direction exploring better generalizing policy representations. policies learn approximate planning computation relevant solving task shown computation leads better generalization diverse tasks ranging simple gridworlds amenable value iteration continuous control even navigation wikipedia links. future work intend learn different planning computations based simulation optimal linear control combine reactive policies potentially develop solutions task motion planning acknowledgments research funded part siemens pecase award army research ofﬁce mast program career award partially funded viterbi scholarship technion. partially funded darpa ppaml program contract fa--c-.", "year": 2016}