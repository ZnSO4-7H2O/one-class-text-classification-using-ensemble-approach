{"title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.", "text": "abstract. deep neural networks able learn powerful representations large quantities labeled input data however canalways generalize well across changes input distributions. domain adaptation algorithms proposed compensate degradation performance domain shift. paper address case target domain unlabeled requiring unsupervised adaptation. coral frustratingly easy unsupervised domain adaptation method aligns second-order statistics source target distributions linear transformation. here extend coral learn nonlinear transformation aligns correlations layer activations deep neural networks experiments standard benchmark datasets show state-of-the-art performance. many machine learning algorithms assume training test data independent identically distributed however assumption rarely holds practice data likely change time space. even though state-of-the-art deep convolutional neural network features invariant level cues degree donahue showed still susceptible domain shift. instead collecting labelled data training classiﬁer every possible scenario unsupervised domain adaptation methods compensate degradation performance transferring knowledge labelled source domains unlabelled target domains. recently proposed coral method aligns second-order statistics source target distributions linear transformation. even though frustratingly easy works well unsupervised domain adaptation. however relies linear transformation end-to-end needs ﬁrst extract features apply transformation train classiﬁer separate step. work extend coral incorporate directly deep networks constructing diﬀerentiable loss function minimizes diﬀerence between source target correlations–the coral loss. compared coral proposed deep coral approach learns non-linear transformation previous techniques unsupervised adaptation consisted re-weighting training point losses closely reﬂect test distribution ﬁnding transformation lower-dimensional manifold brings source target subspaces closer together. re-weighting based approaches often assume restricted form domain shift–selection bias–and thus applicable general scenarios. geodesic methods bridge source target domain projecting source target onto points along geodesic path ﬁnding closed-form linear transforms source points target align subspaces computing linear minimizes frobenius norm diﬀerence eigenvectors. contrast coral minimizes domain shift aligning second-order statistics source target distributions. adaptive deep neural networks recently explored unsupervised adaptation. dlid trains joint source target architecture adaptation layers. applies single linear kernel layer minimize maximum mean discrepancy minimizes multiple kernels applied multiple layers. reversegrad adds binary classiﬁer explicitly confuse domains. proposed deep coral approach similar reversegrad sense loss added minimize diﬀerence learned feature covariances across domains similar minimizing polynomial kernel. however powerful much simpler optimize reversegrad integrated diﬀerent layers architectures seamlessly. address unsupervised domain adaptation scenario labelled training data target domain propose leverage deep features pre-trained large generic domain labelled source data. meantime also want ﬁnal learned features work well target domain. ﬁrst goal achieved initializing network parameters generic pre-trained network ﬁne-tuning labelled source data. second goal propose minimize diﬀerence second-order statistics source target feature activations i.e. coral loss. figure shows sample deep coral architecture using proposed correlation alignment layer deep domain adaptation. refer deep coral deep network incorporating coral loss domain adaptation. fig. sample deep coral architecture based classiﬁer layer. generalization simplicity apply coral loss layer alexnet integrating layers network architectures straightforward. suppose number source target data respectively. d-dimensional deep layer activations input trying learn. suppose indicates j-th dimension i-th source data example denote feature covariance matrices. describe method taking multi-class classiﬁcation problem running example. mentioned before ﬁnal deep features need discriminative enough train strong classiﬁer invariant diﬀerence source target domains. minimizing classiﬁcation loss likely lead overﬁtting source domain causing reduced performance target domain. hand minimizing coral loss alone might lead degenerated features. example network could project source target data single point making coral loss trivially zero. however strong classiﬁer constructed features. joint training classiﬁcation loss coral loss likely learn features work well target domain denotes number coral loss layers deep network weight trades adaptation classiﬁcation accuracy source domain. show below losses play counterparts reach equilibrium training ﬁnal features expected work well target domain. evaluate method standard domain adaptation benchmark oﬃce dataset oﬃce dataset contains object categories oﬃce environment image domains amazon dslr ebcam. follow standard protocol labelled source data target data without labels. since domains conduct experiments shifts taking domain source another target. experiment apply coral loss last classiﬁcation layer general case–most deep classiﬁer architectures contain fully connected layer classiﬁcation. applying coral loss layers network architectures straightforward. weight coral loss training classiﬁcation loss coral loss roughly same. seems reasonable choice want feature representation discriminative also minimizes distance source target domains. used caﬀe bvlc reference caﬀenet experiments. compare recently published methods coral manifold based methods project source target distributions lower-dimensional manifold end-to-end deep methods. adds domain confusion loss alexnet ﬁne-tunes source target domain. similar utilizes multi-kernel selection method better mean embedding matching adapts multiple layers. direct comparison paper uses hidden layer coral feature ﬁne-tuned source domain achieves better performance generic pre-trained features train linear fair comparison accuracies reported authors exactly setting conduct experiments using source code provided authors. table deep coral achieves better average performance coral baseline methods. three shifts achieves highest accuracy. shifts margin d-coral best baseline method small accuracies training without coral loss. clearly adding coral loss helps achieve much better performance target domain maintaining strong classiﬁcation accuracy source domain. training test accuracies training coral loss. adding coral loss helps achieve much better performance target domain maintaining strong classiﬁcation accuracy source domain. classiﬁcation loss coral loss training coral loss. last fully connected layer loss large beginning. training hundred iterations losses same. coral distance training coral loss distance getting much larger times larger compared training coral loss). classiﬁcation loss large. training hundred iterations losses same. figure show coral distance domains training coral loss distance getting much larger times larger compared training coral loss). comparing figure figure even though coral loss always decreasing training weight distance source target domains becomes much larger. reasonable ﬁne-tuning without domain adaptation likely overﬁt features source domain. coral loss constrains distance source target domain ﬁne-tuning process helps maintain equilibrium ﬁnal features work well target domain. work extended coral simple eﬀective unsupervised domain adaptation method perform end-to-end adaptation deep neural networks. experiments standard benchmark datasets show state-of-the-art performance. deep coral works seamlessly deep networks easily integrated diﬀerent layers network architectures.", "year": 2016}