{"title": "Plummer Autoencoders", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Estimating the true density in high-dimensional feature spaces is a well-known problem in machine learning. This work shows that it is possible to formulate the optimization problem as a minimization and use the representational power of neural networks to learn very complex densities. A theoretical bound on the estimation error is given when dealing with finite number of samples. The proposed theory is corroborated by extensive experiments on different datasets and compared against several existing approaches from the families of generative adversarial networks and autoencoder-based models.", "text": "remaining paper organized follows. section provide formulation optimization problem analyze properties objective function provide bound estimation error. section review literature recent generative models. section discuss experimental evaluation. section deals problem density estimation. goal estimate unknown density function whose support deﬁned start providing overall idea learning density underlying training data formulate optimization problem used approach ﬁnally analyze properties objective function provide bound estimation error. consider continuous functions equal intrinsic dimensionality furthermore consider every namely left inverse domain work neural networks parameterized vectors respectively. called encoding function taking random input density producing random vector density decoding function taking input producing random vector distributed according note that since every already density estimator drawback general cannot written closed form. estimating true density high-dimensional feature spaces well-known problem machine learning. work shows possible formulate optimization problem minimization representational power neural networks learn complex densities. theoretical bound estimation error given dealing ﬁnite number samples. proposed theory corroborated extensive experiments different datasets compared several existing approaches families generative adversarial networks autoencoder-based models. deep generative models like autoregressive models generative adversarial networks variational autoencoders represent promising research directions solving problem density estimation. families limitations. performance existing autoregressive models based recurrent neural networks degrade number features data grows. generative adversarial networks difﬁcult train mini-max nature optimization problem therefore becomes difﬁcult ensure global convergence algorithms without making hard assumptions like networks inﬁnite capacity. performance variational autoencoders strongly dependent choice posterior density direct consequence quality learnt density. work show possible cast problem density estimation minimization problem thus overcoming difﬁculties encountered minimax optimization generative adversarial networks. allows exploit existing optimization routines neural networks converge global optimal solutions. furthemore provide ﬁnite-sample bounds estimation error true density. whole framework validated extensive experimental analysis variety synthetic real-world datasets comparing proposed choice kernel function fundamental importance. fact authors proved gradient-based algorithms converge uniformly global solution second term choices kernel function e.g. plummer ker\u0001 arbitrary small parameter used avoid singularity gaussian kernel satisfy property therefore cannot used directly minimize second term practically speaking local minima present function space considering plummer kernel whereas kernels problem local minima arise. reality local minima present considering parameter space neural networks. nevertheless theoretically shown sufﬁciently powerful networks almost local minima global minima result holds also objective since ﬁrst addend affected kernel choice. extended version plummer kernel above-mentioned properties leads faster convergence important mention integrals cannot computed exactly since unknown deﬁned explicitly. consequence unbiased estimate surrogate optimization namely dx={xi}n ﬁnite samples drawn respectively. note ﬁrst term corresponds reconstruction error training data. therefore model considered autoencoder. based fact chosen kernel refer model plummer autoencoder deﬁne arbitrary density support closed form. goal guarantee whole support maintaining every allows decoding function generator produce samples distributed according therefore problem density estimation high-dimensional feature space converted problem estimation lower dimensional vector space thus overcoming curse dimensionality. figure provides graphical interpretation concepts. note ﬁrst term reaches global minimum encoding decoding functions invertible support second term globally optimal equals therefore global minimum satisﬁes initial requirements optimal solution corresponds case autoregressive models convert problem density estimation sequence problem based fact unknown density function expressed product conditional distributions. recurrent neural networks generally used learn conditional distributions. main drawback difﬁculty recurrent neural networks catching long-term dependencies. therefore performance tend degrade length sequences increase. anlimitation models absence latent representation data. generative adversarial networks cast problem density estimation mini-max game neural networks namely discriminator tries distinguish true generated samples generator tries produce samples similar true ones fool discriminator. reputation difﬁcult train also require careful design network architectures known issues problem vanishing gradients happens output discriminator saturated true generated data perfectly classiﬁed gradient information provided generator problem mode collapse happens samples generator collapse single point corresponding maximum output value discrimitwo terms vanish large. ﬁrst reconstruction error term depends also number features side length hypercube second estimation second integral depends maximum value kernel function plethora solutions exist literature therefore focus relevant ones. solve problem vanishing gradients original paper gans proposes different objective generator called alternative. improved version authors proposes another objective generator based feature matching namely comparing mean statistics true generated data features extracted discriminator. work incorporates multiple discriminators original framework ensure gradient continually ﬂows. arjowski analyzes problem vanishing gradients theoretical perspective provides solution consists adding instance noise training increase attraction force generated true data manifold. solve problem mode collapse generally problem instability authors introduce regularizer optimization objective acts form consensus discriminator generator favors convergence training. nevertheless local convergence guaranteed. metz propose unrolled optimization discriminator. update generator computed backpropagating gradient multiple updates discriminator generated samples attracted single data point. zhao formulate problem differently. particular consider discriminator energy function assigning scores samples true data manifold high scores samples generated manifold autoencoder discriminator network. authors show experimentally proposed model leads better stability training. work considers application image generation introduce sort curriculum training gans. progressive growing network capacity image resolution allows stabilize training. model able generate images high resolution. nevertheless method heavily based heuristics research effort required order formalize approach generalize domains. gans formulated also divergence minimization problem. seminal paper goodfellow shows that assumption optimal discriminator generator objective equivalent computing jensen shannon divergence true generated density. authors extend analysis broader families divergences called f−divergences. compare different measures class provide experimental insights divergence choose natural images. work deﬁnes pathological examples many divergences including jensen shannon yield suboptimal solutions generator therefore propose wasserstein distance. heuristic based weight clipping used constrain critic class −lipschitz functions. follow-up paper substitute heuristic gradient penalty. another research direction gans consists using integral probability metrics optimization objective. particular maximum mean discrepancy used measure distance train generator network. general problem formulated following generative moment matching networks rkhs induced gaussian kernel. drawback models fact maximization performed resulting solutions suboptimal. another limitation fact similarity scores associated kernel function directly computed sample space. therefore performance degrade dimensionality feature space increases work introduces encoding function represent data compact distances computed latent representation thus solving problem dimensionality. authors propose extend maximum mean discrepancy include also covariance statistics ensure better stability. common drawback gans lack latent representation data. authors autoencoder network original framework reconstructing part latent code. identical works propose encoding function together generator perform adversarial game ensure joint density input/output generator agrees joint density input/output encoder. prove optimal solution achieved generator encoder invertible. practice fail guarantee convergence solution adversarial nature game. authors extend previous works explicitly imposing invertibility condition. achieve adding term generator objective computes reconstruction error latent space. adversarial autoencoders similar approaches differences estimation reconstruction error performed sample space adversarial game performed latent space. related work proposed authors optimize similar objective extended version plummer kernel. computation distances performed directly sample space performance degrade number features increases. another drawback model allow infer latent representation data. variational autoencoders represent another family implicit generative models. framework based maximizing log-likelihood training data. order efﬁciently sample prior therefore estimate log-likelihood authors introduce inference network gives information likely regions latent space sample from. nevertheless performance model strongly dependent capability inference network approximate true posterior density. reason work proposes adversarial game enforce condition. proposed solution compared methods namely coulomb closely related method improved version wasserstein bidirectional variational encoder enhancement gans variational autoencoders adversarial autoencoders adversarial variational bayes adaptive contrast note ﬁrst four competitors belong family generative adversarial networks last three generative models based autoencoders. make huge effort implementing comparing approaches different synthetic real-world datasets. code replicate experiments including ones competitors available xxxxx. start comparing approaches two-dimensional dataset consisting isotropic gaussians placed according grid call grid dataset training dataset contains samples generated true density. following methodology works choose fully connected mlps hidden layers encoder decoder discriminator networks models trained million iterations using adam optimizer. model chosen range obtain lowest value objective training. models strictly follow details origifigure detection mode collapse classiﬁcation generated samples dimensional embedding dataset. x-axis plot represents modes y-axis counts number generated samples given mode. models evaluated qualitatively visual inspecting generated samples quantitatively computing loglikelihood test data. compute log-likelihood ﬁrst apply kernel density estimation using gaussian kernel generated samples evaluate log-likelihood test samples true distribution. results averaged repetitions. figure shows samples generated models table provides quantitative results. immediate bigan affected mode collapse avb-ac generate noisy samples. veegan achieves performance terms log-likelihood since assigns high prior modes. model cougan compare favourably wgan turn obtain best performance. dimensional embedding dataset second dataset consists isotropic gaussians embedded dimensional vector space call dimensional embedding dataset. generate samples true density train models. classiﬁer count number samples generated models mode. evaluation procedure allows detect presence mode collapse. important mention procedure fooled speciﬁc pathological cases like memorization training samples. therefore log-likelihood test data assess quality learnt distribution. differences batch normalization bigan lead better performance. experience high instability training cougan also convergence failures. figure shows histograms generated samples obtained models. note bigan veegan affected mode collapse. table provides log-likelihood scores. model achieves best performance meaning able better estimate underlying true density. methodology similar previous dataset. network encoder decoder discriminator networks train models epochs batch size equal visualize generated samples nearest neighbors latent space understand quality learnt representation whether semantic consistency preserved neighborhood samples. similarly previous case batch normalization bigan veegan aae. figure conditional generation samples. ﬁrst column plot contains true samples columns obtained generating samples latent codes associated true data perturbing latent representation additive isotropic gaussian noise learn good approximation true density. figure shows nearest neighbors true data. worth mention preserve semantic consistency representation. fact small perturbation latent representation output image completely change semantic content. instead model capable fulﬁll property. references arjovsky bottou towards principled methods training generative adversarial networks. international conference learning representations chen x.chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems dumoulin belghazi poole lamb arjovsky mastropietro courville adversarially learned inference. international conference learning representations mescheder nowozin geiger adversarial variational bayes unifying variational autoencoders generative adversarial networks. international conference machine learning goodfellow pouget-abadie mirza wardefarley ozair courville bengio generative adversarial nets. advances neural information processing systems nowozin cseke tomioka f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems ramdas reddi poczos singh wasserman decreasing power kernel distance based nonparametric hypothesis tests high dimensions. aaai conference artiﬁcial intelligence srivastava valkoz russell gutmann sutton veegan reducing mode collapse gans using implicit variational learning. advances neural information processing systems unterthiner nessler klambauer heusel ramsauer hochreiter coulomb gans provably optimal nash equilibria potential ﬁelds. arxiv preprint arxiv. consider reconstruction error term deﬁne x−g). note therefore bounded interval considering random variable apply hoeffding’s inequality obtain following statistical bound note second addend equivalent empirical unbiased estimate refer based observation reuse statistical bound provided theorem valid bounded positive deﬁnite kernel namely ﬁrst inequality obtained applying union bound introducing second inequality obtained replacing union operator addition; third inequality obtained exploiting triangle inequality; last equality follows lemma table provides details network architectures. hyperbolic tangent activations used output activations encoders outputs decoders/generators linear. methods relu hidden activations. cougan uses provide also results relu. furthermore show results veegan without batch normalization figure table provides details network architectures. hyperbolic tangent activations used output activations encoders outputs decoders/generators linear. methods relu hidden activations. cougan uses provide also results relu. furthermore report results bigan veegan without batch normalization table table provides details network architectures. hyperbolic tangent activations used output activations encoders outputs decoders/generators sigmoid methods relu hidden activations. cougan uses provide also results relu. furthermore report results bigan veegan without batch normalization figure", "year": 2018}