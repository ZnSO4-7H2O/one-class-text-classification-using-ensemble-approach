{"title": "On the Importance of Normalisation Layers in Deep Learning with  Piecewise Linear Activation Units", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.", "text": "deep feedforward neural networks piecewise linear activations currently producing state-of-the-art results several public datasets combination deep learning models piecewise linear activation functions allows estimation exponentially complex functions large number subnetworks specialized classiﬁcation similar input examples. training process subnetworks avoid overﬁtting implicit regularization scheme based fact must share parameters subnetworks. using framework made empirical observation improve even performance models. notice models assume balanced initial distribution data points respect domain piecewise linear activation function. assumption violated piecewise linear activation units degenerate purely linear activation units result signiﬁcant reduction capacity learn complex functions. furthermore number model layers increases unbalanced initial distribution makes model ill-conditioned. therefore propose introduction batch normalisation units deep feedforward neural networks piecewise linear activations drives balanced activation units region activation function trained relatively large proportion training samples. also batch normalisation promotes pre-conditioning deep learning models. show introducing maxout batch normalisation units network network model results model produces classiﬁcation results better comparable current state cifar- cifar- mnist svhn datasets. piecewise linear activation units deep learning models deep convolutional neural network produced models showing state-of-the-art results several public databases cifar- mnist ∗this research supported australian research council censvhn piecewise linear activation units subject study montufar srivastava main conclusions achieved works multi-layer composition piecewise linear activation units allows exponential division input space given activation units trained based local competition selects region activation function training sample specialized subnetworks formed consistency respond similar training samples even though subnetworks formed trained potentially small number training samples models prone overﬁtting subnetworks share parameters resulting implicit regularization training process assumption made works large proportion regions piecewise linear activation units active training inference. instance rectiﬁer linear unit leaky-relu parametric-relu must sets points lying negative side another positive side activation function domain moreover maxout local winner takes activation units must sets points lying regions activation function domain assumption utmost importance violated activation units degenerate simple linear functions capable exponentially dividing input space training specialized subnetworks moreover learning models deep architectures violation assumption makes model ill-conditioned shown example below. paper propose introduction batch normalisation units piecewise linear activation units guarantee input data evenly distributed respect activation function domain results balanced regions piecewise linear activation units pre-conditions model. note goodfellow acknowledged assumption proposed dropout regularize training process dropout cannot guarantee balanced distribution input data terms activation function domain. furthermore dropout regularization technique help pre-condition model. therefore issues identiﬁed remains dropout. order motivate observation model proposed paper show problem illustrates well points. assume binary problem samples drawn using uniform distribution partition shown fig. colors blue yellow indicating class labels. train multi-layer perceptron varying number nodes layer varying number layers possible place types piecewise linear activation functions layer relu maxout maxout vary number regions also activation function option placing batch normalisation unit training based backpropagation using mini-batches size learning rate epochs another epochs momentum weight decay times training report mean train test errors. finally weights initialized normal distribution scaled layers. analysing mean train test error fig. ﬁrst notice models good generalization capability characteristic already identiﬁed deep networks piecewise linear activation units looking curves networks layers models seem trained properly models containing batch normalisation units produce smallest train test errors indicating higher capacity models. beyond layers models batch normalisation units become ill-conditioned producing errors effectively means points classiﬁed binary classes. general batch normalisation allows maxout deeper mlps contain nodes layer maxout function contains regions best result achieved layers layer contains nodes maxout regions best results relu also achieved batch normalisation using large number layers nodes layer notice smallest relu errors signiﬁcantly higher maxout ones indicating maxout larger capacity. images fig. show division input space used train subnetworks within model worth noticing best maxout model produces large number linear regions generate class regions similar original classiﬁcation problem. input space division used train subnetworks generated clustering training points produce activation pattern nodes layers mlp. also experiments using dropout relative results similar ones presented fig. test errors dropout around larger indicate dropout pre-condition model balance input data activation units experiment motivates propose model that contains large number layers nodes layer uses maxout activation function uses batch normalisation unit maxout layer. speciﬁcally extend network network model replace original relu units batch normalisation units followed maxout units. replacing relu maxout potential increase capacity model mentioned before batch normalisation units guarantee balanced distribution input data maxout units increases model capacity pre-conditions model. call proposed model maxout network maxout network fig. assess performance model following datasets cifar cifar- mnist svhn ﬁrst show empirically improvements achieved introduction maxout batch normalisation units model forming proposed model show study model provides better pre-conditioning proposed deep learning model ﬁnally show ﬁnal classiﬁcation results datasets above compared state demonstrated best ﬁeld datasets competitive mnist svhn. piecewise linear activation units section ﬁrst explain piecewise linear activation units followed introduction batch normalisation unit works presentation proposed model including training inference procedures. represents normalisation function non-linear activation function. preactivation function deﬁned wlxl− output layer denoting activations units layer output computed activations preceding layer gl)). also note array preactivation vectors after normalisation results array normalised vectors produced hli) activation unit layer represented gli)). piecewise linear activation units figure proposed model. model based model. model contains three blocks nearly identical architectures small differences terms number ﬁlters stride convolution layers. ﬁrst blocks pooling third block uses average pooling. according montufar network structure deﬁned input dimensionality number layers width layer. linear region function maximal connected subset note rectiﬁer units behaviour types constant linear small slope input negative; linear slope input positive. behaviours separated hyperplane hyperplanes within rectiﬁer layer forms hyperplane arrangement split input space several linear regions. multi-layer network uses rectiﬁer linear units inputs hidden layers nodes regions multi-layer network uses maxout activation units layers width rank compute functions kl−kn linear regions results indicate multi-layer networks maxout rectiﬁer linear units compute functions number linear regions grows exponentially number layers note linear regions observed colored polygons fig. number linear regions denoted subnets. training process networks containing piecewise linear activation units uses divide conquer strategy moves classiﬁcation boundary layer according loss function respect points current linear region moves offending points away current linear regions. dividing data points exponentially large number linear regions advantageous training algorithm focus minimizing loss regions almost independently others uses divide conquer algorithm. also almost independent training linear region training parameters region shared regions helps regularization training process. however initialization training process critical data points evenly distributed beginning points regions piecewise linear unit. drive learning classiﬁcation boundary speciﬁc linear region loss minimized points region boundary linear regions trained less effectively much fewer points. means even points relatively high loss remain initial region regions ineffectively trained consequently larger loss. issue clear maxout units extreme case regions active means maxout unit behave simple linear unit. large amount maxout units behave linear units order force initialization distribute data points evenly domain piecewise activation functions large proportion regions used propose batch normalisation ioffe szegedy’s normalisation proposed difﬁculty initializing network parameters setting value learning rate also inputs layer affected parameters previous layers. issues lead complicated learning problem input distribution layer changes continuously issue called covariate shift main contribution batch normalisation introduction simple feature-wise centering normalisation make mean zero variance followed batch normalisation unit shifts scales normalised value. instance assuming input normalisation unit unit consists stages shift scale parameters network parameters participate training procedure another important point unit process training sample independently uses training sample samples mini-batch. maxout network maxout network model mentioned sec. number linear regions networks piecewise linear activation unit grows exponentially number layers important many layers possible order increase ability network estimate complex functions. reason extend recently proposed network network model based uses multi-layer perceptron activation layer original formulation model introduces mlpconv relu activation convolution layer replaces fully connected layers classiﬁcation spatial average feature maps last mlpconv layer softmax layer. particular extend model replacing relu activation convolution layer mlpconv maxout activation unit potential increase even model capacity. addition also unit maxout units. contributions form proposed model give simple name maxout network maxout network model depicted fig. finally include dropout layer blocks regularizing model. table proposed model architectures used experiments. maxout-conv unit convolution kernel deﬁned ﬁrst block second block contains information convolution stride padding maxout rank third contains units. layer maxout-mlp unit equivalent maxout-conv unit convolution kernel size. softmax layer present last layer model model used cifar- svhn model bottom mnist. evaluate proposed method four common deep learning benchmarks cifar- cifar- mnist svhn cifar- dataset contains images classes common visual objects images training rest testing. cifar- extension cifar- difference cifar- classes training images testing images class. cifar- visual objects well-centred images. mnist dataset standard benchmark comparing learning methods. contains grayscale images numerical digits divided images training images testing. finally street view house number dataset real-word digit dataset images containing images house numbers cropped digits well-centred original aspect ratio kept distracting digits present next centred digits interest. dataset partitioned training test extra sets extra images less difﬁcult samples used extra training data. datasets validate algorithm using training validation splitting described goodfellow order estimate model hyperparameters. reported results training processes different model initializations test results consist mean standard deviation errors runs. model initialization based randomly producing weight values using normal distribution multiplied ﬁrst layer ﬁrst block remaining layers. moreover perform data augmentation datasets compare model state-of-the-art methods report data-augmented results. implementation matconvnet toolbox experiments standard below ﬁrst show experiments demonstrate performance original model introduction maxout units comprise contributions paper form model. then show study units pre-conditions model. finally show comparison proposed model current state aforementioned datasets. introducing maxout batch normalisation section cifar- show contribution provided component proposed paper. ﬁrst tab. shows published results ﬁrst experiment replace relu units model maxout units training test experiments described second include units relu unit model show test results third tab. finally include maxout units model effectively forms proposed model test results displayed fourth tab. ill-conditioning study real datasets results sec. show replacement relu maxout increases test error cifar- similarly shown fig. introduction relu activation units provide signiﬁcant improvement test error introduction units maxout units produce smallest error happens even input data distribution respect activation function domain resulting balanced regions maxout units. study sec. clearly shows introduction units pre-conditions model allowing large learning rates produce accurate classiﬁcation. comparison current state shows proposed model produces best result ﬁeld cifar- cifar-. mnist best result runs comparable best result ﬁeld. finally svhn result slightly worse current best result ﬁeld. interesting point make respect number regions maxout units. note notice signiﬁcant improvement bigger values also computational time memory requirements training become intractable. paper provides empirical demonstration combination piecewise linear activation units units provides powerful framework explored design deep learning models. speciﬁcally work shows guarantee assumption made piecewise linear activation units balanced distribution input data units. empirical evfigure ill-conditioning measured model’s inability converge function learning rate. training error test error models trained without stay initial error learning rate certain value showing sign convergence maxout units standard model ensure learning rate varying parameter. train distinct models learning rates cifar- mnist plot error curves mean standard deviation values. fig. without deep learning model become ill-conditioned. also interesting un-normalized models give best performance right learning rate drives ill-conditioning mode. compare proposed model stochastic pooling maxout networks network network deeply-supervised nets recurrent cifar- show results tab. comparison cifar- state-of-the-art models above also tree based priors shown tab. performance mnist model compared stochastic pooling conv. maxout+dropout network network deeplysupervised nets recurrent tab. important mention best result observed model mnist runs finally model ﬁrst tab. compared models above plus dropconnect svhn results displayed tab. matthew zeiler sixin zhang yann fergus regularization neural networks using dropconnect proceedings international conference machine learning references guido montufar razvan pascanu kyunghyun yoshua bengio number linear regions deep neural networks advances neural information processing systems rupesh srivastava jonathan masci sohrob kazerounian faustino gomez j¨urgen schmidhuber compete compute advances neural information processing systems alex krizhevsky ilya sutskever geoffrey hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning. granada spain vol. rupesh kumar srivastava jonathan masci faustino gomez j¨urgen schmidhuber understanding locally competitive networks international conference learning representations sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol.", "year": 2015}