{"title": "FastNet", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Inception and the Resnet family of Convolutional Neural Network archi-tectures have broken records in the past few years, but recent state of the art models have also incurred very high computational cost in terms of training, inference and model size. Making the deployment of these models on Edge devices, impractical. In light of this, we present a new novel architecture that is designed for high computational efficiency on both GPUs and CPUs, and is highly suited for deployment on Mobile Applications, Smart Cameras, Iot devices and controllers as well as low cost drones. Our architecture boasts competitive accuracies on standard Datasets even out-performing the original Resnet. We present below the motivation for this research, the architecture of the network, single test accuracies on CIFAR 10 and CIFAR 100 , a detailed comparison with other well-known architectures and link to an implementation in Keras.", "text": "present motivation research architecture network single test accuracies cifar cifar detailed comparison well-known architectures link implementation keras. recent models explored absolute depth wide resnet uses high depth great width. however results works demonstrates rate increase depth directly proportional increase accuracy marginally disproportionate. increasing depth greatly often slows training inference little gain accuracy. questions benefits high depth neural networks. challenges rather seek efficient means improving model accuracy. layer made batchnormalization followed relu activation finally convolution. batch normalization introduced used normalize feature maps zero mean unit variance. helps correct internal covariate shift phenomenon results shift distribution activation maps result changing parameters. batch normalization expressed formally kaiming imagenet ground breaking work residual connections. improved work identity mappings deep residual networks almost architectures since based upon framework fact architectures adaptations original residual framework exception fractal stochastic depth wide resnets share resnet sought improve speed residual networks. however still focus exploration depth represent significant improvements original resnet still unsuitable deployment edge devices. fractal great deviation resnet family. highly impressive work proved residual connections absolute requirement improving accuracy. essentially uses highly parallel layers central design architecture. excellent gpus layer parallelization good dependent edge devices. limited cores thread context switching would fundamentally hamper performance network devices. note observed number parameters resnet stochastic depth slightly fastnet however actual difference model size performance speed much higher significant margin beyond difference number parameters tells. networks thin layers makes parameters less high depth significantly slows down. independent researchers limited resources cannot present conduct experiments imagenet. however good results cifar cifar always yields good results imagenet well makes highly optimistic fastnet. hope availability resources near future shall able significantly improve upon baseline work. make computer vision available everyone absolute need great research accurate models setting state accuracy highly efficient models work well cost edge devices. search deeply direction efficiency someday build computer vision systems near human efficiency.", "year": 2018}