{"title": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Motivated by an important insight from neural science, we propose a new framework for understanding the success of the recently proposed \"maxout\" networks. The framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. Elaborating further on this insight, we propose a novel deep network architecture, called \"channel-out\" network, which takes a much better advantage of sparse pathway encoding. In channel-out networks, pathways are not only formed a posteriori, but they are also actively selected according to the inference outputs from the lower layers. From a mathematical perspective, channel-out networks can represent a wider class of piece-wise continuous functions, thereby endowing the network with more expressive power than that of maxout networks. We test our channel-out networks on several well-known image classification benchmarks, setting new state-of-the-art performance on CIFAR-100 and STL-10, which represent some of the \"harder\" image classification benchmarks.", "text": "motivated important insight neural science propose framework understanding success recently proposed maxout networks. framework based encoding information sparse pathways recognizing correct pathway inference time. elaborating insight propose novel deep network architecture called channel-out network takes much better advantage sparse pathway encoding. channel-out networks pathways formed posteriori also actively selected according inference outputs lower layers. mathematical perspective channel-out networks represent wider class piece-wise continuous functions thereby endowing network expressive power maxout networks. test channel-out networks several well-known image classiﬁcation benchmarks setting state-of-the-art performance cifar- stl- represent harder image classiﬁcation benchmarks. recent work deep learning focused ways regularize network behavior avoid over-ﬁtting. dropout widely accepted effective deep network regularization. dropout initially proposed avoid co-adaptation feature detectors turns also regarded efﬁcient ensemble model. maxout network newly proposed micro architecture deep networks works well dropout technique. sets state-of-the-art performance many popular image classiﬁcation datasets. retrospect methods follow approach restrict updates triggered training sample affect sparse sub-graph network. note neural science researchers come perceived similar principle years study human brain shape signal pathway along signal ﬂows determines functionality principle important clue explore encoding capabilities deep networks. paper provide insight possible reason success maxout namely partially takes advantage call sparse pathway coding much robust encoding categorical information encoding magnitudes. sparse pathway encoding pathway selection carries signiﬁcant amount categorical information. carefully designed scheme network extract pattern-speciﬁc pathways training time recognize correct pathway inference time. guided principle propose class network architectures called channel-out networks. unlike maxout network type networks form sparse input pathways posteriori also actively selects outgoing pathways. experiments channel-out networks using several image classiﬁcation benchmarks show similar better performance results compared maxout. fact channelnetwork sets state-of-the-art performance image classiﬁcation datasets harder spectrum namely cifar- stl- demonstrating potential encode large amounts information higher level complexity. channel-out example using sparse pathway encoding principle believe promising research direction designing types deep network architectures. dropout regularizes network training phase randomly crossing nodes upon processing training sample. therefore restricts updates happen along relatively sparse subnetwork. pointed number papers dropout networks regarded performing efﬁcient model averaging large ensemble models randomly sampled original network model learns different representation data. maxout network recently proposed microarchitecture signiﬁcantly different traditional networks activation function take normal single-input-single-output form instead maximum activations several candidate nodes. advantage maxout normal differentiable activation functions attributed better approximation exact model averaging advantage maxout rectiﬁed linear activation function attributed easier optimization maxout networks. propose another insight power maxout network. maxout node activates candidate input pathways depending input gradient back-propagated selected pathway updated information encodes therefore information conveyed training sample compactly encoded onto local portion network instead disseminated across entire network. call behavior sparse pathway coding. moreover local invariance max-linear function network remember correct pathway selection similar test sample. inference time test sample similar certain pattern encoded likely activate pathway similar activated pattern. correct inference therefore made awareness pattern-speciﬁc pathway. maxout network encodes information sparsely distinct pathways capability retrieving correct pathway inference time. observation well line famous principle neural science information conveyed action potential determined form signal pathway signal travels brain link selection ever made. higher layer pathway selections aware selections made lower layers. natural extension endow network capability active output pathway selection result efﬁcient pathway coding. along line propose class deep network architectures called channelnetworks. channel-out network characterized channel-out groups linear portion typical layer output nodes arranged groups group special channel selection function performed decide channel opens information ﬂow. activation selected channel passed through channels blocked gradient back-propagated channel-out layer passes open channels selected forward propagation. formally ﬁrst deﬁne vector-valued channel selection function takes input vector length outputs index length elements index selected domain input vector channel-out group implements following activation functions indicator function indexes candidates channel-out group candidate input output many possible choices channel selection function ensure good performance require channel selection function possesses following properties although maxout networks implement concept sparse pathway coding pathways formed posteriori meaning pathway selection implicitly implemented already-formed input paths active examples good channel selection functions median indices largest candidates absolute-max max. test results reported paper used function. characteristics channel-out network better illustrated comparison maxout network maxout network maxout node takes maximum value candidates without outgoing links aware index. contrast channelnetwork channel-out group knows index open channel output link selection made possible extra piece information. summary main characteristic channel-out network channel-out group uses channel selection information determine future inference pathways. deep convolutional network image classiﬁcation channel-out groups implemented performing channel selection function across groups feature maps convolution/multiplication. similar maxout networks channel-out regarded special kind cross-feature pooling. moreover addition channel output value index vector also recorded index matrix open pathway retrieved back-propagation. take advantage index vector implement sparse convolution sparse matrix multiplication avoiding wasteful computations multiplications zeros. figure difference maxout channel-out maxout node attached fixed output links resulting output pathway different input pathways; channel-out group connected different output links resulting distinct output pathways. potentially make computation fast. training channel-out network similar number parameters maxout network assuming scalar channel selection function theory done times faster maxout network size channel-out/maxout groups. currently working fast implementation. conﬁrm pathway selection indeed indicative patterns data record pathway selections well-trained maxout model channel-out model channel selection function) using cifar- dataset. ease visualization analysis size maxout/channel-out groups represent pathway selection maxout/channel-out group. figure shows channel-out pathway patterns classes records pathway selections channel-out groups performing inference speciﬁc test sample. clearly observe many class-speciﬁc string patterns indicating pathway selection patterns indeed subsume important clue categorization. salient patterns marked rectangles. better visualize space pathway patterns perform analysis pathway pattern vectors project three dimensional space. figures show results channel-out maxout respectively. clusters well formed. although clusters perfect still demonstrate considerable amount categorical information encoded pathway patterns. also channel-out model results better clusters generated maxout. example frog class better separated classes channel-out case. another interesting observation that channel-out maxout models trained independently completely random parameter initialization relative positions classes visualization show similar spatial structures. implies pathway pattern provides robust representation underlying patterns data. next sub-sections argue different perspectives channel-out network manifests concept sparse pathway selection better maxout potentially exhibit better performances classiﬁcation tasks. universal representation/approximation property good indication expressive power model. fact two-layer circuit logic gates represent boolean function implies nonlinear expressive power early shallow neural networks. existence functions computable polynomial-size networks layers requiring exponential-size networks layers implies beneﬁt using deep architectures recently fact maxout networks universal approximator continuous function indication good performance channel-out networks also strong universal approximation properties. take channel selection function example prove following theorem theorem piece-wise continuous function deﬁned compact domain euclidean space approximated arbitrarily well two-layer channelnetwork hidden channel-out group ﬁnite number candidate nodes. denote target function approximated two-layer channel-out network hidden channel-out group implements function tinuous function refer function constituted ﬁnite number continuous segments. local learning behaviors maxout channel-out networks categorized classes activation tuning pathway switching activation tuning happens desired parameter update signiﬁcant enough change pathway selection otherwise pathway switching occurs. channel-out maxnetworks show different behaviors upon pathway switching. discuss simple scenario repeatedly present training sample network. maxout node pathway switching happens gradient propagated maxout node must decrease activation. however effective activation decrease thresholded activation level second largest candidate activation slows down steer away decreasing trend scenario updates typically alternate candidate pathways reducing overall efﬁciency updates. contrast channel-out network whenever pathway switches channel-out group effective structure channel-out group drastically changed distinct output links selected channel-out group resulting signiﬁcant change desired direction activation update. words current pathway ﬁtting data well channel-out group tends switch start training pathway scratch. clearly desired behavior pathway switching. tion method section consider different principles underlying sparse pathway methods dropout terms regularizing network training. argue strengths sparse pathway selection dropout complement other. explains main reason sparse pathway methods combined dropout outperforms traditional neural network models. believe sparse pathway encoding general direction well worth pursuing future research designing deep network architectures. maxout channel-out networks examples along direction. recall dropout presentation training sample samples sub-network encodes information revealed training sample onto sub-network. since sampling data sub-networks independent processes statistical sense information provided training sample eventually squeezed sub-networks advantage scheme pointed various papers piece information encoded many different representations adding robustness inference time. side-effect highlighted before encoding conﬂicting pieces information densely sub-networks small capacities causes interference problem. data samples different patterns attempt build different maybe highly conﬂicting network representations. sub-network large enough hold information conﬂicting parts tend cancel other resulting signiﬁcant information loss. extreme case practice training several networks independently combine naive averaging link weights clearly make much sense general. note although dropout disseminating training information entire network signiﬁcantly different normal deep network without regularization. latter retrieval information relies overall cooperation entire network subnetwork encodes part information contrast dropout network tries encode entire information sub-network. subnetworks interact averaging rather cooperation. contrast dropout sparse pathway regularization methods tend encode information specialized way. pattern training data likely encoded onto specialized sub-networks illustrated fourth figure figure information encoding patterns. network capacity represents certain size sub-network. dropout tends encode patterns capacity resulting efﬁcient network capacity high level interference; sparse pathway methods tend encode pattern speciﬁc sparse sub-network resulting least interference waste network capacity; best approach combination schemes. sparse path regularization however network capacity could under-utilized. since sub-networks highly pattern-speciﬁc fundamental patterns data small subset entire network selected trained leaving rest network capacity idle state. waste network capacity. hard understand combining dropout sparse pathway encoding could result better performance combination takes advantage strengths methods efﬁcient information coding illustrated last figure notice encoding data pattern sparse pathways also different encoding partial patterns sub-networks done normal network without regularization. non-regularized network information encoded sub-network likely form complete patterns sparse pathway methods certain subnetwork always tries encode complete patterns. note also signiﬁcant difference sparse pathway methods standard sparse feature learning techniques sparse pathway model must also possess ability recognizing correct pathway inference time. empirical observation support arguments dropout/sparse pathway encoding pattern seen take extremes either regularization directions experiments. observe dropout rate high dominating problem under-ﬁtting training test precision cease improve early epochs. dropout applied dominating problem over-ﬁtting training precision growing fast test precision catching improvement training ﬁtness. line major assumption deﬁciencies method dropout causes interference pure sparse pathway encoding forgoes using redundant network capacity enhance robustness thus vulnerable over-ﬁtting. section show performance channelnetwork several image classiﬁcation benchmarks. channel-out networks used experiments channel selection function function. tests cifar- cifar- stl- signiﬁcantly outperforming state-of-the-art results last datasets. implementation built efﬁcient convolution cuda kernels developed alex krizhevsky experiments done using tesla gpu. time limit control size networks could limited effort optimizing hyper-parameters despite constraints still managed state-of-the-art performance cifar- stl- indicating great potential channel-out networks difﬁcult classiﬁcation tasks. believe further improve current result channel-out networks cifar- parameters better tuned. effort developing optimized channel-out networks easier image classiﬁcation tasks mnist svhn since believe tasks demonstrate strengths channel-out networks better encoding large amount highly variant patterns. cifar- dataset consists small color images object classes. training images test images class. signiﬁcant variations images regarding shape pose images well centered. previous experiments cifar- typically take methodologies without data augmentation. better compare generalization power different models focus non-augmented version namely using whole images without translation ﬂipping rotation. however perform whitening using make cifar whitened.py code pylearn repository convolutional channel-out layers followed fully connected channel-out layer softmax layer. best model ﬁlters corresponding convolutional layers nodes fully connected layer. convolutional layer consists linear convolution portion pooling portion channel-out portion. fully-connected layer fully-connected linear portion channel-out portion. channel-out group sizes corresponding layers. dropout probability applied inputs layers dropout probability applied input ﬁrst layer image itself. would like thank goodfellow etc. providing opensource maxcode pylearn since many hyper-parameters cifar- dataset tuned help .yaml conﬁguration ﬁles pylearn repository. best test precision channel-out network good state-of-the-art reported using maxout network better previous methods best results cifar- without data augmentation summarized table believe channel-out performance could improved larger network hyper-parameters better tuned. aware fact performance highly dependent hyper-parameter settings implemented code maxout networks compared model using different control settings similar number parameters similar number feature maps. former case maxout network size maxnetwork size similar total number parameters channel-out network. latter case size i.e. exactly number feature maps channel-out network. note results signiﬁcantly less parameters maxout network. results shown table channel-out network nearly performance maxout network similar number parameters signiﬁcantly better maxout network number feature maps. argue second comparison reasonable sense although different numbers parameters implementations require number multiplication/addition operations resulting similar training time. faster version channel-out network still construction take advantage structured sparseness outputs channel-out groups. predict least signiﬁcantly faster maxout network similar number parameters. cifar- dataset similar cifar- classes. training images test images class. larger number labels smaller training sets make task much difﬁcult cifar- dataset. channel-out network developed cifar- convolutional layers followed fully connected layer softmax layer feature maps channel-out group sizes ---. similar cifar- experiment applied dropout probability input layer dropout layers. images pre-whitened presented network time horizontally ﬂipped probability test precision improving current state-of-the-art nearly percentage points. table shows best results cifar-. motivated better performance cifar- perform another experiment illustrate fact channel-out networks better harder tasks data patterns show variances. extract classes original dataset generate training-testing pairs. train channelmaxout network similar number parameters four classiﬁcation tasks. performance results shown figure note facilitate faster experimentation deliberately used much smaller networks series experiments. results justify claim channel-out best suited case relatively small amount information needs stored case interference patterns less problem. patterns become complex channelsurpasses maxout better implementation sparse pathway encoding. notice unusual fact methods perform better -class task -class task. uneven distribution difﬁculty among data. ﬁrst classes seem difﬁcult discriminate. stl- contains images classes typically variant shapes complex backgrounds cifar- images. figure shows comparison between stl- cifar- images. original images size training test images class. additional unlabeled images unsupervised learning. best channel-out network stl- consists three convolutional layers followed fully-connected layer softmax layer feature maps ---. channel-out group sizes ---. dropout applied before. reduce complexity down-sampled images images pre-whitened horizontally ﬂipped probability presented network. state-of-the-art test precision improves current stategood performance stl- shows channelnetwork better discriminating variant patterns ruling interference complex backgrounds. demonstrates information stored efﬁciently channel-out network. introduced concept sparse pathway coding argued robust efﬁcient encoding categorical information deep network. using sparse pathway encoding interference conﬂicting patterns mitigated therefore combined dropout network utilize network capacity effective way. along direction proposed novel class deep networks channel-out networks. experiments show channel-out networks perform well image classiﬁcation tasks especially harder tasks complex patterns. believe concept sparse pathway coding well worth pursuing designing robust deep networks. upon ﬁnishing paper found recent work idsia proposed similar model version channel-out network. work independently developed. compared work model motivated analysed different perspective. also provided theoretical experimental results regarding channel-out model. references bengio. learning deep architectures foundations goodfellow warde-farley lamblin dumoulin mirza pascanu bergstra bastien bengio. pylearn machine learning research library. arxiv preprint arxiv. hinton srivastava krizhevsky sutskever salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. huang darrell. beyond spatial pyramids receptive ﬁeld learning pooled image features. computer vision pattern recognition ieee conference pages ieee kandel schwartz jessell principles neural science volume mcgraw-hill york krizhevsky sutskever hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume theorem piece-wise continuous function deﬁned compact domain euclidean space approximated arbitrarily well two-layer channel-out network hidden channel-out group ﬁnite number candidate nodes. i.e. denote target function approximated two-layer channel-out network hidden channel-out group implements function inductively construct prototype function piece-wise linear continuous convex deﬁned hypercube centered origin covering domain target function. it’s easy verify constructed convex piece-wise continuous. thus expressed max-linear function max. furthermore assign linear weights input weights channel-out group. channel selection function constructed max. i.e. function constructed approximate arbitrary precision total number lattices construct output weights channel-out network previously deﬁned). easily channel-out network implements function", "year": 2013}