{"title": "Manifold Relevance Determination", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a \"softly\" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.", "text": "paper present fully bayesian latent variable model exploits conditional nonlinear -dependence structures learn efﬁcient latent representation. latent space factorized represent shared private information multiple views data. contrast previous approaches introduce relaxation discrete segmentation allow softly shared latent space. further bayesian techniques allow automatically estimate dimensionality latent spaces. capable capturing structure underlying extremely high dimensional spaces. illustrated modelling unprocessed images tenths thousands pixels. also allows directly generate novel images trained model sampling discovered latent spaces. also demonstrate model prediction human pose ambiguous setting. bayesian framework allows perform disambiguation principled manner including latent space priors incorporate dynamic nature data. multiview learning characterised data contain observations several different modalities example depth cameras provide colour depth images scene meeting might represented audio video feed. motivates latent variable models align different views assuming portion data variance shared modalities whilst explaining remaining variance latent spaces private modality. model structure allows inference subset modalities available observation spaces aligned possible transfer information modalities conditioning model underlying concept. several approaches combine multiple views suggested. line work aims low-dimensional representation observations seeking transformation view. different approaches exploit different characteristics data correlation mutual information however methods encode shared variance provide probabilistic model. address shortcomings different generative models suggested. particular approaches formulated gaussian processes latent variable models especially successful however models assume single latent variable capable representing modality implying modalities fully aligned. overcome this idea factorized latent space presented view associated additional private space representing variance cannot aligned addition shared space idea independently suggested klami kaski main challenge applicability proposed models factorization latent variable structural essentially discrete property model making challenging learn. salzmann introduced regularizers allowing dimensionality factorization learned. however regularizers motivated necessity rather principle introduced several additional parameters model. present principled approach learning factorized latent variable representation multiple observation spaces. introduce relaxation structural factorization model original hard discrete representation latent variable either associated private space shared space smooth continuous representation latent variable important shared space private space. contrast previous approaches model fully bayesian allowing estimation dimensionality structure latent representation done automatically. further provides approximation full posterior latent points given data.we describe model variational approximation next section. model capable handling extremely high dimensional data. illustrate modelling image data directly pixel space section also demonstrate model’s ability reconstruct pose silhouette human motion example ﬁnally considering class labels second ‘view’ dataset show model used improve classiﬁcation performance well known visualization benchmark data. model wish relate views rn×dy rn×dz dataset within model. assume existence single latent variable rn×q which mappings gives dimensional representation data. assumption data generated dimensional manifold corrupted additive gaussian noise \u0001{yz} z}nd represents dimension point leads likelihood model collectively denotes parameters {yz} mapping functions noise variances finding latent representation mappings ill-constrained problem. lawrence suggested regularizing problem placing gaussian process priors mappings resulting models known gaussian process latent variable models fully bayesian treatment requires integration latent variable equation intractable appears non-linearly inverse covariance matrices priors practice maximum posteriori solution often used. however failure marginalize latent variables means possible automatically estimate dimensionality latent space parameters prior distributions used latent space. show obtain approximate bayesian training inference procedure variationally marginalizing achieve building recent variational approximations standard gp-lvms introduce automatic relevance determination priors view data allowed estimate separate vector parameters. allows views determine emerging private shared latent spaces relevant them. refer idea manifold relevance determination wish recover factorized latent representation variance shared different observation spaces aligned separated variance speciﬁc separate views. manifold relevance determination notion hard separation between private shared spaces relaxed continuous setting. model allowed completely allocate latent dimension private shared spaces also choose endow shared latent dimension less relevance particular dataview. importantly factorization learned data maximizing variational lower bound model evidence rather construction bespoke regularizers achieve effect. model propose seen generalisation traditional approach manifold learning; still assume existence low-dimensional representation encoding underlying phenomenon variance contained observation space necessarily need governed figure evolution structure gp-lvm model variants. left lawrence original model shown single latent variable used represent observed data evolved shared models assume ﬁrstly variance observations shared secondly introduced private latent spaces explain variance speciﬁc views. estimates used model meant structure latent space could automatically determined. rightmost image shows model propose paper. ﬁgure separated weights w{yz} w{yz}} emphasize usage covariance functions. full model hyperparameters θ{yz} latent space marginalised learn distribution latent points additional hyperparamters encode relevance dimension independently observation spaces thus automatically deﬁne factorisation data. distribution placed latent space also enables incorporation prior knowledge structure. expressive power model comes ability consider non-linear mappings within bayesian frameselected work. speciﬁcally latent functions independent draws zero-mean covariance function form ard)e− similarly accordingly learn common latent space allow sets weights automatically infer responsibility latent dimension generating points spaces respectively. automatically recover segmentation latent space subspace deﬁned dimensions number close zero equips model ﬂexibility allows softly shared latent space sets weights greater dissimilar general. private spaces also inferred automatically along dimensionalities bution placed prior standard normal distribution generally depend parameters looking integral intractable nonlinear appears standard variational approximations also intractable situation. here describe non-standard method leads analytic solution. starting point consider mean ﬁeld methodology seek maximise variational lower bound logarithm true marginal likelihood relying variational distribution factorises assume explained later clearly approach distribution depends additional variational parameters additional parameters well exact form deﬁned later constitute crucial ingredient non-standard variational approach. similarly however solve problem intractability since challenging terms still appear circumvent problem follow titsias lawrence apply data augmentation principle i.e. expand joint probability space extra samples latent functions evaluated pseudo-inputs respectively. here rmz×dz rmz×q expression joint probability similarly integrations u{yz} tractable assume gaussian prior distributions variables. shall inducing points variational rather model parameters. details variational learning inducing variables found titsias free form distributions. factors cancel difﬁcult terms seen replacing equations back becomes ﬁnal objective function trivially extended observed datasets. function jointly maximised respect model parameters involving latent space weights variational parameters ¯x}. standard variational inference optimisation gives by-product approximation i.e. obtain distribution latent space. adds extra robustness model since previous approaches rely estimates latent points. detailed derivation variational bound found suppl. material. dynamical modelling model formulation described previously also covering case wish additionally model correlations datapoints output space e.g. multivariate timeseries. dynamical scenario follow damianou lawrence moore choose prior latent space depend observation times e.g. covariance function approach also allowed learn structure multiple independent sequences share commonality learning common latent space timeseries while time ignoring correlations datapoints belonging different sequences. inference given model trained jointly represent output spaces common factorised input space wish generate outputs rn∗×dz given observed test points rn∗×dy done three steps. firstly predict latent points rn∗×q likely generated this approximation posterior form standard bayesian gp-lvm model given variational distribution optimise variational lower bound marginal likelihood analogous form training objective function speciﬁcally ignore replace second step training latent points closest shared latent space. third step outputs likelihood procedure returns training points best match observed test points wish generate novel outputs propagate information recovered predicting since shared latent space encodes kind information datasets achieve simply replacing features corresponding shared latent space complexity common sparse methods gaussian processes typical cubic complexity reduces total number training inducing points respectively. experiments further model scales linearly data dimensionality. indeed gaussian densities equation result objective function involves data matrices expressions form matrices matter many features used describe original data. also quantities constant precomputed. consequently approach model datasets large numbers features. method designed represent multiple views data factorized latent spaces. section show experiments exploit factorized structure. source code recreating experiments included supplementary material. yale faces show ability method model high-dimensional spaces ﬁrst experiment applied yale dataset contains images several human faces different poses illumination conditions. consider single pose subject variations location light source subject’s appearance. since model capable working high-dimensional data directly applied pixel values rely image feature extraction pre-process data directly sample novel outputs. full yale database constructed dataset containing pictures corresponding different illumination conditions subjects similarly different subjects. formed datasets consisting images corresponding three different faces possible illumination conditions therefore rn×d aligned order images dataset image ﬁrst randomly correspond possible zn’s second dataset depicted illumination condition matched datapoints datasets according illumination condition identity faces model explicitly forced learn correspondence face characteristics. latent space variational means initialised concatenating datasets performing pca. alternative approach would perform dataset separately concatenate dimensional representations initialise found initializations achieved similar results. optimized relevance weights visualized graphs ﬁgure figure relevance weights faces data. despite allowing soft sharing ﬁrst dimensions switched approximately weight views data. remaining dimensions used explain private variance. latent space clearly segmented shared part consisting dimensions indexed private irrelevant part data views allocated approximately equal weights shared latent dimensions visualized ﬁgures interaction three latent dimensions reveals structure shared subspace resembles hollow hemisphere. corresponds shape space deﬁned ﬁxed locations light source. indicates shared space successfully encodes information position light source face characteristics. indication enhanced results found performed dimensionality reduction standard bayesian gp-lvm pictures corresponding illumination conditions single face figure projection shared latent space dimensions projection −private dimensions clear latent points ﬁgure form three clusters responsible modelling three faces figure latent space learned standard bayesian gplvm single face dataset. weight associated learned latent space shown ﬁgures plotted pairs dominant latent dimensions other. dimensions small negligible weight represent minor differences pictures face subjects often blink smile etc. private manifolds discovered correspond subspaces disambiguating faces dataset. indeed plotting largest dimensions ﬁrst latent private subspace reveals three clusters corresponding three different faces within dataset. similarly standard bayesian gp-lvm applied single face private dimensions small weight model slight changes across faces subject also conﬁrm visually subspaces’ properties sampling novel inputs xsamp subspace mapping back observed data space using likelihoods thus obtaining novel outputs better understand kind information encoded dimensions shared private spaces sampled latent points varying dimension time keeping rest ﬁxed. ﬁrst rows ﬁgure show outputs obtained sampling across shared dimensions respectively clearly encode coordinates light source whereas dimension found model overall brightness. sampling procedure intuitively thought walk space shown ﬁgure left right bottom top. although learned latent inputs discrete corresponding latent subspace continuous interpolate images illumination conditions sampling areas training inputs similarly sample private subspaces obtain novel outputs interpolate non-shared characteristics involved data. results morphing effect across different faces shown last ﬁgure example videos found supplementary material. ﬁnal test conﬁrm efﬁcient segmentation latent space private shared parts automatically recovering illumination similarities found training set. speciﬁcally given datapoint ﬁrst dataset search whole space training inputs nearest neigbours latent representation based shared dimensions. latent points obtain points output space second dataset using likelihood seen ﬁgure model returns images matching illumination condition. moreover fact that typically ﬁrst neighbours given point correspond outputs belonging different faces indicates shared latent space pure polluted information encodes face appearance. figure given images ﬁrst column model searches shared latent space pictures opposite dataset illumination condition. images found sorted columns relevance. human motion data second experiment consider human poses associated silhouettes coming dataset agarwal triggs used subset sequences totalling frames corresponding walking motions various directions patterns. separate walking sequence frames used test set. pose represented −dimensional vector joint locations silhouette represented −dimensional vector features. given test silhouette features used model generate corresponding poses. challenging data multi-modal i.e. silhouette representation generated poses figure although poses second column dissimilar correspond resembling silhouettes similar feature vectors. happens information lost silhouette space also seen third column depicting poses silhouettes’ viewpoint. described inference section given test silhouettes model optimises test latent point ﬁnds series candidate initial training inputs sorted according similarity taking account shared dimensions. based initial latent points generates sorted series novel poses {z}k dynamical version model test points considered together predicted outputs forced form smooth sequence. experiments show initial training inputs typically correspond silhouettes similar given something conﬁrms segmentation latent space efﬁcient. however ambiguities arise example shown ﬁgure non-dynamical version model selecting correct input since points test sequence treated independently. dynamical version employed model forces whole training test inputs create smooth paths latent space. words dynamics disambiguate model. indeed seen ﬁgure method forced select candidate training input initialisation necessarily correspond training silhouette similar test one. more assume test pose known seek nearest training neighbour pose space corresponding silhouette similar figure sampling inputs produce novel outputs. first shows interpolation positions light source coordinate second coordinate last shows interpolation face characteristics produce morphing effect. note images presented scaled here suppl. material original -dimensional ones. figure given features test silhouette column predict corresponding pose using dynamical version nearest neighbour silhouette space obtaining results ﬁrst columns respectively. last ﬁrst poses rotated highlight ambiguities. notice silhouette shown second correspond exactly pose ﬁrst model generates novel pose given test silhouette. instead training silhouette found performing shared latent space. training pose given test pose shown column given above quantify results compare method linear gaussian process regression nearest neighbour silhouette space. also compared shared gp-lvm optimises latent points using therefore requires initial factorisation inputs given priori. finally compared dynamical version nearest neighbour kept multiple nearest neighbours selected coherent ones sequence. errors shown table well video provided supplementary material show performs better methods task. table mean euclidean distances joint locations predicted true poses. nearest neighbour pose space fair comparison reported provides insight lower bound error achieved task. mean training pose linear regression regression nearest neighbour nearest neighbour sequences nearest neighbour shared gp-lvm without dynamics dynamics classiﬁcation ﬁnal experiment demonstrate ﬂexibility model supervised dimensionality reduction scenario classiﬁcation task. training dataset created matrix contained actual observations matrix corresponding class labels -of-k encoding. used ‘oil’ database contains −dimensional examples split classes. selected random subsets data increasing number training examples compared nearest neighbor method data space. seen ﬁgure successfully determines shared information between data label space outperforms carl henrik torr phil lawrence neil. gaussian process latent variable models human pose estimation. proceedings international conference machine learning multimodal interaction georghiades a.s. belhumeur p.n. kriegman d.j. many illumination cone models face recognition ieee trans. pattern anal. variable lighting pose. mach. intelligence klami arto kaski samuel. generative models discover dependencies data sets. proceedings mlsp’ ieee international workshop machine learning signal processing lawrence neil. probabilistic non-linear principal component analysis gaussian process latent variable models. journal machine learning research memisevic roland sigal leonid fleet david shared kernel information embedding discriminative inference. transactions pattern analysis machine intelligence salzmann mathieu carl henrik urtasun raquel darinterrell trevor. factorized orthogonal latent spaces. national conference artiﬁcial intelligence statistics titsias michalis variational learning inducing variables sparse gaussian processes. proceedings twelfth international workshop artiﬁcial intelligence statistics volume presented factorized latent variable model multi view data. model automatically factorizes data using variables representing variance exists view separately variance speciﬁc particular view. model learns distribution latent points variationally. allows automatically dimensionality latent space well incorporate prior knowledge structure. example showed dynamical priors included latent space. allowed temporal continuity disambiguate model’s predictions ambiguous human pose estimation problem. model capable learning extremely high-dimensional data. illustrated learning model directly pixel representation image. model capable learning compact intuitive representation data exempliﬁed generating novel images sampling latent representation structured manner. finally showed generative model discriminative capabilities obtained treating observations class labels dataset separate modalities. research partially supported university shefﬁeld moody endowment fund greek state scholarships foundation would like thank reviewers useful feedback. bishop christopher james gwilym analysis multiphase ﬂows using dual-energy gamma densitometry neural networks. nuclear instruments methods physics research", "year": 2012}