{"title": "Neural networks and rational functions", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close, and similarly for any rational function there exists a ReLU network of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By contrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.", "text": "neural networks rational functions efﬁciently approximate other. detail shown relu network exists rational function degree \u0001-close similarly rational function exists relu network size \u0001-close. contrast polynomials need degree approximate even single relu. converting relu network rational function above hidden constants depend exponentially number layers shown tight; words compositional representation beneﬁcial even rational functions. signiﬁcant effort invested characterizing functions efﬁciently approximated neural networks. goal present work characterize neural networks ﬁnely ﬁnding class functions well-approximated neural networks also well-approximates neural networks. function class investigated class rational functions functions represented ratio polynomials denominator strictly positive polynomial. simplicity neural networks taken always relu activation max{ review neural networks terminology reader directed section sake brevity network relu activations simply called relu network. perhaps main wrinkle appearance approximating neural networks rational functions. following theorem shows dependence tight. theorem integer given. exists function computed relu network layers nodes rational function total terms numerator denominator must satisfy note statement implies desired difﬁculty approximation since integral distance implies earlier uniform distance furthermore r-degree rational function necessarily total terms numerator denominator. ﬁnal piece story note conversion between rational functions relu networks seamless instead converts rational networks meaning neural networks activation function rational function. lemma relu network given theorem meaning layers node computes pair satisﬁes exists rational function degree replacing yields function combining theorem lemma yields intriguing corollary. corollary every exists function computed rational network layers total nodes node invoking rational activation degree every rational function less total terms numerator denominator satisﬁes ﬁrst thing stress theorem impossible polynomials namely true relu networks efﬁciently approximate polynomials hand polynomials require degree rather approximate single relu equivalently absolute value function another point interest depth needed converting rational function relu network. theorem impossible depth speciﬁcally impossible approximate degree rational function size depth proposition reciprocal map. relu network layers nodes lastly implementation division relu network requires steps arguably interesting continuous switch statement computes reciprocals differently based magnitude input. ability compute switch statements appears fairly foundational operation available neural networks rational functions available polynomials hard-to-approximate function rational network description size despite this attempting approximate rational function usual form requires description size said anway even rational functions beneﬁt neural network representation results present work follow long line work representation power neural networks related functions. ability relu networks continuous functions doubt proved many times appears earliest reference lebesgue though course results type usually given much contemporary attribution recently shown certain function classes admit succinct representations many layers followed proofs showing possibility depth function require exponentially many nodes rewritten layers also variety result giving ability relu networks approximate various function classes recently variety works pointed neural networks approximate polynomials thus smooth functions essentially taylor’s theorem somewhat motivates present work since polynomials turn approximate neural networks dependence require degree even single relu. rational functions extensively studied classical approximation theory literature literature draws close connections rational functions splines connection used machine learning literature draw connections neural networks approximation theory literature following astonishing fact possible approximate absolute value function accuracy rational function degree moreover optimal rate known results form basis results show rational functions approximate relu networks. rational functions approximate well present work stick relu simplicity.) icml reviewer revealed prior work embarrassingly overlooked author known since decades neural networks using threshold nonlinearities approximate division moreover proof similar proof part theorem moreover work threshold networks invoked newman polynomials prove lower bound linear threshold networks together suggests connections rational functions neural networks tight also threshold networks relu networks perhaps similarities suggested differing dimension bounds approximation results algorithmic results brief description sorts neural networks used work. neural networks represent computation directed graph nodes consume outputs parents apply computation them pass resulting value onward. present work nodes take parents’ outputs compute vector scalar max{ anpopular choice nonlineary sigmoid graphs present work acyclic connected single node lacking children designated univariate output literature contains many variations choices. stated previously rational function ratio polynomials. following conventions approximation theory literature denominator polynomial always strictly positive. degree rational function maximum degrees numerator denominator. starting point seminal result theory rational functions exists rational function degree approximate absolute value function along accuracy turn gives approximate relu since newman polynomials depicted figure typical polynomials approximation theory instance chebyshev polynomials active oscillations; comparison newman polynomials look little funny lying close quickly increasing monotonically seminal result newman single relu easily converted rational function next task replace every relu relu network rational function compute approximation error. precisely statement lemma proof lemma induction layers full details relegated appendix. computation however follows. denote rational approximation layer denote multi-valued mapping computed layer denote mapping obtained replacing node layer denote output function input. easy rational function approximates neural network bound size. ﬁrst step lemma replace rational function degree second step inductively collapse network single rational function. reason dependence number nodes that unlike polynomials summing rational functions involves increase degree k-fold composition piecewise afﬁne function regularly spaced peaks function demonstrated inapproximable shallow networks subexponential size shown hard case rational approximation well. poor estimate number zeros simply degree however since univariate stronger tool becomes available descartes’ rule signs number zeros upper bounded number terms multiplication exponentiation representation result polynomials follows. lemma given. denote polynomial monomials degree scalar coefﬁcient within exists function computed network remainder proof focuses division operation. since multiplication handled sufﬁces compute single reciprocal. lemma nonnegative integer given. exists relu network size depth thanks earlier development exponentiation truncating summation gives expression easily approximate neural network follows. lemma given. exists relu network layers ln)) nodes satisfying representation polynomials based upon constructions yarotsky starting point following approximation squaring function. lemma given. exists represented relu network nodes layers supx∈ yarotsky’s proof beautiful deserves mention. approximation function deﬁned triangle section every convex piecewise-afﬁne interpolation points along graph going adjust interpolation points adds interpolation points. squaring place multiplication comes polarization identity y)/. lemma given. exists represented relu network nodes layers next follows relu networks efﬁciently approximate exponentiation thanks repeated squaring. lemma positive integer given. exists represented relu network nodes layers small choosing larger cause summation converge quickly. thus compute accurately wide range inputs solution multiplex approximations truncated many choices order rely value them possible encode large switch style statement neural network. notably rational functions also representat switch statements however polynomials lemma reals function given. moreover suppose exists relu network size depth along remains show shallow networks hard time approximating reciprocal proof uses scheme various proofs also followed recent works idea ﬁrst upper bound number afﬁne pieces relu networks certain size point linear segment must make substantial error curved function namely rational functions also higher ﬁdelity approximation achieved relu networks rational functions compared polynomials. course qualitative demonstration still lends intuition. demonstrations rational functions polynomials degree unless otherwise marked. relu networks hidden layers nodes. exactly apples apples still reasonable approximation literature ﬁxes polynomial rational degrees comparisons. figure shows ability three classes approximate truncated reciprocal. rational functions relu networks ability form switch statements approximate different functions different intervals complexity polynomials lack ability; even approximate relu well despite degree polynomials separate intervals. figure shows rational functions threshold function errily well; particular rational function used based using newman polynomials approximate figure shows newman polynomials discussed text unlike orthogonal polynomials used rational function approximations except figure used least squares figure shows polynomials rational functions relu relu representation based newman polynomials used proofs here. despite apparent slow convergence polynomials regime polynomial still quite respectable. rational functions approximating class used tightly bound generalization properties neural networks? notably dimension sigmoid networks uses conversion polynomials rational functions approximating class used design algorithms training neural networks? seem easy design reasonable algorithms minimization rational functions; fundamental moreover contrast neural networks suggests algorithmic beneﬁt neural networks. rational functions approximating class give sufﬁciently reﬁned complexity estimate neural networks turned regularization scheme neural networks? author thanks adam klivans suvrit stimulating conversations. adam klivans author thank almare gelato italiano downtown berkeley necessitating stimulating conversations topic health exercise. lastly author thanks university illinois urbana-champaign simons institute berkeley ﬁnancial support work. details converting relu network rational network follows. lemma represented relu network layers node computing every exists function obtained replacing relu r-rational function proof lemma construction newman-based approximation degree lemma degree sufﬁces guarantee first note induction layers output every node absolute value base case inputs themselves thus statement holds assumption inductive step consider node multivariate input node. inductive hypothesis thus such remains prove error bound. node denote function compute node denote function obtained replacing relus shown every node layer i\u0001/l base case inputs themselves thus approximation error meaning bound holds error \u0001/l. consider node layer suppose claim holds nodes layers lower. convenience denote multivalued computed previous layer next collapsing rational network single rational function proved follows. lemma rational network nodes layers activation function degree rational function obtained collapsing degree proof. throughout proof denote rational activation function node write polynomials degree proof establishes induction layers nodes layer compute rational functions degree base case layer node computes rational function ajgj deﬁne target function regular-spaced crossings along written network layers nodes. next consider rational function text necessary count zeros writing equivalently means zeros since together terms descartes’ rule signs crosses times along therefore following similar calculation proof lines inner loop squaring function lemma multiplication function lemma accuracy network returns ensure output lies procedure increase error. since loop invoke times inner loop requires network size full network size remains show network computes function satisﬁes approach product together individual variables concern multiplicities individual variables. denote coordinates desired multinomial. denote multiplication error provided lemma network compute scalar coefﬁcient monomial recursively deﬁned alternatively network uses fast exponentiation routine lemma multiplies together terms individual coordinates. particular exponentiation coordinate accuracy requires network size analysis similar preceding construction multiplying networks result network approximating monomial error size next proof relu networks efﬁciently compute reciprocals namely lemma stated text ﬁrst necessary establish lemma gives computes reciprocals choice magnitude lemma combines circuits across scales. family necessarily consecutive nonzero point interval. uniform \u0001-approximation relu networks multiplication lemma nodes layers moreover multiplication exact either input finally deﬁne proof lemma denote relu network \u0001-approximation along lemma layers nodes. furthermore max{i min{ ˜qi}} approximation size properties ˜qi. applying lemma reals i−k− functions follows exists \u0001-approximates along size depth ln). proof part theorem deﬁne \u0001/k+ lemmas choose relu network approximations resolution well relu network multiplication along approximate along resolution desired network compute function deﬁned verifying approximation guarantee upon necessary verify inputs correct magnitude lemmas applied. note ﬁrstly since implies thus −k−g) arguments within deﬁnition within consequently approximation guarantees lemmas hold whereby proof proposition relu network nodes layers computes function afﬁne along intervals forming partition cardinality subdivide collection intervals point intersects since convex afﬁne within existing piece subdivision number intervals three times large before. together total number intervals satisﬁes finally intersect family intervals obtaining ﬁnal number intervals denote ﬁnal partition denote corresponding interval lengths. index subcollection intervals length least meaning", "year": 2017}