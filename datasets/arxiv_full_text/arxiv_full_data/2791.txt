{"title": "Nonparametric Inference for Auto-Encoding Variational Bayes", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We would like to learn latent representations that are low-dimensional and highly interpretable. A model that has these characteristics is the Gaussian Process Latent Variable Model. The benefits and negative of the GP-LVM are complementary to the Variational Autoencoder, the former provides interpretable low-dimensional latent representations while the latter is able to handle large amounts of data and can use non-Gaussian likelihoods. Our inspiration for this paper is to marry these two approaches and reap the benefits of both. In order to do so we will introduce a novel approximate inference scheme inspired by the GP-LVM and the VAE. We show experimentally that the approximation allows the capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large without losing a highly interpretable representation, allowing reconstruction quality to be unlimited by Z at the same time as a low-dimensional space can be used to perform ancestral sampling from as well as a means to reason about the embedded data.", "text": "variational approximations attractive approach inference latent variables unsupervised learning. however often computationally intractable faced large datasets. recently variational autoencoders kingma welling proposed method tackle limitation. methodology based formulating approximating posterior distributions terms deterministic relationship observed data consequently title auto-encoding variational bayes. importantly decision regarding approximate inference scheme confused auto-encoder model. unsupervised learning ill-conditioned problem requires prior knowledge reach solution. would like learn latent representations low-dimensional highly interpretable. model characteristics gaussian process latent variable model lawrence beneﬁts negative gp-lvm complementary former provides useful low-dimensional latent representations latter able handle large amounts data non-gaussian likelihoods. inspiration paper marry approaches reap beneﬁts both. order introduce novel approximate inference scheme inspired gp-lvm vae. standard formulation kingma welling adopts unit gaussian prior creating trade-off embedded data residing location latent space ability reconstruct data observed space. encourages tight packing data around shared origin hope similar data observed space overlapping probability mass latent space i.e. mutual information. shown simple prior over-regularizes latent space leading poor reconstructions hoffman johnson ﬂexible priors used change dynamic reconstruction mutual information recent work using mixture tomczak welling prior autoregressive chen quality output improved expressive generative models shown lead tendency ignoring latent space defeating purpose unsupervised learning zhao summary reconstruction quality mutual information latent space traded other. paper address limiting trade-off escaping space encourage sharing separated space generative capacity set. approximation model observations generated either space. show experimentally approximation allows capacity generative bottle-neck arbitrarily large without losing sharing beneﬁcial properties sharing space allowing reconstruction quality unlimited time low-dimensional space used perform ancestral sampling well means reason embedded data. kingma welling inference scheme optimises traditional evidence lower bound latent space posterior approximated deterministic relationship observed latent variable conditionally independent given figure contrasting variational approximation schemes unsupervised learning. specify unsupervised generative model latent observed proposes fully factored variational approximation. inference proceeds conditioning variational latent parameters observed data explicit deterministic function model proposes additional latent space ties together factored space. inference proceeds also conditioned observed data additional deterministic function. tractable inference match moments observed data. paper introduce additional latent variable model interaction latent variables approach means longer independent conditionally independent given leads following updated evidence lower bound additional divergence term standard lower bound approximative posterior respectively. facilitate batch processing rather matching joint distribution latent space match predictive posteriors. conditionally independent given observed data leads following updated objective function covariance function. evaluating posterior datapoints computationally expensive inverse covariance function. proceed introduce additional approximations ﬁrst approximate mean predictive posterior directly parametrising intuition behind want encourage sharing latent space space represented distributed fashion. secondly rather minimising kl-divergence match ﬁrst mode distributions. leads ﬁnal objective function prediction latent space prediction using approximative posterior vae. effect separated models retaining connection matching ﬁrst modes predicting latent space proceed show experimental evaluation model showing capable using additional lowdimensional latent space proxy latent space infer parameters. ensure subtracting diagonal computed normalise rows create convex combination. latent locations represented implicitly function observed data lawrence quiñonero-candela trained models illustrate extension used obtain low-dimensional space highly interpretable permitting high-dimensional space provide high quality data generation. validate approach comparison standard showing data embeddings space generation data. experiments performed decoder encoders multilayer perceptrons architecture original kingma welling used hidden layers units each mini-batch sizes drop-out probability throughout training. decoder used bernoulli variant. furthermore adam kingma optimiser used learning rate varied dimensionality inner layer autoencoder experiments. used mnist data lecun comprised training examples test examples pixel greyscale images corresponding data dimensions. fig. show -dimensional space corresponding -dimensional space training test data embedded well examples data generation. despite using high capacity space nonparametric still sample dimension using ancestral sampling space. ensures test samples maintain high ﬁdelity space highly interpretable easy visualise easy sample from. fig. show corresponding space embeddings different dimensionalities demonstrates space maintains virtues independent dimension. finally fig. show sample interpolations latent space standard extension illustrating reconstruction quality preserved interpolations meaningful. presented hierarchical model unsupervised learning associated efﬁcient approximative inference scheme. inference takes inspiration amortised inference recognition model parameterise approximate posterior using deterministic relationship observed data. rather using traditional mean-ﬁeld approximation forces latent representation independent introduce additional latent representation models dependence. model results signiﬁcantly lower dimensional latent representation allowing visualise generate data intuitive manner without sacriﬁcing quality reconstruction. shown experimental results retain representative power dimensional model dimensional latent space. figure learned space embeddings nonparametric vae. inferred locations training data test data colors encoding mnist digit classes. generated samples corresponding locations using space dimensions. figure latent space visualisation. upper space embedding visualised standard possible bottom space dimensionalities used nonparametric allows space visualised sampled spaces higher dimension become impractical visualise interpret whereas space provides embedding easy display interpretation. figure latent space interpolation. upper rows show interpolants mnist training examples standard latent dimensionality bottom rows show interpolants training examples nonparametric respective dimensionalities interpolation performed inferred latent space dimension observe similar reconstruction quality obtained corresponding z-dimensionalities however interpolants space nonparametric meaningful credible intermediate states digits. thus obtain dimensional latent space provides interpretability without sacriﬁcing reconstruction quality. matthew hoffman matthew johnson. elbo surgery another carve variational evidence lower bound. workshop advances approximate bayesian inference nips chen diederik kingma salimans duan prafulla dhariwal john schulman ilya sutskever pieter abbeel. variational lossy autoencoder. arxiv preprint arxiv. neil lawrence quiñonero-candela. local distance preservation gp-lvm back constraints. proceedings international conference machine learning pages", "year": 2017}