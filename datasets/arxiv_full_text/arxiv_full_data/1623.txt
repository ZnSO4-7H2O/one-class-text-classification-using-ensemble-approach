{"title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with  Weak Supervision", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "text": "figure main challenges training semantic parser weak supervision compositionality variables store execution results intermediate generated programs. search prune search space augment reinforce pseudo-gold programs. bolic representation executed executor weak supervision remains challenging. model must interact symbolic executor non-differentiable operations search large program space. semantic parsing recent work handled training manually annotated programs avoiding program execution training time. however annotating programs known expensive scales poorly. program induction attempts address problem either utilized low-level memory required memory differentiable model trained backpropagation. makes difﬁcult efﬁcient discrete operations memory traditional computer limited application synthetic small knowledge bases. paper propose utilize memory discrete operations traditional comharnessing statistical power neural networks perform language understanding symbolic reasoning difﬁcult requires executing efﬁcient discrete operations large knowledge-base. work introduce neural symbolic machine contains neural programmer i.e. sequence-to-sequence model maps language utterances programs utilizes key-variable memory handle compositionality symbolic computer i.e. lisp interpreter performs program execution helps good programs pruning search space. apply reinforce directly optimize task reward structured prediction problem. train weak supervision improve stability reinforce augment iterative maximum-likelihood training process. outperforms state-of-theart webquestionssp dataset trained question-answer pairs only without requiring feature engineering domain-speciﬁc knowledge. deep neural networks achieved impressive performance supervised classiﬁcation structured prediction tasks speech recognition machine translation more. however training neural networks semantic parsing program induction language mapped sym. manager provides weak supervision reward indicating well task accomplished. unlike full supervision weak supervision easy obtain scale language input generates program sequence tokens programmer learns reward must overcome hard search problem ﬁnding correct programs computer executes programs nonhigh level programming language. differentiable memory enables abstract scalable precise operations makes training challenging help programmer prune search space provides friendly neural computer interface detects eliminates invalid choices within framework introduce neural symbolic machine apply semantic parsing. contains neural sequenceto-sequence programmer symbolic non-differentiable lisp interpreter executes programs large knowledge-base technical contribution work threefold. first support language compositionality augment standard seqseq model key-variable memory save reuse intermediate execution results novel application pointer networks compositional semantics. second alleviate search problem ﬁnding correct programs training questionanswer pairswe computer execute partial programs prune programmer’s search space checking syntax semantics generated programs. generalizes weakly supervised semantic parsing framework leveraging semantic denotations structural search. third train weak supervision directly maximize expected reward turn reinforce algorithm. since learning scratch difﬁcult reinforce combine iterative maxwebquestionssp dataset achieves state-of-the-art results weak supervision signiﬁcantly closing weak full supervision task. unlike prior works trained end-toend require feature engineering domain-speciﬁc knowledge. introduce ﬁrst describing computer non-differentiable lisp interpreter executes programs large provides code assistance propose seqseq model supports compositionality using key-variable memory save reuse intermediate results finally describe training procedure based reinforce augmented pseudo-gold programs found iterative training procedure diving details deﬁne semantic parsing task given knowledge base question produce program logical form executed generates right answer denote entities denote properties knowledge base assertions triples semantic parsing typically requires using operations query knowledge base process results. operations learned neural networks addition sorting perfectly generalize inputs larger ones observed training data contrast operations implemented high level programming languages abstract scalable precise thus generalizes perfectly inputs arbitrary size. based observation implement operations necessary semantic parsing oradopt lisp interpreter computer. program list expressions expression either special token return indicating program list tokens enclosed parentheses function takes input arguments speciﬁc types. table deﬁnes semantics function types arguments function executed returns entity list expression’s denotation save variable. introducing variables save intermediate results execution program naturally models language compositionality describes left right bottom-up derivation full meaning natural language input convenient seqseq model reminiscent ﬂoating parser derivation tree grounded input incrementally constructed. programs deﬁned functions equivalent subset λ-calculus presented full lisp programming language here constructs like control loops unnecessary current semantic parsing tasks simple functions model necessary. create friendly neural computer interface interpreter provides code assistance programmer producing list valid tokens step. first valid token cause syntax error e.g. previous token error detected using denotation saved variables. example previously generated tokens reachable entities checks enabled variables derived deﬁnition functions table interpreter prunes programmer’s search space orders magnitude enables learning weak supervision large given computer programmer needs natural language program sequence tokens reference operations values computer. base programmer standard seqseq model attention extend key-variable memory allows model learn represent refer program variables sequence-to-sequence models consist rnns encoder decoder. used -layer encoder decoder. given sequence words w...wm word mapped embedding then encoder reads embeddings updates hidden state step step using θencoder parameters. decoder updates hidden states embedding last step’s output token θdecoder parameters. last hidden state encoder used decoder’s initial state. also adopt dot-product attention similar dong lapata tokens program a...an generated using softmax vocabulary valid tokens step provided computer achieve compositionality decoder must learn represent refer intermediate variables whose value saved computer execution. therefore augment model key-variable memory entry components continuous embedding corresponding variable token referencing value computer encoding entity linker link text spans entities. linked entity memory entry average hidden states entity span variable token name variable computer holding linked entity value. decoding full expression generated gets executed result stored value variable computer. variable keyed hidden state step. variable embedding added key-variable memory figure semantic parsing nsm. embeddings key-variable memory output sequence model certain encoding decoding steps. illustration purposes also show values variables parentheses sequence model never sees values references name variable special token indicates start decoding return indicates decoding. reinforce formulate training reinforcement learning problem given question state action reward time step since environment deterministic state deﬁned question action sequence history actions time valid action time valid tokens given computer. since action corresponds token full history corresponds program. reward non-zero last step decoding score computed comparing gold answer answer generated executing program thus cumulative reward program embeddings variables dynamically generated example. training model learns represent variables backpropagating gradients time step variable selected decoder key-variable memory earlier time step embedding computed. thus encoder/decoder learns generate representations variables used right time construct correct program. embeddings differentiable values referenced variables stored computer symbolic non-differentiable. distinguishes keyvariable memory memory-augmented neural networks continuous differentiable embeddings values memory entries training weak supervision executes operations thus end-to-end backpropagation possible. therefore base training procedure reinforce reward signal sparse search space large common utilize full supervision pre-train reinforce training iterative fast program example gradient weighted model probability. decoding large beam size slow could train multiple epochs decoding. iterative process bootstrapping effect better model leads better program abest decoding better program abest leads better model training. even large beam size programs hard large search space. common solution problem curriculum learning size search space controlled functions used program program length. apply curriculum learning gradually increasing quantities performing iterative nevertheless iterative uses pseudogold programs directly optimize objective truly care about. adverse effects could spurious program accidentally produces correct answer thus generalize questions. training observe full negative programs model often fails distinguish tokens related another. example differentiating parentsof siblingsof childrenof challenging. present learning combine iterative reinforce. augmented reinforce bootstrap reinforce iterative pseudogold programs programs beam reasonably large probability. similar methods imitation learning deﬁne proposal distribution linearly interpolating model distribution oracle. reinforce assumes stochastic policy beam search gradient estimation. thus contrast common practice approximating gradient sampling model top-k action sequences beam normalized probabilities. allows training focus sequences high probability decision boundaries reduces variance gradient. empirically reinforce converged slowly often stuck local optima difﬁculty training resulted sparse reward signal large search space caused model probabilities programs non-zero reward small beginning. beam size small good programs fall beam leading zero gradients programs beam. beam size large training slow normalized probabilities good programs model untrained still small leading near zero baselines thus near zero gradients programs near zero gradients good programs probability combat this present alternative training strategy based maximum-likelihood. iterative gold programs could directly optimize likelihood. since gold programs perform iterative procedure search good programs given ﬁxed parameters optimize probability best program found far. decoding example large beam size declare abest pseudogold program achieved highest reward shortest length among programs decoded algorithm describes overall training procedure. ﬁrst iterative iterations record best program found every example then reinforce normalize probabilities programs beam probability best found program consequently model always puts reasonable amount probability program high reward training. note randomly initialized parameters reinforce since initializing ﬁnal parameters seems stuck local optimum produced worse results. imitation learning approach related common practice reinforcement learning replay rare successful experiences reduce training variance improve training efﬁciency. also similar recent developments machine translation objectives linearly combined anchoring model high-reward outputs stabilizes training. empirically show learn semantic parser weak supervision large evaluate webquestionssp challenging semantic parsing dataset strong baselines. experiments show achieves webquestionssp dataset webquestionssp dataset contains full semantic parses subset questions webquestions original dataset found answerable. consists question-answer pairs training testing collected using google suggest answers originally obtained using amazon mechanical turk workers. updated annotators familiar design freebase added semantic parses. separated questions training validation set. query pre-processing used in-house named entity linking system entities question. quality entity linker similar gold root entities included. similar dong lapata replaced named entity tokens special token ent. example question plays family changed plays ent. helps reduce overﬁtting instead memorizing correct program speciﬁc entity model focus context words sentence improves generalization. following used last publicly available snapshot freebase since training requires random access freebase decoding preprocessed freebase removing predicates related world knowledge removing text valued predicates rarely answer. relations relations removed preprocessing. results graph memory relations nodes edges. transform embeddings dimensions. decoder side used glove embeddings construct embedding property using freebase also added projection matrix transform embedding dimensions. freebase contains three parts domain type property. example freebase parentsof /people/person/parents. people domain person type parents property. embedding constructed concatenating average word embeddings domain type name average word embeddings property name. example embedding dimension embedding dimension /people/person/parents ﬁrst dimensions average embeddings people person second dimensions embedding parents. dimension encoder hidden state decoder hidden state embeddings embeddings functions special tokens randomly initialized truncated normal distribution mean=. stddev=.. weight matrices initialized input dimension. dropout rate clear tendency larger dropout rate produce better performance indicating overﬁtting major problem learning. iterative training decoder uses beam size update pseudo-gold programs model trained epochs decoding step. adam optimizer initial learning rate experiment process usually converges iterations. reinforce training best hyperparameters chosen using validation set. beam size decoding dataset small relations used whole training train model entire training iterations best hyperparameters. train model learning rate decay convergence. learning rate decayed number training steps iteration since decoding needs query knowledge base constantly speed bottleneck training decoding. address problem implementation partitioning dataset using multiple decoders parallel handle partition. decoders queries servers trainer. neural network model implemented tensorflow. since model small didn’t significant speedup using decoders trainer using only. inspired staged generation process curriculum learning includes steps. ﬁrst iterative iterations programs constrained function maximum number expressions then iterative again filter. maximum number expressions relations used restricted appeared abest ﬁrst step. results discussion evaluate performance using ofﬁcal evaluation script webquestionssp. answer question contain multiple entities values precision recall computed based output individual question average reported main evaluation metric. accuracy measures proportion questions answered exactly. comparison stagg previous state-ofthe-art model shown table model beats stagg weak supervision signiﬁcant margin metrics relying feature engineering handcrafted rules. stagg trained strong supervision obtains thus closes half training weak full supervision. augmented reinforce training. table compares augmented reinforce reinforce iterative validation set. reinforce gets stuck local optimum performs poorly. iterative training directly optimizing measure achieves sub-optimal results. contrast augmented reinforce able bootstrap using pseudo-gold programs found iterative achieves best performance training validation set. third ingredient curriculum learning during iterative compare performance best programs found without curriculum learning table best programs found curriculum learning substantially better found without curriculum learning large margin every metric. last important ingredient reducing overﬁtting. given small size dataset overﬁtting major problem training neural network models. show contributions different techniques controlling overﬁtting table note techniques applied model still overﬁtting training f=.% validation f=.%. tional depth increases indicating model effective capturing compositionality. observe programs three expressions limited properties mainly focusing answering types questions plays family what college jeff corwin which countries russia border. contrast programs expressions diverse properties could explain lower performance compared programs three expressions. search failure programs high reward found search pseudo-gold programs either beam size large enough functions implemented interpreter insufﬁcient. score table indicates least questions kind. ranking failure programs high reward exist beam ranked decoding. training error largely overﬁtting spurious programs. score table indicates questions kind. among deep learning models program induction reinforcement learning neural turing machines similar nondifferentiable machine controlled sequence model. therefore models rely reinforce training. main difference abstraction level programming language. rl-ntm uses lower level operations memory address manipulation byte reading/writing uses high level programming language large knowledge base includes operations following properties entities sorting based property suitable representing semantics. earlier works oops desirable characteristics example ability deﬁne functions. remain future improvements nsm. formulate training instance reinforcement learning order directly optimize task reward structured prediction problem compared imitation learning methods interpolate model distribution oracle needs solve challenging search problem training weak supervisions large search space. solution employs techniques symbolic computer helps good programs pruning search space iterative training process beam search used pseudogold programs. wiseman rush proposed max-margin approach train sequence-to-sequence scorer. however training procedure involved implement work. mixer also proposed combine training reinforce considered tasks full supervisions. berant liang applied imitation learning semantic parsing still requires hand crafted grammars features. similar neural programmer dynamic neural module network solve problem semantic parsing structured data generate programs using similar semantics. main difference approaches intermediate result represented. neural programmer dynamic-nmn chose represent results vectors weights enables backpropagation search possible programs parallel. however strategy applicable large freebase contains entities properties. instead chooses scalable approach computer saves intermediate results neural network refers variable names similar path ranking algorithm semantics encoded sequence actions denotations used prune search space learning. powerful allowing complex semantics composed key-variable memory; controlling search procedure trained neural network samples actions uniformly; allowing input questions express complex relations dynamically generating action sequences. combine multiple semantic representations produce ﬁnal prediction remains future work nsm. propose manager-programmer-computer framework neural program induction. integrates neural networks symbolic nondifferentiable computer support abstract scalable precise operations friendly neural computer interface. within framework introduce neural symbolic machine integrates neural sequence-to-sequence programmer key-variable memory symbolic lisp interpreter code assistance. interpreter non-differentiable directly optimize task reward apply reinforce pseudo-gold programs found iterative training process bootstrap training. achieves state-of-the-art results challenging semantic parsing dataset weak supervision signiﬁcantly closes between weak full supervision. trained endto-end require feature engineering domain-speciﬁc knowledge. acknowledgements help arvind neelakantan mohammad norouzi kwiatkowski eugene brevdo lukasz kaizer thomas strohmann yonghui zhifeng chen alexandre lacoste john blitzer. second author partially supported israel science foundation grant bollacker evans paritosh sturge taylor. freebase collaboratively created graph database structuring human knowledge. international conference management data pages kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing association computational linguistics doha qatar pages http//www.aclweb.org/anthology/d. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine mitchell william cohen. random walk inference learning large scale knowledge base. proceedings conference empirical methods natural language processing. association computational linguistics pages alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwinska sergio colmenarejo edward grefenstette tiago ramalho john agapiou adri badia karl hermann yori zwols georg ostrovski adam cain helen king christopher summerﬁeld phil blunsom koray kavukcuoglu demis hassabis. hybrid computing using neural network dynamic external memory. nature advance online publication. https//doi.org/./nature. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwi´nska sergio g´omez colmenarejo edward grefenstette tiago ramalho john agapiou mohammad norouzi samy bengio zhifeng chen navdeep jaitly mike schuster yonghui dale schuurmans. reward augmented maximum likelihood neural structured prediction. advances neural information processing systems patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. corr abs/.. http//arxiv.org/abs/.. wen-tau ming-wei chang xiaodong jianfeng gao. semantic parsing staged query graph generation question answering knowledge base. association computational linguistics wen-tau matthew richardson chris meek mingwei chang jina suh. value semantic parse labeling knowledge base question association computational linanswering. guistics zettlemoyer collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. uncertainty artiﬁcial intelligence pages schaul john quan ioannis antonoglou david silver. prioritized experience replay. international conference learning representations. puerto rico. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant", "year": 2016}