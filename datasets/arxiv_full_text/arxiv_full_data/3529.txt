{"title": "Deep Hyperspherical Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.", "text": "convolution inner product founding basis convolutional neural networks end-to-end visual representation learning. beneﬁting deeper architectures recent cnns demonstrated increasingly strong representation abilities. despite improvement increased depth larger parameter space also challenges properly training network. light challenges propose hyperspherical convolution novel learning framework gives angular representations hyperspheres. introduce spherenet deep hyperspherical convolution networks distinct conventional inner product based convolutional networks. particular spherenet adopts sphereconv basic convolution operator supervised generalized angular softmax loss natural loss formulation sphereconv. show spherenet effectively encode discriminative representation alleviate training difﬁculty leading easier optimization faster convergence comparable classiﬁcation accuracy convolutional counterparts. also provide theoretical insights advantages learning hyperspheres. addition introduce learnable sphereconv i.e. natural improvement preﬁxed sphereconv spherenorm i.e. hyperspherical learning normalization method. experiments veriﬁed conclusions. recently deep convolutional neural networks signiﬁcant breakthroughs many vision problems image classiﬁcation segmentation object detection etc. showing stronger representation power many conventional hand-crafted features cnns often require large amount training data face certain training difﬁculties overﬁtting vanishing/exploding gradient covariate shift etc. increasing depth recently proposed architectures aggravated problems. address challenges regularization techniques dropout orthogonality parameter constraints proposed. batch normalization also viewed implicit regularization network normalizing layer’s output distribution. recently deep residual learning emerged promising overcome vanishing gradients deep networks. however pointed residual networks essentially exponential ensembles shallow networks avoid vanishing/exploding gradient problem provide direct solutions. result training ultra-deep network still remains open problem. besides vanishing/exploding gradient network optimization also sensitive initialization. finding better initializations thus widely studied general large parameter space double-edged considering beneﬁt representation power associated training difﬁculties. therefore proposing better learning frameworks overcome challenges remains important. paper introduce novel convolutional learning framework effectively alleviate training difﬁculties giving better performance product based convolution. idea project parameter learning onto unit hyperspheres layer activations depend geodesic distance kernels input signals instead inner products. propose sphereconv operator basic module network layers. also propose softmax losses accordingly representation framework. speciﬁcally proposed softmax losses supervise network learning also taking sphereconv activations last layer instead inner products. note geodesic distances unit hypersphere angles inputs kernels. therefore learning objective essentially function input angles call generalized angular softmax loss paper. resulting architecture hyperspherical convolutional network shown fig. motivation propose spherenet angular information matters convolutional representation learning. argue motivation several aspects training stability training efﬁciency generalization power. spherenet also viewed implicit regularization network normalizing activation distributions. weight norm longer important since entire network operates angles. result weight decay also longer needed spherenet. sphereconv extent also alleviates covariate shift problem output sphereconv operators bounded makes variance output also bounded. second intuition angles preserve abundant discriminative information convolutional learning. gain intuition fourier transform image decomposed combination templates magnitude phase information frequency domain. reconstructs image original magnitudes random phases resulting images generally recognizable. however reconstructs image random magnitudes original phases. resulting images still recognizable. shows important structural information image visual recognition encoded phases. fact inspires project network learning angular space. terms low-level information sphereconv able preserve shape edge texture relative color. sphereconv learn selectively drop color depth preserve ratio. thus semantic information image preserved. spherenet also viewed non-trivial generalization proposing loss discriminatively supervises network hypersphere achieves state-of-the-art performance face recognition. however rest network remains conventional convolution network. contrast spherenet generalizes hyperspherical constraint every layer also different nonlinearity functions input angles. speciﬁcally propose three instances sphereconv operators linear cosine sigmoid. sigmoid sphereconv ﬂexible parameter controlling shape angular function. simple extension sigmoid sphereconv also present learnable sphereconv operator. moreover proposed generalized angular softmax loss naturaly generalizes angular supervision using sphereconv operators. additionally sphereconv serve normalization method comparable batch normalization leading extension spherical normalization spherenet easily applied network architectures googlenet resnet simply needs replace convolutional operators loss functions proposed sphereconv operators hyperspherical loss functions. summary sphereconv viewed alternative original convolution operators serves measure correlation. spherenet open interesting direction explore neural networks. question whether inner product based convolution operator optimal correlation measure tasks? answer question likely hyperspherical convolutional operator deﬁnition convolutional operator cnns simply linear matrix multiplication written convolutional ﬁlter denotes local patch bottom feature bias. matrix multiplication essentially computes similarity local patch ﬁlter. thus standard convolution layer viewed patch-wise matrix multiplication. different standard convolutional operator hyperspherical convolutional operator computes similarity hypersphere deﬁned angle kernel parameter local patch indicates function bias. simplify analysis discussion bias terms usually left out. angle interpreted geodesic distance unit hypersphere. contrast convolutional operator works entire space sphereconv focuses angles local patches ﬁlters therefore operates hypersphere space. paper present three speciﬁc instances sphereconv operator. facilitate computation constrain output sphereconv operators linear sphereconv. linear sphereconv operator linear function form parameters linear sphereconv operator. order constrain output range cosine sphereconv. cosine sphereconv operator nonlinear function form therefore reformulated viewed doubly normalized convolutional operator bridges sphereconv operator convolutional operator. sigmoid sphereconv. sigmoid sphereconv operator derived sigmoid function written parameter controls curvature function. close approximate step function. becomes larger like linear function i.e. linear sphereconv operator. sigmoid sphereconv instance parametric sphereconv family. parameters introduced parametric sphereconv richer representation power. increase ﬂexibility parametric sphereconv discuss case parameters jointly learned back-prop later paper. optimization optimization sphereconv operators nearly convolutional operator also follows standard back-propagation. using chain rule gradient sphereconv respect weights feature input straightforward compute therefore neglected here. linear sphereconv cosine sphereconv sigmoid sphereconv sin) k/k−π/k)) respectively partial gradients easily computed. theoretical insights provide fundamental analysis cosine sphereconv operator case linear neural network justify sphereconv operator improve conditioning problem. speciﬁc consider layer linear neural network observation rn×k weight rm×k input embeds weights previous layers. without loss generality assume columns satisfying consider closely related matrix factorization also viewed expected version matrix sensing problem following lemma demonstrates critical scaling issue signiﬁcantly deteriorate conditioning without changing objective lemma consider pair global optimal points satisfying lemma implies conditioning problem unbalanced global optimum scaled constant times larger conditioning problem balanced global optimum. note λmin happen thus consider restricted condition here. similar results hold beyond global optima. undesired geometric structure leads slow unstable optimization procedures e.g. using stochastic gradient descent motivates consider sphereconv operator discussed above equivalent projecting data onto hypersphere leads better conditioned problem. next consider proposed cosine sphereconv operator one-layer linear neural network. based previous discussion sphereconv consider equivalent problem lemma issue increasing condition caused scaling eliminated sphereconv operator entire parameter space. enhances geometric structure results improved convergence optimization procedures. extend result layer multiple layers scaling issue propagates. roughly speaking train layers worst case conditioning problem times worse scaling factor analysis similar layer case computation hessian matrix associated eigenvalues much complicated. though analysis elementary provide important insight straightforward illustration advantage using sphereconv operator. extension general cases e..g using nonlinear activation function requires much sophisticated analysis bound eigenvalues hessian objectives deferred future investigation. discussion comparison convolutional operators. convolutional operators compute inner product between kernels local patches sphereconv operators compute function angle kernels local patches. normalize convolutional operator terms normalized convolutional operator equivalent cosine sphereconv operator. essentially different metric spaces. interestingly sphereconv operators also interpreted function geodesic distance unit hypersphere. extension fully connected layers. fully connected layers viewed special convolution layer kernel size equal input feature sphereconv operators could easily generalized fully connected layers. also indicates sphereconv operators could used deep cnns also linear models like logistic regression etc. network regularization. norm weights longer crucial stop using weight decay regularize network. spherenets learned hyperspheres regularize network based angles instead norms. avoid redundant kernels want kernels uniformly spaced around hypersphere difﬁcult formulate constraints. tradeoff encourage orthogonality. given kernels i-th column weights i-th kernel network also minimize determining optimal sphereconv. practice could treat different types sphereconv hyperparameter cross validation determine sphereconv suitable one. sigmoid sphereconv could also cross validation determine hyperparameter general need specify sphereconv operator using preﬁxing sphereconv optimal choice treat hyperparameter sigmoid sphereconv learnable parameter back-prop learn following idea extend sigmoid sphereconv learnable sphereconv next subsection. sphereconv normalization. sphereconv could partially address covariate shift could also serve normalization method similar batch normalization. differently sphereconv normalizes network terms feature kernel weights batch normalization mini-batches. thus contradict used simultaneously. extension learnable sphereconv spherenorm learnable sphereconv. natrual idea replace current preﬁxed sphereconv learnable one. plenty parametrization choices sphereconv learnable present simple learnable sphereconv operator based sigmoid sphereconv. sigmoid sphereconv hyperparameter could treat learnable parameter denotes updated back-prop. back-prop updated using current iteration index easily computed chain rule. usually also require positive. learning fact similar parameter learning prelu spherenorm hyperspherical learning normalization method. similar batch normalization note hyperspherical learning also viewed normalization sphereconv constrain output value relu). different batchnorm spherenorm normalizes network based spatial information weights nothing mini-batch statistic. spherenorm normalize input weights could avoid covariate shift large weights large inputs batchnorm could prevent covariate shift caused inputs. sense work better batchnorm batch size small. besides sphereconv ﬂexible terms design choices lead different advantages. similar batchnorm could rescaling strategy spherenorm. speciﬁcally rescale output sphereconv learned back-prop fact spherenorm contradict batchnorm used simultaneously batchnorm. interestingly using empirically better using either alone. learning objective hyperspheres learning hyperspheres either conventional loss function softmax loss loss functions tailored sphereconv operators. present possible choices tailored loss functions. weight-normalized softmax loss. input feature label denoted respectively. original softmax loss written number training samples score j-th class number classes). class score vector usually output fully connected layer i-th training sample j-th yi-th column respectively. rewrite decision boundary softmax loss considering intuition sphereconv operators want make decision boundary depend angles. normalize weights zero biases following intuition decision boundary becomes cos. similar sphereconv could generalize decision boundary weight-normalized softmax loss written take form linear sphereconv cosine sphereconv sigmoid sphereconv. thus also term three difference weight-normalized loss functions linear w-softmax loss cosine w-softmax loss sigmoid w-softmax loss respectively. generalized angular softmax loss. inspired multiplicative parameter impose margins hyperspheres. propose generalized angular softmax loss extends w-softmax loss loss function favors large angular margin feature distribution. general ga-softmax loss formulated could also linear cosine sigmoid form similar w-softmax loss. a-softmax loss exactly cosine ga-softmax loss w-softmax loss special case ga-sofmtax loss. note usually require monotonically decreasing address this construct monotonically decreasing function recursively using part cos. although indeed partially addressed issue introduce number saddle points loss surfaces. originally close close however l-softmax a-softmax case. instability training. sigmoid ga-softmax loss also similar issues. however linear ga-softmax loss problem automatically solved training possibly become stable practice. also choices design speciﬁc ga-sofmtax loss different optimization dynamics. optimal depend task discussion sphere-normalized softmax loss. also considered sphere-normalized softmax loss simultaneously normalizes weights feature seems natural choice w-softmax proposed sphereconv makes entire framework uniﬁed. fact tried empirical results good optimization seems become difﬁcult. s-softmax loss train network scratch reasonable results without using extra tricks reason paper. completeness give discussions here. normally difﬁcult make s-softmax loss value small enough normalize features unit hypersphere. make loss work need either normalize feature value much larger tune learning rate ﬁrst train network softmax loss scratch s-softmax loss ﬁnetuning. experiments results experimental settings ﬁrst perform comprehensive ablation study exploratory experiments proposed spherenets evaluate spherenets image classiﬁcation. image classiﬁcation task perform experiments cifar cifar+ cifar large-scale imagenet datasets general settings. cifar cifar+ cifar follow settings imagenet dataset mostly follow settings attach details appendix fairness batch normalization relu used methods speciﬁed. comparisons made fair. compared cnns architecture spherenets. training. appendix gives network details. cifar- cifar- adam starting learning rate batch size speciﬁed. learning rate divided iterations training stops a-softmax ga-softmax loss ablation study exploratory experiments perform comprehensive ablation exploratory study spherenet evaluate every component individually order analyze advantages. -layer default perform image classiﬁcation cifar- without data augmentation. comparison different loss functions. ﬁrst evaluate sphereconv operators different loss functions. compared sphereconv operators -layer architecture experiment. results table observe sphereconv operators consistently outperforms original convolutional operator. compared loss functions except a-softmax ga-softmax effect accuracy seems less crucial sphereconv operators sigmoid w-softmax ﬂexible thus works slightly better others. sigmoid sphereconv operators suitably chosen parameter also works better others. note that w-softmax loss fact comparable original softmax loss spherenet optimizes angles w-softmax derived original softmax loss. therefore fair compare spherenet w-softmax softmax loss. table sphereconv operators consistently better covolutional operators. large-margin loss function like a-softmax proposed ga-softmax accuracy boosted. notice a-softmax actually cosine ga-softmax. superior performance a-softmax spherenet shows architecture suitable learning angular loss. moreover proposed large-margin loss performs best among compared loss functions. comparison different network architectures. also interested sphereconv operators work different architectures. evaluate proposed sphereconv operators architecture different layers totally different architecture baseline architecture follows design network different convolutional layers. fair comparison cosine w-softmax sphereconv operators original softmax original convolution operators. results table spherenets greatly outperforms baselines usually improvement. applied resnet sphereconv operators also work better baseline. note that similar resnet architecture cifar- experiment data augmentation cifar- experiment resnet accuracy much lower reported results different network architectures show consistent signiﬁcant improvement cnns. comparison different width evaluate spherenet different number ﬁlters. fig. shows convergence different width spherenets. means conv.x conv.x conv.x ﬁlters respectively. could observe number ﬁlters small spherenet performs similarly cnns however increase number ﬁlters ﬁnal accuracy surpass baseline even faster stable convergence performance. large width spherenets perform consistently better baselines showing spherenets make better width. learning without relu. notice sphereconv operators longer matrix multiplication essentially non-linear function. sphereconv operators already introduce certain figure testing accuracy iterations. resnet sphereresnet. plain plain spherenet. different width spherenet. ultra-deep plain ultra-deep plain spherenet. non-linearity network evaluate much gain non-linearity bring. therefore remove relu activation compare spherenet cnns without relu. results given table compared methods -layer cnns although removing relu greatly reduces classiﬁcation accuracy spherenet still outperforms without relu signiﬁcant margin showing rich non-linearity representation power. convergence. signiﬁcant advantages spherenet training stability convergence speed. evaluate convergence different architectures cnn- resnet-. fair comparison original softmax loss compared methods adam used stochastic optimization learning rate networks. fig. sphereresnet converges signiﬁcantly faster original resnet baseline cifar- cifar-+ ﬁnal accuracy also higher baselines. fig. evaluate spherenet without orthogonality constraints kernel weights. network architecture spherenet also converges much faster performs better baselines. orthogonality constraints also bring performance gains cases. generally fig. could also observe spherenet converges fast stably every case baseline ﬂuctuates relative wide range. optimizing ultra-deep networks. partially alleviation covariate shift problem improvement conditioning spherenet able optimize ultra-deep neural networks without using residual units form shortcuts. spherenets cosine sphereconv operator cosine w-softmax loss. directly optimize deep plain network stacked convolutional layers. fig. convergence spherenet much easier baseline spherenet able achieve nearly ﬁnal accuracy. preliminary study towards learnable sphereconv although learnable sphereconv main theme paper still preliminary evaluations proposed learnable sigmoid sphereconv learn parameter independently ﬁlter. also trivial learn layer-shared network-shared fashsion. -layer architecture used section learnable sphereconv achieves cifar- best sigmoid sphereconv achieves fig. also plot frequency histogram conv. conv. conv. ﬁnal learned spherenet. fig. observe layer learns different distribution ﬁrst convolutional layer tends uniformly distribute large range values potentially extracting information levels angular similarity. fourth convolutional layer tends learn concentrated distribution conv. seventh convolutional layer learns highly concentrated distribution centered around note that initialize constant learn back-prop. evaluation spherenorm section could clearly convergence advantage spherenets. general view sphereconv normalization method applied kinds networks. section evaluates challenging scenarios minibatch size small -layer section simple cosine sphereconv spherenorm. softmax loss used cnns spherenets. fig. could observe spherenorm achieves ﬁnal accuracy similar batchnorm spherenorm converges faster stably. spherenorm plus orthogonal constraint helps convergence little rescaled spherenorm seem work well. batchnorm spherenorm used together obtain fastest convergence highest ﬁnal accuracy showing excellent compatibility spherenorm. ﬁrst evaluate spherenet classic image classiﬁcation task. cifar-+ cifar datasets perform random random crop data augmentation resnet- baseline architecture. spherenet architecture evaluate sigmoid sphereconv operator sigmoid w-softmax loss linear sphereconv operator linear w-softmax loss cosine sphereconv operator cosine w-softmax loss sigmoid sphereconv operator ga-softmax loss table could spherenet outperforms current state-of-the-art methods even comparable resnet- deeper ours. experiment validates idea learning hyperspheres constrains parameter space semantic label-related one. large-scale image classiﬁcation imagenet- evaluate spherenets large-scale imagenet dataset. minimum data augmentation strategy experiment resnet- baseline sphereresnet- ﬁlter numbers layer. develop types sphereresnet- termed respectively. sphereresnet--v sphereconv shortcut convolutions used match number channels. sphereresnet--v sphereconv shortcut convolutions. fig. shows single crop validation error iterations. could observe sphereresnets converge much faster resnet baseline sphereresnet-v converges fastest yields slightly worse comparable accuracy. sphereresnet--v converges faster resnet- also shows slightly better accuracy. limitations future work work still limitations spherenets large performance gain network wide enough. network wide enough spherenets still converge much faster yield slightly worse recognition accuracy. computation complexity neuron slightly higher cnns. sphereconvs still mostly preﬁxed. possible future work includes designing/learning better sphereconv efﬁciently computing angles reduce computation complexity applications tasks require fast convergence better angular regularization replace orthogonality etc. thank zhen helping experiments providing suggestions. project supported part iis- bigdata career iis- iis- eager cns- n--- intel istc nvidia amazon aws. xingguo supported doctoral dissertation fellowship university minnesota. yan-ming zhang supported national natural science foundation china grant shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. ijcv pages christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. cvpr jiang xiong shiliang need beyond good init exploring better solution training extremely deep convolutional neural networks orthonormality modulation. arxiv. table plain architectures different convolutional layers. conv.x conv.x conv.x denote convolution units contain multiple convolution layers. e.g. denotes cascaded convolution layers ﬁlters size table resnet architectures different convolutional layers. conv.x conv.x conv.x conv.x conv.x denote convolution units contain multiple convolutional layers residual units shown double-column brackets. conv.x conv.x conv.x usually operate different size feature maps. networks essentially different number ﬁlters layer. downsampling performed convolutions stride e.g. denotes cascaded convolution layers ﬁlters size denotes stride input data imagenet- experiment minimum data augmentation. speciﬁcally ﬁrst resize images resolution randomly crop patches size resized images. besides that also randomly image horizontally. sphereresnet--v cosine sphereconv cosine w-softmax loss. sphereresnet--v cosine sphereconv softmax loss. generally standard softmax loss kinds w-softmax loss usually similar empirical performance. note that could obtain better performance using sphereconvs requires memory. width architecture limitation memory mini-batch size methods imagenet- experiment. sphere-normalized softmax loss essentially applying sphereconv fully connected layer softmax loss. however simply applying sphereconv make loss work loss function difﬁcult converge practice. address this rescale logit output s-softmax loss scaling factor therefore output range changed typically setting works pretty well practice. could also cross-validation strategy hyperparameter global optimal point positive semideﬁnite matrix smallest eigenvalue equal speciﬁcally existence invariance i.e. orthogonal matrix rr×r number eigenvectors corresponding eigenvalue", "year": 2017}