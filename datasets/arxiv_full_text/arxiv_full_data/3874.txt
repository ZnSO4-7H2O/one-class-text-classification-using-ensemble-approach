{"title": "An eigenanalysis of data centering in machine learning", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.SP", "math.ST", "stat.TH"], "abstract": "Many pattern recognition methods rely on statistical information from centered data, with the eigenanalysis of an empirical central moment, such as the covariance matrix in principal component analysis (PCA), as well as partial least squares regression, canonical-correlation analysis and Fisher discriminant analysis. Recently, many researchers advocate working on non-centered data. This is the case for instance with the singular value decomposition approach, with the (kernel) entropy component analysis, with the information-theoretic learning framework, and even with nonnegative matrix factorization. Moreover, one can also consider a non-centered PCA by using the second-order non-central moment.  The main purpose of this paper is to bridge the gap between these two viewpoints in designing machine learning methods. To provide a study at the cornerstone of kernel-based machines, we conduct an eigenanalysis of the inner product matrices from centered and non-centered data. We derive several results connecting their eigenvalues and their eigenvectors. Furthermore, we explore the outer product matrices, by providing several results connecting the largest eigenvectors of the covariance matrix and its non-centered counterpart. These results lay the groundwork to several extensions beyond conventional centering, with the weighted mean shift, the rank-one update, and the multidimensional scaling. Experiments conducted on simulated and real data illustrate the relevance of this work.", "text": "abstract—many pattern recognition methods rely statistical information centered data eigenanalysis empirical central moment covariance matrix principal component analysis well partial least squares regression canonical-correlation analysis fisher discriminant analysis. recently many researchers advocate working non-centered data. case instance singular value decomposition approach entropy component analysis information-theoretic learning framework even nonnegative matrix factorization. moreover also consider noncentered using second-order non-central moment. main purpose paper bridge viewpoints designing machine learning methods. provide study cornerstone kernel-based machines conduct eigenanalysis inner product matrices centered noncentered data. derive several results connecting eigenvalues eigenvectors. furthermore explore outer product matrices providing several results connecting largest eigenvectors covariance matrix non-centered counterpart. results groundwork several extensions beyond conventional centering weighted mean shift rank-one update multidimensional scaling. experiments conducted simulated real data illustrate relevance work. index terms—kernel-based methods gram matrix machine learning pattern recognition principal component analysis kernel entropy component analysis centering data. pattern recognition machine learning explained performing eigenanalysis eigendecomposition spectral decomposition machines seek relevant axes given dataset. principal component analysis prominent eigenanalysis problem feature extraction dimensionality reduction. case relevant axes obtained eigendecomposition covariance matrix capture largest amount variance data. machines include multidimensional scaling partial least squares regression canonicalcorrelation analysis classiﬁcation-based version known fisher discriminant analysis latter methods solve generalized eigendecomposition problem. kernel-based machines provide elegant framework generalize linear pattern recognition methods nonlinear domain. rely concept kernel trick initially introduced aizerman main breakthrough lies folds. hand pattern recognition classiﬁcation regression algorithms written terms inner products data. hand substituting inner product kernel function nonlinear transformation implicitly operated data without signiﬁcant computational cost. therefore eigenanalysis well operations performed kernel matrix corresponds inner product gram matrix feature space. property revealed kernelized versions survey kernel-based machines. several kernel-based machines given instance data centered feature space shifting origin centroid data. algorithmic point view centering performed easily matrix algebra either batch mode subsequent column centering kernel matrix recursive dealing online learning theoretical point view centering reveals central moments i.e. moments centroid/mean available data well related statistics. well-known central moments include second-order central moment also called covariance investigated estimating maximum-variance directions. furthermore directions minimize reconstruction error. many researchers advocate non-centered data pattern recognition. information extracted directly either data matrix gram matrix non-central moments. several motivations revealed favor working non-centered data. intuitive motivation application spectral decomposition without data-centering many pattern recognition machine learning problems thus providing sort non-centered using second-order non-central moment. case instance signal analysis classiﬁcation designing dictionaries sparse representation motivation towards keeping data non-centered nonparametric density estimation kernel functions revealed recently exceptional performance entropy component analysis information-theoretic learning framework also motivation data often deviate origin measure deviation constitute interesting feature hyperspectral unmixing turns many ﬁelds computer science signal processing machine learning acquired data nonnegative even positive. case study gene expressions bioinformatics eigenfaces computer vision problem human face recognition online handwritten character recognition numerous applications nonnegative matrix factorization consequence nonnegative gram matrices pervasive. information lost centering data well several interesting properties. issue centering data versus keeping data uncentering open question pattern recognition correlation versus congruence coefﬁcients versus non-centered covariance centered gram matrices versus non-centered counterparts. paper study impact centering data distribution eigenvalues eigenvectors inner-product outer-product matrices. examining gram matrix centered counterpart show interlacing property eigenvalues. devise bounds connecting eigenvalues matrices including lower bound largest eigenvalue centered gram matrix. furthermore examine eigenvectors inner product outer product matrices. provide connections relevant eigenvector covariance matrix non-centered matrix result corroborate work study focus eigenanalysis gram matrices. work opens understanding impact centering data kernelbased machine. shown bridging eca. moreover work goes beyond since extends naturally many kernel-based machines eigen-decomposition gram matrix crucial. revisit multidimensional scaling problem centering essential. moreover provide extension beyond conventional mean-centering. instance perron–frobenius theorem states that mild conditions non-negativity matrix inherited unique largest eigenvalue corresponding eigenvector positive components. result longer valid data centered. bias-free formulation support vector machines least-squares yields eigendecomposition centered gram matrix gaussian processes yield similar expressions applied non-centered gram matrix. authors propose modiﬁed take account fact centering leads singular gram matrix even noncentered gram matrix non-singular. recently devised data label centered dealing alignment criterion. author consider issue optimizing centering well low-rank approximation problem. bayesian statistics impact centering extensively studied multilevel models dealing hierarchically nested models case centering performed either grand mean partially group mean centering different levels comprehensive review centering issue multilevel modeling. problem revisited within bayesian framework issue centered non-centered parameterisations. beyond scope paper since investigate non-parametric methods eca. best knowledge cadima jolliffe investigated relationship non-centered variant. confronted eigendecomposition covariance matrix eigendecomposition non-centered counterpart. connection done thanks rank-one update connects matrices. paper study eigendecompositions centered non-centered gram matrices. easy much harder problem. performing eigenanalysis gram matrices provide framework integrates analysis beyond. work opens door study centering kernel-based machines. matrix n-by-n identity matrix vector all-ones vector entries. then nby-n matrix ones moreover grand matrix subscript allows recognize case centered data opposed non-centered versus versus versus versus etc. spectral decomposition matrix deﬁnes singular values given non-increasing order. largest eigenvalue square matrix denoted trace ﬁrst analysis eigenvalues matrix given trace with sake clarity i-th largest eigenvalues inner product matrices respectively namely λci. eigenpair denotes i-th largest eigenvalue corresponding eigenvector ﬁrst eigenvector associated largest eigenvalue. introduction section present eigendecomposition problems inner product outer product matrices non-centered centered data. then kernelbased machines presented order contrast paradigm centering data. tional inner product data matrix corresponding i.e. inner product matrix. turns gram matrix matrix encapsulates essence information data illustrated kernel trick throughout literature. therefore natural focus gram matrix study. vectors matrix entries kernel functions provide nonlinear extension conventional inner product since thanks mercer’s theorem corresponds inner product transformed samples columns called left-singular vectors deﬁne spectral decomposition matrix known realized covariation matrix ﬁnancial economics. coherent analysis consider paper second-order non-central moment matrix written matrix form xx⊤. following spectral decomposition turns derivations given paper easily generalized kernel functions since kernel matrix corresponds gram matrix feature space. centering performed feature space consequence centroid expression kµφk pattern recognition tasks need quantify inner product estimate counterpart input space. pre-image problem clearly beyond scope paper. recent survey. nonparametric density estimation essential many applied mathematical problems. many machine learning techniques based density estimation often parzen window approach case information-theoretic methods essentially based quadratic r´enyi entropy form often unknown probability density estimated using parzen estimator form given kernel function centered available sample using kernel matrix estimator given order confront centering data keeping data non-centered present next well-known machines pattern recognition. hand advocating central matrices presented two-folds conventional kernelized formulations. hand advocating exploration non-centered data nonparametric density estimation using eca. used. expression uncovers composition entropy terms eigenvectors main motivation relevant eigenvectors selected order maximize estimated entropy thus smallest terms therefore eigenvector contributes entropy estimate contrast finally emphasize non-centered data used density estimation. centering data leads null density estimator yields inﬁnite quadratic r´enyi entropy. fact also corroborated several studies including entropy component analysis. details. therefore lies span data. means exists vector αci. injecting relation expression xcxc⊤xc αci. equivalently multiplying side αci. thus following eigenproblem order satisfy unit-norm eigenvectors normalized. indeed wc⊤i αc⊤i xc⊤xcαci αc⊤i kcαci λciαc⊤i αci. therefore deﬁne feature directly eigenvector gram matrix normalization writing vectors matrix maximizing variance projected data written constraint using lagrangian multipliers taking derivative resulting cost function corresponding eigenproblem. even conventional issue centering still open question. best knowledge work studied link eigendecompositions expression reveals rank-one update nature matrices. analysis gram matrices obviously much harder problem illustrated expression paper take initiative study inner product matrices carry eigenanalysis outer product matrices turns work derived easily proposed approach. moreover study inner product matrices broadens scope work analysis kernel-based techniques beyond approach. next section gives main contributions paper study completed section several extensions beyond centering. main results given next two-folds gram matrix covariance matrix. each describe relations eigenvectors centered matrix obtained non-centered one. relations eigenvalues studied examining inner product matrices allows generalization results nonlinear kernel functions. before need brieﬂy introduce orthogonal projection link centering operation. projection onto orthogonal complement. projections idempotent transformations i.e. particular interested paper projection onto all-ones vector entries relation applied last equality expressions reveal double centering corresponds subtracting column means matrix entries adding grand mean. byproduct double centering obtained eigenvalue associated eigenvector ﬁrst study understand distribution data associated matrix given traces since correspond eigenvalues. following lemma provides measure variance reduction centering. instance ways performs hand r-mode using eigendecomposition covariance matrix given function princomp; hand q-mode using singular value decomposition non-centered data given function svd. next explore beyond eigenvalues. section show eigenvalues interlaced section provide bounds largest eigenvalues particular lower bound largest eigenvalue schur–horn theorem proven situation diagonal entries also given non-increasing order. still also subset diagonal entries although resulting lower bound theorem tight direct application schur–horn theorem separately matrix give particular result. following lemma ﬁrst step towards connection eigenvalues matrices allows establish bounds given theorem easy prove results replacing either spectral decomposition verifying product orthonormal matrices orthonormal. lemma shows matrix a⊤kca eigenvalues matrix eigenvectors columns a⊤ac matrix whose entries inner products eigenvectors eigenvectors since matrices a⊤kca share eigenvalues propose next apply schur–horn theorem latter matrix. results show impact centering data distribution eigenvalues eigenvalues sandwitched eigenvalues illustrates behaves like coarse matrix compared propose beyond analysis eigenvalues given far. section study eigenvectors centered gram matrix. following theorem provides insights eigenvectors matrix combining theorems eigenvector centered gram matrix veriﬁes sum-to-one boxed constraints. constraints equivalent well-known constraints svm. also describe data-driven bounds normalization operated given instance. eigenvectors gram matrix seldom used directly often considered deﬁne relevant axes. shown section principal component analysis section entropy component analysis. either methods relevant axes determined weighted linear combination data weights eigenvectors gram matrix normalization factor. considering normalization given ﬁrst equality follows expression second equality fact eigenpair last equality follows deﬁnition given conclude proof theorem applied matrix a⊤kca observing lemma matrices a⊤kca share eigenvalues. theorem provides characterization eigenvalues matrices beyond relation traces given lemma moreover latter lemma obtained particular case theorem equality theorem λci. this ﬁrst furthermore worth noting largest value needs given largest eigenvalue corresponding eigenvector case. apply celebrated courantfischer theorem matrix states v⊤kv consequence larger maxv equal special case consequently since vector mean then analogy deﬁnition simpler expression. therefore revisit results given section describe relations eigenvectors eigenvalues matrices still satisfy interlacing theorems given section since eigenvalues respectively analogy lemma kµk. conclude section giving summary relations obtained eigenvectors associated largest eigenvalues covariance matrix non-centered counterpart. non-centered case theorem eigenvector tends collinear mean vector consider case data centered leads ﬁrst eigenvector theorem corollary provide inequalities measure fact closer results obtained confronting covariance matrix non-centered counterpart corroborate work cadima jolliffe latter eighth property proposition gives result equivalent lemma however proof much shorter signiﬁcantly simpler likewise ninth property proposition equivalent theorem proof slightly simpler. finally theorem corollary provide bounds depend relation opposed fourteenth property proposition case comprehensive expressions simpler bounds thus offering straightforward interpretation. beyond conventional centering section show several research activities take advantage study apart conventional centering issue beyond scope bridging eca. issue centering data also investigated vector mean data. weighted means provide generalization conventional mean. commonly studied literature statistics population studies recently weighted mean considered derive robust algorithm. maps data weighted mean becomes matrix deﬁnes projection zero since idempotent follows sample similar many samples dataset. hubs emerge curse dimensionality tend close data mean i.e. centroid. details concept hubs machine learning. given expression matrix special case rank-one update matrix turns study given sections extended rank-one update covariance matrix. update great interest covariance matrix adaptation within machines rely gaussian random variations evolutionary strategies ensemble optimization author provides connections genetic machines monte carlo-based methods including particle ﬁltering population monte carlo. without lost generality section presents issue covariance matrix adaptation evolutionary strategies also survey. re-sampling techniques solve hard optimization problems generating candidate solutions. performance highly depends population’s distribution investigation. evolutionary strategies provide elegant approach derive parameterfree techniques user. principle selfadaptation allows adjust distribution direction relevant regions search space. rule allows adjust distribution towards zero-mean gaussian distribution covariance matrix vtv⊤t namely distribution highest probability generate among zero-mean gaussian distributions. bound shows ﬁrst eigenvector forms greater angle eigenvectors vector result independent value parameter illustrates diversity introduced applying update rule multidimensional scaling well-known dimensionality reduction technique seeks preserve pairwise distances dissimilarity measures problem estimate available distances denoted expanding expression therefore deﬁne inner product distances equivalently matrix form bounds eigenvalues derived help schur–horn theorem given theorem virtue lemma describe results following steps proof theorem thus lower bound largest eigenvalue given applied noisy data practice technique considers factorization resulting matrix acλcac⊤ largest nonnegative eigenvalues retained. expression ac⊤. deﬁnes gram matrix setting construction leads uncorrelated data since obtained kernel xjk. consider scaling data positive factor since corresponding matrix relation easy matrices share eigenvectors eigenvalue former deﬁnes eigenvalue latter. considering normalization given kαik obtain x⊤wj xξ⊤wξ therefore projections onto axes deﬁned either provide scale-invariant features show within approach. results extends work studies. established mathematical statements easily veriﬁed. show this consider well-known iris dataset extensively studied pattern recognition literature since fisher’s seminal paper dataset consists samples divided equally three classes sample attributes. table shows interlacing property largest three eigenvalues gram matrix settled theorem fig. illustrates theorem lower bound largest eigenvalues derived speciﬁc cases shown. expression similar double centering gram matrix while construction expressions gram matrix however major difference positive deﬁnite matrix. turns author provides thorough description conditionally positive deﬁnite kernels argues good positive deﬁnite kernels whenever translation invariant problem investigated svm. worth noting include positive bias large enough positive deﬁnite thus eliminating term associated negative eigenvalue. return main issue section analysis relations matrices turns take advantage mathematical statements derived section purpose illustrated next substituting write expression follows moreover since diagonal entries null trace null well eigenvalues. lemma lemma provide insights. former namely schur-horn theorem given theoi= equality lemma shows variability two-dimensional data generated banana-shaped distribution follows uniform distribution follows zero-mean gaussian distribution standard deviation. gram matrices constructed bandwidth parameter table illustrates interlacing property largest eigenvalues kernel matrices non-centered entries corresponding centered matrix fig. shows contours ﬁrst principal functions data centered feature space data centered illustrates ﬁrst principal function non-centered case related data mean principal functions similar obtained centered case order higher namely results comparable results main objective paper bridge centered uncentered data. studied impact centering data eigendecomposition gram matrix thereby beneﬁt kernel-based methods. speciﬁc paper explored eigendecomposition covariance matrix results corroborate recent work conventional pca. motivation reconcile centered gram matrix non-centered gram matrix nonparametric density estimation. moreover provided several extensions main results beyond conventional centering issue. techniques manifold learning dimensionality reduction also take advantage work include isomap locally-linear embedding eigenmaps spectral clustering name few. future work address issue impact kernel functions centering issue well impact centering data input space opposed implicit centering feature space kernel-based methods. also study connections spectral analysis random matrix theory least squares formulation class generalized eigenvalue problems machine learning proceedings annual international conference machine learning icml jolliffe principal component analysis. york springer-verlag geladi kowalski partial least-squares regression tutorial analytica chimica acta vol. h¨ardle simar canonical correlation analysis applied multivariate statistical analysis springer berlin heidelberg fig. illustration contours ﬁrst principal functions kernel-based gaussian kernel samples banana-shaped distribution. ﬁrst ﬁgures obtained eigendecomposition centered matrix second corresponds noncentered case hastie tibshirani friedman elements statistical learning data mining inference prediction. springer series statistics springer second edition honeine online kernel principal component analysis reduced-order model ieee transactions pattern analysis machine intelligence vol. september c.-d. chang c.-c. wang jiang singular value decomposition based feature extraction technique physiological signal analysis journal medical systems vol. bioucas-dias plaza dobigeon parente gader chanussot hyperspectral unmixing overview geometrical statistical sparse regression-based approaches ieee sel. topics appl. earth observations remote sens. vol. april ruiz-del solar navarrete eigenspace-based face recognition comparative study different approaches systems cybernetics part applications reviews ieee transactions vol. deepu madhvanath ramakrishnan principal component analysis online handwritten character recognition pattern recognition icpr proceedings international conference vol. vol. gestel suykens lanckriet lambrechts moor vandewalle bayesian framework least-squares support vector machine classiﬁers gaussian processes kernel ﬁsher discriminant analysis neural comput. vol. park park nonlinear discriminant analysis using kernel functions generalized singular value decomposition siam journal matrix analysis applications vol. papaspiliopolous roberts ˆold non-centered parameterizations hierarchical models data augmentation baysian statistics oxford university press x.-l. meng center center question ancillarity-sufﬁciency interweaving strategy boosting mcmc efﬁciency journal computational graphical statistics vol. barndorff-nielsen shephard econometric analysis realized covariation high frequency based covariance regression correlation ﬁnancial economics econometrica vol. suzuki hara shimbo saerens fukumizu centering similarity measures reduce hubs proceedings conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group fonseca leeuwenburgh jansen improving ensemble optimization method covariance matrix adaptation reservoir simulation symposium society petroleum engineers february ¨uller sbalzarini gaussian adaptation revisited entropic view covariance matrix adaptation proceedings international conference applications evolutionary computation volume part evoapplicatons’ springer-verlag juszczak kernel whitening one-class classiﬁcation proc. first international workshop pattern recognition support vector machines springer-verlag paul honeine born beirut lebanon october received dipl.-ing. degree mechanical engineering m.sc. degree industrial control faculty engineering lebanese university lebanon. received ph.d. degree systems optimisation security university technology troyes france postdoctoral research associate systems modeling dependability laboratory since september assistant professor university technology troyes france. research interests include nonstationary signal analysis classiﬁcation nonlinear signal processing sparse representations machine learning wireless sensor networks. co-author best paper award ieee workshop machine learning signal processing.", "year": 2014}