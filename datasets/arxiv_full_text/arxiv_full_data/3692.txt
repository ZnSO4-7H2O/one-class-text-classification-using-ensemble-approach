{"title": "Classification Accuracy as a Proxy for Two Sample Testing", "tag": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "abstract": "When data analysts train a classifier and check if its accuracy is significantly different from random guessing, they are implicitly and indirectly performing a hypothesis test (two sample testing) and it is of importance to ask whether this indirect method for testing is statistically optimal or not. Given that hypothesis tests attempt to maximize statistical power subject to a bound on the allowable false positive rate, while prediction attempts to minimize statistical risk on future predictions on unseen data, we wish to study whether a predictive approach for an ultimate aim of testing is prudent. We formalize this problem by considering the two-sample mean-testing setting where one must determine if the means of two Gaussians (with known and equal covariance) are the same or not, but the analyst indirectly does so by checking whether the accuracy achieved by Fisher's LDA classifier is significantly different from chance or not. Unexpectedly, we find that the asymptotic power of LDA's sample-splitting classification accuracy is actually minimax rate-optimal in terms of problem-dependent parameters. Since prediction is commonly thought to be harder than testing, it might come as a surprise to some that solving a harder problem does not create a information-theoretic bottleneck for the easier one. On the flip side, even though the power is rate-optimal, our derivation suggests that it may be worse by a small constant factor; hence practitioners must be wary of using (admittedly flexible) prediction methods on disguised testing problems.", "text": "data analysts train classiﬁer check accuracy signiﬁcantly diﬀerent random guessing implicitly indirectly performing hypothesis test importance whether indirect method testing statistically optimal not. given hypothesis tests attempt maximize statistical power subject bound allowable false positive rate prediction attempts minimize statistical risk future predictions unseen data wish study whether predictive approach ultimate testing prudent. formalize problem considering two-sample mean-testing setting must determine means gaussians analyst indirectly checking whether accuracy achieved fisher’s classiﬁer signiﬁcantly diﬀerent chance not. unexpectedly asymptotic power lda’s sample-splitting classiﬁcation accuracy actually minimax rate-optimal terms problem-dependent parameters. since prediction commonly thought harder testing might come surprise solving harder problem create information-theoretic bottleneck easier one. side even though power rate-optimal derivation suggests worse small constant factor; hence practitioners must wary using prediction methods disguised testing problems. recent popularity machine learning resulted extensive teaching prediction theoretical applied communities relative lack awareness popularity topic neyman-pearson style hypothesis testing computer science related data science communities. result practically faced eﬀectively hypothesis testing problem statisticians would construct study appropriate test statistic direct testing data scientists often take route involving prediction proxy indirect hypothesis testing. study example phenomenon paper concerning arguably classical testing prediction problems discuss sample testing classiﬁcation problems studied linear setting underlying distributions gaussians statement become clearer formally deﬁne problem setups section suﬃces gaussians natural linear classiﬁer fisher’s linear discriminant practitioners familiar machine learning hypothesis testing literature might recognize testing problem instead intuitive perform indirect testing following ﬁrst estimate classiﬁer accuracy signiﬁcantly diﬀerent chance conclude distributions diﬀerent. central question paper seeks answer what price testing indirectly instead directly?. shall detail section notion cost price appropriate neyman-pearson hypothesis testing paradigm power achievable ﬁxed false positive level would like answer question worst-case sense relying minimax theory frequentist statistics. formally restate question much power lose compared minimax power performing hypothesis testing indirectly prediction?. question interesting prediction sense harder problem testing case chasing real vector case want binary output estimated real vector testing optimal indeed vapnik’s advice solving problems limited information could tempted conjecture attempting solve harder problem needed ﬁrst could serve bottleneck easier problem. surprisingly analysis shows aforementioned possible hurdle occur problems study prediction testing seem pose signiﬁcant bottleneck. indeed perspective minimax testing error rates linear classiﬁcation allow delve details worth mentioning even though paper theoretical endeavor question initially practically motivated. many scientiﬁc questions naturally posed sample tests examples abound epidemiology neuroscience. hypothetical example latter consider particular region interest brain hippocampus. interested determining whether hippocampus responds diﬀerently situations person medical condition person without condition condition could varied depression autism parkinson’s disease etc. then collects analyzes brain data patient contrasting stimuli diﬀerent normal patients stimulus increasingly common ﬁeld neuroscience assess whether signiﬁcant diﬀerence sets data collected learning classiﬁer diﬀerentiate neuroscientists call style brain decoding pattern discrimination positive answer seen preliminary evidence mental process interest might occur within portion brain studied recent discussion related issues. results paper would reassure neuroscientist prediction instead testing even high dimensional setting reduce power much. time also serve warning constant factor loss power might possible scientiﬁc disciplines data plenty scientist wary using prediction techniques disguised hypothesis testing problems. section formally deﬁne problems provide relevant paper outline. background information. section discuss lower bounds sample testing. section study power linear classiﬁcation sample mean testing. section discuss related problem settings concluding section notation refer d-variate gaussian distribution mean positive deﬁnite covariance matrix slighty abuse notation shall also denote corresponding gaussian point given −d/det−/ expt refers standard euclidean -norm. denote standard indicator function denote reals denote expectation. section introduce main topics touch upon paper sample mean testing fisher’s linear discriminant analysis problems working high-dimensional setting means number dimensions sample mean testing consider problem testing whether d-variate gaussian distributions density def= density def= identical not. given i.i.d. samples respectively want diﬀerentiate null hypothesis alternate hypothesis diﬀerent two-sample testing fundamental decision-theoretic problem long history statistics example past century seen wide adoption t-statistic decide samples diﬀerent population means. introduced parametric setting univariate gaussians generalized multivariate non-gaussian settings well. assume known hotelling’s t-statistic remark assume number points drawn however instead points respectively claimed results still hold spirit whenever converges constant fact simply veriﬁed looking general expressions power derived choose avoid complication since unnecessary paper’s main message. consider problem learning classiﬁer diﬀerentiate d-variate gaussian distributions density def= density def= paper assume known brieﬂy discuss diﬃculties unknown section given i.i.d. samples respectively want classify point i.e. need predict whether came denotes expectation respect input points class denote input datasets. note since input data already integrated depend input data functions estimate diﬀerent ways. simple sample splitting form classiﬁer using ﬁrst samples class estimate test error remark clarity deal case equal sample sizes prior probability drawing sample equal. unbalanced prior probabilities sampling class verify results carry forward spirit. explain would observe points class converges overall error would generalize results using expressions achieve similar conclusions speciﬁcally present analysis power indirect sample testing ﬁrst begin understanding fundamental minimax lower bounds sample testing. prove random variables gaussian identity covariance mean diﬀerence minimax power testing given standard gaussian point representing right α-quantile standard gaussian distribution i.e. paper treats constant translate lower bound setting follows. given dataset gaussians mean diﬀerence common covariance form standardized variables def= σ−/yi. mean diﬀerence σ−/δ also identity covariance. apply aforementioned lower bound resubstituting lower bound power given convenience notation shall denote signal noise ratio. interpret bound follows. ﬁrst term inside parentheses interest purposes magnitude bounded constant second term determines rate power approaches power reduces thought ﬁxed larger leads larger test power asymptotically approach unity scaling unlike scaling typically sees prediction problems power classiﬁcation accuracy fisher’s fares aforementioned minimax lower bounds power. introduce subscript remind expression captures error classiﬁer points used training. expression proved authors asymptotically exact high-dimensional setting so-called raudyskolmogorov double asymptotics unbiased estimator i.e. since interested setting signal noise ratio small enough problem hard restrict thinking regime near close lose much accuracy conservatively approximate upper bound hence unconditional training data reasonable asymptotic approximation given variance approximation hence power comes behaves around more since interested regime close half lose much accuracy following taylor expansion conclude using linear classiﬁcation accuracy essentially minimax optimal small constant factors. speciﬁcally i.e. regime errors close half asymptotic power fisher’s given sample testing ﬁxed setting strong reasons prefer hotelling’s t-test. example known uniformly powerful test univariate gaussians fairly general assumptions seminal paper authors proved asymptotic power tending value high dimensional setting small classiﬁcation problem similar vein results paralleling results sample testing even though connection seemingly explicitly mentioned either papers best knowledge. example prove unknown error fisher’s terrible high dimensional setting. instead consider naive bayes classiﬁcation rule given practice instead using kind asymptotic approximations analyzed often employs randomization tests also known permutation tests. direct sample test would following. permutation test calculate full data repeat times pool samples permute samples split parts evaluate sort statistics test statistic permuted samples call rank right α-quantile reject null hypothesis. would like note possible ways applying permutation testing within classiﬁcation sample splitting framework subtleties involved. methods diﬀer italicized text. method split data halves call train classiﬁer call evaluate accuracy call repeat times pool samples permute samples split parts evaluate accuracy permuted data call sort accuracies rank right α-quantile reject null hypothesis. method split data halves call train classiﬁer call evaluate accuracy call repeat times pool samples permute samples split parts train classiﬁer ﬁrst half evaluate second half accuracy sort accuracies rank right α-quantile reject null hypothesis. opinion method preferred method diﬀerence rather subtle. ﬁrst method tests whether signiﬁcantly diﬀerent chance. second method tests whether classiﬁer learned performs signiﬁcantly diﬀerent chance. words testing null hypothesis stated permutation testing wrapped around whole procedure calculating test statistic around second half procedure. currently don’t formal analysis support expect minor diﬀerences ﬁnite-sample performance studied test permutation-variant making qualitative statements analysis carry forward permutation setting ﬁrst train data test data. course estimate would overoptimistic would scorned upon estimate true accuracy classiﬁer. however might wonder null distribution would another natural setting nonlinear classiﬁcation. examination test statistics used shows closely related statistics based kernel maximum mean discrepancy kernel speciﬁcally instantiated linear kernel. similarly classiﬁcation kernelized proposed specializes fisher’s linear kernel employed. given parallels observed settings might naturally conjecture spirit results paper extended kernelized nonlinear settings well. would like mention neyman-pearson classiﬁcation framework proposed analyzed setting quite diﬀerent since work considers problem minimizing probability error class subject bound probability error class. instead interested minimizing probability detecting classes diﬀerent subject bound probability detecting diﬀerent actually same. thus consider diﬀerent connection testing classiﬁcation approach testing problem appears harder testing here couple simple simulations compare performance algorithm theoretical lower bound. setup follows diﬀerence choices draw samples dimensional identitycovariance gaussians mean diﬀerence given split sample parts train classiﬁer ﬁrst test second. cutoﬀ determine test rejects not. repeat procedure times determine power i.e. probability rejecting controlling level plots higher better. following experiment choose varying increments setting selected leads constant power according theoretical analysis upper lower bounds theoretical minimax power approach hence would suitable visualization graph already proved lower bound asymptotically tight including constants serves excellent benchmark). slightly bumps blue curve repetitions calculate power larger used smoother estimated curve would however already make curve tracking minimax power accurately consistently slightly lower. following experiment setup almost previous experiment except used setting theoretical prediction power increase zero ﬁrst setting last setting. paper gave basic statistical analysis classiﬁcation accuracy test statistic sample testing. theoretically classiﬁcation accuracy fisher’s rate-optimal minimax sense sample mean testing gaussians known covariance. conjecture results also hold covariance unknown nonlinear settings. practically possible constant factor loss power worry settings sample sizes dimensionality high signal noise ratio small practitioners want data.", "year": 2016}