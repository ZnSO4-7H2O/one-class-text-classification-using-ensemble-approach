{"title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error  Minimization Framework", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.", "text": "introduce approach unsupervised estimation feature-rich semantic role labeling models. model consists components encoding component semantic role labeling model predicts roles given rich syntactic lexical features; reconstruction component tensor factorization model relies roles predict argument ﬁllers. components estimated jointly minimize errors argument reconstruction induced roles largely correspond roles deﬁned annotated resources. method performs accurate role induction methods english german even though unlike previous approaches incorporate prior linguistic knowledge languages. shallow representations meaning semantic role labels particular long history linguistics recently emergence large annotated resources propbank framenet automatic semantic role labeling attracted attention semantic role representations encode underlying predicate-argument structure sentences speciﬁcally every predicate sentence identify arguments associate argument underlying semantic role agent patient semantic roles many potential applications shown beneﬁt question answering textual entailment machine translation dialogue systems among others. current statistical approaches supervised requiring large quantities human annotated data estimate model parameters. however resources expensive create available small number languages domains. moreover moved domain performance models tends degrade substantially scarcity annotated data motivated research unsupervised learning semantic representations existing methods number serious shortcomings. first make strong assumptions example assuming arguments conditionally independent given predicate. second unlike state-of-the-art supervised parsers rely simplistic features sentence. factors lead models insufﬁciently expressive capture syntax-semantics interface inadequate handling language ambiguity overall introduces upper bound performance. moreover approaches especially problematic languages freer word order english richer features necessary account interactions surface realizations syntax semantics. example accurate previous models treat role induction task clustering argument signatures argument signature encode syntactic properties argument realization consists syntactic function argument along additional informations argument position respect predicate. though possible design signatures mostly single role set-up limits oracle performance even english quite restrictive languages freer word order. shortcomings inherent limitations modeling frameworks used previous work cannot addressed simply incorporating features relaxing modeling assumptions. work propose method effective unsupervised estimation feature-rich models semantic roles. demonstrate reconstruction-error objectives shown effective primarily training neural networks well suited inducing feature-rich log-linear models semantics. model consists components log-linear feature rich semantic role labeler tensor-factorization model captures interaction semantic roles argument ﬁllers. estimated jointly unlabeled data roles induced model mostly corresponds roles deﬁned existing resources annotators. method rivals accurate semantic role induction methods english german importantly prior knowledge languages incorporated feature-rich model whereas clustering counterparts relied language-speciﬁc argument signatures. languages-speciﬁc priors crucial success example using english-speciﬁc argument signatures german bayesian model titov klementiev results drop performance clustering considerably lower conﬁrms intuition using richer features helps capture syntax-semantics interface multi-lingual settings reducing need using language-speciﬁc model engineering highly desirable unsupervised learning. rest paper structured follows. section begins deﬁnition semantic role labeling task discusses speciﬁcs unsupervised setting. section describe approach starting general motivation proceeding technical details model learning procedure section provides evaluation analysis. finally additional related work presented section task involves prediction predicate argument structure i.e. identiﬁcation arguments assignment labels according underlying semantic role. example following sentences work focus labeling stage semantic role labeling. identiﬁcation though important problem tackled heuristics unsupervised techniques potentially using supervised classiﬁer trained small amount data. core approach statistical model encoding interdependence semantic role structure realization sentence. unsupervised learning setting sentences syntactic representations argument positions observable whereas associated semantic roles latent need induced model. idea underlines much latent variable modeling good latent representation helps reconstruct practice interested predicting observable rather interested inducing appropriate latent representations thus crucial design model good indeed encodes roles rather form abstraction. follows refer roles using names though unsupervised setting method latent variable model yield human-interpretable labels them. following sentence motivating example discussion model model consists components. ﬁrst component responsible prediction argument tuples based roles predicate. experiments component represent arguments lemmas lexical heads also restrict verbal predicates. intuitively think predicting argument time argument predicted based predicate lemma role assigned argument role-argument pairs learning predict arguments inference algorithm search role assignments simplify prediction task much possible. hypothesis assignments correspond roles accepted linguistic theories hypothesis plausible? primarily semantic representations introduced abstraction capturing crucial properties relation thus representations rather surface linguistic details like argument order syntactic functions crucial modeling sets potential argument tuples. reconstruction component part model. crucially referred ‘searching role assignments simplify argument prediction’ would actually correspond learning another component semantic role labeler predicts roles relying rich sentence features. components estimated jointly minimize errors recovering arguments. role labeler end-product learning used process sentences compared existing methods evaluation. paragraph regarded desiderata; discuss achieve them. standard approach latent variable modeling generative framework deﬁne family joint models estimate parameters example maximizing likelihood. generative models semantics necessarily make strong independence assumptions simplistic features thus cannot meet desiderata stated above. importantly also much simplistic assumptions state-of-the-art supervised role labelers generative modeling learn latent representations. alternative popular neural network community instead autoencoders optimize reconstruction error autoencoders latent representation predicted encoding model used recover reconstruction model parameters encoding reconstruction components chosen minimize form reconstruction error example euclidean distance ˜x||. though currently popular within deep learning community latent variable models neural networks also trained moreover encoding reconstruction models belong different model families; reconstruction component focused recovering part rather entire rely remaining part observations crucial allow implement desiderata. speciﬁcally encoding model feature-rich classiﬁer predicts semantic roles sentence reconstruction model model predicts argument given role given rest arguments roles. idea training linear models reconstruction error previously explored daum´e recently ammar several possible ways translate ideas speciﬁc method consider simplest instantiations. simplicity discussion assume exactly predicate realized sentence mentioned above focus argument labeling assume arguments known roles need induced. encoder log-linear model feature vector encoding interactions sentence semantic role representation model used long posterior distributions roles efﬁciently computed approximated experiments used model factorizes individual arguments reconstruction component predicts argument given semantic roles predicate arguments bilinear softmax model rd×k model parameters partition function ensuring probabilities one. intuitively embeddings learned data encode semantic properties argument example embeddings words demonstrator protestor somewhere near space away word cat. product cprua k-dimensional vector encoding beliefs arguments based argument-role pair example seeing argument demonstrator patient position predicate charge would predict agent perhaps word police role instrument ﬁlled word baton perhaps cannon. contrary patient agent likely police. turn product cvrj large expectations argument pair small otherwise. intuitively objective corresponds scoring argument tuples according factorization though generalization notion selection preferences. selectional preferences characterize arguments licensed given role given predicate example agent predicate charge police table idea. generalization model soft restrictions imposed role also arguments assignment roles. practice extend model slightly introduce word-speciﬁc bias argument prediction model smooth model using predicate-speciﬁc cross-predicate projection matrices instead cvr. however optimizing objective practical exact form reasons marginalization exponential number arguments; partition function requires summation entire potential argument lemmas. existing techniques address challenges. order deal ﬁrst challenge basic mean-ﬁeld approximation. namely instead computing expectation posterior distributions score argument predictions posteriors arguments score associated predicting lemma argument order address second problem computation negative sampling technique specﬁcally softmax equation optimize following sentence-level objective random sample elements unigram distribution lemmas logistic sigmoid function. assuming posteriors derived closed form gradients objective respect parameters encoding component reconstruction component computed using back propagation. experiments used adagrad algorithm perform optimization. learning algorithm quite efﬁcient reconstruction computation bilinear whereas computation posteriors semantic roler labeling component expensive discriminative supervised learning role labeler. moreover computations sped substantially observing µiscvs expression precomputed reused across predictions different arguments predicate. test time linear semantic role labeler used inference straightforward. english followed lang lapata used dependency version propbank released conll shared task dataset divided three segments. previous work unsupervised role labeling used largest segment evaluation learning. permissible unsupervised models gold labels training. small segments used model development. experiments relied gold standard syntax gold standard argument identiﬁcation set-up allows evaluate much previous work. refer reader lang lapata details experimental set-up. much work unsupervised induction roles languages english perhaps primarily above-mentioned model limitations. german replicate set-up considered titov klementiev used conll version salsa corpus instead using syntactic parses provided conll dataset re-parsed malt dependency parser similarly rather relying gold standard annotations argument identiﬁcation used supervised classiﬁer predict argument positions. details preprocessing found titov klementiev previous work unsupervised evaluate model using purity collocation harmonic mean purity measures average number arguments gold role label cluster collocation measures extent speciﬁc gold role represented single cluster. formally semantic role labeling component relied feature patterns used argument labeling popular supervised role labelers patterns include nontrivial syntactic features dependency path target predicate considered argument. resulting feature space quite large arguably sufﬁcient accurately capture syntax-semantics interface languages. importantly dimensionality feature space different used typically unsupervised srl. principle features could used chose feature patterns fairly simple generic. also easily extracted treebank. used feature patterns english german. however little doubt language-speciﬁc feature engineering language-speciﬁc priors constraints would beneﬁt performance. faithful goal constructing simplest possible feature-rich model logistic classiﬁers independently predicting role distribution every argument. respectively. model sensitive parameter deﬁning number roles long large enough training used uniform random initialization adagrad model selections done basis respective development set. table summarizes results method well alternative approaches baselines. following baseline simply clusters predicate arguments according dependency relation head. separate cluster allocated frequent relations dataset additional cluster used relations. observed previous work hard baseline beat. also compare previous approaches latent logistic classiﬁcation model agglomerative clustering method graph partitioning approach global role ordering model also report results improved version agglom recently reported lang lapata strongest previous model bayes bayes accurate version bayesian model titov klementiev estimated conll data without relying external data. titov klementiev also showed using brown clusters induced large external corpus resulted improvement version entirely comparable systems induced solely conll text. model outperforms performs best previous models terms interestingly purity collocation balance different model rest systems. fact model induces roles. contrary bayes predicts roles majority frequent predicates though tendency reduces purity scores model also means roles human interpretable. example agents patients clearly identiﬁable model predictions. model similar purity syntactic baseline outperforms vastly according collocation metric suggesting substantially beyond recovering syntactic relations. additional experiments observed model regimes starts induce roles speciﬁc individual verb senses speciﬁc groups semantically similar predicates. suggests adding latent variable capturing predicate senses conditioning reconstruction component variable result informative semantic representation also improve role induction performance. leave exploration future work. german replicate experimental set-up previously used titov klementiev english report results syntactic baseline results approaches presented table compare bayes+langspeciﬁc bayes model argument signatures specialized german also consider original version bayes model recently lang lapata evaluated agglom+ version german salsa dataset. best result however score results directly comparable. instead using conll dataset processed corpus themselves. also relied syntactic features constituent parser whereas dependency representations used experiments. overall picture german closely resembles english. method achieves results comparable best method evaluated setting. importantly parameters features model german english identical. contrary comparing bayes bayes+langspeciﬁc specialization argument signatures crucial bayesian model. also similarly english method induces less ﬁne-grain sets semantic roles achieves much higher collocation scores. recent years unsupervised approaches semantic role induction attracted considerable attention. however exist ways address insufﬁcient coverage provided existing semantically-annotated resources. natural direction semi-supervised role labeling annotated unannotated data used construct model. previous semi-supervised approaches mostly regarded extensions supervised learning either incorporating word features induced unnannoted texts creating form ‘surrogate’ supervision beneﬁts using unlabeled data moderate signiﬁcant harder version frame-semantic parsing another important direction includes cross-lingual approaches leverage resources research-rich languages well parallel data transfer annotation resource-poor languages. however translation shifts noise word alignments harm performance cross-lingual methods. nevertheless even joint unsupervised induction across languages appears beneﬁcial unsupervised learning also central paradigms closely-related area relation extraction several techniques proposed cluster semantically similar verbalizations relations similarly unsupervised methods mostly rely generative modeling agglomerative clustering. learning perspective methods reconstruction-error objective estimate linear models certainly related. however consider learning factorization models also deal semantics. tensor factorization methods used context modeling knoweldge bases also close spirit. however deal inducing semantics rather factorize existing relations work introduces method inducing feature-rich semantic role labelers unannoated text. approach view semantic role representation encoding latent relation predicate tuple arguments. capture relation probabilistic tensor factorization model. factorization model feature-rich model jointly estimated optimizing objective favours accurate reconstruction arguments given latent semantic representation estimation method yields semantic role labeler achieves state-of-the-art results english german. unlike previous work role induction approach virtually computationally tractable structured model used role labeler including almost semantic role labeler introduced context supervised opens interesting possibilities extend approach semi-supervised setting. previous unsupervised models making strong assumption used limited features effectively unlabeled data. model reconstruction objective easily combined likelihood objective yielding potentially powerful semi-supervised method. leave direction future work. work partially supported google focused award natural language understanding. authors thank dipanjan mikhail kozhevnikov ashutosh modi alexis palmer insightful suggestions.", "year": 2014}