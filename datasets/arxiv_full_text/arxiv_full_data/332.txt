{"title": "Feedback-Controlled Sequential Lasso Screening", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000.", "text": "solve lasso problems dictionary available memory ﬁrst screen dictionary remove unneeded features. prior research shown sequential screening methods offer greatest promise endeavor. existing work sequential screening targets context tuning parameter selection screens solves sequence lasso problems ﬁxed grid geometrically spaced regularization parameters. contrast focus scenario target regularization parameter already chosen cross-validated model selection need solve many lasso instances using ﬁxed value. context propose explore feedback controlled sequential screening scheme. feedback used iteration select next problem solved. allows sequence problems adapted instance presented number intermediate problems automatically selected. demonstrate feedback scheme using several datasets including dictionary approximate size sparse dictionary-based representation data proved effective wide range applications computer vision machine learning signal processing statistics sparse representation uses dictionary rn×p columns {di}p called features represent data point widi relatively small number features. thus many many today’s sparse coding applications dictionary large data dimension large adequate representation requires many features example music genre classiﬁcation authors employed scattering representations short segments music data resulting dictionaries size dictionaries reach size solving become bottleneck overall computation. importantly signiﬁcantly larger dictionaries ﬁtting dictionary available memory concern. several approaches possible solving large dictionary. example assuming dictionary memory could early termination iterative solver quickly obtain approximate solution. alternatively could ﬁnite number steps sequential greedy method approximate solution iteratively selects feature largest correlation current residual. searching feature time consuming part algorithm requires part dictionary loaded once. obtain solution m-sparse algorithm needs hold features memory. several recent studies investigated following alternative approach. given duality quickly identify subset features certiﬁed zero weight solution removing features dictionary solving smaller lasso problem obtain exact solution original problem. approach called safe screening indicating screening reject needed feature. important screening executed features loaded memory once. hence potential signiﬁcantly reduce size dictionary provided lasso solver. recent work safe screening developed idea survey reviews recent methods. related distinct form lasso screening allows false rejection features small probability. explicitly consider screening tests results also relevant approach. core idea safe screening bound solution dual problem within region compute maxθ∈r value used decide feature removed. since tests applied prior solving refer one-shot roughly tests. parameter often useful tests value λmax maxi gives measure well feature matches empirical studies indicate current one-shot screening tests perform well normalized values λ/λmax moderate large size performance quickly declines λ/λmax falls address problem employ complex form screening known sequential screening sequential schemes screens solves sequence lasso problems point instance utilizes dual solution ˆθk− obtain region bound ˆθk. used apply one-shot screening test reduce dictionary. standard solver used solve reduced problem ˆwk. existing state-of-the-art sequential screening algorithms include sequential dome sequential strong rule sequential enhanced rule among others situated context model selection. setting solutions sequence problems interest sequence regularization parameters thus ﬁxed priori sequential screening conducted along predetermined sequence expedite parameter tuning process. contrast focus another distinct application context best regularization parameter denoted already chosen cross-validated model selection need solve many lasso instances using ﬁxed value. particular interest λt/λmax around since many applications sparse representation framework found range helpful. example found λt/λmax range maximizes classiﬁcation accuracy handwritten digit recognition problem. music genre classiﬁcation authors selected λt/λmax range cross-validation maximize classiﬁcation accuracy using classiﬁer proper known problem efﬁcient sequential screening different sequential screening targeted model selection purpose solution interest problem instances merely points. thus given freedom design sequence regularization parameters {λk}n speciﬁcally target single problem instance scenario open-loop sequential screening schemes using geometrically spaced sequence {λk}n determined problem sequence solved good idea. would advantage {λk}n paper makes contributions. first design feedback mechanism adaptively select sequence {λk}n sequential screening. feedback mechanism automatically selects next value function results seen previous steps sequential screening process. also determines stop hence automatically selects call feedback controlled sequential screening scheme data-adaptive sequential screening dass advantage {λk}n automatically adapted screening process particular instance addition feedback mechanism bounds region diameter used one-shot screening iteration value user. second examine effects inevitable errors accrue obtaining solutions intermediate lasso problems sequential screening. step sequential screening assumes exact knowledge previous dual solution. however practical lasso solvers introduce small error. context classiﬁcation show dass scheme robust errors. give required background introduce dass examine properties show dass ensures diameters regions used one-shot screening bounded user selected value number intermediate lasso problems also bounded. address issue inaccuracies lasso solutions. discusses performance dass selection datasets conclude proofs given supplementary material. dual problem extensively discussed literature feasible points dual problem nonempty closed convex polyhedron formed intersection ﬁnite closed half spaces {±di}p objective maximized projection onto unique point satisfying λmax λmax maxp y/λmax boundary λmax moves away unique projection onto boundary form one-shot test bounds dual solution compact computes feature satisﬁes maxθ∈r bounding region selected intersection sphere center radius half spaces i.e. maxθ∈r obtained solving convex program closed form solutions available sequential screening screens solves sequence problems open-loop scheme {λk}n selected before solution obtained. example .λmax space geometrically αλk− note must sphere center radius y/λmax dmax feature satisfying lies half space maxθ screen dictiohmax nary one-shot screening test hmax. figure sequential screening. step ˆθk− deﬁnes spherical bound hyperplane separates y/λk−. hence contained dome formed within sphere hyperplane. solid line hyperplane active feature θk−. large hemisphere bound ˆθk− /λk− used diagram drawn scale. assumption exact computation develop sequential screening feedback rule analyze properties. issue approximate lasso solution examined later. diameter denoted diam maximum distance points following preliminary lemma useful. lemma nonempty interior region provides initial bound one-shot test applied step half space constraints used form ﬁnal bounding region example one-shot test adding second appropriately selected half space constraint dual problem. matter many additional half space constraints added provides upper bound diameter ﬁnal bounding region. thus following corollary. corollary obtained adding ﬁnite number additional half space constraints diam bounded expression proposition suggests rule selecting diam user speciﬁed parameter. using achieved selecting that assuming aligned ˆθk− sequential screening scheme yields dass algorithm algorithm initialize .λmax. step ﬁrst one-shot screening based region solve resulting reduced lasso problem hence problem solved efﬁciently. write becomes k−y). thus ﬁrst order autoresk− addition dual regularization path bounded i.e. exists assumption theorem bounded holds range always holds rank show number iterations required algorithm terminate real problem instances practice realistic assume approximation ˜θk− ˆθk− ˆθk− simplicity assume depend essential. ﬁrst verify dass algorithm still terminates ﬁnite number steps give bound number steps required. state corollary theorem corollary suppose approximate dual solution assumption dass sequence terminates many classiﬁcation applications lasso regression often used feature extraction step. data point classiﬁed lasso regression ﬁrst used compute solution feature vector classiﬁer e.g. src. analyze classiﬁcation setting effect accumulated inaccuracy sequential screening process. consider using approximate solution classiﬁcation. used sequential screening ˜wk− gives approximate dual solution ˜θk− used one-shot screening step oneshot test fail reject falsely reject features. error thus sources lasso solver itself false feature rejection. latter errors propagate leading dass fail give adequate rejection make many false rejections causing classiﬁcation performance suffer. investigate compare dass state-of-the-art open-loop sequential screening schemes ﬁxed grid geometrically spaced regularization parameters. open-loop sequential screening schemes sequential dome sequential strong rule sequential enhanced rule sequential dome sequential version dome test proposed authors figure λt/λmax particular interest cross validation shows range helpful many classiﬁcation problems. handwritten digit recognition mnist music genre classiﬁcation gtzan object recognition coil text categorization figure comparison dass sequential dome sequential strong rule sequential enhanced rule mnist. scatter plot speedup versus rejection percentage lasso instances λt/λmax average rejection speedup λt/λmax average dass shown speedup curve. commented dome test unclear whether sequential version exists include comparison sequential enhanced rule sequential dome. claim true hereby include algorithms comparison. among four screening algorithms sequential strong rule falsely discard features. performance metrics rejection percentage ratio number features rejected screening rules number features. speedup time solve without screening divided total time screen plus solve reduced lasso problem along sequence note difference metrics deﬁnition compared rejection percentage deﬁned ratio number features discarded screening rules actual number zero features ground truth; speedup deﬁned time solve sequence lasso problems without screening divided total time screen plus solve reduced lasso problem along sequence difference speedup deﬁnition mainly originates different motivation application discussed earlier datasets mnist hand-written digit images image rearranged vector scaled unit norm. sample training images form dictionary. test images randomly selected testing set. gtzan music clips genres music clip divided -sec texture windows overlap adjacent represented using ﬁrst order scattering vector length randomly select dictionary scattering vectors randomly select test vectors remaining york times articles represented vectors respect vocabulary words i-th entry vector gives occurrence count word document removing documents word counts documents remain. documents randomly selected form dictionary remaining documents selected test vectors λmax experiments mnist gtzan randomly select dictionaries dictionary randomly selected test vectors. reported results averaged across selected dictionaries test vectors. using scaling invariant ratio λt/λmax higher value yielding sparser solution. speedup results using feature-sign lasso solver shown experiment indicate consistent results variety solvers. example supplementary material additional results using fista solver. open-loop sequential screening scheme given selected sequence {λk}n individual problem test effectiveness adaptation solving problem instances λt/λmax mnist gtzan. plot scatter plot speedup versus rejection percentage problem instances. rows fig. fig. mnist average used dass standard error dass successfully pushes rejection percentage speedup distribution higher ranges compared openloop schemes using figure comparison dass sequential dome sequential strong rule sequential enhanced rule gtzan. scatter plot speedup versus rejection percentage lasso instances λt/λmax average rejection speedup λt/λmax average dass shown speedup curve. dass also exhibits robust average rejection percentage speedup λt/λmax varies mnist dass consistently beats rivals open-loop schemes ranges open-loop schemes close dass rejection average speedup around less values λt/λmax. dass speedup curve gives nice outer envelope across open-loop speedup curves. average used dass corresponding value λt/λmax given dass speedup curve λt/λmax decreases dass increases case-by-case basis done without user intervention. using properly cross-validated dass able automatically design efﬁcient sequence {λk}n individual similar results seen gtzan. since overhead accumulates increases dass open-loop sequential screening schemes exhibit less speedup λt/λmax decreases. nevertheless dass pushes efﬁcient robust screening λt/λmax includes range useful many classiﬁcation applications. dass scheme easy implement parameter selected cross-validation using reasonable value λt/λmax. scheme gives strong screening performance across wide range values λt/λmax. smaller value user-speciﬁed parameter yields tighter bound diam hence stronger rejection also larger value hence speedup sweet spot decreases. empirical investigation effect given fig. examine effect noisy lasso solutions noise computed various values noise-to-signal ratio hard threshold yield corrupted sparse solution. impact rejection fraction speedup classiﬁcation accuracy shown mnist fig. rejection remains around noise-to-signal ratio goes speedup drops around dass remains effective noise-to-signal ratio within bound small penalty added corruption dass robust errors. addition used sparse representation classiﬁcation classiﬁcation accuracy also robust error finally dataset explore problems many high dimensional features. test vector pool documents select documents subject λmax λt/λmax since dictionary cannot computer memory screening loading small segments dictionary memory time. test dass average resulting values problem instances standard error comparison sequential dome sequential strong rule sequential enhanced rule allocate amount system resources algorithm problem instance. sequential dome sequential edpp able complete problem instances instances reject enough features process memory figure comparison dass sequential dome sequential strong rule sequential enhanced rule nyt. dass three open-loop sequential screening schemes applied lasso instances λmax λt/λmax colored sign marks average performance algorithm. error. thus include completion rate metric fig. deﬁned percentage problem instances completed given system resources. dass sequential strong rule take lead metric able complete problem instances. problem instances completed algorithm provide scatter plot total running time versus number remaining features fig. speedup information shown since cannot solve problems without screening. results show dass sequential strong rule signiﬁcantly outperform sequential dome sequential edpp rules sequential strong rule able reject dass able ﬁnish less runtime. note however strong rule commit false rejections. shown analytically empirically feedback controlled sequential screening yields improved performance. given cross-validated target dass rule tailors individualized sequence {λk}n optimized particular problem instance demonstrated signiﬁcantly greater robustness respect inherent variability lasso problem. feedback idea extends bound used selection rule yields feedback controlled version sequential uses looser bound results also suggest possible feedback control sequential screening targeted ways. example varying parameter response previously solved instances possible regulate size dictionary screening. would useful situations large number features limited memory. and´en mallat. deep scattering spectrum. ieee trans. signal processing beck teboulle. fast iterative shrinkage-thresholding algorithm linear inverse problems. j.-b. hiriart-urruty lemarechal. fundamentals convex analysis. springer lecun cortes. mnist database handwritten digits battle raina a.y. efﬁcient sparse coding algorithms. advances neural wagner wright ganesh zhou mobahi towards practical face recognition system robust alignment illumination sparse representation. ieee transactions pattern analysis machine intelligence wang chen ramadge. sparse representation classiﬁcation sequential lasso screening. global conference signal information processing ieee pages dec. objective function inequality constraint functions convex functions assumption lemma exist strictly feasible points. hence conditions necessary sufﬁcient optimality thus solution sufﬁcient points satisfy four constraints complementary slackness conditions λkθk construct candidate solution. suppose contained denote orthogonal projection onto hyperplane since since lies plane must hence candidate solution take case candidate points satisfy spherical bound equality. half space constraint also satisﬁed generally inequality. candidate solution satisﬁes constraints ﬁrst complementary slackness conditions. case substitution expressions yields proof lemma proof. y/λmax since λmax outside dual feasible outside half space thus outside ck−} given equation y/λk− since λmax. since k−qk− ck−. hence /r/y sufﬁcient termination deduce y/r). bound loose. proposition feedback rule give diam stronger bound requires follow detailed argument. decreasing sequence {λk}n ˆθk−/ ˆθk−. ﬁrst consider possibility scalar aligned. since λmax case must ˆθk− y/λmax. y/λmax case easily handled. hence assume ˆθk− aligned lemma range bounded. proof. optimality must subdifferential hence rearrangement gives compact svd. since range hence result follows observing bounded. proof. inequality still holds. hence dass terminates ﬁnite number steps. step solution returned ˜θk− ˆθk−+\u0001k− ˆθk−− ˜θk− discussed earlier generically ˆθk− aligned large ˜θk− aligned well. ˜θk−/ ˜θk−. substituting ˆθk− ˜θk− equation error-free case show another empirical evaluation effect using dataset. yalebxf uses frontal face images subjects extended yale face recognition data randomly select face images dictionary randomly choose remaining images. trend fista result similar featuresign observed datasets tested featuresign seems faster fista fista sensitive reduction features used conjunction screening rule thus greater speedup. extend feedback idea sequential rule proposed sequential uses ˆθk− /λk−/λk−} one-shot screen dictionary spherical bound sdpp step feedback rule sequential rule follows sequence terminates ﬁnite number steps compared dass feedback controlled sequential larger upper bound number iterations required.", "year": 2016}