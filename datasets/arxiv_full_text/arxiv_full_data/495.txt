{"title": "Learning to Reason With Adaptive Computation", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model.", "text": "multi-hop inference necessary machine learning systems successfully solve tasks recognising textual entailment machine reading. work demonstrate effectiveness adaptive computation learning number inference steps required examples different complexity learning correct number inference steps difﬁcult. introduce ﬁrst model involving adaptive computation time provides small performance beneﬁt similar model without adaptive component well enabling considerable insight reasoning process model. recognising textual entailment task determining whether hypothesis inferred premise. argue natural language inference requires combination inferences provide stepping stone towards development method. steps compared backtracking logic programming language employing attention mechanism able visualise inference step allowing interpret inner workings model. centre approach adaptive computation time ﬁrst example neural network deﬁned using static computational graph execute variable number inference steps conditional input. however originally applied vanilla recurrent neural network whereas show applied arbitrary computation problem explicitly deﬁned beneﬁt act. humans resolve entailment problems adept breaking large problems sub-problems also re-combining sub-problems structured way. often simple problems negation resolution ﬁrst step necessary. however complicated cases multiple co-reference resolution lexical ambiguity necessary decompose reason. instance contradicting statements require multi-step temporally dependent reasoning resolve correctly. closer examination resolution action relating entities conditions scene leading inference chain similar ﬁnal true/false statement built ﬁrst observing facts scene combining extending them. idea combining distinct low-level inferences show adaptive computation multi-step inference used examine incorporating additional inferences inference chain inﬂuence ﬁnal classiﬁcation. visualisation provides additional tool analysis deep learning based models. attention mechanisms neural networks ﬁrst introduced machine translation demonstrated state-of-the-art performance model extension iterative alternating attention model originally employed machine reading combined decomposable attention model previously proposed rte. original motivation behind decomposable attention model bypass bottleneck generating single document representation prohibitively restrictive document grows size. instead model incorporates inference’ step query document iteratively attended ﬁxed number iterations order generate representation classiﬁcation. contribution generalise inference step adaptive number steps conditioned input using single step adaptive computation time algorithm recurrent neural networks. also show used execute arbitrary functions considering differentiable implementation loop attention encoders original formulation vector representations words simply generated product result single feedforward network appended original word vector. un-normalised alignment weight given hypothesis premise representations h...hm p...pn deﬁned feedforward network relu activation functions. vector representations hypothesis premise generated taking softmax respective dimensions matrix concatenating original word vector representation alternating iterative attention representation hypothesis premise iteratively attend them. inference iteration generate attention representation hypothesis attention weights dimensionality inference state denotes concatenation vectors. gating mechanism although attention representations could concatenated input inference also make last addition proposed order allow attention representations forgotten/not used. layer feedforward networks rs+d generated attention representations multiplied element-wise result gating function concatenated forming input time inference gru. number inference steps hyperparameter model. instead learn number inference steps take using single step adaptive computation time algorithm described appendix input halting layer output inference gru. figure visualisation attention weights produced inference step used form states inference gru. time-step hypothesis attended using inference state query. representation appended query used generate attention mask premise. right show softmax classiﬁcation model used inference state timestep classify here taking multiple steps corrects classiﬁcation. figure appendix provided attention visualisations demonstrating model uses it’s access premise hypothesis representations. found following points particularly note model rarely takes single inference step even cases high regularisation. several instances large weight ﬁnal loop inference gru. demonstrating model found attention representation provides necessary insight learn halt immediately. additionally ponder cost parameter shows context dependent behaviour preliminary experiments using encoders optimal ponder cost settings distinct. reinforces argument parameter difﬁcult correctly. below present results stanford natural language inference corpus model logistic regression lexical features baseline lstm \"skip thought\" spinn-pi recursive lstm word-by-word attention decomposable attention full tree matching nti-slstm-lstm decomposable attention adaptive attention adaptive attention adaptive attention adaptive attention noted unable replicate performance originally reported model implementation https//github.com/deneutoy/decomposable_attn despite correspondence authors continue seek reason discrepancy. however given objective demonstrate usefulness adaptive computation applied reasoning using attention feel detract signiﬁcantly purpose paper. additionally model takes mean steps inference time meaning efﬁcient performant using large ﬁxed number steps. performance plateau demonstrated appendix likely artifact snli corpus constructed amazon turkers ﬁnancial incentive provide minimal hypothesis contradicting agreeing with neutral premise. construction naturally beneﬁts words approaches minimal changes often result word overlap features particularly discriminatory. additionally approach involves combining conditional attention masks less beneﬁt situation little difference premise hypothesis. overall demonstrated taking steps multi-hop learning scenario always improve performance desirable adapt number steps individual data examples. exploration regularisation methods tune ponder cost parameter considered given model’s sensitivity regard. assumption single state rather combinations differences states best method determine halting probability investigated subject future work halting promptly demonstrably aids performance. additionally believe investigation attention mechanisms particularly efﬁcient implementations combined input conditional computation important avenue future research. work supported marie curie career integration award allen distinguished investigator award. authors would like thank rocktäschel helpful feedback comments. samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. corr abs/. below show accuracy function ﬁxed inference steps evaluating model repeatedly whilst ﬁxing maximum number inference steps take test time. graph clear difference models using single inference ones employing this. however performance clearly scale linearly respect number inference hops taken model given similar results hops. argue indifference performance inference steps shows choosing value data dependant preferable. note uptick ﬁnal performance combines inferences using weighted average individual attentions. figure example incorrect classiﬁcation standard analysis would simply demonstrate failure actually model extremely uncertain whether falls contradiction neutral categories chooses wrong class small margin. example additionally provides evidence support idea steps adaptive although classiﬁed incorrectly single step taken here example would correctly classiﬁed. ﬁnal addition so-called ponder cost used regularise number computational steps taken. made combination remainder function iteration counter added original loss function. adaptive computation seen differentiable implementation loop perform operations within body single timestep. idea generalises away context within vanilla recurrent neural network allowing complex functionality modules including attention extraneous inputs. section describe results insights running models described methods section stanford natural language inference corpus adaptive attention model decomposable attention model grid searches following hyperparameter ranges epochs full training data. table hyperparameter ranges grid searches. bold italicised text represent best settings adaptive attention decomposable attention models respectively. included hyperparameter search vocab size ﬁxed frequent words inference size ﬁxed clipped gradients absolute value models trained using adagrad optimiser adam optimiser respectively models. word embeddings initialised using glove pre-trained vector representations words without pre-trained representation initialised vectors drawn standard normal distribution word embeddings updated training non-linear embedding projection parameter trained. following approach taken bowman original snli paper discard examples gold label leaving training validation testing respectively. running hyperparameter searches hyperparameter setting best accuracy validation split selected trained convergence. models evaluated test set. generate comparison adaptive attention model models ﬁxed numbers hops hyperparameters best variant grid search simply halt inference different ﬁxed numbers memory hops.", "year": 2016}