{"title": "Efficient Large-Scale Multi-Modal Classification", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "While the incipient internet was largely text-based, the modern digital world is becoming increasingly multi-modal. Here, we examine multi-modal classification where one modality is discrete, e.g. text, and the other is continuous, e.g. visual representations transferred from a convolutional neural network. In particular, we focus on scenarios where we have to be able to classify large quantities of data quickly. We investigate various methods for performing multi-modal fusion and analyze their trade-offs in terms of classification accuracy and computational efficiency. Our findings indicate that the inclusion of continuous information improves performance over text-only on a range of multi-modal classification tasks, even with simple fusion methods. In addition, we experiment with discretizing the continuous features in order to speed up and simplify the fusion process even further. Our results show that fusion with discretized features outperforms text-only classification, at a fraction of the computational cost of full multi-modal fusion, with the additional benefit of improved interpretability.", "text": "incipient internet largely text-based modern digital world becoming increasingly multi-modal. here examine multi-modal classiﬁcation modality discrete e.g. text continuous e.g. visual representations transferred convolutional neural network. particular focus scenarios able classify large quantities data quickly. investigate various methods performing multi-modal fusion analyze trade-offs terms classiﬁcation accuracy computational efﬁciency. ﬁndings indicate inclusion continuous information improves performance text-only range multi-modal classiﬁcation tasks even simple fusion methods. addition experiment discretizing continuous features order speed simplify fusion process even further. results show fusion discretized features outperforms text-only classiﬁcation fraction computational cost full multimodal fusion additional beneﬁt improved interpretability. text classiﬁcation core problems machine learning natural language processing plays crucial role important tasks ranging document retrieval categorization sentiment topic classiﬁcation however incipient largely text-based recent decade seen surge multi-modal content billions images videos posted shared online every single day. text either replaced dominant modality case instagram posts youtube videos augmented non-textual content today’s pages. makes multi-modal classiﬁcation important problem. here examine task multi-modal classiﬁcation using neural networks. primarily interested questions best combine data different modalities efﬁcient manner? examine various efﬁcient multi-modal fusion methods investigate ways speed fusion process. particular explore discretizing continuous features leads much faster training requires less storage still able beneﬁt inclusion multi-modal information. best knowledge work constitutes ﬁrst attempt examine accuracy/speed trade-off multi-modal classiﬁcation; ﬁrst directly show value discretized features particular task. current trends continue become increasingly multi-modal making question multi-modal classiﬁcation ever pertinent. time keeps growing able efﬁciently handle ever larger quantities data making important focus machine learning methods applied large-scale scenarios. work aims examine questions together. contributions follows. first compare various multi-modal fusion methods examine trade-offs show simpler models often desirable. second experiment discretizing continuous features order speed simplify fusion process even further. third examine learned representations discretized features show yield interpretability beneﬁcial side effect. work reported constitutes solid scalable baseline approaches follow; investigation discretized features shows multi-modal classiﬁcation necessarily imply large performance penalty feasible large-scale scenarios. text classiﬁcation. neural network-based methods become increasingly popular text classiﬁcation recent work used neural networks text classiﬁcation either sentence full document level. many core tasks essentially text classiﬁcation tweets reviews spam. even though extensive work feature engineering text classiﬁcation modern approaches often make word embeddings sentence representations learned large corpus unsupervised fashion. fusion strategies. multi-modal fusion integration input various modalities important topic ﬁeld multimedia analysis question fusion explored variety tasks audio-visual speech recognition multi-sensor management face recognition much research focused combination continuous modalities. here speciﬁcally interested fusion discrete textual input another continuous modality. multi-modal nlp. usage non-textual information natural language processing become increasingly popular. hand interest cross-modal applications image annotation image captioning mapping images text vice versa visual question answering hand multi-modal fusion extensively explored context grounded representation learning lexical semantics much work focused vision perceptual modalities modalities also explored well robotics videos games work similar spirit explore fusion techniques. however similarly learn integrate multi-modal inputs transferred representations multi-modal deep learning. work relates previous work integrating information multiple modalities neural networks here enhance well-known neural network architecture efﬁcient text classiﬁcation ability include continuous information explore methods combining multi-modal features. works explore complex gating mechanisms compact bilinear pooling multi-modal fusion methods. order obtain visual representations transfer continuous features neural networks trained tasks shown work well wide variety tasks trade-offs meaningful way. datasets medium-sized; third dataset large today’s standards. quantitative properties respective datasets shown table described detail follows. upmc food dataset contains pages textual recipe descriptions food labels automatically retrieved online. page matched single image images obtained querying google image search given category. examples food labels filet mignon thai breakfast burrito spaghetti bolognese. pages processed htmltext obtain text. recently introduced mm-imdb dataset contains movie plot outlines movie posters. objective classify movie genre. multilabel prediction problem i.e. movie multiple genres. dataset speciﬁcally introduced address lack multi-modal classiﬁcation datasets. flickrtag dataset based massive yfccm flickr dataset used dataset consists flickr photographs together short user-provided captions. objective predict user-provided tags belong photograph. large-scale dataset perform multi-modal fusion operator speed-versus-accuracy studies subset studies denote flickrtag-. show inclusion discretized features yields classiﬁcation accuracy improvements respect text whole dataset. starting point take highly efﬁcient text classiﬁcation approach fasttext ensure fair comparison enhance model capability handle continuous discretized features. specifically -dimensional continuous features obtained transferring pre-softmax layer -layer resnet trained imagenet classiﬁcation task. case large-scale flickrtag datasets resnet- features shown convolutional network features transferred successfully variety tasks take approach here. explore variety models experiment discretization. think approach performing attention modality other. conceptually similar simpliﬁcation multi-modal gated units introduced modality gated hyperparameter approach thought simpler version complex multi-modal bilinear pooling introduced also experiment method introduce gating non-linearity bilinear model call bilinear-gated. downside continuous models require expensive matrix-vector multiplication storing large matrices ﬂoating point numbers requires space. resnet features used experiments consist relatively small number components easily tens thousands consider e.g. combinations sift fisher vectors used state-of-the-art computer vision applications hence experiment discretizing continuous features convert continuous features discrete sequence tokens treated special tokens normalize separately used standard fasttext setup. simple computationally less intensive solution. discretized features also obviously require less storage. particular investigate product quantization divide continuous vector subvectors equal size perform k-means clustering subvectors. image subsequently determine closest centroid subwords combined subvector index order obtain discretized vector. example case however also take account efﬁciency want focus models simple efﬁcient enough handle large-scale datasets obtaining improved performance baselines. experiment comprehensive models listed increasing order complexity. text ﬁrst baseline consists fasttext library highly efﬁcient word representation learning sentence classiﬁcation. fasttext trained asynchronously multiple cpus using stochastic gradient descent learning rate linearly decays amount words. yields competitive performance sophisticated text classiﬁcation approaches being much efﬁcient. ignore visual signal altogether textual information i.e. -dim continuous vector divided dimensional subvectors denote index nearest centroid discretized representation given include tokens text treat special tokens standard fasttext model i.e. discretized features reweighting hyperparameter. normalize independently. discretized models closely related additive model except weight matrix discretized features used words text. great compressing information discretized sequence impose hard boundaries subvectors means overlapping semantic content shared subvectors lost. hence introduce novel quantization method called random sample product quantization order maintain overlapping semantic information. rspq process except perform repetitions random permutations cases treat discretized features reweighted special tokens included textual data standard fasttext. various trade-offs stake models. additive max-pooling gated models simplest result hidden layer size normal fasttext. computational complexity linear classiﬁer thus number classes size max-pooling gating models slightly complicated additive requiring extra operation. bilinear model complexity amounts thus bilinear model expensive compute. additive model beneﬁt strictly require continuous input times. experiments tuned validation set. tried following hyperparameters number epochs reweighting parameter embedding dimensionality either hyperparameters sweeped using grid search used softmax loss. hyperparameters number threads parallel optimization minimum word count ﬁxed values standard values fasttext since found impact classiﬁcation accuracy. case gated bilinear-gated models modality used serve gate modality treated hyperparameter well. results comparison found table compare continuous discretized multi-modal models text-only fasttext model continuous features-only model. also include results food- used tf-idf features text deep convolutional neural network features images well results gated multimodal units probs model. gmus substantially complicated model architecture relatively simple fusion methods study good test capability. note case food methods work considerably better previously reported results. mm-imdb continuous multi-modal models perform close model outperform probs method simpler computationally efﬁcient. observe multi-modal models always outperform standard fasttext continuous-only approach disregarding particular type fusion. shows inclusion multi-modal information always helps making multi-modal information available lead increased performance. fasttext outperforms continuousmethod datasets indicates text plays role tasks relatively important visual information. examine continuous multi-modal models bilinear-gated model clear winner outperforms methods three tasks. however also complicated model result less efﬁcient. found placing gating non-linearity text best performance food mm-imdb placing visual modality best performance flickrtag. interesting observe complicated gated model well non-gated bilinear model necessarily outperform simpler additive max-pooling models. fact performance much simpler models removed best scores. take-away message appears care accuracy bilinear method gating; care speed simple model like additive max-pooling additional potential beneﬁt necessarily require presence continuous information none available. draw inspiration fact additive model performed reasonably well speed essential—if necessary expense accuracy—the discretized models obvious choice simplify speed model. even though outperform standard fasttext large margin shown table come minor performance impact. table shows training times various models flickrtag- dataset bilinear models take around hour train discretized methods similarly fasttext take around minute. scale full flickrtag dataset table shows discretized models substantially outperform standard fasttext. increase accuracy seen fasttext rspq represents additional test documents correctly classiﬁed using model non-negligible amount. interesting side effect discretized multi-modal methods allow examine nearest neighbors quantized features particular feature corresponds something looks like donut example embedding close words related donut. indeed table shows clearly identiﬁable clusters e.g. donuts cr`eme brˆul´ee certain types japanese food. interpretability important often overlooked aspect classiﬁcation models show simple efﬁcient method outperforms textmethods large margin yields additional beneﬁt allows interpretation visual features classiﬁer picks on—something difﬁcult achieve standard convolutional features. internet becoming increasingly multi-modal makes task multi-modal classiﬁcation ever pertinent. order able handle large quantities data need efﬁcient models large-scale multi-modal classiﬁcation. work examined questions together. first compared various multi-modal fusion methods found bilinear-gated model achieve highest accuracy simpler additive max-pooling models yielded reasonably high accuracy higher speed. second showed model speeded even introducing discretized multi-modal features. lastly showed method yields additional beneﬁt interpretability examine multi-modal model picks making classiﬁcation decision. hope work serve useful baseline work multi-modal classiﬁcation.", "year": 2018}