{"title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "text": "propose algorithm meta-learning model-agnostic sense compatible model trained gradient descent applicable variety different learning problems including classiﬁcation regression reinforcement learning. goal meta-learning train model variety learning tasks solve learning tasks using small number training samples. approach parameters model explicitly trained small number gradient steps small amount training data task produce good generalization performance task. effect method trains model easy ﬁne-tune. demonstrate approach leads state-of-the-art performance fewshot image classiﬁcation benchmarks produces good results few-shot regression accelerates ﬁne-tuning policy gradient reinforcement learning neural network policies. learning quickly hallmark human intelligence whether involves recognizing objects examples quickly learning skills minutes experience. artiﬁcial agents able same learning adapting quickly examples continuing adapt data becomes available. kind fast ﬂexible learning challenging since agent must integrate prior experience small amount information avoiding overﬁtting data. furthermore form prior experience data depend task. such greatest applicability mechanism learning learn general task work propose meta-learning algorithm general model-agnostic sense directly applied learning problem model trained gradient descent procedure. focus deep neural network models illustrate approach easily handle different architectures different problem settings including classiﬁcation regression policy gradient reinforcement learning minimal modiﬁcation. meta-learning goal trained model quickly learn task small amount data model trained meta-learner able learn large number different tasks. idea underlying method train model’s initial parameters model maximal performance task parameters updated gradient steps computed small amount data task. unlike prior meta-learning methods learn update function learning rule algorithm expand number learned parameters place constraints model architecture siamese network readily combined fully connected convolutional recurrent neural networks. also used variety loss functions including differentiable supervised losses nondifferentiable reinforcement learning objectives. process training model’s parameters gradient steps even single gradient step produce good results task viewed feature learning standpoint building internal representation broadly suitable many tasks. internal representation suitable many tasks simply ﬁne-tuning parameters slightly produce good results. effect procedure optimizes models easy fast ﬁne-tune allowing adaptation happen right space fast learning. dynamical systems standpoint learning process viewed maximizing sensitivity loss functions tasks respect parameters sensitivity high small local changes parameters lead primary contribution work simple modeltask-agnostic algorithm meta-learning trains model’s parameters small number gradient updates lead fast learning task. demonstrate algorithm different model types including fully connected convolutional networks several distinct domains including few-shot regression image classiﬁcation reinforcement learning. evaluation shows meta-learning algorithm compares favorably state-of-the-art one-shot learning methods designed speciﬁcally supervised classiﬁcation using fewer parameters also readily applied regression accelerate reinforcement learning presence task variability substantially outperforming direct pretraining initialization. train models achieve rapid adaptation problem setting often formalized few-shot learning. section deﬁne problem setup present general form algorithm. goal few-shot meta-learning train model quickly adapt task using datapoints training iterations. accomplish this model learner trained meta-learning phase tasks trained model quickly adapt tasks using small number examples trials. effect meta-learning problem treats entire tasks training examples. section formalize metalearning problem setting general manner including brief examples different learning domains. discuss different learning domains detail section consider model denoted maps observations outputs meta-learning model trained able adapt large inﬁnite number tasks. since would like apply framework variety learning problems classiﬁcation reinforcement learning introduce generic notion learning task below. formally task consists loss function distribution initial observations transition distribution episode length i.i.d. supervised learning problems length model generate samples length choosing output time loss provides task-speciﬁc feedback might form misclassiﬁcation loss cost function markov decision process. meta-learning scenario consider distribution tasks want model able adapt k-shot learning setting model trained learn task drawn samples drawn feedback generated meta-training task sampled model trained samples feedback corresponding loss tested samples model improved considering test error data changes respect parameters. effect test error sampled tasks serves training error meta-learning process. meta-training tasks sampled meta-performance measured model’s performance learning samples. generally tasks used meta-testing held meta-training. contrast prior work sought train recurrent neural networks ingest entire datasets feature embeddings combined nonparametric methods test time propose method learn parameters standard model meta-learning prepare model fast adaptation. intuition behind approach internal representations transferrable others. example neural network might learn internal features broadly applicable tasks rather single individual task. encourage emergence general-purpose representations? take explicit approach problem since model ﬁne-tuned using gradient-based learning rule task learn model gradient-based learning rule make rapid progress tasks drawn without overﬁtting. effect model parameters sensitive changes task small changes parameters produce large improvements loss function task drawn altered direction gradient loss formally consider model represented parametrized function parameters adapting task model’s parameters become method updated parameter vector computed using gradient descent updates task example using gradient update α∇θlti. step size ﬁxed hyperparameter metalearned. simplicity notation consider gradient update rest section using multiple gradient updates straightforward extension. note meta-optimization performed model parameters whereas objective computed using updated model parameters effect proposed method aims optimize model parameters small number gradient steps task produce maximally effective behavior task. products supported standard deep learning libraries tensorflow experiments also include comparison dropping backward pass using ﬁrst-order approximation discuss section species maml section discuss speciﬁc instantiations meta-learning algorithm supervised learning reinforcement learning. domains differ form loss function data generated task presented model basic adaptation mechanism applied cases. few-shot learning well-studied domain supervised tasks goal learn function input/output pairs task using prior data similar tasks meta-learning. example goal might classify images segway seeing examples segway model previously seen many types objects. likewise few-shot regression goal predict outputs continuous-valued function datapoints sampled function training many functions similar statistical properties. formalize supervised regression classiﬁcation problems context meta-learning deﬁnitions section deﬁne horizon drop timestep subscript since model accepts single input produces single output rather sequence inputs outputs. task generates i.i.d. observations task loss represented error model’s output corresponding target values observation task. common loss functions used supervised classiﬁcation regression cross-entropy mean-squared error describe below; though supervised loss functions used well. regression tasks using mean-squared error loss takes form sample trajectories using evaluate ∇θlti using equation compute adapted parameters gradient descent α∇θlti sample trajectories using update equation according conventional terminology k-shot classiﬁcation tasks input/output pairs class total data points n-way classiﬁcation. given distribution tasks loss functions directly inserted equations section perform meta-learning detailed algorithm reinforcement learning goal few-shot metalearning enable agent quickly acquire policy test task using small amount experience test setting. task might involve achieving goal succeeding previously trained goal environment. example agent might learn quickly ﬁgure navigate mazes that faced maze determine reliably reach exit samples. section discuss maml applied meta-learning task contains initial state distribution transition distribution loss corresponds reward function entire task therefore markov decision process horizon learner allowed query limited number sample trajectories few-shot learning. aspect change across tasks model learned policy maps states distribution actions timestep loss task model takes form since expected reward generally differentiable unknown dynamics policy gradient methods estimate gradient model gradient update meta-optimization. since policy gradients on-policy algorithm additional gradient step adaptation requires samples current policy detail algorithm algorithm algorithm structure algorithm principal difference steps require sampling trajectories environment corresponding task practical implementations method also variety improvements recently proposed policy gradient algorithms including state action-dependent baselines trust regions method propose paper addresses general problem meta-learning includes few-shot learning. popular approach metalearning train meta-learner learns update parameters learner’s model approach applied learning optimize deep networks well learning dynamically changing recurrent networks recent approach learns weight initialization optimizer few-shot image recognition unlike methods maml learner’s weights updated using gradient rather learned update; method introduce additional parameters meta-learning require particular learner architecture. speciﬁc tasks generative modeling image recognition successful approach few-shot classiﬁcation learn compare examples learned metric space using e.g. siamese networks recurrence attention mechanisms approaches generated successful results difﬁcult directly extend problems reinforcement learning. method contrast agnostic form model particular learning task. another approach meta-learning train memoryaugmented models many tasks recurrent learner trained adapt tasks rolled out. networks applied few-shot image recognition learning fast reinforcement learning agents experiments show method outperforms recurrent approach fewshot classiﬁcation. furthermore unlike methods approach simply provides good weight initialization uses gradient descent update learner meta-update. result straightforward ﬁnetune learner additional gradient steps. approach also related methods initialization deep networks. computer vision models pretrained large-scale image classiﬁcation shown learn effective features range problems contrast method explicitly optimizes model fast adaptability allowing adapt tasks examples. method also viewed explicitly maximizing sensitivity task losses model parameters. number prior works explored sensitivity deep networks often context initialization works considered good random initializations though number papers addressed datadependent initializers including learned initializations contrast method explicitly trains parameters sensitivity given task distribution allowing extremely efﬁcient adaptation problems k-shot learning rapid reinforcement learning gradient steps. goal experimental evaluation answer following questions maml enable fast learning tasks? maml used meta-learning multiple different domains including supervised regression classiﬁcation reinforcement learning? meta-learning problems consider require amount adaptation tasks test-time. possible compare results oracle receives identity task additional input upper bound performance model. experiments performed using tensorflow allows automatic differentiation gradient update meta-learning. code available online. start simple regression problem illustrates basic principles maml. task involves regressing input output sine wave amplitude phase sinusoid varied tasks. thus continuous amplitude varies within phase varies within input output dimensionality training testing datapoints sampled uniformly loss mean-squared error prediction true value. regressor neural network model hidden layers size relu nonlinearities. training maml gradient update examples ﬁxed step size adam metaoptimizer baselines likewise trained adam. evaluate performance ﬁnetune single meta-learned model varying numbers examples compare performance baselines pretraining tasks entails training network regress random sinusoid functions then test-time ﬁne-tuning gradient descent provided points using automatically tuned step size oracle receives true amplitude phase input. appendix show comparisons additional multi-task adaptation methods. evaluate performance ﬁne-tuning model learned maml pretrained model datapoints. ﬁne-tuning gradient step computed using datapoints. qualitative results shown figure expanded appendix show learned model able quickly adapt datapoints shown purple triangles whereas model pretrained using standard supervised learning tasks unable adequately adapt datapoints without catastrophic overﬁtting. crucially datapoints half input range figure few-shot adaptation simple regression task. left note maml able estimate parts curve datapoints indicating model learned periodic structure sine waves. right fine-tuning model pretrained distribution tasks without maml tuned step size. often contradictory outputs pre-training tasks model unable recover suitable representation fails extrapolate small number test-time samples. follow experimental protocol proposed vinyals involves fast learning n-way classiﬁcation shots. problem n-way classiﬁcation follows select unseen classes provide model different instances classes evaluate model’s ability classify instances within classes. omniglot randomly select characters training irrespective alphabet remaining testing. omniglot dataset augmented rotations multiples degrees proposed santoro model follows architecture embedding function used vinyals modules convolutions ﬁlters followed batch normalization relu nonlinearity max-pooling. omniglot images downsampled dimensionality last hidden layer baseline classiﬁer used vinyals last layer softmax. omniglot used strided convolutions instead max-pooling. miniimagenet used ﬁlters layer reduce overﬁtting done order also provide fair comparison memory-augmented neural networks test ﬂexibility maml also provide results non-convolutional network. this network hidden layers sizes including batch normalization relu nonlinearities followed linear layer softmax. models loss function cross-entropy error predicted true class. additional hyperparameter details included appendix present results table convolutional model learned maml compares well state-of-the-art results task narrowly outperforming prior methods. existing methods matching networks siamese networks memory models designed few-shot classiﬁcation mind readily applicable domains reinforcement learning. additionally model learned maml uses figure quantitative sinusoid regression results showing learning curve meta test-time. note maml continues improve additional gradient steps without overﬁtting extremely small dataset meta-testing achieving loss substantially lower baseline ﬁne-tuning approach. model trained maml still infer amplitude phase half range demonstrating maml trained model learned model periodic nature sine wave. furthermore observe qualitative quantitative results model learned maml continues improve additional gradient steps despite trained maximal performance gradient step. improvement suggests maml optimizes parameters region amenable fast adaptation sensitive loss functions discussed section rather overﬁtting parameters improve step. evaluate maml comparison prior meta-learning few-shot learning algorithms applied method few-shot image recognition omniglot miniimagenet datasets. omniglot dataset consists instances characters different alphabets. instance drawn different person. miniimagenet dataset proposed ravi larochelle involves training classes validation classes test classes. omniglot miniimagenet image recognition tasks common recently used few-shot learning benchmarks table few-shot classiﬁcation held-out omniglot characters miniimagenet test maml achieves results comparable outperform state-of-the-art convolutional recurrent models. siamese nets matching nets memory module approaches speciﬁc classiﬁcation directly applicable regression scenarios. shows conﬁdence intervals tasks. note omniglot results strictly comparable since train/test splits used prior work available. miniimagenet evaluation baseline methods matching networks ravi larochelle fewer overall parameters compared matching networks meta-learner lstm since algorithm introduce additional parameters beyond weights classiﬁer itself. compared prior methods memory-augmented neural networks speciﬁcally recurrent meta-learning models general represent broadly applicable class methods that like maml used tasks reinforcement learning however shown comparison maml signiﬁcantly outperforms memory-augmented networks meta-learner lstm -way omniglot miniimagenet classiﬁcation -shot -shot case. signiﬁcant computational expense maml comes second derivatives backpropagating meta-gradient gradient operator meta-objective miniimagenet show comparison ﬁrst-order approximation maml second derivatives omitted. note resulting method still computes meta-gradient post-update parameter values provides effective meta-learning. surprisingly however performance method nearly obtained full second derivatives suggesting improvement maml comes gradients objective post-update parameter values rather second order updates differentiating gradient update. past work observed relu neural networks locally almost linear suggests second derivatives close zero cases partially explaining good performance ﬁrst-order approximation. approximation removes need computing hessian-vector products additional backward pass found roughly speed-up network computation. evaluate maml reinforcement learning problems constructed several sets tasks based simulated continuous control environments rllab benchmark suite discuss individual domains below. domains model trained maml neural network policy hidden layers size relu nonlinearities. gradient updates computed using vanilla policy gradient trust-region policy optimization meta-optimizer order avoid computing third derivatives negative absolute value current velocity agent goal chosen uniformly random cheetah ant. goal direction experiments reward magnitude velocity either forward backward direction chosen random task horizon rollouts gradient step problems except forward/backward task used rollouts step. results figure show maml learns model quickly adapt velocity direction even single gradient update continues improve gradient steps. results also show that challenging tasks maml initialization substantially outperforms random initialization pretraining. fact pretraining cases worse random initialization fact observed prior work figure reinforcement learning results half-cheetah locomotion tasks tasks shown right. gradient step requires additional samples environment unlike supervised learning tasks. results show maml adapt goal velocities directions substantially faster conventional pretraining random initialization achieving good performs three gradient steps. exclude goal velocity random baseline curves since returns much worse ﬁnite differences compute hessian-vector products trpo. learning meta-learning updates standard linear feature baseline proposed duan ﬁtted separately iteration sampled task batch. compare three baseline models pretraining policy tasks ﬁne-tuning training policy randomly initialized weights oracle policy receives parameters task input tasks corresponds goal position goal direction goal velocity agent. baseline models ﬁne-tuned gradient descent manually tuned step size. videos learned policies viewed sites.google.com/view/maml navigation. ﬁrst meta-rl experiment study tasks point agent must move different goal positions randomly chosen task within unit square. observation current position actions correspond velocity commands clipped range reward negative squared distance goal episodes terminate agent within goal horizon policy trained maml maximize performance policy gradient update using trajectories. additional hyperparameter settings problem following problems appendix evaluation compare adaptation task gradient updates samples. results figure show adaptation performance models initialized maml conventional pretraining tasks random initialization oracle policy receives goal position input. results show maml learn model adapts much quickly single gradient update furthermore continues improve additional updates. locomotion. study well maml scale complex deep problems also study adaptation high-dimensional locomotion tasks mujoco simulator tasks require simulated robots planar cheetah quadruped particular direction particular velocity. goal velocity experiments reward introduced meta-learning method based learning easily adaptable model parameters gradient descent. approach number beneﬁts. simple introduce learned parameters metalearning. combined model representation amenable gradient-based training differentiable objective including classiﬁcation regression reinforcement learning. lastly since method merely produces weight initialization adaptation performed amount data number gradient steps though demonstrate state-of-the-art results classiﬁcation examples class. also show method adapt agent using policy gradients modest amount experience. reusing knowledge past tasks crucial ingredient making high-capacity scalable models deep neural networks amenable fast training small datasets. believe work step toward simple general-purpose meta-learning technique applied problem model. research area make multitask initialization standard ingredient deep learning reinforcement learning. authors would like thank chen trevor darrell helpful discussions duan alex technical advice nikhil mishra haoran tang greg kahn feedback early draft paper anonymous reviewers comments. work supported part pecase award grfp award. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. neural information processing systems bengio yoshua bengio samy cloutier jocelyn. learning synaptic learning rule. universit´e montr´eal d´epartement d’informatique recherche op´erationnelle donahue jeff yangqing vinyals oriol hoffman judy zhang ning tzeng eric darrell trevor. decaf deep convolutional activation feature generic visual recognition. international conference machine learning duan chen houthooft rein schulman john abbeel pieter. benchmarking deep reinforcement international conlearning continuous control. ference machine learning duan schulman john chen bartlett peter sutskever ilya abbeel pieter. fast reinforcement learning slow reinforcement learning. arxiv preprint arxiv. hochreiter sepp younger steven conwell peter learning learn using gradient descent. international conference artiﬁcial neural networks. springer husken michael goerick christian. fast learning problem classes using knowledge based network initialneural networks ijcnn proization. ceedings ieee-inns-enns international joint conference volume ieee ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal international conference machine covariate shift. learning kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalho tiago grabskabarwinska agnieszka overcoming catastrophic forgetting neural networks. arxiv preprint arxiv. kr¨ahenb¨uhl philipp doersch carl donahue jeff darrell trevor. data-dependent initializations convolutional neural networks. international conference learning representations maclaurin dougal duvenaud david adams ryan. gradient-based hyperparameter optimization reinternational conference maversible learning. chine learning wang jane kurth-nelson tirumala dhruva soyer hubert leibo joel munos remi blundell charles kumaran dharshan botvinick matt. learning reinforcement learn. arxiv preprint arxiv. parisotto emilio jimmy salakhutdinov ruslan. actor-mimic deep multitask transfer reinforceinternational conference learning ment learning. representations rezende danilo jimenez mohamed shakir danihelka gregor karol wierstra daan. one-shot generalization deep generative models. international conference machine learning salimans kingma diederik weight normalization simple reparameterization accelerate training deep neural networks. neural information processing systems santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy. meta-learning internamemory-augmented neural networks. tional conference machine learning saxe andrew mcclelland james ganguli surya. exact solutions nonlinear dynamics learning international conference deep linear neural networks. learning representations schmidhuber jurgen. evolutionary principles selfreferential learning. learning learn meta-meta-... hook.) diploma thesis institut informatik tech. univ. munich schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. international conference machine learning n-way k-shot classiﬁcation gradient computed using batch size examples. omniglot -way convolutional non-convolutional maml models trained gradient step step size meta batch-size tasks. network evaluated using gradient steps step size -way convolutional maml model trained evaluated gradient steps step size training meta batch-size tasks. miniimagenet models trained using gradient steps size evaluated using gradient steps test time. following ravi larochelle examples class used evaluating post-update meta-gradient. used meta batch-size tasks -shot -shot training respectively. models trained iterations single nvidia pascal titan gpu. reinforcement learning experiments maml policy trained using single gradient step evaluation found halving learning rate ﬁrst gradient step produced superior performance. thus step size adaptation ﬁrst step future steps. step sizes baseline methods manually tuned domain. navigation used meta batch size locomotion problems used meta batch size tasks. maml models trained meta-iterations model best average return training used evaluation. goal velocity task added positive reward bonus timestep prevent ending episode. figure show full quantitative results maml model trained -shot learning evaluated -shot -shot -shot. figure show qualitative performance maml pretrained baseline randomly sampled sinusoids. pretraining baseline main text trained single network tasks referred pretraining tasks. evaluate model maml ﬁne-tuned model test task using examples. domains study different tasks involve different output values input. result pre-training tasks model would learn output average output particular input value. instances model learn little actual domain instead learn range output space. experimented multi-task method provide point comparison instead averaging output space averaged parameter space. achieve averaging parameter space sequentially trained separate models tasks drawn model initialized randomly trained large amount data assigned task. took average parameter vector across models ﬁne-tuned datapoints tuned step size. experiments method sinusoid task computational requirements. error individual regressors less respective sine waves. tried three variants set-up. training individual regressors tried using following regularization standard weight decay weight regularization mean parameter vector thus trained regressors. latter variants encourage individual models parsimonious solutions. using regularization magnitude regularization high possible without significantly deterring performance. results refer approach multi-task. seen results table averaging parameter space performed worse averaging output space suggests difﬁcult parsimonious solutions multiple tasks training tasks separately maml learning solution sophisticated mean optimal parameter vector. developed method learns context vector adapted online application recurrent language models. parameters context vector learned adapted parameters maml model. provide comparison using context vector meta-learning problems concatenated free parameters input allowed gradient steps modify rather modifying model parameters maml. imfigure quantitative sinusoid regression results showing test-time learning curves varying numbers test-time samples. gradient step computed using examples. note maml continues improve additional gradient steps without overﬁtting extremely small dataset meta-testing achieves loss substantially lower baseline ﬁne-tuning approach. table additional multi-task baselines sinusoid regression domain showing -shot mean squared error. results suggest maml learning solution sophisticated mean optimal parameter vector. image. method omniglot domains following experimental protocol. report results tables learning adaptable context vector performed well pointmass problem sub-par difﬁcult problems likely less ﬂexible meta-optimization.", "year": 2017}