{"title": "Syntax-based Deep Matching of Short Texts", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Many tasks in natural language processing, ranging from machine translation to question answering, can be reduced to the problem of matching two sentences or more generally two short texts. We propose a new approach to the problem, called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The approach consists of two components, 1) a mining algorithm to discover patterns for matching two short-texts, defined in the product space of dependency trees, and 2) a deep neural network for matching short texts using the mined patterns, as well as a learning algorithm to build the network having a sparse structure. We test our algorithm on the problem of matching a tweet and a response in social media, a hard matching problem proposed in [Wang et al., 2013], and show that DeepMatch$_{tree}$ can outperform a number of competitor models including one without using dependency trees and one based on word-embedding, all with large margins", "text": "limited work thread makes word embedding building blocks matching model. although embedding-based methods proven effective tasks like question answering paraphrase identiﬁcation even short text conversation enough good handling subtlety general short-text matching. short-texts often represent rich content relations also complicated sophisticated structures required comparing shorttexts. example judging appropriateness response rest more. utterance work weekend consider semantic correspondence work weekend need rest more hard captured embedding-based model. study problem short-text matching genmethod named deep match tree eral setting. consists sequentially connected components mining algorithm discover rich subtle patterns deﬁned product space dependency trees large corpus paired short-texts learning algorithm construct deep neural network making matching decision short-texts basis mined patterns. model speciﬁcally trained based contrastive sampling negative examples. without loss generality focus task matching response given tweet weibo popular chinese microblog service large amount data available. hard problem requiring consideration complicated correspondence structures texts. experimental results show deepmatchtree superior existing methods problem. main contributions proposal algorithm mining dependency tree matching patterns large scale proposal algorithm learning deep matching model using mined matching patterns empirical validation efﬁcacy efﬁciency proposed method using large scale real datasets. many tasks natural language processing ranging machine translation question answering reduced problem matching sentences generally short texts. propose approach problem called deep match tree general setting. approach consists components mining algorithm discover patterns matching short-texts deﬁned product space dependency trees deep neural network matching short texts using mined patterns well learning algorithm build network sparse structure. test algorithm problem matching tweet response social media hard matching problem proposed show deepmatchtree outperform number competitor models including without using dependency trees based word-embedding large margins. introduction matching central importance natural language processing. fact many problems natural language processing formalized matching shorttexts different matching relations different applications. example paraphrase identiﬁcation relation synonymy information retrieval relevance. meantime matching also challenging problem since requires modeling short-texts well relation. machine translation example model needs determine whether sentence source language meaning sentence target language. dialogue model needs judge whether message appropriate response given utterance. dependency trees them propose treating subgraphs product graph matching patterns. dependency tree represent sentence dependency tree. choose dependency tree tends expose skeleton sentence revealing short-distance longdistance grammatical relations words example dependency tree fig. contains structures like na←win→championship} represented sub-treewhere words necessarily adjacent sentence. gx×y example sentences worked night good rest direct product trees given right panel fig.. note gx×y general graph even though trees. gx×y directly describes interaction relation sentences hosting rather rich structures lexical syntactic contribute overall matching sentences. next make abstraction representation. abstraction consider types abstraction vertices gx×y entity replace vertex gx×y representing entity general vertex sameentity. example sentences vertex abstraction treated vertex type abstraction. graph type abstraction named similar word conduct clustering words based wordvectors using k-means algorithm. vertex product graph belong word cluster vertex replaced vertex simwordk. graph type abstraction named sub-graphs matching patterns little abuse notation ¯gx×y {gx×y denote sentence pair well variants types abstraction. sentence pair sub-graph corresponding ¯gx×y describes part interaction sentences therefore contribute matching two. instance sameentity sub-graph describing matching sentences conversation weather general ¯gx×y contains meaningful matching patterns task. mining matching patterns responsibility mining algorithm discover sub-graphs ¯gx×y work matching patterns discriminate matched sentence pairs mismatched ones measured terms discriminative ability discriminative roughly means gives evidence matching i.e. appears matched pairs frequently unmatched pairs. efﬁcient mining algorithm vital success method number instances order number mined patterns order speeding-up mining process fortunately leverage following fact respect sub-graphs gx×y gx×y proposition connected sub-graph uniquely determine minimal sub-tree minimal sub-tree whose direct product cover implies mining sub-graphs trees reduced jointly selecting sub-trees sides. greatly speed mining process also avoid ﬁnding patterns duplicate functionality matching. remainder paper sub-tree sub-tree denote tree-pair mined pog. however apply general case ¯gx×y vertices replaced non-factorable variants like simword introduce tricks. mining without abstraction algorithm mining without abstraction sketched algorithm recursively grow mined sub-graphs maintaining discriminative ability. starts simplest pattern standing one-word tree side side grows mined trees recursively. growing step rightextend) size sub-trees increased either side side followed ﬁltering step remove found pairs discriminative ability less threshold. growing step efﬁcient since limit search patterns candidates formed merging patterns practice time looking-up sub-tree pair almost constant help hashmap. following table gives examples matching patterns discovered algorithm algorithm discriminative mining parse trees parallel texts input tree pairs original maxsize output mined features initinalize enqueue foreach node patterns without abstraction exam score information theory shannon thank→present happy→birthday win→game trying→keep out-of-control→prices regulation work→weekend rest mining abstraction algorithm mining abstraction variant algorithm taking sameentity abstraction example ﬁrst replace named entity e.g. vertex growing step algorithm except counting support pattern replaces entity appearing sides wildcard therefore groups many patterns one. example instances following patterns congratulations nadal nadal congratulations following table gives examples matching patterns discovered algorithm graph abstraction. stand wildcards considered similar enough algorithm. advantage tree pattern mining important note dependency tree matching patterns provide better correspondence sentences word co-occurrences sides illustrate superiority using dependency tree matching patterns suppose tweet fig. want pick appropriate responses word-based model tends assign high matching score pair pattern {beijing travel} {great wall} however spurious since traveling york word beijing distractor. hand tree-based model relies patterns like follows discriminates word co-occurrence dependency-tree pattern gives higher score mining algorithm allows patterns representing deep long-distance relationship within short-texts matched. deep features therefore provide sophisticated matching structures texts. contrast shallow features give word-level correspondences words texts. difference analogous syntax-based translation model word-based translation model model description diagram deep matching model given fig.. pair short-texts given ﬁrst obtain dependency trees form direct product them perform abstraction that look table dependency tree matching patterns convert input text-pair binary vector element corresponding pattern apply input textpair otherwise zero. binary vector mdimension sparse typically ones experiments deep neural network ﬁnal match decision. architecture learning since features number parameters large input layer fully connected ﬁrst hidden layer reasonable size therefore necessary specify sensible sparse patterns ensure information features well abstracted ﬁrst hidden layer. believed neural networks suited dense continuous input little work building appropriate architecture sparse discrete input demanding size. work take simple procedure ensure input node connected approximately hidden nodes average activations hidden nodes approximately same. underlying belief preserve much information possible going sparse patterns dense representation. selection overall architecture overall architecture neural network illustrated fig.. shows units ﬁrst hidden layer second hidden layer third hidden layer output layer. empirical results show architecture performs slightly better -layer approximately number parameters retrieval-based conversation second dataset denoted datalabeled consists tweets around labeled responses tweet introduced retrieval-based conversation. datalabeled test different matching models enhance performance retrieval-based conversation model ﬁnding suitable response given tweet. rather hard since negative responses topically related tweet. retrieval strategy individually adding scores matching models feature ranking function rank retrieved responses experiments precision measure accuracy matching. basically given tweet calculate matching scores between candidate responses select highest score. ranking gets point selected original labeled good measures chance getting selection right averaged tweets test set. competitor methods cossim simply calculate cosine similarity between short-texts tf-idf representations. method still better random since good response tends share words original tweet; wordembed represent short-text embedding vectors words contains. matching score short-texts calculated using multi-layer perceptron concatenation vectors input; lrtree show power mined patterns also train logistic regression model taking mined patterns input contrastive sampling training strategy. viewed shallow version deepmatchtree. methods roughly categorized patternbased methods embedding-based methods embeddingbased methods represent word vector based ﬁnal matching decision made. parameter learning employ discriminative training strategy large margin objective. suppose given following triples oracle matched better following pairwise loss objective training generic back-propagation algorithm adapted sparse patterns ﬁrst layer. speciﬁcally updating weights ﬁrst layer update weights associated active nodes input layer faithfully respects back-prop makes learning efﬁciently enough even training millions instances. easy number parameters greater number positive instances thus kind regularization needed. consider employing dropout early stopping turns important success model especially number parameters original-vs-random ﬁrst dataset denoted dataorignal consists million pairs. positive pair randomly select responses negative examples rendering million triples. evaluation shows given tweet chance randomly selected response suitable. results conversation data model -fold cross validation choose hyper-parameter ranking model ranksvm report best result. clearly deepmatchtree greatly improve performances retrieving suitable response pool signiﬁcantly better accuracies competitor models. result consistent result original-vs-random despite difference experimental setting. deep shallow patterns deep patterns represent information cannot adequately modeled shallow patterns deep neural network. indeed study shows original-vs-random data decreases removing deep features. real case experiment. observation interesting since feature learning previously often taken partially responsibility deep learning. effect abstraction abstraction step helps improve generalization ability matching model improving original-vs-random also illustrated following real example experiment section present results orig nal-vs-random setting. model report best performance test data since large size test data removes chance accidental cheating. ﬁrst study architecture variations deepmatchtree compare best setting competitors. compare performances deepmatchtree different settings specially number hidden layers nodedensity architecture learning nutshell performance peaks around nodedensity= architecture learning. nodedensity≥ matching model parameters needs regularization prevent overﬁtting addition early stopping. inﬂuence architecture learning salient relatively large nodedensity number hidden layers stops bringing signiﬁcant improvement generally found architectures deeper larger current bring signiﬁcant improvement much slower. comparison competitor models table compares deepmatchtree competitor models. shows model outperforms competitor models large margins. contribution deep architectures manifested differences deep architectures shallow ones mined patterns. vast pattern-based models embedding-based models. although embedding-based methods perform fairly well versus setting performance drops dramatically versus nine settings pattern-based methods maintain test setting. contrast suggests pattern-based methods varying coverage feature space certain matched positive cases negative cases yielding reliable ranking results. deep matching models works using deep neural networks matching task build upon given learned representations objects. model directly mine learn representations matching. graph-based kernel deepmatchtree extends important notions conventional graph kernels senses. first model allows matching different subgraphs domains graph kernels consider common subgraphs sides. second model captures nonlinear hierarchical relations different matching patterns graph kernels simply together different weights determined types sub-graphs. string-rewriting kernel deepmatchtree also related string-rewriting kernel paraphrase identiﬁcation also generates many patterns matching learns weigh training. main difference matching patterns considered exhaustively enumerated discovered mining algorithm. conclusion propose generic model matching short-texts relies tree-mining algorithm discover vast amount matching patterns perform task using patterns. empirical study rather difﬁcult task tweet response matching shows model outperform competitor large margins. acknowledge work supported part china national project liu’s work partially supported science foundation ireland part adapt centre dublin city university. peter brown vincent della pietra stephen della pietra robert mercer. mathematics statistical machine translation parameter estimation. computational linguistics rich caruana steve lawrence giles. overﬁtting neural nets backpropagation conjugate gradient early stopping. advances neural information processing systems pages zhang hong cheng jing xifeng jiawei philip olivier verscheure. direct mining discriminative essential frequent patterns proceedings model-based search tree. sigkdd international conference knowledge discovery data mining pages katja filippova michael strube. proceedings dependency tree based sentence compression. fifth international natural language generation conference inlg pages stroudsburg association computational linguistics. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. corr abs/. baotian zhengdong hang qingcai chen. convolutional neural network architectures matching natural language sentences. ghahramani welling cortes n.d. lawrence k.q. weinberger editors advances neural information processing systems pages curran associates inc. po-sen huang xiaodong jianfeng deng alex acero larry heck. learning deep structured semantic models search using clickthrough data. proceedings international conference conference information knowledge management pages quoc building high-level features using large acoustics speech signal scale unsupervised learning. processing ieee international conference pages ieee richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems xiaogang wang xiaoou tang. hyieee internabrid deep learning face veriﬁcation. tional conference computer vision december vishwanathan nicol schraudolph risi kondor karsten borgwardt. graph kernels. mach. learn. res. wang zhengdong hang enhong chen. dataset research short-text conversations. proceedings conference empirical methods natural language processing pages seattle washington october association computational linguistics.", "year": 2015}