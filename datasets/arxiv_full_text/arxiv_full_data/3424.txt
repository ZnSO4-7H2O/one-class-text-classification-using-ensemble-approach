{"title": "Cross-modal Deep Metric Learning with Multi-task Regularization", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.", "text": "text. existing methods mainly project data different modalities common space shared representations them. cross-modal similarity directly measured distance computing. example canonical correlation analysis learn common space maximizing correlation data modalities. many cca-based methods crossmodal factor analysis approach aims minimize frobenius norm pairwise data common space. besides zhai propose joint representation learning model pairwise correlation semantic information jointly uniﬁed graph-based framework. kang propose local group based consistent feature learning adopts local group-based priori learn basis matrices different modalities. propose ﬁrst build semantic hierarchy content ontology similarities learn local linear projections probabilistic membership functions local expert aggregation. however methods mostly perform shared representation projection linear functions insufﬁcient complex cross-modal correlation. inspired improvement deep neural network single-modal retrieval researchers also attempted apply cross-modal similarity measure. instance input different modalities converted shared representations shared code layer also cross-modal deep architectures consisting linked deep encodings deep corr-ae methods mainly focus pairwise correlation reconstruction error labeled multimodal data. however ignore semantically similar dissimilar constraints different modalities provide similarity ranking information better semantically discriminative ability. unlabeled data also taken consideration increase diversity training data boost accuracy shared representation learning. addressing problems paper proposes cross-modal deep metric learning multi-task regularization integrates quadruplet ranking loss semi-supervised contrastive loss modeling crossmodal semantic similarity uniﬁed metric learning ardnn-based cross-modal retrieval become research hotspot users search results across various modalities like image text. however existing methods mainly focus pairwise correlation reconstruction error labeled data. ignore semantically similar dissimilar constraints different modalities cannot take advantage unlabeled data. paper proposes cross-modal deep metric learning multi-task regularization integrates quadruplet ranking loss semi-supervised contrastive loss modeling crossmodal semantic similarity uniﬁed multi-task learning architecture. quadruplet ranking loss model semantically similar dissimilar constraints preserve crossmodal relative similarity ranking information. semisupervised contrastive loss able maximize semantic similarity labeled unlabeled data. compared existing methods cdmlmr exploits similarity ranking information also unlabeled cross-modal data thus boosts cross-modal retrieval accuracy. nowadays multimedia retrieval increasingly important data management utilization research hotspot long time. however existing methods single-modal retrieval measure similarity data single modality. different modalities different views semantics. image ﬂying bird text description bird semantic bird. describe semantics different views similar semantic level. modeling similarities among different modalities important better understanding multimedia data also multimedia applications internet. chitecture. hand triplet network proposed single-modal metric learning model relative similarity images. cdmlmr extends single-modal triplet network cross-modal quadruplet network preserve relative similarity ranking information different modalities. hand semi-supervised contrastive loss preserve similarity information labeled unlabeled data maximizing semantic correlation online graph construction strategy. loss functions integrated uniﬁed multi-task network optimized simultaneously inspired cdmlmr capture fully useful intrinsic hints cross-modal similarity measure improve retrieval accuracy. experiment results show cdmlmr achieves better performance comparing state-of-the-art methods datasets wikipedia nus-wide-k perform cross-modal deep metric learning base network deep belief network bimodal autoencoders ﬁrst used feature different modalities shallow crossmodal shared representations. shown figure cdmlmr model integrates loss functions modeling cross-modal semantic similarity uniﬁed optimization deep network semi-supervised contrastive loss quadruplet ranking loss. network trained simultaneous optimization based loss functions ﬁnal semantically discriminative shared representations. number labeled image noted data dimensional number image feature unlabeled image data denoted total number image data. text data represented similar image data. noted p-th image text i.e. base network convert data different modalities representations dimensional number shallow shared representations used input deep metric learning network. ﬁrst employ separate twolayer model modality. model distribution gaussian restricted boltzimage feature mann machine used undirected graphical model visible units connected hidden units text feature replicated softmax used model distribution them. probability model assigns input feature deﬁned follows outputs denoted bimodal shallow shared representations dimension. bimodal ability reconstruct modalities minimizing reconstruction error input reconstruction representations reconstruction layers. shallow shared shown figure cdmlmr model pathways three fully-connected layers modality separately taking shallow shared representations obtained base network input. pathways network multiple loss branches embedded fully-connected layer using sigmoid nonlinearity integrates semi-supervised quadruplet ranking regularization uniﬁed optimization deep network. image batch data iteration consist labeled images unlabeled images total number image data mini-batch labeled images. similarly outputs pathways separately denoted denotes image mapping denotes text mapping. semi-supervised contrastive loss cdmlmr semi-supervised contrastive loss proposed preserve similarity information labeled unlabeled data. basic idea similar image/text pairs similar shared representations vice versa. pair data similar close shallow shared representation space semantic class. modeling semi-supervised information neighborhood graph constructed vertices represent image text data edges represent cross-modal similarity matrix image text data denoted labeled image/text pairs unlabeled image/text pairs. labeled image labeled text similarity matrix constructed based labels follows unlabeled image/text pairs mean least image text data pair unlabeled analyze k-nearest-neighbors image instead constructing graph ofﬂine data much time-consuming online graph construction strategy proposed generate cross-modal similarity matrix unlabeled image/text pairs within mini-batch. similarity matrix unlabeled image/text pairs deﬁned follows input image text data obtained base network respectively margin parameter. combining loss functions ﬁnally semi-supervised contrastive loss function follows balancing number similar dissimilar pairs randomly select similar pair dissimilar pair image training. minimizing loss function preserve similarity information labeled unlabeled data. quadruplet ranking loss inspired idea preserving relative similarity triplet network cross-modal quadruplet ranking loss designed further modeling cross-modal relative similarity sample layer generate quadruplet samples output separate two-pathway network. quadruplet samples organized form satisfy following relative similarity constraints text sample similar image sample image sample si−. image sample similar text sample text sample st−. similarity according data labels quadruplet samples generated labeled data mini-batch. based this quadruplet ranking loss function deﬁned follows margin parameter. capturing between-class within-class differences different modalities quadruplet network effectively preserve cross-modal relative similarity improve semantic discriminative ability shared representations boost retrieval accuracy. cdmlmr involves loss functions quadruplet ranking loss semi-supervised contrastive loss. first calculate derivative loss functions separately. semi-supervised contrastive loss derivative loss function calculated image text follows opposite. margin parameter. moreover easily calculate derivative similar calculate derivative thus backpropagation applied update parameters network. gradients follows calculating derivative loss functions gradients modality fully-connected layers loss branch summed together fully-connected layers proposed pathways network parameter updating. network parameters need adjusted according input dimensions. present layer parameters designed wikipedia dataset introduced experiment section. base network two-layer image input hidden units ﬁrst layer second layer hidden units. text input two-layer hidden units layers. three-layer feedforward neural network softmax layer adopted optimization dimensional number layer. bimodal input layer reconstruction layer number dimension dimensional number middle layer half input. also softmax layer connected middle layer optimization. two-pathway network figure three fully-connected layers dimensional number dimension fully-connected layers sigmoid nonlinearity loss branch also generality output dimensions datasets according retrieval accuracy validation wikipedia dataset. networks trained base learning rate stochastic gradient descent momentum weight decay parameter network easy train converges less steps experiment. introduce datasets brieﬂy follows. fair comparison dataset partition feature extraction strictly noted experiment unlabeled data test ratio labeled/unlabeled data according ratio training/test data. wikipedia dataset based wikipedia’s feature articles wikipedia dataset widely used dataset cross-modal retrieval consists documents categories document image/text pair. experiment following dataset split parts documents training documents testing documents validation set. image feature catenation phow descriptor gist descriptor mpeg- descriptor. text feature representation vector. nus-wide-k dataset nus-wide dataset consists images tags them categorized classes. nus-wide-k subset nus-wide dataset image/text pairs largest classes dataset also randomly split parts documents training documents testing documents validation set. take catenation image feature color histogram color correlogram edge direction histogram wavelet texture block-wise color moments siftbased bovw features. texts represented vector. retrieval tasks conducted retrieving text image query retrieving image text query image test used retrieve text test vice versa. state-of-the-art cross-modal retrieval methods used comparison kcca bimodal multimodal corr-ae cmdn obtaining cross-modal shared representations cdmlmr compared methods ranking list cosine distance adopt mean average precision score evaluation metric results. table show scores datasets results. wikipedia nuswide datasets cdmlmr achieves inspiring improvement compared state-of-the-art methods image→text text→image tasks. general kcca shows clear advantage non-linearity achieves hight accuracy methods without dnn. dnn-based methods cmdn best performance four dnn-based compared methods models inter-modal intra-modal information simultaneously. shown results cdmlmr method measure cross-modal similarity effectively. compared existing methods cdmlmr fully capture useful intrinsic hints cross-modal similarity metric exploiting similarity ranking information crossmodal quadruplets make full unlabeled data increase diversity training data. thus learn semantically discriminative shared representations boost cross-modal retrieval accuracy. table shows experiments cdmlmr method base network baselines. compared three baselines base means directly perform retrieval output base network; semi means semi-supervised contrastive loss; quad means quadruplet ranking loss. page limitation show score results here. seen cdmlmr clearly improves cross-modal retrieval accuracy shows similarity ranking information cross-modal quadruplets rich cross-modal unlabeled instances provide useful hints cross-modal similarity learning effectively modeled cdmlmr model uniﬁed framework. paper proposed cdmlmr modal integrates quadruplet ranking loss semi-supervised contrastive loss modeling cross-modal semantic similarity uniﬁed multi-task learning architecture. compared existing methods cdmlmr exploit similarity ranking information cross-modal quadruplets learn semantically discriminative shared representations also make full unlabeled data increase diversity training data improve retrieval accuracy. future still focus cross-modal deep metric learning modeling modalities simultaneously. jose costa pereira emanuele coviello gabriel doyle gert lanckriet roger levy nuno vasconcelos approach crossmodal multimedia retrieval international conference multimedia yunchao gong qifa michael isard svetlana lazebnik multi-view embedding space modeling internet images tags semantics international journal computer vision vol. xiaohua zhai yuxin peng jianguo xiao learning cross-media joint representation sparse semi-supervised regularization ieee transactions circuits systems video technology vol. yuxin peng huang jinwei cross-media shared representation hierarchical learning multiple deep networks international joint conference artiﬁcial intelligence jiang wang yang song thomas leung chuck rosenberg jingbin wang james philbin chen ying learning ﬁne-grained image similarity deep ranking conference computer vision pattern recognition shaoqing kaiming ross girshick jian faster r-cnn towards real-time object detection region proposal networks conference neural information processing systems cuicui kang shiming xiang shengcai liao changsheng chunhong learning consistent feature representation cross-modal multimedia retrieval ieee transactions multimedia vol. tat-seng chua jinhui tang richang hong haojie zhiping yantao zheng nus-wide realworld image database national university singapore international conference image video retrieval shuhui wang siyuan anni cross-modal correlation learning qingming huang adaptive hierarchical semantic aggregation ieee transactions multimedia vol.", "year": 2017}