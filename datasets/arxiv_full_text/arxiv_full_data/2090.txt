{"title": "Factored Contextual Policy Search with Bayesian Optimization", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different \"contexts\". Bayesian optimization approaches to contextual policy search (CPS) offer data-efficient policy learning that generalize over a context space. We propose to improve data- efficiency by factoring typically considered contexts into two components: target- type contexts that correspond to a desired outcome of the learned behavior, e.g. target position for throwing a ball; and environment type contexts that correspond to some state of the environment, e.g. initial ball position or wind speed. Our key observation is that experience can be directly generalized over target-type contexts. Based on that we introduce Factored Contextual Policy Search with Bayesian Optimization for both passive and active learning settings. Preliminary results show faster policy generalization on a simulated toy problem.", "text": "scarce data major challenge scaling robot learning truly complex tasks need generalize locally learned policies different \"contexts\". bayesian optimization approaches contextual policy search offer data-efﬁcient policy learning generalize context space. propose improve dataefﬁciency factoring typically considered contexts components targettype contexts correspond desired outcome learned behavior e.g. target position throwing ball; environment type contexts correspond state environment e.g. initial ball position wind speed. observation experience directly generalized target-type contexts. based introduce factored contextual policy search bayesian optimization passive active learning settings. preliminary results show faster policy generalization simulated problem. robots artiﬁcial agents complex environments reinforcement learning commonly used learn desired behaviors interacting environment. however limited main factors learned policies difﬁcult generalize different problem settings; require large amount data often expensive obtain various costs interaction environment. order perform truly complex tasks need learning algorithms generalize well context space little data. factorization generally used computer science decompose problem structures components easier address. introduce factorization contexts reduces data requirements class learning tasks context space exhibits speciﬁc properties. focus contextual policy search problem formulation concept context introduced generalize task settings. speciﬁcally bayesian optimization approach based previous work metzen assumption approaches optimal policy parameters similar similar contexts. observe however possible directly generalize experience policy parameters speciﬁc context types ultimately reduces data requirements. problem agent learning upper level policy deﬁnes distribution parameter vectors lower-level policy context executing lower level policy parameters context results observed trajectory reward context encode types task parameters insight work single trial provides useful information target-type contexts. consider robot learning throw balls different targets. first asked target chooses according upper level policy execute throw observes ball hits target even trial poor results reward agent gains valuable information future asked yield high reward. previous work relies reward values assumed correlation generalize contexts. proposed factorization enables trial evaluated target-type contexts leading faster generalization. introduce factored contextual policy search bayesian optimization variant bo-cps factored contexts. also extend fcps-bo active learning setting based aces active learning agent choose context episode training unlike standard formulation context given. number approaches generalize discrete continuous context space. group work interpolates policy parameters contexts given number local policies approach limited problems local policies available easy learn inefﬁcient learning local policies challenging themselves. second group work jointly learns local policies generalizes context space. rewardweighted regression kernelized variant cost-regularized regression linear function represent upper-level policy perform updates weighted linear regression. contextual relative entropy policy search represents upper-level policy probability distribution lower-level policy parameters conditioned context updates performed using information theoretic insights. crkr c-reps applied variety real-world tasks playing table tennis throwing darts playing hockey although tasks involve target-type contexts leveraged evaluate previous experience query contexts. recently metzen introduced bo-cps aces bayesian optimization approach passive active learning settings respectively. directly learn gaussian process model expected reward given parameter vector context methods offer high data-efﬁciency handle noisy observations limited dimensional parameter spaces evaluated real world learning tasks. method extends approaches concept factored contexts. best knowledge work explicitly factors context space. similar ideas implicitly used kober apply crkr learn contextual policy discrete targets performing higher-level task experience gained context another; however purpose estimating discrete outcome probabilities improving policy. kupcsik introduced gp-reps model-based policy search approach iteratively learns transition model system using prediction used generate trajectories ofﬂine updating policy. authors discuss option generating additional samples artiﬁcial contexts similar idea mapping method -but deﬁne explicit factorization generate multiple samples observed trajectory rather explicitly mapping query context. note argument hold environment-type contexts wind speed imperfect throw wind speed tell anything optimal parameters wind speed apart assumed correlation. bayesian optimization approach trains directly policy parameters contexts expected reward given previous experience ri}. posterior estimate reward uncertainty episode agent given query context needs choose appropriate parameter vector acquisition function order leverage factored contexts maintain dataset observed executing policy parameters context trajectories instead rewards. query stage given experience light current target-type query context trajectories target context query point. form dataset speciﬁc query context. optimal parameters context found optimizing prediction given data evaluating previous experience current query directly generalize target-type contexts instead solely relying assumed correlation case environment-type contexts. active learning setting agent chooses parameters context next trial. optimizing gp-ucb would lead good result achievable rewards different contexts different. instead follow aces entropy search based acquisition function aims choose informative query points global optimization addition mapping contexts parameters expected rewards aces maintains explicit probability distribution popt probability global optimal context value popt approximated heuristically chosen representer points using monte carlo integration. aces predicts popt distribution hypothetical query θq}. need predict posterior query. posterior approximated ﬁrst sampling current posterior query point adding sample training data. informative query point chosen maximizing change relative entropy popt integrated context space. speciﬁcally deﬁne loss function query point context practical implementation large observed store mapping trajectory sufﬁcient computing reward arbitrary target-type contexts. e.g. ball throwing task need store coordinates ball rather full trajectory. integrate factored contexts follows. iteration previous experience representer points context space resulting models ..n. loss function evaluated predict change popt using corresponding gpi. directly generalize target-type context space similarly passive learning case. note choice target-type query context indifferent ignore rewards training thus need select fcps-bo simulated cannon problem evaluating previous trials targets trivially beneﬁcial. task inspired similar nature ball throwing task cannon placed center coordinate system shoot targets ground range contextual policy maps targets launch parameters horizontal orientation vertical angle velocity reward negative quadratic function distance target projectile hits plus terms penalizing larger launch speeds vertical angles. increase difﬁculty problem randomly place hills environment. learning agent unaware hills target coordinates carry information target elevation. furthermore gaussian noise desired launch angle training. note evaluating previous trials given context trivially beneﬁcial problem setting. shooting direction opposite target results extremely reward launch parameters achieve high reward another target shooting direction. figure example environment target optimal trajectory. mean rewards using fcps-bo fcps-bo active setting baseline bo-cps shaded area corresponds standard deviations. trials randomly generated environments evaluate policies ofﬂine test targets placed grid factored approach fcps-bo achieves near optimal performance less episodes baseline bo-cps requires episodes. also evaluate fcps-bo active learning setting; however observe beneﬁt passive setting. aggressive approximations used active approach; problem setting difﬁcult learn good policy target another. investigation conducted future. introduced fcps-bo bayesian optimization approach factors context space generalize policies little amount data. present extension fcps-bo active learning setting show beneﬁt factorization problem. addressing scalability higher dimensions evaluation realistic problems remain future work. also plan apply factorization standard policy search methods c-reps gp-reps. kupcsik deisenroth peters vadakkepat neumann. data-efﬁcient generalization robot skills contextual policy search. proceedings twenty-seventh aaai conference artiﬁcial intelligence pages", "year": 2016}