{"title": "Automatic Selection of t-SNE Perplexity", "tag": ["cs.AI", "cs.LG", "stat.AP", "stat.ML"], "abstract": "t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely used dimensionality reduction methods for data visualization, but it has a perplexity hyperparameter that requires manual selection. In practice, proper tuning of t-SNE perplexity requires users to understand the inner working of the method as well as to have hands-on experience. We propose a model selection objective for t-SNE perplexity that requires negligible extra computation beyond that of the t-SNE itself. We empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets. The similarities of our approach to Bayesian information criteria (BIC) and minimum description length (MDL) are also analyzed.", "text": "t-distributed stochastic neighbor embedding widely used dimensionality reduction methods data visualization perplexity hyperparameter requires manual selection. practice proper tuning t-sne perplexity requires users understand inner working method well hands-on experience. propose model selection objective t-sne perplexity requires negligible extra computation beyond t-sne itself. empirically validate perplexity settings found approach consistent preferences elicited human experts across number datasets. similarities approach bayesian information criteria minimum description length also analyzed. keywords t-sne perplexity hyperparameter tuning bayesian information criteria introduction t-distributed stochastic neighbor embedding arguably widely used nonlinear dimensionality reduction method data visualization machine learning data science. using t-sne requires tuning hyperparameters notably perplexity. although according maaten hinton t-sne results robust settings perplexity practice users would still interactively choose perplexity visually comparing results multiple settings. often complete novice users potentially need practice tuning t-sne various simple problems order gain enough insight skills properly lack automation selecting crucial hyperparameter poses diﬃculty expert users understand inner working t-sne algorithm could lead misinterpretation data. work propose approach automatically perplexity requires signiﬁcant extra computation beyond runs t-sne optimization. proposed approach based objective function perplexity resulting divergence learned t-sne. motivate novel objective perspective model selection validate showing minimum agrees human expert selection empirical studies. t-distributed stochastic neighbor embedding t-sne tries preserve local neighborhood structure high dimensional space dimensional space converting pairwise distances pairwise joint distributions optimize dimensional embeddings match high dimensional joint distributions. speciﬁcally {xi}n corresponding dimensional embedding points t-sne deﬁnes joint distribution point follows perplexity includes deﬁnes local scale around value optimized speciﬁed hand individually rather found bisection search match pre-speciﬁed perplexity value perplexity value divergence diﬀerent perplexities cannot compared assess quality embeddings since ﬁnal divergence typically decreases perplexity increases illustrated fig. model selection based divergence alone always lead large however resulting embeddings large usually suboptimal capturing underlying pattern data demonstrated fig. limit equal number data points resulting embeddings usually form gaussian uniform like blob completely fails capture interesting structure. suggests trading ﬁnal divergence could potentially lead good embeddings. based intuition design following criteria corresponding fig. function illustrated fig. automatically perform derivative free optimization respect instance bayesian optimization t-sne takes long time simply grid search computational cost low. implicit proposal t-sne optimal given particular practice poor convergence optimization would aﬀect ﬁnal values hence could potentially impact result automatic tuning. practice however default values sec. demonstrate minimizes agrees selection human users across number datasets. that motivate relating bayesian information criteria minimizing description length. ﬁrst term goodness-of-ﬁt maximum-likelihood-estimated model second term logk controls complexity model penalizing number free parameters scaled log. although formal derivation similar derivation large sample approximation negative marginal likelihood strong parallel terms forms behaviours balancing data-ﬁt complexity. terms analogous complexity changes reversed instead increasing complexity model data better increasing reduces complexity pattern data modelled lower dimensional space embed better. projecting high dimensional spaces enough room lower dimensional space preserve structure high dimension i.e. crowding problem. increases diﬀerences distances among points become less less signiﬁcant respect length scales kernel distribution tend toward uniform. forward form objective function large cost under-estimating probability point over-estimating. words large small divergence term large opposite direction small large aﬀected. increasing leads larger uniform easier student-t distribution dimensional space assign suﬃcient probability mass points. short increases relaxes problem reducing amount structure modelled less error made according pays cost second term result same balancing data-ﬁt complexity model relative data complexity. reason refer pseudo experiments. minimum description length realize occam’s razor principle model selection. recognizes model capturing regularity data compress data accordingly hence reduced description length data description length model plus description length data compressed model. given dimensionality original embedding spaces saving description length ﬁxed need consider extra description length paid encode error minimize average number extra bits required encode samples using code optimized since assumed tsne number unique pairwise probabilities. total number extra bits required. hand need encode extra length costs paid need encode neighborhood membership information. takes −log encode identity data point data point number neighbors average. bits required encode neighbor identities. taking factor validation inferred human experts preferences perplexity validate pbic infer human experts’ hidden utility perplexities learning pairwise preferences t-sne maps diﬀerent perplexities. show selection pbic generally agrees experts consensus. preference elicitation using gaussian process t-sne results precomputed grid perplexities ranging half number data samples users presented randomly selected pairs t-sne results diﬀerent settings. user chooses believe better reveal structures data. users also indicate strength preference scale four discrete choices. user preferences collected gaussian process model pairwise ranking likelihood learn latent utility function collected pairwise preferences. likelihood laplace approximate inference using bayesian framework crucial properly compare pbic result user preferences user preferences uncertain given inherent noise potential lack information insuﬃcient sampling. unlike model diﬀerences across human experts instead pooling selections together. note human expert experiments want avoid introducing complicated sequential biases active sampling preference elicitation rather random sampling ﬁxed grid. select optimal setting using pbic rule practice done eﬃciently bayesian optimization bisection search. experiment results conducted experiments using process described handwritten digits coil- olivetti faces dataset. dataset preferences collected eight people pairs visualizations each. test subjects machine learning practitioners application research level expertise t-sne. divided groups four experts given t-sne maps classes colored four presented without information. classes shown colours additional side information help assess quality embeddings available second group pbic method. fig. shows results automatic selection pbic consensus implied human expert preferences close. match exactly corresponding inferred human utility pbic selection close peak utility diﬀerence statistically signiﬁcant. fig. diﬀerence signiﬁcant lies dashed bounds capture posterior credible region around peak. caption fig. details. coil- datasets contains gray-scale pictures objects. pictures taken rotation angles therefore projected t-sne maps exhibit circlar shapes perplexity selected appropriately. fig. shows optimal perplexity pbic close argmax learned utility function. fig. results diﬀerent setting class label shown users user-preferred twice pbic-picked however later still within conﬁdence bounds ther former inferred utility showing signiﬁcant statistical diﬀerence. olivetti faces dataset proﬁle pictures people. used random subset people test behaviour small dataset. pbic selects optimal perplexity close preferred humans shown fig. proposed simple objective automatically setting perplexity parameter t-sne making accessible novice users well reducing risk mis-interpreting data. motivated objective relating well known approaches model selection demonstrated empericially proposed automated method ﬁnds perplexity settings http//archive.ics.uci.edu/ml/datasets/pen-based+recognition+of+handwritten+digits http//www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html figure inferred perplexity utility functions. three rows correspond three datasets. left column experiments class labels colored; right column without class labels colored; black solid lines posterior mean; shadow region standard deviation posterior variance. optimal perplexity inferred human experts marked cross posterior credible region point marked dash lines. optimal perplexity pbic shown dot. blue dash lines show locations chosen perplexities x-axis.", "year": 2017}