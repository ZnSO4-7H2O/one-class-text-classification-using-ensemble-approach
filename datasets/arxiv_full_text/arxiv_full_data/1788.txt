{"title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language  Modeling", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "text": "recurrent neural networks successful predicting sequences words tasks language modeling. however models based conventional classiﬁcation framework model trained one-hot targets word represented input output isolation. causes inefﬁciencies learning terms utilizing information terms number parameters needed train. introduce novel theoretical framework facilitates better learning language modeling show framework leads tying together input embedding output projection matrices greatly reducing number trainable variables. framework leads state performance penn treebank variety network models. neural network models recently made tremendous progress variety applications speech recognition sentiment analysis text summarization machine translation despite overwhelming success achieved recurrent neural networks modeling long range dependencies words current recurrent neural network language models based conventional classiﬁcation framework major drawbacks first assumed metric output classes whereas evidence suggesting learning improved deﬁne natural metric output space language modeling well established metric space outputs based word embeddings meaningful distances words second classical framework inputs outputs considered isolated entities semantic link them. clearly case language modeling inputs outputs fact live identical spaces. therefore even models moderately sized vocabularies classical framework could vast source inefﬁciency terms number variables model terms utilizing information gathered different parts model work introduce novel loss framework language modeling remedy problems. framework comprised closely linked improvements. first augment classical cross-entropy loss additional term minimizes kl-divergence model’s prediction estimated target distribution based word embeddings space. estimated distribution uses knowledge word vector similarity. theoretically analyze loss leads second synergistic improvement tying together large matrices reusing input word embedding matrix output classiﬁcation matrix. empirically validate theory practical setting much milder assumptions theory. also empirically large networks improvement could achieved reusing word embeddings. test framework performing extensive experiments penn treebank corpus dataset widely used benchmarking language models demonstrate models trained using proposed framework signiﬁcantly outperform models trained using conventional framework. also perform experiments newly introduced wikitext- dataset verify empirical performance proposed framework consistent across different datasets. matrix rdx×|v word embedding matrix word embedding dimension size vocabulary. function represents recurrent neural network takes current input previous hidden state produces next hidden state. |×dh output projection matrix bias respectively size hidden state. dimensional models discrete probability distribution next word. note formulation make assumptions speciﬁcs recurrent neural units could replaced standard recurrent unit gated recurrent unit long-short term memory unit etc. experiments lstm units layers. shall refer model prediction distribution example empirical target distribution since crossentropy kullback-leibler divergence equivalent target distribution one-hot rewrite loss example therefore think optimization conventional loss rnnlm trying minimize distance model prediction distribution empirical target distribution which many training examples close minimizing distance actual target distribution. framework introduce utilize kullback-leibler divergence opposed cross-entropy intuitive interpretation distance distributions although equivalent framework. above hyperparameter adjusted almost identical regular model prediction distribution exception logits divided temperature parameter deﬁne probability distribution estimates true data distribution satisﬁes e˜yt goal framework minimize understand effect optimizing setting let’s focus ideal case given true data distribution augmented loss aug. carry investigation stochastic gradient descent technique dominantly used training neural networks. gradient implication following every time optimizer sees training example takes step account label seen proceeds taking account class labels conditional probability zero relative step size step given conditional probability label ˜yti. furthermore much less noisy update since target distribution exact deterministic. therefore unless examples exclusively belong speciﬁc class probability optimization much differently train greatly improved supervision. idea proposed recent work hinton might considered application framework obtain good ˜y’s training large models using model prediction distributions those. although ﬁnding good general rather nontrivial context language modeling hope achieve exploiting inherent metric space classes encoded model namely space word embeddings. speciﬁcally propose following words ﬁrst target word vector corresponds target word token take inner product target word vector word vectors unnormalized probability distribution. adjust temperature parameter used obtaining apply softmax. target distribution estimate therefore measures similarity word vectors assigns similar probability masses words language model deems close. note estimation procedure iterative estimates initial phase training necessarily informative. however training procedes expect capture word statistics better yield consistently accurate estimate true data distribution. theoretically motivate introduce second modiﬁcation improve learning language model. analyzing proposed augmented loss particular setting observe implicit core mechanism loss. make proposition making mechanism explicit. start introducing setting analysis. restrict attention case input embedding dimension equal dimension hidden state i.e. also augmented loss i.e. assume achieve zero training loss. finally temperature parameter large. training example i.e. gradient contributed example zero. provided full rank matrices linearly independent examples ht’s embedding dimension space spanned columns equivalent spanned columns let’s introduce square matrix case rewrite although scenario could difﬁcult exactly replicate practice uncovers mechanism proposed loss augmentation acts trying constrain output probability space small subspace governed embedding matrix. suggests make mechanism explicit constrain training setting output bias zero. would eliminate matrix dominates network size models even moderately sized vocabularies would also optimal setting loss augmentation would eliminate much work done augmented loss. since introduction mikolov many improvements proposed rnnlms including different dropout methods novel recurrent units pointer networks complement recurrent neural network however none improvements dealt loss structure best knowledge work ﬁrst offer loss framework. technique closely related hinton also estimate informed data distribution augment conventional loss divergence model prediction distribution estimated data distribution. however estimate data distribution training large networks data improve learning smaller networks. fundamentally different approach improve learning transferring knowledge different parts network self contained manner. work present paper based report made public inan khosravi recently come across concurrent preprint authors reuse word embedding matrix output projection improve language modeling. however work purely empirical provide theoretical justiﬁcation approach. finally would like note idea using representation input output words explored past exists language models could interpreted simple neural networks shared input output embeddings however shared input output representations implicitly built models rather proposed supplement baseline. consequently possibility improvement particularly pursued sharing input output representations. experiments penn treebank corpus wikitext dataset standard dataset used benchmarking language models. consists training validation test words. version dataset processed mikolov frequent words selected vocabulary rest replaced <unk> token wikitext- dataset released recently alternative ptb. contains training validation test tokens vocabulary words; therefore comparison roughly times larger dataset size times larger vocabulary. closely follow lstm based language model proposed zaremba constructing baseline model. speciﬁcally -layer lstm number hidden units layer different network sizes small medium large train models using stochastic gradient descent variant dropout method proposed defer details regarding training models section appendix. refer baseline network variational dropout lstm vd-lstm short. section showed particular loss augmentation scheme choose constrains output projection matrix close input embedding matrix without explicitly reusing input embedding matrix. ﬁrst experiment validate theoretical result. this simulate setting section following select randomly chosen contiguous word sequence training train -layer lstm language model units layer loss augmentation minimizing following loss here proportion augmented loss used total loss scaled approximately match magnitudes derivatives since achieve minimum training loss possible goal show particular result rather achieve good generalization kind regularization neural network experiments also constrain input embedding matrix norm training becomes difﬁcult without constraint augmented loss used. training compute metric measures distance subspace spanned rows input embedding matrix spanned columns output projection matrix this common metric based relative residual norm projection matrix onto another computed distance subspaces orthogonal same. interested reader refer section appendix details metric. figure shows results tests. test effect using augmented loss sweeping reasonably high temperature loss augmentation distance almost augmented loss used downloaded http//www.ﬁt.vutbr.cz/ imikolov/rnnlm/simple-examples.tgz wikitext- downloaded https//s.amazonaws.com/research.metamind.io/wikitext/wikitext--v.zip figure subspace distance different experiment conditions validation experiments. results averaged independent runs. results validate theory practical conditions. distance decreases rapidly eventually reaches around augmented loss used. second test effect temperature subspace distance notably augmented loss causes approach sufﬁciently even temperatures although higher temperatures still lead smaller subspace distances. results conﬁrm mechanism proposed loss pushes learn column space suggests reusing input embedding matrix explicitly constraining simply kind regularization fact optimal choice framework. achieved separately proposed improvements well combined question empirical nature investigate next section. order investigate extent proposed improvements helps learning train different models network size -layer lstm variational dropout -layer lstm variational dropout augmented loss -layer lstm variational dropout reused embeddings -layer lstm variational dropout figure shows validation perplexities four models training corpus small large networks. real networks signiﬁcantly outperform baseline cases. table compares ﬁnal validation test perplexities four models wikitext- network size. datasets improve upon baseline individually using together leads best performance. based performance comparisons make following notes proposed improvements provides better performance gains smaller networks. surprising given fact small models rather inﬂexible would expect improved learning training informative data distribution smaller dataset performance surpasses comparison larger wikitext- dataset improvement limited. expected given larger training sets better represent true data distribution mitigating supervision problem. fact validate reasoning direct manner additionally train small networks separately ﬁrst second halves wikitext- training set. results distinct datasets size seen table signiﬁcantly improved competitive performance real despite fact embedding size times larger compared ptb. results support argument proposed augmented loss term acts improve amount information gathered dataset. signiﬁcantly outperforms larger networks. indicates that large models effective mechanism proposed framework enforces proximity output projection space input embedding space. model complexity perspective nontrivial gains offered network sizes datasets could largely attributed explicit function reduce model size preserving representational power according framework. list table comparison models without proposed modiﬁcations penn treebank corpus. best lstm model outperforms previous work uses conventional framework including large ensembles. recently proposed recurrent highway networks trained reused embeddings achieves best overall performance improving vd-rhn perplexity table performance four different small models trained equally sized partitions wikitext training set. results consistent similar training size partitions although word embedding dimension three times smaller. table comparison work previous state word-level validation test perplexities penn treebank corpus. models using framework signiﬁcantly outperform models. important feature framework leads better word predictions explicit mechanism assign probabilities words merely according observed output statistics also considering metric similarity words. observe direct consequences mechanism qualitatively penn treebank different ways first notice probability generating <unk> token proposed network signiﬁcantly lower compared baseline network across many words. could explained noting fact <unk> token aggregated token rather speciﬁc word often expected close speciﬁc words word embedding space. observe behavior frequent words owing fact correlated particular words. second observe better probability assignments target words also observe relatively higher probability weights associated words close targets. sometimes happens form predicting words semantically close together plausible even target word successfully captured model. provide examples test compare prediction performance unit vd-lstm unit vd-lstm +real table would like note prediction performance vd-lstm similar vd-lstm +real large network. table prediction next word baseline proposed networks example phrases test set. word predictions sorted descending probability arranged column-major format. work introduced novel loss framework language modeling. particularly showed metric encoded space word embeddings could used generate informed data distribution one-hot targets additionally training distribution improves learning. also showed theoretically approach lends second improvement simply reusing input embedding matrix output projection layer. additional beneﬁt reducing number trainable variables model. empirically validated theoretical link veriﬁed proposed changes fact belong framework. experiments penn treebank corpus wikitext- showed framework outperforms conventional even simple modiﬁcation reusing word embedding output projection layer sufﬁcient large networks. improvements achieved framework unique vanilla language modeling readily applicable tasks utilize language models neural machine translation speech recognition text summarization. could lead signiﬁcant improvements models especially large vocabularies additional beneﬁt greatly reducing number parameters trained. kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. charlie frogner chiyuan zhang hossein mobahi mauricio araya tomaso poggio. learning wasserstein loss. advances neural information processing systems kazuki irie zolt´an t¨uske tamer alkhouli ralf schl¨uter hermann ney. lstm highway attention empirical overview language modeling speech recognition. interspeech francisco tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. citeseer begin training learning rate start decaying constant rate certain epoch. small medium large networks respectively. decay rate small medium networks large network. dropout method introduced particularly dropout mask example unrolled network. differently proposed dropout weights hidden states further mask propagated states current layer used inputs next layer. don’t dropout input embedding layer dropout probability inputs hidden states. dropout probabilities small medium large networks respectively. wikitext- probabilities small medium networks. training networks augmented loss temperature empirically observed setting weight augmented loss according networks works satisfactorily. values dataset wikitext- dataset. would like note observed sudden deteriorations performance respect moderate variations either section detail metric used computing subspace distance matrices. computed metric closely related principle angles subspaces ﬁrst deﬁned jordan note calculated valid metric equivalence matrices span column space although going show instead mention metric properties relate principal angles subspaces. ﬁrst work expression", "year": 2016}