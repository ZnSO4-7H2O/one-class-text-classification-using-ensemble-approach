{"title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function. Their application to new points consists in two steps: first, embedding the new data point into the low dimensional space and then, estimating the function value on the test point from its neighbors in the embedded space.  However, finding the low dimension representation of a test point, while easy for simple but often not powerful enough procedures such as PCA, can be much more complicated for methods that rely on some kind of eigenanalysis, such as Spectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target function is to be evaluated, averaging methods like nearest neighbors may give unstable results if the function is noisy. Thus, the smoothing of the target function with respect to the intrinsic, low-dimensional representation that describes the geometric structure of the examined data is a challenging task.  In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension of the standard Laplacian Pyramids model that incorporates a modified LOOCV procedure that avoids the large cost of the standard one and offers the following advantages: (i) it selects automatically the optimal function resolution (stopping time) adapted to the data and its noise, (ii) it is easy to apply as it does not require parameterization, (iii) it does not overfit the training set and (iv) it adds no extra cost compared to other classical interpolation methods. We illustrate numerically ALP's behavior on a synthetic problem and apply it to the computation of the DM projection of new patterns and to the extension to them of target function values on a radiation forecasting problem over very high dimensional patterns.", "text": "important challenge data mining machine learning proper analysis given dataset especially understanding working functions deﬁned particular manifold learning algorithms become common processing analyzing high-dimensional data called diﬀusion analysis allows appropriate geometry study functions methods based construction diﬀusion operator depends local geometry data used embed high-dimensional points lower-dimensional space maintaining geometric properties hopefully making easier analysis functions hand extending functions embedding data points challenging either noise presence low-density areas make insuﬃcient number available training points. also diﬃcult neighborhood size unseen points done according local behavior function. classical methods function extension like geometric harmonics parameters need carefully addition exist robust method picking correct neighborhood embedding function smoothing evaluation. ﬁrst attempt simplify approaches laplacian pyramids multiscale model generates smoothed version function iterative manner using gaussian kernels decreasing widths simple method learning functions general coordinates also applied extend embedding coordinates challenges diﬀusion methods. recently introduced geometric based out-of-sample extension purpose adding points constructed embedding coordinates. na¨ıve extend target function data point could point’s nearest neighbors embedded space average function values. method data lifting compared version proposed last method performed better buchman also described diﬀerent point-wise adaptive approach requires setting nearest neighborhood radius parameter every point. nevertheless often case machine learning apply previous model overﬁt data reﬁne much prediction training phase. fact diﬃcult decide stop training obtain good generalization capabilities. usual approach apply cross validation method validation error stop error starts increase. extreme form leave model built using non-linear dimensionality reduction techniques manifold learning algorithms become common processing analyzing high-dimensional patterns often attached target corresponds value unknown function. application points consists steps ﬁrst embedding data point dimensional space then estimating function value test point neighbors embedded space. however ﬁnding dimension representation test point easy simple often powerful enough procedures much complicated methods rely kind eigenanalysis spectral clustering diﬀusion maps similarly target function evaluated averaging methods like nearest neighbors give unstable results function noisy. thus smoothing target function respect intrinsic low-dimensional representation describes geometric structure examined data challenging task. paper propose auto-adaptive laplacian pyramids extension standard laplacian pyramids model incorporates modiﬁed leave cross validation procedure avoids large cost standard loocv oﬀers following advantages selects automatically optimal function resolution adapted data noise easy apply require parameterization overﬁt training adds extra cost compared classical interpolation methods. illustrate numerically alp’s behavior synthetic problem apply computation projection patterns extension samples used single validation pattern; repeated sample dataset validation error average errors. although loocv theoretical backing often yields good results drawback computational cost though important cases paper propose auto-adaptive modiﬁcation training algorithm merges training approximate loocv single phase. simply build kernel matrix zeros diagonal. shall change implement loocv approximation without additional cost training step. reduces signiﬁcantly training complexity provides automatic criterion stop training greatly avoid risk severe overﬁtting appear standard eﬀect observed figure proposal applied synthetic example used section solid dashed black lines represent training error loocv error iteration respectively dashed blue line represents error proposed method. blue line corresponds training error attains minimum iteration prescribed loocv therefore doesn’t overﬁt data moreover doesn’t essentially require parametrization expert knowledge problem still achieving good test error. moreover adds extra cost compared classical neighbor-based interpolation methods. paper organized follows. section brieﬂy review model present detailed analysis training error. describe section apply synthetic example section real world example section paper ends conclusions laplacian pyramid iterative model introduced burt adelson application image processing particular image encoding. traditional algorithm decomposes input image series images capturing diﬀerent frequency band original one. process carried constructing gaussian-based smoothing masks diﬀerent widths followed down-sampling step. later proved tight frame used signal processing applications example reconstruction scheme introduced multi-scale algorithm spirit applied setting high-dimensional data analysis. particular proposed simple method extending low-dimensional embedding coordinates result application non-linear dimensionality reduction technique high-dimensional dataset several reﬁnements error approximations. slight abuse language notation general function also vector sample values result process gives function approximation model doesn’t change anymore. words care taken deciding stop iterations avoid overﬁtting. fact show next using gaussian kernels norm errors decay extremely fast. training error alp. words working matrix instead training error step gives fact approximation loocv error step. cost running steps thus gain advantage exhaustive loocv without additional cost overall algorithm. complete training procedure presented algorithm test algorithm shown algorithm algorithm training algorithm input output trained model. ytr. e−xtr xtr/σ normalize. -diagonal. loocv. ˜fi− ˜pi. ˜fi. erri di/str number patterns xtr. σ/µ; obvious advantage evaluate training error actually estimating loocv error iteration. therefore evolution loocv values tells optimal iteration stop algorithm i.e. training error approximation loocv error starts growing. thus remove danger overﬁtting also training error approximation generalization error. eﬀect seen figure illustrates application synthetic problem described next section optimum stopping time exactly would give loocv error training error stabilizes afterwards slightly larger value. moreover achieves automatic selection width gaussian kernel makes version auto-adaptative require costly parameter selection procedures. fact choosing customarily done required parameter would initial provided wide enough scalings yield adequate ﬁnal kernel width. soon validation error subset starts increase. problematic small samples introduces random dependence choice particular validation subset; k-fold cross validation usually standard choice randomly distribute sample remaining validation. extreme case i.e. pattern validation arrive leave cross validation stop training iterations loocv error starts increase. besides simplicity loocv attractive almost unbiased estimator true generalization error although possibly high variance case loocv easily applied using training normalized kernel matrix previous matrix p-th rows columns held training sample used validation. obvious drawback loocv rather high cost case models ways estimate loocv error smaller cost. done exactly case k-nearest neighbors ordinary least squares chapter approximately support vector machines gaussian processes observation leads modiﬁcation propose standard algorithm given simply consist applying procedure described section replacing matrix -diagonal version computing beginning vectors iteration. recall model automatically adapts multiscale behavior data trying reﬁne prediction iteration using localized kernel given smaller behavior observed figure shows evolution prediction small noise experiment. beginning model approximates function coarse mean target function values; however subsequent iterations start using sharper kernels reﬁned residuals approximating function starts capturing diﬀerent frequencies amplitudes composite sines. particular case minimum loocv value reached iterations relatively small number makes sense simple model small noise. predicted function represented figure obtained iterations. expected number iterations slightly smaller previous example algorithm selects conservative smoother prediction presence noisier thus diﬃcult data. always carries information) retaining thus number parameter choose besides previous usual values either strict ﬁxed would thus arrive diﬀusion coordinates imated euclidean distance projected space. makes useful tool apply procedures projected space k-means clustering usually rely implicit euclidean distance assumption. steps compute summarized algorithm elegant powerful drawback relying eigenvectors markov matrix. makes diﬃcult compute coordinates unseen pattern moreover eigenanalysis matrix would principle potentially high cost sample’s size. however issues addressed terms function approximation. standard approach apply nystr¨om extension formula hence alps also used setting discuss next. illustrate next application techniques analysis solar radiation data relate actual aggregated radiation values numerical weather prediction values. preceding assumption given rise number methods among mention multidimensional scaling local linear embedding isomap spectral clustering several variants laplacian eigenmaps hessian eigenmaps diﬀusion methods also follow center discussion them. assumption diﬀusion methods metric dimensional riemannian manifold data approximated suitably deﬁned diﬀusion metric. starting point weighted graph representation sample similarity maorder control eﬀect sample distribution parameter stationary distribution markov process transition probability steps work shown inﬁnitesimal generator diﬀusion process coincides manifold’s laplace– beltrami operator thus expect diﬀusion projection capture underlying geometry. eigenvalues left eigenvectors markov matrix eigenvectors’ components eigenvalues decay rather fast fact perform dimensionality reduction setting fraction second eigenvalue training patterns coordinates training points. apply algorithm decide stopping iteration compute diﬀerences ﬁnally apply algorithm obtain approximations values test points. order measure goodness coordinates also performed eigenanalysis entire data i.e. training test inputs together. compare extended embedding obtained using would obtained known correct diﬀusion coordinates. experiments results obtained computing embedding training sample using ﬁrst extend test sample radiation prediction. figure training test results shown. three diﬀusion coordinates example colored target i.e. solar radiation prediction given ﬁrst third image captures structure target radiation sample radiation data appearing apart points high radiation compare prediction-colored embeddings observe radiation values smoothed across color bands general radiation trend captured approximately along second feature even every detail modeled behavior observed training points ﬁrst second plots also test ones third fourth ones seen method makes good extension target function points. meteorological stations located oklahoma context solar energy prediction contest hosted kaggle company. ultimate goal would obtain best predictions problem illustrate application previously described setting. input data ensemble numerical weather predictions noaa/esrl global ensemble forecast system main ensemble highest probability correct. input patterns contain time-steps embedding computed entire sample shown figure comparing figure embeddings similar target prediction colors seem less same. shows apply compute coordinates test sample points embedding quite close ideal obtained jointly train test patterns. order give quantitative measure quality projections perform euclidean k-means three-dimensional embeddings test sample. want compare clusters obtained ideal embedding computed entire sample ones obtained train embedding extended test coordinates. resulting clusterings seen ﬁgure notice clusters reﬂect radiation structure; instead weight apparently given ﬁrst feature anyway recall embedding made diﬀerent variable types radiation variables. this cluster structure doesn’t reﬂect radiation’s eﬀect embedding overall variable behavior. obtained concrete metric compare cluster assignments test points extended embedding made full embedding looking classiﬁcation problem accuracy i.e. percentage test points assigned real clusters measures well extensions match true embedding structure. confusion matrix table shows quite well total accuracy thus clustering embedded features desired makes possible patterns high accuracy. getting back radiation prediction test sample embedding obtained predict total daily incoming solar energy using step procedure. applied training sample obtain embedded features ﬁrst figure left depicted second test year real radiation light blue prediction dark blue. although winning models kaggle competition followed diﬀerent approaches seen captures radiation’s seasonality. right plot zoom appreciated tracks radiation variations. even every peak caught yields reasonably good approximation actual radiation without requiring particular parameter choices expert knowledge problem wanted address. figure shows evolution standard training error associated loocv error loocv estimation given applied radiation forecasting. illustrates robustness model overﬁtting. again model requires number iterations suggested applying full loocv standard finally figure shows test point plotted black evolution advances inﬂuence sample points larger points smaller blue ones. seen gets smaller number high inﬂuence points also decreases sharply possibility overﬁtting. classical laplacian pyramid scheme burt adelson widely studied applied many problems particularly image processing. however risk overﬁtting thus requires rather costly techniques cross validation prevent loocv value iteration allowing thus automatically decide stop order avoid overﬁtting. illustrated robustness method synthetic example shown real radiation problem extend diﬀusion maps embeddings patterns provide simple reasonably good radiation predictions. authors wish thank prof. yoel shkolnisky prof. ronald coifman helpful remarks. ﬁrst author acknowledge partial support grant tin--c spain’s ministerio econom´ıa competitividad uam–adic chair machine learning modeling prediction.", "year": 2013}