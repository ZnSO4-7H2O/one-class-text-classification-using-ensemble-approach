{"title": "How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In numerous applicative contexts, data are too rich and too complex to be represented by numerical vectors. A general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are. This approach has been used to define several variants of the Self Organizing Map (SOM). This paper reviews those variants in using a common set of notations in order to outline differences and similarities between them. It discusses the advantages and drawbacks of the variants, as well as the actual relevance of the dissimilarity/kernel SOM for practical applications.", "text": "abstract. numerous applicative contexts data rich complex represented numerical vectors. general approach extend machine learning data mining techniques data really dissimilarity kernel measures diﬀerent similar objects are. approach used deﬁne several variants self organizing paper reviews variants using common notations order outline diﬀerences similarities them. discusses advantages drawbacks variants well actual relevance dissimilarity/kernel practical applications. complex data frequently rich elaborate represented simple tabular form object described ﬁxed attributes/variables numerical and/or nominal values. especially case relational data objects diﬀerent categories interconnected relations diﬀerent types. instance online retailers interconnected customers products databases customer several copies product also leave score and/or review said products. adapting data mining machine learning methods complex data possible time consuming complex theoretical level practical point view therefore tempting build generic methods properties shared types data. generic approaches used successfully dissimilarity based approach kernel based approach based fairly generic assumptions analyst given data either dissimilarity kernel deﬁned. dissimilarity measures much objects diﬀers kernel seen form similarity measure least correlation sense. dozens dissimilarities kernels proposed years covering many types complex data needs adapt classical data mining machine learning method dissimilarity/kernel setting order obtain fully generic approach. dissimilarity always constructed kernel dissimilarity algorithms probably generic ones. typical example nearest neighbor method based dissimilarities. review paper variants self organizing proposed following line research variants operate dissimilarity/kernel data. discuss whether variants really usable helpful practice. paper organized follows. section describes general setting dissimilarity data kernel data self organizing map. section dedicated oldest dissimilarity variant median section focuses modern variant relational som. section presents diﬀerent approach extensions based deterministic annealing principle. section describes kernel based variants som. unifying view provided section shows diﬀerences variants mainly explained optimization strategy rather data properties. finally section gathers personal remarks insights dissimilarity/kernel variants. dissimilarity data setting assumed data described indirectly square symmetric matrix contains dissimilarities data points. convention negative real number high diﬀerent similar. minimal assumptions symmetry negativity element. also natural assume basic ordering used variants. theoretical results also need strong constraint. notice given either dissimilarity function directly matrix important aspect kernel setting lays moore-aronszajn theorem states reproducing kernel hilbert space associated mapping function mapping called feature map. enables leverage hilbert structure order build machine learning algorithms indirectly.this done general without using rather relying only known kernel trick construction shows dissimilarity setting general kernel setting. always possible kernel basis dissimilarity dissimilarity variants used kernel data. therefore focus mainly dissimilarity algorithms discuss relate kernel counterparts. notice ﬁnally case dissimilarity setting kernel given function kernel matrix latter case symmetric positive deﬁnite associated dissimilarity matrix equation contrast classical setting dissimilarity kernel ones introduce notations brieﬂy recall principle algorithm dimensional clustered representation data set. needs ﬁrst specify dimensional prior structure general regular lattice units/neurons positioned ≤k≤k structure induces time dependent neighborhood function measures much prototype/model associated unit close associated unit step learning algorithm discuss numerous possible variants neighborhood function lattice made points classical choice attaches unit/neuron prior structure prototype/model data space objective algorithm adapt values models data point close possible closest model data space addition closest model data point also close close prior structure. words proximities prior structure reﬂect proximities data space vice versa. unit/neuron associated closest model data point called best matching unit point. points deﬁnes cluster data space denoted essentially achieved major algorithms assume data space classical normed vector space. algorithms initialize prototypes ≤k≤k appropriate proceed iteratively. discuss initialization strategies paper. turns vector space operations involved equation optimization problem uses squared euclidean norm prototypes observations. arbitrary space dissimilarity replaced dissimilarity turns problem however general dissimilarity setting assumes availability dissimilarities observations arbitrary points fact generating points might diﬃcult complex data texts. general solution consists looking optimal prototypes data rather median variants based principle. median consists iterating steps. ﬁrst step best matching unit data point determined variant median proposed rather solving problem associates unit generalized median corresponding cluster data point chosen randomly using neighborhood structure dissimilarities. means data point moved natural nearby one. know variant studied details. median numerous problems. batch expected request iterations converge potential stochastic version addition also exhibit sensitivity initial conﬁguration. also problems speciﬁc median som. iteration algorithm rather high computational cost naive implementation leads cost iteration careful still costs numerous tricks used reduce actual cost iteration factor cannot avoided without introducing approximations. arguably main drawbacks median intrinsic nature. firstly restricting prototypes chosen data adverse eﬀects. basic important problem comes collisions prototypes diﬀerent units optimal solution according equation corresponds massive folding dimensional representation associated thus sub-optimal data summary. addition equation needs breaking rule general increase cost determination example rule). solution proposed used avoid problems reasonable computational cost. subtle consequence restriction prototypes data points unit remain empty apart collided prototypes. indeed data point used prototype unit prototype. means interpolation eﬀect take place median fact limits strongly usefulness visual representations u-matrix speciﬁc data types strings avoided introducing ways generating data points form interpolations. studied together stochastic/online algorithm. second intrinsic problem median reliance prototype based representation cluster dissimilarity context justiﬁed euclidean context. indeed consider data points ≤i≤n belong euclidean space. vector positive weights ≤i≤n well known k¨onig-huygens identity states second problem makes clear classical based quantization also optimizing within pairwise distances clusters deﬁned mapping. hkci considered form membership hkci cluster pairwise distances cluster measures compactness cluster terms within variance. minimizes quantities seen clustering algorithm. however dissimilarity satisfy triangular inequality criteria directly related more. fact prototype close data points points remain apart other. form quantization solving problem thing form clustering solving problem choosing prototype based solution median appears quantization method rather clustering one. goal display prototypes organized choice make sense goal display clusters organized choice intrinsically suboptimal. pointed section dissimilarity soms adapted prototype display puts question interest median particular quantization approach general. quantiﬁcation prototypes induced restricting data points quite negative eﬀects described section relational approach address problem. based simple following observation ...n points hilbert space equipped inner product αixi arbitrary real valued coeﬃcients ...n squared distance matrix given xji. means computing distance linear combination data points data points done using coeﬃcients combination distance matrix points. shown equation prototypes classical exactly linear combinations data points whose coeﬃcients one. therefore possible express batch algorithm without using directly values rather keeping coeﬃcients prototypes using equation squared distance matrix perform calculations. simply apply called relational version algorithm arbitrary dissimilarity matrix squared euclidean one. essentially done c-means batch using concept pseudo-euclidean spaces shown general approach given rigorous derivation amounts using original algorithm pseudo-euclidean embedding data points. practice batch relational proceeds iterating steps similar classical batch steps. main diﬀerence prototype given vector represents coeﬃcients linear combination pseudoeuclidean embedding. best matching unit computation equation replaced equals cases. notice initialized preserved update. shown stochastic variant tends less sensitive initial values prototypes. however overlooks batch online relational algorithms share computational cost iteration negates traditional computational gain provided online versions. relational solves several problems median som. particular subject quantization eﬀect induced constraining prototypes data points. consequence exhibits practice interpolation eﬀects classical som. availability stochastic version provides also simple reduce adverse eﬀects initialization. large cost motivated research approximation techniques principled approach consists approximating calculation matrix product nystr¨om technique explored pointed section algorithm relies prototypes general possibly metric dissimilarity provides quantization clustering. organized clusters looked solve problem directly without relying prototypes. however problem combinatorial highly convex. particular absence prototypes rules standard alternating optimization schemes. following analysis done case dissimilarity version k-means graepel introduce deterministic annealing approach address problem approach introduces mean ﬁeld approximation estimates eﬀects criterion problem assigning data point cluster addition computes soft assignments cluster/unit denoted membership cluster practice so-called soft topographic mapping proximity data trained iterative batch like procedure. given annealing schedule initial random values mean ﬁeld algorithm iterates evaluating equation equation well known quality results obtained deterministic annealing highly dependent annealing scheme particularly important avoid missing transition phases. graepel analyzed transition phases stmp ﬁrst critical temperature related dominant eigenvalue dissimilarity matrix. general dense matrix minimal cost computing critical temperature addition internal iteration algorithm dominated update mean ﬁeld according equation cost full update stmp therefore computationally intensive. noted however approximation mean ﬁeld update reduces cost proposed leading computational cost relational som. addition appear clearly section stmp based prototypes even appear indirectly. therefore tries optimize clustering criterion associated resorts similar quantization quality proxy relational som. recalled section kernel setting easier deal dissimilarity one. indeed embedding hilbert space enables apply classical machine learning method kernel data leveraging euclidean structure kernel trick allows implement methods eﬃciently. case kernel trick based fundamental remark enables relational batch prototypes linear combinations data points. initial values prototypes linear combinations data points also case stochastic/online som. equation typical result kernel trick computing distance data point linear combination data points done using solely kernel function knowledge ﬁrst kernel trick context made exactly equation earliest kernel optimized using deterministic annealing kernel trick enables traditional online batch derived previous equations. built indirectly hilbert space embedding kernel suﬀer constrained prototypes. stronger assumptions made kernels compared dissimilarities guarantee equivalence ﬁnding good prototypes ﬁnding compact clusters. kernel also online batch versions. main limitation kernel computational cost. indeed relational evaluating distances equation cost. approximation schemes proposed relational used kernel cost reduced performances terms data representation. might seem ﬁrst variants presented previous sections quite diﬀerent terms goals algorithms. contrary exception median speciﬁc aspects variations diﬀerent methods explained optimization strategies rather hypothesis data. already pointed relational kernel share principle representing prototypes linear combination data points. cases coeﬃcient update formulas whose structure depends type algorithm connections even stronger sense given kernel relational algorithm obtained using dissimilarity associated kernel exactly identical kernel algorithm. indeed kernel matrix dissimilarity matrix given kij. equivalence shows batch relational rediscovery batch kernel online relational rediscovery online kernel results show rediscoveries fact generalizations kernel variants extend hilbert embedding general pseudo-euclidean embedding. practice reason distinguish kernel relational som. surface stmp described section looks diﬀerent relational/kernel approaches tries address combinatorial optimization problem rather diﬀerent problem associated generalized median. however analyzed details stmp diﬀers relational approach deterministic annealing absence prototypes. careful analysis equations clariﬁes point. indeed consider ≤j≤n coeﬃcient vector linear combination data points embedded pseudo-euclidean space associated dissimilarity matrix right hand part distance pseudo-euclidean space prototype associated equation weighted average distances weights given neighborhood function. pointed seen relational extension assignment rule proposed heskes kappen however rather using crisp assignments best matching unit lowest value stmp uses soft maximum strategy implemented equation obtain assignment probabilities γik. used turn update coeﬃcients prototypes equation fact three algorithms proposed based deterministic annealing scheme initial implementation generalization hilbert space associated kernel pseudo-euclidean space associated dissimilarity discussion previous section shows kernel dissimilarity variants strictly equivalent. opinion almost reason median practice except possibly reduced computational burden compared relational compared dominating terms). indeed median suﬀers constraining prototypes data points gives general lower performances relational/kernel compared ground truth based usability results lack interpolation capability particularly damaging prevents general display gaps natural clusters u-matrix like visual representation large data sets factor increase cost iteration relational compared median could seen strong argument latter. opinion approximation techniques probably better choice. remains however tested knowledge eﬀects nystr¨om approximation studied extensively relational neural relational knowledge systematic study inﬂuence optimization strategy conducted variants even case numerical data. latter case well known online/stochastic less sensitive initial conditions batch som. also generally faster converge leads general better overall topology preservation similar results observed dissimilarity case noted however analyses random initializations well known based initialization gives much better results random case batch som. also pointed neighborhood annealing schedule strong eﬀects topology preservation batch som. therefore terms ﬁnal quality completely obvious online solution provide better results batch one. addition relational setting negates computational advantage online versus batch som. indeed numerical case epoch online roughly cost iteration batch som. online converges generally small number epochs complete computational cost lower batch som. contrary cost relational dominated calculation equation batch relational quantity computed time prototype iteration leading cost iteration reports erroneously complexity iteration). online version also computed data point means epoch online relational costs times iteration batch relational som. think therefore careful implementation batch relational outperform online version provided initialization conducted properly. compare relational neural dissimilarity deterministic annealing clustering conclusion expected similar comparisons done numerical data sophisticated annealing strategy deterministic annealing techniques leads general better solutions provided critical temperatures properly identiﬁed. comes largely increased cost really cost iterations rather algorithm comprises loops inner loop given temperature outer annealing loop. therefore total number iterations general order magnitude higher classical batch algorithms similar results context graph speciﬁc variant principle). also noted deterministic variants proposed neighborhood function adapted learning. eﬀects choice usability ﬁnal results remain studied. summarize opinion prefer careful implementation batch relational paired like algorithm initialization using nystr¨om approximation large data sets. experimental work needed validate choice. explained section algorithm resorts prototypes arbitrary dissimilarity fact form quantization rather form clustering. knowledge attempt made minimize directly prototype free criterion used problem speculate point. ﬁrst note case classical clustering shown optimizing directly criterion problem k-means simpliﬁed form gives better results using relational version kmeans. computational burden approaches comparable direct optimization pairwise dissimilarities criterion based much sophisticated algorithm combines state-of-the-art hierarchical clustering multi-level reﬁnement graph clustering assuming complex technique could used train like algorithm would obtain empty clusters organized according lattice dimensions something similar obtained median som. clusters would better quality interpolation would possible median som. personal opinion main interest provide rich readable visual representations complex data unfortunately visualization possibilities reduced case dissimilarity data. fact aspects results displayed case numerical data dissimilarities prototypes well numerical characteristics clusters pointed among others type visualization interesting mainly uses large number units. possible relational implies high computational dominating term. case deterministic annealing versions even problematic complexity term induced soft memberships. som’s results. instance clustering graph nodes kernel/dissimilarity draw clustered graph representation proposed however shown case specialized models derived simpler dual approaches based graph clustering graph visualization give general better ﬁnal results. summarize opinion appeal generic dissimilarity somewhat reduced limited visualization opportunity oﬀers compared traditional som. work needed explore whether classical visualization techniques e.g. brushing linking could used provide interesting displays based dissimilarity som. reviewed paper main variants adapted dissimilarity data kernel data. following shown variants diﬀer terms optimisation strategy aspects. recalled particular kernel variants strictly identical relational counterpart. taking account computational aspects known experimental results opinion best solution batch relational coupled structured initialization nystr¨om approximation large data sets thus need dissimilarity/kernel variant only. however discussed above practical usefulness dissimilarity reduced compared numerical rich visual representations associated available dissimilarity version. without improvement visual outputs completely clear dissimilarity serves real practical purpose beyond elegant generality simplicity.", "year": 2014}