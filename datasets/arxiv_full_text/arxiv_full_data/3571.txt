{"title": "The Robust Manifold Defense: Adversarial Training using Generative  Models", "tag": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "abstract": "Deep neural networks are demonstrating excellent performance on several classical vision problems. However, these networks are vulnerable to adversarial examples, minutely modified images that induce arbitrary attacker-chosen output from the network. We propose a mechanism to protect against these adversarial inputs based on a generative model of the data. We introduce a pre-processing step that projects on the range of a generative model using gradient descent before feeding an input into a classifier. We show that this step provides the classifier with robustness against first-order, substitute model, and combined adversarial attacks. Using a min-max formulation, we show that there may exist adversarial examples even in the range of the generator, natural-looking images extremely close to the decision boundary for which the classifier has unjustifiedly high confidence. We show that adversarial training on the generative manifold can be used to make a classifier that is robust to these attacks.  Finally, we show how our method can be applied even without a pre-trained generative model using a recent method called the deep image prior. We evaluate our method on MNIST, CelebA and Imagenet and show robustness against the current state of the art attacks.", "text": "deep neural networks demonstrating excellent performance several classical vision problems. however networks vulnerable adversarial examples minutely modiﬁed images induce arbitrary attacker-chosen output network. propose mechanism protect adversarial inputs based generative model data. introduce pre-processing step projects range generative model using gradient descent feeding input classiﬁer. show step provides classiﬁer robustness ﬁrst-order substitute model combined adversarial attacks. using min-max formulation show exist adversarial examples even range generator natural-looking images extremely close decision boundary classiﬁer unjustiﬁedly high conﬁdence. show adversarial training generative manifold used make classiﬁer robust attacks. finally show method applied even without pre-trained generative model using recent method called deep image prior. evaluate method mnist celeba imagenet show robustness current state attacks. deep neural network classiﬁers currently demonstrating excellent performance various computer vision tasks. models work well benign inputs recent work shown possible make small changes input image drastically fool state-of-the-art models adversarial examples barely perceivable humans targeted create desired labels even black-box access classiﬁers made robust real objects physical world phenomenon receiving tremendous amount recent attention references therein) good reasons first classiﬁer easily fooled non-perceivable noise poses security threat real deployment. second illustrates even best models making correct predictions wrong reasons. relates interpretability trust modern complex models important emerging topic. typical methods attack involve modifying pixel values keeping small distance original image. recent work however shown small rotations spatial transformations also fool classiﬁers. would like propose extended deﬁnition adversarial examples captures important aspects building legal theory reasonable person would class classiﬁer produces signiﬁcantly diﬀerent outputs. deﬁnition useful someone defaced stop sign reasonable person could confuse diﬀerent sign nobody accuse classiﬁer making mistake. contrary attacks like robust physical perturbations traﬃc signs shown would never make reasonable person think stop sign. many attempts made defend dnns adversarial examples. survey literature subsequent section overall message defending possible methods attack previously deﬁned remains challenging. intuition adversarial examples exist original natural image perturbed xadv point manifold natural images. classiﬁer never trained objects natural images behave unexpected ways. furthermore natural image manifold low-dimensional noisy objects reached even small perturbations high dimensional hence much harder learn. paper make critical assumption generative model data working generative model either explicit implicit model like generative adversarial network several methods train neural networks project image manifold end-to-end diﬀerentiable hence easy attack compressed sensing inversion method instead given input image classiﬁer feed image directly input classiﬁer rather treat noisy measurements another true image xtrue range generator solve minimization problem close input image feed classiﬁer. minimization solved gradient descent makes non-diﬀerentiable method projecting manifold. since thousands gradient steps required easy unfold operation attack diﬀerentiable substitute model show evaluation section. formulate method show able resist ﬁrst-order black-box attacks. explore robustness even formulating minmax optimization problem adversary much power process tries simply lagrangifying constraints using ﬁrst-order method able solve problem pairs adversarial points manifold unjustiﬁed drastically diﬀerent classiﬁer conﬁdence. shows natural problematic points exist idea also supported recent work shows artiﬁcially constructed classiﬁer spheres. thus seek show min-max attack used robustify using adversarial training examples manifold. show proposed classiﬁer robust various types attacks including end-to-end substitution models. accuracy classiﬁer drops compared clean-image performance inversion operation seems provide eﬀective protection. last innovation deals robust classiﬁcation without pre-trained generative model. relevant several rich datasets like imagenet hard train accurate generative model. address problem rely deep image prior untrained convolutional neural network latent code kept ﬁxed random value weights trained match desired output image. ulyanovet showed used denoising inpainting super-resolution without pre-training dataset. siﬁer projecting inputs onto range given generator eﬀectively serves prior classiﬁcation. demonstrate algorithm induces robustness across wide variety attacks including ﬁrst-order methods substitute models enhanced attacks combining two. models unavailable instead structural prior given untrained generator introduced show deep image prior defense actually defend adversarial attacks imagenet dataset. given classiﬁer parametrized vector parameters want defend ﬁltering input generator samples natural inputs. would pre-trained generative model assumed produce natural inputs diﬀerent categories classifying. step overpowered attack given input attack search close also close manifold classiﬁcation projections signiﬁcantly diﬀerent. attack exists must exist close cθ)) far. following optimization problem captures furthest cθ)) subject constraint distance provides upper bound magnitude attack optimization problem upper-bounds size attack inc. fact also captures loss arise potential imperfect optimization ﬁrst step input projected range namely value objective function solution found experimentally identify good solutions using gradient descent describe section show attacks gender classiﬁer trained celeba protected framework using began generator. experiments show another interesting experimental ﬁnding pairs images identiﬁed attack framework appeared change gender-relevant features confuse classiﬁer often producing images whose class ambiguous even human observers. show pairs classiﬁed section main limitation inc-protected classiﬁer predictions high conﬁdence even ambiguous images small changes image cause abrupt changes classiﬁcation probabilities. clearly interesting empirical ﬁnding robustness would hope base security noise introduced gradient descent projection step. next section adversarial pairs inputs robustify classiﬁer using adversarial training. anticipating attack outlined section take defense approach outlined section step further retraining parameters classiﬁer minimize damage attack. results following min-max formulation propose deep image prior i.e. untrained generative model convolutional neural network topology. surprising result training weights approximate given input image eﬀectively projects onto manifold natural images. leverage idea defend classiﬁers imagenet without relying pre-trained generative model. method identical diﬀerence projection step optimizes weights speciﬁcally given input image search natural image small. deep image prior method tries solve problem constructing generative convolutional neural network parameterized weights searching weights satisfy important issue search gradient descent procedure terminate early observed many steps performed becomes expressive also reconstructs adversarial noise. number steps empirically tuned experiments depends power adversary. discuss detail section experiments section ﬁnal note emphasized previous methods paper also apply non-image datasets order apply generator-free approach non-image data would develop architecture serving analog type data. figure schematic showing proposed defense strategy invert-and-classify. trapezoid labeled triangle labeled denote generator classiﬁer models respectively. rectangle denotes optimization procedure accepts image input repeatedly queries minimizes found classiﬁer makes prediction based note invert-and-classify refers generator intuition strategy works based observation adversarial noise high dimensional whereas natural images form dimensional manifold hence searching image span close norm equivalent projecting onto manifold natural images assuming learned true probability distribution training dataset. figure schematic showing deep image prior inc. trapezoid labeled triangle labeled denote generator classiﬁer models respectively. rectangle denotes optimization procedure accepts image input repeatedly queries minimizes found classiﬁer makes prediction based gφ∗. note deep image prior defense refers generator randomly initialized search optimal parameters held constant throughout. observed ulyanov deep image prior yield accurate reconstructions many iterations. found method remove adversarial noise early stopping carefully tuned. evaluate robustness invert-and-classify classiﬁcation tasks datasets handwritten digit classiﬁcation mnist dataset gender classiﬁcation celeba dataset evaluate robustness deep image prior defense experiments validation images randomly sampled imagenet dataset generate images celebrities trained began ﬁrst images celeba dataset. generator began fully connected layer convolutional layers. input generator distributed according fully connected layer size ﬁrst convolutional layers ﬁlters size ﬁnal convolutional layer ﬁlters size demonstrate robustness method standard attacks. note since inversion non-diﬀerentiable gradient descent ﬁrst-order attacks end-to-end system infeasible. methods construct adversarial examples perturbations small norm. first-order attacks full system intractable generate; section demonstrate robustness ﬁrst-order attacks unprotected classiﬁer. ﬁrst focus case adversarial perturbation norm fast gradient sign method. perform untargeted attacks—we require classiﬁer predicts label true label; adversarial perturbation given code borrowed https//jmetzen.github.io/notebooks/vae.ipynb; architecture kept same. code borrowed https//github.com/carpedm/began-tensorflow; architecture kept same. supplementary material https//dmitryulyanov.github.io/deep_image_prior precise details original image label; cross entropy loss label classiﬁer prediction celeba dataset additionally evaluated robust invert-and-classify carlini-wagner attacks attack conﬁdence parameter ﬁgure original images images obtained inverting xadv adversarial examples obtained images obtained inverting xadv. values corner image indicate conﬁdence classiﬁer predicts image correct gender. shown unprotected gender classiﬁer conﬁdence protected conﬁdence investigate robustness end-to-end protected classiﬁer. gradient descent inversion procedure non-diﬀerentiable hence much harder attack. possible attack unfold gradient descent steps create diﬀerentiable model subsequently attacked. since gradient descent projection involves thousands iterations could make attack work. second approach leverage transferability adversarial examples design black-box attack evaluate invert-and-classify model show attack also ineﬀective. train end-to-end substitute network typical black-box setting using inputoutput pairs target model. emphasize substitute model diﬀerentiable attempts approximate decision boundaries non-diﬀerentiable invert-and-classify model. architecture substitute model follows inner classiﬁer described section train network images celeba dataset labeled output invert-and-classify model. finally improve substitute model even also train inputs adversarial inner classiﬁer incorporates white information since provide adversary knowledge gradients training samples. craft ﬁrst-order adversarial examples using substitute model feed target model. measure percentage adversarial examples generated substitute model misclassiﬁed target model well. figure shows inability ﬁrst-order attacks namely fast gradient sign method basic iterative method substitute model transfer invert-and-classify. figure non-transferability attacks substitute networks network trained natural adversarial input-output pairs celeba. validation consists natural images correctly classiﬁed target substitute model. images adversarially perturbed fgsm steps another idea attacking train diﬀerentiable substitute model inversion step only combine model standard ﬁrst-order attacks classiﬁer produce adversarial inputs. mentioned unfolding gradient descent process intractable since thousands iterations projection. thus model inversion step using convolutional neural network eﬀectively training diﬀerentiable encoder making autoencoder. show table appendix indeed easy attack autoencoder attacks typically transfer system. reasonable person would disagree conﬁdence changes pairs images adversarial attacks classiﬁer according deﬁnition. adversarial attacks manifold natural images classiﬁer must made robust. optimization terminated images satisfying constraint; within average kl-divergence classiﬁer outputs inducing diﬀerent classiﬁcations. figure shows randomly selected successful results attack. first note contrast attacks found figure unprotected classiﬁer attacks found optimization tend yield yield images semantically relevant features classes furthermore often introduce meaningful diﬀerences hard decision boundary introduced classiﬁer training. secondly described section none images actually induce diﬀerent classiﬁcations end-to-end classiﬁer implement adversarial training iteration addition sampling cross-entropy loss images dataset also sample adversariality loss generate batch adversarial inputs using steps min-max attack ﬁnal distance classiﬁcation outputs cross-entropy loss. shown figure classiﬁer eventually learns minimize adversary’s ability examples likely learning softening decision boundaries exploited generator. robustifying classiﬁer using adversarial training attack described earlier section iterations. figure shows convergence attack initial adversarially trained classiﬁer values showing ineﬃcacy attack adversarially trained classiﬁer. iterations images valid inducing diﬀerent classiﬁcation average divergence showing classiﬁer indeed signiﬁcantly softened decision boundary. though causing softer decision boundaries adversarial training signiﬁcantly impact classiﬁcation accuracy standard classiﬁer normal input data model achieves accuracy undefended. also feed adversarial inputs generated min-max attack initial classiﬁer adversarially trained classiﬁer observe average classiﬁcation divergence examples drops valid images classiﬁed inconsistently. figure shows randomly selected subset examples respective classiﬁer output. original image label; cross entropy loss label classiﬁer’s prediction clipping lies ball radius centered original images rescaled pixel lies range note running iterations leads decrease accuracy images adversarial. iterations adversarial images adversarial perturbation also reconstructed reconstruction remains adversarial. attributed signal noise ratio results adding adversarial perturbation table table shows percentage random images correctly classiﬁed resnet. accuracy unprotected classiﬁer deep image prior protected classiﬁer clean images reported columns respectively; accuracy unprotected classiﬁer deep image prior protected classiﬁer adversarial examples reported columns respectively. adversarial images constructed performing basic iterative method ∞-norm adversarial perturbation. table reports accuracy resnet adversarial examples constructed using basic iterative method varying protected classiﬁer refers ﬁrst reconstructing images deep image prior method followed classiﬁcation using resnet. figure shows reconstructions obtained using deep image prior method original adversarial images. adversarial images generated using steps basic iterative method deep image prior method iterations mean square error fell currently deluge recent work adversarial attacks defenses. common defense approaches involve modifying training dataset classiﬁer made robust modifying network architecture increase robustness performing defensive distillation idea adversarial training connection robust optimization leads fruitful line defenses. attacker side carlini wagner show diﬀerent ways overcoming many existing defense strategies. approach defending adversarial examples leverages power gans gat-trainer work uses generative models perform adversarial training diﬀerent work without projecting range gan. magnet ape-gan similar idea denoising adversarial noise using generative model figure figure showing reconstructions obtained deep image prior method. rows show original images reconstructions bottom rows show adversarially perturbed images reconstructions. deep image prior method writing paper found related submissions appearing online closely related concurrent work defensegan submitted iclr proposes similar method independently work. however current manuscript validates mnist discuss min-max attack robust process deep image prior method. second related paper pixeldefend main diﬀerence work paper uses pixelcnn generators opposed gans hence projection attack defense processes diﬀerent. work demonstrates possibility resisting adversarial attacks using generative model. propose invert-and-classify algorithm based idea projecting inputs range trained generative adversarial network classiﬁcation. projection performed using gradient descent z-space non-diﬀerentiable process. demonstrate mechanism’s ability resist oﬀ-the-shelf speciﬁcally designed ﬁrst-order black-box attacks. then crafted min-max optimization demonstrate still adversarial images range gan. points close induce drastically diﬀerent classiﬁcations. points show classiﬁer tremendously conﬁdent disagree human judgement. show solve problem adversarially training inputs obtain robust model displays natural uncertainty around decision boundaries. finally cases pre-trained generative model available propose deep image prior defense. relies structural prior given untrained generator defend adversarial examples. show allows defense imagenet dataset robust ﬁrst-order methods unprotected classiﬁer.", "year": 2017}