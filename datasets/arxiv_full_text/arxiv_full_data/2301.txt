{"title": "Learning to Make Predictions on Graphs with Autoencoders", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a densely connected autoencoder architecture capable of learning a joint representation of both local graph structure and available external node features for the multi-task learning of link prediction and node classification. To the best of our knowledge, this is the first architecture that can be efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification. We provide comprehensive empirical evaluation of our models on a range of challenging benchmark graph-structured datasets, and demonstrate significant improvement in accuracy over related methods for graph representation learning. Code implementation is available at https://github.com/vuptran/graph-representation-learning", "text": "abstract examine fundamental tasks associated graph representation learning link prediction semi-supervised node classification. present densely connected autoencoder architecture capable learning joint representation local graph structure available external node features multi-task learning link prediction node classification. best knowledge first architecture efficiently trained end-to-end single learning stage simultaneously perform link prediction node classification. provide comprehensive empirical evaluation models range challenging benchmark graph-structured datasets demonstrate significant improvement accuracy related methods graph representation learning. code implementation available https//github.com/vuptran/graph-representation-learning. growing ubiquity. work examine task learning make predictions graphs broad range real-world applications. specifically study canonical subtasks associated graph-structured datasets link prediction semisupervised node classification graph partially observed edges nodes learning task predict labels edges nodes. real-world applications input graph network nodes representing unique entities edges representing relationships entities. further labels nodes edges graph often correlated exhibiting complex relational structures violate general assumption independent identical distribution fundamental traditional machine learning. therefore models capable exploiting topological present densely connected autoencoder architecture capable learning shared representation latent node embeddings local graph topology available explicit node features lpnc. resulting autoencoder models many useful applications across multiple domains including analyses biological metabolic networks drug-target interaction bibliographic networks social networks facebook terrorist networks communication networks cybersecurity recommender systems knowledge bases freebase dbpedia wikidata extreme class imbalance link prediction number known present positive edges often significantly less number known absent negative edges making difficult reliably learn rare examples; learn complex graph structures edges directed undirected weighted unweighted highly sparse occurrence and/or consisting multiple types. useful model versatile address variety graph types including bipartite graphs; incorporate side information nodes sometimes described covariate features called side information could encode information complementary topological features input graph. explicit data nodes edges always readily available considered optional. useful model able incorporate optional side information nodes and/or edges whenever available potentially improve lpnc accuracy; efficiency scalability real-world graph datasets usually contain large numbers nodes and/or edges. essential model memory computationally efficient achieve practical utility real-world applications. contribution work simple versatile autoencoder architecture addresses technical challenges. demonstrate autoencoder models handle extreme class imbalance several link prediction tasks; learn expressive latent features nodes topological structures sparse bipartite graphs directed and/or weighted edges; flexible incorporate explicit side features nodes optional component improve predictive performance help mitigate cold-start problem utilizes extensive parameter sharing reduce memory footprint computational complexity leveraging available gpu-based implementations improved efficiency scalability. further autoencoder architecture trained end-to-end joint multi-task learning link prediction node classification tasks. best knowledge first architecture capable performing simultaneous link prediction node figure schematic depiction local neighborhood graph autoencoder left partially observed input graph known positive links known negative links nodes; nodes connected unknown status links. middle symmetrical densely connected autoencoder parameter sharing trained end-to-end learn node embeddings adjacency vector multi-task representation. right exemplar task-specific outputs link prediction node classification. classification single learning stage. lastly conduct comprehensive evaluation proposed autoencoder architecture several benchmark graph-structured datasets illustrating wide range lpnc applications. numerical experiments validate efficacy models showing significant improvement accuracy previous related methods specifically designed link prediction and/or node classification. characterize proposed autoencoder architecture schematically depicted figure lpnc formalize notation used paper. input autoencoder graph nodes. graph represented adjacency matrix rn×n partially observed graph unk}n×n denotes known present positive edge denotes known absent negative edge denotes unknown status edge. general input autoencoder directed undirected weighted unweighted and/or bipartite graphs. however remainder paper throughout numerical experiments assume undirected symmetric graphs binary edges maintain parity previous related work. optionally given matrix available explicit node features i.e. side information rn×f autoencoder model learn low-dimensional latent variables nodes rn×d produce approximate reconstruction output error minimized thereby preserving global graph structure. paper capital variables denote matrices lower-case variables denote vectors. example mean matrix research link prediction attempts answer principal question given entities connection them? focus structural link prediction problem task compute likelihood unobserved missing edge exists nodes partially observed input graph. comprehensive survey link prediction include structural temporal link prediction using unsupervised supervised models wang adjacency vector link prediction graph topology contains local neighborhood node. proposed autoencoder architecture comprises non-linear transformations summarized component parts encoder decoder stack layers encoder part derive d-dimensional latent feature representation node stack layers decoder part obtain approximate reconstruction output resulting four-layer autoencoder architecture. concisely hidden representations encoder decoder parts computed follows choice non-linear element-wise activation function rectified linear unit relu max. last decoder layer computes linear transformation score missing links part reconstruction. constrain autoencoder symmetrical shared parameters encoder decoder parts resulting almost less free parameters unconstrained architecture. parameter sharing powerful form regularization helps improve learning generalization widely cited main motivation behind dating back caruana recently yang hospedales notice bias units share link prediction node features optionally matrix explicit node features rn×f available concatenate obtain augmented adjacency matrix perform encoder-decoder transformations usual. refer variant αlongae. notice augmented adjacency matrix longer square symmetric. intuition behind concatenation node features enable shared representation graph node features throughencoding-decoding transformations tied parameters idea inspired recent work vukoti´c successfully applied symmetrical autoencoders parameter sharing multi-modal cross-modal representation learning textual visual features. inference complexity αlongae number nodes dimensionality node features dimensionality hidden layer. practice usually choose rarely exceeds thus overall complexity autoencoder linear number nodes. inference learning forward pass inference model takes input adjacency vector computes reconstructed output note highly sparse percent edges missing experiments dense reconstructed output contains predictions missing edges. parameters learned backpropagation. backward pass estimate minimizing masked balanced cross-entropy loss allows contributions parameters associated observed edges sedhain moreover exhibit extreme class imbalance known present absent links common link prediction problems. handle class imbalance defining weighting factor used multiplier positive class cross-entropy loss formulation. approach referred balanced crossentropy. approaches class imbalance include optimizing ranking loss recent work focal loss. single example reconstructed output compute mbce loss follows absent links hadamard product sigmoid autoencoder architecture applied vector augmented adjacency matrix however final decoder layer slice reconstruction ˆ¯ai outputs corresponding reconstructed example original adjacency matrix corresponding reconstructed example matrix node features. learning optimize concatenation graph topology side node features compute losses reconstructed outputs separately different loss functions. motivation behind design maintain flexibility handle different input formats; input usually binary input binary real-valued both. work enforce inputs range simplicity improved performance compute augmented αmbce loss follows αlongae model also used perform efficient information propagation graphs task semi-supervised node classification. given augmented adjacency vector autoencoder learns corresponding node embeddings obtain optimal reconstruction. intuitively encodes vector latent features derived concatenation graph node features used predict label node. multi-class classification directly decode using softmax activation function learn probability distribution node labels. precisely predict node labels following transformation softmax real-world applications small fraction nodes labeled. semisupervised learning advantageous utilize unlabeled examples conjunction labeled instances better capture underlying data patterns improved learning generalization. achieve jointly training autoencoder masked softmax classifier collectively learn node labels minimizing combined losses node labels node belongs class ˆyic softmax probability node belongs class lmbce loss defined autoencoder boolean function maski node label otherwise maski notice configuration perform multi-task learning link prediction semi-supervised node classification simultaneously. field graph representation learning seeing resurgence research interest recent years driven part latest advances deep learning. learn mapping encodes input graph low-dimensional feature embeddings preserving original global structure. hamilton succinctly articulate diverse previously proposed approaches graph representation learning graph embedding belonging within unified encoder-decoder framework. section summarize three classes encoder-decoder models related current work matrix factorization autoencoders graph convolutional networks roots dimensionality reduction gained popularity extensive applications collaborative filtering models take input matrix learn shared linear latent representation rows columns encoder step bilinear decoder based inner product ricj produce reconstructed matrix mathematically similar link prediction goal essentially matrix completion. menon elkan proposed model capable incorporating side information nodes and/or edges demonstrate strong link prediction results several challenging network datasets. kuchaiev ginsburg related structural deep network embedding model wang link prediction task. similar sdne models rely autoencoders learn non-linear node embeddings local graph neighborhoods. however contrast sdne models several important distinctions enhancements masked cross-entropy loss works better squared error; leverage extensive parameter sharing encoder decoder parts enhance representation learning; αlongae model optionally concatenate side node features adjacency matrix improved link prediction performance; αlongae model trained end-to-end multi-task learning link prediction semi-supervised node classification sdne capable finally gcns recent class algorithms based convolutional encoders learning node embeddings. model motivated localized first-order approximation spectral convolutions layer-wise information propagation graphs. similar αlongae model model learn hidden layer representations encode local graph structure features nodes. choice decoder depends task. link prediction bilinear inner product used semi-supervised node classification softmax activation function employed. model provides end-to-end learning framework scales linearly number graph edges demonstrated achieve state-of-the-art results number graph-structured datasets lpnc. evaluate proposed autoencoder models range challenging graph-structured datasets spanning multiple application domains previous graph embedding methods achieved strong results lpnc. datasets summarized table include networks protein interactions metabolic pathways military conflict countries u.s. powergrid scientific publication citations cora citeseer pubmed databases. {protein metabolic conflict powergrid} reported menon elkan {cora citeseer pubmed} reported kipf welling empirically compare autoencoder models following strong baselines designed specifically lpnc graph autoencoder based closely follow experimental protocol baseline conduct -fold cross-validation splits {protein metabolic conflict} split powergrid. {cora citeseer pubmed} train/validation/test splits provided kipf welling validation split used hyperparameter tuning. hyperparameters include mini-batch size dimensionality hidden layers dropout rate. general strive keep similar hyperparameters across datasets highlight consistency models. implement autoencoder architecture using keras tensorflow-gpu backend along several additional details. diagonal elements adjacency matrix interpretation every node connected itself. impute missing elements adjacency matrix better performance. note imputed elements contribute masked loss computations. number hidden units autoencoder apply mean-variance normalization relu activation layer help improve link prediction performance compensates noise training test instances normalizing activations zero mean unit variance. enables fast efficient learning shown effective cardiac semantic segmentation speech recognition training apply dropout regularization throughout architecture mitigate overfitting depending sparsity input graph. link prediction dropout also applied input layer produce effect similar using denoising autoencoder. denoising technique previously employed link prediction chen zhang initialize weights according scheme described glorot bengio apply weight decay regularization. employ adam gradient descent optimization fixed learning rate link prediction train epochs using mini-batch size samples. node classification train epochs using mini-batch size samples. utilize early stopping form regularization time model shows signs overfitting. link prediction table shows comparison autoencoder models matrix factorization model proposed menon elkan link prediction without node features. evaluation metric area curve results reported mean -fold cross-validation. train test splits randomly sampled exhibiting extreme class imbalance. comparison choose best results reported model. table comparison performance link prediction proposed autoencoder models previous model. number format mean value ∗these results incorporated additional edge features link prediction leave future work. featureless link prediction longae model marginally outperforms across board although results statistically significant {protein metabolic conflict}. consistent results observe incorporating external node features provides boost link prediction accuracy especially protein dataset. metabolic conflict also come external edge features exploited model performance gains. leave task combining edge features future work. node conflict three features unable significantly boost link prediction accuracy. powergrid node features results respective rows. table summarizes performances autoencoder models related graph embedding methods link prediction without node features. addition also report average precision scores model held-out test containing percent randomly sampled positive links number negative links. show mean standard error runs random initializations fixed data splits. results baseline methods taken kipf welling pick best performing models comparison. similar model graph embedding methods combine side node features always produce boost link prediction accuracy. comparison significantly outperform best graph embedding methods much percent without node features. αlongae model achieves competitive link prediction performance compared best model kipf welling pubmed. table comparison performances link prediction autoencoder models related node embedding methods. number format mean value †denotes best performing model presented kipf welling node classification results semi-supervised node classification summarized table closely follow experimental setup kipf welling provided train/validation/test splits. accuracy performance evaluated held-out test examples. optimize hyperparameters validation examples. train contains examples class. methods complete adjacency matrix available node features learn latent embeddings node classification. comparison train test αlongae model data splits runs random weight initializations report mean accuracy. kipf welling report mean results data splits runs random initializations. baseline methods taken yang comparison αlongae model achieves competitive performance compared model cora dataset outperforms baseline methods citeseer pubmed datasets. multi-task learning lastly report lpnc results obtained αlongae model setting runs random initializations. scenario αlongae model takes input incomplete graph percent positive edges number negative edges missing random available node features simultaneously produce predictions missing edges labels nodes. table shows efficacy αlongae model compared best performing task-specific link prediction node classification models require complete adjacency matrix input. link prediction multi-task αlongae achieves competitive performance task-specific αlongae significantly outperforms best model kipf welling cora citeseer datasets. node classification multi-task αlongae best performing model across board trailing behind model cora dataset. table link prediction node classification performances obtained αlongae model multi-task learning setting. link prediction performance combined average scores. accuracy metric used node classification performance. experiments show simple autoencoder architecture parameter sharing consistently outperforms previous related methods range challenging graphstructured benchmarks link prediction node classification tasks. link prediction observe combining available node features always produces significant boost predictive performance. observation reported menon elkan kipf welling among others. intuitively expect topological graph features provide complementary information present node features combination feature sets improve predictive power. although explicit node features always readily available link prediction model capable incorporating optional side information broader applicability. autoencoder models also perform favorably well task semi-supervised node classification. models capable learning expressive non-linear node embeddings decoded softmax activation function yield accurate node labels. efficacy proposed autoencoder evident especially pubmed dataset label rate efficacy likely result parameter sharing used autoencoder architecture provides regularization help improve representation learning generalization. lastly autoencoder architecture amenable multi-task learning joint representation link prediction node classification enabled reinforced parameter sharing. aims exploit commonalities differences across multiple tasks find shared representation result improved performance task-specific metric. work show multi-task model improves node classification accuracy learning predict missing edges time. multitask model broad practical utility address real-world applications input graphs missing edges node labels. work studied task graph representation learning using densely connected autoencoders showed outperform related methods accuracy performance range real-world graph-structured datasets link prediction semi-supervised node classification. success models primarily result extensive parameter sharing encoder decoder parts coupled capability learn expressive non-linear latent node representations local graph neighborhoods explicit node features. work provides end-to-end framework used make accurate meaningful predictions variety complex graph structures wide range real-world applications.", "year": 2018}