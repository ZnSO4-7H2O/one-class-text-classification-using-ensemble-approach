{"title": "Generative learning for deep networks", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.", "text": "learning taking account full distribution data referred generative feasible deep neural networks model conditional distribution outputs given inputs. current solutions either based joint probability models facing difﬁcult estimation problems learn separate networks mapping inputs outputs vice-versa propose intermediate approach. first show forward computation dnns logistic sigmoid activations corresponds simpliﬁed approximate bayesian inference directed probabilistic multi-layer model. connection allows interpret probabilistic model output hidden units given input. second propose order recognition generation networks consistent joint model data weights recognition generator network related transposition. demonstrate tentative experiment coupled pair learned generatively modelling full distribution data enough capacity perform well recognition generation. neural networks logistic sigmoid activations look similar bayesian networks structure logistic conditional distributions sigmoid belief networks however hidden units deterministic take real values hidden units bayesian networks binary random variables associated distribution. given enough capacity training data models estimate posterior distribution output arbitrary well. besides somewhat different modelling properties principled difference stochastic models possible pose least theoretically number inference learning problems missing data marginalizing latent unobserved variables. unfortunately even forward inference bayesian networks requires methods sampling optimization variational approximation. likewise tightly coupled graphical models deep boltzmann machine deep belief networks practically computations needed e.g. computing marginal posterior probabilities tractable sense approximations typically involve sampling optimization. paper propose stacked conditional independent model views model deﬁned. bayesian network logistic conditional distributions. other assuming conditional probabilities general bayesian network factor parent nodes normalising factor. binary units necessary implies conditional probabilities logistic. noteworthy form conditional probabilities neural probabilistic models restricted boltzmann machine etc. units arranged layers typical layered bayesian network viewed markov chain state space variables layer. view necessary assumptions summarized property termed strong conditional independence forward conditional transition probabilities layers factorize dimensions input output state spaces transition normalising factor. making assumption show simple approximate bayesian inference dcim recovers main constructive elements dnns sigmoid softmax activations real-valued variables corresponding expectations binary random variables. interpretation view performing approximate inference efﬁciently. knowledge relationship established before. approximation conditional likelihood learning dcim equivalent performed back-propagation. second objective propose alternative generative learning approach dnns. number recent works prior work pair deterministic recognition stochastic generator networks trained. denote input recognition network speciﬁc image output recognition network latent state. although networks often taken symmetric structure parameters decoupled. stochastic generator network typically generate samples cannot directly evaluate posterior distribution gradient thereof requiring variational sampling-based approximations. methods proposed assume samples generated latent state must fall close together image space. prohibits using categorical latent spaces digit class mnist digits class naturally look differently. instead model’s continuous latent space used ﬁxing point deﬁnes class shape digit. works thus restricted unsupervised learning. given full training data pairs recognition network could learn distribution generator network could principle learn distribution link dcim question dcims modelling conditional distributions consistent i.e. correspond implicitly modelled case data hidden layers. x)/q functions cannot strictly satisﬁed strong conditional independence assumptions observe terms ratio cancel weights recognition network transposed network models therefore efﬁciently represented share parameters learned simultaneously using estimator similar pseudo-likelihood. link dcim approximately compute posterior distribution generator network given sample inner layer approximation reasonable posterior expected single mode reconstructing image lower level features. thus fully partially avoid sampling generator model. demonstrate tentative experiment coupled pair learned simultaneously modelling full distribution data enough capacity perform well recognition generation. outline section consider strongly conditional independent model layers derive model serve building block dcim formally introduced section section consider coupled pairs dcims propose novel approach generative learning. section discuss connections related work. section propose proof-of-concept experiments generative learning using inference sampling single layer. collections discrete random variables joint distribution conditional distribution strongly conditional independent factors gij. without loss generality written functions arbitrary denote corresponding normalization constants. sampling easy computing marginalizing tractable even factorizes components consider following approximation marginal distribution computing expectation w.r.t. gives constant term ﬁrst order term vanishes. example assume random variables collections }-valued. function binary variables written yjwjixi bjyj cixi terms cixi cancel normalization remark joint model strongly conditional independent above. general conditional independent. conditional distributions strongly conditional independent joint model restricted boltzmann machine same bipartite graph. observation assume joint distribution deep boltzmann machine i.e. layered d-partite graph. neither forward conditionals backward conditionals independent. seen follows. joint distribution model written w.l.o.g. therefore consider similar different class forward conditionals independent. deﬁnition joint distribution sequence collections binary valued random variables deep conditional independent model form dcim thus seen inhomogeneous markov chain model high dimensional state spaces however unspeciﬁed distribution given model realisation posterior conditional distribution computed applying approximation recursively leads recursion obvious computation dcim exactly forward computation corresponding dnn. hence discriminative learning dcim maximizing conditional likelihood training samples means learn corresponding loss. state variables model equivalent bayesian network preceding layer conditional distributions form i.e. logistic. networks proposed neal name sigmoid belief networks simpler alternative boltzmann machines. derive model strong conditional independence assumption establishes link deep boltzmann machines graphical models factorizing d-partite graph. generative learning deep networks learn dcim generatively given i.i.d. training data necessary specify joint model choose appropriate estimator. theory dcim model completed joint model specifying prior distribution images maximum likelihood estimator applied. however realistic model complex multi-modal distribution closed form. propose circumvent problem applying following bidirectional conditional likelihood estimator avoids model explicitly. deﬁnition dcim particularly fact dcim also markov chain model follows reverse conditional distribution factorises similar i.e. holds completing dcim joint distribution. unfortunately however reverse conditionals dcim independent. follows argument similar used observation order resolve problem propose consider pairs tightly connected dcims forward model reverse model holds realisation functions already know impossible achieve retaining conditional independence both forward backward conditionals. therefore propose choose parameters models equation fulﬁlled close possible. substituting binary case gives denote partition functions respectively. readily seen terms exponentials cancel holds weights holds biases. remaining cancelling terms partition functions conditional distributions. summarising pair dcims share structure well weights biases. therefore represented single learned simultaneously estimator reads since both forward backward conditionals strongly independent approximation applied computing probabilities remark model consists layer pair deﬁne single joint distribution i.e. bipartite graph. estimator becomes pseudo-likelihood estimator. brieﬂy discuss relations proposed model models mentioned introduction. connection bayesian networks neural networks injected noise used different ours. relates equivalent representations stochastic models relate stochastic deterministic models. similar work hinton uses constraint generator recognition networks related transposition weights unsupervised layer-by-layer pre-training phase motivated study deep boltzmann machine. supervised ﬁne-tuning models decoupled. proposed model course related classical auto-encoders least technically. learning approach generative contrast auto-encoders supervised. moreover decoding part case constrained parameters encoding part model proposed generative learning approach undoubtedly related generative adversarial networks proposed goodfellow similar them reverse part model aims generating data. contrast model uses speciﬁc noise space conditions image distribution class. randomness generating part comes solely reverse markov chain. believe sufﬁce modelling rich distributions input space. however expect approximation used learning might impose limitations expressive power model part compared gans another difference model learned jointly twin i.e. forward part rather competition adversarial. ﬁrst experiment simple proof concept. trained small network standard mnist dataset learning approaches discriminative generative one. sigmoid activation chosen last layer used soft-max activation i.e. considering nodes categorical variable. standard optimiser without drop-out regularisers used searching maximum objective function. fig. shows training validation accuracies approaches functions iterations. clearly seen combined objective generative learning imposes rather insigniﬁcant drop classiﬁcation accuracy. fig. also shows training validation loss backward model. asses reverse model learned generative approach sampled images classes models. results shown fig. sampled image also applied forward model classify classes highest probability along values probability shown sampled image. clearly seen generative learning approach found model parameters yields simultaneously good classiﬁcation accuracy ability generate images average digits. hand also visible fig. generative model part learned mean shapes digits. figure images sampled form reverse model part along classiﬁcation probability values shows images sampled class. left images obtained recursive sampling starting class layer till image layer. right images obtained sampling layer recursively computing probabilities till image layer. second experiment intends analyse behaviour proposed generative learning approach convolutional neural networks. learned deep architecture mnist data. components triplets denote window size stride kernel number convolutional layers. singletons denote size dense layers. sigmoid activation used last layer. using objective achieves validation accuracy forward part loss reverse model part. observed neither over-ﬁtting vanishing gradient effects. conjecture reverse learning task serves strong regulariser. left tableaux fig. shows images sampled reverse model part. again better visibility sampled distribution recursively computed probabilities image layer applying approximation clearly seen learned generative model part able capture multi-modal image distribution. conjecture possible reasons simplicity approximation especially applied learning reverse model part. analyse problem considered somewhat different learning objective model part sampled current model learned objective achieved validation accuracy validation loss reverse model part. images sampled model described above shown right tableaux fig. clearly seen model captured modes image distribution i.e. different writing styles digits. experiments presented paper carried using keras tensorﬂow figure images sampled generatively learned cnns mnist data. better visibility images obtained sampling layer recursively computing probabilities till image layer. left learning method described sec. right alternative learning method described sec. proposed class probabilistic models leads statistical interpretation dnns neurons become random variables. contrast boltzmann machines class allows efﬁcient approximation computing distribution output variables conditioned realisation input variables. computation becomes identical recursive forward computation dnn. second objective paper design generative learning approach dcims reached partially. proposed approach variants allow learn forward backward model parts simultaneously. however achievable expressive power backward part model still inferior gans. notwithstanding this believe presented approach opens interesting research directions. analyse proposed approximation search better ones leading better activation functions. another question direction statistical interpretation relu activation expectation possibly continuous random variable. different direction calls generalising dcims introducing interactions neurons layers. would lead deep crfs. envision learn models least discriminatively using e.g. recent results efﬁcient marginal approximations log-supermodular crf. shekhovtsov supported toyota motor europe fikar supported czech technical university prague grant sgs//ohk/t/ flach supported czech science foundation grant chollet keras. https//github.com/fchollet/keras. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages kingma welling auto-encoding variational bayes. corr abs/.. kingma welling efﬁcient gradient-based inference transformations bayes nets neural nets. proceedings international conference international conference machine learning volume icml’ pages ii––ii–. jmlr.org. lecun cortes mnist handwritten digit database. grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning icml pages york usa. acm. mnih gregor neural variational inference learning belief networks. proceedings international conference machine learning cycle volume jmlr proceedings pages jmlr.org. rezende mohamed wierstra stochastic backpropagation approximate inference deep generative models. jebara xing editors proceedings international conference machine learning pages jmlr workshop conference proceedings.", "year": 2017}