{"title": "A Flexible Approach to Automated RNN Architecture Generation", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The process of designing neural architectures requires expert knowledge and extensive trial and error. While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components. We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization. Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains. The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.", "text": "martin schrimpf∗ stephen merity* james bradbury richard socher department brain cognitive sciences massachusetts institute technology mschmit.edu salesforce research {james.bradburycxiongrsocher}salesforce.com process designing neural architectures requires expert knowledge extensive trial error. automated architecture search simplify requirements recurrent neural network architectures generated existing methods limited ﬂexibility components. propose domain-speciﬁc language automated architecture search produce novel rnns arbitrary depth width. ﬂexible enough deﬁne standard architectures gated recurrent unit long short term memory allows introduction non-standard components trigonometric curves layer normalization. using different candidate generation techniques random search ranking function reinforcement learning explore novel architectures produced language modeling machine translation domains. resulting architectures follow human intuition perform well targeted tasks suggesting space usable architectures larger previously assumed. developing novel neural network architectures core many recent advances process architecture search engineering slow costly laborious. human experts guided intuition explore extensive space potential architectures even minor modiﬁcations produce unexpected results. ideally automated architecture search algorithm would optimal model architecture given task. many explorations automation machine learning made including optimization hyperparameters various methods producing novel model architectures architecture search ensuring automated methods able produce results similar humans usually requires traversing impractically large search space assuming high quality architectures exist search space all. choice underlying operators composing architecture typically constrained standard across architectures even though recent work found promising results non-standard operators propose meta-learning strategy ﬂexible automated architecture search recurrent neural networks explicitly includes novel operators search. consists three stages outlined figure instantiate versions. candidate architecture generation function produces potential architectures using highly ﬂexible dsl. enforces constraints size complexity generated tree incrementally constructed using either random policy agent. ranking function processes candidate architecture’s recursive neural network predicting architecture’s performance. unrolling representation ranking function also model interactions candidate architecture’s hidden state time. ∗equal contribution. work completed ﬁrst author interning salesforce research. figure generator produces candidate architectures iteratively sampling next node full architectures processed ranking function promising candidates evaluated. results running model baseline experiment used improve generator ranking function. evaluator takes promising candidate architectures compiles dsls executable code trains model speciﬁed task. results evaluations form architecture-performance pairs used train ranking function generator. section describe domain speciﬁc language used deﬁne recurrent neural network architectures. sets search space candidate generator traverse architecture search. comparison zoph produced binary tree matrix multiplications leaves allows broader modeling search space explored. represents single linear layer bias i.e. similarly deﬁne sigmoid operator mult represents element-wise multiplication mult gate operator performs weighted summation inputs deﬁned gate operators applied source nodes input vectors current previous timestep output previous timestep optional long term memory. gate operator required architectures re-use output single sigmoid purposes gating. allowing possible node re-use scope gate ternary operator allows frequent case. hidden state long term memory. value extracted internal node computed producing extended support numbering nodes specifying node extract append node number deﬁnition delimiter. example nodes bold used produce number appended indicating node’s number. nodes numbered bottom left right. domain speciﬁc language entirely generic ﬂexible enough capture standard architectures. includes limited lstm minimal gate unit quasi-recurrent neural network neural architecture search cell simple rnns. many standard non-standard architectures deﬁned using core promise automated architecture search designing radically novel architectures. architectures formed removing human bias search process including operators sufﬁciently explored. expanded include extensions inverses currently used operators instead addition instead multiplication) trigonometric curves activations respectively posenc introduces variable result applying positional encoding according current timestep) optimizations input selu activation function deﬁned klambauer given architecture deﬁnition compile code traversing tree source nodes towards ﬁnal node produce sets source code initialization required node deﬁning weights matrix multiplication forward call runtime. details regarding speed optimizations refer appendix candidate architecture generator responsible producing candidate architectures later ﬁltered evaluated. architectures grown beginning output ordered prevent multiple representations equivalent architectures growing architectures beginning output node operators selected added computation graph depicted figure whenever operator children ﬁlled children ﬁlled order left right. wish place limit height tree force next child source nodes would otherwise exceed maximum height. issue commutative operators deﬁne canonical ordering architecture sorting arguments commutative nodes. special consideration required non-commutative operators gate div. full details refer appendix architectures constructed incrementally node time starting output simplest agent random selects next node operators without internalizing knowledge architecture optima search space. allowing intelligent agent construct architectures would preferable agent learn focus promising directions space possible architectures. agent make intelligent decisions regarding node select next must representation current state architecture working memory direct actions. propose achieving components tree encoder lstm applied recursively node token children weights shared state reset nodes. applied encoded partial architecture predicts action scores operation. sample multinomial encourage exploration epsilon-greedy strategy. components model trained jointly using reinforce algorithm partial architecture contain empty nodes gate) introduce target token indicates node next selected. thus gate) tree encoder understands ﬁrst argument slot ﬁlled. filtering candidate architectures using ranking function even intelligent generator understanding likely performance architecture difﬁcult especially interaction hidden states timesteps. propose approximate full training candidate architecture training ranking network regression architecture-performance pairs. ranking function speciﬁcally constructed allow richer representation transitions ranking function uses architecture-performance samples training data human experts also inject previous best known architectures training dataset. possible on-policy reinforcement learning done using off-policy reinforcement learning additional care complexity required effective given architecture-performance pair ranking function constructs recursive neural network reﬂects nodes candidate architecture one-to-one. sources nodes represented learned vector operators represented learned function. ﬁnal vector output passes linear activation attempts minimize difference predicted real performance. source nodes represented learned vector representations. operators tree treelstm nodes unrolling graph accurately representing strong assumption made vector representation source nodes accurately represent contents source nodes across variety architectures. hold true true ct−. value deﬁned operations within given architecture itself. remedy assumption unroll architecture single timestep replacing relevant graph subgraph. would allow representation understand source nodes access operations applied produce ht−. unrolling useful improving representation essential allowing accurate representation ct−. many small variations possible selecting subgraph activation result substantially different architecture performance. evaluated architecture generation experiments language modeling machine translation computational requirements experiments limited experiment combination generator components. language modeling explore core using randomly constructed architectures directed learned ranking function. machine translation extended construct candidate architectures incrementally using generator without ranking function. evaluating architectures found architecture search wikitext- dataset evaluating proposed novel cell construct layer c-rnn unit hidden size. aggressive gradient clipping performed ensure architectures relu would able train without exploding gradients. weights ranking network trained regression architecture-perplexity pairs using adam optimizer mean squared error hyperparameters training details listed appendix explicit restrictions generated architectures candidate generation phase ﬁlter generated architectures based upon speciﬁc restrictions. include structural restrictions restrictions aimed effectively reducing search space removing likely invalid architectures. gate operations force input forget gate result sigmoid activation. also require cell current timestep previous timestep’s output satisfy requirements rnn. candidate architectures limited nodes number nodes used maximum allowed distance steps. also prevent stacking identical operations. aggressive ﬁlter successfully removes many problematic architectures. problematic architectures include sigmoid activations relu activations matrix multiplications used succession ﬁrst unlikely useful second null operator second activation third mathematically rewritten single matrix multiplication. given candidate architecture deﬁnition contained architecture queried valid subgraphs could generated. subgraphs must contain recurrent must contain three nodes prevent trivial recurrent connections. candidate architecture generated valid subgraph. random architecture search directed learned ranking function candidate architecture deﬁnitions produced random architecture generator beginning search step. full candidate architectures simulated ranking network estimated perplexity assigned each. given relative simplicity small training dataset ranking function retrained previous full training results used estimate next batch candidate architectures. architectures selected full training. selected candidate architectures best perplexity last selected weighted sampling without replacement prioritizing architectures better estimated perplexities. architectures introduced part architecture search valid architectures evaluated architectures used bootstrap architecture vector representations. figure provides visualization architecture search time showing valid architectures. equations produce ﬁrst gate equations produce second gate output ﬁrst gate becomes value passing tanh activation. core used still breaks many human intuitions regarding architectures. formulation gates standard many architectures rest architecture less intuitive. gate produces mixing matrix multiplication current input complex interaction passes multiple matrix multiplications gate tanh activation becoming non-conventional architectures allow become directly usually gating operation. architecture also feature masking output gate like lstm outputs similar poorly language modeling. architecture would able learn without severe instability succumbing exploding gradients intuitively obvious. ﬁnal results experimental setup merity report results penn treebank wikitext- datasets. show standard achieve similar perplexity given setup also implemented tuned based model found strongly underperform compared lstm nascell recurrent highway network full hyperparameters appendix model inan variational lstm augmented loss inan variational lstm augmented loss zilly variational zoph variational cell zoph variational cell melis -layer skip connection lstm merity -layer weight drop lstm nt-asgd -layer weight drop nt-asgd -layer weight drop nt-asgd model inan variational lstm inan variational lstm augmented loss melis -layer lstm melis -layer skip connection lstm merity -layer weight drop lstm nt-asgd -layer weight drop nt-asgd -layer weight drop nt-asgd model uses equal fewer parameters compared models compared against. outperform highly tuned awd-lstm skip connection lstm outperform recurrent highway network nascell penn treebank nascell found using reinforcement learning architecture search speciﬁcally optimized penn treebank. experiments involving extended based generator machine translation domain. candidate architectures produced agent directly used without assistance ranking function. leads different kind generator whereas ranking function learns global knowledge whole architecture agent trimmed towards local knowledge operator ideal next. training details evaluating constructed architectures pre-train generator internalize intuitive priors. priors include enforcing well formed rnns moderate depth restrictions full list priors model details appendix model evaluation architectures parallel optimizing batch receiving results least four architectures. failing architectures return early needed ensure batch contained positive negative results. ensure generator yielded mostly functioning architectures whilst understanding negative impact invalid architectures chose require least three good architectures maximum failing architecture batch. candidate architectures multiple placement options memory gate evaluated possible locations waited received results variations. best architecture result used reward architecture. baseline machine translation experiment details ensure baseline experiment fast enough evaluate many candidate architectures used multik english german machine translation dataset. training consists sentence pairs brieﬂy describe flickr captions. experiments based opennmt codebase attentional unidirectional encoder-decoder lstm architecture speciﬁcally replace lstm encoder architectures designed using extend dsl. hyper-parameters baseline experiment hidden word encoding size layers encoder decoder rnns batch size back-propagation time timesteps dropout input feeding stochastic gradient descent. learning rate starts decays validation perplexity fails improve. training stops learning rate drops analysis machine translation architecture search figure shows relative frequency operator architectures used optimize generator batch. architectures batch absolute number operator occurs divide total number operators architectures batch. batches operators generator prefers time. core intriguingly usage generating early batches. extended operators also operators frequently resulting unstable architectures thus ignored early batches. part training however generator begins successfully using wide variety extended hypothesize generator ﬁrst learns build robust architectures capable inserting varied operators without compromising rnn’s overall stability. since reward function ﬁtting complex unknown generator requires substantial training time generator understand robust architectures structured. however generator seems view extended beneﬁcial given continues using operators. overall generator found architectures out-performed lstm based test bleu score total evaluated architectures best architecture achieved test bleu score respectively compared standard lstm’s multiple cells also rediscovered variant residual networks xt)) highway networks sigmoid every operation core extended made architecture outperformed lstm many architectures found generator would likely considered valid standards current architectures. results suggest space successful architectures might hold many unexplored combinations human bias possibly preventing discovery. table take architectures found automated architecture search multik dataset test iwslt dataset training consists sentence pairs transcribed presentations cover wide variety topics conversational language multik dataset. table model loss bleu multik iwslt’ datasets. architectures generated multik dataset lstm architecture search. perform hyperparameter optimizations either dataset avoid unfair comparisons though initial opennmt hyperparameters likely favored baseline lstm model. dataset larger number sentences vocabulary seen architecture search. architectures achieved higher validation test bleu multik lstm baseline appears architectures transfer cleanly larger iwslt dataset. suggests architecture search either larger datasets begin evaluated multiple datasets produce general architectures. also found correlation loss bleu optimal architectures performing exceptionally well loss sometimes scored poorly bleu. also unclear metrics generalize perceived human quality model thus using qualitatively quantitatively accurate metric likely beneﬁt generator. hyper parameters iwslt model refer appendix architecture engineering long history many traditional explorations involving large amount computing resources extensive exploration hyperparamters approach similar work zoph introduces policy gradient approach search convolutional recurrent neural architectures. approach generating recurrent neural networks slot ﬁlling element-wise operations selected nodes binary tree speciﬁc size. node produce selected slots ﬁlled. slot ﬁlling approach highly ﬂexible regards architectures allows. opposed possible matrix multiplications internal nodes inputs used bottom tree complex representation hidden states unrolling ranking function provides. many similar techniques utilizing reinforcement learning approaches emerged designing architectures q-learning neuroevolution techniques neuroevolution augmenting topologies hyperneat evolve weight parameters structures neural networks. techniques extended producing non-shared weights lstm small neural network evolving structure network introduced ﬂexible domain speciﬁc language deﬁning recurrent neural network architectures represent human designed architectures. ﬂexibility allowed generators come novel combinations tasks. architectures used core operators already used current architectures well operators largely unstudied division sine curves. resulting architectures follow human intuition perform well targeted tasks suggesting space usable architectures larger previously assumed. also introduce component-based concept architecture search instantiated approaches ranking function driven search allows richer representations complex architectures involve long term memory nodes reinforcement learning agent internalizes knowledge search space propose increasingly better architectures. computing resources continue grow automated architecture generation promising avenue future research. jozefowicz zaremba sutskever. empirical exploration recurrent network architectures. proceedings international conference machine learning pages improve running speed cell architectures collect matrix multiplications performed single source node batch single matrix multiplication. example optimization would simplify lstm’s small matrix multiplications large matrix multiplications. allows higher utilization lower cuda kernel launch overhead. exist many possible speciﬁcations result equivalent cell. matrix multiplications applied source node matrix multiplication reaching equivalent result achieved constructing speciﬁc matrix calculating additional equivalences found operator commutative equivalent reordered deﬁne canonical ordering architecture sorting arguments commutative nodes. work nodes sorted according represented string though consistent ordering allowable. core non-commutative operation gate ﬁrst arguments sorted input gate must remain original position. extended operators order sensitive disallow reordering. models trained using stochastic gradient descent initial learning rate training continues epochs learning rate divided validation perplexity improved since last epoch. dropout applied word embeddings outputs layers zaremba rate weights word vectors tmax also tied aggressive gradient clipping performed ensure architectures relu would able train without exploding gradients. embeddings initialized randomly training candidate architectures experienced exploding gradients perplexity epochs regarded failed architectures. failed architectures immediately terminated. desirable failed architectures still serve useful training examples ranking function. ranking function hidden size treelstm nodes batch size regularization dropout ﬁnal dense layer output interested reducing perplexity error better architectures sample architectures frequently perplexity lower. unrolling architectures proper unroll would replace xt−. found ranking network performed better without substitutions however thus substituted ct−. baseline experiments used architecture search important dictating models eventually generated. example discovered used standard regularization techniques baseline language modeling experiment. analyzing variational dropout would work applied frames importance hyperparameter selection baseline experiment. lstm cells variational dropout performed upon otherwise long term hidden state would destroyed. equation shows ﬁnal gating operation mixes ht−. variational dropout applied equation bc’s hidden state permanently lost information. applying variational dropout values gates ensures information lost. observation provides good justiﬁcation performing variational dropout baseline experiment given architecture would disadvantaged otherwise. penn treebank language modeling results majority hyper parameters left equal baseline awd-lstm. model trained epochs using nt-asgd learning rate batch size bptt variational dropout input hidden layers output respectively. embedding dropout used. word vectors dimensionality hidden layers dimensionality used layers weight drop applied recurrent weight matrices. activation regularization temporal activation regularization used. gradient clipping finetuning additional epochs. wikitext- language modeling results parameters kept equal penn treebank experiment. model total epochs epochs ﬁnetuning. penn treebank language modeling results hyper parameters equal experiment hidden size weight drop learning rate gradient clipping temporal activation regularization model epochs epochs ﬁnetuning. wikitext- language modeling results hyper parameters kept equal penn treebank experiment. model epochs epochs ﬁnetuning weight drop reduced represent architecture encoder traverse architecture recursively starting root node. node operation tokenized embedded vector. lstm applied vector well result vectors current node’s children. note lstm every node reset hidden states nodes always starts scratch every child node. choose speciﬁc action multinomial applied action scores. encourage exploration randomly choosing next action according epsilon-greedy strategy reward expresses well architecture performed computed based validation loss. re-scale according soft exponential last increases rewarded more. speciﬁc reward function .×−) follows earlier efforts keep reward zero pre-training generator list priors obvious reasons deﬁnitions would within result table. give indication ﬂexibility however ranging minimal quite complex -deep nested gate layernorm models trained iwslt’ english german dataset hyper parameters kept largely equivalent default openmt lstm baseline. models unidirectional dimensionality word vectors hidden states required many generated architectures residual nature. models layers deep utilized batch size standard dropout layers. learning rate began decayed whenever validation perplexity failed improve. learning rate fell training ﬁnished. models evaluated using batch size ensure padding impact results. also brieﬂy explored automated architecture generation decoder well encoder decoder jointly yielded good results interesting architectures performances fell short promising approach focusing encoder. given differences generated architectures usage components likely impact long term hidden state models began explore progression hidden state time. activations differs substantially architectures even though parsing input. input features likely captured different ways also stored processed differently suggests ensembles highly heterogeneous architectures effective. figure ﬁgure shows progress generator time highlighting switches exploitation exploitation valid architectures shown. higher reward better x-axis showing progress time.", "year": 2017}