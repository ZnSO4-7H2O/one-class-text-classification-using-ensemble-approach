{"title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in  Generative Models", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.", "text": "lack characterization explicit density gans however problematic reasons. several application areas deep generative models rely density estimates; instance count based exploration strategies based density estimation using generative models recently achieved state-of-the-art performance challenging reinforcement learning environments secondly makes quantitative evaluation generalization performance models challenging. typical evaluation criteria based ad-hoc sample quality metrics address issue since possible generate good samples memorizing training data missing important modes distribution alternatively density estimates based approximate inference techniques annealed importance sampling non-parameteric methods kernel density estimation computationally slow crucially rely assumptions gaussian observation model likelihood could lead misleading estimates shall demonstrate paper. sidestep issues propose flow-gans generative adversarial network normalizing generator. flow-gan generator transforms prior noise density model density sequence invertible transformations. using invertible generator flowgans allow tractably evaluate exact likelihoods using change-of-variables formula perform exact posterior inference latent variables still permitting efﬁcient ancestral sampling desirable properties probabilistic model typical would provide. using flow-gan perform principled quantitative comparison maximum likelihood adversarial learning benchmark datasets viz. mnist cifar-. adversarial learning outperforms sample quality metrics expected based strong evidence prior work log-likelihood estimates adversarial learning orders magnitude worse mle. difference stark simple gaussian mixture model baseline outperforms adversarially learned models sample quality held-out likelihoods. quantitative analysis reveals poor likelihoods adversarial learning explained result ill-conditioned jacobian maadversarial learning probabilistic models recently emerged promising alternative maximum likelihood. implicit models generative adversarial networks often generate better samples compared explicit models trained maximum likelihood. gans sidestep characterization explicit density makes quantitative evaluations challenging. bridge propose flow-gans generative adversarial network perform exact likelihood evaluation thus supporting adversarial maximum likelihood training. trained adversarially flow-gans generate high-quality samples attain extremely poor log-likelihood scores inferior even mixture model memorizing training data; opposite true trained maximum likelihood. results mnist cifar- demonstrate hybrid training attain high held-out likelihoods retaining visual ﬁdelity generated samples. highly expressive parametric models enjoyed great success supervised learning learning objectives evaluation metrics typically well-speciﬁed easy compute. hand learning objective unsupervised settings less clear. fundamental level idea learn generative model minimizes notion divergence respect data distribution. minimizing kullback-liebler divergence data distribution model instance equivalent performing maximum likelihood estimation observed data. maximum likelihood estimators asymptotically statistically efﬁcient serve natural objectives learning prescribed generative models contrast alternate principle recently attracted much attention based adversarial learning objective generate data indistinguishable training data. adversarially learned models generative adversarial networks sidestep specifying explicit density data point belong class implicit generative models copyright association advancement artiﬁcial intelligence rights reserved. resolve dichotomy perceptually good-looking samples expense held-out likelihoods case adversarial learning propose hybrid objective bridges implicit prescribed learning augmenting adversarial training objective additional term corresponding loglikelihood observed data. hybrid objective achieves intended effect smoothly trading-off goals case cifar- regularizing effect mnist outperforms adversarial learning held-out likelihoods sample quality metrics. overall paper makes following contributions propose flow-gans generative adversarial network invertible generator perform efﬁcient ancestral sampling exact likelihood evaluation. propose hybrid learning objective flow-gans attains good log-likelihoods generates highquality samples mnist cifar- datasets. begin review maximum likelihood estimation adversarial learning context generative models. ease presentation distributions w.r.t. arbitrary unless otherwise speciﬁed. upper-case denote probability distributions assume admit absolutely continuous densities reference measure consider following setting learning generative models. given data rd}m sampled i.i.d. unknown probability density pdata interested learning probability density denotes parameters model. given parameteric family models typical approach learn minimize notion divergence pdata choice divergence optimization procedure dictate learning leading following objectives. maximum likelihood estimation maximum likelihood estimation minimize kullback-liebler divergence data distribution model distribution. formally learning objective expressed hence evaluating learning objective requires ability evaluate model density models provide explicit characterization likelihood function referred prescribed generative models adversarial learning generative model learned optimize divergence notions beyond divergence. large family divergences conveniently expressed ex∼pθ ex∼pdata denotes parameters appropriate real-valued functions parameterized different choices lead variety fdivergences jenson-shannon divergence integral probability metrics wasserstein distance. instance objective proposed goodfellow also cast form below denotes parameters neural network function refer reader details possible choices divergences. importantly monte carlo estimate objective requires samples model. hence model allows tractable sampling used evaluate following minimax objective result even differentiable implicit models provide characterization model likelihood allow tractable sampling learned adversarially optimizing minimax objectives form given adversarial learning latent variable models statistical perspective maximum likelihood estimators statistically efﬁcient asymptotically hence minimizing divergence natural objective many prescribed models however models allow well-deﬁned tractable easy-to-optimize likelihood. example exact likelihood evaluation sampling tractable directed fully observed models bayesian networks autoregressive models hence usually trained maximum likelihood. undirected models hand provide unnormalized likelihoods sampled using expensive markov chains. hence usually learned approximating likelihood using methods contrastive divergence pseudolikelihood likelihood generally intractable compute latent variable models popular class latent variable models learned adversarially consist generative adversarial networks gans comprise pair generator discriminator networks. generator deterministic function differentiable respect parameters function takes input source randomness sampled tractable prior density transforms sample forward pass. evaluating likelihoods assigned challenging model density speciﬁed implicitly using prior density generator function fact likelihood data point ill-deﬁned prior distribution deﬁned support smaller support data distribution. gans typically learned adversarially help discriminator network. discriminator another real-valued function differentiable respect parameters given discriminator function express functions compositions divergence-speciﬁc functions. instance wasserstein optimizes following objective discussed above generative adversarial networks tractably generate high-quality samples intractable ill-deﬁned likelihoods. monte carlo techniques non-parameteric density estimation methods around assuming gaussian observation model generator. assumption alone sufﬁcient quantitative evaluation since marginal case would intractable requires integrating latent factors variation. would require approximate inference computational challenge highdimensional distributions. circumvent issues propose generative adversarial networks flow-gan consists pair generator-discriminator networks generator speciﬁed normalizing model normalizing model speciﬁes parametric transformation prior density another density space nonnegative reals. generator transformation invertible exists inverse function using change-of-variables formula letting have denotes jacobian formula applied recursively compositions many invertible transformations produce complex ﬁnal density. hence evaluate optimize loglikelihood assigned model data point long prior density tractable determinant jacobian evaluated efﬁciently computed. evaluating likelihood assigned flow-gan model requires overcoming major challenges. first requiring generator function reversible imposes constraint dimensionality latent variable match data thereafter require transformations various layers generator invertible overall composition results invertible secondly jacobian high-dimensional distributions however computationally expensive compute. transformations designed jacobian upper lower triangular matrix determinant easily evaluated product diagonal entries. consider family transformations. volume preserving transformations. here jacobian transformations unit determinant. example nice model consists several layers performing location transformation layer diagonal scaling matrix nonzero determinant. non-volume preserving transformations. determinant jacobian transformations necessarily unity. example real-nvp layers performs location scale transformations brevity direct reader dinh krueger bengio dinh sohl-dickstein bengio speciﬁcations nice real-nvp respectively. crucially volume preserving non-volume preserving transformations invertible determinant jacobian computed tractably. learning objectives flow-gan likelihood well-deﬁned computationally tractable exact evaluation even expressive volume preserving non-volume preserving transformations. hence flow-gan trained maximum likelihood estimation using case discriminator redundant. additionally perform ancestral sampling like regular whereby sample random vector transform model generated sample makes possible learn flow-gan using adversarial learning objective natural question adversarial learning given statistically efﬁcient asymptotically besides difﬁculties could arise optimization optimality holds model misspeciﬁcation generator i.e. true data distribution pdata member parametric family distributions consideration generally case high-dimensional distributions hence choice learning objective becomes largely empirical question. unlike models flow-gan allows maximum likelihood adversarial learning hence investigate question experimentally. criteria evaluation based held-out loglikelihoods sample quality metrics. focus natural images since allow visual inspection well quantiﬁcation using recently proposed metrics. good generative model generalize images outside training data assign high log-likelihoods held-out data. inception mode scores standard quantitative measures quality generated samples natural images labelled datasets softmax probability labels assigned pretrained classiﬁer overall distribution labels generated samples intuition conditional distribution entropy good looking images marginal distribution high entropy ensure sample diversity. hence generative model perform well metric divergence distributions large. mode score given below modiﬁes inception score take account distribution labels training data klp)) compare learning flow-gans using adversarial learning mnist dataset handwritten digits cifar- dataset natural images normalizing generator architectures chosen nice real-nvp mnist cifar- respectively. wasserstein distance choice divergence optimized lipschitz constraint critic imposed penalizing norm gradient respect input discriminator based dcgan architecture choices among current state-of-the-art maximum likelihood estimation adversarial learning greatly stabilize training. experimental setup details provided appendix code reproducing results available https//github.com/ermongroup/flow-gan. evaluation results log-likelihood. log-likelihood learning curves flow-gan models learned using shown figure figure respectively. following convention report negative log-likelihoods nats mnist bits/dimension cifar-. mle. figure normalizing models attain validation nlls gradient updates expected explicitly optimizing objective continued training however could lead overﬁtting train nlls begin diverge validation nlls. adv. surprisingly models show consistent increase validation nlls training progresses shown figure based learning curves disregard overﬁtting explanation since increase nlls observed even training data. training validation nlls closely track suggesting models simply memorizing training data. comparing left right panels figure log-likelihoods attained orders magnitude worse attained sufﬁcient training. finally note wgan loss correlate well estimates. wgan loss stabilizes iterations training nlls continue increase. observation contrast prior work showing loss strongly correlated sample quality metrics sample quality. samples generated advbased models best mode/inception shown figure figure respectively. models signiﬁcantly outperform respect ﬁnal mode/inception scores achieved. visual inspection samples conﬁrms observations made based sample quality metrics. curves monitoring sample quality metrics every training iteration given appendix experiments suggest produce excellent samples assigns likelihoods observed data. however direct comparison loglikelihoods unfair since latter explicitly optimizing desired objective. highlight generating good samples expense likelihoods challenging goal propose simple baseline. compare adversarially learned flow-gan models achieves highest mode/inception score gaussian mixture model consisting isotropic gaussians equal weights centered training points baseline gaussian mixture model bandwidth hyperparameter mixture components optimized lowest validation line search show results cifar figure observations hold mnist well; results deferred appendix overload y-axis figure report nlls sample quality metrics. horizontal maroon cyan dashed lines denote best attainable mode/inception scores corresponding validation nlls respectively attained adversarially learned flow-gan model. clearly attain better sample quality metrics since explicitly overﬁtting training data values bandwidth parameter surprisingly simple also outperforms adversarially learned model respect nlls attained several values bandwidth parameter bandwidth parameters models outperform adversarially learned model loglikelihoods sample quality metrics highlighted using green shaded area. show samples appendix. hence trivial baseline memorizing training data generate high quality samples better held-out log-likelihoods suggesting loglikelihoods attained adversarial training poor. ﬁndings contrast prior work report much better log-likelihoods adversarially learned models standard generator architecture based annealed importance sampling kernel density estimation methods rely approximate inference techniques log-likelihood evaluation make assumptions gaussian observation model hold gans. since flow-gans allow compute exact loglikelihoods evaluate quality approximation made density estimation invertible generators. detailed description methods refer reader prior work consider mnist dataset methods previously applied goodfellow respectively. since inherently rely samples generated evaluate methods hybrid flowgan model checkpoints corresponding best mode scores observed training. table observe produce estimates log-likelihood ground truth accessible exact flow-gan log-likelihoods. even worse ranking log-likelihood estimates obey relative rankings flow-gan estimates explaining log-likelihood trends order explain variation log-likelihoods attained various flow-gan learning objectives investigate distribution magnitudes singular values jacobian matrix several generator functions mnist figure evaluated noise vectors randomly sampled prior density x-axis ﬁgure shows singular value magnitudes scale singular value show corresponding cumulative distribution function value y-axis signiﬁes fraction singular values less results cifar- appendix show similar trend. previous section observed adversarially learning flow-gans models attain poor held-out log-likelihoods. makes challenging models applications requiring density estimation. hand flowgans learned using mode covering generate high quality samples. flow-gan possible trade-off goals combining learning objectives corresponding inductive principles. without loss generality denote minimax objective model hybrid objective flow-gan expressed λex∼pdata summarize results hybrid log-likelihood sample quality evaluation table table mnist cifar- respectively. tables report test log-likelihoods corresponding best validated models highest mode/inception scores observed training. samples generated models best mode/inception scores objective shown figure results cifar- along expected lines hybrid objective interestingly outperforms test log-likelihoods sample quality metrics case mnist. potential explanation objective regularize generalize test turn objective stabilize optimization objective. hence hybrid objective smoothly balance objectives using tunable hyperparameter cases mnist performance tasks could improve result hybrid objective. singular value distribution jacobian invertible generator learned using concentrated narrow range hence jacobian matrix well-conditioned easy invert. case invertible generators learned using wasserstein distance however spread singular values wide hence jacobian matrix ill-conditioned. average determinant jacobian matrices hybrid models −.−. respectively translates trend adv<hybrid<mle. indicates models trying squish sphere unit volume centered latent vector small volume observed space tiny perturbations training well held-out datapoints hence manifest poor log-likelihoods. spite limited representational capacity cover entire space data distribution match invertible generators) prefers learn distribution smaller support. hybrid learning objective however able correct behavior distribution singular value magnitudes matches closely mle. also considered variations involving standard dcgan architectures minimizing wasserstein distance jenson-shannon divergence relative shift distribution singular value magnitudes lower values apparent even cases. model allows efﬁcient likelihood evaluation sampling trained using maximum likelihood adversarial learning. line reasoning explored extent prior work combine objectives prescribed latent variable models vaes adversarial learning however beneﬁts procedures come free since still need form approximate inference handle log-likelihoods. could expensive instance combining introduces additional inference network increases overall model complexity. approach sidesteps additional complexity approximate inference considering normalizing model. trade-off made normalizing model generator function needs invertible generative models vaes requirement. positive side tractably evaluate exact log-likelihoods assigned model data point. normalizing models previously used context maximum likelihood estimation fully observed latent variable models dimensional support distributions learned adversarial learning often manifests lack sample diversity referred mode collapse. prior work mode collapse detected based visual inspection heuristic techniques techniques avoiding mode collapse explicitly focus stabilizing training rather quantitative methods based likelihoods. attempt quantitatively evaluate generative models introduced flow-gan. generative adversarial network allows tractable likelihood evaluation exactly like model. since trained adversarially terms quantitatively evaluate trade-offs involved. observe adversarial learning assigns low-likelihoods training validation data generating superior quality samples. observation perspective demonstrate naive gaussian mixture model outperform adversarially learned models log-likelihood estimates sample quality metrics. quantitative evaluation methods based fail detect behavior poor approximations true log-likelihood analyzing jacobian generator provides insights contrast maximum likelihood estimation adversarial learning. latter tendency learn distributions support lead likelihoods. correct behavior proposed hybrid objective function involves loss terms corresponding adversarial learning. models applications requiring density estimation sample generation exciting direction future work. thankful poole daniel levy helpful discussions. research supported microsoft research fellowship machine learning ﬁrst author grants future life institute grant intel.", "year": 2017}