{"title": "Deep Learning with Sets and Point Clouds", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce a simple permutation equivariant layer for deep learning with set structure.This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST-digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information.", "text": "siamak ravanbakhsh jeff schneider barnab´as p´oczos school computer science carnegie mellon university pittsburgh {mravanbajeff.schneiderbapoczos}cs.cmu.edu introduce simple permutation equivariant layer deep learning structure. type layer obtained parameter-sharing simple implementation linear-time complexity size set. deep permutation-invariant networks perform point-could classiﬁcation mnistdigit summation cases output invariant permutations input. semi-supervised setting goal make predictions instance within demonstrate usefulness type layer set-outlier detection well semi-supervised learning clustering sideinformation. recent progress deep learning witnessed application structured settings including graphs groups sequences hierarchies here introduce simple permutation-equivariant layer deep learning structure primary dataset collection sets possibly different sizes. note instance structure graph image another set. typical machine-learning applications assumption implies entire data-set structure. therefore special treatment structure necessary multiplicity distinct homogeneous sets. here show simple parameter-sharing scheme enables general treatment sets within supervised semi-supervised settings. following introducing layer section explore several novel applications. section studies supervised learning sets requires invariance permutation inputs. section considers task summing multiple mnist digits section studies important application sets representing low-dimensional point-clouds. here show deep networks successfully classify objects using point-cloud representation. section presents numerical study semi-supervised setting output multi-layer network equivariant permutation inputs. permutation-equivariant layer perform outlier detection celeba face dataset section improve galaxy red-shift estimates using clustering information section permutation-equivariant layer denote denote vector instances. here could feature-vector image structured object. goal design neural network layers indifferent permutations instances achieving goal amounts treating rather vector. function equivariant permutation inputs here action vector represented permutation matrix. abuse notation }n×n also denote matrix. given permutation equivariance function composition also permutation-equivariance; πg). weight vector nonlinearity sigmoid function. point-wise application input vector. following theorem states necessary sufﬁcient conditions permutation-equivariance type function. theorem function deﬁned permutation equivariant off-diagonal elements tied together diagonal elements equal well function simply non-linearity applied weighted combination input and; input values since summation depend permutation layer permutation-equivariant. therefore manipulate operations parameters layer example another variation operation elements commutative using instead amounts reparametrization. practice using variation performs better applications. fact input non-linearity max-normalized. multiple input-output channels speed operation layer using matrix multiplication. suppose input channels –corresponding features instance set– size output channels. here permutation-equivariant layer parameters output layer becomes figure examples object classes modelnet. point-cloud produces sampling particles mesh representation original meodelnet instances. pointclouds column class. projection particles planes added better visualization. fully connected layer input features max-normalized within set. multiple input-output channels complexity layer subtracting mean also reduces internal covariate shift observe deep networks batch-normalization required. applying dropout regularize permutation-equivariant layers multiple output channels often beneﬁcial simultaneously dropout channels instances within set. particular set-members share similar features independent dropout effectively regularize model network learns replace missing features set-members. remainder paper demonstrate simple treatment sets solve novel non-trivial problems occasionally alternative working solutions within deep learning. permutation-equivariant layers introduced useful semi-supervised learning setting intend predict value instance every set. supervised setting task make prediction require permutation invariance pooling operation set-dimension turn permutation equivariant function permutation invariant related work chen construct deep permutation invariant features pairwise invariant pling features previous layer fij) transposition another related work vinyals approach unordered instances ﬁnding good orderings. shortly section supervised setting even simple application set-pooling without max-normalization performs well practice need permutation invariance. however semi-supervised setting since pooling operation permutation invariant layer requires max-normalization order obtain required information context instance i.e. permutation equivariance. predicting mnist digits mnist dataset contains instances grey-scale stamps digits randomly sample subset images dataset build sets training sets validation images set-label digits change number features/channels layer approach cannot produce permutationequivariant layers. also method requires graph guide multi-resolution partitioning nodes used deﬁne pairing features layer. concatenating instances along horizontal axis. stacking images different input channels. iii) using set-pooling without max-normalization. using permutation equivariant layer set-pooling. figure show prediction accuracy validation-set different models using set-layer performs best. however interestingly using set-pooling alone produces similarly good results. also observe concatenating digits eventually performs well despite lack invariance. sufﬁciently large size dataset permutations length three appear training set. however increase size permutation invariance becomes crucial; figure using default dropout rate model simply memorizes input instances increasing dropout rate model simply predicts values close mean value. however permutation-invariant layer learns predict digits accuracy without access individual image labels. performance using set-pooling alone similar. low-dimensional point-cloud low-dimensional vectors. type data frequently encountered various applications robotics vision cosmology. applications point-cloud data often converted voxel mesh representation preprocessing step since output many range sensors lidar extensively used applications autonomous vehicles form point-cloud direct application deep learning methods point-cloud highly desirable. moreover working point-clouds rather voxelized objects easy apply transformations rotation translation differentiable layers cost. here show treating point-cloud data set-equivariant layer classify point-cloud representation subset shapenet objects called modelnet subset consists representation training test instances belonging classes objects; fig. produce point-clouds particles mesh representation objects using point-cloud-library’s sampling routine normalized initial layer deep network zero mean unit variance. additionally experiment k-nearest neighbor graph point-cloud report results using graph-convolution; appendix model details. table compares method competition. note achieve best accuracy using dimensional representation object much smaller methods. techniques either voxelization multiple view object classiﬁcation. interestingly variations view/angle-pooling interpreted set-pooling class-label invariant permutation different views. results also shows using fully-connected layers set-pooling alone works relatively well. reducing number particles still produces comparatively good results. using graph-convolution computationally challenging produces inferior results setting. results using particles also invariant small changes scale rotation around z-axis; appendix details. features. visualize features learned layers used adamax locate particle coordinates maximizing activation unit. activating tanh units beyond second layer proved difﬁcult. figure shows particle-cloud-features learned ﬁrst second layers deep network. observed ﬁrst layer learns simple localized point-clouds different locations second layer learns complex surfaces different scales orientations. semi-supervised transductive learning some/all instances within training labelled. goal make predictions individual instances within test set. therefore permutation equivariant layer leverages interaction set-members label individual member. note case perform pooling operation dimension data. started uniformly distributed particles used learning rate adamax ﬁrst second order moment respectively. optimized input iterations. results fig. limited instances tanh units successfully activated. since input ﬁrst layer deep network normalized zero mean unit standard deviation need constrain input maximizing unit’s activation. figure shows constructed celeba dataset members except outlier share least attributes outlier identiﬁed frame. model trained observing examples sets anomalous members without access attributes. probability assigned member outlier detection network visualized using bottom image. probabilities one. appendix examples. face images annotated boolean attributes. stamps using attributes build sets containing images follows randomly selecting attributes draw images attributes present single image attributes absent. using similar procedure build sets test images. individual person’s face appears train test sets. deep neural network consists d-convolution max-pooling layers followed permutation-equivariant layers ﬁnally softmax layer assigns probability value member trained model successfully ﬁnds anomalous face test sets. visually inspecting instances suggests task non-trivial even humans; fig. details model training identiﬁcation examples appendix baseline repeat experiment using set-pooling layer convolution layers replacing permutation-equivariant layers fully connected layers number hidden units/output-channels ﬁnal layer -way softmax. resulting network shares convolution ﬁlters instances within sets however input softmax equivariant permutation input images. permutation equivariance seems crucial baseline model achieves training test accuracy random selection. important regression problem cosmology estimate red-shift galaxies corresponding well distance common types observation distant galaxies include photometric spectroscopic observations latter produce accurate red-shift estimates. estimate red-shift photometric observations using regression model multi-layer perceptron purpose accurate spectroscopic red-shift estimates ground-truth. another baseline photometric redshift estimate provided catalogue uses various observations estimate individual galaxy-red-shift. objective clustering information galaxies improve red-shift prediction using multi-layer preceptron. note prediction galaxy change permuting members galaxy cluster. therefore treat galaxy cluster permutation-equivariant layer estimate individual galaxy red-shifts. galaxy photometric features redmapper galaxy cluster catalog contains photometric readings galaxy clusters. task contrast previous ones sets different cardinalities; galaxy-cluster catalog galaxies i.e. cluster-size. fig. distribution cluster sizes. catalog also provides accurate spectroscopic red-shift estimates subset galaxies well photometric estimates uses clustering information. fig. reports distribution available spectroscopic red-shift estimates cluster. randomly split data training test clusters following simple architecture semi-supervised learning. four permutation-equivariant layers output channels respectively output last layer used red-shift estimate. squared loss prediction available spectroscopic red-shifts minimized. fig. shows agreement estimates spectroscopic readings galaxies test-set spectroscopic readings. ﬁgure also compares photometric estimates provided catalogue ground-truth. customary cosmology literature report average scatter |zspec−z| zspec accurate spectroscopic measurement photometric estimate. average scatter using model compared scatter original photometric estimates redmapper catalog. values averaged galaxies spectroscopic measurements test-set. repeat experiment replacing permutation-equivariant layers fully connected layers individual galaxies available spectroscopic estimate training. resulting average scatter multi-layer perceptron demonstrating using clustering information indeed improves photometric red-shift estimates. figure application permutation-equivariant layer semi-supervised red-shift prediction using clustering information distribution cluster size; distribution reliable red-shift estimates cluster; prediction red-shift test-set using clustering information well redmapper photometric estimates introduced simple parameter-sharing scheme effectively achieve permutation-equivariance deep networks demonstrated effectiveness several novel supervised semi-supervised tasks. treatment structure also generalizes various settings multi-instance learning addition experimental settings permutation-invariant layer used distribution regression classiﬁcation become popular recently experiments point-cloud data observed model robust variations number particles cloud suggesting usefulness method general setting distribution regression number samples qualitatively affect representation distribution. leave investigation direction future work. single measurement band well measurement error bars location galaxy well probability galaxy cluster center. include information regarding richness estimates clusters catalog methods baseline multi-layer preceptron blind clusters. references martın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. angel chang thomas funkhouser leonidas guibas hanrahan qixing huang zimo silvio savarese manolis savva shuran song shapenet information-rich model repository. arxiv preprint arxiv. daniel maturana sebastian scherer. voxnet convolutional neural network real-time object recognition. intelligent robots systems ieee/rsj international conference ieee siamak ravanbakhsh junier oliva sebastien fromenteau layne price shirley jeff schneider barnab´as p´oczos. estimating cosmological parameters dark matter distribution. proceedings international conference machine learning richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing volume citeseer nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research hang subhransu maji evangelos kalogerakis erik learned-miller. multi-view convolutional neural proceedings ieee international conference computer zhi-hua zhou yu-yin yu-feng multi-instance learning treating instances non-iid samples. proceedings annual international conference machine learning proof. theorem deﬁnition permutation equivariance deﬁnition condition becomes equivalent therefore need show necessary sufﬁcient conditions matrix commute permutation matrices given prove directions off-diagonal elements identical show since commutes product transpositions choice off-diagonal elements identical. index off-diagonal elements moreover assume application transposition πiiθ swaps rows similarly θπjj switches column column. commutativity property nonlinearities exponential linear units models convolution layers followed max-pooling. convolution layers respectively output channels receptive ﬁelds. pooling fully-connected set-layer followed dropout. models simultaneous dropout. models convolution layers followed fullyconnected layers hidden units. model ﬁrst fully connected layer perform figure images shows constructed celeba dataset images members except outlier share least attributes. outlier identiﬁed frame. model trained observing examples sets anomalous members without access attributes. probability assigned member outlier detection network visualized using bottom image. probabilities one. set-pooling followed another dense layer hidden units. model convolution layers followed permutation-equivariant layer output channels followed setpooling fully connected layer hidden units. optimization used learning rate adam using default model convolution layers receptive ﬁelds. model convolution layers feature-maps followed max-pooling followed convolution layers feature-maps followed another max-pooling layer. ﬁnal convolution layers feature-maps followed max-pooling layer pool-size reduces output dimension batch size.n set-size forwarded three permutation-equivariant layers output channels. output ﬁnal layer softmax identify outlier. exponential linear units drop dropout rate convolutional layers dropout rate ﬁrst layers. applied layers selected feature simultaneously dropped members particular set. adam optimization batch-normalization convolutional layers. mini-batches sets total images batch. convolution. network comprising permutation-equivariant layers channels followed max-pooling structure. resulting vector representation fully connected layer units followed -way softmax unit. tanh activation layers dropout layers set-max-pooling dropout rate. applying dropout permutation-equivariant layers point-cloud data deteriorated performance. observed using different types permutation-equivariant layers channels layers changes result less classiﬁcation accuracy. setting particles increase number units layers randomly rotate input around z-axis. also randomly scale point-cloud setting only adamax instead adam reduce learning rate graph convolution. point-cloud instance particles build sparse k-nearest neighbor graph three point coordinates input features. normalized graphs preprocessing step. direct comparison layer exact architecture graphconvolution layer followed set-pooling dense layer units. exponential linear activation function instead tanh performs better graphs. over-ﬁtting heavy dropout graph-convolution dense layers. similar dropout sets randomly selected features simultaneously dropped across graph nodes. mini-batch size adam optimization learning rate despite efﬁcient sparse implementation using tensorﬂow graph-convolution signiﬁcantly slower layer. prevented thorough search hyper-parameters quite possible better hyper-parameter tuning would improve results report here.", "year": 2016}