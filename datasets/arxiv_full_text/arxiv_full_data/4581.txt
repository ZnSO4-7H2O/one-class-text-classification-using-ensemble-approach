{"title": "Feature Selection for Value Function Approximation Using Bayesian Model  Selection", "tag": ["cs.AI", "cs.LG"], "abstract": "Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions, we can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance.", "text": "abstract. feature selection reinforcement learning i.e. choosing basis functions useful approximations unkown value function obtained main challenges scaling real-world applications. consider gaussian process based framework gptd approximate policy evaluation propose feature selection marginal likelihood optimization associated hyperparameters. approach appealing beneﬁts given sample transitions solve policy evaluation problem fully automatically model selection allows consider sophisticated kernels turn enable identify relevant subspaces eliminate irrelevant state variables achieve substantial computational savings improved prediction performance. wiφi represent denotes state scalar reward discount factor. given trajectory states rewards sampled goal determine weights good approximation fundamental problem arising policy iteration framework inﬁnite-horizon dynamic programming reinforcement learning e.g. unfortunately problem also diﬃcult problem that present completely satisfying solution. particular deciding features rather challenging general needs done manually thus tedious prone errors important requires considerable insight domain. hence would desirable learning system could automatically choose representation. particular considering eﬃciency want adapt actual diﬃculties faced without wasting resources often many factors recent work applying nonparametric function approximation gaussian processes equivalently regularization networks promising step direction. instead explicitly specify individual basis functions specify general kernel depends small number hyperparameters. contribution paper demonstrate feature selection sample transitions automated using several possible model selection methods hyperparameters marginal likelihood optimization bayesian setting leave-one-out error minimization frequentist setting. here focus bayesian setting adapt marginal likelihood optimization gp-based approximate policy evaluation method gptd introduced without model selection overall following beneﬁts first automatic model selection able sophisticated kernels allow uncover hidden properties given problem. example choosing kernel independent lengthscales individual dimensions state space model selection automatically drive components zero correspond state variables irrelevant task. allow concentrate computational eﬀorts parts input space really matter improve computational eﬃciency. second generally easier learn smaller spaces also beneﬁt generalization thus help reduce sample complexity. despite many promises previous work rarely explores beneﬁts model selection variant stochastic search used determine hyperparameters covariance gptd using score function online performance agent. standard marginal likelihood based model selection employed; however since approach based ﬁtted value iteration task value function approximation reduced ordinary regression. remaining paper structured follows section contain background information summarize gptd framework. beneﬁts model selection reduction computational complexity section describes gptd solved large-scale problems using sr-approximation. section introduces model selection gptd derives detail associated gradient computation. finally section illustrates approach providing experimental results. overall goal learning representations feature selection linearly parameterized within context roughly past methods categorized along dimensions basis functions represented quantity/target function considered guide construction process conceptually closely related work approach described adapts hyperparameters rbfbasis functions using either gradient descent cross-entropy method bellman error. however basis functions adapted individually method prone overﬁtting e.g. placing basis functions small width near discontinuities. problem compounded data points available. contrast using bayesian approach automatically trade-oﬀ model model complexity number data points choosing always best complexity e.g. small data sets prefer larger lengthscales larger data sets aﬀord smaller lengthscales alternative approaches rely predeﬁned basis functions method incremental approach uses dimensionality reduction state aggregation create basis functions every step remaining bellman error trajectory states successively reduced. related approach given incrementally constructs orthogonal basis bellman error. graph-based unsupervised approach presented derives basis functions eigenvectors graph laplacian induced underlying mdp. suppose observed sequence states rewards xi+). practice mdps considered often episodic nature absorbing terminal states. therefore transform problem resulting markov chain still ergodic done introducing zero reward transition terminal state episode start state next episode. addition sequence states rewards training data thus also includes sequence non-terminal state terminal state covariance matrix entries note covariance alone fully speciﬁes assume simple function parameterized number scalar parameters collected vector however unlike ordinary regression cannot observe samples target function directly. instead values observed indirectly value state recursively deﬁned value successor state immediate reward. engel propose following generative model distribution ﬁrst choice noise covariance would unknown hyperparameter however model capture stochastic state transitions hence would applicable deterministic mdps. environment stochastic noise model appropriate detailed explanations. remainder solely consider latter choice i.e. regarding implementation gptd policy evaluation shares weakness traditional machine learning tasks solving requires inversion dense matrix done exactly would require operations hence infeasible anything small-scale problems submatrix approximation motivated example nystr¨om approximation denote submatrix corresponding columns data points subset. rank-reduced approximation knmk− knmk− mean means huge gain computational savings solving reduced problem costs initialization requires storage every prediction costs additionally evaluate variance). compared complexity full problem initialization storage prediction. thus computational complexity depends linearly note sr-approximation produces degenerate consequence predictive variance underestimate true variance. particular near zero subset {˜x}m situation remedied considering projected process approximation results expression mean adds term selecting best subset combinatorial problem cannot solved eﬀeciently. instead compact subset summarizes relevant information incremental forward selection. every step procedure element remaining unselected elements active performs best respect given speciﬁc criterion. general distinguish supervised unsupervised approaches i.e. consider target variable regress not. focus incomplete cholesky decomposition unsupervised approach performing partial gram-schmidt mercer-induced feature representation every step element active whose distance span currently selected elements largest procedure stopped residual remaining elements falls given threshold given maximum number allowed elements exceeded. online variants thereof considered general number elements selected depend eﬀective rank major advantage using gp-based function approximation ’learning’ weight vector speciﬁcation architecture/hyperparameters/basis functions handled principled essentially automated way. determine hyperparameters gptd consider marginal likelihood process i.e. probability generating rewards observed given sequence states particular setting hyperparameter maximize function respect gptd thus plugging deﬁnition multivariate gaussian taking logarithm obtain optimizing function respect nonconvex problem resort iterative gradient-based solvers need able evaluate gradient partial derivatives respect individual hyperparameter obtained closed form note automatically incorporates trade-oﬀ model model complexity thus regarded indicator generalization capabilities i.e. well gptd predict values states training set. ﬁrst term measures complexity model large ’ﬂexible’ small ’rigid’ models. second term measures model shown value error function penalized least-squares would correspond gptd. property manifests eigenvalues general ﬂexible models achieved smaller bandwidths covariance meaning eﬀective rank large eigenvalues fall slowly. hand rigid models achieved larger bandwidths meaning eﬀective rank eigenvalues fall quickly. note eﬀective rank also important sr-approximation since eﬀectiveness depends building low-rank approximation spending resources possible. common choice consider function parameterized small number scalar parameters stationary isotropic gaussian parameterized lengthscale following consider three variants form ﬁrst variant assumes every coordinate input equally important predicting value. however particular high-dimensional state vectors might simple along dimensions produce much resolution wasted along dimensions produce little resolution would otherwise needed. second variant powerful includes diﬀerent parameter every coordinate state vector thus assigning diﬀerent scale every state variable. covariance implements automatic relevance determination since individual scaling factors automatically adapted data marginal likelihood optimization inform relevant state variable predicting value. large value means i-th state variable important even small variations along coordinate relevant. small value means j-th state variable less important large variations along coordinate impact prediction value close zero means corresponding coordinate irrelevant could left beneﬁt removing irrelevant coordinates complexity model decrease model stays same thus likelihood increase. third variant ﬁrst identiﬁes relevant directions input space performs rotation coordinate system second variant diﬀerent scaling factors applied along rotated axes. fig. three variants stationary squared exponential covariance. directions/scaling factors third case derived eigendecomposition i.e. usut mkmt section demonstrates proposed model selection used solve approximate policy evaluation problem completely automated without manual tweaking hyperparameters. also show additional beneﬁts model selection improved accuracy reduced complexity automatically hyperparameters sophisticated covariance functions depend larger number hyperparameters thus better regularities particular dataset therefore waste unnecessary resources irrelevant aspects state-vector. latter aspect particularly interesting computational reasons becomes important large-scale applications. first consider pendulum swing-up task common benchmark goal swing underpowered pendulum balance around inverted upright position details equations motion found e.g. since gptd solves policy evaluation test model selection approach chose generate sample trajectory optimal policy generated sequence state-transitions policy applied gptd three choices covariance isotropic axis-aligned factor analyis case best setting hyperparameters found running scaled conjugate gradients giving figure shows results three produce adequate representation true value function shown figure near states visited trajectory diﬀer start predicting values states training data despite slightly higher error known training data substantially outperforms used full data model selection avoid complexities involved subset-based likelihood approximation e.g. implementation model selection data points took secs .ghz taking closer look figure indeed value function varies strongly along diagonal direction lower left upper right whereas varies slowly along opposite diagonal upper left lower right. relevance assigned along coordinates case gives particular beneﬁt; able assign diﬀerent importance diﬀerent state variables. additional insight gained looking eigenspectrum figure shows eigenvalues decrease slowest whereas decrease fastest. consequences. first eigenspectrum intimately related complexity generalization capabilities thus helps explain delivers better prediction performance. second eigenspectrum also indicates eﬀective rank strongly impacts ability build eﬃcient low-rank approximation using small subset possible small subset turn important computational eﬃciency size dominant factor employ sr-approximation batch online learning operation count depends quadratically size subset keeping size small possible without losing predictive performance essential. figure shows regard performs best worst example approximate using sr-approximation selection tolerance level samples would choose would chose would choose elements. illustrate detail approach handles irrelevant state variables speciﬁcally designed gridworld states. every step entails reward except state starts episode consider policy moves left right addition every time move left right also move randomly corresponding value function shown figure generated transitions applied gptd covariance automatic model selection resulting fig. bottom gptd approximation value function figure covariances case hyperparameters obtained marginal likelihood optimization gptd process right associated predictive variance. black indicates variance white indicates high variance circles indicate location states training seen figure obtain reasonable approximation. however automatically detects y-coordinate state irrelevant thus assigns small weight uniform lengthscale unable equal weight state variables. consequence estimate less exact wiggly additional insight gained looking likelihood models lower complexity data better thus higher combined likelihood moreover completely remove state variable eigenspectrum decreases rapidly; thus without even lower complexity still indicates state component safely ignored task/domain. addition mentioned before lower eﬀective rank also allow make eﬃcient sr-based approximations. noted proposed framework automatic feature generation model selection primarily thought practical tool despite oﬀering principled solution important problem ultimately come theoretical guarantees practical applications might less issue general care taken. framework easily extended perform policy evaluation joint state-action space learn model-free q-function choose diﬀerent covariance function taking example product δaa′ problems small number discrete actions opens fig. learned value functions d-gridworld domain. left true value function. note y-coordinate irrelevant value. center approximation isotropic covariance. right approximation axis-aligned covariance model-free policy improvement thus optimal control approximate policy iteration. next step apply approach real-world highdimensional control tasks batch settings hybrid batch/online settings; latter case exploiting gain computational eﬃciency obtained model selection improve work taken place learning agents research group artiﬁcial intelligence laboratory university texas austin. larg research supported part grants darpa federal highway administration general motors.", "year": 2012}