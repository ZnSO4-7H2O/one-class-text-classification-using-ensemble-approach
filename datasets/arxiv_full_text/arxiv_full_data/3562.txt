{"title": "Deep Burst Denoising", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Noise is an inherent issue of low-light image capture, one which is exacerbated on mobile devices due to their narrow apertures and small sensors. One strategy for mitigating noise in a low-light situation is to increase the shutter time of the camera, thus allowing each photosite to integrate more light and decrease noise variance. However, there are two downsides of long exposures: (a) bright regions can exceed the sensor range, and (b) camera and scene motion will result in blurred images. Another way of gathering more light is to capture multiple short (thus noisy) frames in a \"burst\" and intelligently integrate the content, thus avoiding the above downsides. In this paper, we use the burst-capture strategy and implement the intelligent integration via a recurrent fully convolutional deep neural net (CNN). We build our novel, multiframe architecture to be a simple addition to any single frame denoising model, and design to handle an arbitrary number of noisy input frames. We show that it achieves state of the art denoising results on our burst dataset, improving on the best published multi-frame techniques, such as VBM4D and FlexISP. Finally, we explore other applications of image enhancement by integrating content from multiple frames and demonstrate that our DNN architecture generalizes well to image super-resolution.", "text": "noise inherent issue low-light image capture exacerbated mobile devices narrow apertures small sensors. strategy mitigating noise low-light situation increase shutter time camera thus allowing photosite integrate light decrease noise variance. however downsides long exposures bright regions exceed sensor range camera scene motion result blurred images. another gathering light capture multiple short frames burst intelligently integrate content thus avoiding downsides. paper burst-capture strategy implement intelligent integration recurrent fully convolutional deep neural build novel multiframe architecture simple addition single frame denoising model design handle arbitrary number noisy input frames. show achieves state denoising results burst dataset improving best published multi-frame techniques vbmd flexisp finally explore applications image enhancement integrating content multiple frames demonstrate architecture generalizes well image super-resolution. noise reduction important problems solve design imaging pipeline. straight-forward solution collect much light possible taking photograph. addressed camera hardware large aperture lens sensors large photosites high quality conversion. however relative larger standalone cameras e.g. dslr modern smartphone cameras compromised hardware elements. makes noise much problem smartphone capture. placing camera tripod. tripod necessary motion camera cause collected light blur across multiple photosites. technique limited though. first moving objects scene residual camera motion cause blur resulting photo. second shutter time long brightest objects scene saturate electron collecting capacity photosite. means high dynamic range scenes darkest regions image still exhibit signiﬁcant noise brightest ones might staturate. method also collect light longer period time capturing burst photos. burst photography addresses many issues available inexpensive hardware capture moving subjects less likely suffer blown-out highlights. using burst make design choice leveraging computational process integrate light instead hardware process words turn computational photography. computational process runs several steps. first burst stabilized ﬁnding homography frame geometrically registers common reference. second employ fully convolutional deep neural network denoise frame individually. third extend parallel recurrent network integrates information frames burst. paper presents work follows. section review previous single-frame multi-frame denoising techniques. also look super-resolution leverage multi-frame information. section describe recurrent network detail discuss training. order compare previous work network trained simulated gaussian noise. also show solution works well trained poisson distributed noise typical real-world imaging pipeline section show signiﬁcant increase reconstruction quality burst sequences comparison state single-frame denoising performance better recent state multi-frame denoising methods. addition demonstrate burst capture coupled recurrent network architecture generalizes well super-resolution. summary main contributions introduce recurrent feature accumulator network simple effective extension single-frame denoising models demonstrate bursts provide large improvement best deep learning based single-frame denoising techniques show model reaches performance better recent state multi-frame denoising methods demonstrate recurrent architecture generalizes work addresses variety inverse problems formulated consisting target restored image temporally-ordered burst images corrupted observation target image function mapping burst images restored target. tasks include denoising super-resolution. goal craft function either domain knowledge data-driven approach solve multi-image restoration problems. data-driven single-image denoising research dates back work leverages block-level statistics within single image. earliest works nature non-local means method taking weighted average blocks within image based similarity reference block. dabov extend concept block-level ﬁltering novel ﬁltering formulation. algorithm facto method single-image methods compared today. learning-based methods proliferated last years. methods often make neural networks purely feed-forward recurrent hybrid methods field experts shown successful modeling natural image statistics tasks denoising inpainting contrastive divergence. moreover related tasks demosaicing denoising shown beneﬁt joint formulations posed learning framework finally recent work applied recurrent architecture context denoising ray-traced sequenced. multi-image variants denoising methods exist often focus best ways align combine images. tico returns block-based paradigm time blocks within across images burst used produce denoised estimate. vbmd vbmd provide extensions existing framework. showed similar denoising performance terms psnr could obtained tenth time vbmd one-hundredth time vbmd using novel homography alignment scheme along consistent pixel compositing operator. systems flexisp proximal offer end-to-end formulations entire image processing pipeline including demosaicing alignment deblurring etc. solved jointly efﬁcient optimization. nasrollahi offers comprehensive survey single-image super-resolution methods yang offers benchmark evaluation several methods. glasner show single images super-resolved without need external database prior exploiting block-level statistics within single image. methods make sparse image statistics borman offers survey multi-image methods farsiu offers fast robust method solving multi-image super-resolution problem. recently convolutional networks shown good results single image super-resolution works dong state ledig worthwhile taking note image restoration approaches often learning-based methods recent years there’s also great diversity learning problems modeled. particular neural network-based approaches experienced gradual progression architectural sophistication time. work dong single feed-forward used super-resolve input image. natural design leveraged advancements discriminatively-trained neural networks designed classiﬁcation applied regression task. next step architecture evolution recurrent neural networks rnns place convolutional layers previous design. rnns network design used increase effective depth thus receptive ﬁeld single-image network integrate observations across many frames multi-image network. work makes latter principle. introduction rnns network architectures effective depth thus larger receptive ﬁeld context success skip connections classiﬁcation networks segmentation networks motivated restoration networks. work remez illustrates principle computing additive noise predictions level network form ﬁnal noise prediction. also make concept rather skip connections directly extract activations level network corresponding rnns integration across frames burst sequence. goal derive method which given sequence noisy images produces denoised sequence. identiﬁed desirable properties multi-frame denoising technique satisfy denoise entire sequence. rather simply denoise single reference frame goal prior work denoise entire sequence putting goal closer video denoising. generalize variety image restoration tasks. discussed section tasks super-resolution beneﬁt image denoising methods albeit trained different data. remainder section ﬁrst describe single-frame denoising model produces competitive results current state models. discuss extend model accommodate arbitrary number frames multi-frame denoising meets goals. single frame denoising treat image denoising structured prediction problem network tasked regressing pixel-aligned denoised image noisy image model parameters following train network minimizing distance predicted output ground-truth target image competitive single-frame denoising scenario meet goal take inspiration state derive initial network architecture. several existing architectures consist base design fully convolutional architecture consisting layers channels each. decoupling per-frame feature extraction multiframe aggregation enable possibility pre-training network rapidly using single-frame data. practice found pre-training accelerates learning process also produces signiﬁcantly better results terms psnr train entire scratch. core intuition ﬁrst learning good features network good state learning aggregate features across observations still grant freedom update features freezing weights training. also important note rnns connected permit aggregation observation features several different ways. temporal connections within rnns help aggregate information across frames lateral connections within track permit aggregation information different physical scales different levels abstraction. figure recurrent denoising architecture. part model single-frame denoiser takes input noisy image regresses clean image multi-frame denoiser also makes recurrent connections output clean image therefore follow suit choosing simple architecture single frame denoising base convolutions relu activation functions except last layer seen figure trained networks evaluation using dataset consisting apple live photos. live photos burst sequences captured apple iphone above. dataset representative captures mobile phone users like photograph exhibits wide range scenes motions. approximately public sequences scraped social media website resolution apply burst stabilizer sequence resulting approximately sequences successfully stabilized. section describe stabilization procedure detail. sequences used training additional reserved validation reserved testing. implemented burst sequence stabilization using opencv. particular lucas-kanade tracker correspondences successive frames rotation-only motion model static focal length guess arrive homography frame. warp frames sequence back reference frame’s pose crop scale sequence maintain original size aspect ratio region interest contained entirely within valid regions warp. stabilized sequences still exhibit residual motion either moving objects people camera scene motion cannot following goals want model competitive single-frame case able denoise entire input sequence. hence given noisy images forming sequence task network regress denoised version noisy model parameters frame complete training objecting thus natural approach already popular natural language audio processing literature process temporal data recurrent neural networks modules rnns operate sequences maintain internal state combined input time step. model make recurrent connections aggregate activations produced network frame show figure allows arbitrary input sequence length ﬁrst goal. unlike utilize single-track network design track network architecture track dedicated bottom track dedicated fusing results ﬁnal prediction mfd. implemented neural network section using caffe framework. model trained using tesla gpus. described section training took place stages. first single-frame model trained. model used batch size trained epochs approximately hours. using single-frame model initialization multi-frame model continue training batch size accommodate increased size multi-frame model single-frame model. second stage trained epochs approximately hours. used adam learning rate decays zero following square root law. trained crops random ﬂips. finally train multi-frame model using back-propagation time ﬁrst evaluate architecture using additive white gaussian noise order make comparison possible previous methods vbmd. able denoise real burst sequences modeled sensor noise following trained separate models adding poisson noise labelled intensity ranging linear space converting back srgb clipping. also simulate bayer ﬁltering reconstruct image using bilinear interpolation. unless otherwise mentioned synthetic noise stabilization. compare single frame denoiser current state methods additive white gaussian noise. compare composed layers layer networks denoisenet dncnn sake comparison also include layer version well reimplementations dncnn denoisenet. models trained epochs images pascal using training split also include comparison tnrd models tested natural images berkeley segmentation dataset figure diminishing returns single frame denoising psnr years conﬁrms levin describe despite deep neural networks. simpler layers model figure effect pre-training multi-frame denoising gaussian noise color corresponds average psnr frames sequence etc. pre-trained model shows constant lead model trained scratch reaches stable state much quicker. slightly underperforms denoisenet dncnn .db. however show following section psnr gains brought multi-frame processing vastly outshine fractional single frame psnr improvements. evaluate method held-out test live photos synthetic additive white gaussian noise added. table compare architecture single frame models well multi-frame method vbmd show qualitative results figure figures demonstrate method capable denoising real sequences. evaluation performed real noisy bursts hdr+ please supplementary material results. table ablation study live photos test sequences additive white gaussian noise models trained frames long sequences. represent concat models trained respectively concatenated frame input. nostab trained tested unstabilized sequences. figure effect frame ordering test time. burn-in period ﬁrst pass well repeat pass. feeding sequence forward backward mostly alleviates problem. concat ﬁrst compare method naive multiframe denoising approach dubbed concat input consists concatenated frames single pass denoiser. evaluated architecture well table model performs signiﬁcantly worse model. number layers also evaluate impact depth network experimenting seen figure layers network fail surpass layers epochs training likely increased depth parameter count. layers network shows marginal increase layer model decided latter think modest increase psnr worth increase memory computation time. length training sequences perhaps surprising result encountered training recurrent model importance number frames training sequences. figure show models trained sequences frames fail generalize beyond training length sequence. models trained frames able generalize longer sequences test time still denoise beyond frames. pre-training main advantages using two-track network train track independently ﬁrst. mentioned before sequence length required ensure generalization longer sequences makes training full model much slower training single-frame pass. show figure pre-training makes training signiﬁcantly faster. frame ordering recurrent nature network exhibits period burn-in ﬁrst frames denoised lesser extent later ones. order denoise entire sequence high quality level explored different options frame ordering. show figure feeding sequence twice network able obtain higher average psnr. propose variants either repeat sequence order reverse second time show figure forward-backward schedule suffer burn-in ﬂickering thus meeting goal. thus forward-backward experiments. compare method denoising approaches flexisp dataset show results figure sequence denoised using ﬁrst frames only. synthetic sequences flickr doll kodak figure multi-frame gaussian denoising stabilized live photo test data produces signiﬁcantly sharper image vbmd latter exhibiting signiﬁcant temporal color ﬂickering. figure multi-frame super-resolution stabilized live photo test data. single frame model achieves good upsampling increase sharpness multi-frame approach brings signiﬁcant quality improvement. figure denoising results real synthetic bursts flexisp dataset bottom livingroom flickr doll kodak fence. recurrent model able match quality flexisp flickr doll beat kodak fence despite showing demoisaicing artifacts. fence generated randomly warping input image adding respectively additive multiplicative white gaussian noise additive gaussian noise well simulating bayer ﬁlter. thus trained models replicating conditions live photos dataset. flickr doll method achieves psnr matching flexisp falling short proximal shown here. kodak fence recurrent model achieves advantage flexisp psnr .db. despite reaching higher psnr flexisp method mitigate demosiacing artifacts fence likely absence high frequency demosaicing artifacts training data. illustrate approach generalizes tasks beyond denoising goal trained model perform super-resolution keeping rest training procedure identical denoising pipeline. input patch downsampled using pixel area resampling resized original size using bilinear sampling. figure shows couple results. please refer supplemental material results. memory computationally expensive small receptive ﬁeld given network depth. using multiscale architectures u-nets could help alleviate issues reducing computational memory load increasing receptive ﬁeld. finally trained network pre-stabilized sequences observed signiﬁcant drop accuracy unstabilized sequences seen table well unstability longer sequences. would interesting train network stabilize sequence warping inside network presented novel deep neural architecture process burst images. improve simple single frame architecture making recurrent connections show single-frame models reaching performance limdenoising recurrent architecture vastly outperform models multi-frame data. carefully designed method align goals stated section result approach achieves state-of-the-art performance live photos dataset matches existing multi-frame denoisers challenging existing datasets real camera noise.", "year": 2017}