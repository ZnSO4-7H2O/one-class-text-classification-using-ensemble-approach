{"title": "Efficient Point-to-Subspace Query in $\\ell^1$ with Application to Robust  Object Instance Recognition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Motivated by vision tasks such as robust face and object recognition, we consider the following general problem: given a collection of low-dimensional linear subspaces in a high-dimensional ambient (image) space, and a query point (image), efficiently determine the nearest subspace to the query in $\\ell^1$ distance. In contrast to the naive exhaustive search which entails large-scale linear programs, we show that the computational burden can be cut down significantly by a simple two-stage algorithm: (1) projecting the query and data-base subspaces into lower-dimensional space by random Cauchy matrix, and solving small-scale distance evaluations (linear programs) in the projection space to locate candidate nearest; (2) with few candidates upon independent repetition of (1), getting back to the high-dimensional space and performing exhaustive search. To preserve the identity of the nearest subspace with nontrivial probability, the projection dimension typically is low-order polynomial of the subspace dimension multiplied by logarithm of number of the subspaces (Theorem 2.1). The reduced dimensionality and hence complexity renders the proposed algorithm particularly relevant to vision application such as robust face and object instance recognition that we investigate empirically.", "text": "abstract. motivated vision tasks robust face object recognition consider following general problem given collection low-dimensional linear subspaces high-dimensional ambient space query point eﬃciently determine nearest subspace query distance. contrast naive exhaustive search entails large-scale linear programs show computational burden signiﬁcantly simple two-stage algorithm projecting query data-base subspaces lower-dimensional space random cauchy matrix solving small-scale distance evaluations projection space locate candidate nearest; candidates upon independent repetition getting back high-dimensional space performing exhaustive search. preserve identity nearest subspace nontrivial probability projection dimension typically low-order polynomial subspace dimension multiplied logarithm number subspaces reduced dimensionality hence complexity renders proposed algorithm particularly relevant vision application robust face object instance recognition investigate empirically. introduction. although visual data reside high-dimensional spaces often exhibit much lower-dimensional intrinsic structure. modeling exploiting lowdimensional structure central goal computer vision impact applications low-level tasks signal acquistion denoising higher-level tasks object detection recognition. object interest associate low-dimensional subset approximates images generated diﬀerent physical conditions varying pose illumination. given objects corresponding approximation subsets recognition problem becomes ﬁnding nearest low∗department electrical engineering columbia university york gratefully acknowledges support family private foundation. partially supported n--- columbia university startup funding. variations encountered. example variations illumination wellcaptured using low-dimensional linear models whereas variations pose alignment highly nonlinear linear subspaces. mentioned above subspace models well-justiﬁed modeling illumination variations also form basic building block modeling computing general nonlinear sets metric though valid norm verify |xi|p indeed also induces valid metric i.e. also triangular inequality holds latter cases turn empirically interesting norm actually sharper proxy counting norm norm. since stable distributions exist current algorithm analysis methodology likely extend choose smallest optimal objective value. total cost time required solve linear program example interior point methods exist scalable ﬁrstorder methods improve dependence expense higher iteration complexity. best known complexity guarantees methods superlinear although linear runtimes achievable residual sparse problem otherwise well-structured even best case however aforementioned algorithms complexity terms large dependence prohibitive although problem simple state easy solve polynomial time achieving real-time performance scaling massive databases objects appears require careful study. paper present simple practical approach problem much improved computational complexity reasonably strong theoretical guarantees. rather working directly high-dimensional space randomly embed query subspaces random embedding given matrix required dimension depend ambient dimension often signiﬁcantly smaller e.g. typical example face recognition. resulting regression problems solved eﬃciently using customized interior point solvers methods numerically reliable yield speedup several folds standard approach relying solving make things concise main interest mostly eﬀect complexity. lower order possible speciﬁc case careful implementation e.g. page also discussion running time section price paid improved computational proﬁle small increase probability failure recognition algorithm randomized embedding. theory quantiﬁes large needs render probability error control. repeated trials independent projections used make probability failure small desired. regression much cheaper low-dimensional space original space provided repeated trials aﬀordable. result simple practical algorithm guarantees maintain good properties regression substantially improved computational complexity. demonstrate model problems subspace-based face object instance recognition. addition improved complexity theory observe remarkable improvements real data examples suggesting point-to-subspace query could become practical strategy face object recognition tasks involving large databases small databases hard time constraints. relationship existing work. problem example subspace search problem. -dimensional aﬃne subspaces problem coincides nearest neighbor problem. approximate version solved time sublinear number points using randomized techniques locality sensitive hashing dimension larger zero problem becomes signiﬁcantly challenging. case sublinear time algorithms exist although complicated random hash functions case hyperplanes approaches pertain only. perform well numerical examples limitations theory neither known yield algorithm provably sublinear complexity inputs. results theoretical computer science suggest limitations intrinsic problem sublinear time algorithm approximate nearest hyperplane search would refute strong version exponential time hypothesis conjectures general boolean satisﬁability problems cannot solved time algorithms exploit special properties version problem apply variant. however variant retains aforementioned diﬃculties suggesting algorithm near subspace search sublinear dependence unlikely well. motivates focus ameliorating dependence approach simple natural cauchy projections chosen cauchy family unique -stable distribution i.e. cauchy projection given vector remains cauchy appendix details) property widely exploited previous algorithmic work however technical level obvious cauchy embedding succeed problem. cauchy heavy tailed distribution yield embeddings tightly preserve distances points johnsonlindenstrauss lemma fact exist lower bounds showing certain point sets cannot embedded signiﬁcantly lower-dimensional spaces without incurring non-negligible distortion single subspace embedding results exist notably sohler woodruﬀ distortion incurred large render inapplicable problem nevertheless several elegant technical ideas proof turn useful analyzing problem well. problem studied also related recent work sparse modeling sparse error correction. indeed strongest technical motivations using norm provable good performance sparse error correction results give conditions possible recover vector grossly corrupted observation distinction important shows signiﬁcant dimensionality reduction possible gross errors errors present cardinality error vector gives hard lower bound number observations required correct recovery. contrast simpler problem ﬁnding nearest model possible give algorithm purpose quite diﬀerent bunch regression problems instead concerned quality solving individual problem needs ensure regression problem smallest objective value remains approximation. moreover state-of-the-art coreset-based approximation algorithms regression depend heavily obtaining importance sampling measure leverage score well conditioned basis turn depends simultaneously. database-query model common recognition tasks complicated dependency directs lots computation query time. comparison considerable portion computation framework performed training rendering framework attractive recognition hard time constraint. notation. deﬁne commonly used notations here. distance point subspace i.e. minv∈s {··· denotes equality distribution. notations deﬁned inline. input subspaces s··· dimension query output identity closest subspace preprocessing generate rd×d cauchy rv’s compute projections psn; repeat independent repetitions candidates search compute projection compute distance psi. repeat several versions locate nearest candidates reﬁned scanning scan candidates return probability subspace selected original closest subspace theorem suppose given linear subspaces {s··· dimension query point ﬁxed exists rd×d cauchy choice ﬁrst subspace nearest notational expository convenience. also write mini∈ mean ﬁrst subspace nearest unambiguously i.e. minimizers singleton condition theorem depends several factors. perhaps interesting relative closest subspace distance second notice also depends number models logarithm. rather weak dependence strong point interestingly mirrors johnsonlindenstrauss lemma dimensionality reduction even though jl-syle embeddings impossible stating overall algorithm suggest additional practical implications theorem first theorem guarantees success constant probability. probability easily ampliﬁed taking independent trials. probability failure drops exponentially usually suﬃces keep rather small. trials generates candidate subspaces perform regression determine candidates actually nearest query. note also possible perform second step second importance suggests another means controlling resources demanded algorithm. namely reason believe especially small instead according denotes distance query nearest subspace. choice theorem implies constant probability desired subspace amongst nearest query. again subspaces need retained examination. however still complement main result result lower bound projecting dimension basically says randomized embedding oblivious query subspaces target dimension dictated reciprocal ηmin ηmin nominal relative distance order preserve identity nearest subspace non-negligible probability. restrict probability greater rule case worse random guess. proof provided appendix note signiﬁcant upper bound theorem lower bound theorem particular clear whether ηmin enter bound current form extremely small ηmin resemble lower bound signiﬁcantly milder. resolve issues remains open problem. taneously including query subspaces .sn. here encounter challenge although cauchy unambiguously correct distribution estimating norms rather ill-behaved mean variance exist sample averages |zi| obey classical central limit theorem. fig. shows behavior aﬀects point-to-subspace distance ﬁgure shows histogram random variable randomly generated cauchy matrices diﬀerent conﬁgurations query subspace properties especially noteworthy. first upper tail distribution quite heavy non-negligible probability signiﬁcantly exceed median. hand lower tail much better behaved high probability signiﬁcantly smaller median. inhomogeneous behavior precludes probability least median. second event needs wellbehaved subspaces simultaneously. notice however subspaces distance cause error sec. describes arguments needed establish property theorem follows directly results secs. argument well proofs several routine technical lemmas deferred appendix. hindsight exponent power gives rise exponential factor bound theorem unfortunately able establish concrete lower bound probability shows estimate gives optimal power. detailed discussions proofs deferred appendix establishing bounding lipschitz constant cope heavy tails cauchy simple arguments like argument insuﬃcient. rather borrow elegant argument sohler woodruﬀ rough idea work certain special basis considered analogue orthonormal basis. orthonormal basis preserves norm well-conditioned basis approximately preserves norm distortion argument controls action elements basis. space limitations defer discussion idea appendix instead simply state resulting bound remark ..we allow theorem corresponding ties nearest subspaces. special case seems natural instead dimension reduction preserve nearest subspaces; problem actually becomes easier. this nearest subspaces good ignore rest nearest treat rest subspaces. relative distance ηeﬀective number distances want control becomes smaller number subspaces present hence problem actually easier compared generic problem setting theorem parameters note implementation. projection matrices subspaces. theorem ﬁxed subspaces ﬁxed query point. course projection matrix consider many diﬀerent query points success failure approximation query dependent. suggests sampling matrix query would require randomly sample combination nrep matrices corresponding projected subspaces also apply projections query. sampling strategy ﬁnite pool generate independent projections diﬀerent query points allows economic implementation empirically still yields impressive performance. specify values nrep diﬀerent experiments. solvers regression. perform high-dimensional nearest subspace search baseline. considering scale regression case employ augmented lagrange method numerical solver whenever recognition performance noticeably aﬀected otherwise employ accurate interior point method solvers instances regression projected dimensions handled interior point method solvers. experiments synthesized data. independently generated random subspaces -dimensional subspace generated column span standard normal matrix. also prepared pool cauchy matrices dimension takes values verify theory randomly picked subspace generate sample orthonormal basis subspace contains standard normal entries. induce reasonable distance also simulate sparse errors divided magnitude largest entries added errors uniformly step size. growth fraction corruption diminishes distance evidenced legend left subﬁgure figure estimate success probability low-dimensional regression retrieve nearest subspace setting exhausted pool projection matrices obtained empirical success rate. left subﬁgure figure reports results. note least chance preserve nearest subspace. also reasonably level success probability small distance gaps evidently entails large projection dimensions. figure left probabilities preserving nearest subspaces diﬀerent projection dimensions ﬁxed sample corrupted diﬀerent levels additive errors; right fraction samples still identify nearest subspace random projections diﬀerent dimensions. induce diﬀerent distance gaps. keep things simple query randomly picked projection pool omitted repetitions reﬁned scanning altogether. success probability deﬁned fraction samples successfully identify respective nearest subspaces randomly chosen low-dimensional space. right subﬁgure figure gives results. even much trimmed version algorithm helps half samples nearest subspace corruption level robust face recognition extended yale certain physical assumptions images person taken ﬁxed pose varying illumination well-approximated using nine-dimensional linear subspace physical phenomena occlusions specularities well physical properties nonconvexity cause violation low-dimensional linear model formulate recognition problem ﬁnding closest subspace norm total corrupted acquisition used here). subject randomly divided images halves leading training images test images. better illustrate behavior algorithm strategically divided test subsets moderately illuminated extremely illuminated division based light source direction images taken either azimuth angle greater elevation angle greater would classiﬁed extremely illuminated since faces supposed known hence closedworld assumption holds true setting. recognition original images. figure presents evolution recognition rate subset projection dimension grows repetition projection took subspace dimension nine conventional. experiment shows achieves perfect recognition subset implying recognition subset corresponds perfectly search figure actually represents evolution average success probability repetition subset. overall recognition rate nearly perfect. figure presents failing cases. either contain signiﬁcant artifacts approach extremely illuminated cases failing mechanism remedy explained below. words formulate problem search. diﬀerent idea sparse representation face recognition. since focus propose optimal face recognition algorithm method happens task) prefer save detailed discussions line future work. nevertheless preliminary results indeed suggest competitive popular extended yale face recognition benchmark used here. figure samples moderately/extremely illuminated face images distances subject subspaces. subjects ordered ascending order distance sample distances normalized ﬁrst distance note moderately illuminated sample distance observed extremely illuminated sample. suggests increased compensate weak experimental results conﬁrm prediction. speciﬁcally achieves accuracy method achieves nback recognition rate boosted signiﬁcantly increase increase nback recognition artiﬁcially corrupted images. order illustrate robustness approach recognition particularly capability method preserve property emulated robust recognition experiment artiﬁcially corrupted images done speciﬁc subset subset comprise images taken nearfrontal illuminations used training; subset used testing. corrupted original test image randomly-distributed sparse corruptions structured occlusions. ﬁrst setting replaced respectively randomly chosen pixels test images i.i.d. uniform integer values second mandril image scaled image size imposed image randomly chosen locations. figure shows typical samples cases also eﬀect corruptions distance gaps corruptions signiﬁcantly weaken gaps. particular drops rapidly corruption level increases suggesting according theory signiﬁcant dimension reduction projection likely beyond corruption levels ﬂavor level approximation nrep nback compare approximation scheme respectively. demonstrate advantage norm terms stability corruptions also include comparison natural variant figure summarizes recognition performances setting. method exhibits comparable level performance corruptions less equal observable performance beyond level. reasonable price insist working dimensions eﬃciency. current setting dimension performance even worse random corruption model particular corruption level high. structured occlusion model figure left sample original images corrupted versions. corrupted images pixels contaminated. right evolution distance corruptions corresponding exponents calculated distance estimated taking random example test subject. object instance recognition. investigate applicability proposal large-scale recognition tasks took subset multi-purpose amsterdam library object images library subset comprises images toy-like objects ﬁxed pose taken diﬀerent illumination directions object hence includes images object. randomly took images object training rest test. although objects general nonconvex shapes non-lembertian reﬂectance property still approximate collection images object ninedimensional subspace proposed turns recognition problem naturally subspace search problem. method exhibits impressive tolerance corruption compared variant. particular tolerates corruptions almost perfectly test set. comparison fails badly corruption level beyond approximation scheme turns eﬀective corruptions lower higher projection dimensions computational burden would expand rapidly estimate figure exponent associated predicted dimensions theory would signiﬁcant distance lower leading signiﬁcant demand large dimension costs total whereas level search algorithm propose costs nnrept nbackt project onto lower-dimensional repeat nrep boost success probability select best nback reﬁned scanning original space. proposed algorithm practically interesting systematic report recognition results aloi rare many subset objects perhaps signiﬁcant scale. exception reports recognition performance many diﬀerent settings state-of-the-art visual recognition schemes. particularly relevant result evaluated recognition illumination subset choose biologically-inspired hmax model. data training achieved recognition rate. magic implements customized outlined section figure plots running time dimension based- logarithm. make comparison fair possible turned -singlecompthread ensure matlab using thread simulation. seems running time scales illustrate means practice take random instance yale recognition task random corruptions take previous experiment conﬁrmed projection dimension works well case take nrep nback single-thread simulation high dimension exhaustive search costs sec’s two-level search algorithm needs sec’s times faster cost algorithm largely dictated nback larger dataset nback taken much smaller relative advantage could signiﬁcant. comparing characteristic functions clear symmetric p-stable distribution stable distribution also obvious stable distributions exist virtue existence stable distribution remarkable aspect standard cauchy -stable. furthermore inverting characteristic function stated deﬁnition -stable distribution standard cauchy scaled version facts fundamental subsequent analysis. addition following two-sided bound upper tail half-cauchy also useful. implementation available online http//math.bu.edu/people/mveillet/html/alphastablepub. html. convention used almost identical zolotarev’s form following correspondences γaλa. deﬁnition ).let r-dimensional linear subspace dual norm matrix rd×r -well-conditioned basis columns linearly independent; βuzp. said p-well-conditioned basis independent appendix proof theorem high level proof proceeds exploiting approximate subspace search solve sparse recovery problem. invoking known lower bounds sparse recovery problem arrive bound stated theorem ﬁrst record/show useful results. basri hassner zelnik-manor approximate nearest subspace search applications pattern recognition ieee conference computer vision pattern recognition ieee jain vijayanarasimhan grauman hashing hyperplane queries near points applications large-scale active learning advances neural information processing systems simard denker victorri transformation invariance pattern recognition tangent distance tangent propagation neural networks tricks trade springer wagner wright ganesh zhou mobahi towards practical automatic face recognition system robust alignment illumination sparse representation ieee trans. pattern analysis machine intelligence", "year": 2012}