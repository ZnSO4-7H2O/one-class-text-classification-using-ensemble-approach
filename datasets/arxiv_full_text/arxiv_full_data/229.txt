{"title": "Peephole: Predicting Network Performance Before Training", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network's strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction.", "text": "figure peephole proposed network performance predictor dramatically reduce cost network design. circle relies post-training veriﬁcation takes hours loop time-consuming training process bottom circle relies proposed predictor provide fast effective feedback within second architecture. large design space costly training process. speciﬁcally devise convolutional network make number modeling choices e.g. number layers number channels within layers whether insert pooling layer certain points etc. choices together constitute huge design space simply beyond means conduct thorough investigation. previous efforts mostly motivated intuitions though fruitful early days approach increasing difﬁculties networks become complicated. recent works automatic search methods proposed. methods seek better designs gradually adjusting parts networks validating generated designs real datasets. without effective prior guidance search procedures tend spend lots resources evaluating unpromising options. also note training network time-consuming process. even dataset moderate size take hours train network. consequently excessively long process generally needed positive adjustment. it’s quest performant networks signiﬁcant force drives advancements deep learning recent years. rewarding improving network design never easy journey. large design space combined tremendous cost required network training poses major obstacle endeavor. work propose approach problem namely predicting performance network training based architecture. speciﬁcally develop uniﬁed encode individual layers vectors bring together form integrated description lstm. taking advantage recurrent network’s strong expressive power method reliably predict performances various network architectures. empirical studies showed achieved accurate predictions also produced consistent rankings across datasets desideratum performance prediction. computer vision community witnessed series breakthroughs past several years. lies behind remarkable progress advancement convolutional neural networks alexnet googlenet resnet come long improving network design also results substantial performance improvement. take ilsvrc example classiﬁcation error rate dropped years primarily thanks evolution network architectures. nowadays using better network become commonly adopted strategy boost performance strategy simple repeatedly shown effective practical applications e.g. recognition detection segmentation like many others community share painful experience ﬁnding good network designs. mitigate lengthy costly process develop approach quantitatively assess architecture investing resources training accurately propose model called peephole predict ﬁnal performance architecture training. work explore natural idea formulate network performance predictor regression model accepts network architecture input produces score predictive estimate performance e.g. accuracy validation set. here foremost question turn network architecture numerical representation. nontrivial given diversity possible architectures. tackle problem stages. first develop vector representation called uniﬁed layer code encode individual layers. scheme allows various layers represented uniformly effectively vectors ﬁxed dimension. second introduce lstm network integrate layer representations allows architectures different depths topologies handled uniform way. another challenge face obtain training set. note task differs essentially conventional ones samples network architectures together performances instead typical data samples like images data records. here sample space huge expensive obtain even sample addressing issue draw inspirations engineering practice develop block-based sampling scheme generates architectures integrating blocks sampled markov process. allows explore large design space limited budget ensuring sample architecture reasonable. overall main contributions three aspects develop peephole framework predicting network performance based uniﬁed layer code layer embedding. peephole predict network’s performance training. develop block-based generation simple effective strategy generate diverse reasonable network architectures. allows proposed performance predictor learned affordable budget. conducted empirical studies thousand networks show proposed framework make reliable predictions wide range network architectures produce consistent ranking across datasets. hence predictions provide effective search better network designs shown figure related work network design. since debut alexnet cnns become widely adopted solve computer vision problems. past several years advances network design crucial driving force behind progress computer vision. many representative architectures alexnet vggnet googlenet resnet densenet designed manually based intuitions experience. nonetheless approach become less rewarding. huge design space combined costly training procedure makes increasingly difﬁcult obtain improved design. recently community become interested alternative approach namely automatic network design. several methods proposed. methods rely reinforcement learning learn improve network design. order supervise learning process methods rely actual training processes provide feedback costly time computational resources. work differs essentially. instead developing automatic design technique focus crucial often overlooked problem quickly performance feedback. network performance prediction. mentioned approach predict network performance. emerging topic existing works remain limited. previous methods performance prediction developed context hyperparameter optimization using techniques like gaussian process last-seenvalue heuristics works mainly focus designing special surrogate function better evaluation hyper-conﬁgurations. also attempts directly predict network performance. works along line intend extrapolate future part learning curve given elapsed part. this domhan proposed mixture parametric functions model learning curve. klein extended work replacing mixture functions bayesian neural network. baker furthered study additionally leveraging information network architectures handcrafted features using v-svr curve prediction. works rely partially observed learning curves make predictions still involve partly training procedure therefore time-consuming. support large-scale search network designs desire much quicker feedback thus explore fundamentally different challenging approach predict performance purely based architectures. figure overall pipeline peephole framework. given network architecture ﬁrst encodes layer vector integer coding layer embedding. subsequently applies recurrent network lstm units integrate information individual layers following network topology structural feature. structural feature together epoch index ﬁnally predict accuracy corresponding time point i.e. given epoch. note blocks indicated green color including embeddings lstm jointly learned end-to-end manner. called peephole shown figure formalized function denoted function takes arguments network architecture epoch index produces scalar value prediction accuracy t-th epoch. here incorporating epoch index input reasonable validation accuracy generally changes training proceeds. therefore predict performance speciﬁc time point prediction. note formulation differs fundamentally previous works methods require observation initial part training curve extrapolate remaining part. contrary method aims predict entire curve relying network architecture. provide feedback much quicker thus particularly suited large-scale search network designs. however developing predictor nontrivial. towards goal facing signiﬁcant technical challenges e.g. unify representation various layers integrate information individual layers various network topologies. follows present answers questions. particularly sec. presents uniﬁed vector representation layers constructed steps namely coding embedding. sec. presents lstm model integrating information across layers. general convolutional neural network considered directed graph whose nodes represent certain operations e.g. convolution pooling. hence develop representation graph ﬁrst step deﬁne representation individual nodes i.e. layers. table coding table uniﬁed layer code corresponds layer type. layer encode type kernel width kernel height channel number note ratio output channel number input number instead absolute value quantize integer. integer coding. notice operations commonly used including convolution pooling nonlinear activation considered applying kernel input feature map. produce output value kernel takes local part feature input applies linear nonlinear transform yields output. particular element-wise activation function considered nonlinear kernel size operation also characterized number output channels. typical number channels vary signiﬁcantly thousands. however speciﬁc layer number usually decided based input according ratio within limited range. particularly pooling nonlinwork. work focus networks sequential structures already constitute signiﬁcant portion networks used real-world practice. here challenge cope varying depths uniform way. inspired success recurrent networks sequential modeling e.g. language modeling video analytics choose explore recurrent networks problem. speciﬁcally adopt long-short term memory effective variant integrating information along sequence layers. particular lstm network composed series lstm units time step lstm maintains hidden state cell memory uses input gate output gate forget gate control information ﬂow. step takes input decides value gates yields output updates hidden state cell memory follows here denotes sigmoid function elementwise product. along low-level high-level layers lstm network would gradually incorporate layer-wise information hidden state. last step i.e. layer right fully connected layer classiﬁcation extract hidden state lstm cell represent overall structure network refer structural feature. shown figure peephole framework ﬁnally combine structural feature epoch index multi-layer perceptron make ﬁnal prediction accuracy. particular component ﬁnal step comprised three fully connected layers batch normalization relu activation. output component real value serves estimate accuracy. like predictive models peephole requires sufﬁcient training samples learn parameters. however problem preparation training challenge. randomly sampling sequences layers viable solution reasons design space grows exponentially number layers increases figure layer embedding component. takes integer codes input maps embedded vectors respectively table lookup ﬁnally concatenates real vector representation. note share lookup table. activation numbers output channels always equal input channels thus ratio convolution ratio usually ranges depending whether operation intends reduce preserve expand representation dimension. light this choose represent output-input ratio instead absolute number. effectively limit dynamic range quantize bins overall represent common operation tuple four integers form integer indicates type computation respectively width height kernel represents ratio output-input channels details scheme summarized table layer embedding. capturing information layer discrete representation introduced amenable complex numerical computation deep pattern recognition. inspired word embedding strategy proven effective natural language processing take step develop layer embedding scheme turn integer codes uniﬁed real-vector representation. shown figure embedding done table lookup. speciﬁcally module associated three lookup tables respectively layer types kernel sizes channel ratios. note kernel size table used encode given tuple integers convert element real vector retrieving corresponding lookup table. concatenating embedded vectors derived respectively individual integers form vector representation layer. types empirically estimated practical networks. example convolution layer high chance followed batch normalization layer nonlinear activation. activation layer likely ensued another convolution layer pooling layer. details provided supplemental materials. collection blocks build complete network assembling following skeleton. design skeleton follows general practice computer vision. shown figure skeleton comprises three stages different resolutions. stage stack blocks followed pooling layer reduce spatial resolution. features last block average pooling layer linear layer classiﬁcation. replicating blocks within stage convolution layers inserted dimension adaptation output dimension preceding layer match input dimension next layer. block-based generation scheme presented effectively constrain sample space ensuring generated networks mostly reasonable making feasible prepare training affordable budget. given sample networks {xi}n obtain performance curves network i.e. validation accuracy function epoch numbers training network given dataset. hence obtain pairs learn parameters predictor supervised way. here denotes predictor parameters. note train sample network epochs results ﬁnal epoch supervise learning process. framework ﬂexible entire learning curves principle results multiple epochs training. however found empirically using ﬁnal epochs already yields reasonably good results. first peephole predictor task-speciﬁc. trained predict performance certain dataset speciﬁc performance metric. second besides network architectures epoch numbers performance network also depends number factors e.g. initialized learning rate adjusted time well settings optimizers. work train sample networks ﬁxed design choices. figure block-based generation. ﬁrst generate individual blocks assemble network following skeleton predeﬁned dataset layers blue color ﬁxed skeleton. different networks dataset differ mainly block designs. expensive obtain training sample hence unaffordable explore entire design space freely even large amount computational resources. many combinations layers reasonable options practical point view training networks simply waste resources. section draw inspirations existing practice propose block-based generation scheme acquire training samples sec. then present learning objective supervising training process sec. engineering practice network design suggests good strategy construct neural network stacking blocks structurally alike. zoph zhong also proposed search transferable blocks assemble network efforts towards automatic network search. inspired works propose block-based generation simple effective strategy prepare training samples. illustrated figure ﬁrst designs individual blocks stacks network following certain skeleton. block deﬁned short sequence layers layers. generate block follow markov chain. speciﬁcally begin convolution layer randomly choosing kernel size ratio output/input channel numbers step draw next layer conditioned current following predeﬁned transition probabilities layer table performance statistics networks respectively sampled cifar- mnist. here size means number sampled networks mean respectively mean standard deviation minimum maximum validation accuracies corresponding sets networks. admittedly setting sound restrictive. however actually reﬂects typical practice tuning network designs ablation studies. moreover automatic network search schemes also choices search process order fairly compare among architectures. therefore predictor trained already provide good support practice. being said plan incorporate additional factors predictor future exploration. tested peephole proposed network performance prediction framework public datasets cifar mnist sec. presents experiment settings including datasets used implementation details framework. sec. presents results obtained datasets compares performance prediction methods. sec. presents preliminary results using peephole guide search better networks imagenet. finally sec. presents qualitative study learned representations visualization. experiment conﬁgurations datasets. cifar- dataset object classiﬁcation. recent years usually serves testbed convolutional network designs. mnist dataset hand-written digit classiﬁcation early widely used datasets neural network research. datasets moderate scale. chose basis study affordable train thousand networks thereon investigate effectiveness proposed predictor. goal explore performance prediction method works diverse architectures instead pursing state-of-the-art network large-scale vision benchmarks. prepare samples training validation follow procedure described detailed settings. fair comparison train sampled networks setting momentum weight decay epoch loops entire training random order. learning rate initialized scaled factor every epochs epochs network weights initialized following scheme table shows statistics networks performances. peephole model -dimensional vectors layer embedding epoch embedding. dimension hidden states lstm multi-layer perceptron ﬁnal prediction comprises linear layers hidden units. comparison prediction accuracies methods compare. compare peephole method representative methods recent works bayesian neural network method devised extrapolate learning curves given initial portions represents curve linear combination basis functions uses bayesian neural network yield probabilistic extrapolations. ν-support vector regression method relies regression model ν-svr make predictions. predict performance network model takes input initial portion learning curve simple heuristic features derived based network architecture. method represents state task. kendall’s measures correlation predictive rankings among testing networks actual rankings. value kendall’s ranges higher value indicates higher correlation. results cifar-. table compares prediction results networks trained cifar- obtained different predictors. observe peephole consistently outperforms ν-svr across metrics. particularly achieving smaller means predictions peephole generally accurate others. makes viable predictor practice. hand high values indicate ranking among multiple networks produced peephole quite consistent ranking actual performances. makes peephole good criterion select performant network architectures. scatter plots figure visualize correlations predicted accuracies actual accuracies obtained different methods. qualitatively predictions made peephole demonstrate notably higher correlation actual values methods especially high-accuracy area results mnist. also evaluated predictions networks trained mnist results shown table note since networks yield high accuracies mnist would easier produce precise predictions accuracy numbers difﬁcult yield consistent rankings. reﬂected performance metrics table. despite difference data characteristics peephole still signiﬁcantly outperforms methods across metrics. figure scatter plots x-axes representing actual validation accuracies cifar- y-axes representing predicted accuracies. plots show much predictions correlated actual values. table accuracies imagenet obtained network selected based peephole predictions. here pred predicted accuracy cifar-. real actual accuracy cifar-. param number parameters. top- top- indicates accuracies obtained imagenet. getting performance imagenet holy grail convolutional network design. directly training peephole based imagenet prohibitively expensively lengthy processes training network imagenet. nevertheless suggests alternative search scalable transferable block architectures smaller dataset like cifar-. following idea select network architecture highest peepholepredicted accuracy among validation cifar- scale transfer imagenet. compared network vgg- widely used network designed manually. results table selected network achieves moderately better accuracy imagenet substantially smaller parameter size. preliminary study. shows peephole promising pursuing performant network designs transferable larger datasets. figure responses cell learned lstm block architectures. color drawn according response value cell. right color used ﬁgure darker color represents higher responses. every time convolution layer appears response higher ﬁgure better viewed color. study examined hidden cells inside lstm using method presented particularly recorded dynamics cell responses lstm traverses sequence layers. figure shows responses certain cell response raises every time gets convolution layer. behavior observed different blocks. observation suggests cell learns detect convolution layers even without explicitly directed certain sense also reﬂects capability lstm capture architectural patterns. another study visualized structural feature using t-sne embedding figure shows visualized results. gradual transition performance networks high performance networks. shows structural features contain information related network performances. presented peephole predictive model predicting network performance based architectures training. speciﬁcally developed uniﬁed layer code uniﬁed representation network architectures lstm-based model integrate information individual layers. tackle difﬁculties preparing training propose block-based generation scheme allows explore wide variety reasonable designs constraining search space. systematic studies thousand networks trained cifar mnist showed proposed method yield reliable predictions highly correlated actual performance. three different metrics method signiﬁcantly outperforms previous methods. note ﬁrst step towards goal fast search network designs. future work plan incorporate additional factors predictor various design choices extend applicability predictive model. also explore effective ways optimize network designs predictive model.", "year": 2017}