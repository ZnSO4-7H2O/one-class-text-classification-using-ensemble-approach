{"title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.", "text": "paine* pooya khorrami* thomas huang beckman institute advanced science technology university illinois urbana-champaign urbana painepkhorraweihant-huangillinois.edu convolutional neural networks perform well object recognition number recent advances rectiﬁed linear units data augmentation dropout large labelled datasets. unsupervised data proposed anway improve performance. unfortunately unsupervised pre-training used state-of-the-art methods leading following question unsupervised pre-training still useful given recent advances? when? answer three parts develop unsupervised method incorporates relus recent unsupervised regularization techniques analyze beneﬁts unsupervised pre-training compared data augmentation dropout cifar- varying ratio unsupervised supervised samples verify ﬁndings stl-. discover unsupervised pre-training expected helps ratio unsupervised supervised samples high surprisingly hurts ratio low. also unsupervised pre-training additional color augmentation achieve near state-of-the-art performance stl-. analyze beneﬁts unsupervised pre-training context recent deep learning innovations including rectiﬁed linear units data augmentation dropout. recent work shows convolutional neural networks achieve state-of-the-art performance object classiﬁcation object detection enough training data. however many cases dearth labeled data. cases regularization necessary good results. common types regularization data augmentations dosovitskiy dropout another form regularization unsupervised pre-training bengio erhan recently fallen favor. signiﬁcant work unsupervised learning works came rectiﬁed linear units signiﬁcantly help training deep supervised neural networks simpler regularization schemes unsupervised learning zero-bias linear encoding auto-encoders train unsupervised method takes advantage improvements call zero-bias convolutional auto-encoders previous work showed pre-trained tanh caes achieved increase performance randomly initialized tanh cnns. conduct experiment zero-bias observe larger boost performance. analyze effectiveness technique combined popular regularization techniques used supervised training cifar- varying ratio unsupervised supervised samples. comparing randomly initialized cnns without additional regularization. that ratio large unsupervised pre-training provides useful regularization increasing test performance. ratio small unsupervised pre-training hurts performance. verify ﬁnding unsupervised pre-training boost performance ratio unsupervised supervised samples high running algorithm stl- dataset ratio expected observe improvement combined additional color augmentation achieve near state-of-the-art results. unsupervised regularization still yields improvement begin reviewing related work fully-connected convolutional auto-encoders. section present method trained unsupervised pre-training supervised ﬁne-tuning. present results cifar- stl- datasets section section conclude paper. many methods used unsupervised learning learn parameters subsequently used initialize neural network trained supervised data. called unsupervised pretraining supervised ﬁne-tuning respectively. highlight unsupervised learning methods related work. equation represents weight matrix transforms input hidden representation vector biases hidden unit nonlinear function. commonly chosen examples include sigmoid hyperbolic tangent functions. meanwhile weight matrix maps back hidden representation input space vector biases input unit. parameters commonly learned minimizing loss function training data stochastic gradient descent. constraints imposed loss function auto-encoder weights tend learn identity function. combat this form regularization must imposed upon model model uncover underlying structure data. forms regularization include adding noise input units requiring hidden unit activations sparse small derivatives models known de-noising sparse contractive auto-encoders respectively. recent work memisevic showed training auto-encoder rectiﬁed linear units caused activations form tight clusters negative bias values. showed using thresholded linear thresholded rectiﬁer activations bias allow train auto-encoder without need additional regularization. aforementioned fully-connected techniques shown impressive results directly address structure images. convolutional neural networks present reduce number connections hidden unit responsible small local neighborhood visible units. schemes allow dense feature extraction followed pooling layers stacked could allow network learn larger larger receptive ﬁelds. convolutional auto-encoders combined aspects auto-encoders convolutional neural nets making possible extract highly localized patchbased information unsupervised fashion. several works area including jarrett zeiler rely sparse coding force unsupervised learning learn non-trival solutions. zeiler extended work introducing pooling/unpooling visualizing individual feature maps different layers inﬂuenced speciﬁc portions reconstruction. sparse coding approaches limitations used iterative procedure inference. later work masci trained deep feed forward convolutional auto-encoders using max-pooling saturating tanh non-linearities form regularization still showing modest improvement randomly initialized cnns. tanh natural choice time krizhevsky showed relus suitable learning given non-saturating behavior. method incorporates aspects previous unsupervised learning methods order learn salient features efﬁcient train. model similar deconvolutional network zeiler cost minimize layer mean square error original image. however unlike network zeiler method form sparse coding. model also similar masci however improve upon introducing regularization convolutional layers zero-biases relus discussed memisevic describe model architecture detail. like previous work described above model involves several encoding modules followed several decoding modules. single encoding module consists convolution layer nonlinearity followed pooling layer switches train encoder/decoder pair greedy fashion keeping parameters previous layers ﬁxed. like zeiler compute cost taking mean squared error original image network’s reconstruction input. thus costs layer network layer network would expressed following manner regularize learned representation ﬁxing biases convolutional deconvolutional layers zero using relus activation function encoding. linear activations decoders. unlike work memisevic analyzes fullyconnected auto-encoders work ﬁrst knowledge trains zero-bias caes unsupervised learning. weight initialization often component successful neural network training. relu’s important ensure input relu greater achieved setting bias appropriately. cannot done zero-bias auto-encoders. instead methods initializing weights achieve ﬁrst layer initialize ﬁlters randomly drawn patch dataset later layers sample weights gaussian distribution nearest orthogonal matrix taking singular value decomposition weight matrix setting singular values one. cnns must take account additive effect overlapping patches thus weight ﬁlter hamming window prevent intensity build-up. weights trained remove decoder modules leave encoding modules. additional fully-connected layer softmax layer pretrained encoding modules. weights layers drawn gaussian distribution zero mean standard deviation unsupervised supervised training stochastic gradient descent constant momentum weight decay parameter select highest learning rate doesn’t explode duration training. experiments anneal learning rate. pre-processing patch centering scaling unit variance. experiments natural image datasets cifar- stl- cifar- common benchmark object recognition. many unsupervised supervised neural network approaches tested consists pixel color images drawn object categories. training images testing images. stl- also object recognition benchmark designed test unsupervised learning algorithms relatively small labeled training images class additional unsupervised contains unlabeled images. test contains labeled images class. examples pixel color images. cifar- train network structure similar masci directly show beneﬁts modiﬁcations. network consists three convolutional layers ﬁlters respectively. ﬁlters ﬁrst layers size ﬁlters third layer size also pooling layers ﬁrst convolutional layers. also full-connected layer hidden units followed softmax layer output units. nets trained using open source neural network library stated methods section ﬁrst train unsupervised model training images supervised ﬁne-tuning report overall accuracy test set. present qualitative results unsupervised learning show zero-bias convolutional auto-encoder performs well compared previous convolutional auto-encoder work masci developed popularization rectiﬁed linear units zero-bias auto-encoders show analysis various regularization techniques vary ratio unsupervised supervised data completeness report best results training full cifar- dataset however main point work. ﬁlters presented masci trained additional zero-bias convolutional auto-encoder ﬁlters size ﬁrst layer. figure that indeed model able capture interpretable patterns gabor-like oriented edges center-surrounds. quantitative experiments ﬁrst compare performance tanh proposed masci zero-bias cae. paper masci trained tanh random initialization compared pre-trained using tanh cae. also added translations form data augmentation. re-conduct experiment using zero-bias trained random initialization compare pretrained using zero-bias cae. table compare improvements model masci various subsets cifar-. expected zero-bias performs signiﬁcantly better tanh interestingly notice subset compared masci pre-trained model shows similar better performance randomly initialized cnn. ratio unsupervised supervised data high experience increase accuracy opposed masci increase. next analyze different supervised regularization techniques affect model’s performance. speciﬁcally consider effects dropout data augmentation unsupervised pre-training combinations. compare regularization technique zero-bias trained random initialization withregularization figure shows classiﬁcation accuracy improvement type regularization individually together. perform analysis subsets cifar- different unsupervised supervised sample ratios ranging ﬁxing unsupervised data size varying number supervised examples. important note ratio approaches experimental setup favors data augmentation dropout number virtual supervised samples larger number unsupervised samples. figure ratio unsupervised supervised samples three notable effects unsupervised pre-training alone yields larger improvement data augmentation dropout unsupervised pre-training combined either data augmentation dropout improvement greater individual contributions experience largest gains combine three forms regularization. effect also observed case ratio unsupervised supervised samples lesser extent ratio unfortunately effects observed ratio unsupervised supervised samples decreases. elaborate effect below. figure observe improvement performance unsupervised learning decreases rapidly ratio unsupervised supervised samples decreases. surprisingly ratio unsupervised learning actually hurts performance also compare performance algorithm full cifar- dataset techniques table though show method performs worse ratio unsupervised supervised samples outperform methods unsupervised pre-training coates dosovitskiy kung however competitive supervised state-of-the-art. include representative supervised methods table next assess effects unsupervised pre-training stl-. cifar- experiments clear unsupervised pre-training beneﬁcial unsupervised dataset much larger supervised dataset. stl- designed mind ratio unsupervised supervised data experimentally show beneﬁt. design network structure similar dosovitskiy ease comparison. network used consists convolutional layers ﬁlters layer fully-connected layer units softmax layer output units. also apply maxpooling layers size ﬁrst convolutional layers quadrant pooling third convolutional layer. train zero-bias unlabeled images. ﬁne-tune network provided splits training consisting samples evaluate test set. accuracies subsequently averaged obtain ﬁnal recognition accuracy. similar cifar- experiments also train zero-bias structure zero-bias splits highlight beneﬁts unsupervised learning. figure beneﬁts unsupervised learning unsupervised supervised sample ratio. ratio increase performance. beneﬁt shrinks ratio decreases. ratio penalty using unsupervised pre-training. current best result stl- makes extensive additional data augmentation including scaling rotation color forms contrast. perform augmentations supervised training discriminative unsupervised feature learning period. test regularizing effects additional augmentations applied directly supervised training test regularization effects hold combined unsupervised pre-training. this additional data-augmentations supervised training color augmentation contrast augmentation. color augmentation images represented color space generate single random number image value pixel like algorithm convolutional auto-encoders masci single layer k-means coates convolutional k-means networks coates exemplar dosovitskiy convolutional kernel networks mairal nomp kung max-out networks goodfellow network network deeply-supervised nets zero-bias +adu zero-bias algorithm convolutional k-means networks coates convolutional kernel networks mairal hierarchical matching pursuit nomp kung multi-task bayesian optimization swersky exemplar dosovitskiy zero-bias zero-bias +adu zero-bias +adc zero-bias +adcu present type convolutional auto-encoder zero-bias relu activations achieves superior performance previous methods. conduct thorough experiments cifar analyze effects unsupervised pre-training form regularization used isolation combination supervised forms regularization data augmentation dropout. observe that indeed unsupervised pre-training provide large gain performance ratio unsupervised supervised samples large. finally verify ﬁndings applying model stl- dataset unlabeled samples labeled samples additional regularization color augmentation method able achieve nearly state-of-the-art results. material based upon work supported national science foundation grant iis-. tesla gpus used research donated nvidia corporation. would like acknowledge theano pylearn code based. also would like thank shiyu chang many helpful discussions suggestions. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano proceedings python scientiﬁc computing conference math expression compiler. june oral presentation. adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. international conference artiﬁcial intelligence statistics pages alexey dosovitskiy jost tobias springenberg martin riedmiller thomas brox. discriminative unsupervised feature learning convolutional neural networks. ghahramani welling cortes n.d. lawrence k.q. weinberger editors advances neural information processing systems pages curran associates inc. dumitru erhan yoshua bengio aaron courville pierre-antoine manzagol pascal vincent samy bengio. unsupervised pre-training help deep learning? journal machine learning research goodfellow david warde-farley pascal lamblin vincent dumoulin mehdi mirza razvan pascanu james bergstra fr´ed´eric bastien yoshua bengio. pylearn machine learning research library. arxiv preprint arxiv. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutimproving neural networks preventing co-adaptation feature detectors. corr kevin jarrett koray kavukcuoglu ranzato yann lecun. best multi-stage architecture object recognition? computer vision ieee international conference pages ieee alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages honglak roger grosse rajesh ranganath andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning pages tsung-han kung. stable efﬁcient representation learning nonnegativity constraints. tony jebara eric xing editors proceedings international conference machine learning pages jmlr workshop conference proceedings julien mairal piotr koniusz zaid harchaoui cordelia schmid. convolutional kernel networks. ghahramani welling cortes n.d. lawrence k.q. weinberger editors advances neural information processing systems pages curran associates inc. jonathan masci ueli meier cires¸an j¨urgen schmidhuber. stacked convolutional autoencoders hierarchical feature extraction. artiﬁcial neural networks machine learning– icann pages springer salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning pages pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research matthew zeiler dilip krishnan graham taylor robert fergus. deconvolutional networks. computer vision pattern recognition ieee conference pages ieee matthew zeiler graham taylor fergus. adaptive deconvolutional networks high level feature learning. computer vision ieee international conference pages ieee", "year": 2014}