{"title": "Sparse Image Representation with Epitomes", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Sparse coding, which is the decomposition of a vector using only a few basis elements, is widely used in machine learning and image processing. The basis set, also called dictionary, is learned to adapt to specific data. This approach has proven to be very effective in many image processing tasks. Traditionally, the dictionary is an unstructured \"flat\" set of atoms. In this paper, we study structured dictionaries which are obtained from an epitome, or a set of epitomes. The epitome is itself a small image, and the atoms are all the patches of a chosen size inside this image. This considerably reduces the number of parameters to learn and provides sparse image decompositions with shiftinvariance properties. We propose a new formulation and an algorithm for learning the structured dictionaries associated with epitomes, and illustrate their use in image denoising tasks.", "text": "figure dictionary epitome sparse coding epitome similar sparse coding dictionary except atoms extracted epitome overlap instead chosen unstructured patches assumed independent other. signature dictionary applied image denoising. formulation uniﬁes concept epitome dictionary learning allowing image patch represented sparse linear combination several patches extracted epitome resulting sparse representations highly redundant dictionaries represented reasonably small number parameters representation also proven useful texture synthesis different line work research focusing learning shift-invariant dictionaries sense possible dictionary elements different shifts represent signals exhibiting patterns appear several times different positions. different image-signature dictionaries aharon elad ideas related shown paper shift invariance achieved using collection smaller epitomes. fact main contributions unify frameworks epitome dictionary learning establish continuity dictionaries dictionaries shift invariance epitomes. propose formulation based concept epitomes/image-signature-dictionaries introduced allows learn collection epitomes generic enough used epitomes sparse coding decomposition vector using basis elements widely used machine learning image processing. basis also called dictionary learned adapt speciﬁc data. approach proven effective many image processing tasks. traditionally dictionary unstructured atoms. paper study structured dictionaries obtained epitome epitomes. epitome small image atoms patches chosen size inside image. considerably reduces number parameters learn provides sparse image decompositions shiftinvariance properties. propose formulation algorithm learning structured dictionaries associated epitomes illustrate image denoising tasks. jojic frey kannan introduced probabilistic generative image model called epitome. intuitively epitome small image summarizes content larger sense patch large image similar epitome. intriguing notion applied image reconstruction tasks epitomes also extended video domain used denoising superresolution object removal video interpolation. successful applications epitomes include location recognition face recognition formulation removes constraint replaces -norm weighted -norm. shown appendix equivalent sense solution also solution every solution solution obtained normalizing columns one. best knowledge equivalent formulation learning epitome regularization convex regularizer -norm) empirically provides better-behaved dictionaries pseudo-norm counts number nonzero elements vector) denoising tasks differentiates formulation prevent degenerate solutions dictionary learning formulation -norm important constrain dictionary elements -norm. whereas constraint easily imposed classical dictionary learning extension epitome learning straightforward original formulation compatible convex regularizers. equivalent unconstrained formulation lends well epitome learning. formally introduce general concept encoded epitome small image size vector also introduce linear operator rm×p extracts overlapping patches epitome rearranges columns matrix rm×p integer number overlapping patches. concretely context interpreted traditional dictionary elements except generated small number parameters compared parameters dictionary. approach thus generalizes much wider range epitomic structures using mapping admits fast projections functions used relatively simple give framework easily extends families epitomes shift-invariant dictionaries plain dictionaries. assumption make linear operator rank list exhaustive naturally opens perspectives. fact dictionary obtained epitome characterized fact image linear operator given dictionary unique epitome representation obtained computing inverse closed form using pseudo-inverses exists shown appendix different shapes different dictionary parameterizations. present formulation speciﬁc case image patches simplicity applies spatiotemporal blocks straightforward manner. paper organized follows section introduces formulation. present dictionary learning algorithm section section introduces different improvements algorithm section demonstrates experimentally usefulness approach. given training image patches size pixels represented columns matrix rm×n classical dictionary learning formulation introduced revisited tries dictionary rm×p signal represented sparse linear combination columns precisely dictionary learned along matrix decomposition coefﬁcients rp×n every signal following consider following formulation quadratic term ensures vectors close approximation -norm induces sparsity coefﬁcients controls amount regularization. prevent columns arbitrarily large dictionary constrained belong convex matrices rm×p whose columns -norm less equal constraint adapted dictionaries extracted epitomes since overlapping patches cannot expected norm. thus introduce unconstrained formulation equivalent since optimization problem invariant multiplying scalar inverse proceed following renormalization ensure numerical stability prevent entries becoming large rescale ﬁxed recall denotes j-th row. function differentiable except column equal zero assume without loss generality case. suppose indeed column equal zero. then without changing value cost function corresponding zero well results function deﬁned depend anymore. have however observed situation experiments. several motivations approach. discussed above choice function lets adapt technique different problems multiple epitomes type dictionary representation. formulation therefore deliberately generic. practice mainly focused simple cases experiments paper single epitome epitomes. furthermore come traditional well studied problem dictionary learning. therefore techniques algorithms developed dictionary learning literature solve epitome learning problem. basic algorithm classical dictionary learning optimization problem jointly convex convex respect ﬁxed vice-versa. block-coordinate descent scheme alternates optimization keeping parameter ﬁxed emerged natural simple learning dictionaries proven relatively efﬁcient training large. even though formulation remains nonconvex therefore method guaranteed global optimum proven experimentally good enough many tasks therefore adopt optimization scheme well detail different steps below. note algorithms stochastic gradient descent could used well fact easily derived material section. however chosen investigate kind techniques simplicity reasons. indeed stochastic gradient descent algorithms potentially efﬁcient block-coordinate scheme mentioned above require tuning learning rate. step optimization fixed. step algorithm ﬁxed constraint involved optimization furthermore note updating matrix consists solving independent optimization problems respect column them solve weighted optimization problem. consider update column diagd ..dp] non-singular show closed form hand efﬁcient algorithmic procedure performing projection. method therefore quite generic adapt wide variety functions extending linear still injective efﬁcient method project topic future work. present section several improvements basic framework either improve convergence speed algorithm generalize formulation. accelerated gradient method updating ﬁrst improvement accelerate convergence update using accelerated gradient technique methods build upon early works nesterov attracted attention recently machine learning signal processing especially because fast convergence rate ability deal large possibly nonsmooth problems. whereas value objective function classical gradient descent algorithms solving smooth convex problems guaranteed decrease convergence rate number iterations algorithmic schemes proposed convergence rate cost iteration classical gradient algorithms difference methods gradient descent algorithms sequences parameters maintained iterative procedure update uses information past iterations. leads theoretically better convergence rates often also better practice. chosen simplicity algorithm fista beck teboulle includes practical line-search scheme automatically tuning gradient step. interestingly indeed observed algorithm fista signiﬁcantly faster converge projected gradient descent algorithm. multi-scale version improve results without increasing computing time also implemented multi-scale approach exploits spatial nature epitome. instead directly learning epitome size ﬁrst learn epitome smaller size reduced image corresponding smaller patches upscaling resulting epitome initialization next scale. iterate process practice three times. procedure illustrated figure intuitively learning smaller epitomes easier task directly learning large procedure provides good initialization learning large epitome. another improvement consider single epitome family epitomes order learn dictionaries shift invariance focus recent work note different types structured dictionaries also proposed motivation learning shift-invariant features image classiﬁcation tasks signiﬁcantly different framework comes different sparsity-inducing penalization). mentioned before able learn epitomes instead single changing function introduced earlier. vector contains pixels several small epitomes linear operator extracts overlapping patches epitomes. projector still easy compute closed form rest algorithm stays unchanged. epitomic structures could easily used within framework even though limited multi-epitome version approach seen interpolation classical dictionary single epitome. indeed deﬁning multitude epitomes size considered patches equivalent working dictionary. deﬁning large number epitomes slightly larger patches equivalent shiftinvariant dictionaries. section experimentally compare different regimes task image denoising. nonconvexity optimization problem question initialization important issue epitome learning. already mentioned multiscale strategy overcome issue ﬁrst scale problem remains. whereas classical dictionaries naturally initialized prespeciﬁed dictionaries overcomplete basis epitome admit natural choice. experiences initialization single epitome common experiments learned using algorithm initialized gaussian low-pass ﬁltered random image random patches extracted natural images difference contrast scaling data displaying process. experiment illustrates different initializations lead visually different epitomes. whereas property might desirable classical dictionary learning framework also suffers issue successful applications image processing figure three epitomes obtained boat image different initializations parameters. left epitome obtained initialization epitome learned random patches natural images. middle right epitomes obtained different random initializations. size patches seem play important role visual aspect epitome. illustrate figure experiment pairs epitome size learned different sizes patches. provide section qualitative quantitative validation. ﬁrst study inﬂuence different model hyperparameters visual aspect epitome moving image denoising task. choose represent epitomes images order visualize easily patches extracted form images. since epitomes contain negative values arbitrarily rescaled display. approximate noisy patch using learned dictionary greedy algorithm called orthogonal matching pursuit clean estimate every patch addressing following problem present section experiment number learned epitomes vary keeping numbers columns epitomes learned image barbara shown figure number epitomes small observe epitomes discontinuities texture areas different visual characteristics case learning several independant epitomes. order evaluate performance epitome learning various regimes methodology uses successful denoising method ﬁrst introduced consider ﬁrst classical problem restoring noisy image quantitative results single epitome multi-scale multi-epitomes presented table images levels noise. evaluate performance denoising process computing peak signal-to-noise ratio pair images. level noise selected best regularization parameter overall images used experiments. pnsr values averaged experiments different noise realizations. mean standard deviation single epitome multi-scale multi-epitomes. experiment formulation propose competitive compared learning multi epitomes instead single seems provide better results might explained lack ﬂexibility single epitome representation. evidently results good recent state-of-the-art denoising algorithms exploit sophisticated appendix show compute orthogonal projection vector space denote binary matrix }m×m extracts i-th patch note notation matrix binary matrix corresponding linear operator takes patch size place location epitome size zero everywhere else. therefore denote rm×p linear operator table quantitative comparative evaluation. psnr values averaged images. compare previous epitome learning based algorithms epitomes jojic frey kannan reported three elaborate dictionary learning based algorithms k-svd lssc introduced paper formulation efﬁcient algorithm learning epitomes context sparse coding extending work aharon elad unifying recent work shift-invariant dictionary learning. approach generic interpolate between regimes possibly applied formulations. future work extend framework video setting image processing tasks inpainting learning image features classiﬁcation recognition tasks shift invariance proven property achieving good results another direction pursuing encode invariant properties different mapping functions nesterov. gradient methods minimizing composite objective function. technical report center operations research econometrics catholic university louvain peyré. sparse modeling textures. journal mathematical imaging vision thiagarajan ramamurthy spanias. shift-invariant sparse representation images using learned dictionaries. ieee international workshop machine learning signal processing since vec)) r−rt orthogonal projection onto results following properties useful framework classical signal processing overcomplete representations", "year": 2011}