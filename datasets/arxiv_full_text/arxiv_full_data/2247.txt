{"title": "Low Precision RNNs: Quantizing RNNs Without Losing Accuracy", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Similar to convolution neural networks, recurrent neural networks (RNNs) typically suffer from over-parameterization. Quantizing bit-widths of weights and activations results in runtime efficiency on hardware, yet it often comes at the cost of reduced accuracy. This paper proposes a quantization approach that increases model size with bit-width reduction. This approach will allow networks to perform at their baseline accuracy while still maintaining the benefits of reduced precision and overall model size reduction.", "text": "similar convolution neural networks recurrent neural networks typically suffer over-parameterization. quantizing bit-widths weights activations results runtime efficiency hardware often comes cost reduced accuracy. paper proposes quantization approach increases model size bit-width reduction. approach allow networks perform baseline accuracy still maintaining benefits reduced precision overall model size reduction. recurrent neural networks type neural network memory aspect allowing network draw previous knowledge. rnns loop cyclical information used interpret assess current information processed. rnns typically used translation language modeling become increasingly popular industry research impressive performance owing state accuracy networks drawn considerable interest hardware community various studies proposed methods reduce runtime complexity rnns using quantization schemes lowering precision data types used networks lowers storage size runtime memory requirements makes rnns amenable deployed low-memory hardware platforms. popular methods quantize network include reducing bit-widths order shrink size network directly reducing number bits needed weights biases activations also decreasing complexity computations. bit-width reduction however come cost network accuracy drastic loss accuracy seen networks reduced original width. paper investigates quantization rnns without sacrificing network accuracy overall goal efficient hardware implementation. scheme pair reduction width increase model dimension i.e. increasing number neurons layer. method retain accuracy still reducing network size runtime memory usage thus achieving baseline accuracy fraction memory cost baseline network. quantization method similar proposed study quantization scheme along increasing number neurons layer extensively language model penn treebank dataset apply scheme increasing number neurons lowering bit-width baidu’s deep speech model results show deep number neurons layer neural network representative computing complexity network. typically number neurons directly affects accuracy network. increasing number neurons layer increase network accuracy allow discriminative power within layer well layer layer. neuron increase approach target certain layers model order re-gain accuracy lost bit-width quantization. done first evaluating network layer layer order identify smaller layers benefit increase neurons still maintaining reduced model size. layers neuron increase potential identified altered factor percentage order increase neurons. layers retain original amount neurons. ncreasing decreasing neurons layer linear relationship terms number compute operations layer size. reason layers targeted neuron increase never network’s larger layers. find increasing neurons certain layers combined bit-width quantization effectively retains accuracy. model size slightly increased neurons added certain layers values weights biases activations still stored limited bit-widths. result network retain benefit smaller storage size less complex computations. computations simpler instead floating point compute operates integer mode. neuron increase approach studied extensively language model dataset applied deep speech model achieving goal quantized network retains accuracy models. order evaluate rate accuracy deteriorated rnns bit-width reduced language model studied. using quantization methods discussed weights activations long-short-term memory model reduced. neuron increase approach applied order effect re-gaining lost accuracy quantized rnns. model used testing consisted lstm layer containing neurons tested dataset. dataset considered small size contains unique words. quantized model called bit-rnn measures accuracy perplexity word table summarizes results quantized experiments model. table shows decreasing bit-width lstm model much drastic effect model accuracy accuracy drops approximately activation bits halved drops approximately weight bits halved. sing quantized bit-rnn models neuron increase approach tested relationship increasing neurons network accuracy. bit-rnn model consists layer neuron increase approach applied without needing layer layer network evaluation. bit-rnn model quantized neuron increase approach applied model size reduction runtime memory usage evaluated. quantization methods used leads model size earlier increasing number neurons linear effect model size. however model size increase also lower bit-width lowers overall memory requirements. able shows effect increasing number neurons network neurons increased full accuracy regained -bit weights -bit activations. accuracy within -bit weights -bit activations number neurons increased find accuracy plateau neurons increased beyond additionally -bit weights -bit activations model size baseline model size runtime memory usage neurons increased original amount network surpass baseline accuracy jump model size runtime memory usage. deep speech model speech recognition system perform well large datasets. model serves much larger arena prove effectiveness bit-width reduction paired neuron increase. deep speech model used following experiments constructed layers; five fully connected layers recurrent bidirectional lstm layer. three five layers consist neurons layer feeds lstm layer contains double that entirety lstm layer final layer network contains number neurons characters language used. deep speech model tested first quantizing network applying neuron increase approach. models trained using librispeech training dataset contains approximately hours speech ted-lium training dataset contains approximately hours speech. models tested ted-lium testing dataset. accuracy measured using word error rate recurrent lstm layer. done first quantizing layers weights biases separate model quantizing recurrent lstm layer. resource limitations recurrent lstm layer quantized activations weights. quantized respective network models evaluated neuron increase potential. first quantized model neuron increase first second fifth layers model layers relatively smaller layers network thus greatly effect model size number neurons increased. second quantized model receive neuron increases data previous section’s study suggests activations quantized model accuracy maintained. able shows difference baseline model accuracy reveals accuracy increases neurons added layers increase increase. results show even -bit layer model able retain accuracy neurons first second fifth layer increased effect added neurons model size also measured. -bit layer model neurons increased original size layers five model size increased neurons increased original size baseline accuracy retained model size jumps able shows decreasing activations recurrent layer minimal effect accuracy even reduced bits. additionally runtime memory usage reduced original size lstm activation bits quantized bits original size lstm activation bits quantized bits. results language model dataset deep speech model study conclude bit-width quantization paired neuron increases effectively retain accuracy. language model dataset study revealed quantizing weights much larger effect model accuracy compared quantizing activations neuron increase approach re-build accuracy neuron increase. deep speech model study proved model retain baseline accuracy layers quantized bits three layers neuron increase additionally model size increased order accuracy retained -bit model. future work topic include quantizing weight bits recurrent layer combination activation bits quantizing recurrent layer layers simultaneously. rousseau deléglise estève \"enhancing ted-lium corpus selected data language modeling talks\" proceedings ninth international conference language resources evaluation donahue jeffrey \"long-term recurrent convolutional networks visual recognition description.\" proceedings ieee conference computer vision pattern recognition. miller clifford giles. \"experimental comparison effect order recurrent neural networks.\" international journal pattern recognition artificial intelligence", "year": 2017}