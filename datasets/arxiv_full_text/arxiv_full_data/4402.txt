{"title": "Learning Texture Manifolds with the Periodic Spatial GAN", "tag": ["cs.CV", "stat.ML"], "abstract": "This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.", "text": "paper introduces novel approach texture synthesis based generative adversarial networks extend structure input noise distribution constructing tensors different types dimensions. call technique periodic spatial psgan several novel abilities surpass current state texture synthesis. first learn multiple textures datasets complex large images. second show image generation psgans properties texture manifold smoothly interpolate samples structured noise space generate novel samples perceptually textures original dataset. addition also accurately learn periodical textures. make multiple experiments show psgans ﬂexibly handle diverse texture image data sources. method highly scalable generate output images arbitrary large size. textures important perceptual elements real world visual arts. many textures random noise characteristics formally deﬁned stationary ergodic stochastic processes many natural image examples properties e.g. rice randomly spread ground. however goal texture synthesis learn given example image generating process allows create many images similar properties. classical texture synthesis methods include instance based approaches pixels patches source image resampled copied next similar image regions seamless bigger texture image obtained. methods good visual quality deal periodic images high runtime complexity generating images. addition since learn explicit model images copy patches original pixels cannot used generate novel textures multiple examples. parametric methods deﬁne explicit model good texture specifying statistical properties; texture images optimal w.r.t. speciﬁed criteria synthesized optimization. method yields good results creating various textures including periodic ones however run-time complexity high even small output images. authors also tried blending textures results satisfactory patch-wise mixtures obtained rather homogeneous texture perceptually interpolates originals. recently deep learning methods shown powerful fast data-driven parametric approach texture synthesis. work milestone showed ﬁlters discriminatively trained deep neural network used effective parametric image descriptors. texture synthesis modeled optimization problem. also showed interesting application painting target content photo style given input image neural style transfer. related works speed-up texture synthesis style transfer approximating optimization figure psgans extract textures complex datasets natural images oxford describable textures dataset category scaly. image shows quilt different tiles containing novel synthesized texture originally dataset. methodologically image created setting global dimensions tensor local regions spatial dimensions identical resulting image total size pixels. however choice descriptor related works gram matrix learned ﬁlters speciﬁc prior learnable textures method. generalizes many textures e.g. periodic textures reproduced inaccurately. another limitation texture synthesis performed single example image only lacking ability represent morph textures deﬁned several different images. related work explored blending multiple styles parametrically mixing statistical descriptors. results interesting terms image stylization synthesis novel blended textures shown. purely data driven generative models alternative deep learning approach texture synthesis. introduced generative adversarial networks train model learns data distribution example data discriminator attempts distinguish generated training data. architecture improved using deep convolutional layers stride. gans successfully created natural images great perceptual quality fool even human observers. however pixel resolution usually output image size pre-speciﬁed ﬁxed training time. texture synthesis case fully convolutional layers scale image size advantageous. presented interesting architecture combines ideas gans pre-trained descriptor order generate small patches statistics layer activations network. method allows fast texture synthesis style transfer. spatial applied ﬁrst time fully unsupervised gans texture synthesis. sgans properties like good scalability w.r.t. speed memory showed excellent results certain texture classes surpassing results however classes textures cannot handled plausible texture morphing possible. base design generator network discriminator network dcgan model empirically choosing symmetric architecture turned stabilize learning dynamics. particular chose equal sizes image patches generated data deviation symmetry rule found removing batch normalization discriminator yields better results especially training single images. contrast dcgan architecture model contains exclusively convolutional layers. convolutional weight sharing allows network trained small image patches rolled synthesize arbitrary large output images training. upon successful training sampled images match local image statistics training data. hence model implements spatial stochastic process. further components sampled independently limited receptive ﬁelds generator imply generator implements stationary ergodic strongly mixing stochastic process. means sampling different textures possible would require non-ergodic process. independent sampling learning textures results generation textures combining elements whole set. another limitation independent sampling impossibility align away regions generated image alignment violates translation invariance stationarity mixing. however periodic textures depend long-range correlations. limitations extend composed three distinct parts local independent part spatially global part periodic part part spatial dimensions vary respective channel dimensions concatenation total channel dimension proceed discussion three parts. conceptually simplest approach sample slice independently position uniform distribution affects ﬁnite region image speak local dimensions. intuitively local dimensions allow generative process produce spatial variance diversity sampling current contribution psgan makes great step forward respect types images neural texture synthesis method create periodic nonperiodic images learned unsupervised single images large datasets images. afterwards ﬂexible sampling noise space allows create novel textures potentially inﬁnite output size smoothly transition them. figure shows example textures generated psgan. next section describe detail architecture psgan proceed illustrate abilities number experiments. gans generative model maps noise vector input data space. sgans generalize generator noise tensor rl×m×d image rh×w× figure ﬁrst dimensions spatial dimensions blown generator respective input spatial dimensions ﬁnal dimension channel dimension. analogy extension generator extend discriminator input image two-dimensional ﬁeld spatial size position resulting discriminator responds local part call dλµ’s effective receptive ﬁeld. response represents estimated probability respective part real instead generated function minimized maximized ming maxd maximizing ﬁrst line leads discriminator return values close generated images vice versa minimization aims discriminator taking large output values close hand maximizing second line anchors discriminator real data pdata return values close want model able learn single image input image data augmented selecting patches figure illustration psgan model. fully convolutional generator network maps spatial tensor zλµi spatial indices input image every subvector spatial location e.g. blue green columns figure limited area alleviate independence property distant areas construct tensor three parts local part global part periodic part text. usual training discriminator gets either generated image image patch real data distribution. global dimensions unique vector dimensionality sampled repeated along spatial dimensions thus global impact whole image allows selection type structure generated employing global dimensions generative stochastic process becomes non-ergodic. consider task learning texture images generator needs learn splitting half-spaces vectors half-space generate samples style textures. besides scenario learning texture images combination random patch selection larger image particularly interesting here converged generator samples textures consistent local statistics image. notably source image necessarily texture method extract texture generating stochastic process image nevertheless learning vector represents texture manifold learned textures psgan corresponds generating stochastic process texture static image. purpose image generation need composed single vector smooth function long neighboring vectors don’t vary rapidly statistics close statistics training. hence smoothness implies smooth texture change matrix contains wave vectors column vectors. vectors parametrize direction number radians spatial unit distance periodic channel random phase offset uniformly sampled mimics random positional extraction patches real images. adding wave numbers could ﬁxed basis note speciﬁc texture associated wave vectors i.e. different textures different axes periodicities scales. hence make dependent global dimensions multi-layer perceptron texture learned. texture learned i.e. wave numbers direct parameters system. figure indicate alternative dependency dotted arrow parameters learned end-to-end alongside parameters generator discriminator base system dcgan architecture stride discriminator. local global noise dimensions sampled uniform distribution. dcgan ﬁlters channels highest spatial resolution doubled every layer halves spatial resolution. e.g. layer architecture channels noise input output image. training done adam settings learning rate minibatch size typical image patch size pixels. usually used layers kernels size zero padding batch normalization. generator upsamples spatial noise factor receptive ﬁeld size receptive ﬁeld image patch size affect learning hardware measured seconds generation pixels image seconds pixels image. point-wise rectiﬁed-linear unit function rdh×dg rdp×dh rdp. used experiments. parameters initialized independent random gaussian distribution except non-zero mean constant vector chosen entries spread interval simplicity write ϕλµi ζλµi) brieﬂy summarize periodic dimensions arise global ones. alternatively composed single vector write simplicity understand denotes vector slice along last dimension. following image sources used experiments paper oxford describable textures dataset composed various categories containing images; facades dataset contains facades different houses prague. datasets comprise objects different scales sizes. also used satellite images sydney google maps. merrigum house wikimedia commons. criteria good texture synthesis? humans perceive texture easily quantiﬁable statistic metric. still qualitatively assess whether texture synthesis method captures right properties source image. order illustrate this demonstrate learn complex periodic images texture manifolds allow texture blending. first demonstrate learning single periodic texture image. figure illustrates results psgan compared sgan methods text example periodic stochastic dimension. psgan learns arranges text regular lines varying ideally wave numbers within valid interval negative positive nyquist wave numbers however wave numbers single sinusoids projected back interval. hence constraint necessary. figure comparing results neural texture synthesis methods input images text honeycomb middle row. green boxes show receptive ﬁelds generator pixels text honeycomb example. cases psgan best captures underlying data periodicity. autocorrelation plot honeycomb shown bottom reveals periodicity grid intensity peaks. arrows periodicity psgan neatly align autocorrelation structure second figure demonstrates learning honeycomb texture basic hexagonal pattern method captures underlying periodicity random coloring effects inside cells. method inaccurate texture borders copied patches inaccurately aligned. methods fail produce hexagonal structure even locally. last ﬁgure shows autocorrelation plots honeycomb textures periodicity reveals regular grid superimposed onto background feature psgan able reproduce. periodic dimensions enough learn patterns noticed training convergence faster setting however beating sinusoids close wave numbers occur rarely happens also sub-nyquist artefacts i.e. texture periodicity close integer fractional nyquist wavenumber. figure shows larger slice learned periodic textures. particular figure shows learning works complex patterns pattern wallpaper group symmetry non-orthogonal symmetry axes. figure accurate wave number learning psgan allows correct generation periodic textures even large images. honeycomb repeated large images aliasing. non-orthogonal bases periodicities complicated symmetries rotations present pattern faithfully reproduced. next extract multiple textures single large image images. chosen images global structure also exhibit characteristics many textures single image structured psgan generator noise global dimensions allows extract textures corresponding different image regions. order visualize texture diversity model deﬁne quilt array generate different textures trained psgan model setting rectangular spatial reλλ+∆µµ+∆ size vecgions randomly sampled prior. since generator convolutional network receptive ﬁelds several spatial elements borders tiles look partially aligned. example figure borders tiles scaly elements across them rather sharply separated figure shows results trained single large image. psgan extracts diverse bricks grass leaf textures. contrast sgan forces output single mixing process rather multitude different visual textures. gatys’ method also learns single texture-like process statistics whole image. figure shows texture learning city satellite images challenging image domain details images. figures show results training multiple texture-like images dtd. order show textures vary smoothly sample different values four corners target image interpolate bi-linearly construct tensor. figure shows values lying between original points generate proper textures well. figure learning single large photo clip-out shown order scale generated textures. psgan extract multiple varied textures samples shown quilt tile size total pixels generated image. gatys’ method sgan whole image instead. figure examples learned textures using rich variable input image information. uses satellite images sydney. small texture images. outputs show different textures quilt sampled model total image size pixels best seen maximally zoomed-in. section explore global periodic dimensions inﬂuence output generated noise tensor. take array quilt structure. deﬁne array size calculate different periodic tensors ﬁrst tensor wave numbers varying function different elements quilt second tensor wave numbers everywhere. psgan trained minibatches holds model ﬂexible produces meaningful outputs even setting different values. figure shows global periodic dimensions encode complementary aspects image generation process texture identity periodicity. facades dataset strong vertical horizontal periodicity easily interpretable height ﬂoors window placement directly depends frequencies. disentangling leads instructive visualizations. figure shows generation tensor constructed linear interpolation sampled left right border. however wave numbers periodic dimensions ﬁxed independently changing global dimensions. ﬁgure clearly shows change visual appearance texture preserving consistent periodic structure psgan disentangling property reminiscent construct categorical continuous noise variables explain factors variation object identity spatial transformation. figure morphing house textures linearly interpolating between different textures. disentangling properties psgan allows morph controlled manner house window periodicity stays same facade type appearance change signiﬁcantly changing global dimensions. plexity w.r.t. output image pixel size) psgan generating outputs. high resolution images created splitting parts arrays rendering sequentially thus constant memory footprint. annice property architecture ability stitch seamlessly output texture images tileable textures potentially increasing output image size even more. summarize abilities psgan learn textures great variability large images learn periodical textures learn whole manifolds textures smoothly blend method limitations convergence sometimes tricky noted models like gans psgan suffer mode dropping given large textures learn them especially data varies scale periodicity. finally psgans represent arbitrary probability distributions extend spatial scale largest periods generalize periodic structures beyond that. however images larger structures general non-periodic features representable e.g. images global trend perspective projection aperiodic images like penrose tilings. psgan great potential adapted cases. in-painting possible application method random missing image regions ﬁtting textures. texture style transfer painting target image textures done similar quilts paper constructed. further explicit modeling periodic dimensions psgan could great figure psgan learns whole texture manifold scaly allowing smooth texture morphing illustrated single image size pixels. regions image plausible textures. generator input tensor calculated bi-linear interpolation randomly sampled corners. texture synthesis large unlabeled image datasets requires novel data-driven methods going beyond older techniques learn single textures rely prespeciﬁed statistical descriptors. previous methods like sgan limited stationary ergodic stochastic textures even trained many images sgan fuses outputs single mixing process them. experiments suggest gatys’ method exhibits similar limitations. contrast psgan models non-ergodic cyclostationary processes learn whole texture manifold sets images single large image. cgans additional label information input generator discriminator allows class conditional generation. comparison psgan also uses additional information generator input discriminator. method remains fully unsupervised uses sampled noise unlike cgans require speciﬁc label information. concerning model architecture sgan model similar seen ablated psgan instance architecture allows great scalability resulting houses different material window periodicity; shows houses different material color aligned periodical structure shows color different window periodicity. local dimensions ﬁxed. modalities particular time-series audio data. here we’d expect model extract sound textures might useful synthesizing completely novel sounds interpolating manifold. theoretical side capture symmetries texture images could extend tensor even further adding dimensions reﬂection rotation symmetries. terms model stability convergence we’ll investigate alternative training criteria alleviate mode dropping problem. chen duan houthooft rein schulman john sutskever infogan interpretable representation learning information maximizing generative adversarial nets. corr abs/. http//arxiv.org/ abs/.. image efros alexei freeman william proquilting texture synthesis transfer. ceedings annual conference computer graphics interactive techniques siggraph isbn ---x. http//doi.acm.org/./ efros alexei leung thomas texture synproceedings thesis non-parametric sampling. international conference computer vision isbn ---. http//dl.acm.org/ citation.cfm?id=.. gatys leon ecker alexander bethge matthias. texture synthesis using convolutional neural networks. advances neural information processing systems http//arxiv.org/abs/. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems portilla javier simoncelli eero parametric texture model based joint statistics complex wavelet coefﬁcients. int. comput. vision october ./a. http//dx. doi.org/./a. radford alec metz luke chintala soumith. unsupervised representation learning deep concorr volutional generative adversarial networks. abs/. http//arxiv.org/ abs/.. ulyanov dmitry lebedev vadim vedaldi andrea lempitsky victor. texture networks feed-forward synthesis textures stylized images. international conference machine learning", "year": 2017}