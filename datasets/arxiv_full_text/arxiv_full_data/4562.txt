{"title": "Accelerating Reinforcement Learning through Implicit Imitation", "tag": ["cs.LG", "cs.AI"], "abstract": "Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.", "text": "imitation viewed means enhancing learning multiagent environments. augments agent’s ability learn useful behaviors making intelligent knowledge implicit behaviors demonstrated cooperative teachers experienced agents. propose study formal model implicit imitation accelerate reinforcement learning dramatically certain cases. roughly observing mentor reinforcement-learning agent extract information capabilities relative value unvisited parts state space. study speciﬁc instantiations model learning agent mentor identical abilities designed deal agents mentors diﬀerent action sets. illustrate beneﬁts implicit imitation integrating prioritized sweeping demonstrating improved performance convergence observation single multiple mentors. though make stringent assumptions regarding observability possible interactions brieﬂy comment extensions model relax restricitions. application reinforcement learning multiagent systems oﬀers unique opportunities challenges. agents viewed independently trying achieve ends interesting issues interaction agent policies must resolved however fact agents share information mutual gain distribute search optimal policies communicate reinforcement signals another oﬀers intriguing possibilities accelerating reinforcement learning enhancing agent performance. another individual agent performance improved novice agent learn reasonable behavior expert mentor. type learning brought explicit teaching demonstration sharing privileged information explicit cognitive representation imitation imitation agent’s exploration used ground observations agents’ behaviors capabilities resolve ambiguities observations arising partial observability noise. common thread work mentor guide exploration observer. typically guidance achieved form explicit communication mentor observer. less direct form teaching involves observer extracting information mentor without mentor making explicit attempt demonstrate speciﬁc behavior interest paper develop imitation model call implicit imitation allows agent accelerate reinforcement learning process observation expert mentor agent observes state transitions induced mentor’s actions uses information gleaned observations update estimated value states actions. distinguish settings implicit imitation occur homogeneous settings learning agent mentor identical actions; heterogeneous settings capabilities diﬀer. homogeneous setting learner observed mentor transitions directly update estimated model actions update value function. addition mentor provide hints observer parts state space worth focusing attention. observer’s attention area might take form additional exploration area additional computation brought bear agent’s prior beliefs area. heterogeneous setting similar beneﬁts accrue potential agent misled mentor possesses abilities diﬀerent own. case learner needs mechanism detect situations make eﬀorts temper inﬂuence observations. derive several techniques support implicit imitation largely independent speciﬁc reinforcement learning algorithm though best suited model-based methods. include model extraction augmented backups feasibility testing k-step repair. ﬁrst describe implicit imitation homogeneous domains describe extension heterogeneous settings. illustrate eﬀectiveness empirically incorporating moore atkeson’s prioritized sweeping algorithm. implicit imitation model several advantages direct forms imitation teaching. require agent explicitly play role mentor teacher. observers learn simply watching behavior agents; observed mentor shares certain subtasks observer observed behavior incorporated observer improve estimate value function. important many situations observer learn mentor unwilling unable alter behavior accommodate observer even communicate information example common communication protocols unavailable agents designed diﬀerent developers agents competitive situation disincentive share information skills; simply incentive agent provide information another. imitate mentor. learner decide whether explicit imitation worthwhile. implicit imitation thus seen blending advantages explicit teaching explicit knowledge transfer independent learning. addition agent learns observation exploit existence multiple mentors essentially distributing search. finally assume observer knows actual actions taken mentor mentor shares reward function mentor. again stands sharp contrast many existing models teaching imitation behavior learning observation. make strict assumptions paper respect observability complete knowledge reward functions existence mappings agent state spaces model generalized interesting ways. elaborate generalizations near paper. remainder paper structured follows. provide necessary background markov decision processes reinforcement learning development implicit imitation model section section describe general formal framework study implicit imitation reinforcement learning. speciﬁc instantiations framework developed. section model homogeneous agents developed. model extraction technique explained augmented bellman backup proposed mechanism incorporating observations model-based reinforcement learning algorithms. model conﬁdence testing introduced ensure misleading information undue inﬂuence learner’s exploration policy. mentor observations focus attention interesting parts state space also introduced. section develops model heterogeneous agents. model extends homogeneous model feasibility testing device learner detect whether mentor’s abilities similar k-step repair whereby learner attempt mimic trajectory mentor cannot duplicated exactly. techniques prove crucial heterogeneous settings. eﬀectiveness models demonstrated number carefully chosen navigation problems. section examines conditions implicit imitation work well. section describes several promising extensions model. section examines implicit imitation model context related work section considers future work drawing general conclusions implicit imitation ﬁeld computational imitation broadly. provide formal model implicit imitation whereby agent learn optimally combining experience observations behavior expert mentor. describe section standard model reinforcement learning used artiﬁcial intelligence. model build singleagent view learning act. begin reviewing markov decision processes provide model sequential decision making uncertainty move describe reinforcement learning emphasis model-based methods. markov decision processes proven useful modeling stochastic sequential decision problems widely used decision-theoretic planning model domains agent’s actions uncertain eﬀects agent’s knowledge environment uncertain agent multiple possibly conﬂicting objectives. section describe basic model consider classical solution procedure. consider action costs formulation mdps though pose special complications. finally make assumption full observability. partially observable mdps much computationally demanding fully observable mdps. imitation model based fully observable model though generalizations model mentioned concluding section build pomdps. refer reader bertsekas boutilier dean hanks puterman material mdps. viewed stochastic automaton actions induce transitions states rewards obtained depending states visited agent. formally deﬁned tuple ﬁnite states possible worlds ﬁnite actions state transition function reward function. agent control state system extent performing actions cause state transitions movement current state state. actions stochastic actual transition caused cannot generally predicted certainty. transition function describes eﬀects action state. probability distribution speciﬁcally probability ending state action performed state denote quantity require dynamics system controlled. assumption system fully observable means agent knows true state time decisions based solely knowledge. thus uncertainty lies prediction action’s eﬀects determining actual eﬀect execution. policy describes course action adopted agent controlling system. agent adopting policy performs action whenever ﬁnds state policies form markovian since action choice state depend system history stationary since action choice depend stage decision problem. problems consider optimal stationary markovian policies always exist. assume bounded real-valued reward function instantaneous reward agent receives occupying state number optimality criteria adopted measure value policy measuring reward accumulated agent traverses state space execution work focus discounted inﬁnite-horizon problems current value reward received stages future discounted factor allows simpler value function reﬂects value policy state simply expected discounted future rewards obtained executing beginning policy optimal policies guaranteed optimal policies exist setting value state value optimal policy solving refer problem constructing optimal policy. value iteration simple iterative approximation algorithm optimal policy construction. given arbitrary estimate true value function iteratively improve estimate follows computation given known bellman backup. sequence value functions produced value iteration converges linearly iteration value iteration requires computation time number iterations polynomial |s|. ﬁnite actions maximize right-hand side equation form optimal policy approximates value. various termination criteria applied; example might terminate algorithm diﬃculty mdps construction optimal policy requires agent know exact transition probabilities reward model speciﬁcation decision problem requirements especially detailed speciﬁcation domain’s dynamics impose undue burden agent’s designer. reinforcement learning viewed solving full details model particular known agent. instead agent learns optimally experience environment. provide brief overview reinforcement learning section details please refer texts sutton barto bertsekas tsitsiklis survey kaelbling littman moore general model assume agent controlling initially knows state action spaces transition model reward function agent acts environment stage process makes transition takes action state receives reward moves state based repeated experiences type determine optimal policy ways model-based reinforcement learning experiences used learn true nature solved using standard methods model-free reinforcement learning experiences used directly update estimate optimal value function q-function. probably simplest model-based reinforcement learning scheme certainty equivalence approach. intuitively learning agent assumed current estimated transition model environment consisting estimated probabilities estimated rewards model experience agent updates estimated models solves estimated obtain policy would optimal make certainty equivalence approach precise speciﬁc form estimated model update procedure must adopted. common approach used empirical distribution observed state transitions rewards estimated model. instance action attempted times state occasions prior estimate used bayesian approach uses explicit prior distribution parameters transition distribution updates experienced transition. instance might assume dirichlet distribution parameters associated possible successor state dirichlet parameters equal experience-based counts plus prior count representing agent’s prior beliefs distribution expected transition probability using expected values. furthermore model updated ease simply increasing observation model advantage counter-based approach allowing ﬂexible prior model generally could circumvent extent batching experiences updating model periodically. alternatively could computational eﬀort judiciously apply bellman backups states whose values likely change given change model. moore atkeson’s prioritized sweeping algorithm this. updated changing bellman backup applied update estimated value well q-value suppose magnitude change given predecessor q-values q—hence values —can change magnitude change bounded cpr∆bv predecessors placed priority queue cpr∆bv serving priority. ﬁxed number bellman backups applied states order appear queue. backup change value cause predecessors inserted queue. computational eﬀort focused states bellman backup greatest impact model change. furthermore backups applied subset states generally applied ﬁxed number times. contrast certainty equivalence approach backups applied convergence. thus prioritized sweeping viewed speciﬁc form asynchronous value iteration appealing computational properties certainty equivalence agent acts current approximation model correct even though model likely inaccurate early learning process. optimal policy inaccurate model prevents agent exploring transitions form part optimal policy true model agent fail optimal policy. reason explicit exploration policies invariably used ensure action tried state suﬃciently often. acting randomly agent assured sampling action state inﬁnitely often limit. unfortunately actions agent fail exploit knowledge optimal policy. explorationexploitation tradeoﬀ refers tension trying actions order environment executing actions believed optimal basis current estimated model. common method exploration ε–greedy method agent chooses random action fraction time typically decayed time increase agent’s exploitation knowledge. boltzmann approach action selected probability proportional value proportionality adjusted nonlinearly temperature parameter probability selecting action highest value tends typically started high actions randomly explored early stages learning. agent gains knowledge eﬀects actions value eﬀects sophisticated methods attempt information model conﬁdence value magnitudes plan utility-maximizing exploration plan. early approximation scheme found interval estimation method bayesian methods also used calculate expected value information gained exploration concentrate paper model-based approaches reinforcement learning. however point model-free methods—those estimate optimal value function q-function learned directly without recourse domain model—have attracted much attention. example td-methods q-learning proven among popular methods reinforcement learning. methods modiﬁed deal model-free approaches discuss concluding section. also focus so-called tablebased representations models value functions. state action spaces large table-based approaches become unwieldy associated algorithms generally intractable. situations approximators often used estimate values states. discuss ways techniques extended allow function approximation concluding section. model inﬂuence mentor agent decision process learning behavior observer must extend single-agent decision model mdps account actions objectives multiple agents. section introduce formal framework studying implicit imitation. begin introducing general model stochastic games impose various assumptions restrictions general model allow focus aspects implicit imitation. note framework proposed useful study forms knowledge transfer multiagent systems brieﬂy point various extensions framework would permit implicit imitation forms knowledge transfer general settings. stochastic games viewed multiagent extension markov decision processes. though shapley’s original formulation stochastic games involved zero-sum assumption various generalizations model proposed allowing arbitrary relationships agents’ utility functions formally n-agent stochastic game comprises agents states actions agent state transition function reward function agent unlike individual agent actions determine state transitions; rather joint action taken collection agents determines system evolves point time. interests individual agents odds strategic reasoning notions equilibrium generally involved solution stochastic games. study reinforcement agent might learn observing behavior expert mentor wish restrict model strategic interactions need considered want focus settings actions observer mentor interact. furthermore want assume reward functions agents conﬂict requires strategic reasoning. deﬁne noninteracting stochastic games appealing notion agent projection function used extract agent’s local state underlying game. games agent’s local state determines aspects global state relevant decision making process projection function determines global states identical agent’s local perspective. formally agent assume local state space projection function write equivalence relation partitions equivalence classes elements within speciﬁc class need distinguished agent purposes individual decision making. stochastic game noninteracting exists local state space projection function agent that intuitively condition imposes distinct requirements game perspective agent first ignore existence agents provides notion state space abstraction suitable agent speciﬁcally clusters together states state equivalence class identical dynamics respect abstraction induced type abstraction form bisimulation type studied automaton minimization automatic abstraction methods developed mdps hard show—ignoring presence agents—that underlying system markovian respect abstraction condition met. quantiﬁcation imposes strong noninteraction requirement namely dynamics game perspective agent independent strategies agents. condition simply requires states within given equivalence class agent reward agent means states within class need distinguished—each local state viewed atomic. state element a−i. optimal policy extend strategy underlying stochastic game simply applying every state following proposition shows term noninteracting indeed provides appropriate description game. given arbitrary stochastic game generally quite diﬃcult discover whether noninteracting requiring construction appropriate projection functions. follows simply assume underlying multiagent system noninteracting game. rather specifying game projection functions specify individual mdps themselves. noninteracting game induced individual mdps simply cross product individual mdps. view often quite natural. consider example three robots moving two-dimensional oﬃce domain. able neglect possibility interaction—for example robots occupy position require resources achieve tasks—then might specify individual robot. local state might determined robot’s y-position orientation status tasks. global state space would cross product local spaces. individual components joint action would aﬀect local state agent would care local state. note projection function viewed equivalent observation function. assume agent distinguish elements si—in fact observations agents’ states crucial imitation. rather existence simply means that point view decision making known model agent need worry distinctions made assuming computational limitations agent need solve observations agents order improve knowledge mi’s dynamics. another. keep discussion simple assume existence expert mentor agent implementing stationary policy local rmi. also assume second agent observer local roi. nothing mentor’s behavior relevant observer knows situation quite diﬀerent reinforcement learner without complete knowledge model well observed behavior mentor provides valuable information observer quest learn optimally within take extreme case mentor’s identical observer’s mentor expert behavior mentor indicates exactly observer even mentor acting optimally mentor observer diﬀerent reward functions mentor state transitions observed learner provide valuable information dynamics domain. thus agent learning behavior another potentially relevant learner even underlying multiagent system noninteracting. similar remarks course apply case observer knows computational restrictions make solving diﬃcult—observed mentor transitions might provide valuable information focus computational eﬀort. main motivation underlying model implicit imitation behavior expert mentor provide hints appropriate courses action reinforcement learning agent. intuitively implicit imitation mechanism learning agent attempts incorporate observed experience expert mentor agent learning process. like classical forms learning imitation learner considers eﬀects mentor’s action context. unlike direct imitation however assume learner must physically attempt duplicate mentor’s behavior assume mentor’s behavior necessarily appropriate observer. instead inﬂuence mentor agent’s transition model estimate value various states actions. elaborate points below. follows assume mentor associated learner observer associated described above. mdps fully observable. focus reinforcement learning problem faced agent extension multiple mentors straightforward discussed below clarity assume mentor description abstract framework. clear certain conditions must observer extract useful information mentor. list number assumptions make diﬀerent points development model. observability must assume learner observe certain aspects mentor’s behavior. work assume state mentor’s fully observable learner. equivalently interpret full observability underlying noninteracting game together knowledge mentor’s projection instance algorithms like asynchronous dynamic programming prioritized sweeping beneﬁt guidance. indeed distinction reinforcement learning solving mdps viewed rather blurry focus case unknown model opposed computational issues key. function general partially observable model would require speciﬁcation observation signal observation function so×sm denotes probability observer obtains signal local states observer mentor respectively. pursue model here. important note assume observer access action taken point time. since actions stochastic state results mentor invoking speciﬁc control signal generally insuﬃcient determine signal. thus seems much reasonable assume states observable actions gave rise them. analogy observer mentor acting diﬀerent local state spaces clear observations made mentor’s state transitions oﬀer useful information observer unless relationship state spaces. several ways relationship speciﬁed. dautenhahn nehaniv homomorphism deﬁne relationship mentor observer speciﬁc family trajectories slightly diﬀerent notion might involve analogical mapping observed state transition provides information observer dynamics value state certain circumstances might require mapping homomorphic respect perhaps even respect discuss issues detail below. order simplify model avoid undue attention topic constructing suitable analogical mappings simply assume mentor observer identical state spaces; sense isomorphic. precise sense spaces isomorphic—or cases presumed isomorphic proven otherwise—is elaborated discuss relationship agent abilities. thus point simply refer state without distinguishing mentor’s local space observer’s abilities even mapping states observations mentor’s state transitions tell observer something mentor’s abilities own. must assume observer duplicate actions taken mentor induce analogous transitions local state space. words must presumption mentor observer similar abilities. sense analogical mapping state spaces taken homomorphism. speciﬁcally might assume mentor observer actions available homomorphic respect requirement weakened substantially without diminishing utility requiring observer able implement actions actually taken mentor given state finally might observer assumes duplicate actions taken mentor ﬁnds evidence contrary. case presumed homomorphism state spaces. follows distinguish implicit imitation homogeneous action settings—domains general ways deﬁning similarity ability example assuming observer able move state space similar fashion mentor without following trajectories instance mentor moving directly locations state space observer able move analogous locations less direct fashion. case analogy states determined single actions rather sequences actions local policies. suggest ways dealing restricted forms analogy type section objectives even observer mentor similar identical abilities value observer information gleaned mentor depend actual policy implemented mentor. might suppose closely related mentor’s policy optimal policy observer useful information thus extent expect closely aligned objectives mentor observer valuable guidance provided mentor. unlike existing teaching models suppose mentor making explicit eﬀorts instruct observer. objectives identical force observer explicitly imitate behavior mentor. general make explicit assumptions relationship objectives mentor observer. however that extent closer utility derived implicit imitation. finally remark important assumption make throughout remainder paper observer knows reward function state observer evaluate without visited state view consistent view reinforcement learning automatic programming. user easily specify reward function prior learning. diﬃcult specify domain model optimal policy. setting unknown component transition function pro. believe approach reinforcement learning fact common practice approach reward function must sampled. reiterate describe mechanism observer accelerate learning; emphasize position implicit imitation—in contrast explicit imitation—is merely replicating behaviors observed another agent even attempting reach similar states. believe agent must learn capabilities adapt information contained observed behavior these. agents must also explore appropriate application observed behaviors integrating appropriate achieve ends. therefore imitation interactive process behavior agent used guide learning another. given setting list possible ways observer mentor interact contrasting along perspective assumptions existing models literature. first observer could attempt directly infer policy observations mentor state-action pairs. model conceptual simplicity intuitive appeal forms basis behavioral cloning paradigm however assumes observer mentor share reward function action capabilities. also assumes complete unambiguous trajectories observed. related approach attempts deduce constraints value function inferred action preferences mentor agent again however approach assumes congruity objectives. model also distinct models explicit teaching assume mentor incentive move environment explicitly guides learner explore environment action space eﬀectively. instead trying directly learn policy observer could attempt observed state transitions agents improve environment model pro. accurate model reward function observer could calculate accurate values states. state values could used guide agent towards distant rewards reduce need random exploration. insight forms core implicit imitation model. approach developed literature appropriate conditions listed above speciﬁcally conditions mentor’s actions unobservable mentor observer diﬀerent reward functions objectives. thus approach applicable general conditions many existing models imitation learning teaching. addition model information mentors also communicate information relevance irrelevance regions state space certain classes reward functions. observer states visited mentor heuristic guidance perform backup computations state space. next sections develop speciﬁc algorithms insights agents observations others improve models assess relevance regions within state spaces. ﬁrst focus homogeneous action case extend model deal heterogeneous actions. begin describing implicit imitation homogeneous action settings—the extension heterogeneous settings build insights developed section. develop technique called implicit imitation observations mentor used accelerate reinforcement learning. first deﬁne homogeneous setting. develop implicit imitation algorithm. finally demonstrate implicit imitation works number simple problems designed illustrate role various mechanisms describe. homogeneous action setting deﬁned follows. assume single mentor observer individual mdps respectively. note agents share state space also assume mentor executing stationary policy often treat policy deterministic remarks apply stochastic policies well. support supp state actions accorded nonzero probability state assume observer abilities mentor following sense supp exists action prm. words observer able duplicate actual behavior mentor; equivalently agents’ local state spaces isomorphic respect actions actually taken mentor subset states actions might taken. much weaker requiring full homomorphism course existence full homomorphism suﬃcient perspective; results require this. implicit imitation algorithm understood terms component processes. first extract action models mentor. integrate information observer’s value estimates augmenting usual bellman backup mentor action models. conﬁdence testing procedure ensures augmented model observer’s model mentor reliable observer’s model behavior. also extract occupancy information observations mentor trajectories order focus observer’s computational eﬀort speciﬁc parts state space. finally augment action selection process choose actions explore high-value regions revealed mentor. remainder section expands upon processes together. information available observer quest learn optimally divided categories. first action takes receives experience tuple fact often ignore sampled reward since assume reward function known advance. standard model-based learning experience used update transition model pro. second mentor transition observer obtains experience tuple note observer direct access action taken mentor induced state transition. assume mentor implementing deterministic stationary policy denoting mentor’s choice action state policy induces markov chain denoting relative observed frequency mentor transitions observer prior possible mentor transitions standard bayesian update techniques used instead. term model extraction process estimating mentor’s markov chain. homogeneity assumption action replicated exactly observer state thus policy principle duplicated observer such deﬁne value mentor’s policy observer’s perspective notice equation uses mentor’s dynamics observer’s reward function. letting denote optimal value function clearly provides lower bound observer’s value function. action since action identical observer’s actions term redundant augmented value equation valid. course observer using augmented backup operation must rely estimates quantities. observer exploration policy ensures state visited inﬁnitely often estimates terms converge true values. mentor’s policy ergodic state space also converge true value. mentor’s policy restricted subset states estimates subset converge correctly respect chain ergodic. states remain unvisited estimates remain uninformed data. since mentor’s policy control observer observer inﬂuence distribution samples attained prm. observer must therefore able reason accuracy estimated model restrict application augmented equation states known suﬃcient accuracy. cannot used indiscriminately argue highly informative early learning process. assuming mentor pursuing optimal policy many states observer much accurate estimates speciﬁc since observer learning must explore state space—causing less frequent visits s—and action space—thus spreading experience actions generally ensures sample size upon based greater action forms part mentor’s policy. apart accurate often give informed value estimates state since prior action models often uniform become distinguishable given state observer suﬃcient experience state note reasoning holds even mentor implementing stochastic policy direction oﬀered mentor implementing deterministic policy tends focused empirically found mentors oﬀer broader guidance moderately stochastic environments implement stochastic policies since tend visit state space. note extension multiple mentors straightforward—each mentor model incorporated augmented bellman equation without diﬃculty. mentor’s markov chain ergodic mixing rate suﬃciently mentor visit certain state relatively infrequently. estimated mentor transition model corresponding state rarely visited mentor provide misleading estimate—based small sample prior mentor’s chain—of value mentor’s action since mentor’s policy control observer misleading value persist extended period. since augmented bellman equation consider relative reliability mentor observer models value state overestimated; observer tricked overvaluing mentor’s action consequently overestimating value state overcome this incorporate estimate model conﬁdence augmented backups. mentor’s markov chain observer’s action transitions assume dirichlet prior parameters multinomial distributions reﬂect observer’s initial uncertainty possible transition probabilities. sample counts mentor observer transitions update distributions. information could attempt perform optimal bayesian estimation value function; sample counts small simple closed form expression resultant distributions values. could attempt employ sampling methods mixing rate refers quickly markov chain approaches stationary distribution. note underestimates based considerations problematic since augmented bellman augmented bellman backup respect using conﬁdence testing proceeds follows. ﬁrst compute observer’s optimal action based estimated augmented values observer’s actions. denote value. best action model uncertainty encoded dirichlet distribution construct lower bound value state observer using model derived behavior employ transition counts denote number times observer made transition state state action performed number times mentor observed making transition state respectively. counts estimate uncertainty model using variance q-value action uncertainty local model found simple application rule combining linear combinations variances expression bellman backup using chebychev’s inequality obtain conﬁdence level even though dirichlet distributions small sample counts highly non-normal. lower bound suitable constant interpret penalizing value state subtracting uncertainty value mentor’s action estimated similarly analogous lower bound supersedes write either mentor-inspired model fact lower expected value uses nonoptimal action mentor-inspired model lower conﬁdence. either case reject information provided mentor standard bellman backup using action model derived solely observer’s experience —the backed value case. algorithm computing augmented backup using conﬁdence test shown table algorithm parameters include current estimate augmented value function current estimated model associated local variance omodel model mentor’s markov chain associated variance mmodel. calculates lower bounds returns mean value greatest lower bound. parameter determines width conﬁdence interval used mentor rejection test. augmented bellman backups improves accuracy observer’s model. second observer exploit observations mentor focus attention states visited mentor. model-based approach speciﬁc focusing mechanism adopt require observer perform bellman backup state whenever mentor makes transition three eﬀects. first mentor tends visit interesting regions space signiﬁcant values backed mentor-visited states bias observer’s exploration towards regions. second computational eﬀort integration exploration techniques action selection policy important reinforcement learning algorithm guarantee convergence. implicit imitation plays second crucial role helping agent exploit information extracted mentor. improved convergence results rely greedy quality exploration strategy bias observer towards higher-valued trajectories revealed mentor. expediency adopted ε-greedy action selection method using exploration rate decays time. could easily employed semi-greedy methods boltzmann exploration. presence mentor greedy action selection becomes complex. observer examines actions state usual obtains best action corresponding value value also calculated mentor’s action observer’s action model used greedy action deﬁned exactly mentor present. however would like deﬁne greedy action action dictated mentor’s policy state unfortunately observer know action deﬁne greedy action observer’s action closest mentor’s action according observer’s current model estimates precisely action similar mentor’s state denoted whose outcome distribution minimum kullback-leibler divergence mentor’s action outcome distribution observer’s experience-based action models poor early training chance closest action computation select wrong action. rely exploration policy ensure observer’s actions sampled appropriately long run. present work assumed state space large agent therefore able completely update q-function whole space. absence information state’s true values would like bias value states along mentor’s trajectories look worthwhile explore. assuming bounds reward function setting initial qvalues entire space bound. simple examples rewards strictly positive bounds zero. mentor trajectories intersect states valued observing agent backups cause states trajectories higher value surrounding states. causes greedy step exploration method prefer actions lead mentor-visited states actions agent information. model extraction augmented backups focusing mechanism extended notion greedy action selection integrated model-based reinforcement learning algorithms relative ease. generically implicit imitation algorithm requires that observer maintain estimate cprm markov chain induced mentor’s dence testing. course backups implemented using estimated models cpro cprm. addition focusing mechanism requires augmented backup model state change; bellman backup performed incorporate revised model number additional backups performed selected states. states selected using priority estimates potential change values based changes precipitated earlier backups. eﬀectively computational resources focused states beneﬁt backups. keeping samples mentor behavior implements model extraction. augmented backups integrate information observer’s value function performing augmented backups observed transitions incorporates focusing mechanism. observer forced follow otherwise mimic actions mentor directly. back value information along mentor’s trajectory had. ultimately observer must move states discover actions used; meantime important value information propagated guide exploration. implicit imitation alter long theoretical convergence properties underlying reinforcement learning algorithm. implicit imitation framework orthogonal ε-greedy exploration alters deﬁnition greedy action greedy action taken. given theoretically appropriate decay factor ε-greedy strategy thus ensure distributions action models state sampled inﬁnitely often limit converge true values. since extracted model mentor corresponds observer’s actions eﬀect value function calculations diﬀerent eﬀect observer’s sampled action models. conﬁdence mechanism ensures model samples eventually come dominate fact better. therefore sure convergence properties reinforcement learning implicit imitation identical underlying reinforcement learning algorithm. beneﬁt implicit imitation lies models extracted mentor allow observer calculate lower bound value function lower bound choose greedy actions move agent towards higher-valued regions state space. result quicker convergence optimal policies better short-term practical performance respect accumulated discounted reward learning. implicit imitation model easily extended extract model information multiple mentors mixing matching pieces extracted mentor achieve good results. searching state mentors knows mentor highest value estimate. value estimate best mentor compared using conﬁdence test described observer’s value estimate. formal expression algorithm given multi-augmented bellman equation candidate mentors. ideally conﬁdence estimates taken account comparing mentor estimates other mentor high mean value estimate large variance. observer experience state mentor likely rejected poorer quality information observer already experience. observer might better picking mentor lower mean conﬁdent estimate would succeeded test observer’s model. interests simplicity however investigate multiple mentor combination without conﬁdence testing. assumed action costs however general reward functions diﬃculty lies backing action costs mentor’s chosen action unknown. section deﬁned closest action function function used choose appropriate reward. augmented bellman equation generalized rewards takes following form note bayesian methods could used could used estimate action costs mentor’s chain well. case generalized reward augmented equation readily amended conﬁdence estimates similar fashion transition model. following empirical tests incorporate model extraction focusing mechanism prioritized sweeping. results illustrate types problems scenarios implicit imitation provide advantages reinforcement learning agent. experiments expert mentor introduced experiment serve model observer. case mentor following ε-greedy policy small tends cause mentor’s trajectories within cluster surrounding optimal trajectories even small amount exploration environment stochasticity mentors generally cover entire state space conﬁdence testing important. experiments prioritized sweeping used ﬁxed number backups observed experienced sample. ε-greedy exploration used decaying observer agents given uniform dirichlet priors q-values initialized zero. observer agents compared control agents beneﬁt mentor’s experience otherwise identical tests performed stochastic grid world domains since make clear extent observer’s mentor’s optimal policies overlap figure simple example shows start state grid. typical optimal mentor trajectory illustrated solid line start states. dotted line shows typical mentor-inﬂuenced trajectory quite similar observed mentor trajectory. assume eight-connectivity cells state grid nine neighbors including itself agents four possible actions. experiments four actions move agent compass directions although agent initially know action which. focus primarily whether imitation improves performance learning since learner converge optimal policy whether uses imitation not. ﬁrst experiment compare performance observer using model extraction expert mentor performance control agent using independent reinforcement learning. given uniform nature grid world lack intermediate rewards conﬁdence testing required. agents attempt learn policy maximizes discounted return grid world. start upper-left corner seek goal value lower-right corner. upon reaching goal agents restarted upper-left corner. generally mentor follow similar identical trajectory mentors trained using greedy strategy leaves path slightly highly valued rest. action dynamics noisy intended direction realized time directions taken otherwise discount factor figure plot cumulative number goals obtained previous time steps observer control ctrl agents observer able quickly incorporate policy learned mentor value estimates. results steeper learning curve. contrast control agent slowly explores space build model ﬁrst. delta curve shows diﬀerence performance agents. agents converge optimal value function. next experiment illustrates sensitivity imitation size state space action noise level. again observer uses model-extraction conﬁdence testing. figure plot delta curves basic scenario described scale scenario state space size increased percent stoch scenario noise level increased percent total gain represented area curves observer non-imitating prioritized sweeping agent increases state space size. reﬂects whitehead’s observation grid worlds exploration requirements increase quickly state space size optimal path length increases linearly. guidance mentor help larger state spaces. increasing noise level reduces observer’s ability upon information received mentor therefore erodes advantage control agent. note however beneﬁt imitation degrades gracefully increased noise present even relatively extreme noise level. sometimes observer’s prior beliefs transition probabilities mentor mislead observer cause generate inappropriate values. conﬁdence mechanism proposed previous section prevent observer fooled misleading priors mentor’s transition probabilities. demonstrate role conﬁdence mechanism implicit imitation designed experiment based scenario illustrated figure again agent’s task navigate top-left corner bottom-right corner grid order attain reward created pathological scenario islands high reward enclosed obstacles. since observer’s priors reﬂect eight-connectivity uniform high-valued cells middle island believed reachable states diagonally adjacent small prior probability. reality however agent’s action precludes agent therefore never able realize value. four islands scenario thus create fairly large region center space high estimated value could potentially trap observer persisted prior beliefs. notice standard reinforcement learner quickly learn none actions take rewarding islands; contrast implicit imitator using augmented backups could fooled prior mentor model. mentor visit states neighboring island observer evidence upon change prior belief mentor actions equally likely take eight possible directions. imitator falsely conclude basis mentor action model action exist would allow access islands value. observer therefore needs conﬁdence mechanism detect mentor model less reliable model. test conﬁdence mechanism mentor follows path around outside obstacles path cannot lead observer trap combination high initial exploration rate ability prioritized sweeping spread value across large distances virtually guarantees observer trap. given scenario observer agents control. ﬁrst observer used conﬁdence interval width given which according chebychev rule cover approximately percent arbitrary distribution. second observer given interval eﬀectively disables conﬁdence testing. observer conﬁdence testing consistently became stuck. examination value function revealed consistent peaks within trap region inspection agent state trajectories showed stuck trap. observer conﬁdence testing consistently escaped trap. observation value function time shows trap formed faded away observer gained enough experience actions allow ignore overcome erroneous priors mentor actions. figure performance observer conﬁdence testing shown performance control agent observer’s performance slightly degraded unaugmented control agent even pathological case. next experiment demonstrates potential gains imitation increase diﬃculty problem. observer employs model extraction conﬁdence testing though conﬁdence testing play signiﬁcant role here. maze scenario introduce obstacles order increase diﬃculty learning problem. maze grid obstacles complicating agent’s journey top-left bottom-right corner. optimal solution takes form snaking -step path distracting paths branching solution path necessitating frequent backtracking. discount factor percent noise optimal goal-attainment rate goals steps. graph figure control agent takes order steps build decent value function reliably leads goal. point achieving four goals steps average exploration rate still reasonably high imitation agent able take advantage mentor’s expertise build reliable value function steps. since control agent unable reach goal ﬁrst steps delta control imitator simply equal imitator’s performance. augmented backup rule require reward structure mentor observer identical. many useful scenarios rewards dissimilar value functions policies induced share structure. experiment demonstrate interesting scenario relatively easy suboptimal solution diﬃcult optimal solution. observer ﬁnds suboptimal path however able exploit observations mentor shortcut signiﬁcantly shortens path goal. structure scenario shown figure suboptimal solution lies path location around scenic route location goal location mentor takes vertical path location location shortcut. discourage shortcut novice agents lined cells agent immediately jumps back start state. therefore diﬃcult novice agent executing random exploratory moves make shortcut obtain value would reinforce future use. observer control therefore generally scenic route ﬁrst. figure performance control observer compared indicating value observations. observer control agent longer scenic route though control agent takes longer observer goes shortcut increases return almost double goal rate. experiment shows mentors improve observer policies even observer’s goals mentor’s path. ﬁnal experiment illustrates model extraction readily extended observer extract models multiple mentors exploit valuable parts each. again observer employs model extraction conﬁdence testing. figure learner must move start location goal location expert agents diﬀerent start goal states serve potential mentors. mentor repeatedly moves location location along dotted line second mentor departs location ends location along dashed line. experiment observer must figure observer successfully pulls together information sources order learn much quickly control agent value-based technique allows observer choose mentor’s inﬂuence state-by-state basis order best solution problem. homogeneity assumption violated implicit imitation framework described cause learner’s convergence rate slow dramatically cases cause learner become stuck small neighborhood state space. particular learner unable make state transition mentor given state drastically overestimate value state. inﬂated value estimate causes learner return repeatedly state even though exploration never produce feasible action attains inﬂated estimated value. mechanism removing inﬂuence mentor’s markov chain value estimates—the observer extremely conﬁdent observations mentor’s model. problem lies fact augmented bellman backup justiﬁed assumption observer duplicate every mentor action. state equivalent action exist guarantee value calculated using mentor action model fact achieved. heterogeneous settings prevent lock-up poor convergence explicit action feasibility test augmented backup performed observer tests whether mentor’s action diﬀers actions given current estimated models. augmented backup suppressed standard bellman backup used update value function. default mentor actions recall uncertainty agent’s true transition probabilities captured dirichlet distribution derived sampled transitions. comparing eﬀected diﬀerence means test respect corresponding dirichlets. complicated fact dirichlets highly non-normal small parameter values transition distributions multinomial. deal non-normality requiring minimum number samples using robust chebychev bounds pooled variance distributions compared. conceptually evaluate equation critical value test. parameter signiﬁcance test probability falsely reject actions diﬀerent actually same. given highly non-normal distributions early training process appropriate value given computed chebychev’s bound solving samples accurate test persist augmented backups value estimate inﬂated backups agent biased obtain additional samples allow agent perform required feasibility test. assumption therefore self-correcting. deal multivariate complications performing bonferroni test shown give good results practice eﬃcient compute known robust dependence variables. bonferroni hypothesis test obtained conjoining several single variable tests. suppose actions result possible successor states hypothesis denotes transition probability successor state denote complementary hypothesis bonferroni inequality states ei—the action models same—by testing complementary hypotheses conﬁdence level α/r. reject hypotheses reject notion actions equal conﬁdence least mentor action deemed infeasible every observer action multivariate bonferroni test described rejects hypothesis action mentor’s. pseudo-code bonferroni component feasibility test appears table assumes suﬃcient number samples. eﬃciency reasons cache results feasibility testing. duplication mentor’s action state ﬁrst determined infeasible state eﬀect. action feasibility testing essentially makes strict decision whether agent duplicate mentor’s action speciﬁc state decided mentor’s action infeasible augmented backups suppressed potential guidance oﬀered eliminated state. unfortunately strictness test results somewhat impoverished notion similarity mentor observer. this turn unnecessarily limits transfer mentor observer. propose mechanism whereby mentor’s inﬂuence persist even speciﬁc action chooses feasible mentor; instead rely possibility observer approximately duplicate mentor’s trajectory instead exactly duplicating suppose observer previously constructed estimated value function using augmented backups. using mentor action model high value calculated state subsequently suppose mentor’s action state judged infeasible. illustrated figure estimated value state originally mentor’s action sake illustration moves high probability state lead highly-rewarding region state space. number experiences state however learner concludes action πm—and associated high probability transition t—is feasible. point things must occur either value calculated state predecessors collapse exploration towards highly-valued regions beyond state ceases; estimated value drops slightly exploration continues towards highly-valued regions. latter case arise follows. observer previously explored vicinity state observer’s action model suﬃciently developed still connect higher value-regions beyond state state bellman backups. example learner suﬃcient experience learned highly-valued region reached alternative trajectory newly discovered infeasibility mentor’s transition deleterious eﬀect value estimate highly-valued likely states close mentor’s trajectory explored degree. case state highly-valued using mentor’s action model still valued highly enough likely guide exploration toward area. call alternative mentor’s action bridge allows value higher value regions over infeasible mentor transition. bridge formed without intention agent call process spontaneous bridging. spontaneous bridge exist observer’s action models generally undeveloped typically undeveloped models assign small probability every possible outcome therefore diffuse value higher valued regions lead poor value estimate state result often dramatic drop value state predecessors; exploration towards highly-valued region neighborhood state ceases. example could occur observer’s transition models state assign probability moving state lack experience spontaneous bridging eﬀect motivates broader notion similarity. observer short sequence actions bridges infeasible action mentor’s trajectory mentor’s example still provide extremely useful guidance. moment assume short path path length greater given integer observer k-step similar mentor state observer duplicate fewer steps mentor’s nominal transition state suﬃciently high probability. given notion similarity observer test whether spontaneous bridge exists determine whether observer danger value function collapse concomitant loss guidance decides suppress augmented backup state this observer initiates reachability analysis starting state using action model determine sequence actions leads suﬃciently high probability state state mentor’s trajectory downstream infeasible action. k-step bridge already exists augmented backups safely suppressed state eﬃciency maintain state mark bridged. state known bridged k-step reachability analysis need repeated. spontaneous bridge cannot found might still possible intentionally build one. build bridge observer must explore state k-steps away hoping make contact mentor’s trajectory downstream infeasible mentor action. implement single search attempt k-step random walk result trajectory average steps away long ergodicity local connectivity assumptions satisﬁed. order search occur must motivate observer return state engage repeated exploration. could provide motivation observer asking observer assume infeasible action repairable. observer therefore continue augmented backups support high-value estimates state observer repeatedly engage exploration point. danger course fact bridge case observer repeat search bridge indeﬁnitely. therefore need mechanism terminate repair process k-step repair infeasible. could attempt explicitly keep track possible paths open observer paths explicitly tried observer determine repair possibilities exhausted. instead elect follow probabilistic search eliminates need bookkeeping bridge cannot constructed within attempts k-step random walk repairability assumption judged falsiﬁed augmented backup state suppressed observer’s bias explore vicinity state eliminated. bridge found state used mark state irreparable. approach course na¨ıve heuristic strategy; illustrates basic import bridging. systematic strategies could used involving explicit planning bridge using local search another aspect problem address persistence search bridges. speciﬁc domain number unsuccessful attempts bridges learner conclude unable reconstruct mentor’s behavior case search bridges abandoned. involves simple higher-level inference notion similarity capabilities. notions could also used automatically determine parameter settings parameters must tuned empirically estimated given knowledge connectivity domain prior beliefs similar trajectories mentor observer instance seems suitable -connected grid world noise based number trajectories required cover perimeter states k-step rectangle around state. note large values reduce performance non-imitating agents results temporary lock feasibility k-step repair easily integrated homogeneous implicit imitation framework. essentially simply elaborate conditions augmented backup employed. course additional representation introduced keep track whether state feasible bridged repairable many repair attempts made. action selection mechanism also overridden bridge-building algorithm required order search bridge. bridge building always terminates attempts however cannot aﬀect long convergence. aspects algorithm however exploration policy unchanged. complete elaborated decision procedure used determine augmented backups employed state respect mentor appears table uses internal state make decisions. original model ﬁrst check observer’s experience-based calculation value state supersedes mentorbased calculation; observer uses experience-based calculation. mentor’s action feasible accept value calculated using observationbased value function. action infeasible check state bridged. ﬁrst time test requested reachability analysis performed results drawn cache subsequent requests. state bridged suppress augmented backups conﬁdent cause value function collapse. state bridged repairable. ﬁrst requests agent attempt k-step repair. repair succeeds state marked bridged. cannot repair infeasible transition mark not-repairable suppress augmented backups. wish employ implicit imitation feasibility testing multiple-mentor scenario. change implicit imitation without feasibility testing observer imitate feasible actions. observer searches mentors action results highest value estimate observer must consider mentors whose actions still considered feasible section empirically demonstrate utility feasibility testing k-step repair show techniques used surmount diﬀerences actions agents small local diﬀerences state-space topology. problems ﬁrst experiment shows importance feasibility testing implicit imitation agents heterogeneous actions. scenario agents must navigate across obstacle-free grid world upper-left corner goal location lowerright. agent reset upper-left corner. ﬁrst agent mentor news action mentor given optimal stationary policy problem. study performance three learners skew action unable duplicate mentor exactly nature grid world control imitation agents actually execute actions goal mentor optimal goal rate control imitator therefore lower mentor. ﬁrst learner employs implicit imitation feasibility testing second uses imitation without feasibility testing third control agent uses imitation agents experience limited stochasticity form chance action randomly perturbed. last section agents model-based reinforcement learning prioritized sweeping. eﬀectiveness feasibility testing implicit imitation seen figure horizontal axis represents time simulation steps vertical axis represents average number goals achieved time steps imitation agent feasibility testing converges much quickly optimal goal-attainment rate agents. agent without feasibility testing achieves sporadic success early frequently locks repeated attempts duplicate infeasible mentor actions. agent still manages reach goal time time stochastic actions permit agent become permanently stuck obstaclefree scenario. control agent without form imitation demonstrates signiﬁcant delay convergence relative imitation agents lack form guidance easily surpasses agent without feasibility testing long run. gradual slope control agent higher variance control agent’s discovery time optimal path feasibility-testing imitator control agent converge optimal solutions. shown comparison imitation agents feasibility testing necessary adapt implicit imitation contexts involving heterogeneous actions. developed feasibility testing bridging primarily deal problem adapting agents heterogeneous actions. techniques however applied agents diﬀerences state-space connectivity test this constructed domain agents news action alter environment learners introducing obstacles aren’t present mentor. figure learners mentor’s path obstructed figure results qualitatively similar previous experiment. contrast previous experiment imitator control news action therefore shortest path length mentor. consequently optimal goal rate imitators control higher previous experiment. observer without feasibility testing diﬃculty maze value function augmented mentor observations consistently leads observer states whose path goal directly blocked. agent feasibility testing quickly discovers mentor’s inﬂuence inappropriate states. conclude local diﬀerences state well handled feasibility testing. next demonstrate feasibility testing completely generalize mentor’s trajectory. here mentor follows path completely infeasible imitating agent. mentor’s path runs give imitating agent maze shown figure states mentor visits blocked obstacle. imitating agent able mentor’s trajectory guidance builds parallel trajectory completely disjoint mentor’s. results figure show gain imitator feasibility testing control agent diminishes still exists marginally imitator forced generalize completely infeasible mentor trajectory. agent without feasibility testing poorly even compared control agent. gets stuck around doorway. high value gradient backed along mentor’s path becomes accessible agents doorway. imitation agent feasibility conclude cannot proceed south doorway diﬀerent strategy. imitator without feasibility testing never explores enough away doorway setup independent value gradient guide goal. slower decay schedule exploration imitator without feasibility testing would goal would still reduce performance imitator feasibility testing. imitator feasibility testing makes prior beliefs follow mentor backup value perpendicular mentor’s path. value gradient therefore form parallel infeasible mentor path imitator follow along side infeasible path towards doorway makes necessary feasibility test proceeds goal. explained earlier simple problems good chance informal eﬀects prior value leakage stochastic exploration form bridges feasibility testing cuts value propagation guides exploration. diﬃcult problems agent spends time exploring accumulate suﬃcient samples conclude mentor’s actions infeasible long agent constructed bridge. imitator’s performance would drop unaugmented reinforcement learner. demonstrate bridging devised domain agents must navigate upper-left corner bottom-right corner across river three steps wide exacts penalty step goal state worth ﬁgure path mentor shown starting corner proceeding along edge river crossing river goal. mentor employs news action set. observer uses skew action attempts reproduce mentor trajectory. fail reproduce critical transition border river mentor action longer used backup value rewarding state alternative paths river blocks greedy exploration region. without bridging optimistic lengthly exploration phase observer agents quickly discover negative states river curtail exploration direction actually making across. examine value function estimate imitator feasibility testing repair capabilities that suppression feasibility testing darkly shaded high-value states figure terminate abruptly infeasible transition without making across river. fact dominated lighter grey circles showing negative values. experiment show bridging prolong exploration phase right way. employ k-step repair procedure examining graph figure imitation agents experience early negative guided deep river mentor’s inﬂuence. agent without repair eventually decides mentor’s action infeasible thereafter avoids river imitator repair also discovers mentor’s action infeasible immediately dispense mentor’s guidance. keeps exploring area mentor’s trajectory using random walk accumulating negative reward suddenly ﬁnds bridge rapidly converges optimal solution. control agent discovers goal runs. simple experiments presented demonstrate major qualitative issues confronting implicit imitation agent speciﬁc mechanisms implicit imitation address issues. section examine assumptions mechanisms presented previous sections determine types problems suitable implicit imitation. present several dimensions prove useful predicting performance implicit imitation types problems. already identiﬁed number assumptions implicit imitation applicable—some assumptions models imitation teaching cannot applied assumptions restrict applicability model. include lack explicit communication mentors observer; independent objectives mentors observer; full observability mentors observer; unobservability mentors’ actions; heterogeneity. assumptions full observability necessary model—as formulated—to work assumptions lack communication unobservable actions extend applicability implicit imitation beyond models literature; conditions hold simpler form explicit communication preferable. finally assumptions bounded heterogeneity independent objectives also ensure implicit imitation applied widely. however degree rewards actions homogeneous impact utility implicit imitation. turn attention predicting performance implicit imitation function certain domain characteristics. section examine questions ﬁrst given implicit imitation applicable implicit imitation bias agent suboptimal solution; second performance implicit imitation vary structural characteristics domains might want apply show analysis internal structure state space used motivate metric predicts implicit imitation performance. conclude analysis problem space understood terms distinct regions playing diﬀerent roles within imitation context. implicit imitation model observations agents improve observer’s knowledge environment rely sensible exploration policy exploit additional knowledge. clear understanding knowledge environment aﬀects exploration therefore central understanding implicit imitation perform domain. within implicit imitation framework agents know reward functions knowledge environment consists solely knowledge agent’s action models. general models take form. simplicity restricted models decomposed local models possible combination system state agent action. local models state-action pairs allow prediction j-step successor state distribution given initial state sequence actions local policy. quality j-step state predictions function every action model encountered initial state states time unfortunately quality j-step estimate drastically altered quality even single intermediate state-action model. suggests connected regions state space states fairly accurate models allow reasonably accurate future state predictions. since estimated value state based immediate reward reward expected received subsequent states quality value estimate also depend quality action models states connected since greedy exploration methods bias exploration according estimated value actions exploratory choices agent state also dependent connectivity reliable action models states reachable analysis implicit imitation performance respect domain characteristics therefore organized around idea state space connectivity regions connectivity deﬁnes. since connected regions play important role implicit imitation introduce classiﬁcation diﬀerent regions within state space shown graphically figure follows describe regions aﬀect imitation performance model. ﬁrst observe many tasks carried agent small subset states within state space deﬁned problem. precisely many mdps optimal policy ensure agent remains small subspace state space. leads deﬁnition ﬁrst regional distinction relevant irrelevant regions. relevant region states non-zero probability occupancy optimal policy. ε-relevant region natural generalization optimal policy keeps system within region fraction time. within relevant region distinguish three additional subregions. explored region contains states observer formulated reliable action models basis experience. augmented region contains states observer lacks reliable action models improved value estimates mentor observations. note explored augmented regions created result observations made learner regions therefore signiﬁcant connected components; contiguous regions state space reliable action mentor models available. finally blind region designates states observer neither personal experience beneﬁt mentor observations. information states within blind region come agent’s prior beliefs. regions interact imitation agent. first consider impact relevance. implicit imitation makes assumption accurate dynamics models allow observer make better decisions will turn result higher returns sooner learning process. however model information equally helpful imitator needs enough information irrelevant region able avoid since action choices inﬂuenced relative value actions irrelevant region avoided looks worse relevant region. given diﬀuse priors action models none actions open agent initially appear particularly attractive. however mentor provides observations within relevant region quickly make relevant region look much promising method achieving higher returns therefore constrain exploration signiﬁcantly. therefore considering problems point view relevance problem small relevant region relative entire space combined mentor operates within relevant region result maximum advantage imitation agent non-imitating agent. partitioning states explored blind augmented regions bears resemblance kearns singh’s partitioning state space known unknown regions. unlike kearns singh however partitions analysis. implicit imitation algorithm explicitly maintain partitions compute policy. states within explored region provided mentor still improve performance somewhat signiﬁcant evidence required accurately discriminate expected value actions. hence mentor observations explored region help result dramatic speedups convergence. consider augmented region observer’s q-values augmented observations mentor. experiments previous sections seen observer entering augmented region experience signiﬁcant speedups convergence information inherent augmented value function location rewards region. characteristics augmented zone however aﬀect degree augmentation improves convergence speed. since observer receives observations mentor’s state actions observer improved value estimates states augmented region policy. observer must therefore infer actions taken duplicate mentor’s behavior. observer prior beliefs eﬀects actions able perform immediate inference mentor’s actual choice action observer’s prior model uninformative observer explore local action space. exploring local action space however agent must take action action eﬀect. since guarantee agent took action duplicates mentor’s action somewhere diﬀerent mentor. action causes observer fall outside augmented region observer lose guidance augmented value function provides fall back performance level non-imitating agent. important consideration then probability observer remain augmented regions continue receive guidance. quality augmented region aﬀects observer’s probability staying within boundaries relative coverage state space. policy mentor sparse complete. relatively deterministic domain deﬁned begin states sparse policy covering states adequate. highly stochastic domain many start states agent need complete policy implicit imitation provide guidance agent domains stochastic require complete policies since policy cover larger part state space. important completeness policy predicting guidance must also take account probability transitions augmented region. actions domain largely invertible agent chance re-entering augmented region. ergodicity lacking however agent wait process undergoes form reset opportunity gather additional evidence regarding identity mentor’s actions augmented region. reset places agent back explored region make frontier last explored. lack ergodicity would reduce agent’s ability make progress towards high-value regions resets agent still guided attempt augmented region. eﬀectively agent concentrate exploration boundary explored region mentor augmented region. regions accurate action models allow agent move quickly possible high value regions. augmented regions augmented q-values inform agents states lead highly-valued outcomes. augmented region abuts explored region improved value estimates augmented region rapidly communicated across explored region accurate action models. observer resultant improved value estimates explored region together accurate action models explored region rapidly move towards promising states frontier explored region. states observer explore outward thereby eventually expand explored region encompass augmented region. case explored region augmented region overlap blind region. since observer information beyond priors blind region observer reduced random exploration. non-imitation context states explored blind. however imitation context blind area reduced eﬀective size augmented area. hence implicit imitation eﬀectively shrinks size search space problem even overlap explored augmented spaces. challenging case implicit imitation transfer occurs region augmented mentor observations fails connect observer explored region regions signiﬁcant reward values. case augmented region initially provide guidance. observer independently located rewarding states augmented regions used highlight shortcuts. shortcuts represent improvements agent’s policy. domains feasible solution easy optimal solutions diﬃcult implicit imitation used convert feasible solution increasingly optimal solution. seen distinctive regions used provide certain level insight imitation perform various domains. also analyze imitation performance terms properties across state space. analysis model information impacts imitation performance regions connected accurate action models allowed observer mentor observations learn promising direction exploration. then mentor observations useful concentrated connected region less useful dispersed state space unconnected components. fortunate completely observable environments observations mentors tend capture continuous trajectories thereby providing continuous regions augmented states. partially observable environments occlusion noise could lessen value mentor observations absence model predict mentor’s state. eﬀects heterogeneity whether diﬀerences action capabilities mentor observer diﬀerences environment agents also understood terms connectivity action models. value propagate along chains action models state mentor observer diﬀerent action capabilities. state possible achieve mentor’s value therefore value propagation blocked. again sequential decision making aspect reinforcement learning leads conclusion many scattered diﬀerences mentor observer create discontinuity throughout problem space whereas contiguous region diﬀerences mentor observer cause discontinuity region leave large regions fully connected. hence distribution pattern diﬀerences mentor observer capabilities important prevalence diﬀerence. explore pattern next section. characterize connectivity form metric. since diﬀerences reward structure environment dynamics action models aﬀect connectivity would manifest diﬀerences policies mentor observer designed metric based diﬀerences agents’ optimal policies. call metric fracture. essentially computes average minimum distance state mentor observer disagree policy state mentor observer agree policy. measure roughly captures diﬃculty observer faces proﬁtably exploiting mentor observations reduce exploration demands. formally mentor’s optimal policy observer’s. state space sπm=πo disputed states mentor observer diﬀerent optimal actions. neighboring disputed states constitutes disputed region. sπm=πo called undisputed states. distance metric space metric corresponds number transitions along minimal length path states standard grid world correspond manhattan distance. deﬁne fracture state space average minimal distance disputed state closest undisputed state things equal lower fracture value tend increase propagation value information across state space potentially resulting less exploration required. test metric applied number scenarios varying fracture coeﬃcients. diﬃcult construct scenarios vary fracture coeﬃcient expected value. scenarios figure constructed length possible paths start state goal state scenario. scenario however upper path lower path. mentor trained scenario penalizes lower path mentor learns take upper path. imitator trained scenario upper path penalized therefore take lower path. equalized diﬃculty problems follows using generic ε-greedy learning agent ﬁxed exploration schedule scenario tuned magnitude penalties exact placement along loops scenarios learner using exploration policy would converge optimal policy roughly number steps each. figure mentor takes loop optimal imitator would take bottom loop. since loops short length common path long average fracture low. compare figure loops long—the majority states scenario loops. states loop distance nearest state observer mentor policies agree namely state loop. scenario therefore high average fracture coeﬃcient. since loops various scenarios diﬀer length penalties inserted loops vary respect distance goal state therefore aﬀect total discounted expected reward diﬀerent ways. penalties also cause agent become stuck local minimum order avoid penalties exploration rate low. experiments therefore compare observer agents basis likely converge optimal solution given mentor example. figure presents percentage runs imitator converged optimal solution function exploration rate scenario fracture. distinct diagonal trend table illustrating increasing fracture requires imitator increase levels exploration order implicit imitation fundamentally biasing exploration observer. such worthwhile positive eﬀect observer performance. short answer mentor following optimal policy observer cause observer explore neighborhood optimal policy generally bias observer towards ﬁnding optimal policy. detailed answer requires looking explicitly exploration reinforcement learning. theory ε-greedy exploration policy suitable rate decay cause implicit imitators eventually converge optimal solution unassisted counterparts. however practice exploration rate typically decayed quickly order improve early exploitation mentor input. given practical theoretically unsound exploration rates observer settle mentor strategy feasible non-optimal. easily imagine examples consider situation agent observing mentor following policy. early learning process value policy followed mentor look better estimated value alternative policies available observer. could case mentor’s policy actually optimal policy. hand case alternative policies observer neither personal experience observations mentor actually superior. given lack information aggressive exploitation policy might lead observer falsely conclude mentor’s policy optimal. implicit imitation bias agent suboptimal policy reason expect agent learning domain suﬃciently challenging warrant imitation would discovered better alternative. emphasize even mentor’s policy suboptimal still provides feasible solution preferable solution many practical problems. regard classic exploration/exploitation tradeoﬀ additional interpretation implicit imitation setting. component exploration rate correspond observer’s belief suﬃciency mentor’s policy. paradigm then seems somewhat misleading think terms decision whether follow speciﬁc mentor not. question much exploration perform addition required reconstruct mentor’s policy. applications implicit imitation variety contexts. emerging electronic commerce information infrastructure driving development vast networks multi-agent systems. networks used competitive purposes trade implicit imitation used agent learn buying strategies information ﬁltering policies agents order improve behavior. implicit imitation could used transfer knowledge existing learned controller already adapted clients learning controller completely diﬀerent architecture. many modern products elevator controllers cell traﬃc routers automotive fuel injection systems adaptive controllers optimize performance system speciﬁc user proﬁles. upgrading technology underlying system quite possible sensors actuators internal representation system incompatible system. implicit imitation provides method transferring valuable user information systems without explicit communication. traditional application imitation-like technologies lies area bootstrapping intelligent artifacts using traces human behavior. research within behavioral cloning paradigm investigated transfer applications piloting aircraft controlling loading cranes researchers investigated imitation simplify programming robots ability imitation transfer complex nonlinear dynamic behaviors existing human agents makes particularly attractive control problems. model implicit imitation presented makes certain restrictive assumptions regarding structure decision problem solved simplifying assumptions aided detailed development model believe basic intuitions much technical development extended richer problem classes. suggest several possible extensions section provides interesting avenue future research. current paradigm assumes observer knows reward function. assumption consistent view form automatic programming. however relax constraint assuming ability generalize observed rewards. suppose expected reward expressed terms probability distribution features observer’s state pr). model-based distribution learned agent experience. features applied mentor’s state observer learned reward distribution estimate expected reward mentor states well. extends paradigm domains rewards unknown preserves ability observer evaluate mentor experiences terms. imitation techniques designed around assumption observer mentor share identical rewards utgoﬀ’s would course work absence reward function. notion inverse reinforcement learning could adapted case well. challenge future research would explore synthesis implicit imitation reward inversion approaches handle observer’s prior beliefs intermediate level correlation reward function observer mentor. cast general imitation model framework stochastic games restriction model presented thus noninteracting games essentially means standard issues associated multiagent interaction arise. course many tasks require interactions agents; cases implicit imitation oﬀers potential accelerate learning. general solution requires integration imitation general models multiagent based stochastic markov games would doubt rather challenging rewarding endeavor. take simple example simple coordination problems might imagine imitator learning mentor reversing roles roles considering observed state transition inﬂuenced joint action. general settings learning typically requires great care since agents learning nonstationary environment converge again imitation techniques oﬀer certain advantages instance mentor expertise suggest means coordinating agents challenges opportunities present imitation used multiagent settings. example competitive educational domains agents choose actions maximize information exploration returns exploitation; must also reason actions communicate information agents. competitive setting agent wish disguise intentions context teaching mentor wish choose actions whose purpose abundantly clear. considerations must become part action selection process. extension model partially observable domains critical since unrealistic many settings suppose learner constantly monitor activities mentor. central idea implicit imitation extract model information observations mentor rather duplicating mentor behavior. means mentor’s internal belief state policy relevant learner. take somewhat behaviorist stance concern mentor’s observed behaviors tell possibilities inherent environment. observer keep belief state mentor’s current state done using estimated world model observer uses update belief state. preliminary investigation model suggests dealing partial observability viable. derived update rules augmented partially observable updates. updates based bayesian formulation implicit imitation turn based bayesian fully observable contexts seen eﬀective exploration using mentor observations possible fully observable domains bayesian model imitation used extension model cases mentor’s state partially observable reasonably straightforward. anticipate updates performed using belief state mentor’s state interesting dealing additional factor usual exploration-exploitation tradeoﬀ determining whether worthwhile take actions render mentor more visible many realistic domains continuous attributes large state action spaces prohibit explicit table-based representations. reinforcement learning domains typically modiﬁed make function approximators estimate q-function points direct evidence received. important approaches parameter-based models memory-based approaches approaches model-free learning generally employed. agent keeps value function uses environment implicit model perform backups using sampling distribution provided environment observations. straightforward approach casting implicit imitation continuous setting would employ model-free learning paradigm first recall augmented bellman backup function used implicit imitation examine augmented backup equation converted model-free form much ordinary bellman backup. standard q-function observer actions additional action corresponds action taken mentor. imagine observer state took action ended state time mentor made transition state discussed earlier relative quality mentor observer estimates qfunction speciﬁc states vary. again order avoid inaccurate prior beliefs mentor’s action models bias exploration need employ conﬁdence measure decide apply augmented equations. feel natural setting kind tests memory-based approaches function approximation. memorybased approaches locally-weighted regression provide estimates functions points previously unvisited also maintain evidence used generate estimates. note implicit bias memory-based approaches assumes smoothness points unless additional data proves otherwise. basis bias propose compare average squared distance query exemplars used estimate mentor’s q-value average squared distance query exemplars used observer-based estimate heuristically decide agent reliable q-value. approach suggested beneﬁt prioritized sweeping. prioritizedsweeping however adapted continuous settings feel reasonably eﬃcient technique could made work. research imitation spans broad range dimensions ethological studies abstract algebraic formulations industrial control algorithms. ﬁelds crossfertilized informed other come stronger conceptual deﬁnitions better understanding limits capabilities imitation. many computational models proposed exploit specialized niches variety control paradigms imitation techniques applied variety real-world control problems. conceptual foundations imitation clariﬁed work natural imitation. work apes octopi animals know socially facilitated learning widespread throughout animal kingdom. number researchers pointed however social facilitation take many forms instance mentor’s attention object draw observer’s attention thereby lead observer manipulate object independently model provided mentor. true imitation therefore typically deﬁned restrictive fashion. visalberghi fragazy cite mitchell’s deﬁnition deﬁnition perhaps presupposes cognitive stance towards imitation agent explicitly reasons behaviors agents behaviors relate action capabilities goals. imitation analyzed terms type correspondence demonstrated mentor’s behavior observer’s acquired behavior correspondence types distinguished level. action level correspondence actions. program level actions completely diﬀerent correspondence found subgoals. eﬀect level agent plans actions achieve eﬀect demonstrated behavior direct correspondence subcomponents observer’s actions mentor’s actions. term abstract imitation proposed case agents imitate behaviors come imitating mental state agents study speciﬁc computational models imitation yielded insights nature observer-mentor relationship aﬀects acquisition behaviors observers. instance related ﬁeld behavioral cloning observed mentors implement conservative policies generally yield reliable clones highly-trained mentors following optimal policy small coverage state space yield less reliable clones make mistakes partially observable problems learning perfect oracles disastrous choose policies based perceptions available observer. observer therefore incorrectly biased away less risky policies require additional perceptual capabilities finally observed successful clones would often outperform original mentor cleanup eﬀect original goals behavioral cloning extract knowledge humans speed design controllers. extracted knowledge useful argued rule-based systems oﬀer best chance intelligibility become clear however symbolic representations complete answer. representational capacity also issue. humans often organize control tasks time typically lacking state perception-based approaches control. humans also naturally break tasks independent components subgoals studies also demonstrated humans give verbal descriptions control policies match actual actions potential saving time acquisition borne study explicitly compared time extract rules time required program controller addition traditionally considered imitation agent also face problem learning imitate ﬁnding correspondence actions states observer mentor fully credible approach learning observation absence communication protocols deal issue. theoretical developments imitation research accompanied number practical implementations. implementations take advantage properties diﬀerent control paradigms demonstrate various aspects imitation. early behavioral cloning research took advantage supervised learning techniques decision trees decision tree used learn human operator mapped perceptions actions. perceptions encoded discrete values. time delay inserted order synchronize perceptions actions trigger. learning apprentice systems also attempted extract useful knowledge watching users goal apprentices independently solve problems. learning apprentices closely related programming demonstration systems later eﬀorts used sophisticated techniques extract actions visual perceptions abstract actions future work associative recurrent learning models allowed work area extended learning temporal sequences associative learning used together innate following behaviors acquire navigation expertise agents related slightly diﬀerent form imitation studied multi-agent reinforcement learning community. early precursor imitation found work sharing perceptions agents closer imitation idea replaying perceptions actions agent second agent here transfer agent another contrast behavioral cloning’s transfer human agent. representation also diﬀerent. reinforcement learning provides agents ability reason eﬀects current actions expected future utility agents integrate knowledge knowledge extracted agents comparing relative utility actions suggested knowledge source. seeding approaches closely related. trajectories recorded human subjects used initialize planner subsequently optimizes plan order account diﬀerences human eﬀector robotic eﬀector technique extended handle notion subgoals within task subgoals also addressed others work based idea agent extracting model mentor using model information place bounds value actions using reward function. agents therefore learn mentors reward functions diﬀerent own. another approach family based assumption mentor rational reward function observer chooses actions. given assumptions conclude action chosen mentor particular state must higher value mentor alternatives open mentor therefore higher value observer alternative. system utgoﬀ clouse therefore iteratively adjusts values actions constraint satisﬁed model. related approach uses methodology linear-quadratic control first model system constructed. inverse control problem solved cost matrix would result observed controller behavior given environment model. recent work inverse reinforcement learning takes related approach reconstructing reward functions observed behavior similar inversion quadratic control approach formulated discrete domains. several researchers picked idea common representations perceptual functions action planning. approach using representation perception control based controller model. controller represents behavior. output compared observed behaviors order select action closest observed behavior explicit motor action schema also investigated dual role perceptual motor representations friedrich munch dillmann bocionek sassin aeration plants container loading cranes imitation learning also applied acceleration generic reinforcement learning less traditional applications include transfer musical style support social atmosphere imitation also investigated route language acquisition transmission described formal principled approach imitation called implicit imitation. stochastic problems explicit forms communication possible underlying model-based framework combined model extraction provides alternative imitation learning-by-observation systems. approach makes model compute actions imitator take without requiring observer duplicate mentor’s actions exactly. shown implicit imitation oﬀer signiﬁcant transfer capability several test problems proves robust face noise capable integrating subskills multiple mentors able provide beneﬁts increase diﬃculty problem. seen feasibility testing extends implicit imitation principled manner deal situations homogeneous action assumption invalid. adding bridging capabilities preserves extends mentor’s guidance presence infeasible actions whether diﬀerences action capabilities local diﬀerences state spaces. approach also relates idea following sense imitator uses local search model repair discontinuities augmented value function before acting world. process applying imitation various domains learned properties. particular developed fracture metric characterize eﬀectiveness mentor given observer speciﬁc domain. also made considerable progress extending imitation problem classes. model developed rather ﬂexible extended several ways example bayesian approach imitation building work shows great potential initial formulations promising approaches extending implicit imitation multiagent problems partially observable domains domains reward function speciﬁed priori. number challenges remain ﬁeld imitation. bakker kuniyoshi describe number these. among intriguing problems unique imitation evaluation expected payoﬀ observing mentor; inferring useful state reward mappings domains mentors observers; repairing locally searching order observed behaviors observer’s capabilities goals. also raised possibility agents attempting reason information revealed actions addition whatever concrete value actions agent. model-based reinforcement applied numerous problems. since implicit imitation added model-based reinforcement learning relatively little eﬀort expect applied many problems. basis simple elegant theory markov decision processes makes easy describe analyze. though focused simple examples designed illustrate diﬀerent mechanisms required implicit imitation expect variations approach provide interesting directions future research. thanks anonymous referees suggestions comments earlier versions work michael littman editorial suggestions. price supported irisiii project bac. boutilier supported nserc research grant iris-iii project bac. parts paper presented implicit imitation reinforcement learning proceedings sixteenth international conference machine learning bled slovenia pp.– imitation reinforcement learning agents heterogeneous actions proceedings fourteenth biennial conference canadian society computational studies intelligence ottawa pp.–", "year": 2011}