{"title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping  Tasks", "tag": ["cs.HC", "cs.AI", "cs.CV", "cs.LG", "cs.RO", "H.1.2; I.2.9; I.2.10; I.2.7; H.5.2; H.5.1; I.2.6; I.4.8; I.4.7;  I.4.6"], "abstract": "A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.", "text": "information using integrating diﬀerent perceptual channels. paper present combination integration active vision gestural instruction speech input instruct robot system grasping tasks though parts functional modules described evaluated standalone applications detail earlier integration full scale architecture described ﬁrst time proven major challenge enormous complexity overall system. therefore focus architecture module interconnections highlight lessons learnt building interactive system. whole described project part larger research eﬀort aiming towards development situated artiﬁcial communicators interacted natural human-like fashion combined verbal non-verbal instructions. line earlier work devoted robot teaching showing imitation learning much work various aspects learning cognimajor challenge realization intelligent robots supply cognitive abilities order allow ordinary users program easily intuitively. programming teaching work tasks interactive demonstration. make eﬀective convenient user machine must capable establish common focus attention able integrate spoken instructions visual perceptions non-verbal clues like gestural commands. report progress building hybrid architecture combines statistical methods neural networks ﬁnite state machines integrated system instructing grasping tasks man-machine interaction. system combines gravis-robot visual attention gestural instruction intelligent interface speech recognition linguistic interpretation modality fusion module allow multi-modal task-oriented man-machine communication respect dextrous robot manipulation objects. recent years generation intelligent robots found applications natural environments like museums hospitals private households. conventional programming eﬃcient factory ﬂoor applications cognitively oriented robots must instructable ordinary human users robust intuitive way. respect program work task interactive human demonstration requires endowment robot suﬃcient perceptual cognitive motor skills communicate user natural fashion. humans inevitably diﬀerent modalities interpersonal man-machine communication intelligent robot system take advantage tive architectures trajectory acquisition object recognition grasp pose determination sensor fusion grasping design integrated architecture widely believed hard achieve. thus developed advanced architectures capable integrating perceptual attention mechanism higher level functions next sections provide overview oversystem highest level building blocks special emphasis mutual interactions. demonstrate system’s capabilities discuss illustrate idea exists critical level skills development system towards complex capabilities progresses much faster. architecture design issues realizing complex intelligent robot system. ideal perspective common uniform software framework speciﬁed beforehand support subsequent distributed development modules according certain speciﬁcations. diﬀerent approaches like behavior based architectures agentbased concepts blackboard systems proposed context. however truly complex system diﬀerent types signals generated diﬀerent time scales require many sub-skills developed diverse programming paradigms. section discuss reasons experience unreasonable impose strong constraints submodules easier software engineering. consequence rather level architecture support integration heterogeneous components. entire system implemented larger number separate processes running parallel several workstations communicating distributed architecture communication system developed earlier purpose project. hereby submodules diﬀerent programming languages various visualization tools variety processing paradigms ranging neurally inspired attention system statistical declarative methods inference knowledge representation. thus architecture whole cannot easily subsumed single programming paradigms mentioned above. linguistic visual/gestural inputs converging integration module passes control manipulator. additionally control commands parts system modules interactions described following sections. vision hardware currently consists binocular active vision head -chip-ccd colorcameras controllable tilt left/right vergence motorized lenses determining focus zoom aperture combine total dofs. grasping manipulation carried standard puma manipulator operated real-time rccl-command library. additionally equipped wrist camera obtain local visual feedback grasping phase. grasping carried dextrous robot hand developed technical university munich. three approximately human-sized ﬁngers driven hydraulics system. ﬁngertips custom built ﬁngertip sensors provide force feedback control evaluation grasp. hardware setting control design described detail recently changed original hand design adding palm rearranging ﬁngers humanlike conﬁguration allow larger variety twothree-ﬁnger grasps. vision system robot. furthermore short term visual memory realized order understand linguistic reference objects spoken instructions. attention system places high emphasis spatial organization visual clues enhances design proposed consists layered system topographically organized neural maps integrating diﬀerent low-level feature maps continually updated focus attention active camera head. similar mechanisms also employed however results highly idealized synthetic images using lower number less complex maps reported. particular stereo images number feature maps indicating presence oriented edges hsi-color saturation intensity motion skin color computed. main goals system recognize pointing hands multiply diﬀerence skin segmentation result moving skin considered separate feature map. weighted feature maps multiplied fadeoutmap form ﬁnal attention highest peak determines next ﬁxation fig. after stereo matching resulting loop continuously generates saccades ﬁxations active exploration behavior persists whole system operation. interaction human user modify attention diﬀerent mechanisms. spoken instruction references colored object corresponding weight increased bias attention system towards spots image. increases probability ﬁxations things time decay mechanism hand gesture recognition modules detect pointing gesture image d-direction pointing ﬁnger computed corresponding region interest virtually projected table. respective manipulation multiplied coordinate-wise attention restrict explorative attention region next step fig. exploration behavior tends ﬁxate repetitively upon interesting points cases objects. emerging regularity used establish short term visual memory integration module d-ﬁxation coordinates sent. uses temporal integration stabilize salient points additionally homogeneous color blob detected assumed object referenced spoken instructions. future extensions sophisticated object recognition point. also plan speciﬁc object maps attention system favored spoken instructions exactly like color maps. allow ﬂuent communication instructor artiﬁcial communicator system capable understanding speaker independent speech input. instructor neither needs know special command syntax exact terms identiﬁers objects. consequently complete speech understanding system face high degree referential uncertainty vague meanings speech recognition errors un-modeled language structures. approach robust spoken language understanding uses vertical organization knowledge representation integrated processing scheme overcome drawbacks traditional horizontal architecture baseline module employs enhanced statistical speech recognizer. recognition process directly inﬂuenced partial parser provides linguistic domain-speciﬁc restrictions word sequences. therefore partial syntactic structures instead simple word sequences generated like e.g. object descriptions spatial relations combined subsequent speech understanding module form linguistic interpretations. cope out-of-vocabulary words employ recognition lexicon exceeds used understanding component covers lexical items frequently found corpus humanhuman human-machine dialogs. syntactic modeling allows additional words ﬁlled-in open lexical categories nouns robust system speech processing modules able cope spontaneous speech input largely deviates speech read text prompts used dictation task. particularly clear pronunciation vocabulary limitations restrictions languageuse never enforced. meet challenges recognition lexicon contains acoustic models spontaneous speech phenomena namely so-called human noises hesitations naive user describes object scene using attributes typically vocabulary diﬀerent ﬁxed appropriate processing visual data. therefore several kinds uncertainties considered correlating verbal object description object recognition results vague attributes vague spatial structural descriptions speech object recognition errors. order cope uncertainty developed bayesian network approach robustly combines verbal visual information different abstraction levels ﬁrst level basic features vision speech modeled evidential nodes bayesian network visual objects verballyreferenced objects. second level fused visual object class ovis k∈{...n} verbal object class oio/roj j∈{...n connected intended object reference object variables verbally-mentioned spatial structural relations objects established introducing additional evidential nodes diﬀerent kinds uncertainties modeled conditional probability tables estimated experimental data objects denoted utterance explaining observed visual verbal evidences evis everb bayesian network maximum posteriori probability. additional causal support intended object deﬁned optional target region interest provided d-pointing evaluation. intended object used dialog component system response manipulator instruction. many dialog systems developed recently lack integration modalities. contrast uni-modal approaches dialog module integrates utterances instructor information visible scene feedback robot realize natural ﬂexible robust dialog strategy. dialog module realized within semantic network language ernest using dialog model shown fig. model based investigation corpus human-human simulated human-machine dialogs. every path model reﬂects course possible humanmachine dialog. admissible sequence intermediate states nearly unrestricted leading natural robust dialog behavior state transitions initiated information instructor robot available. state transition function analyzes information combines current dialog context information gathered interrelation module select next state. using dialog context references objects resolved take bolt. cube. information accumulated dialog combined. dialog module react upon information diﬀerent modalities inform instructor errors execution action actively control dialog query illustrate capabilities system present typical action sequence picking deploying object sequential order. videos found sequence consists major stages initially number objects spread table workspace robot camera. system started partially calibrated speech commands shown fig. attention systems explores scene shown fig. transmits ﬁxation points integration module visual memory stabilized spatial object relations analyzed fig. user gives spoken instruction referencing objects. instruction semantically analyzed dialog initiated fig. system additional pointing information e.g. resolving ambiguities. also determines whether attention system biased towards particular colors. pointing hand found gesture evaluated visualized fig. interest region integration module. bayesian network integrates spoken inanalyzed figure speech input segmented semantic categories like action object spatial relations objects short time memory referenced instructions. right window number direct commands available also given spoken instructions. figure attention spots camera image stereo matched points transferred integration module hand ﬁnger recognition uses multi-layer perceptron based classiﬁcation intensity histograms projection d-pointing direction table missing unprecise information. overall goal module continue dialog every situation. actions cannot executed immediately rejected. verbal instructions could analyzed repetition requested times. dialog gathered completely contradictory information system expresses confusion asks instruction. integration module resolved ambiguities control passed robot arm/hand. starting d-coordinates determined vision integration modules approaching movement grasping executed semi-autonomous fashion relying local feedback only. hand control implemented ﬁnite state automaton switching diﬀerent modes hand states whose transitions triggered visual tactile feedback. particular wrist camera provides visual feedback object recognition approach grasp oﬀset position grasping phase ﬁngertip sensors provide necessary force feedback. grasping sequence starts approach movement recenters manipulator object chooses grasp prototype according recognized object aligns hand along main axis object executes grasp prototype tual interdependencies less specialized hardware delivering lower quality sensory inputs. consequently potential hidden capabilities resources exists overall system. approach avoid apparent waste capabilities restrict solution space individual modules ensure high degree homogeneity towards beforehand speciﬁed scenario. however reasonable robust functioning overall system achieved beneﬁt hidden capabilities certain modules quite easily. experience small coordinated modiﬁcations several modules slight changes control quickly open unforeseen perspectives system. give examples illustrate this recognition bars together corresponding grasp prototype allows progress cube-based pyramid building cube based building bridges houses closed boxes etc.. secondly slight change speech-initiated control allows reuse ﬁngertip detection algorithm initially employed objects deploying ﬁngertip positions. capabilities used teach multi-point trajectories pointing consecutive positions indicate small relative movements pointing nearby positions subsequently. believe experience summarized approaching critical level skills. level characterized situation small improvements reconﬁguration single modules slight changes control immediately open whole variety action opportunities. hereby beneﬁt certain amount robustness possibility readapt recalibrate rather loose coupling modules architecture realized message-passing communication paradigm allows quick reorganization control ﬂows. interactive teaching tasks take full advantage user’s creativity recombine system’s skills towards previously unexpected results. figure visual tactile feedback grasping view hand camera approach movements; ﬁnger grasp cube; hand camera view three ﬁnger grasp;d) three ﬁnger grasp; force feedback ﬁngertips evaluate grasp. struction visual memory gesture-based region bias determine object grasped. case fails dialog asks repetition instruction gesture. control passed hand/arm system performs visually guided approach movement determines grasp primitive pre-shapes hand aligns respect object ﬁnally grasps object force feedback control upon failure retries success integration module informed. dialog system asks user indicate deploy object. pointing evaluation part repeated slight diﬀerence d-ﬁngertip position directly determines position object deployment. control redirected hand/arm system deploys object system returns starting mode exploration. described above system integrates larger number skills local feedback mechanisms state machines. many modules developed tested independently other trained oﬄine adaptive calibration facilities much powerful operated standalone; however integrated system full capabilities module always employed. mupresented architecture integrates capabilities enable intuitive programming grasping tasks human user. ranges perceptual grounding active exploration scene interpretation complex user commands sophisticated speech analysis modality fusion system. widely accepted benchmarks cognitive robotic systems interacting humans diﬃcult assess performance systems systematically beyond demonstrating indeed running examples. thus think major challenges lift learning system oﬄine training widely used lower level modules level behavior. current system enhanced system monitor oﬀer tool study learning needs organized progress imitation human-instructed action sequences extracting knowledge task level. many issues propagate errors ﬂexibly reorganize control without losing robustness functionality system. come closer easily-instructable intelligent systems robustly carry non-trivial tasks natural environments.", "year": 2005}