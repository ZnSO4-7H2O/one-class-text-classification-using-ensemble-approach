{"title": "Probabilistic Graphical Models for Credibility Analysis in Evolving  Online Communities", "tag": ["cs.SI", "cs.AI", "cs.CL", "cs.IR", "stat.ML"], "abstract": "One of the major hurdles preventing the full exploitation of information from online communities is the widespread concern regarding the quality and credibility of user-contributed content. Prior works in this domain operate on a static snapshot of the community, making strong assumptions about the structure of the data (e.g., relational tables), or consider only shallow features for text classification.  To address the above limitations, we propose probabilistic graphical models that can leverage the joint interplay between multiple factors in online communities --- like user interactions, community dynamics, and textual content --- to automatically assess the credibility of user-contributed online content, and the expertise of users and their evolution with user-interpretable explanation. To this end, we devise new models based on Conditional Random Fields for different settings like incorporating partial expert knowledge for semi-supervised learning, and handling discrete labels as well as numeric ratings for fine-grained analysis. This enables applications such as extracting reliable side-effects of drugs from user-contributed posts in healthforums, and identifying credible content in news communities.  Online communities are dynamic, as users join and leave, adapt to evolving trends, and mature over time. To capture this dynamics, we propose generative models based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian Motion to trace the continuous evolution of user expertise and their language model over time. This allows us to identify expert users and credible content jointly over time, improving state-of-the-art recommender systems by explicitly considering the maturity of users. This also enables applications such as identifying helpful product reviews, and detecting fake and anomalous reviews with limited information.", "text": "first foremost would like express deepest gratitude supervisor mentor gerhard weikum giving opportunity pursue research guidance. constant motivation excellent scientiﬁc advice wisdom vision quintessential importance make work possible. always cherish interactions helped mature researcher also person. would like thank additional reviewers examiners dissertation jiawei dietrich klakow valuable feedback. extremely grateful collaborators co-authors cristian danescu-niculescu-mizil stephan günnemann hemank lamba kashyap popat sourav dutta jannik strötgen actively contributing shaping dissertation. thankful colleagues planck institute informatics participating discussions providing insightful ideas valuable feedback course doctoral studies. thankful friends made journey enjoyable especially arunav mishra sarvesh nikumbh tomasz tylenda dilafruz amanova sourav dutta nikita dutta. would also like thank administrative staff planck institute supportive providing assistance whenever necessary could freely indulge research. many thanks international planck research school planck society ﬁnancial support allowed pursue research present work conferences around world. last least would like thank parents sushama subrata mukherjee brother subhojyoti mukherjee continued support encouragement. importantly thank wife sarah john side since beginning time. major hurdles preventing full exploitation information online communities widespread concern regarding quality credibility user-contributed content. prior works domain operate static snapshot community making strong assumptions structure data consider shallow features text classiﬁcation. address limitations propose probabilistic graphical models leverage joint interplay multiple factors online communities like user interactions community dynamics textual content automatically assess credibility user-contributed online content expertise users evolution userinterpretable explanation. devise models based conditional random fields different settings like incorporating partial expert knowledge semi-supervised learning handling discrete labels well numeric ratings ﬁne-grained analysis. enables applications extracting reliable side-effects drugs user-contributed posts healthforums identifying credible content news communities. online communities dynamic users join leave adapt evolving trends mature time. capture dynamics propose generative models based hidden markov model latent dirichlet allocation brownian motion trace continuous evolution user expertise language model time. allows identify expert users credible content jointly time improving state-of-the-art recommender systems explicitly considering maturity users. also enables applications identifying helpful product reviews detecting fake anomalous reviews limited information. eine größten hürden vollständige nutzung informationen sogenannten online-communities verhindert sind weitverbreitete bedenken bezüglich qualität glaubwürdigkeit nutzer-generierten inhalten. frühere arbeiten diesem bereich gehen einer statischen version einer community machen starke annahmen bezüglich struktur daten oder berücksichtigen oberﬂächliche merkmale klassiﬁkation texten. oben genannten einschränkungen adressieren schlagen eine reihe probabilistischen graphischen modellen zusammenspiel mehrerer faktoren onlinecommunities berücksichtigen interaktionen zwischen nutzern dynamik communities textuell inhalt. dadurch können glaubwürdigkeit nutzer-generierten online inhalten sowie expertise nutzern ihrer entwicklung interpretierbaren erklärungen bewertet werden. hierfür konstruieren neue conditional random fields basierende modelle verschiedene szenarien beispielsweise partielles expertenwissen mittels semi-überwachtem lernen berücksichtigen. genauso können diskrete labels sowie numerische ratings präzise analysen genutzt werden. somit werden anwendungen ermöglicht etwa automatische extrahieren nebenwirkungen medikamenten nutzer-erstellten inhalten gesundheitsforen identiﬁzieren vertrauenswürdigen inhalten nachrichten-communities. online-communities sind dynamisch nutzer communities hinzustoßen oder diese verlassen. passen sich entstehenden trends entwickeln sich über zeit. diese dynamik abzudecken schlagen generative modelle hidden markov modellen latent dirichlet allocation brownian motion basieren. diese können kontinuierliche entwicklung nutzer-erfahrung sowie ihrer sprachentwicklung über zeit nachzeichnen. dies ermöglicht expertennutzer glaubwürdigen inhalt über zeit gemeinsam identiﬁzieren sodass aktuell besten recommendersysteme durch explizite berücksichtigen entwicklung expertise nutzern verbessert werden können. dadurch wiederum können anwendungen entwickelt werden nützliche produktbewertungen erkennen sowie ﬁngierte anomale bewertungen geringem informationsgehalt identiﬁzieren. probabilistic graphical models truth discovery trust reputation management information extraction language analysis social media information credibility social media collaborative filtering online communities iii. introduction motivation iii.. use-case study health communities iii.. use-case study news communities iii.. contributions iii. problem statement iii. overview model iii.. credibility classiﬁcation iii.. credibility regression iii. model components iii.. postings language iii.. semi-supervised conditional random fields credibility classiﬁcation iii.. continuous conditional random fields credibility regression topic model support vector regression continuous conditional random field iii. experimental evaluation health communities iii.. data iii.. baselines iii.. experiments quality measures iii.. results discussions iii.. discovering rare side effects iii.. following trustworthy users iii. experimental evaluation news communities iii.. data iii.. predicting user credibility ratings news articles iii.. finding credible news articles iii.. finding trustworthy sources iii.. finding expert users iii.. discussion iii. conclusions introduction motivation approach iv.. discete experience evolution iv.. continuous experience evolution discrete experience evolution iv.. model dimensions iv.. hypotheses initial studies iv.. building blocks model latent factor recommendation experience-based latent factor recommendation user-facet model supervised user-facet model iv.. joint model user experience facet preference writing style generative process review supervision rating prediction inference iv.. experiments setup data baselines quantitative comparison qualitative analysis continuous experience evolution importance time continuous experience evolution experience-aware language evolution iv.. joint model experience-language evolution generative process inference iv.. experiments data likelihood smoothness convergence experience-aware item rating prediction quantitative results qualitative results use-case study iv.. recommending news articles iv.. identifying experienced users conclusion introduction motivation approach finding useful product reviews finding credible reviews limited information exploring latent semantic factors find useful product reviews review helpfulness factors item facets review writing style reviewer expertise distributional hypotheses consistency timeliness early-bird bias preliminary study feature signiﬁcance joint model review helpfulness incorporating consistency factors incorporating latent facets incorporating latent expertise difference prior works modeling expertise generative process inference experiments setup data tasks evaluation measures baselines quantitative comparison qualitative comparison finding credible reviews limited information using consistency features review credibility analysis facet model consistency features additional language behavioral features tasks credible review classiﬁcation item ranking evaluation measures domain transfer yelp amazon ranking experiments setup data baselines quantitative analysis qualitative analysis conclusions variation experience years reviews user. stacked chart corresponds user recent experience number years spent number reviews posted community. iii. stylistic features. iii. examples affective features. iii. subjectivity bias features. iii. latent topics words. iii. features source trustworthiness. iii. symbol table. iii. user statistics. iii. information sample drug families number postings number users iii. number common less common rare side-effects listed experts iii. accuracy comparison setting iii. performance setting iii. experiment ﬁnding rare drug side-effects. iii. experiment following trustworthy users. iii. dataset statistics. iii. graph statistics. iii. comparison models predicting users’ credibility rating behavior -fold cross-validation. improvements statistically signiﬁcant p-value iii. comparison models predicting aggregated article credibility rating -fold cross-validation. improvements statistically signiﬁcant p-value iii. ndcg scores ranking trustworthy sources. iii. ndcg scores ranking expert users. iii. pearson’s product-moment correlation various factors rating prediction. model performs better dataset contains review texts user/activity information. kendall-tau correlation different models across domains. variation kendall-tau-m correlation reviews mamazon blogs online review portals provide overwhelming amount information various topics like health politics movies music travel more. however usability massive data largely restricted concerns quality credibility user-contributed content. online communities massive repositories knowledge accessed regular everyday users well expert professionals. instance adult u.s. population nearly half u.s. physicians consult online resources health-related information. product domain online consumers would electronics without consulting online reviews ﬁrst however user-contributed content highly noisy unreliable subjective rampant amount spams rumors misinformation injected users postings. greatly eroded public trust conﬁdence social media information. statistics show web-using u.s. adults trust social media information counter these stakeholders industry developing defense mechanism. certain domains like healthforums misinformation hazardous consequences frequently accessed users potential side-effects drugs symptoms diseases getting advice health professionals. give example consider following user-post online healthforum healthboards. prior works natural language processing dealing fake reviews opinion spam would analyze linguistic cues writing style post subjective biased fake. however difﬁcult arrive conclusion analyzing post isolation. general online communities provide many signals help task. instance post refuted experienced health professional community. similarly credible postings statements corroborated experienced users community. signiﬁcant challenge priori know users experienced trustworthy need inferred part task. kinds implicit explicit feedback users identities prove helpful credibility analysis community-speciﬁc setting. prior works data fusion truth discovery survey) leverage interactions sources queries general setting. typical queries height mount everest fetch different answers various sources birthplace obama includes answers hawaii africa. methods resolve conﬂicts among multisource data obtaining reliability estimates sources providing information aggregating responses obtain truth. however approaches operate structured data factual claims whereby ignore content context information. approaches geared online communities ﬁne-grained interactions subjective unstructured data. context helps understanding attitude emotional state user writing posts topics postings users’ topic-speciﬁc expertise objectivity rationality postings etc. similar principles hold true online community like music travel politics news. discussion demonstrates complex interplay several factors online communities like writing style cross-talk users interactions user experience topics inﬂuences credibility statements therein. natural represent interactions dependencies various factors provided probabilistic graphical models aspects envisioned random variables edges depicting interactions them. pgms provide natural framework compactly represent high-dimensional distributions many random variables product local factors subsets variables i.e. factoring joint probability distribution marginal distributions subsets variables. conditional independence assumptions factorization help make problem tractable. also effective practice random variable interacts subset variables. inference learning estimate joint probability distribution marginals queries interest. terms interpretability output probabilistic models better explained end-user. instance label sources trustworthy corresponding probabilities easier envision obtaining corresponding estimates contribution work bringing different aspects together computational model namely probabilistic graphical model credibility analysis online communities providing efﬁcient inference techniques same. information extraction methods previously used extracting information user-contributed content account inherent bias subjectivity noise data. additionally also consider role language assessing reliability extracted statements. lation analyze writing style capture bias subjectivity. typically ignore identity users writing postings interactions them. typically works bag-of-words features resources like wordnet sentiwordnet create feature vectors supervised machine learning models classify postings credible otherwise. hand works data fusion truth discovery survey) make strong assumptions nature structure data whereby model interactions sources queries edges network ignore textual content context altogether. typically works approaches like belief propagation label propagation propagate reliability estimates network. availability ground-truth data typical problem faced works domain. therefore prior works operate unsupervised fashion. however prior works show performance methods improved using small labeled data training. absence proper ground-truth data prior works make strong assumptions e.g. duplicates near-duplicates fake harness rich information users items form activity posting history meta-data. proﬁle history readily available several domains especially long-tail users items community also policy tends emphasize long-term contributors suppress outlier opinions mainstream. prior works collaborative ﬁltering consider static snapshot data whereby ignore temporal evolution users interactions. activity history proxy experienced members community. online communities dynamic nature users join leave adopt vocabulary adapt evolving trends. therefore user experienced decade could evolved matured user reﬁned preferences writing style trustworthiness. dimension user evolution ignored static analysis. works involving classiﬁers machine learning models generate discrete decision labels output. models limited interpretability rarely explain model arrived particular verdict. geared ﬁnegrained analysis involving continuous data types. additionally prior works output scores estimates reliability difﬁcult explain end-user. develop models jointly leverage context interactions online communities analyzing credibility user-contributed content? complement expert knowledge large-scale non-expert data online communities? develop novel forms probabilistic graphical models capture complex interplay several factors writing style user-user user-item interactions latent semantic factors like topics postings experience users etc. speciﬁcally develop conditional random field based models factors modeled random variables edges depicting interactions. furthermore variables observable features capture context postings relevant background information develop efﬁcient joint probabilistic inference techniques models classiﬁcation regression settings. speciﬁcally develop learns partial expert supervision using expectation maximization principle. model healthforum healthboards identify rare uncommon side-effects drugs user-contributed posts. problems large-scale non-expert data potential complement expert medical knowledge. model leverages partial expert knowledge drugs side-effects jointly identify credible statements reliable postings trustworthy users community. cikm deal user-assigned numeric ratings online communities. application use-case consider news communities plagued misinformation bias polarization induced style reporting political viewpoint media sources users. show joint probability distribution function continuous multivariate gaussian propose constrained gradient ascent based algorithm scalable inference. healthforum dataset contains million posts anonymized users community healthboards along demographic information. additionally also provide side-effects drugs drug families contributed expert health articles crawled media sources dataset contains newstrust-member reviews articles corresponding ratings various qualitative aspects like objectivity correctness information bias credibility; well interactions members demographic information. quantify changes users’ maturity experience online communities? model users’ evolution progression maturity? improve recommendation considering user’s evolved maturity experience timepoint consuming items? online communities dynamic users mature time evolved preferences writing style experience interactions. study temporal evolution users’ experience respect item recommendation collaborative ﬁltering framework review communities propose approaches model evolving user experience writing style ence progress discrete manner employing hidden markov model latent dirichlet allocation model traces experience progression models facets interest timepoint function experience. framework used identify useful product reviews terms helpful end-consumers communities like amazon useful reviews buried deep within heap non-informative ones. drawbacks discrete evolution develops natural continuous mode temporal evolution user’s experience language model using geometric brownian motion brownian motion respectively. develop efﬁcient inference techniques combine discrete multinomial distributions continuous brownian motion processes experience evolution. combination metropolis hastings kalman filter gibbs sampling shown work coherently increase data log-likelihood smoothly continuously time. utilize latent topic models leveraging review texts item ratings timestamps derive consistency features without relying extensive item/user histories typically unavailable long-tail items/users. used learn inconsistencies discrepancy contents review rating temporal bursts facet descriptions etc. also propose approach transfer model learned ground-truth data domain another domain missing ground-truth information. results presented ecml-pkdd models product review communities information user reviewing item explicit timepoint. makes approach fairly generalizable across communities domains limited meta-data requirements. tasks provide user-interpretable explanations form interpretable word clusters representative snippets evolution traces etc. explain end-user model arrived particular verdict. model shows userinterpretable word clusters depicting user maturity give interesting insights. example experienced users beer communities fruity words depict beer taste smell; news communities experienced users talk policies regulations contrast amateurs interested polarizing topics. similarly evolution traces show experienced users progress faster amateurs acquiring maturity also exhibit higher variance. dissertation organized follows. chapter discusses state-of-the-art domain related prior work. chapter lays foundation credibility analysis framework. develops probabilistic graphical models methods joint inference online communities credibility classiﬁcation credibility regression. also presents large-scale experimental studies largest health community sophisticated news community. chapter develops approaches modeling temporal evolution users online communities. presents stochastic models discrete continuous modes experience evolution users collaborative ﬁltering framework. also presents large-scale experimental studies real world datasets like movies beer food news. chapter uses principles methods developed earlier chapters credibility analysis product review communities tasks namely ﬁnding useful product reviews helpful end-consumers communities like amazon detecting non-credible reviews limited information users items communities like yelp tripadvisor amazon. chapter presents conclusions future research directions. chapter presents overview related work several overlapping domains like truth discovery sentiment analysis opinion mining information extraction collaborative ﬁltering online communities. discusses state-of-the-art domains limitations. following sections give brief overview usage probabilistic graphical models related tasks. since full primer pgms beyond scope work refer readers general overview pgms. probabilistic graphical models graph-based representation encode complex highdimensional distributions involving many random variables. provides natural framework model probabilistic interactions them represented edges graph random variables nodes. objective probabilistically reason values subsets random variables possibly given observations others. order need construct joint probability distribution function space possible value assignments random variables. often intractable. practice random variable interacts subset others. allows represent joint distribution product factors composed smaller random variables representing marginals. several advantages. factorization decomposition lead tractable solution even though complete speciﬁcation possible value assignments asymptotically large. secondly easy interpret semantics model output users; highlight interactions factors answer queries interest probabilistic interpretations. thirdly also easy encode expert knowledge framework specifying structure graph terms dependencies priors parameters. typically families pgms bayesian networks directed representation markov networks undirected representation. mrfs model joint probability distribution representing multi-dimensional input representing multi-dimensional output since fully generative used model arbitrary prediction problems. work mostly conditional random fields speciﬁc type mrf. discriminative nature model conditional distribution standard prediction problems accurate settings. also viewed structured extension logistic regression output dependencies them. please refer introduction crfs. probabilistic topic models extend principles pgms discover thematic information unstructured collection documents. latent dirichlet allocation simplest type topic model. assume documents distribution topics topics distribution words. example news article talk sports politics speciﬁc words describe topics. topics known priori treated hidden random variables need inferred data. uses generative process model principles assumptions. refer overview probabilistic topic models. crucial component pgms involve inference algorithms computing marginals conditionals maximum posteriori probabilities efﬁciently answering queries interest. several variants message passing belief propagation algorithms exact inference. however computational complexity often exponential large size cliques long loops arbitrary graph structures. therefore often resort approximate probabilistic inference. large classes inference techniques monte carlo variational algorithms. monte carlo methods algorithms based fact although computing expectation original distribution difﬁcult obtain samples closely related distribution compute sample-based averages. work mostly gibbs sampling metropolis hastings. gibbs sampling type markov chain monte carlo algorithm samples obtained markov chain whose stationary distribution desired collapsed gibbs sampling inference probabilistic topic models. metropolis hastings also type mcmc algorithm. instead sampling true distribution often quite complex uses proposal distribution proportional density true distribution sampling random variables. followed acceptance rejection newly sampled value. iteration algorithm samples value random variable current estimate depends previous estimate thereby forming markov chain. principle advantage monte carlo algorithms easy implement quite general. however difﬁcult guarantee convergence time taken converge quite long. work empirically demonstrate fast convergence certain settings. variational mthods class approximate inference involving variational methods family approximate distributions variational parameters. objective setting parameters make approximate distribution close posterior interest. thereafter approximate distributions ﬁtted parameters used proxy true posterior. approaches truth discovery goal resolve conﬂicts multi-source data input data assumed structured representation entity interest along potential values provided different sources truth discovery methods kind survey) starting seminal work assume claims follow structured template clear identiﬁcation questionable values correspond subject-predicate-object triples obtained information extraction classic example obama born assumption structure crucial order identify alternative values questionable slot appropriate checking facts tasks like knowledge-base curation. alternative values provided many sources. objective resolve conﬂict multi-source data given query obtain truth. assumed conﬂicting values already available. resolve conﬂicts particular entity approaches exploit reliable trustworthy sources often provide correct information. exploit principle works propagate aggregate scores networks objects sources provide information objects. signiﬁcant challenge priori know sources reliable trustworthy need inferred task. uses information-retrieval techniques systematically generate alternative hypotheses given statement assess evidence alternative. however relies user providing doubtful portion input statement making doubtful unit alternative statements generated search ranked identify correct statement. work goes step proposing method generate conﬂicting values fact candidates contents. make linguistic features detect objectivity source reporting fact. note approaches handle input statements alternative facts values given retrieved priori. develop methods statistical reasoning cues statement true false. developed approaches structured data ﬂight times stock quotes different sources often yield contradictory values. addressed truth assessment medical claims diseases treatments ir-style evidence-aggregation ranking method curated health portals. probabilistic graphical models recently presented lda-style latenttopic model discriminating true false claims various ways generating incorrect statements proposed lda-style model capture expertise users different topics. model question content answer quality best candidate answer. proposed latent truth model based generative process types errors modeling different aspects source quality. also propose sampling based algorithm scalable inference. proposed gaussian truth model deal numerical data based generative process. approaches limited resolving conﬂicts amongst multi-source data where input data structured format conﬂicting facts always available. although elaborate models take account language statements reported user postings trustworthiness users making statements. none prior works considered online discussion forums credibility statements intertwined factors. moreover limited availability ground-truth data problem setting models work unsupervised fashion. work propose general approaches require alternative claims. approaches geared online communities rich interactions users postings statements. also models partially weakly supervised well fully supervised depending availability labeled data. moreover provide userinterpretable explanations models’ verdict unlike many previous works. area received much attention mostly motivated analyzing customer reviews product recommendations also context social networks. seminal works modeled propagation trust within network users. trustrank become popular measure trustworthiness based random walks user graph. reputation management also studied context peer-to-peer systems blogosphere online interactions works focused explicit relationships users infer authority trust levels. content-aware model trust propagation work develops hits-style algorithm propagating trust scores heterogeneous network claims sources documents. evidence claim collected related documents using generic ir-style word-level measures. also requires weak supervision evidence level form human judgment trustworthiness articles. however ignores ﬁne-grained interaction users making statements postings evolve time. show factors jointly captured using sophisticated probabilistic graphical models. ample work extracting subject-predicate-object like statements natural-language text. survey gives overview; provide additional references. state-of-the-art methods combine pattern matching extraction rules consistency reasoning. done either shallow manner sequences text tokens combination deep parsing linguistic analysis. resulting triples often highly varying conﬁdence whether really expressed text picked spuriously. judging credibility statements out-of-scope itself. give overview probabilistic graphical models used information extraction. extracting facts diseases symptoms drugs customized techniques developed biomedical publications like pubmed articles. emphasis molecular level i.e. proteins genes regulatory pathways lesser extent biological medical events scientiﬁc articles clinical narratives used ldastyle models summarization drug-experience reports. employed techniques build large knowledge base life science health. recently demonstrated derive insight drug effects query logs search engines. social media played minor role prior work. work sentiment analysis looked language features based phrasal dependency relations narratives perspectives modalities discourse relations lexical resources etc. customer reviews classify sentiment positive negative objective. going beyond special class texts studied biased language wikipedia similar collaborative communities. even broadly task characterizing subjective language addressed among others work explored beneﬁts subjectivity analysis information extraction. opinion mining methods recognizing speaker’s stance online debates proposed structural linguistic features users’ posts harnessed infer stance towards discussion topics temporal textual information exploited stance classiﬁcation sequence tweets several existing works consider textual content user reviews tackling fake reviews using word-level unigrams bigrams features along speciﬁc lexicons psycholinguistic lexicon wordnet affect learn latent topic models classiﬁers works learn linguistic features artiﬁcially created fake review dataset leading biased features dominant real-world data. conﬁrmed study yelp ﬁltered reviews n-gram features used prior works performed poorly despite outstanding performance artiﬁcial datasets. additionally linguistic features text sentiment readability score flesch reading ease etc.) textual coherence rules based probabilistic context free grammar studied. aspect rating prediction received vigorous interest recent times. shallow dependency parser used learn product aspects aspect-speciﬁc opinions jointly considering aspect frequency consumers’ opinions aspect. presents approach capture user-speciﬁc aspect preferences requires manual speciﬁcation ﬁxed aspects learn from. jointly learns ranking models individual aspects modeling dependencies assigned ranks analyzing meta-relations opinions agreement contrast. review. however model ignores user identity writing style learns parameters review. rated aspect summary short comments done similar laram statistics aggregated comment-level. topic model used assign words induced topics. model extended maximum entropy classiﬁers rated aspect used predict aspect speciﬁc ratings. joint sentiment topic model described detects sentiment topic simultaneously text. document sentiment label distribution. topics associated sentiment labels words associated topics sentiment labels. contrast similar works require kind supervised setting like ratings aspects overrating fully unsupervised. cfacts model extends model capture facet coherence review using hidden markov model. extended capture author preferences writing style completely unsupervised. generative models root latent dirichlet allocation model assumes document probability distribution mixture topics topics probability distribution words. topic-syntax model document distribution topics; topic distribution words drawn classes whose transition follows distribution markov dependency. author-topic model author associated multinomial distribution topics. topic assumed multinomial distribution words. however models exception geared credibility analysis consider users writing reviews preferences different topics experience writing style. models capture usercentric factors well interactions capture credibility user-contributed content online communities. prior research credibility assessment social media posts exploits community-speciﬁc features detecting rumors fake deceptive content temporal structural linguistic features used detect rumors twitter addresses problem detecting fake images twitter based inﬂuence patterns social reputation. study wikipedia hoaxes done propose model determine whether wikipedia article hoax measuring long survive debunked many page-views receive heavily referred documents compared legitimate articles. analyzes micro-blog postings twitter related trending topics classiﬁes credible based features user posting re-posting behavior. focuses credibility users harnessing dynamics information underlying social graph tweet content. analyzes topical content information sources social network structure credible information sources social networks. information credibility tweets studied conducts user study analyze various factors like contrasting viewpoints expertise affecting truthfulness controversial claims. approaches geared speciﬁc forums making several community-speciﬁc characteristics cannot generalized across domains communities. moreover none prior works analyze joint interplay sources language topics users inﬂuence credibility information online communities. inﬂuence different kinds bias online user ratings studied proposes approach handle users might subjectively different strategically dishonest. absence proper ground-truth data prior works make strong assumptions e.g. duplicates near-duplicates fake make extensive background information like brand name item description user history addresses location etc. thereafter regression models trained features used classify reviews credible deceptive. works also crude ad-hoc language features like content similarity presence literals numerals capitalization. contrast works approach uses limited information users items available long-tail users items community catering wide range applications. harvest several semantic consistency features information user reviewing item explicit timepoint also give user-interpretable explanation user posting deemed non-credible. deﬁnes citizen journalism citizen group citizens playing active role process collecting reporting analyzing dissemination news information provide independent reliable accurate wide-ranging relevant information democracy requires. focuses user activities like blogging community news websites. although potential citizen journalism greatly highlighted recent arab spring misinformation quite dangerous relying users news sources state-of-the-art recommenders based collaborative ﬁltering exploit user-user item-item similarities latent factors. temporal aspects leading bursts item popularity bias ratings evolution entire community whole studied papers studied temporal issues anomaly detection detecting changes social neighborhood linguistic norms however none prior work considered evolving experience behavior individual users. modeled studied inﬂuence evolving user experience rating behavior targeted recommendations. however disregards vocabulary writing style users reviews. contrast work considers review texts additional insight facet preferences experience progression. address limitations means language models speciﬁc experience level individual user modeling transitions experience levels users hidden markov model. even models limited discrete experience levels leading abrupt changes experience language model users. address this related drawbacks propose continuous-time models smooth evolution user experience corresponding language models. probabilistic graphical models sentiment analysis reviews aimed learn latent topics latent aspects ratings using topic models user-user interactions using markov random fields. uniﬁed various approaches generate user-speciﬁc ratings reviews. leveraged author writing style. however approaches operate static snapshot-oriented manner without considering time all. modeling perspective approaches learn document-speciﬁc discrete rating whereas others learn facet weights outside topic model order incorporate continuous ratings proposed complex computationally expensive variational inference algorithm developed simpler approach using multinomial-dirichlet regression. latter inspired technique incorporating supervision discreteversion experience model. modeled topics time. however topics constant time used better discover them. dynamic topic models introduced prior work developed generic models based brownian motion applied news corpora. argues continuous model avoids making choices discretization also tractable compared ﬁne-grained discretization. language model motivated latter. substantially extend capture evolving user behavior experience review communities using geometric brownian motion. prior works predicting review helpfulness exploit shallow syntactic features classify extremely opinionated reviews helpful. similar features also used ﬁnding review spams similarly approaches utilize features like frequency user posts average ratings users items distinguish helpful unhelpful reviews. community-speciﬁc features explicit user network used however shallow features analyze review about therefore cannot explain helpful given product. approaches proposed also utilize item-speciﬁc meta-data like explicit item facets product brands decide helpfulness review. however approaches heavily rely large number meta-features make less generalizable. related approaches also identify expertise review’s author important feature. however explicitly model user expertise. approach ﬁnding expert users community using experience-aware collaborative ﬁltering models leverage distributional similarity semantics consistency expert-contributed reviews identify useful product reviews. online social media includes wealth topic-speciﬁc communities discussion forums politics music health many domains. user-contributed contents communities offer great potential distilling analyzing facts opinions. instance online health communities constitute important source information patients doctors alike adult population consulting online health resources nearly half physicians relying online resources professional major hurdles preventing full exploitation information online communities widespread concern regarding quality credibility user-contributed content information obtainable form noisy subjective personal bias perspectives injected users postings. state-of-the-art limitations although information extraction methods using probabilistic graphical models previously employed extract statements user generated content account inherent bias subjectivity misinformation prevalent online communities. unlike standard information extraction techniques method considers role language assessing credibility extracted statements. instance stylistic features modals inferential conjunctions help identify accurate statements affective features help determine emotional state user making statements prior works truth discovery fact ﬁnding survey) make strong assumptions nature structure data e.g. factual claims structured input form subject-predicate-object triples like obama_bornin_kenya relational tables approaches also consider address issues propose probabilistic graphical models automatically assess credibility statements made users online communities analyzing joint interplay several factors like community interactions language postings trustworthiness users etc. model settings features inference generic enough applicable online community; however use-case studies validating framework focus disparate communities namely health news. unlike healthforums focusing mostly drugs side-effects latter community highly heterogeneous covering topics ranging sports politics environment current affairs thereby testing generalizability framework. ﬁrst use-case consider healthforums healthboards.com patient.co.uk patients engage discussions experience medical drugs therapies including negative side-effects drugs drug combinations. user-contributed postings focus extracting rare unknown side-effects drugs problems large scale non-expert data potential complement expert medical knowledge misinformation hazardous consequences main intuition behind proposed model important interaction credibility statement trustworthiness user making statement language used posting containing statement. therefore consider mutual interaction following factors language objectivity rationality general quality language users’ postings. objectivity quality posting free preference emotion bias prejudice author. factors strong inﬂuence other. intuitively statement credible posted trustworthy user expressed using conﬁdent objective language. example consider following review drug depo-provera senior member healthboards.com largest online health communities posting contains credible statement potential side-effect depo-provera reduce bone density. conversely highly subjective emotional language suggests lower credibility user’s statements. negative example along lines example iii.. cocktail meds individual users postings statements nodes summarized figure iii.. quality nodes—trustworthiness objectivity credibility—is modeled binary random variables. model semi-supervised subset training statements derived expert medical databases labeled true false. addition model relies linguistic user features directly observed online communities. inference parameter estimation done framework mcmc sampling used e-step estimating label unknown statements trust region newton method used m-step compute feature weights. second use-case consider role media public dissemination information events. many people online information blogs useful magazines. time however people also believe substantial media bias news coverage especially view inter-dependencies cross-ownerships media companies industries several factors affect coverage presentation news media incorporating potentially biased information induced fairness style reporting. news often presented polarized depending political viewpoint media source addition source-speciﬁc properties like viewpoint expertise format news also indicators information credibility. use-case embark in-depth study formal modeling factors inter-dependencies within news communities credibility analysis. news community news aggregator site users give explicit feedback quality news interact other. users rate review news point differences bias perspectives unveriﬁed claims etc. however adds user subjectivity evaluation process users incorporate bias perspectives framework. controversial topics create polarization among users inﬂuence ratings. state online ratings trusted sources user feedback; however systematically biased easily manipulated. approach unlike healthforums focusing single topic news communities heterogeneous nature discussing topics ranging sports politics environment food movies restaurants etc. therefore propose general framework analyze factors inter-dependencies heterogeneous community; speciﬁcally additional factors sources topics well allowing inter user inter source interactions. develop sophisticated probabilistic graphical model regression assign credibility rating postings opposed binary classiﬁcation; speciﬁcally develop continuous conditional random field model exploits several moderate signals interaction jointly following factors derive strong signal information credibility particular model captures following factors. language credibility posting objectivity rationality general quality language posting. objectivity quality news free emotion bias prejudice author. credibility posting refers presenting unbiased informative balanced narrative event. properties trustworthiness source trustworthiness source sense generating credible postings based source properties like viewpoint expertise format news. expertise users review ratings expertise user community properly judging credibility postings. expert users provide objective evaluations form reviews ratings postings corroborating evaluations expert users. used identify potential citizen journalists community. show ccrf performs better sophisticated collaborative ﬁltering approaches based latent factor models regression methods consider interactions. presents consolidate view item therefore similar ensemble learning learning rank based approaches improve methods explicitly considering interaction participating factors. work attributes credibility trustworthiness always associated posting source respectively. joint interaction several factors also captures source garners trustworthiness generating credible postings highly rated expert users. similarly likelihood posting credible increases generated trustworthy source. communities offer users ﬁne-grained scales rating different aspects postings sources. example newstrust.net community analyzes posting aspects like insightful fairness style factual. aggregated overall real-valued rating weighing aspects based importance expertise user feedback community more. setting cannot easily discretized without blow-up risking lose information. therefore model ratings real-valued variables ccrf. model proposes probabilistic graphical models capture mutual interactions dependencies trustworthiness sources credibility postings statements objectivity language expertise users online communities devises comprehensive feature method introduces methods joint inference users sources language postings statements probabilistic graphical models credibility classiﬁcation credibility regression large-scale experimental study largest online health community healthboards.com where apply method million postings contributed users extracting side-effects medical drugs usercontributed posts use-cases evaluates performance models context practical tasks like discovering rare side-effects drugs identifying trustworthy users health community; ﬁnding trustworthy sources expert users news community play role citizen journalists. given users sources generating postings users reviewing postings mutual interactions factors several features objective jointly identify trustworthy sources credible postings statements expert users classiﬁcation regression tasks. process want analyze inﬂuence various factors like writing style posting topic distribution viewpoint expertise users sources credibility analysis. approach leverages intuition important interaction statement credibility linguistic objectivity user trustworthiness. therefore model factors jointly probabilistic graphical model speciﬁcally markov random field statement posting user associated binary random variable. figure iii. provides overview model. given statement corresponding variable value statement credible otherwise. likewise values posting user variables reﬂect objectivity trustworthiness postings users. nodes features labels nodes associated users postings observable features extracted online community. users derive engagement features interaction features demographic information postings extract linguistic features form discourse markers affective phrases. features presented details section iii.. statements observable features derive distant training labels subset statements expert databases like mayo clinic lists typical well rare side-effects widely used drugs. edges primary goal proposed system retrieve credibility label unobserved statements given expert labeled statements observed features leveraging mutual inﬂuence model’s variables. mrf’s nodes connected following edges conﬁgured model capacity capture important interactions statements postings users example credible statements boost user’s trustworthiness whereas false statements bring down. furthermore since inference centered around cliques graph multiple cliques share nodes complex cross-talk also captured. instance several highly trustworthy users agree statement user disagrees reduces trustworthiness disagreeing user. addition classifying statements credibility proposed system also computes individual likelihoods by-product inference process therefore output rankings statements users postings descending order credibility trustworthiness objectivity. earlier model used classifying statements credible not. however many scenarios ﬁne-grained credibility analysis want assign real-valued credibility rating posting. additionally want address several drawbacks earlier model propose general framework models topics users sources explicit interactions prevalent online community. refer figure iii. following discussion. consider sources generating postings reviewed analyzed users credibility. consider review user posting overall credibility rating posting given worthiness objectivity expertise credibility respectively. indicates best quality item obtain worst. discrete ratings special case setting easily handled. node associated observed features extracted community. example source properties like topic speciﬁc expertise viewpoint format news; posting features like topics style writing usage discourse markers subjective words posting. users extract topical perspectives expertise engagement features various interactions users sources community. posting connected source extracted posting connected review rating user user connected reviews user connected postings rated source connected users rated postings source connected reviews postings topics play signiﬁcant role information credibility. individual users community perspectives expertise various topics modeling user-speciﬁc topical perspectives explicitly captures credibility judgment better user-independent model. however many postings explicit topic tags. hence latent dirichlet allocation conjunction support vector regression learn words associated topic user perspectives topics. documents assumed distribution topics latent variables words observables. inference gibbs sampling. model component overall model discussed next. probabilistic graphical model speciﬁcally conditional random field model factors jointly. modeling approach related model discussed previous section iii... however unlike model traditional models problem setting requires continuous version deal real-valued ratings instead instance graph figure iii.c. captures cross-talk different cliques sharing nodes. source garners trustworthiness generating multiple credible postings. users attain expertise correctly identifying credible postings corroborate expert users. inability brings expertise. similarly posting attains credibility generated trustworthy source highly rated expert user. inference algorithm ccrf discussed detail section iii... section outline different components features used probabilistic models credibility analysis focus health news communities. features extracted postings users online communities interactions users sources. since features fairly generic community-speciﬁc easily applicable communities like travel food electronics. style post written plays pivotal role understanding credibility. desired property posting objective unbiased. model stylistic affective features assess posting’s objectivity quality. example iii.. heard xanax pretty side-effects. peeling skin apparently friend mine told develop ulcers lips also. take medicine long time would probably develop physical problems. experience posting evokes uncertainty speciﬁcally point occurrence side effect ﬁrst-hand experience. note usage strong modals would indeﬁnite determiner some conditional adverb possibility probably question particle which. even usage first person second person third person question particles what when adjectives correct extreme visible maybe about probably adverbs proper nouns xanax zoloft depo example iii.. depo dangerous birth control many long term sideeffects like reducing bone density. hence never recommend anyone using birth control. women tolerate well minority. women horrible long lasting side-effects posting uses inferential conjunction hence draw conclusions previous argument deﬁnite determiners this those most pinpoint entities highly certain weak modal will. table iii. shows linguistic features deem suitable discriminating between kinds postings. many features related epistemic modality discussed prior linguistic literature features related discourse coherence also employed earlier computational work user affective state depicts attitude emotions reﬂected postings. note user’s affective state change time; property postings users example consider following posting health community example iii.. i’ve chronic depression since adolescence. past i’ve taken paxil zoloft taking john’s wort months helps enough. wake almost every morning feeling hopeless. afternoon approaches start feel better there’s almost always least level depression throughout day. affection antipathy anxiousness approval compunction conﬁdence contentment coolness creeps depression devotion distress downheartedness eagerness edginess embarrassment encouragement favor fondness guilt harassment humility hysteria ingratitude insecurity jitteriness levity levitygaiety malice misery resignation selfesteem stupefaction surprise sympathy togetherness triumph weight wonder example iii.. diagnosis made suffer excessive anxiety worry least three symptoms including...if symptoms above touch chord speak effective treatments cognitive behavioural therapy particular help wordnet-affect lexicon word sense mapped attributes affective feature space like confusion ambiguity hope anticipation hate. perform word sense disambiguation instead simply take common sense word table iii. shows sample affective features used work. posting supposed objective writers convey opinions feelings prejudices postings. example posting titled conservatives hate children? considered objective journalism news community. following linguistic cues detecting bias subjectivity user-written postings. subset features earlier used apologetic summer advance cornerstone hypocricy swindle unacceptable worse steadiest prominence lucky better heckle grisly defeat peevish disgust anxious revolt guilt conﬁdent discourse markers capture degree conﬁdence perspective certainty propositions made. instance strong modals probabilistic adverbs conditionals depict high degree uncertainty hypothetical situations whereas weak modals inferential conjunctions depict certainty. additionally harness lexicon bias-inducing words extracted wikipedia edit history exploiting neutral point view policy keep postings fairly proportionately possible without bias signiﬁcant views published reliable sources topic. since model allows users interact users give feedback postings also create feature vectors users’ reviews capture whether feedbacks credible biased users’ judgment. consider review written user posting review analogous per-posting stylistic http//mpqa.cs.pitt.edu/lexicons/subj_lexicon/ http//www.cs.uic.edu/ liub/fbs/opinion-lexicon-english.rar http//wndomains.fbk.eu/wnaffect.html community engagement user obvious measure judging user authority community. capture different features number answers ratings given comments ratings received disagreement number raters. case user demography information like gender location etc. available also incorporate features. inter-user agreement expert users typically agree constitutes credible posting. inherently captured proposed graphical model user gains expertise assigning credibility ratings postings corroborate expert users. topical perspective expertise potential harvesting user preference expertise topics rating prediction reviews demonstrated credibility analysis model needs capture user’s perspective bias towards certain topics based political inclination bias ratings topic-speciﬁc expertise allows evaluate postings certain topics better subject matter experts. captured per-user feature weights stylistic indicators topic words language user-contributed reviews. interactions community users upvote ratings users appreciate downvote ones agree with. high review ratings expert users increase value user; whereas ratings bring expertise. similar user-user interaction user-posting user-source source-posting interactions captured edges graphical model consider following anecdotal example community showing expert nuclear energy downvoting another user’s rating nuclear radiation example iii.. non-expert interesting opinion health risks nuclear radiation physicist oxford university. makes reasonable points rating expert review fair assume background biology anything medical? story deﬁnitely important contains enough inaccurate and/or misleading statements... topic tags postings play important role user-perceived prominence bias credibility accordance prominence-interpretation theory example politics often viewed indicator potential bias individual differences; whereas tags like energy environment perceived neutral postings therefore invoke higher agreement community associated postings’ credibility. obviously misleading signiﬁcant inﬂuence politics topics. certain users topic-speciﬁc expertise make judge postings topics better others. sources also expertise speciﬁc topics provide better coverage postings topics others. example national geographic provides good coverage postings related environment whereas wall street journal provides good coverage economic policies. however postings explicit topic tag. order automatically identify underlying theme posting latent dirichlet allocation learn latent topic distribution corpus. assumes document distribution topics topic distribution words. table iii. shows excerpt topic words topic manually added illustrative labels topics. latent topics also capture subtle themes detected explicit tags. example goodman american broadcast journalist syndicated columnist investigative reporter considered highly credible community. also associated topic cluster amanda blackhorse navajo activist plaintiff washington redskins case. source considered trustworthy generates highly credible postings. examine effect different features source trustworthiness based user assigned ratings obama republican party election president senate vote jouralism writers cjrs marx hutchins reporting liberty guides iraq military iran china nuclear obama russia weapons democracy military civil activist protests killing navajo amanda media politics world news activism world civil visions economy energy climate power water change global nuclear fuel warming newspaper blog radio magazine online editorial investigative report news research local state regional national international left left center right neutral politics weather science„ u.s. military u.s. congress middle east crime presidential election bush administration global warming community. consider following source features news community type media format postings viewpoint scope topics covered source topic-speciﬁc expertise. feature vector construction source create feature vector using features table iii.. element indicating presence absence feature. note features include topics covered source topic-speciﬁc expertise subset topics. given users contributing postings containing dubious statements ﬁrst task want classify statements credible not. instance users health community write postings experience drugs side-effects want extract credible side-effects given drug; sources generate postings containing dubious claims whereby interested claims authentic hoaxes. model semi-supervised harness ground-truth labels subset statements derived expert knowledge-bases. statements labeled expert true false unlabeled statements. goal infer labels statements cliques triangles consisting statement posting contains statement user wrote post. statement made different postings users cliques statements. convenient notation denote statement instances correspond cliques statements repeated necessary. potential function clique clique associated feature functions weight vector denote individual features weights features constituted stylistic affective user features explained instead computing joint probability distribution like standard adopt paradigm conditional random fields settle simpler task estimating conditional distribution different ways addressing optimization problem ﬁnding argmax work choose expectation-maximization approach ﬁrst estimate labels variables posterior distribution using gibbs sampling maximize log-likelihood estimate feature weights m-step equation iii.b l-regularized trust region newton method suited large-scale unconstrained optimization many feature values zero. implementation liblinear approach captures user trustworthiness implicitly weights feature vectors. however want model user trustworthiness explicitly aggregates statements made user. denote trustworthiness user measured fraction statements considered true previous iteration therefore random variable trustworthiness depends proportion true statements made user. label statement turn determined language objectivity postings trustworthiness users community make statement. previous section discussed approach classifying statements credible not. however many scenarios want perform ﬁne-grained analysis. communities offer users ﬁne-grained scales rating different aspects item aggregated overall real-valued rating weighing aspects based importance expertise user feedback community more. setting cannot easily discretized without blowup risking lose information. therefore task want perform regression ﬁne-grained credibility analysis whereby want assign real-valued credibility rating posting. also address earlier drawbacks model whereby model users sources separate factors taking consideration inter user inter source interactions well inﬂuence topics discussions. consider sources generating postings users providing feedback postings mutual interactions objective identify credible postings trustworthy sources expert users jointly community incorporating discussed features insights consider posting consisting sequence words denoted w...wnd word drawn vocabulary unique words indexed ...v consider topic assignments z...zk topic possible assumes document associated multinomial distribution topics symmetric dirichlet prior denotes probability occurrence topic document topics multinomial distribution words drawn vocabulary symmetric dirichlet prior denotes probability word belonging topic exact inference possible intractable coupling gibbs sampling approximate inference. denote count word occurring document belonging topic following equation position count indicates marginalization i.e. summing counts values corresponding position conditional distribution latent variable given feature vector occurrence count document. word belongs latent topic probability greater threshold probability word belonging topic added corresponding element feature vector otherwise. support vector regression combine different features discussed section iii.. extension max-margin framework classiﬁcation regression problem. solves following optimization problem learn weights features note overall credibility rating posting train posting stylistic topic models. user model however take user assigned credibility ratings postings per-user features. model captures user subjectivity topic perspective. source models trained source speciﬁc meta-data ground-truth ratings. ratings postings sources 〈si〉 users 〈uk〉 reviews jk〉. objective predict credibility ratings postings cliques consist posting source users 〈uk〉 reviewing corresponding user reviews denotes review user posting associated vertex feature functions. problem setting associate features vertex. features constituted stylistic topic source user features explained problem setting want estimate credibility rating posting. therefore need estimate conditional distribution probability density function continuous random variable pendently predict rating notational brevity hereafter drop argument function. predictors separate feature groups independent other. combine different models capture mutual interactions weight model reﬂects conﬁdence quality. errors penalized squared loss predicted credibility rating posting ground-truth rating. additional constraint clique regression models corresponding source users present activated. thought partitioning input feature space subsets features inside clique capturing local interactions global weights capture overall quality random variables shared information cliques ideal setting using crf. equation iii. shows linear combination. energy function individual clique given |u|+|s|+ linear combination features depict much trust individual predictors. large particular predictor places large penalty mistakes committed therefore depicts higher quality predictor. corresponding user taken proxy user’s expertise allowing obtain ranked list expert users. similarly corresponding source taken proxy source’s trustworthiness allowing obtain ranked list trustworthy sources. represents contribution covariance matrix vector matrix corresponds training instance representing active contribution features present ensure equation iii. represents valid gaussian distribution covariance matrix needs positive deﬁnite inverse exist. diagonal matrix needs positive semi-deﬁnite matrix. ensured making diagonal since constrained optimization problem gradient ascent cannot directly used. follow approach similar maximize log-likelihood respect instead standard gradient ascent making optimization problem unconstrained model parameters learned using gradient ascent inference prediction credibility rating posting straightforward. assume distribution gaussian prediction expected value function given mean section apply predictive power probabilistic model classiﬁcation problem extracting credible side-effects medical drugs user-contributed postings online healthforums. data healthboards.com largest online health communities registered members million posted messages. sampled users based posting frequency postings million postings total experimentation. table iii. shows user categorization terms community engagement. employ tool extract side-effect statements postings. generates tens thousands triple patters although handful credible ones. details experimental setting available website. ground truth drug side-effects rely data mayo clinic portal contains curated expert information drugs side-effects listed common less common rare drug. extracted drugs categorized drug families. experiments select widely used drug families table iii. provides information sample coverage healthboards.com. table iii. shows number common less common rare side-effects drug families given mayo clinic portal. baseline drug possible side-effect determine postings mentioned aggregate features described section iii. postings thus creating single feature vector side-effect. ground-truth labels mayo clinic portal train support vector machine classiﬁer linear kernel loss regularization classifying unlabeled statements. baseline distant supervision number common side-effects drug typically small approach create single feature vector side-effect results small training set. hence notion distant supervision create rich expanded training set. cliques equation iii.. semi-supervised formulation approach allows information sharing cliques estimate labels unobserved statements expert-provided ones. process creates noisy training posting contain multiple side-effects positive negative. results multiple similar feature vectors different labels. testing side-effect different labels different instances. take majority voting labels obtained side-effect across predictions different instances assign unique label experimental setting consider common side-effects listed mayo clinic portal positive ground-truth whereas side-effects considered negative instances training constructed way. setting aims study predictive power model determining common side-effects drug comparison baselines. experimental setting address original motivation discovering less common rare side-effects. durring training positive ground-truth consider common less common side-effects whereas rare unobserved side-effects considered negative instances. goal test well model identify less known rare side-effects true statements. purposely consider rare side-effects positive training examples since evaluate model’s ability retrieve statements starting reliable positive instances. measure performance rare side-effects recall statements labeled true statements spite considering common less common side-effects positive instances durring training. train-test data split drug family create multiple random splits training data test data. results reported averaged splits. baselines model test sets. evaluation metrics standard measure quality binary classiﬁer accuracy n+tn+f also report speciﬁcity sensitivity measures true positive rate model’s ability identify positive side-effects whereas speciﬁcity measures true negative rate. table iii. shows accuracy comparison system baselines different drug families ﬁrst setting. ﬁrst naive baseline simply considers frequency postings containing side-effect different users average accuracy across different drug families. incorporating supervision classiﬁer ﬁrst baseline along rich features users postings language achieves average accuracy improvement second baseline represent posting reporting side-effect separate feature vector. expands training leading better parameter estimation also represents cliques equation iii. brings average accuracy improvement using regularization using regularization. model considering coupling users postings statements allows information cliques feedback loop bringing accuracy improvement strong baseline. speciﬁcity increase baseline maximum alprazolam drug family followed levothyroxine users taking anti-depressants like alprazolam suffer anxiety disorder panic attacks depression etc. report large number side-effects drugs. hence difﬁcult negate certain side-effects model performs well well-designed language features. also alprazolam levothyroxine large number expert-reported side-effects corresponding user-reported ones model learns well negative class. drugs metronidazole metformin omeprazole treat serious physical conditions less number expert user-reported side-effects. consequently model captures user statement corroboration well attain sensitivity improvement .%.% respectively. overall classiﬁer performs best drug categories. table iii. shows overall model performance well recall identifying rare side-effects drug second setting. drugs metformin levothyroxine omeprazole much less number side-effects classiﬁer almost perfect drugs setting however classiﬁer accuracy signiﬁcantly drops anti-depressants introduction less common side-effects positive statements setting performance drop attributed loss speciﬁcity increase number false-positives conﬂict model learns language features introduced groundtruth. feature informativeness order predictive power individual feature classes tests perfomed using l-loss l-regularized support vector machines split test data. affective features found informative followed document length statistics informative user stylistic features. importance document length distribution strengthens observation objective postings tend crisp whereas longer ones often indulge emotional digression. amongst user features signiﬁcant ratio number replies user questions posted community followed gender number postings user ﬁnally number thanks received fellow users. gender-bias community active contributors health forum female. section iii.. focused evaluating predictive power model inference method. shift focus application-oriented use-cases discovering sideeffects drugs covered expert databases identifying trustworthy users would want follow certain topics. members online community report side-effects either ﬂagged rare expert knowledge base listed all. call latter out-of-kb statements. before data mayoclinic.org focus following drugs representing different kinds medical conditions patient-reporting styles alprazolam levothyroxine. drugs perform experiment follows. drug machinery identify side-effects reported regardless whether listed not. method uses side-effects listed drug potential result. example hallucination listed drug drug xanax capture mentions hallucination postings xanax. probabilistic model compute credibility scores out-of-kb side-effects compile ranked list highest-scoring side-effects drug. ranked list extended randomly chosen out-of-kb side-effects ranked list out-of-kb side-effects shown expert annotators manually assess credibility reading complete discussion thread threads involve users reported side-effect. assessment binary true false choose ﬁnal label majority judges. compute quality ranked list terms ndcg position penalizes relevant items appearing lower rank list graded relevance score reduced logarithmically proportional position result. length lists vary different queries scores normalized using ideal score idcg results rank list sorted relevance giving maximum possible score. also report inter-annotator agreement using cohen’s kappa measure. table iii. shows kappa ndcg score comparison baseline model. baseline rank side-effects frequency i.e. often reported postings different users given drug. strength kappa considered moderate depicts difﬁculty identifying side-effects drug looking user postings community. baseline performs poorly anti-depressant alprazolam users suffering anxiety disorders report large number side-effects credible. hand levothyroxine baseline model performs quite well users report serious symptoms conditions associated drug also much less expert-stated side-effects compared alprazolam model performs perfectly drugs. worthiness scores drugs alprazolam levothyroxine. baseline model consider top-thanked contributors community. moderators facilitators community listed models users removed ranked lists order focus interesting obvious cases. judges asked annotate top-ranked users listed model trustworthy based users’ postings target drug. judges asked mark user trustworthy would consider following user community. although exercise seem highly subjective cohen’s kappa scores show high inter-annotator agreement. strength agreement considered very good user postings levothyroxine good alprazolam users. section present ﬁrst full-ﬂedged analysis credibility trust expertise news communities; data newstrust.net sophisticated news communities focus quality journalism. performed experiments data typical news community newstrust.net. community similar digg.com reddit.com reﬁned ratings interactions. chose newstrust availability ground-truth ratings credibility analysis news articles ground-truth available communities. collected stories newstrust diverse topics ranging sports politics environment current affairs. story features news article source posted member reviewed members community many professional journalists content experts. crawled stories explicit topic tags associated meta-data. crawled news articles original sources featured newstrust story. earliest story dates back latest collected member proﬁles containing information demographics occupation expertise members along activity community terms postings reviews ratings; well interaction members. members community also rate others’ ratings. earliest story rating member dates back recent addition collected information member evaluation news sources information source meta data. crawled dataset table iii. shows dataset statistics. total unique news articles reviewed newstrust given period able extract full articles original sources like york times truthdig scientiﬁcamerican total distinct sources. remaining articles available crawling. stories featured newstrust articles stories refer news articles managed extract original sources. average number reviews story general analysis entire dataset. ground-truth evaluation members community rate credibility news article scale regarding qualitative aspects like facts fairness writing style insight popularity aspects like recommendation credibility views. members give overall recommendation article explained quality journalism? would recommend story friend colleague? question similar arrows popular social news sites like digg reddit focus quality journalism.\" article’s aspect ratings different members weighted newstrust based ﬁndings member expertise member level overall article rating taken ground-truth article credibility rating work. user’s member level calculated newstrust based community engagement experience users’ feedback ratings proﬁle transparency validation newstrust staff. member level taken proxy user expertise work. members rate news sources reviewing article. ratings aggregated source taken proxy source trustworthiness work. training data perform -fold cross-validation news articles. training -folds data algorithm learns user source language topic models user-assigned ratings articles sources present train split. combine sources less articles users less reviews background models sources users respectively. avoid modeling sparse observations reduce dimensionality feature space. however testing remaining blind -fold sources users reviewing article; user-assigned ratings sources articles. user source draw parameters user source background model. results averaged -fold cross-validation presented next section. table iii. comparison models predicting users’ credibility rating behavior -fold cross-validation. improvements statistically signiﬁcant p-value experimental settings ﬁrst experiments want power ccrf predicting user rating behavior credibility rating articles. therefore evaluation measure taken mean squared error prediction actual ground-rating community. latter experiments ﬁnding expert users absolute measure predicting user quality; makes sense relative ranking users terms expertise therefore evaluation measure taken normalized discounted cumulative gain ranked list users obtained ccrf actual ranking community. first evaluate good model predict credibility ratings users assign news articles using mean squared error prediction actual user-assigned rating. minimizing rating product user-item latent factors. setting news article considered item rating refers credibility rating assigned user article. experience-based model incorporates experience user rating item lfm. model builds hypothesis users similar levels experience similar rating behaviors evolve time. model extra dimension time rating item used model. note analogy experience user model notion user expertise model. however models ignore text reviews. text-based model incorporates text combining latent factors associated items latent topics text topic models like lda. table iii. comparison models predicting aggregated article credibility rating -fold cross-validation. improvements statistically signiﬁcant p-value incorporate article language features topic features well source-speciﬁc features train user model task. models ignore stylistic features ﬁne-grained user-item interactions community. second part evaluation investigate predictive power different models order credible news articles based aggregated ratings users. models unaware user cliques cannot used directly task news article multiple reviews different users need aggregated. mean squared error estimated overall article rating ground-truth article rating. consider stories least ratings news article. compare ccrf following baselines support vector regression consider model features language topics news-source-speciﬁc features. language model uses lexicons linguistic features discussed chapter iii... source model also includes topic features terms topics covered source topic-speciﬁc expertise subset topics. comparison ﬁrst models table iii. ignore textual content news articles reviews perform worse ones incorporate full text. textbased considers title text performs better predecessors. however user model considers richer features interactions attains reduction best performing baselines. baselines table iii. show model performance incorporating different features different settings news article titles text titles ﬁrst paragraphs article. language model especially bias subjectivity features less effective using article titles sparseness. hand using entire article text lead noisy features. including ﬁrst paragraphs article sweet spot. this made ad-hoc decision included ﬁrst characters article. setting language features made substantial contribution reducing mse. aggregated model brings user features achieves lowest among baselines. shows user-aware credibility model performs better user-independent ones. ccrf model combines features sophisticated manner results reduction competitive baseline empirical evidence joint interactions different factors news community indeed important consider identifying highly credible articles. graph. experience-based consider sources users articles generated items. allows obtain ranking sources based overall authority. second baseline compare ccrf. measure quality ranked lists terms ndcg using actual ranking news sources community ground-truth. ndcg gives geometrically decreasing weights predictions various positions ranked list baseline average rating received user members community. compute ndcg score ranked lists users method. also compare ranked list users experience-aware table iii. shows ndcg scores different methods. hypothesis testing test various hypotheses inﬂuence feature groups using explicit labels ratings available newstrust community. summary tests presented table iii. showing moderate correlation various factors together ccrf strong indicator information credibility. language stylistic features table iii.) like assertives hedges implicatives factives discourse affective play signiﬁcant role credibility analysis conjunction language features like topics. topics topics important indicator credibility. measured inﬂuence politics topics co-occurrence frequency explicit sets postings. found signiﬁcant inﬂuence politics topics average measure association topic overall posting. community gets polarized different perspectives topical aspects news. moderate correlation table iii.) indicates weak trend disagreement measured standard deviation credibility rating users increasing political content. general community disagreement different viewpoints follows right left center neutral users user engagement features strong indicators expertise. although credibility ultimately subjective experts show moderate agreement table iii.) highly credible postings. moderate correlation table iii.) feedback received user ratings community expertise. sources various traits source like viewpoint format topic expertise strong indicators trustworthiness. general science technology websites investigative reporting nonpartisan sources book sites encyclopedia fact checking sites rank among trusted sources. table iii. shows least trusted sources four sample topics. overall sources considered trustworthy average rating variance tables iii. iii. show least trusted sources different viewpoints media types respectively. contents blogs likely posted followed newspaper magazine online sources. contents wire service radio deemed trustworthy although least subscription followed magazines. interactions principle moderate correlation trustworthy sources generating credible postings table iii.) identiﬁed expert users table iii.). negative sign correlation indicates decrease disagreement increase expertise. community observe moderate signals interaction various factors characterize users postings sources. ccrf model brings features together build strong signal credibility analysis. chapter proposed framework credibility analysis postings generated users sources online communities analyzed effect different factors like writing style topics perspectives users sources ascertaining credibility postings. factors mutual interactions features probabilistic graphical models speciﬁcally semi-supervised conditional random field credibility classiﬁcation continuous conditional random field credibility regression jointly capturing credibility postings trustworthiness sources expertise users. application perspective demonstrated method reliably identify credible postings trustworthy sources expert users online communities. novel usecase study healthforums show approach effective reliably extracting side-effects drugs ﬁltering false information prevalent healthforums. designed user study identify rare side-effects drugs scenario large-scale nonexpert data potential complement expert knowledge identify trustworthy users community would want follow certain topics. healthforum setting believe model strong asset possible in-depth analysis like determining speciﬁc conditions side-effects observed. another use-case study presented ﬁrst full-ﬂedged analysis credibility trust expertise news communities model identiﬁed expert users perform role citizen journalists. proposed model also used tasks like crowdsourcing aggregation ensemble learning learning rank where need aggregate information multiple sources taking account mutual interactions weighing source reliablility given task. chapter demonstrated importance modeling trustworthiness expertise users sources credibility analysis online communities. intuitively postings users sources experts given topic reliable amateur users. instance wall street journal national geographic authoritative sources postings related economic policies environmental matters respectively. similarly experienced members health news communities proxy medical experts citizen journalists respective communities contributing credible information. chapter study temporal evolution users’ experience collaborative ﬁltering framework review communities recommend items users based level maturity experience consume them. later propose approach exploit notion evolving user experience extract credible helpful postings online review communities. simplistic mapping task item recommendation previous discussions credibility analysis chapter following. consider side-effects drugs health communities postings sources news communities items collaborative ﬁltering framework users write reviews assign ratings items different timepoints given setting objective retrieve top-ranked items based credibility scores top-ranked credible postings item top-ranked users based experience etc. next section give motivation temporal evolution users’ experience online communities recommendation tasks. state-of-the-art limitations collaborative ﬁltering algorithms heart recommender systems items like movies cameras restaurants beer. methods exploit user-user item-item similarities addition history user-item ratings similarities based latent factor models user item features recently explicit links interactions among users data evolve time leading bursts item popularity phenomena like anomalies. state-of-the-art recommender systems capture temporal aspects introducing global bias components reﬂect evolution user community whole. models also consider changes social neighborhood users. missing approaches though awareness experience maturity levels evolve individual users. individual experience crucial users appreciate items thus react recommendations. example mature cinematographer would appreciate tips movies much recommendations blockbusters. also facets item user focuses change experience. example mature user pays attention narrative light effects style rather actors special effects. similar observations hold ratings wine beer food etc. approach advances state-of-the-art tapping review texts modeling properties latent factors using explain predict item ratings function user’s experience evolving time. prior works considering review texts learn topic similarities static snapshot-oriented manner without considering time all. prior work considering time ignores text user-contributed reviews harnessing experience. however user experience interest speciﬁc item facets different timepoints often observed indirectly ratings vividly vocabulary writing style reviews. user clearly amateur time posting ﬁrst review; whereas clearly experienced decade later writing second review reserved lens quality camera model. ﬁrst user appreciate complex narratives making writing review backwards. second user prefers simpler blockbusters. third user seems appreciate complex narration style inception memento. would consider maturity level experienced user generate future recommendations her. model joint evolution user experience interests speciﬁc item facets rating behavior writing style community. item ratings review texts directly observed capture user’s experience interests latent model learned reviews vocabulary. conditioned time considering maturing rate user. intuitively user gains experience writing many reviews also needs continuously improve quality reviews. varies different users enter community experienced. allows generate individual recommendations take account user’s maturity level interest speciﬁc facets items different timepoints. propose approaches model evolving user experience writing style ﬁrst approach considers user’s experience progress discrete manner whereas next approach addresses several drawbacks discrete evolution proposes natural continuous mode temporal evolution user’s experience language model. allocation model captures interests speciﬁc item facets function experience level. explicit input model ratings review texts upto certain timepoint; everything else especially user’s experience level latent variable. output predicted ratings user’s reviews following given timepoint. addition derive interpretations user’s experience interests salient words distributional vectors latent dimensions. although unsurprising users writing sophisticated words experience observe something interesting. instance specialized communities like beeradvocate.com ratebeer.com experienced users write descriptive fruity words depict beer taste table shows snapshot words used users different experience levels depict facets beer taste movie plot journalism respectively. limitations discrete evolution models section iv.. gives motivation evolution user experience affects ratings. however proposed approach precursor make simplifying assumption user experience categorical discrete levels users progress level next discrete manner. artifact assumption experience level user changes abruptly transition. also undesirable consequence discrete model users level experience treated similarly although maturity could still apart therefore assumption exchangeability reviews latent factor model discrete approach users level experience hold language model changes. prior work assumes user activity play major role experience evolution biases model towards highly active users contrast discrete version approach captures interpretable evidence user’s experience level using vocabulary cast language model latent facets. however approach also exhibits drawbacks discrete levels experience discussed above. therefore propose continuous version experience evolution overcomes limitations modeling evolution user experience corresponding language model continuous-time stochastic process. model time explicitly work contrast prior works. approach ﬁrst work develop continuous-time model user experience language evolution. unlike prior work rely explicit features like ratings number reviews. instead capture user’s experience latent language model learned user-speciﬁc vocabulary review texts. present generative model user’s experience language model evolve according geometric brownian motion brownian motion process respectively. analysis trajectory users offer interesting insights; instance users reach high level experience progress faster also exhibit comparatively higher variance. also number reviews written user strong inﬂuence unless written long period time. facets model generated using latent dirichlet allocation. user experience item facets latent variables whereas observables words explicit timepoints user reviews. parameter estimation inference model challenging since combine discrete multinomial distributions continuous brownian motion process language models’ evolution continuous geometric brownian motion process user experience. contributions solve technical challenge present inference method consisting three steps estimation user experience user-speciﬁc using metropolis hastings algorithm estimation language model evolution kalman filter estimation latent facets using gibbs sampling. experiments real-life data different communities movies food beer news media show three components coherently work together yield better data previously best models discrete experience levels. also achieve improvement mean squared error predicting user-speciﬁc ratings items compared baseline discrete version model model devise probabilistic model tracing continuous evolution user experience combined language model facets explicitly captures smooth evolution time. experiments perform extensive experiments real-word datasets together comprising million ratings million users million items demonstrate substantial improvements method state-of-the-art baselines. interesting use-case application experience-evolution model perform experimental study news community identify experienced members play role citizen journalists community. study similar section iii.. credibility analysis additional incorporation temporal evolution. approach based intuition strong coupling facet preferences user experience writing style reviews rating behavior. factors jointly evolve time given user. model user experience progression discrete stages state-transition model natural. decision made markovian model simplest thus natural choice. experience level user current instant depends experience level previous instant experience levels latent hidden markov model appropriate. experience progression user depends following factors maturing rate user modeled activity community. engaged user community higher chances gains experience advances writing sophisticated reviews develops taste appreciate speciﬁc facets. writing style user expressed language model current level experience. sophisticated vocabulary writing style indicates higher probability progressing mature level. experience level difference since unlikely user directly progress level level without passing level model instant decides whether user stay current level progress order learn facet preferences language model user different levels experience latent dirichlet allocation work assume review refer exactly item. therefore facet distribution items expressed facet distribution review documents. facet distribution words words used describe facet depend user’s vocabulary experience level table shows salient words facets amazon movie reviews different levels user experience automatically extracted latent model. facets latent interpret plot/script narrative style respectively. level stupid people supposed wouldnt pass bizarre totally cant level storyline acting time problems evil great times didnt money ended simply falls pretty level movie plot good young epic rock tale believable acting level script direction years amount fast primary attractive sense talent multiple demonstrates establish level realism moments ﬁlmmaker visual perfect memorable recommended genius ﬁnish details deﬁned talented visceral nostalgia level happy people back supposed good wouldnt cant level storyline believable acting time stay laugh entire start funny level narrative cinema resemblance masterpiece crude undeniable admirable renowned seventies unpleasant myth nostalgic level incisive delirious personages erudite affective dramatis nucleus cinematographic transcendence unerring peerless fevered expect users different experience levels divergent language models experienced users sophisticated writing style vocabulary amateurs. test hypothesis performed initial studies popular communities beeradvocate million reviews users amazon movie reviews million reviews users. span period years. beeradvocate user gets points basis likes received reviews ratings users number posts written diversity number beers rated time community etc. points measure proxy user’s experience. amazon reviews helpfulness votes users. user aggregate votes reviews take proxy experience. partition users bins based points helpfulness votes received representing experience levels. aggregate review texts users construct unigram language model. heatmap figure iv.a shows kullback-leibler divergence lm’s different experience levels beeradvocate case. amazon reviews lead similar heatmap omitted here. main observation divergence higher larger difference experience levels users. conﬁrms hypothesis coupling experience user language. second hypothesis underlying work users similar levels experience similar facet preferences. contrast lm’s words observed facets latent validating falsifying second hypothesis straightforward. performed three-step study support vector regression user. user’s item rating review response variable facet proportions review given features. regression weight interpreted preference user facet figure iv.b shows divergence facet preferences users different experience levels beeradvocate. divergence clearly increases difference user experience levels; conﬁrms hypothesis. heatmap amazon similar omitted. 〈..〉 denotes scalar product. average rating items users. offset average rating given user global rating. likewise rating bias item latent factors associated user item respectively. latent factors learned using gradient descent minimizing mean squared error observed ratings predicted ratings relevant baseline work user learned rate model exploits users experience level similar rating behavior even ratings temporarily apart. experience user item modeled however signiﬁcantly different approach. models work basis user rating behavior ignore review texts completely. additionally smoothness evolution parameters experience levels enforced regularization model natural user maturing rate model. also note parametrization experience level estimated user-item pair. however rare user reviews item multiple times. approach instead trace evolution users user-item pairs. order facets interest user extends latent dirichlet allocation include authorship information. document considered distribution authors. consider special case document exactly author associated multinomial distribution facets symmetric dirichlet prior facets multinomial distribution words drawn vocabulary symmetric dirichlet prior generative process user writing review given algorithm exact inference possible intractable coupling ways approximate inference mcmc techniques like collapsed gibbs sampling variational inference. latter typically much complex computationally expensive. work thus sampling. generative process described unsupervised take ratings reviews account. supervision difﬁcult build mcmc sampling ratings continuous values communities like newstrust.net. discrete ratings reviewspeciﬁc multinomial rating distribution learned discretizing continuous ratings buckets bypasses problem extent results loss information. approaches overcome problem learning feature weights separately user-facet model. supervised version topic model using variational inference proposed tackles problem coupling removing interactions altogether makes problem intractable; learns variational parameters minimizes divergence approximate distribution true joint distribution. however ﬂexibility comes cost increasingly complex inference process. elegant approach using multinomial-dirichlet regression proposed incorporate arbitrary types observed continuous categorical features. facet associated vector whose dimension equals number features. assuming feature vector document dirichlet hyper-parameter document-facet multinomial distribution parametrized exp. model trained using stochastic alternates sampling facet assignments posterior distribution conditioned words features optimizing given facet assignments using l-bfgs. approach explained next section follows similar approach couple user-facet model latent-factor recommendation model start user-facet model based latent dirichlet allocation users distribution facets facets distribution words. determine facets interest user. facet preferences interpreted latent item factors traditional latent-factor recommendation model however supervised opposed ufm. obvious incorporate supervision predict ratings. user-provided ratings items take continuous values cannot incorporate multinomial distribution ratings. propose expectation-maximization approach incorporate supervision latent facets estimated e-step using gibbs sampling support vector regression used m-step learn feature weights predict ratings. subsequently incorporate layer experience ufm-lfm model experience levels drawn hidden markov model e-step. experience level transitions depend evolution user’s maturing rate facet preferences writing style time. entire process supervised generative process generating review based experience level user hinged hmm-lda model. drawn vocabulary unique words indexed {...v consider users involved writing documents corpus author document consider ordered experience levels {ee...ee facets z...zz possible facets. document associated rating item assume time writing review user experience level experience level transitions follow distribution markovian assumption certain constraints. means experience level time depends experience level writing previous document time td−. writing style user. progression also takes account maturing rate modeled intensity activity community time interval writing consecutive reviews. incorporate aspects prior user’s transition rates deﬁned denote number reviews written average number reviews user community respectively. therefore ﬁrst term models user activity respect community average. second term reﬂects time interval successive reviews. user experience unlikely change level writing previous review hours days ago. controls effect time difference small value. note user writes infrequently second term ﬁrst term plays dominating role prior small respect community average active community bringing inﬂuence entire prior. note constructed encapsulates factors experience progression outlined section iv... experience level user multinomial facet-preference distribution distribution draws facet interest word document. example user high level experience choose write beer hoppiness story perplexity movie. word writes depends facet chosen language model current experience level. thus draws word multinomial symmetric dirichlet prior example facet chosen beer distribution φetd taste movie plot experienced user choose words coffee roasted vanilla visceral whereas inexperienced user bitter emotional respectively. latent item factors equation correspond latent facets algorithm assume estimation latent facet distribution document iteration mcmc sampling denotes experience level document written denote latent facet document. also estimation preference user facet experience level given θue. user compute supervised regression function user’s numeric ratings currently estimated experience-based facet distribution reviews input features ratings output. given user timestamp /*current experience level depends previous level*/ conditioned previous experience etd− choose /*user’s facet preferences current experience level inﬂuenced supervision /*facet drawn user’s experience-based facet interests*/ conditioned choose facet /*word drawn chosen facet user’s vocabulary current experience higher preference level reﬂected next sampling iteration draw facet user’s facet preference distribution smoothed draw word φez. sampling process repeated convergence. latent facet model difﬁcult hyper-parameters. therefore prior work assume symmetric dirichlet priors heuristically chosen concentration parameters. approach learn concentration parameter general dirichlet prior multinomial distribution optimize hyper-parameters learn user ratings documents given experience level. describe inference algorithm estimate distributions observed data. user compute conditional distribution hidden variables words review. exact computation distribution intractable. collapsed gibbs sampling estimate conditional distribution hidden variable computed current assignment hidden variables integrating parameters model. denote count word occurring document written user experience level belonging facet following equation position distribution indicates summation counts respective argument. staying experience level. counts capture relative difﬁculty progressing different experience levels. example easier progress level level level level wise. subscript denotes value variable excluding data position. counts transitions exclude transitions sampling value current experience level gibbs sampling. conditional distribution experience level transition given ﬁrst factor models rate experience progression factoring user activity; second third factor models facet-preferences user language model speciﬁc level experience respectively. three factors combined decide whether user stay current level experience matured enough progress next level. gibbs sampling conditional distribution hidden variable computed based current assignment hidden variables. values latent variables sampled repeatedly conditional distribution convergence. problem setting sets latent variables corresponding respectively. levels current document token positions compute equation choose level highest conditional probability. thereafter sample facet word document keeping currently sampled experience level user document ﬁxed. user learn regression model using facet proportions document features along user item biases user’s item rating response variable. besides facet distribution document positive negative real numbered weights. order ensure concentration parameters dirichlet distribution positive reals take exp. learned typically small whereas value equation large. therefore scale hyper-parameter control inﬂuence supervision. tuned using validation varying {...}. e-step next iteration choose chlet. liblinear package support data perform experiments data communities different domains beeradvocate ratebeer beer reviews amazon movie reviews yelp food restaurant reviews newstrust reviews news media. table gives dataset statistics. total million reviews million users communities combined. ﬁrst four communities used product reviews extract corresponding learned parameters rating prediction. models group light users less reviews training data background model treated single user avoid modeling sparse observations. ignore user. test phase light user take parameters background model. beeradvocate ratebeer yelp facets; amazon movies newstrust much richer latent dimensions. experience levels all. however community uniform rate users products community evolve using single global clock different stages community evolution appear uniform time intervals. community prefers different products different times. user uniform rate extends consider individual users modeling different stages user’s progression based preferences experience levels evolving time. model assumes uniform rate experience progression. model past experience level order determine well model captures evolution user experience time consider another baseline randomly sample experience level reached users timepoint previously lifecycle evolved thereafter. learn model parameters data time predict user’s recent three item ratings. note baseline considers textual content user contributed reviews unlike baselines ignore them. therefore better vanilla content-based methods notion past evolution strongest baseline model. experience level smells sweet thin bitter fresh hint honey sticky yellow slight good faint bitter beer brown good malty deep smooth bubbly damn weak experience level golden head lacing ﬂoral dark fruits citrus sweet light spice hops caramel ﬁnish acquired taste hazy body lacing chocolate coffee roasted vanilla creamy bitterness copper malts spicy honey performance improvement prominent newstrust community exhibits strong language features topic polarities reviews. lowest improvement achieved amazon movie reviews. possible reason community diverse wide range movies review texts heavily statements movie plots actual review aspects like praising criticizing certain facets movie. situation similar food restaurants case. nevertheless model always wins best baseline works typically user learned rate\" model. evolution effects observe table model’s predictions degrade applied users’ past experience level compared recent level. signals model captures user evolution past previous timepoint. therefore last experience level attained user informative generating recommendations. salient words facets experience levels point typical word clusters illustrative labels show variation language users different experience levels different facets. tables show salient words describe beer facet taste movie facets plot narrative style respectively different experience levels. note facets latent labels merely interpretation. similar examples found tables iv.. beeradvocate ratebeer focused communities; easier model characterize user experience evolution vocabulary writing style user reviews. observe table users write descriptive fruity words depict beer taste become experienced. movies wording reviews much diverse harder track. especially blockbuster movies tend dominate data reviews kinds aspects. better approach could focus speciﬁc kinds movies better distinguish experienced users amateurs novices terms reﬁned taste writing style. different experience levels observe weak trend decreases increasing experience level. users highest level experience almost always exhibit lowest therefore preditable behavior. tend better predict rating behavior mature users remaining user population. turn enables generating better recommendations connoisseurs\" community. experience progression figure shows proportion reviews written community members different experience levels right advancing next level. plot users minimum reviews certainly amateurs\". large part community progresses level level however users move higher levels leading skewed distribution. observe majority population stays level language model facet preference divergence figure iv.b iv.c show divergence facet-preference language models users different experience levels computed model. facet-preference divergence increases experience levels smooth prominent language models. hand complexity latent facets explicit words. hand also afﬁrms notion grounding model language. baseline model divergence figure iv.a shows facet-preference divergence users different experience levels computed baseline model user learned rate contrast heatmaps model baseline revealing. increase divergence increasing experience levels rough baseline model although trend obvious. previous section presented approach model experience evolution users online communities. however proposed model several assumptions resulting drawbacks. following propose generalized model captures evolution user experience commonly observed nature. previous approaches experience evolution model time implicitly assuming latent experience progress review next. contrast model time explicitly allow experience continuously evolve time able trace joint evolution experience vocabulary. challenging discrete multinomial distribution based language model needs combined continuous stochastic process experience evolution. levels temporal granularity. since experience naturally continuous beneﬁcial model evolution resolution hand language model much coarser granularity show section iv.. smoothly merge granularities using continuous-time models. model language evolution motivated seminal work wang blei major differences extensions. following subsections formally introduce components affected time experience evolution language model evolution. transition abrupt user switches levels. also model distinguish users level experience even though experience quite apart instance figure iv.b language model uses parameters long user stays level although language model changes. natural continuous state alternative discrete-state space based hidden markov model used previous approach figure shows real-world example evolution experienced amateur user beeradvocate community traced proposed model along discrete counterpart previous approach. stochastic process used model population growth ﬁnancial processes like stock price behavior random noise. continuous time stochastic process logarithm random variable follows brownian motion volatility drift. formally stochastic process stants called percentage trend percentage volatility respectively. former captures deterministic trends whereas latter captures unpredictable events occurring motion. brownian motion trajectory capture trend volatility required experience evolution. however real life communities user might show different experience evolution; therefore model considers multivariate version model trajectory per-user. correspondingly inference process learn user experience values user generated log-normal distribution develop language model whose parameters evolve according markov property experience evolution. users experienced sophisticated words express concept. instance experienced cineastes refer movie’s protagonist whereas amateur movie lovers talk hero. similarly beer review community experts fruity words describe beer like caramel ﬁnish coffee roasted vanilla citrus hops. facet preferences users also evolve experience. example users high level experience prefer hoppiest beers considered bitter amateurs encoding explicit time model allows trace evolution vocabulary trends jointly temporal experience dimension. latent dirichlet allocation traditional process document assumed distribution facets facets distribution words ﬁxed vocabulary collection. per-facet word distribution drawn dirichlet distribution words generated multinomial. discrete experience-aware previous approach incorporates layer experience process. user experience manifested facets user chooses write vocabulary writing style used reviews. experience levels drawn hidden markov model reviews assumed exchangeable user level experience assumption generally hold; since language model user discrete experience level different different points time process considers time implicitly transition latent variable experience. continuous time seminal work capture evolving content instance scholarly journals news articles themes evolve time considering time explicitly generative process. language model evolution motivated continuous time dynamic topic model major difference facets case evolve time experience. continuous experience-aware since assumption exchangeability documents level experience user hold want language model explicitly evolve experience time. incorporate effect changing experience levels goal condition parameter evolution experience progression. variance linearly increase experience change successive timepoints. entails experience user change successive timepoints language model remains almost same. incorporate temporal aspects data model multiple distributions time facet furthermore capture smooth temporal evolution facet language model need chain different distributions sequentially evolve time distribution affect distribution βt+z. since traditional parametrization multinomial distribution mean parameters amenable sequential modeling inconvenient work gradient based optimization since gradient step requires projection feasible simplex follow similar approach instead operating mean parameters consider natural parameters multinomial. natural parameters unconstrained thus enable easier sequential modeling. denote natural parameters multinomial time facet identiﬁability parameters βtzw needs ﬁxed zero. applying following mapping obtain back mean parameters located simplex using natural parameters deﬁne facet-model evolution underlying idea strong changes users’ experience lead strong changes language model changes lead changes. capture effect denote average experience word time given average experience reviews containing word time here simply follow idea standard dynamic system gaussian noise mean value previous timepoint variance increases linearly increasing change experience. thereby desired properties language model evolution ensured. unique timestamp unique user experience value review refers experience user time writing model user follows geometric brownian motion trajectory starting time relative ﬁrst review user community parametrized mean variance starting experience value shown equation analytical form translates log-normal distribution given mean variance. user-dependent distribution generate experience value review written timestamp generated experience values generate language model individual words review. here language model βtzw uses state-transition equation actual word based facet timepoint according multinomial) transformation given equation iv.. note technically distribution word generated simultaneously require terms depend experience words. thus joint distribution since however words observed inference experience values reviews experience values words facets timestamps words corpus respectively. following denotes review indexes word denotes per-review facet distribution language model respectively. estimating facets collapsed gibbs sampling standard estimate conditional distribution latent facets computed current assignment hidden variables integrating denote count topic appearing review following equation estimating language model contrast variable cannot integrated process normal multinomial distributions conjugate. therefore refer another approximation technique estimate work kalman filter model sequential language model evolution. widely used model linear dynamic systems series observed measurements time containing statistical noise produces robust estimates unknown variables single measurement. continuous analog hidden markov model state space latent variables continuous observed latent variables evolve gaussian noise. however unlike standard kalman filter observed measurement variables presence latent facets therefore resort inferred measurement gibbs sampling process. update equations kalman filter denote prediction error kalman gain time respectively. variance process noise measurement given difference experience value word observed successive timepoints. estimating experience experience value review depends user language model although state-transition model previous process estimation using kalman filter cannot applied case observed inferred value therefore resort metropolis hastings sampling. instead sampling complex true distribution proposal distribution sampling random variables followed acceptance rejection newly sampled value. iteration algorithm samples value random variable current estimate depends previous estimate thereby forming markov chain. tamps. discussed section iv.. computational feasibility coarse granularity language model inference however need operate temporal resolution reviews’ timestamps note process deﬁned represents aggregated language model multiple ﬁne-grained timestamps. accordingly corresponding ﬁne-grained counterpart review’s individual experience values. since language model given inference easily refer ﬁne-grained deﬁnition metropolis hastings sampling. estimating parameters geometric brownian motion user mean variance trajectory estimated sample mean variance. consider reviews 〈dt〉 written 〈et〉 corresponding experience table gives dataset statistics. total million reviews million users years communities combined. ﬁrst four communities used product reviews extract following quintuple model inference model quite involved different markov chain monte carlo methods. imperative show resultant model stable also improves log-likelihood data. although several measures evaluate quality facet models report following figure contrasts log-likelihood data continuous experience model discrete counterpart continuous model stable smooth increase data log-likelihood iteration. attributed smoothly language model evolves time preserving markov property experience evolution. empirically model also shows fast convergence indicated number iterations. hand discrete model worse also less smooth. exhibits abrupt state transitions hidden markov model experience level changes leads abrupt changes language model coupled experience evolution. furthermore also done baseline works discrete version model consider average rating community; offset average rating given user global average; rating bias item continuous experience model discrete experience model user learned rate community learned rate community uniform rate user uniform rate latent factor model community uniform rate users products community evolve using single global clock different stages community evolution appear uniform time intervals. user uniform rate extends consider individual users modeling different stages user’s progression based preferences experience levels evolving time. model assumes uniform rate experience progression. user learned rate extends allowing experience user evolve personal clock time reach certain experience levels depends user reportedly best version experience evolution models. discrete experience model prior approach discrete version experience-aware language model experience user depends evolution user’s maturing rate facet preferences writing style. table compares mean squared error rating predictions task generated model versus baselines. model outperforms baselines except newstrust community performing slightly worse prior discrete model reducing improvements baselines statistically signiﬁcant level conﬁdence determined paired sample t-test. models used three recent reviews user withheld test data. experience-based models consider last experience value reached user training corresponding learned parameters rating prediction. similar setting consider users minimum reviews. users less reviews grouped background model treated single user. beeradvocate ratebeer yelp facets; amazon movies newstrust richer latent dimensions. discrete experience models consider experience levels. continuous model experience value initialize parameters joint model performance improvement user experience progression figure shows variation users’ recent experience along number reviews posted number years spent community. would expect user’s experience increases amount time spent community. contrary number reviews posted strong inﬂuence experience progression. thus user writes large number reviews short span time experience increase much; contrast reviews written long period time. figure variation experience years reviews user. stacked chart corresponds user recent experience number years spent number reviews posted community. figure variation experience mean variance trajectory user stacked chart corresponds user recent experience mean variance experience evolution. figure shows variation users’ recent experience along mean variance geometric brownian motion trajectory learned inference. observe users reach high level experience progress faster not. experienced users also exhibit comparatively higher variance amateur ones. result also follows using process mean variance tend increase time. language model evolution figure shows variation frequency word used community learned experience value associated word. plots depict bell curve. intuitively experience value word increase general usage; increases used experienced users. highlighted words plot give interesting insights. instance words beer head place food movie story etc. used high frequency beer food movie community average experience value. hand specialized words like beeradvocate budweiser %abv fullness encore minb&w etc. high experience value. table shows words used experienced users amateur ones different communities learned model. note ranked list words numeric values experienced users interested ﬁne-grained facets like mouthfeel fruity ﬂavors texture food drinks; narrative style movies opposed popular entertainment themes; discussing government policies regulations news reviews etc. figure shows evolution sample words time experience different communities. score y-axis combines language model probability βtzw experience value associated word time beeradvocate chestnut_hued near_viscous rampant_perhaps sweet_burning faux_foreign bright_crystal boned_dryness woody_herbal citrus_hops mouthfeel amazon aﬁcionados minimalist underwritten theatrically unbridled seamless retrospect overdramatic diabolical recreated notwithstanding oblivious featurettes precocious figure illustrates frequency words beeradvocate evolution traced figure seen overall usage word increases time; evolution path different word. instance smell convention started aroma dominant; latter less used experienced users time slowly replaced smell. also reported different context. similarly caramel likely used experienced users ﬂavor. also contrast evolution bitterness used experienced users compared bitter. yelp certain food trends like grilled crispy increasing time; contrast decreasing feature like casino restaurants. amazon movies certain genres like horror thriller contemporary completely dominating genres recent times. sections discuss evolution user experience online communities applications focused recommending items users based maturity. another application use-case switch different kind items newspapers news articles tapping newstrust online community newstrust features news stories posted reviewed members many professional journalists content experts. stories reviewed based objectivity rationality general quality language present unbiased balanced narrative event. level religion iraq responsibility level national reform live krugman questions clear meaningful lives california powerful safety impacts level health actions cuts medicare news points climate major jobs house high vote congressional spending unemployment strong taxes citizens events failure focus quality journalism. unlike datasets newstrust contains expertise members used ground-truth evaluating model-generated experience values users. previously section iii.. discussed several characteristics community employed credibility analysis therein. framework item recommendation story item rated reviewed user. facets underlying topic distribution reviews topics healthcare obama administration etc. facet preferences mapped polarity users news community. ﬁrst objective recommend news readers catering facet preferences viewpoints experience. apply joint model task compare predicted ratings ones observed withheld reviews newstrust community. mean squared error results task reported table iv.. continuous model clearly outperforms baselines; performs slightly worse regarding prior discrete model task possibly high rating data sparsity face large number model parameters less number reviews per-user. second task experienced members community potential citizen journalists. order evaluate quality ranked list experienced users generated model consider following proxy measure user experience. newstrust users member levels determined newstrust staff based community engagement time community users’ feedback reviews proﬁle transparency manual validation. member levels categorize users experienced inexperienced. treated ground truth assessing ranking quality model baseline models discrete version prior work considering users model ranked experience. consider top-performing baseline models previous task. report normalized discounted cumulative gain normalized kendall distance ranked lists users generated models. ndcg gives geometrically decreasing weights predictions various positions ranked list chapter propose models capture temporal evolution users online communities. used identify users experienced joined community could evolved matured user now. current recommender systems consider temporal dynamics user experience generating recommendations. propose experience-aware recommendation models adapt changing preferences maturity users community recommend items appreciate current maturity level. exploit coupling facet preferences user experience writing style reviews rating behavior capture user’s temporal evolution. model ﬁrst work considers progression users’ experience expressed text item reviews. furthermore develop experience-aware language model trace continuous evolution user’s experience language explicitly time. combine principles geometric brownian motion brownian motion latent dirichlet allocation model smooth temporal progression user experience language model time. also ﬁrst work develop continuous generalized version user experience evolution. derive interesting insights evolution trajectory users vocabulary usage change experience. instance experienced users progress faster amateurs progression depending time spent community activity. experienced users also show predictable behavior distinctive writing style facet preferences example experienced users beer community fruity words depict smell taste beer; users news community interested policies regulations amateurs interested polarizing topics. experiments data domains like beer movies food news demonstrate model effectively exploits user experience item recommendation substantially reduces mean squared error predicted ratings compared state-of-the-art baselines. shows method generate better recommendations models. demonstrate utility model use-case study identifying experienced members newstrust community users would candidates citizen journalists. another similar use-case model detect experienced medical professionals health community contribute valuable medical knowledge. chapters develop probabilistic graphical models credibility analysis online communities temporal evolution respectively. current chapter principles models developed therein related tasks serious concern product review communities recent times. rapid growth e-commerce product reviews become crucial component business nowadays. consumers cannot test functionality product prior purchase reviews help make informed decision product not. survey conducted nielsen corporations online consumers indicated would electronics without consulting online reviews ﬁrst increasing dependency user-generated reviews crucial understand quality widely vary excellent-detailed opinion superﬁcial criticizing praising spams worst case. unfortunately review forums tripadvisor yelp amazon others increasingly game manipulative deceptive reviews fake incompetent biased example recent studies depict yelp reviews might fake yelp internally rejects user submissions not-recommended. recent research proposed approaches identify helpful reviews spams automatically suffer major drawbacks approaches geared towards active users items community reviews activity information therefore suitable long-tail users items limited data. importantly works based crude user behavioral shallow textual features provide interpretable explanation review deemed helpful non-credible. order address issues propose probabilistic approaches based analyzing reviews several aspects like consistency semantics temporal dynamics tasks online review communities ﬁnding useful product reviews helpful consumers ﬁnding credible reviews limited information users items speciﬁcally long-tail ones using consistency features. provide userinterpretable explanations verdict tasks. motivation online reviews provided consumers valuable asset e-commerce platforms inﬂuencing potential consumers making purchasing decisions. however without indication review quality overwhelming consumers browse multitude reviews. order help consumers ﬁnding useful reviews ecommerce platforms nowadays allow users vote whether product review helpful not. instance amazon product review accompanied information like users found review helpful. helpfulness score considered proxy review quality usefulness consumers. task automatically helpfulness score review based certain consistency semantic aspects review like whether review written expert important facets product outlined review experts given product timeliness review etc. automatically mined latent factors review texts. state-of-the-art limitations prior works predicting review helpfulness mostly operate shallow syntactic textual features like bag-of-words part-of-speech tags tf-idf statistics works related works ﬁnding review spams classify extremely opinionated reviews helpful. similarly works exploiting rating activity features like frequency user posts average ratings users items consider extreme ratings deviations indicative unhelpful reviews. recent works incorporate additional information like community-speciﬁc characteristics explicit user network item-speciﬁc meta-data like explicit item facets product brands apart requirement large number meta-features restrict generalizability many models arbitrary domain shallow features analyze review about therefore cannot explain helpful given product. works identify expertise review’s author important feature. however absence suitable modeling techniques consider prior reputation features like user activity rating deviation proxy user expertise. work closest approach authors identify syntactic features user expertise timeliness review important indicators quality. however even case authors part-of-speech tags syntactic features user preferences explicit item facets proxy user expertise. contrast explicitly model user expertise function writing style rating style preferences item facets jointly learned user-contributed reviews going beyond usage shallow syntactic features requirement additional item meta-data. problem statement work aims overcome limitations prior works exploring semantics consistency review predict helpfulness score given item. unlike prior works features harnessed information user reviewing item explicit timepoint making approach fairly general communities domains. also provide interpretable explanation terms latent word clusters gives interesting insights makes review helpful. approach ﬁrst step towards understanding semantics review uncover facet descriptions target item outlined review. treat facets latent latent dirichlet allocation discover topic clusters. second step expertise users wrote review description different facets item. approach modeling user expertise similar outlined chapter however signiﬁcant differences modiﬁcations modeling joint interactions several factors proposed model better coupling factors learned directly review helpfulness. make distributional hypotheses like expert users agree important facets item description facets inﬂuences helpfulness review. also derive several consistency features reputation item prominence timeliness review used conjunction semantic features. finally leverage interplay factors joint setting predict review helpfulness. interpretable explanation derive interesting insights latent word clusters used experts instance reviews describing underlying theme storytelling movies books style music hygiene food considered helpful respective domains. model propose approach leverage semantics consistency reviews predict helpfulness. propose hidden markov model latent dirichlet allocation based model jointly learns item facets user expertise writing style observed words reviews explicit timepoints. algorithm introduce effective learning algorithm based iterative stochastic optimization process reduces mean squared error predicted helpfulness scores ground scores well maximizes log-likelihood data. experiments perform large-scale experiments real-world datasets different domains amazon together comprising million reviews million users million items demonstrate substantial improvement state-of-the baselines prediction ranking tasks. motivation starting work research efforts undertaken automatically detect non-credible reviews. parallel industry developed standards ﬁlter illegitimate reviews. although details disclosed studies suggest ﬁlters tend fairly crude instance exploiting user activity like number reviews posted treating users whose ratings show high deviation mean/majority ratings suspicious. policy seems over-emphasize trusted long-term contributors suppress outlier opinions mainstream. moreover ﬁlters also employ several aggregated metadata thus hardly viable items initially reviews often active users newcomers community. state-of-the-art limitations research topic cast problem review credibility binary classiﬁcation task review either credible deceptive. supervised semi-supervised methods developed largely rely features users activities well statistics item ratings. techniques also consider spatio-temporal patterns user activities like addresses user locations burstiness posts item item group further correlation measures across users items discussed chapter iii. however classiﬁers built mostly geared popular items meta-information user histories activity correlations always available. example someone interested opinions long-tail bed-and-breakfast rarely visited town helped methods. several existing works consider textual content user reviews tackling opinion spam using word-level unigrams bigrams features along speciﬁc lexicons psycholinguistic lexicon wordnet affect learn latent topic models classiﬁers although methods achieve high classiﬁcation accuracy various gold-standard datasets provide interpretable evidence certain review classiﬁed non-credible. problem statement task focuses detecting credible reviews limited information namely absence rich data user histories community-wide correlations long-tail items. extreme case provided review texts ratings item. goal analyze various inconsistencies exist within reviews using compute credibility score provide interpretable evidence explaining certain reviews categorized non-credible. approach proposed method learn model based latent topic models combining limited metadata provide novel notion consistency features characterizing review. lda-based joint sentiment topic model cast user review texts number informative facets. per-item aggregating text among reviews item also per-review. allows identify score highlight inconsistencies appear review community’s overall characterization item. perform item whole also latent facets separately. additionally learn inconsistencies discrepancy contents review rating temporal bursts number reviews written short span time targeting item. propose kinds inconsistencies form assets credibility scoring model support vector machine classiﬁcation ordinal ranking. model develop novel consistency model credibility analysis reviews works limited information particular attention long-tail items offers interpretable evidence reviews classiﬁed non-credible. tasks investigate credibility scores affect overall ranking items. address scarcity labeled training data transfer learned model yelp amazon rank top-selling items based credible user reviews. presence proxy labels item goodness develop better ranking model domain adaptation. experiments perform extensive experiments tripadvisor yelp amazon demonstrate viability method advantages state-of-the-art baselines dealing long-tail items providing interpretable evidence. given review item essential understand different facets item described review. instance camera review focus different facets like resolution\" zoom\" price\" size movie review focus narration\" cinematography\" acting\" direction etc. however facets equally important item. example review downrating camera late delivery seller helpful general consumer opposed downrating grainy resolution shaky zoom. therefore helpful review focus important facets item. another important aspect detailed review consider wide range facets item rather harping speciﬁc facet similar importance facets outlined review words used describe facets play crucial role making review readable useful consumers. diverse background reviewers different language skills writing style varies widely. important aspect expert writing style precise domain-speciﬁc vocabulary describe facet details rather using generic words. instance contrast expert camera review example focus screen ‘grainy’. ‘precision matte’ surface helps increase contrast minimize depth ﬁeld manual focusing. ef-s screen even fast primes. focus screen smoother brighter compensate dimmer pentamirror design typical economy f/.-. zooms gives less precise manual focus. another important factor observe balance reviewer’s opinion item. expert review depicts detailed judgment item rather criticizing praising therefore essential distinguish writing style experienced user amateur one. prior works capture writing style syntactic features like words part-of-speech tags sentiment lexicons distribution positive negative sentiment words review. contrast learn language model latent facets user expertise uncovers hidden semantics review. previous works domain attempted harness user’s expertise writing review hypothesis expert reviews positively correlated review helpfulness. however none explicitly modeled users’ expertise. instead considered following proxy features user reputation namely activity number posts written user community. rating deviation deviation user rating community rating item. prior user reputation average number helpfulness votes received user previous reviews. work explicitly model user expertise adopting similar approach outlined section iv.. however make substantial modiﬁcations modeling learning joint distributions conditioned expertise distributions explicitly learned review helpfulness scores observables. model expertise latent variable evolves time exploiting hypothesis users similar levels expertise similar rating behavior facet preferences writing style. facets discovered previous step writing style would therefore help ﬁnding reviewer’s expertise. ﬁgure reviewer’s expertise important facets item concerned about well domain-speciﬁc vocabulary describing facets thereby forming effective feedback loop facets writing style expertise. past reviews given user item deemed helpful incoming review given user similar kind item also likely helpful. instance movie domain user’s past reviews drama genre found helpful movie preview also genre review likely helpful. past reviews users certain characteristics deemed helpful given user tastes expertise similar users current review also likely helpful. instance assume learned expert reviews drama genre looks like current review text indicates user expert drama genre review likely helpful. users items gain reputation overnight. therefore prior reputation users items good indicators associated reviews’ helpfulness. work following consistency features used guide model learn latent distributions conditioned reviews’ helpfulness ratings. prior user reputation average helpfulness votes received user’s past reviews users. prior item prominence average helpfulness votes received item’s past reviews users also indicative prominence item. user rating deviation absolute deviation user’s rating item average rating assigned user items. captures mean user rating behavior therefore scenarios user dis-satisﬁed item. item rating deviation absolute deviation user’s rating item average rating received item users. captures scenario user unnecessarily criticizes praises item community agree with. global rating deviation absolute deviation user’s rating item average rating items users community. captures scenario user rating deviates general rating behavior community. prior work shown positive inﬂuence review’s publication date number helpfulness votes received reason early timely reviews useful consumers item launched make informed decision item. also early reviews exposed consumers longer period time allows garner votes time compared recent reviews. timestamp ﬁrst review given item considered reference timepoint therefore timeliness review item time computed order understand signiﬁcance different consistency features predicting review helpfulness correlation various features described previous section helpfulness scores reviews. consider reviews real-world datasets amazon domains namely food movies music electronics. selected reviews received minimum votes maintain robustness task. table observe rating deviations user diverges community rating items prior rating history negatively impact helpfulness; whereas prior reputation users items timeliness positive impact. also user activity alone signiﬁcant impact review helpfulness. cases non-signifcant even negative impact review helpfulness order feature ﬁres unison features linear regression predict review helpfulness considering features together. corresponding f-statistic user user writing review time item w...w|nd|} corresponding review text sequence words rating review associated helpfulness score corresponding timeliness average helpfulness score user reviews written average helpfulness score reviews item average rating assigned user items average rating assigned item users average global rating items users. consistency features include prior item user reputation deviation features burst. tensor dimension number expertise levels users number latent facets items. depicts opinion users expertise level facet therefore distributional hypotheses intrinsically integrated estimated reviews’ text conditioned helpfulness score reviews. principles latent dirichlet allocation learn latent facets associated item. review item assumed multinomial distribution facets symmetric dirichlet prior facet multinomial distribution words drawn vocabulary symmetric dirichlet prior exact inference possible intractable coupling expertise inﬂuences facet distribution users different levels expertise different facet preferences language model writing style also different users different levels expertise. therefore parametrize distributions user expertise similar approach chapter major modiﬁcations consider tensor dimension tensor dimension denotes preference facet users expertise level φezw denotes probability word used describe facet users progression depends writing style facet preferences user evolving respect expert users community well rate activity user. user activity used proxy expertise many prior works however play weak role preliminary study. therefore hyper-parameter controlling denote number posts written average number posts written user community respectively. tensor dimension hyper-parameters 〈γu〉 dimension denotes probability moving expertise level constraint however users start level expertise enter previously learned user-speciﬁc preferences personalized recommendation. however assume users level expertise similar facet preferences. therefore facet distribution conditioned user expertise user explicitly unlike prior works. helps reduce dimensionality exploit correspondence parameters consistency latent factor models together joint inference. previous approach incorporates supervision predicting ratings indirectly optimizing dirichlet hyper-parameters multinomial facet distribution cannot guarantee increase data log-likelihood iterations. contrast exploit learn expertise-facet distribution directly review helpfulness scores minimizing mean squared error inference. also tricky parameters distribution unconstrained optimization guaranteed simplex certain transformations discussed inference. therefore parameters strongly coupled model reducing mean squared error also leading near smooth increase data log-likelihood iterations consider corpus {d...dd} reviews written users timestamps review denote user timestamp review. reviews review consists assumed ordered timestamps i.e. sequence words denoted {w... word drawn unique timestamp unique user expertise value review refers expertise user time writing following markovian assumption user’s expertise level transitions follow distribution markovian assumption i.e. expertise level time depends expertise level writing previous review time td−. expertise level user review known facet preferences given thereafter facet word drawn multinomial expertise level user facets interest known generate language model individual words review user draws word multinomial distribution symmetric dirichlet prior refer figure generative process. given corpus reviews indexed 〈userid itemid rating reviewtext timepoint〉 corresponding helpfulness scores objective learn parameters minimizes mean squared error given equation case known could directly plugged values equation learn model parameters however dimensions corresponding facets user expertise latent need inferred text. parameter weight corresponding equaθ following reason. traditional parametrization multinomial distribution mean parameters. unconstrained optimization take parameters feasible i.e. simplex. hence easier work natural parameters instead. consider unconstrained collapsed gibbs sampling standard estimate conditional distribution latent facets computed current assignment hidden variables integrating following equation indicates summation counts possible subscript denotes value variable excluding data position. consider document containing sequence words corresponding facets ﬁrst factor models probability user reaching expertise level document whereas second third factor models probability facets chosen expertise level probability observing words facets expertise level respectively. following markovian assumption consider sampling select highest conditional expertise levels probability. overall processing scheme exploiting results discussions overall inference iterative stochastic optimization process consisting following steps perform experiments data amazon different domains movies music food books electronics. statistics dataset given table total million reviews million users million items group long-tail users less reviews training data background model treated single user avoid modeling sparse observations. ignore user. test phase long-tail user take parameters background ranking suitable evaluation compare ranking reviews different models based helpfulness scores reviews rank list helpful ones them. predicted helpfulness scores model rank reviews compute rank correlation gold rank list obtained ranking reviews ground-truth helpfulness scores using following measures spearman correlation assesses well relationship variables described using monotonic function unlike pearson correlation indicates linear relationship variables. computed pearson correlation rank values variables rank list. kendall-tau correlation measures number concordant discordant pairs whether ranks elements agree based scores total number combinations possible. unlike spearman correlation kendall-tau affected distance ranks depends whether agree not. several rating based features proxy reviewer reputation sentiment; review length letter cases content; review count statistics social features classify review helpful syntactic features sentiment review length reviewer rating statistics predict quality review. ignore social network related features work absence user-user links dataset. similar kinds syntactic semantic features also used next baseline. structural lexical syntactic semantic meta-data related features rank reviews based helpfulness. ignore explicit productspeciﬁc features absent dataset. predict helpfulness reviews imdb based three factors reviewer expertise syntactic features timeliness review. authors reviewer preferences explicit facets proxy expertise partof-speech tags words syntactic features review publication dates compute timeliness reviews. baseline closest work attempt model similar factors. however model reviewer expertise explicitly facets latent therefore relying additional item meta-data table shows comparison mean squared error squared correlation coefﬁcient review helpfulness predictions generated model four baselines. model consistently outperforms baselines reducing mse. table competitive baseline model since high overlap consistency features model baseline performance improvement model attributed incorporation latent factors model. perform paired sample t-tests performance improvement baselines especially electronics number reviews per-user respectively although still outperform baseline models perform worse. poor performance model last datasets attributed data sparsity user maturity could captured well. log-likelihood data convergence inference model quite involved coupling several variables alternate stochastic optimization process. figure shows increase data log-likelihood model per-iteration datasets. observe model stable achieves near smooth increase depending complexity dataset. electronics convergence quite rapid data quite sparse model sufﬁcient evidence categorizing users different expertise levels; behavior reﬂected experiments involving electronics dataset. language model facet preference divergence initial hypothesis joint interaction review helpfulness reviewer expertise facet preferences writing style expect users different expertise levels divergent facet preferences language models expert users sophisticated writing style vocabulary amateurs. figures show heatmaps kullback-leibler divergence facet preferences language models users different expertise levels computed model conditioned review helpfulness given used explicit product descriptions features able automatically discover latent features textual reviews; whereas food reviews mostly concerned hygiene allergens. least helpful reviews mostly describe generic concepts domain praise criticize item without going depth facets generally quite superﬁcial nature. music album lyrics recommend soundtrack touch songwriting features rare musical ears lyrical enjoy absolutely musically individual bland soothing released inspiration share mainstream deeper ﬂawless wonderfully eclectic heavily critics presence popularity brilliantly inventive books serious complex claims content illustrations picture genre beautifully literary witty critics complicated argument premise scholarship talented divine twists exceptional obsession commentary landscape exposes inﬂuenced accomplished oriented exploration styles storytelling movies scene recommend screenplay business depth justice humanity packaging perfection ﬂicks sequels propaganda anamorphic cliche&acute pretentious goofy ancient marvelous perspective outrageous intensity mildly immensely bland subplots anticipation rendered atrocious electronics adapter wireless computer sounds camera range drives mounted photos shots packaging antenna ease careful broken cards distortion stick media application worthless clarity technical memory steady dock items cord systems amps skin watt monitors arms pointed food expensive machine months clean chips texture spicy odor inside processed robust packs weather sticking alot press poured swallow reasonably portions beware fragrance basket volume sweetness terribly caused scratching serves sensation sipping smelled italian sensitive suffered music will good favorite cool great genius earlier notes attention place putting superb style room beauty realize brought passionate difference fresh save musical grooves consists tapes depressing interview short rock appeared learn brothers considering pitched badly adding kiss books will book time religious liberal material interest utterly moves movie consistent false committed question turn coverage decade novel understood worst leader history kind energy dropped current doubt books building travel sudden fails wanted ghost presents honestly movies movie hour dont close previous features type months meaning wait boring absolutely truth generation going ﬁghting runs fantastic kids quiet kill lost angles previews crafted teens help believes brilliance touches hardcore continue album formula listed drink text electronics order attach replaced write impressed install learn tool offered details turns snap price digital well buds problems photos hear shoot surprisingly continue house card sports writing include adequate nice programming protected mistake response situations effects food night going haven sour avoid sugar coffee store bodied graham variety salsa reasons favorite delicate purpose brands worst litter funny partially sesame handle excited close awful happily fully effects virgin salt returned powdery meals matcha great bites table pistachios unlike prior works opinion spam fake review detection leveraging crude user behavioral shallow textual features reviews credibility classiﬁcation delve deep semantics reviews inconsistencies used explain review non-credible otherwise using facet models. given review snippets like hotel offers free wi-fi different facets present reviews along corresponding sentiment polarities. since work present model requiring limited prior information extract latent facets review text without help explicit facet seed words. ideal machinery wi-ﬁ latent facet cluster like network internet computer access .... also want extract sentiment expressed review facet. interestingly although free polarity example free conjunction wi-ﬁ expresses positive sentiment service offered without charge. hope although free individual polarity appears neighborhood words known polarities helps joint discovery facets sentiment labels free wi-ﬁ internet without extra charge ideally facet cluster similar polarities using co-occurrence similar words positive polarities. work joint sentiment topic model approach jointly discover latent facets along expressed polarities. consider reviews written users items rating assigned review review document consists sequence words denoted w...wnd word drawn vocabulary indexed consider facet assignments z...zk sentiment label assignments {ll...ll} possible facets label adds layer sentiment addition topics standard assumes document associated multinomial distribution facets sentiment labels symmetric dirichlet prior denotes probability occurrence facet polarity document topics multinomial distribution words drawn vocabulary symmetric dirichlet prior denotes probability word belonging facet polarity generative process sentiment label ﬁrst chosen document-speciﬁc rating distribution symmetric dirichlet prior thereafter chooses facet conditioned subsequently word conditioned algorithm outlines generative process. exact inference possible intractable coupling thus collapsed gibbs sampling approximate inference. user review facet description facet-label distribution different items differ; items certain facets important dimensions. instance battery life ease consumer electronics important color; hotels certain services available free charged elsewhere. similarly user reviews involving less relevant facets item discussion e.g. downrating hotels allowing pets also detected. word latent facet dimension consider sentiment label maximizes facet-label-word distribution aggregate words. dimension used feature vector facet-label distribution review classiﬁer ﬁgure importance different latent dimensions also captures domain-speciﬁc facet-label importance. user review rating rating assigned user item consistent opinion expressed review item. instance unlikely user assign average poor rating item expressed positive opinion important facets item review. inferred rating distribution review consisting sequence words learned computed word consider facet label jointly maximizes facet-label-word distribution aggregate words facets. absolute deviation user-assigned rating estimated rating user text taken component overall feature vector. user rating prior works dealing opinion spam fake reviews found kinds reviews tend express overtly positive overtly negative opinions. therefore also component overall feature vector detect cues extreme ratings. temporal burst typically observed group spamming number reviews posted targeting item short span time. consider reviews timepoints posted speciﬁc item. temporal burstiness review given item given user review item description general description facets outlined user review item differ markedly majority. instance user review says internet charged majority says hotel offers free wi-ﬁ presents possible inconsistency. facet model corresponds word clusters facet label different sentiment labels. experiments however feature play weak role presence inconsistency features. aggregate per-review facet distribution reviews item obtain facet-label distribution item. jensen-shannon divergence symmetric smoothed version kullback-leibler divergence feature. depicts much facet-label distribution given review diverges general opinion people item. addition consistency features also limited language user behavioral features. later show experiments features conjunction perform better individual feature classes. order capture distributional difference words deceptive authentic reviews consider unigram bigram language features shown outperform ﬁne-grained linguistic features using psycholinguistic features part-of-speech tags chapter iii.. discusses in-depth various linguistic features effective distinguishing credible reviews non-credible ones. earlier works review spam show user-dependent models detecting user-preferences biases perform well credibility analysis. however information always available especially newcomers active users community. besides show spammers tend open multiple fake accounts write reviews malicious activities using accounts sparsely avoid detection. therefore instead relying extensive user history simple proxies user activity easier aggregate community user rating behavior absolute deviation review rating mean median rating user items well ﬁrst three moments user rating distribution capturing scenario user typical rating behavior across items. item rating pattern absolute deviation item rating mean median rating obtained users captures extent user disagrees users item quality; ﬁrst three moments item rating distribution captures general item rating pattern. regularized loss dual formulation liblinear package default parameters. report classiﬁcation accuracy -fold cross-validation ground-truth tripadvisor yelp. scarcity ground-truth data pertaining review credibility suitable evaluate model examine effect non-credible reviews relative ranking items community. instance case popular items large number reviews even fraction non-credible effect would severe would long-tail items fewer reviews. simple goodness item aggregate ratings reviews using also obtain ranking items. model ﬁlter non-credible reviews aggregate ratings credible reviews re-compute item ranks. evaluation measures kendall-tau rank correlation co-efﬁcient effectiveness rankings reference ranking instance sales rank items amazon. measures number concordant discordant pairs whether ranks elements agree based scores total number combinations possible. given observations pair observations kendall-tau-b measure allows rank adjustment. consider number concordant discordant tied pairs tied pairs respectively whereby kendall-tau-b given however conservative estimate multiple items typically top-selling ones amazon rating therefore second estimate considers non-zero tied ranks concordant. note that item zero-rank reviews classiﬁed non-credible. high positive value kendall-tau indicates series positively correlated; whereas value close zero indicates independent. typical issue credibility analysis task scarcity labeled training data. ﬁrst task labels yelp spam filter train model. however ground-truth labels available amazon. although principle train model myelp yelp ﬁlter non-credible reviews amazon. transferring learned model yelp amazon entails using learned weights features yelp analogous ones amazon. however process encounters following issues facet distribution yelp different amazon therefore facet-label distribution corresponding learned feature weights yelp cannot directly used latent dimensions different. direct transfer model weights yelp amazon assumes distribution credible non-credible reviews corresponding feature importance domains necessarily true. order boost certain features better identify non-credible reviews amazon tune soft margin parameter svm. respectively. c-svm slack variables optimizes parameters provide trade wide margin made moving around certain points incurs penalty high value instance places large penalty mis-classifying instances negative class therefore boosts certain features class. value increases model starts classifying reviews non-credible. worse case reviews item classiﬁed non-credible leading aggregated item rating zero. optimal value using validation amazon shown figure observe increases also increases till certain point non-credible reviews ﬁltered stabilizes. previous approach uses model myelp trained yelp reference ranking amazon used evaluating item ranking using kendalltau measure. objective obtain good item ranking based credible reviews model mamazon directly optimizes kendall-tau using reference ranking training labels. allows entire feature space available amazon including explicit facet-label distribution full vocabulary could used earlier. feature space constructed similarly yelp. users reviews items resp. review text associated rating unigrams bigrams vocab. token types word token type review indicator presence/absence words facets sentiment labels resp. cardinality facets sentiment labels facet-label distr. review item resp. dirichlet priors review rating distr. inferred rating distr. word count reviews feature vec. review using lang. mated using techniques pairwise slack variables optimization problem equivalent classifying operating pairwise difference vectors corresponding labels indicating ranked ahead. implementation maximizes empirical kendall-tau minimizing number discordant pairs. parameter initialization sentiment lexicon consisting positive negative polarity bearing words used initialize review text based facet-labelhotels. data consists reviews positive negative sentiment credible non-credible reviews hotels. authors crawled credible reviews online review portals like tripadvisor; whereas non-credible ones generated users amazon mechanical turk. dataset review text sentiment label corresponding hotel names information users items. recommended reviews non-recommended reviews given yelp ﬁltering algorithm. annotated labels reviews yelp ﬁlter considered ground-truth work. found yelp spam ﬁlter primarily relies linguistic behavioral social networking features. additionally extract following information items three domains namely consumer electronics software sports. review gather information tuple yelp. however metadata dataset rich yelp consisting helpfulness votes reviews. further exists explicit ground-truth characterizing reviews credible deceptive amazon. re-rank items using approaches ﬁltering possible deceptive reviews compare ranking item sales rank considered pseudo ground-truth. language model baselines consider unigram bigram language model baselines shown outperform baselines using psycholinguistic features part-of-speech tags information gain etc. take best baseline work combination unigrams bigrams. proposed model enriches using length normalization presence absence features latent facets etc. recently proposed doc-to-vec model based neural networks overcomes weakness bag-of-words models taking context words account learns dense vector representation document train doc-to-vec model dataset baseline model. addition also consider readability review sentiment scores hypothesis writing styles would random diverse customer background. measures reader’s ability comprehend text measured function total number characters words sentences present review sentiment tries capture fraction occurrences positive/negative sentiment words total number words used. yelp dataset. speciﬁcally utilize number helpful feedbacks review title length review rating brand names percent positive negative sentiments average rating rating deviation features classiﬁcation. further based recent work also user check-in user elite status information additional features comparison. credible review classiﬁcation study performance various approaches distinguishing credible review non-credible one. since forms binary classiﬁcation task consider balanced dataset containing equal proportion data classes. yelp dataset item randomly sample equal number credible non-credible reviews tripadvisor dataset already balanced. table shows -fold cross validation accuracy results different models datasets. observe proposed consistency behavioral features exhibit around improvement yelp classiﬁcation accuracy best performing baselines since tripadvisor dataset review text user/activity models could used there. experiment could performed amazon well ground-truth credibility labels reviews absent. item ranking task examine effect non-credible reviews ranking items community. experiment performed amazon using item sales rank ground reference ranking yelp provide item rankings. sales rank provides indication well product selling amazon.com highlights item’s rank corresponding category. baseline item ranking based aggregated rating reviews item. ﬁrst model myelp trained yelp ﬁlters non-credible reviews aggregating review ratings item. second model mamazon trained amazon using svm-rank reference ranking training labels. -fold cross-validation results reported measures kendall-tau table respect reference ranking. svm-rank since ties. ﬁrst model performs substantially better baseline which turn outperformed second model. model kendall-tau-m rank correlation items less reviews different domains amazon observe model performs substantially well even items reviews performance progressively getting better reviews per-item. language model bigram language model performs well tripadvisor dataset artiﬁcial creation. workers amazon mechanical turk asked study hotel amenities websites write fake reviews them. result reviews closely follow actual hotel descriptions therefore quite difﬁcult facet model contradictions mismatch facet descriptions. consequently facet model gives marginal improvement combined language model. however bigram language model doc-to-vec perform well real-world naturally noisy yelp dataset previous one. facet model also perform well isolation. however components together give signiﬁcant performance improvement ones isolation incorporating writing style using sentiment measures improves performance doc-to-vec tripadvisor dataset. however improvements signiﬁcant real-world yelp data. also really just like perfect little good space pretty everything come_back still right deﬁnitely enough much super free around delicious fresh favorite selection sure friendly dish since huge menu large easy last room guests location time probably helpful great something nice small better sweet though loved happy love anything actually home dirty mediocre charged customer_service signature_lounge nice_place hotel_staff good_service never_go overpriced signature_room establishment architecture_foundation long waste food_great glamour_closet glamour food_service love_place terrible great_place wonderful bill will_never good_food management great_food money worst horrible manager service rude rank features joint model credibility classiﬁcation weights given c-svm show snapshot unigrams bigrams table observe credible reviews mostly contain function content words balanced opinions informative unigrams. non-credible reviews hand contain extreme opinions less function words sophisticated content words like signature bigrams catch readers’ attention. behavioral model activity based model perform best isolation combined language consistency features joint model exhibits around improvement performance. additional meta-data like user elite check-in status improves performance activity based baselines typically available newcomers community. model using limited information performs better activity baselines using ﬁne-grained information items user history. incorporating additional user features effectiveness facet based consistency features. remove consistency model aggregated dataset. tripadvisor dataset performance reduction less compared yelp reasons outlined before. table shows snapshot non-credible reviews corresponding consistency features yelp amazon. observe inconsistencies like ratings deceptive reviews corroborating textual description irrelevant facets inﬂuencing rating target item contradictions users expressing extreme opinions without explanation depicting temporal burst ratings etc. principle features also used detect anomalous phenomena like group-spamming scope work. never inside james. never checked never visited bar. favorite hotels chicago. james friendly area. loves there. evangelical christians working proselytize coffee farmers from. internet charged dollar hotel this. used turbo since now. can’t because turbo doesn’t software updates because hurricane katrina. book amazon offers joke provides forward written kalanithi. don’t sample writing know appeals. great. camera takes pictures. greati give starskeep dan’s apartment beautiful great downtown location... highly recommend working nsra... super friendly demonstrating conﬁdent... condo listing activity really stepped in... ranking task ranking task amazon ﬁrst model myelp trained yelp tested amazon using c-svm performs much better baseline exploiting various consistency features. second model mamazon trained amazon using svm-rank outperforms former exploiting power entire feature space domain-speciﬁc proxy labels unavailable former. long-tail items table shows gradual degradation performance second model mamazon dealing items lesser number reviews. nevertheless observe give substantial kendall-tau correlation reference ranking reviews per-item demonstrating effectiveness model dealing long-tail items. ﬁrst task propose approach predict helpful product reviews exploiting joint interaction user expertise writing style timeliness review consistency using hidden markov model latent dirichlet allocation. unlike prior works exploiting variety syntactic domain-speciﬁc features model uses information user reviewing item explicit timepoint perform task making approach generalizable across communities domains. additionally provide interpretable explanation review helpful terms salient words latent word clusters used experts describe important facets item consideration. thereafter second task harness various consistency features latent facet models analyze consistencies review description facets ratings timestamps credible product reviews limited information. additionally features help providing interpretable explanations review deemed non-credible. approach works well long-tail items newcomers community limited prior information history. develop multiple models domain transfer adaptation model performs well ranking tasks involving long-tail items reviews per-item. perform extensive experiments real-world reviews different domains amazon yelp tripadvisor demonstrate effectiveness approach state-of-the-art baselines. ﬁrst contribution dissertation develop novel forms probabilistic graphical models namely conditional random fields credibility analysis online communities. models jointly leverage context structure interactions sources users postings statements online communities ascertain credibility usercontributed information. capture complex interplay several factors writing style trustworthiness expertise users sources topics postings user-user user-item interactions etc. ﬁrst develop semi-supervised model credibility classiﬁcation postings statements partially supervised expert knowledge. apply framework healthcare domain extract rare unobserved side-effects drugs user-contributed postings online healthforums. problems large-scale non-expert data potential complement expert medical knowledge. furthermore develop continuous model ﬁne-grained credibility regression online communities deal user-assigned numeric ratings items. demonstrate usefulness news communities plagued misinformation bias polarization induced fairness style reporting political perspectives media sources users. model jointly identify objective news articles trustworthy media sources expert users credible postings. second contribution deals temporal evolution dynamics online communities where users join leave adapt evolving trends mature time. study temporal evolution collaborative ﬁltering framework recommend items users based experience maturity consume them. develop models experience evolution users online communities. ﬁrst models users evolve discrete manner employing hidden markov model latent dirichlet allocation captures change writing style vocabulary usage change users’ experience level. second addresses several drawbacks discrete evolution natural continuous evolution model users’ experience corresponding language model employing geometric brownian motion brownian motion latent dirichlet allocation. thereafter develop efﬁcient probabilistic inference techniques using metropolis hastings kalman filter gibbs sampling empirically shown smoothly continuously increase data log-likelihood time well fast convergence. experimentally show experience-aware user models perform item recommendation better state-of-the-art algorithms communities like beer movies food news. also model useful product reviews helpful end-users communities like amazon. third contribution method perform credibility analysis limited information especially long-tail items users limited history activity information. develop methods leveraging latent topic models analyze inconsistencies review texts ratings facet descriptions temporal bursts identify non-credible reviews. methods product review communities operate information user reviewing item explicit timepoint making approach generalizable across communities domains. also propose approaches domain transfer deal missing ground-truth information domain transferring learned models domains. fourth contribution deals providing user-interpretable explanations probabilistic graphical models used explain verdict. show distributional word clusters demonstrate usage words users varying experience trustworthiness discourse affective norms credible non-credible postings evolution traces users evolve time acquire community norms etc. proposed models especially ones product review communities operating user-user user-item item-item interactions fairly generic nature easily applicable communities domains. instance applied questionanswering forums reliable expert answers queries experts would want follow certain topics. also used crowdsourcing applications reliability user-contributed information. models also used analyze inconsistencies credible non-credible behavior detect anomalies frauds networks systems. proposed continuous conditional random field model aggregating information multiple users sources taking account expertise interactions used learning rank ensemble learning. prior works knowledge base construction dbpedia freebase mostly leverage structured information like wikipedia infoboxes category information etc. additionally also require manual curation maintain quality consistency consequently high precision coverage whereby store information mostly prominent entities. contrary crowd-sourced information noisy unstructured high coverage precision. bring together proposed models speciﬁcally semi-supervised conditional random field model learns partial expert knowledge used automatically construct large-scale structured unstructured content structured kbs. recently approach knowledge fusion using similar approach proposed many language features capturing subjectivity rationality information user postings manually identiﬁed using bias affective lexicons discourse relations etc. recent advances representation learning deep learning correspondence graphical models neural networks natural extension work automatically learn linguistic cues patterns credibility analysis joint embeddings context structure communities using neural networks. prior works truth-ﬁnding data fusion operate structured data. although dissertation relaxes many assumptions mostly geared online communities user item interactions. therefore future research address case arbitrary textual claims expressed freely open-domain setting withmaking assumptions structure claim characteristics community website claim made. thomas adler luca alfaro. content-driven reputation system wikipedia. proceedings international conference world wide banff alberta canada pages sören auer christian bizer georgi kobilarov jens lehmann richard cyganiak zachary ives. dbpedia nucleus open data. semantic international semantic conference asian semantic conference iswc aswc busan korea november pages tadas baltrusaitis peter robinson louis-philippe morency. continuous conditional neural fields structured regression. computer vision eccv european conference zurich switzerland september proceedings part pages david blei john lafferty. dynamic topic models. machine learning proceedings twenty-third international conference pittsburgh pennsylvania june pages david blei mcauliffe. supervised topic models. advances neural information processing systems proceedings twenty-first annual conference neural information processing systems vancouver british columbia canada december pages philip bohannon nilesh dalvi yuval filmus nori jacoby sathiya keerthi alok kirpal. automatic web-scale information extraction. proceedings sigmod international conference management data sigmod scottsdale pages kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data sigmod vancouver canada june pages markus bundschus mathäus dejori martin stetter volker tresp hanspeter kriegel. extraction semantic biomedical relations text using conditional random ﬁelds. bioinformatics vol. kevin robert canini bongwon peter pirolli. finding credible information sources social networks based content social structure. passat/socialcom privacy security risk trust ieee third international conference ieee third international conference social computing boston oct. pages carlos castillo marcelo mendoza barbara poblete. information credibility twitter. proceedings international conference world wide hyderabad india march april pages carlos castillo marcelo mendoza barbara poblete. information credibility twitter. proceedings international conference world wide hyderabad india march april pages di-rong chen qiang yiming ying ding-xuan zhou. support vector machine soft margin classiﬁers error analysis. journal machine learning research vol. pages cristian danescu-niculescu-mizil robert west jurafsky jure leskovec christopher potts. country members user lifecycle linguistic change online communities. international world wide conference janeiro brazil pages kushal dave steve lawrence david pennock. mining peanut gallery opinion extraction semantic classiﬁcation product reviews. proceedings international conference world wide pages york acm. dong evgeniy gabrilovich geremy heitz wilko horn kevin murphy thomas strohmann shaohua zhang. knowledge vault web-scale approach probabilistic knowledge fusion. sigkdd international conference knowledge discovery data mining york august pages harris drucker christopher burges linda kaufman alexander smola vladimir vapnik. support vector regression machines. advances neural information processing systems nips denver december pages patrick ernst cynthia meng gerhard weikum. knowlife knowledge graph health life sciences. ieee international conference data engineering chicago icde march april pages andrea esuli fabrizio sebastiani. sentiwordnet publicly available lexical resource opinion mining. proceedings conference language resources evaluation rong-en kai-wei chang cho-jui hsieh xiang-rui wang chih-jen lin. liblinear library large linear classiﬁcation. journal machine learning research vol. pages fang zhang nadia magnenat-thalmann. subjectivity grouping learning users’ rating behavior. international conference autonomous agents multi-agent systems aamas paris france pages geli arjun mukherjee bing meichun malú castellanos riddhiman ghosh. exploiting burstiness reviews review spammer detection. proceedings seventh international conference weblogs social media icwsm cambridge massachusetts july song feng ritwik banerjee yejin choi. syntactic stylometry deception detection. annual meeting association computational linguistics proceedings conference july jeju island korea volume short papers pages fogg. prominence-interpretation theory explaining people assess credibility online. extended abstracts conference human factors computing systems lauderdale florida april pages alban galland serge abiteboul amélie marian pierre senellart. corroborating information disagreeing views. proceedings third international conference search data mining wsdm york february pages stephan greene philip resnik. words syntactic packaging implicit sentiment. human language technologies conference north american chapter association computational linguistics proceedings june boulder colorado pages ramanathan guha ravi kumar prabhakar raghavan andrew tomkins. propagation trust distrust. proceedings international conference world wide york pages stephan günnemann nikou günnemann christos faloutsos. detecting anomalies dynamic rating data robust probabilistic model rating evolution. sigkdd international conference knowledge discovery data mining york august pages aditi gupta ponnurangam kumaraguru. credibility ranking tweets high impact events. proceedings workshop privacy security online social media psosm pages york acm. aditi gupta hemank lamba ponnurangam kumaraguru anupam joshi. faking sandy characterizing identifying fake images twitter hurricane sandy. international world wide conference janeiro brazil companion volume pages minqing bing liu. mining summarizing customer reviews. proceedings tenth sigkdd international conference knowledge discovery data mining seattle washington august pages nitin jindal bing liu. opinion spam analysis. proceedings international conference search data mining wsdm palo alto california february pages prateek jindal roth. end-to-end coreference resolution clinical narratives. ijcai proceedings international joint conference artiﬁcial intelligence beijing china august pages thorsten joachims. optimizing search engines using clickthrough data. proceedings eighth sigkdd international conference knowledge discovery data mining july edmonton alberta canada pages sepandar kamvar mario schlosser hector garcia-molina. eigentrust algorithm reputation management networks. proceedings twelfth international world wide conference budapest hungary pages byungkyu kang john o’donovan tobias höllerer. modeling topic speciﬁc credibility twitter. international conference intelligent user interfaces lisbon portugal february pages ioannis karatzas steven eugene shreve. brownian motion stochastic calculus. graduate texts mathematics. springer-verlag york berlin heidelberg autres tirages corriges soo-min patrick pantel timothy chklovski marco pennacchiotti. automatically assessing review helpfulness. emnlp proceedings conference empirical methods natural language processing july sydney australia pages yehuda koren. factorization meets neighborhood multifaceted collaborative ﬁltering model. proceedings sigkdd international conference knowledge discovery data mining vegas nevada august pages martin krallinger alfonso valencia lynette hirschman. linking genes literature text mining information extraction retrieval applications biology. genome biology vol. page rajasekar krishnamurthy yunyao sriram raghavan frederick reiss shivakumar vaithyanathan huaiyu zhu. information extraction. encyclopedia database systems pages srijan kumar robert west jure leskovec. disinformation impact characteristics detection wikipedia hoaxes. proceedings international conference world wide montreal canada april pages sejeong kwon meeyoung kyomin jung chen yajun wang. prominent features rumor propagation online social media. ieee international conference data mining dallas december pages himabindu lakkaraju chiranjib bhattacharyya indrajit bhattacharya srujana merugu. exploiting coherence simultaneous discovery latent facets associated sentiments. proceedings eleventh siam international conference data mining april mesa arizona pages cliff lampe kelly garrett. it’s news effect instruments ratings provision. hawaii international international conference systems science cd-rom abstracts proceedings january waikoloa island page thomas lavergne tanguy urvoy françois yvon. detecting fake content relative entropy scoring. proceedings ecai’ workshop uncovering plagiarism authorship social software misuse patras greece july quoc tomas mikolov. distributed representations sentences documents. proceedings international conference machine learning icml beijing china june pages seth lewis kelly kaufhold dominic lasorsa. thinking citizen journalism philosophical practical challenges user-generated content community newspapers. journalism practice vol. xian weiyi meng clement t-veriﬁer verifying truthfulness fact statements. proceedings international conference data engineering icde april hannover germany pages jiwei myle claire cardie. identifying manipulated offerings review proceedings conference empirical methods natural portals. language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group pages huayi zhiyuan chen bing xiaokai jidong shao. spotting fake reviews collective positive-unlabeled learning. ieee international conference data mining icdm shenzhen china december pages jiwei myle claire cardie eduard hovy. towards general rule identifying deceptive opinion spam. proceedings annual meeting association computational linguistics june baltimore volume long papers pages yaliang jing zhao jiawei han. resolving conﬂicts heterogeneous data truth discovery source reliability estimation. international conference management data sigmod snowbird june pages huayi zhiyuan chen arjun mukherjee bing jidong shao. analyzing detecting opinion spam large-scale dataset temporal spatial patterns. proceedings ninth international conference social media icwsm university oxford oxford pages yaliang jing zhao jiawei han. discovery evolving truth. proceedings sigkdd international conference knowledge discovery data mining sydney australia august pages ee-peng viet-an nguyen nitin jindal bing hady wirawan lauw. detecting product review spammers using rating behaviors. proceedings conference information knowledge management cikm toronto ontario canada october pages chenghua yulan joint sentiment/topic model sentiment analysis. proceedings conference information knowledge management cikm hong kong china november pages chenghua yulan richard everson. sentence subjectivity detection weakly-supervised learning. fifth international joint conference natural language processing ijcnlp chiang thailand november pages jingjing yunbo chin-yew yalou huang ming zhou. low-quality product review detection opinion summarization. emnlp-conll proceedings joint conference empirical methods natural language processing computational natural language learning june prague czech republic pages yang xiangji huang aijun xiaohui modeling predicting helpfulness online reviews. proceedings ieee international conference data mining december pisa italy pages chengxiang zhai neel sundaresan. rated aspect summarization short comments. proceedings international conference world wide madrid spain april pages panayiotis tsaparas alexandros ntoulas livia polanyi. exploiting social context review quality prediction. proceedings international conference world wide raleigh north carolina april pages michal lukasik srijith kalina bontcheva arkaitz zubiaga trevor cohn. hawkes processes continuous time sequence classiﬁcation application rumour stance classiﬁcation twitter. proceedings annual meeting association computational linguistics august berlin germany volume short papers fenglong yaliang minghui jing zhao heng jiawei han. faitcrowd fine grained truth discovery crowdsourced data aggregation. proceedings sigkdd international conference knowledge discovery data mining sydney australia august pages julian mcauley jure leskovec. hidden factors hidden topics understanding rating dimensions review text. seventh conference recommender systems recsys hong kong china october pages julian john mcauley jure leskovec. amateurs connoisseurs modeling evolution user expertise online reviews. international world wide conference janeiro brazil pages andrew mccallum kedar bellare fernando pereira. conditional random field discriminatively-trained finite-state string edit distance. proceedings conference uncertainty artiﬁcial intelligence edinburgh scotland july pages rada mihalcea carlo strapparava. detector explorations automatic recognition deceptive language. proceedings annual meeting association computational linguistics international joint conference natural language processing afnlp august singapore short papers pages david mimno andrew mccallum. topic models conditioned arbitrary features dirichlet-multinomial regression. proceedings conference uncertainty artiﬁcial intelligence helsinki finland july pages arjun mukherjee abhinav kumar bing junhui wang meichun malú castellanos riddhiman ghosh. spotting opinion spammers using behavioral footprints. sigkdd international conference knowledge discovery data mining chicago august pages arjun mukherjee vivek venkataraman bing natalie glance. yelp fake review filter might doing? proceedings seventh international conference weblogs social media icwsm cambridge massachusetts july subhabrata mukherjee gaurab basu sachindra joshi. incorporating author preference sentiment rating prediction reviews. international world wide conference janeiro brazil companion volume pages subhabrata mukherjee gaurab basu sachindra joshi. joint author sentiment topic model. proceedings siam international conference data mining philadelphia pennsylvania april pages subhabrata mukherjee gerhard weikum cristian danescuniculescu-mizil. people drugs credibility user statements health communities. sigkdd international conference knowledge discovery data mining york august pages subhabrata mukherjee hemank lamba gerhard weikum. experience-aware item recommendation evolving review communities. ieee international conference data mining icdm atlantic city november pages subhabrata mukherjee gerhard weikum. leveraging joint interactions credibility analysis news communities. proceedings international conference information knowledge management cikm melbourne australia october pages subhabrata mukherjee sourav dutta gerhard weikum. credible review detection limited information using consistency features. machine learning knowledge discovery databases european conference ecml pkdd riva garda italy september proceedings part pages subhabrata mukherjee kashyap popat gerhard weikum. exploring latent semantic factors find useful product reviews. proceedings siam international conference data mining houston texas april ndapandula nakashole mitchell. language-aware truth assessment fact candidates. proceedings annual meeting association computational linguistics june baltimore volume long papers pages nytimes.com. reddit blamed spreading smear? http//www.nytimes.com////magazine/should-reddit-beblamed-for-thespreading-of-a-smear.html. accessed myle yejin choi claire cardie jeffrey hancock. finding deceptive opinion spam stretch imagination. annual meeting association computational linguistics human language technologies proceedings conference june portland oregon pages myle claire cardie jeffrey hancock. negative deceptive opinion spam. human language technologies conference north american chapter association computational linguistics proceedings june westin peachtree plaza hotel atlanta georgia pages sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings annual meeting association computational linguistics july barcelona spain. pages pang vaithyanathan shivakumar lillian. thumbs sentiment classiﬁcation using machine learning techniques. proceedings conference empirical methods natural language processing emnlp jeff pasternack roth. knowing believe coling international conference computational linguistics proceedings conference august beijing china pages jeff pasternack roth. making better informed trust decisions generalized fact-finding. ijcai proceedings international joint conference artiﬁcial intelligence barcelona catalonia spain july pages michael paul mark dredze. drug extraction summarizing drug experiences multi-dimensional topic models. human language technologies conference north american chapter association computational linguistics proceedings june westin peachtree plaza hotel atlanta georgia pages geraldine peterson parisa aslani kylie williams. consumers search appraise information medicines internet? qualitative study using focus groups. journal medical internet research vol. page vahed qazvinian emily rosengren dragomir radev qiaozhu mei. rumor identifying misinformation microblogs. proceedings conference empirical methods natural language processing emnlp july john mcintyre conference centre edinburgh meeting sigdat special interest group pages tie-yan xu-dong zhang de-sheng wang hang global ranking using continuous conditional random fields. advances neural information processing systems proceedings twenty-second annual conference neural information processing systems vancouver british columbia canada december pages vladan radosavljevic slobodan vucetic zoran obradovic. continuous conditional random fields regression remote sensing. ecai european conference artiﬁcial intelligence lisbon portugal august proceedings pages mahmudur rahman bogdan carbunar jaime ballesteros duen horng chau. catch fake curbing deceptive yelp ratings venues. statistical analysis data mining vol. pages daniel ramage christopher manning susan dumais. partially labeled topic models interpretable text mining. proceedings sigkdd international conference knowledge discovery data mining diego august pages marta recasens cristian danescu-niculescu-mizil jurafsky. linguistic models analyzing detecting biased language. proceedings annual meeting association computational linguistics august soﬁa bulgaria volume long papers pages michal rosen-zvi thomas grifﬁths mark steyvers padhraic smyth. author-topic model authors documents. proceedings conference uncertainty artiﬁcial intelligence banff canada july pages michal rosen-zvi thomas grifﬁths mark steyvers padhraic smyth. author-topic model authors documents. proceedings conference uncertainty artiﬁcial intelligence banff canada july pages benjamin snyder regina barzilay. multiple aspect ranking using good grief algorithm. human language technology conference north american chapter association computational linguistics proceedings april rochester york pages swapna somasundaran janyce wiebe. recognizing stances online debates. proceedings annual meeting association computational linguistics international joint conference natural language processing afnlp august singapore pages dhanya sridhar lise getoor marilyn walker. collective stance classiﬁcation posts online debate forums. joint workshop social dynamics personal attributes social media carlo strapparava alessandro valitutti. wordnet affect affective proceedings fourth international conference extension wordnet. language resources evaluation lrec lisbon portugal fabian suchanek gjergji kasneci gerhard weikum. yago core semantic knowledge. proceedings international conference world wide banff alberta canada pages fabian suchanek gerhard weikum. knowledge harvesting text sources. ieee international conference data engineering icde brisbane australia april pages huan alex morales xifeng yan. synthetic review spamming defense. sigkdd international conference knowledge discovery data mining chicago august pages jiliang tang huiji huan liu. context-aware review helpfulness rating prediction. seventh conference recommender systems recsys hong kong china october pages ivan titov ryan mcdonald. joint model text aspect ratings sentiment summarization. proceedings annual meeting association computational linguistics june columbus ohio pages peter turney. thumbs thumbs down? semantic orientation applied unsupervised classiﬁcation reviews. proceedings annual meeting association computational linguistics pages stroudsburg association computational linguistics. vinod vydiswaran chengxiang zhai roth. content-driven trust propagation framework. proceedings sigkdd international conference knowledge discovery data mining diego august pages v.g. vinod vydiswaran chengxiang zhai roth. gauging internet doctor ranking medical claims based community knowledge. proceedings workshop data mining medicine healthcare dmmh pages york acm. vinod vydiswaran chengxiang zhai roth peter pirolli. biastrust teaching biased users controversial topics. international conference information knowledge management cikm’ maui october november pages marilyn walker pranav anand abbott ricky grant. stance classiﬁcation using dialogic properties persuasion. human language technologies conference north american chapter association computational linguistics proceedings june montréal canada pages hanna wallach iain murray ruslan salakhutdinov david mimno. evaluation methods topic models. proceedings annual international conference machine learning icml montreal quebec canada june pages xuerui wang andrew mccallum. topics time non-markov continuoustime model topical trends. proceedings twelfth sigkdd international conference knowledge discovery data mining philadelphia august pages hongning wang chengxiang zhai. latent aspect rating analysis review text data rating regression approach. proceedings sigkdd international conference knowledge discovery data mining washington july pages guan wang sihong bing philip review graph based online store review spammer detection. ieee international conference data mining icdm vancouver canada december pages hongning wang chengxiang zhai. latent aspect rating analysis without aspect keyword supervision. proceedings sigkdd international conference knowledge discovery data mining diego august pages robert west hristo paskov jure leskovec christopher potts. exploiting social network structure person-to-person sentiment analysis. tacl vol. pages white harpaz shah dumouchel horvitz. toward enhanced pharmacovigilance using patient-generated data internet. clinical pharmacology therapeutics vol. pages ryen white eric horvitz. health search healthcare explorations intention utilization query logs user surveys. journal american medical informatics association vol. pages janyce wiebe ellen riloff. creating subjective objective sentence classiﬁers unannotated texts. computational linguistics intelligent text processing international conference cicling mexico city mexico february proceedings pages janyce wiebe ellen riloff. finding mutual beneﬁt subjectivity analysis information extraction. ieee transactions affective computing vol. pages liang xiang quan yuan shiwan zhao chen xiatian zhang qing yang jimeng sun. temporal recommendation graphs longshort-term preference fusion. proceedings sigkdd international conference knowledge discovery data mining washington july pages liang xiong chen tzu-kuo huang jeff schneider jaime carbonell. temporal collaborative filtering bayesian probabilistic tensor factorization. proceedings siam international conference data mining april columbus ohio pages qiongkai zhao. using deep linguistic features finding deceptive opinion spam. coling international conference computational linguistics proceedings conference posters december mumbai india pages hong junichi tsujii eric i-chao chang. feature engineering combined machine learning rule-based methods structured information extraction narrative clinical discharge summaries. jamia vol. pages xiaoxin jiawei philip truth discovery multiple conﬂicting information providers web. ieee transactions knowledge data engineering vol. pages june kyung hyan ulrike gretzel. comparison deceptive truthful travel reviews. information communication technologies tourism enter proceedings international conference amsterdam netherlands pages hong vasileios hatzivassiloglou. towards answering opinion questions separating facts opinions identifying polarity opinion sentences. proceedings conference empirical methods natural language processing emnlp pages stroudsburg association computational linguistics. jianxing zheng-jun meng wang tat-seng chua. aspect ranking identifying important product aspects online consumer reviews. annual meeting association computational linguistics human language technologies proceedings conference june portland oregon pages zhao benjamin rubinstein gemmell jiawei han. bayesian approach discovering truth conﬂicting sources data integration. proceedings vldb endowment vol. pages zhao wenzhu tong jing dian heng jiawei han. modeling truth existence truth discovery. proceedings sigkdd international conference knowledge discovery data mining sydney australia august pages xiaojin zoubin ghahramani john lafferty. semi-supervised learning using gaussian fields harmonic functions. machine learning proceedings twentieth international conference august washington pages", "year": 2017}