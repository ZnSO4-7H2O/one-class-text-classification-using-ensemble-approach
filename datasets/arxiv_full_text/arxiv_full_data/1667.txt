{"title": "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy  Gradient", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Learning a goal-oriented dialog policy is generally performed offline with supervised learning algorithms or online with reinforcement learning (RL). Additionally, as companies accumulate massive quantities of dialog transcripts between customers and trained human agents, encoder-decoder methods have gained popularity as agent utterances can be directly treated as supervision without the need for utterance-level annotations. However, one potential drawback of such approaches is that they myopically generate the next agent utterance without regard for dialog-level considerations. To resolve this concern, this paper describes an offline RL method for learning from unannotated corpora that can optimize a goal-oriented policy at both the utterance and dialog level. We introduce a novel reward function and use both on-policy and off-policy policy gradient to learn a policy offline without requiring online user interaction or an explicit state space definition.", "text": "learning goal-oriented dialog policy generally performed ofﬂine supervised learning algorithms online reinforcement learning additionally companies accumulate massive quantities dialog transcripts customers trained human agents encoder-decoder methods gained popularity agent utterances directly treated supervision without need utterance-level annotations. however potential drawback approaches myopically generate next agent utterance without regard dialoglevel considerations. resolve concern paper describes ofﬂine method learning unannotated corpora optimize goal-oriented policy utterance dialog level. introduce novel reward function on-policy off-policy policy gradient learn policy ofﬂine without requiring online user interaction explicit state space deﬁnition. companies increasingly interested building goal-oriented dialog systems domains customer service reservation systems. difference building chatbots industry academia companies usually possess vast amount dialogs conversation transcripts customers trained human agents. example customer service live chat systems usually operated human agents agents take weeks training courses deemed qualiﬁed handle customer issues. thus agent utterances dialogs treated actions produced near-optimal policy machine learning algorithms. build industry-quality goal-oriented chatbots important direction research effectively utilizing corpora trained agent-customer transcripts learn dialog policies ofﬂine minimizing dependence domain knowledge corresponding human effort. recently several supervised learning algorithms proposed learn goal-oriented dialog policies algorithms agent utterances treated labels models trained maximize likelihood agent utterances. intuitive supervised learning models trained optimize myopic rewards simply imitating agent utterances training data problematic suffer compounding errors multi-turn dialogs reinforcement learning hand natural choice learn goal-oriented dialog policies optimizes long-term cumulative reward selects best action achieve goal given current state. recently many rl-based algorithms proposed achieved good results however main drawbacks directly applying reinforcement learning dialog policy learning algorithms learning dialog managers generally trained online requiring interaction real humans user simulators training naturally suited fully utilize large tact corpora proposed algorithms require predeﬁning discrete actions slots state action representation requires signiﬁcant human effort domain knowledge thus readily generalizing domains. overcome ﬁrst problem pre-train models ofﬂine supervised learning methods ﬁne-tune policies online real humans loop addition still requiring human interaction pre-training also requires human effort annotate agent utterance training data pre-deﬁned action annotate entities pre-deﬁned slots kandasamy proposed batch policy gradient algorithm weighs trajectories dataset importance sampling. thus learn dialog policy ofﬂine suffer large gradient variance importance sampling. secondly algorithm also requires reward signals exist dataset necessarily case practice. overcome problem requiring pre-deﬁned action space several works proposed using encoder-decoder neural architecture learn model directly generating dialog responses. approaches enable end-to-end learning need domain knowledge deﬁne action state space easily generalize different domains. however similar previously mentioned supervised learning algorithms algorithms myopically optimize likelihood next utterance neglect overarching goal dialog. also tends generate generic responses inspired recent rl-based algorithms sequence prediction propose rl-based end-to-end dialog policy learning algorithm overcome drawbacks. speciﬁcally model agent response generation markov decision process episode corresponds sequence words agent utterance. note different deﬁned related literature episode corresponds sequence dialog acts. known transition function deﬁned reward function learn optimal policy ofﬂine on-policy algorithm without interacting real users. adopt neural encoder-decoder architecture parameterize policy algorithm trained unannotated data without deﬁning dialog acts slots still reap beneﬁts latest encoder-decoder architectures. main contributions paper follows propose novel reward function takes account utterance-level dialog-level rewards. reward function guides algorithm optimize next agent utterances also overall dialog goals. proposed algorithm learns dialog policy unannotated dataset ofﬂine without interacting real users. enables utilize vast amount existing dialogs tacts makes building industry-quality chatbots possible. dialog policy learning action prediction. line work algorithms ﬁrst predict agent’s next action generate agent utterances based action. supervised learning algorithms treat action class perform multi-class classiﬁcation. example bordes weston williams proposed memory network recurrent neural network underlying multi-class classiﬁcation model respectively. usually algorithms followed training reinforcement learning reﬁne model williams achieved state-of-the-art babi dialog dataset neural architecture relying injecting domain-speciﬁc knowledge constraints action masks. meanwhile paper focus learning dialog policy without domain knowledge. many algorithms also proposed usually algorithms need interact real users user simulators makes difﬁcult scale industryquality applications. batch reinforcement learning algorithms proposed however algorithms requires annotated agent actions rewards often available. dialog policy learning sequence prediction. algorithms line work generate agent utterances token token given dialog context usually algorithms adopt encoder-decoder architecture additional belief tracker latent intent variable outside dialog management rl-based sequence prediction algorithms proposed. speciﬁcally ranzato bahdanau policy gradient actor critic learn generate sequences machine translation document summarization. however directly applied dialog generation methods would optimize reward next utterance instead overarching goal dialog. proposed rl-based dialog generation algorithm optimizes rewards information semantic coherence focusing open-domain dialogs. kandasamy proposed batch policy gradient method train dialog policies ofﬂine existing dataset. proposed algorithm sequence prediction method differs work assume rewards available dataset. instead make realistic assumption trained agent utterances dataset high quality treated targets learn case tact datasets. represent dialog training dataset sequence pairs number turns user utterance turn agent utterance turn deﬁne context dialog turn concatenation utterances usually goal-oriented dialog dataset calls issued agents external systems. treat call agent utterance starts special api_call token followed parameters call. given context dialog algorithm generates agent utterance token token. formulate process markov decision process action space agent’s vocabulary. time action performed token vocabulary generated. sequence tokens generated time then state deﬁned concatenation dialog context tokens already generated. stochastic policy produces distribution tokens vocabulary based current state parameters policy learn. token sampled distribution state deterministically transitioning terminal states states last generated token special token. denote generated agent utterance token end. length denoted shortly describe deﬁne reward function mdp. goal learning algorithm maximize cumulative rewards generated sequences deﬁned section hidden states note algorithm agnostic choice underlying model used parameterize policy; thus state-of-the-art encoder-decoder techniques different attention mechanisms easily applied improve model. natural choice optimize model parameters on-policy policy gradient setting algorithm explores action space generates agent utterance scored pre-deﬁned reward function problem action space agent token vocabulary quite large extremely difﬁcult effectively explore order learn good policy. solve problem ranzato bahdanau proposed start optimizing cross-entropy loss slowly deviate model explore. paper propose combine on-policy policy gradient off-policy policy gradient show actions dataset high returns actions tact datasets off-policy policy gradient speed convergence overall algorithm. moreover off-policy learning enables utilize existing rewards dataset. since transition function deﬁned known deﬁne reward function based generated utterance dialog dataset on-policy policy gradient learn optimal policy. deﬁne types rewards utterance-level rewards dialog-level rewards utterancelevel rewards capture quality generated agent utterances compared existing agent utterances training data. example reward functions include semantic distance utterances simply cosine similarity. experiments bleu scores derive utterance-level rewards popular metrics machine translation shown correlate well human judgement. dialog-level rewards hand captures contribution generated utterances achieving dialog goals. observe goal-oriented dialogs usually driven issuing calls database knowledge base. example scenario customer wants return product bought shopping website customer service agent ﬁrst issue call pull customer’s proﬁle. then agent needs check whether customer eligible refund. finally agent issues another call start refund process goal dialog achieved. calls usually parameters. example parameters ﬁrst call could customer’s email address parameters second could order produce calls usually logged available training. critical policy predict calls parameters calls correctly. call issued later training data increases number turns dialog decrease user experience. worse case call issued earlier training data since risk guess parameters call. worst case call never issued goal dialog missed. results dialog-level reward function deﬁne gives different negative rewards calls made late early positive reward correct parameter call made time. reward assigned last action intermediate actions reward result reward signals sparse learning process slow. similar bahdanau reward shaping deal problem. incomplete utterances deﬁne potential function reward last action action state space large on-policy algorithms converge slowly. reason on-policy algorithms explore experience actions high returns learn pick good actions. however case number possible sequences generate |a|t size agent token vocabulary on-policy exploration difﬁcult. meanwhile tact datasets actions generated trained agents expected high returns. dialogs datasets demonstrations good trajectories sensible reduce exploration efforts using information. paper propose off-policy policy gradient exploit information. offpolicy policy gradient essentially maximize probability actions dataset weighted importance sampling ratios returns actions. denoting i-th token length state represented similar on-policy setting. off-policy policy gradient estimated unknown. solve problem kandasamy proposed train another model estimate small lead high variance gradients. deal problem clip value simply constant value note coefﬁcient constant value utterance-level rewards off-policy policy gradient reduces optimizing cross-entropy loss meaning proposed algorithm outperform underlying encoder-decoder reasonable reward function deﬁned. babi dialog task dataset experiments. babi dialog task converted dialog state tracking challenge context restaurant search. goal dialog recommend restaurant based user’s preferences. dialog agent asked users questions preferences type cuisine location price range. users also initiate dialog providing preferences. agent issued call knowledge base returned list candidate restaurants attributes rating phone number address etc. restaurant highest rating recommended user. user information phone number address restaurants. user update preferences dialog multiple calls dialog. although agent utterances dataset generated trained human agents generated well-performed hand-crafted dialog policy. therefore agent utterances still treated ground truth learning purposes. data contains transcripts dialogs includes agent utterances user utterances knowledge base responses calls. train/validation/test contains dialogs respectively. size vocabulary average number utterances dialog average number records returned calls record tuple consists restaurant name attribute name attribute value. beneﬁt algorithm end-to-end training. experiments trained data without data pre-processing normalizing tokens replacing entities special tokens similar procedures. also knowledge base responses encoder algorithm learn pick restaurant highest rating. note knowledge base responses long lists restaurants attributes dramatically increases length dialog contexts longest dialog contexts training data contains tokens average length contexts length longest agent utterance training data average length agent utterances purpose using data evaluate performance algorithm without domain knowledge. used model hyper-parameters described eric manning encoderdecoder based model directly compare results. encoder single layer bi-directional lstm decoder single layer lstm. word embedding size number units lstm input dropout keep rate used adam optimizer learning rate importance sampling coefﬁcient constant value maximum length decoding sequence reward function since evaluating algorithm existing dataset instead real humans issuing call early late equally terms metrics convex combination on-policy off-policy gradient selected model performed best validation dataset reported performance model test dataset. report following metrics response accuracy accuracy predicting next agent utterance. prediction correct predicted tokens match corresponding ones dataset. bleu score generated utterances. precision recall score issuing calls. prediction considered true positive call correctly predicted call even parameters call wrong. call exact match within true positives issuing calls accuracy predicting every parameter correctly. baseline algorithm used experiment attention-based seqseq model trained using hyper-parameters algorithm. also include performance memory network reported bordes weston performance copy-augmented seqseq model reported eric manning experiment results shown table observe algorithm improved bleu score compared attention-based seqseq model compared eric manning shows learned policy achieved better utterance-level performance. furthermore easily update reward function optimize utterancelevel metrics combination different metrics. algorithm also improved score issuing calls signiﬁcantly improved accuracy calls’ parameters compared attention-based seqseq model. mentioned before goal-oriented dialogs usually driven calls. accordingly better performance call metrics implies better goal-oriented policy. finally algorithm improved per-response accuracy compared attention-based seqseq compared eric manning paper propose rl-based algorithm learn goal-oriented dialog policy. algorithm enables following useful contributions deﬁne reward function algorithm optimize utterance-level dialog-level metrics. learn rl-based dialog policies fully utilizing tact datasets without interacting real users. improve sample efﬁciency on-policy policy gradient incorporating off-policy policy gradient. parameterize policy encoder-decoder architecture enables end-to-end learning domain-speciﬁc knowledge. compared recently proposed methods algorithm achieved better performance utterance-level dialog-level metrics babi dialog dataset. algorithm excels scenario exists large amount tacts common case industry setting. therefore believe work important step towards building industry-quality chatbots.", "year": 2017}