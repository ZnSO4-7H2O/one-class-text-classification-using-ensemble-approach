{"title": "Kullback-Leibler Penalized Sparse Discriminant Analysis for  Event-Related Potential Classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "A brain computer interface (BCI) is a system which provides direct communication between the mind of a person and the outside world by using only brain activity (EEG). The event-related potential (ERP)-based BCI problem consists of a binary pattern recognition. Linear discriminant analysis (LDA) is widely used to solve this type of classification problems, but it fails when the number of features is large relative to the number of observations. In this work we propose a penalized version of the sparse discriminant analysis (SDA), called Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This method inherits both the discriminative feature selection and classification properties of SDA and it also improves SDA performance through the addition of Kullback-Leibler class discrepancy information. The KLSDA method is design to automatically select the optimal regularization parameters. Numerical experiments with two real ERP-EEG datasets show that this new method outperforms standard SDA.", "text": "brain computer interface system provides direct communication mind person outside world using brain activity event-related potential -based problem consists binary pattern recognition. linear discriminant analysis widely used solve type classiﬁcation problems fails number feawork tures large relative number observations. propose penalized version sparse discriminant analysis called kullback-leibler penalized sparse discriminant analysis method inherits discriminative feature selection classiﬁcation properties also improves performance addition kullback-leibler class discrepancy information. klsda method design automatically select optimal regularization parameters. numerical experiments real erp-eeg datasets show method outperforms standard sda. brain computer interface system measures brain activity converts artiﬁcial output able replace restore improve normal output used person communicate control his/her external internal environment. thus signiﬁcantly improve quality life people severe neuromuscular disabilities communication brain person outside world appropriately established means system based eventrelated potentials manifestations neural activity consequence certain infrequent relevant stimuli. main reason using erp-based non-invasive requires minimal user training quite robust main components erps wave positive deﬂection occurring scalp-recorded approximately stimulus applied. wave unconsciously generated latency amplitude vary diﬀerent records person even more records diﬀerent persons using oddball paradigm erp-based decode desired commands subject detecting erps background eeg. words erp-based deals pattern recognition problem classes involved without several authors investigated regards classiﬁcation technique performs best separating target non-target records studies concluded linear discriminant analysis good classiﬁcation scheme resulting times optimal performance keeping solution simple. timely mention classiﬁcation scheme main problems curse-of-dimensionality bias-variance trade-oﬀ former consequence working concatenation multiple time points multiple channels latter depicts generalization capability classiﬁer. however shown performs poorly high dimensional data small training sample used. diﬀerent regularized solutions proposed demonstrated regularized version substantially improve classiﬁcation performance standard lda. however still much room improvement. regards present work propose achieve signiﬁcant gain building model takes account following issues which knowledge jointly considered previous works imize robustness achieving good classiﬁcation performance. timely remember generalizability model tends decrease sparsity decreases i.e. number parameters model increases mind work develop penalized version sparse discriminant analysis call kullback-leibler penalized sparse discriminant analysis main objective solving binary classiﬁcation problem. know never used erp-based classiﬁcation problems. also shall compare penalized version standard context. organization article follows. section made brief review discriminant analysis. proposed approach presented full general formulation section section erp-eeg databases used experiments described. section experiments results erp-based classiﬁcation problem solved klsda shown. finally concluding remarks future works presented section criterion well-known dimensionality reduction tool context supervised classiﬁcation. popularity mainly simplicity robustness lead high classiﬁcation performances many applications method consists ﬁnding discriminant vectors projecting data matrix directions classes well separated other. assumed random vectors independently normally distributed common covariance matrix procedure ﬁnding vectors requires estimates within-class between-class total covariance matrices respectively. estimates given matrix whose columns method seeks vectors maximize separability classes achieved simultaneously maximizing minimizing since equivalent simultaneously maximizing minimizing thus known fails number patterns relation number variables situation matrix becomes usually ill-conditioned produces poor estimation consequence classiﬁcation performance. special case known fisher linear discriminant analysis flda approach formulated linear regression model n-dimensional vector depending whether observation belonging class class respectively consider following ordinary least squares problem whose solutions vectors denotes moore-penrose generalized inverse denotes null space invertible unique solution given convenience assumed therefore given since proportionality constant several works cite few) extended olslda formulation multiclass problems. shown solution obtained multivariate regression particular hastie introduced richer ﬂexible classiﬁcation scheme called optimal scoring brieﬂy describe below. introduced regularized version optimal scoring problem adding penalization terms functional penalization terms side induce sparsity side allow correlated variables solution. regularized formulation named consists recursively solving following problem problem alternately iteratively solved follows. ﬁrst hold ﬁxed optimization performed respect hold ﬁxed optimization performed respect following steps iterated solving involves well-known elastic-net problem similar lasso since e-net performs automatic sparse variable selection also allows selection groups correlated variables -norm term). work propose penalized version method eﬃciently solve binary classiﬁcation problem appearing systems based erps. method seeks increase classiﬁcation performance taking account information diﬀerence classes means inclusion appropriate anisotropy matrices penalizing terms. adaptive penalizers particular anisotropy matrices regularization method inverse ill-posed problems approach shown produce signiﬁcantly better results obtained corresponding non-adaptive isotropic penalizers pattern recognition problem important analyze data able extract much prior information possible since needless good classiﬁcation performance largely depend well problem understood available data. vector solution inherits correlated variables selection sparsity properties also contains components appropriate discriminative information suitable improving separability classes. before classiﬁcation following subsection show kullback-leibler divergence used constructing anisotropy matrices properly incorporate discriminative information klsda. discriminative information incorporated klsda appropriately quantifying distances classes precisely probability distributions. although wide variety metrics comparing probability distributions shall well-known kullback-leibler divergence decision particular metric nice mathematical properties also fact already successfully applied many classiﬁcation problems suppose ﬁrst dealing binary classiﬁcation problem discrete random variable deﬁned discrete crepancy metric rigorous mathematical sense symmetric satisfy triangle inequality. reason symmetry desired modiﬁed called j-divergence deﬁned follows information sample large value means sample contains signiﬁcant amount discriminative information deﬁnitely want take account constructing solution vectors section show detail j-divergence able highlight discriminative samples. available priori discriminative information incorporated klsda formulation means appropriately constructed anisotropy matrices since wish stand samples containing signiﬁcant amounts discriminative information matrices must constructed strongly penalize samples little none discriminative information avoiding penalization remaining ones. computational implementation klsda bellow made appropriately modifying original algorithm thus klsda mainly solved steps. ﬁrst step generalized version e-net problem solved. second step consists updating optimal score vector solving shown j−π)π−yt well-known every regularization method choice regularization parameters crucial. tikhonov-type functionals popular widely used method approximating optimal parameters called l-curve criterion. main advantages selection criterion fact require prior knowledge noise. roughly speaking method ﬁnds optimal compromise norm residual norm regularized solution selecting point maximal curvature log-log plot quantities parametrized regularization parameter. details despite popularity l-curve method cannot directly applied multi-parameter penalization functionals like belge proposed extension l-curve technique approximating optimal regularization parameters cases called l-hypersurface technique brieﬂy describe next. denote regularization parameter vector. deﬁne l-hypersurface associated problem subset deﬁned parametrized point maximal gaussian curvature l-hypersurface although approximating times costly computational point view authors show good approximation given minimizer residual norm i.e. vector satisfying minλ∈rm although generalized e-net deﬁned terms possible choices tuning parameters example -norm coeﬃcients chosen instead fact achieved re-writing lasso version generalized e-net since lars-en algorithm forward stagewise additive ﬁtting procedure number steps algorithm also used tuning parameter replacing because ﬁxed lars-en produces ﬁnite number vectors approximations true solution step numerical experiment adopted tuning parameters accordance belge’s remark described above best solutions selected minimizing residual norm. steps algorithm solving klsda proposal presented algorithm dataset- open-access speller database laboratorio investigaci´on neuroimagenolog´ıa universidad aut´onoma metropolitana mexico d.f. described database consists records acquired healthy subjects recorded channels sampling rate using g.tec gusbamp. acquisition records ﬁltered chebyshev notch order ﬁlter cutoﬀ frequencies chebyshev band-pass order ﬁlter cutoﬀ frequencies -by- matrix containing letters numbers presented subject computer screen. experiment subject asked spell different words. person focus character time. stimulus column matrix randomly highlighted period inter-stimuli intervals stimulating block every every column matrix intensiﬁed once. character spelled stimulating block repeated times. person well concentrated relevant event occurred chosen character illuminated i.e. elicited. thus binary classiﬁcation problem -by- matrix generates twelve possible events labelled target. subject participated sessions ﬁrst copyspelling runs contained true label data vector characters spelled. reason present work used copyspelling sessions dataset. records ﬁltered order forward-backward butterworth band-pass ﬁlter. data segment extracted records beginning stimulus. total patterns obtained neuroelectrical imaging laboratory irccs fondazione santa lucia rome italy. data recorded using g.mobilab g.ladybird active electrode eight channels channels referenced right earlobe grounded left mastoid. signals digitized eight participants required copy-spell seven predeﬁned words characters using speller paradigm. dataset- -by- matrix containing alphanumeric characters used. rows columns matrix randomly intensiﬁed follow inter stimulus interval length. stimulating block rows columns intensiﬁed times. details dataset refer reader pre-processing stage records channel bandpass ﬁltered using order butterworth ﬁlter. then data segments extracted beginning intensiﬁcation. speller dataset consists patterns section show klsda method implemented context aforementioned real erp-based classiﬁcation problem datasets described section since dealing binary classiﬁcation problem direction vector linear classiﬁer implemented codes matlab six-core memory. timely mention although cpu-time needed direction vector around secs found training testing linear classiﬁer projected data vector took secs. experiments decided symmetric version measure discrepancy. probability distribution class estimated using histogram centered bins minimum maximum value target non-target training data. constructed anisotropy matrix diag c/jkl constant makes small vice-versa. matrix cannot formally deﬁned above. case however overcomed simply replacing small. anisotropy matrix deﬁned clearly diagonal symmetric positive deﬁnite. four diﬀerent conﬁgurations klsda denoted klsda klsda klsda klsda implemented. ﬁrst klsda correspond standard approach automatics parameter selection. second third ones klsda klsda incorporate anisotropic matrix penalizers respectively. finally klsda incorporate anisotropic matrix terms. reasonable think appropriate measure enhancing impact wave klsda solution selecting discriminative channels discriminative time samples. mind databases j-divergence sample estimated described section analysis j-divergence function channel time allowed detect samples discriminative ones. corresponding plots selected subjects dataset- selected subjects dataset- presented figure note although subjects discriminative samples mostly located window cases discriminative samples somewhat randomly distributed plot. moreover cases channels j-divergence shows contribution class separation figure also shows high variability morphology subjects pointed section diﬀerent available measures evaluating classiﬁcation method receiver operator characteristics curve powerful tool evaluating two-class unbalanced problem present figure shows classiﬁcation results obtained subject dataset- klsda klsda klsda klsda methods. several remarks order. first method results best classiﬁcation performance seems subject dependent. second observation poorest classiﬁcation performance corresponds precisely subjects randomly spread j-divergence. ﬁnal observation nineteen twenty cases least trivial klsda conﬁguration outperforms pure remaining cases performances essentially equal. average classiﬁcation results klsda conﬁguration presented last column figure figure dataset- dataset respectively. worth noting dataset- average classiﬁcation result dataset- klsda conﬁguration. results encouraging since eﬃcient system requires accuracy allow communication device control also used datasets test flda classiﬁer -fold cross-validation. expected curse-of-dimensionality classiﬁcation results poor. fact dataset- average classiﬁcation performance around dataset- near results clearly indicate regularization improves classiﬁcation performance. given solution sparsity desired mean number nonzeros values klsda conﬁguration analysed. klsda klsda klsda klsda methods mean percentage non-zero values respect number sample points dataset- found respectively dataset- values found note dataset- sparsity consistently signiﬁcantly lower. fact figure area curve test data dataset- derived klsda klsda klsda klsda evaluated -fold cross-validation. errorbar subject denotes standard deviation -fold. errorbar average denotes standard deviation subject. figure area curve test data subject dataset- klsda conﬁguration. errorbar subject denotes standard deviation -fold. errorbar average denotes standard deviation subject. probably reﬂects fact patients database involved complex patterns. finally highly remarkable klsda conﬁgurations percentages non-zero values achieved satisfactory classiﬁcation performances. observation allows conclude klsda constitutes robust classiﬁer method. figure depicts morphology solution vectors subject dataset- four diﬀerent klsda conﬁgurations. parentheses number non-zero values solution vector shown. note number higher anisotropy matrix used penalization term also amplitudes coeﬃcients increase cases. good classiﬁcation performances. number observations signiﬁcantly lower dimensionality performs poorly reason several alternatives method proposed. present work described diﬀerent approaches statistical literature developed penalized sparse discriminant analysis method called kullback-leibler penalized sparse discriminant analysis. method inherits good properties also allows incorporate kld-based discriminative information order enhance classiﬁcation performance. important highlight implementation klsda incorporates automatic tuning parameter selection. light sparsity degree solution classiﬁcation performances obtained procedure proved adequate choosing regularization parameters model. tested klsda approach real erp-eeg datasets. analysis classiﬁcation results indicates klsda conﬁguration leading best performance subject depended. particular classiﬁcation results dataset- show adding information solution necessarily result beneﬁcial specially provides clear discriminative information. regard this shall compare results ones obtained using another discriminant information measure asymmetric ones presented cite few. remarkable cases klsda outperformed fact experiments computational cost added. found sda’s computational cost even higher klsda’s anisotropic penalizing terms results achieved applying klsda context classiﬁcation problems high enough ensure good communication brain person device controlled. results encourage continue research eﬀorts. clearly much room improvement. research currently underway several directions. instance diﬀerent discrepancy measures anisotropy matrices penalizing terms considered. ﬁnal remark that although klsda method inspired idea solving binary classiﬁcation problem systems clearly applied type classiﬁcation problem. work supported part consejo nacional investigaciones cient´ıﬁcas t´ecnicas conicet force oﬃce scientiﬁc research afosr/soard grant fa--- universidad nacional litoral caid-unl project within pact se˜nales sistemas inteligencia computacional.", "year": 2016}