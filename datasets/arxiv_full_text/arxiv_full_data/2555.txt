{"title": "Towards Practical Bayesian Parameter and State Estimation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Joint state and parameter estimation is a core problem for dynamic Bayesian networks. Although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models, the silver bullet---an efficient and general online inference algorithm for such problems---remains elusive, forcing users to write special-purpose code for each application. We propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has following advantages: (a) it is efficient due to its online nature, and (b) it is applicable to both discrete and continuous parameter spaces . On a variety of toy and real models, our system is able to generate more accurate results within a fixed computation budget. This preliminary evidence indicates that the proposed approach is likely to be of practical use.", "text": "joint state parameter estimation core problem dynamic bayesian networks. although modern probabilistic inference toolkits make relatively easy specify large practically relevant probabilistic models silver bullet—an efﬁcient general online inference algorithm problems—remains elusive forcing users write special-purpose code application. propose novel blackbox algorithm hybrid particle ﬁltering state variables assumed density ﬁltering parameter variables. following advantages efﬁcient online nature applicable discrete continuous parameter spaces variety real models system able generate accurate results within ﬁxed computation budget. preliminary evidence indicates proposed approach likely practical use. many problems scientiﬁc studies natural realworld applications involve modelling dynamic processes often modeled dynamic probabilistic models online state parameter estimation –computing posterior probability states parameters incrementally time– crucial many applications simultaneous localization mapping real-time clinical decision support process control. various sequential monte-carlo state estimation methods introduced real-world applications challenge estimate parameters state jointly dpms complex dependencies nonlinear dynamics. real-world models involve discrete continuous variables arbitrary dependencies rich collection nonlinearities distributions. existing algorithms either apply restricted class models expensive time complexity dpms real-world applications often large number observations need large number particles perform inferential task accurately requires inference engine efﬁcient scalable. much success creating generic inference algorithms learning systems non-dynamical models remains open dpms. black-box inference system dpms needs elements practically useful general effective joint state/parameter estimation algorithm efﬁcient implementation inference engine. paper propose practical online solution general combined state parameter estimation problem dpms. developed state parameter estimation compiler arbitrary dpms described declarative modelling language blog spec equipped black inference algorithm named assumed parameter inference hybrid particle ﬁltering state variables assumed density ﬁltering parameter variables. spec also geared optimizing compiler generate application speciﬁc inference code efﬁcient inference real-world applications. contribution paper follows proposed general online algorithm joint state parameter estimation regardless whether parameters discrete continuous whether dynamics linear nonlinear; developed black inference engine spec equipped arbitrary dpms. spec also utilizes modern compilation techniques speedup inference practical applications; conducted experiments demonstrated spec’s superior organize paper follows section reviews state parameter estimation literature section describes algorithm introduces black inference engine spec section provides experiment results. discussion given section finally conclude paper section overview methods parameter estimation provided plain particle ﬁlter fails estimate parameters inability explore parameter space. problem particularly severe high-dimensional parameter spaces particle ﬁlter would require exponentially many particles sufﬁciently explore parameter space. various algorithms proposed achieve combined state parameter estimation task however issue still remains open existing algorithms either suffer bias computational efﬁciency. artiﬁcial dynamics approach although computationally efﬁcient applicable arbitrary continuous parameter models results biased estimates fails intricate models considered paper. resample-move algorithm utilizes kernel moves target invariant. however method requires computation time step leading gilks berzuini propose move rate proportional asymptotically constanttime updates. fearnhead storvik lopes proposed sampling time step models ﬁxed dimensional sufﬁcient statistics. however arbitrary models generally accept sufﬁcient statistics models sufﬁcient statistics algorithms require sufﬁcient statistics explicitly deﬁned user. extended parameter ﬁlter generates approximate sufﬁcient statistics polynomial approximation however requires hand-crafted manual approximations. goldstandard particle markov chain monte carlo sampler introduced andrieu converges true posterior suitable building black inference engine biips black-box engine user purely work machine learning research without worrying implementation algorithms problem comes along. however note pmcmc ofﬂine algorithm unsuitable real-time applications. moreover poor mixing properties turn necessitates launching ﬁltering processes substantially many times extremely expenstate variables unobserved observations assumed conditionally independent observations given assume section states observations vectors dimensions respectively. model parameter continuous discrete. algorithm assumed parameter inference approximates posterior density particles following framework sequential monte-carlo methods. time step particle path samt approximate representation parametric family particles used approximately represent state parameters additional samples particle path utilized perform moment-matching operations required assumed density approximation explained section proposed method illustrated algorithm notice propagation step using bootstrap proposal density i.e. transition probability. particle ﬁltering methods better proposal distributions improve performance algorithm. approximating exploiting family basis distributions. algorithm expressed update function. update function generates approximating density minimizing kldivergence target basis integrals approximated monte carlo summation described section another alternative deterministic sampling. since multivariate gaussian gaussian quadrature rules utilized. context expectation-propagation proposed zoeter heskes context gaussian ﬁltering similar quadrature ideas applied well polynomial order calculated exactly gausshermite quadrature quadrature points. hence required moment matching integrals equation approximated arbitrarily well using quadrature points. unscented transform speciﬁc gaussian quadrature rule would deterministic samples approximating integral involving p-dimensional multivariate gaussian. case samples approximating time step incoming data point approximate posterior distribution tractable compact distribution approach inspired assumed density ﬁltering state estimation link function. thus exponential family update function computes moment matching integrals update canonical parameters integrals general intractable. propose approximating integral monte carlo samples sampled follows computing summations intractable highdimensional models hence propose using monte carlo summation described subsection experiments section consider simultaneous localization mapping problem discrete. discussed above form ﬁxed performed dynamic probabilistic model computing sampling viable. implemented inference engine spec ﬂexible probabilistic programming interface. spec utilizes syntax blog highlevel modelling language deﬁne probabilistic models spec analyzes model automatically generates model-speciﬁc inference code c++. user model written spec. fraction approximated gaussian moment matching weights normalk−)dθ describes well m-th mixture component explains data. mixture component explains data well up-weighted vice versa. resulting approximated density would recursions updatsimilar gaussian case integrals generally intractable. either monte carlo gaussian quadrature rule utilized approximately update means covariances. consider p-dimensional parameter space parameter take values. discrete parameter spaces always track constant time update fashion since posterior evaluated ﬁnitely many points. number points however grows exponentially |θ|p. hence tracking sufﬁcient statistics becomes computationally intractable increasing dimensionality. discrete particle ﬁltering framework often memoryintensive applications large amount data. moreover inefﬁcient memory management inference engine also result tremendous overhead runtime turn hurts estimation accuracy within ﬁxed amount computational budget. black-box inference engine also integrates following compilation optimizations handling practical problems. memory pre-allocation systems manage memory well repeatedly allocate memory time example dynamically allocating memory particles erase memory belonging ones iteration. introduces signiﬁcant overhead since memory allocation extremely slow. contrast spec analyzes input model allocate minimum static memory computation user speciﬁes particles markov order input model spec allocate static memory particles target code. iteration starts utilize rotational array re-use memory previous particles. spec also avoids dynamic memory allocation much possible intermediate computation step. example consider multinomial distribution. parameters change straightforward implementation update parameters re-allocate chunk memory storing parameters pass object multinomial distribution. spec dynamic memory allocation operation also avoided pre-allocating static memory store parameters. efﬁcient resampling resampling step critical particle ﬁlter algorithms since requires large number data copying operations. since single particle might occupy large amount memory real applications directly copying data particles ones induce substantial overhead. spec every particle access data indirect pointer. result redundant memory copying operations avoided copying pointers referring actual particle objects resampling. note particle might need store multiple pointers dealing models markov order larger spec also enhances program locality speed resampling. compiled code indexes array stores pointers carefully aligned take advantage memory locality pointers copied. experiment dealing small models resampling step takes large fraction overall running time spec achieves speedup fastest benchmark toolkit pmcmc. choosing spec analyzes static parameters model compiles different approximation distributions different types random variables. current stage spec supports gaussian mixtures gaussian approximate continuous variables multinomial distribution discrete variables. approximation distributions development. sampling approach default spec assumes parameters independent uses deterministic sampling possible. user could also spec generate random samples instead. furthermore number approximation samples number mixtures also speciﬁed. default time space complexity time steps generating particles well extra samples particle path update sufﬁcient statistics moment matching integrals. setting adequately crucial performance. small prevents exploring state space sufﬁciently whereas small leads inaccurate sufﬁcient statistics updates turn result inaccurate parameter estimation. note taking mcmc iterations pmcmc algorithm also complexity particles time steps. however pmcmc typically requires large amount mcmc iterations mixing properly small sufﬁcient produce accurate parameter estimation. moreover actual running time often much smaller theoretical upper bound notice approximation computation requires local data single particle approximation results inﬂuence weight particle. hence important optimization specialized spec resamples particles prior approximation step iteration updates approximation distribution particles disappear resampling step. notice often case small fraction particles signiﬁcantly large weights. hence practice shown experiment section overhead extra samples causes negligible overhead comparing plain particle ﬁlter. contrast theoretical time complexity tight pmcmc. experiments three benchmark models nonlinear dynamical model single continuous parameter; slam simultaneous localization bayesian learning problem discrete parameters; bird -parameter model track migrating birds real world data. compare estimation accuracy liu-west ﬁlter pmcmc within compiled inference spec. also compare spec system state-of-the-art toolboxes including biips libbi. since biips libbi support discrete variable conditional-dependency model able compare model. evaluate mean square error estimation results trials within ﬁxed amount computation time given generated data points liu-west ﬁlter pmcmc api. note algorithms producing samples ground truth point estimate. take mean samples produced liu-west last time step. pmcmc m-mcmc iterations take mean last samples leave ﬁrst half burn-in. choose default setting gaussian approximation samples. pmcmc experiment different number particles proposal local proposal truncated gaussian distribution. also perform grid search variance proposal report best one. figure estimation accuracy different algorithms various parameters implementations model. produces order-of-magnitude accurate estimation liuwest pmcmc. pmcmc spec faster libbi. error within amount running time quickly produces accurate estimation using particles seconds still smaller best pmcmc result seconds. pmcmc libbi utilize particles. within seconds produces mcmc samples spec ﬁnishes iterations. pmcmc biips mcmc step particle takes within spec could already produce high quality estimation. parallel particle filter libbi libbi supports advanced optimization choices including vectorization multi-thread versions. experimented advanced versions chose fastest figure single-thread compiler option. also experiment particles model libbi’s multi-thread option. parallel versions still slower single thread particles. particles multi-thread versions twice faster single-thread version. note even particles inference code generated spec still faster parallel versions libbi. practice parallelization often incurs signiﬁcant communication memory overhead especially gpu. also resampling step non-trivial come efﬁcient parallel compilation approach particle ﬁltering. experiment demonstrates importance memory management practical setting memory efﬁcient computations even sequential implementation much faster parallel version. bimodal variant deﬁnes unimodal posterior parameter. slightly modifying model follows order explore algorithm’s term bimodal. execute particles using mixtures gaussian approximation. also pmcmc particles using truncated gaussian proposal. pmcmc minutes ensure mixing. measure performance well pmcmc. histograms samples illustrated figure comparing result pmcmc indeed approximates posterior well. note even particles liu-west ﬁlter cannot produce bimodal posterior. able single mode. successfully ﬁnds modes posterior distribution though weights accurate case implies increasing number mixtures used approximation helps improving probability ﬁnding different modes posterior distribution. considering simultaneous localization mapping example modiﬁed deﬁned -dimensional grid cell static label observed robot. formally vector discrete random variables neither robot’s location observed. existing observations label cell robot’s current location action chosen robot. given action left robot moves direction action probability stays current location probability prior product individual cell priors uniform. figure kl-divergence true posterior estimation results produced different algorithms. pmcmc trapped local model quickly produces order-ofmagnitude accurate estimation particles. robot observes label cell currently located correctly probability incorrectly probability original example actions taken. experiment enhance model setting duplicating actions several times ﬁnally derive sequence actions. compare estimation accuracy particle ﬁlter pmcmc within spec. notice liuwest ﬁlter applicable scenario artiﬁcial dynamics approach performed continuous parameters. pmcmc iteration resample single parameter using unbiased bernoulli distribution proposal distribution. approximation samples fully factorized discrete distribution approximation. since still possible compute exact posterior distribution algorithms within various time limmeasure kl-divergence estimated distributions exact posterior. results figure show drastically suffers sample impoverishment problem; pmcmc fails local mode suffers poor mixing rates successfully approximates true posterior even particles. note also measure running time plain particle ﬁlter model. uses particles. although theoretically would times slower uses particles merely slower choices parameters experiment different parameters evaluate average kl-divergence trials. results figure agree theory. increases kl-divergence constantly decreases whereas certain point much gain obtained increasing fact enough moment matching integrals less exactly computed error monte carlo error induced projection step. projection induced error cannot avoided. bird migration problem originally investigated proposes hidden markov model infer bird migration paths large database observations. apply particle ﬁltering framework bird migration model using dataset ofﬁcially released. dataset eastern continent u.s.a partitioned grid. roughly birds totally observed dataset. grid total number birds observed days within years. infer number birds migrating different grid locations consecutive days. bird model continuous parameters dynamic states time step contains observed variables hidden variables. measure mean squared estimation error trials gaussian approximation liu-west ﬁlter pmcmc truncated gaussian proposal distribution within different time limits. results shown right part figure show achieves much better convergence within much tighter computational budget. similar sampling time step ﬁght sample impoverishment. discussed before methods suffer ancestral path degeneracy number particles large enough exists represented single unique particle. dynamic models long memory lead poor approximation sufﬁcient statistics turn affect posterior parameters. poyiadjis showed even favorable mixing assumptions variance additive path functional computed particle approximation grows quadratically time. ﬁght path degeneracy resort ﬁxed-lag smoothing smoothing. olsson used ﬁxed-lag smoothing control variance estimates. moral proposed time step forward smoothing algorithm leads variances growing linearly instead quadratically. poyiadjis similarly proposed algorithm leads linearly growing variances. similarly full kernel move rate also beneﬁcial. another important matter consider convergence assumed density ﬁltering posterior true posterior opper winther analyzed convergence behavior gaussian projection case. analysis convergence moment matching integrals computed approximately monte carlo sums. however experiments indicate approximations sufﬁciently many monte carlo samples similar convergence behavior attained. heess investigated approximating moment matching integrals robustly efﬁciently context expectation-propagation. train discriminative models learn message inputs outputs. idea seems promising applied setting well. paper present inference algorithm state parameter estimation dynamic probabilistic models. also developed black-box inference engine spec performing arbitrary models described high-level modeling language spec. spec leverages multiple compiler level optimizations efﬁcient computation achieves speedup existing toolboxes. experiment produces order-of-magnitude accurate estimation result compared pmcmc within ﬁxed amount computation time able handle real-world applications efﬁciently accurately. arulampalam sanjeev maskell simon gordon neil clapp tim. tutorial particle ﬁlters on-line nonlinear/non-gaussian bayesian tracking. ieee transactions signal processing boyen xavier koller daphne. tractable inference complex stochastic processes. proceedings fourteenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. elmohamed kozen dexter sheldon daniel collective inference markov models modeling bird migration. advances neural information processing systems gilks walter berzuini carlo. following moving target monte carlo inference dynamic bayesian models. journal royal statistical society. series smith novel approach nonlinear/non-gaussian proceedings bayesian state estimation. radar signal processing http//ieeexplore.ieee.org/stamp/ stamp.jsp?arnumber=. kantas nikolas doucet arnaud singh sumeetpal maciejowski chopin nicolas particle methods parameter estimation state-space models. statistical science olsson jimmy capp´e olivier douc randal moulines eric sequential monte carlo smoothing application parameter estimation nonlinear state space models. bernoulli poyiadjis george doucet arnaud singh sumeetpal sindhu. particle approximations score observed information matrix state space models application parameter estimation. biometrika issn http //www.jstor.org/stable/. todeschini adrien caron franc¸ois fuentes marc legrand pierrick moral pierre. biips software bayesian inference interacting particle systems. arxiv preprint arxiv. zoeter onno heskes tom. gaussian quadrature based expectation propagation. ghahramani cowell proceedings tenth international workshop artiﬁcial intelligence statistics society artiﬁcial intelligence statistics", "year": 2016}