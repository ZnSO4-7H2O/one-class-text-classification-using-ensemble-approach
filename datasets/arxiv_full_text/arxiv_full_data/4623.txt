{"title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression  Approach for Discovering Latent Task Parametrizations", "tag": ["cs.LG", "cs.AI"], "abstract": "Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. In the control setting, we show that a learned HiP-MDP rapidly identifies the dynamics of a new task instance, allowing an agent to flexibly adapt to task variations.", "text": "control applications often feature tasks similar identical dynamics. introduce hidden parameter markov decision process framework parametrizes family related dynamical systems low-dimensional latent factors introduce semiparametric regression approach learning structure data. control setting show learned hip-mdp rapidly identiﬁes dynamics task instance allowing agent ﬂexibly adapt task variations. many control applications involve repeated encounters domains similar identical dynamics. agent swinging encounter several bats different weights lengths agent manipulating encounter different amounts liquid. agent driving encounter many different cars unique handling characteristics. scenarios makes little sense agent start afresh encounters car. exposure variety related domains correspond faster reliable adaptation instance type domain. agent already swung several bats example would hope could easily learn swing bat. why? like many domains bat-swinging domain low-dimensional representation affects system’s dynamics structured ways. agent’s prior experience allow learn model related instances domain—such bat’s length latent parameter smoothly changes bat’s dynamics—and speciﬁc model parameters likely. domains closely-related dynamics interesting regime transfer learning. introduce hidden parameter markov decision process formalization types domains important features. first posit exist bounded number latent parameters that known would fully specify dynamics. second assume parameter values remain ﬁxed task’s duration agent know change occurred hip-mdp parameters encode minimum amount learning required agent adapt domain instance. given generative model latent parameters affect domain dynamics agent could rapidly identify dynamics particular domain instance maintaining updating distribution latent parameters. instead learning policy domain instance could synthesize parametrized control policy based point estimate parameter values plan belief space parameters present method learning structure hip-mdp data. generative model uses indian buffet processes model latent parameters relevant particular dynamics gaussian processes model dynamics functions. require knowledge system’s kinematics equations must specify number latent parameters advance. hip-mdp model efﬁciently performs control challenging acrobot domain rapidly identifying dynamics instances. bayesian reinforcement learning reinforcement learning setting consists series interactions agent environment. state agent chooses action transitions state γtrt] discount factor weighs relative importance near-term long-term rewards. series interactions modeled markov decision process -tuple sets states actions transition function gives probability next state performing action state reward function gives reward performing action state refer transition function dynamics system. transition function reward function must learned experience. bayesian approaches reinforcement learning place prior transition function reﬁne prior experience. thus problem learning unknown transformed problem planning known partially-observable markov decision process pomdp consists -tuple sets states actions observations transition reward functions; observation function probability receiving observation taking action state bayesian learns planning pomdp states fully-observed world-state hidden dynamics however solving pomdps high-dimensional continuous state spaces remains challenging despite advances pomdp planning including situations mixed-observability hip-mdp introduce section simpliﬁes bayesian challenge using instances related tasks ﬁrst low-dimensional representation transition function indian buffet processes gaussian processes speciﬁc instantiation hip-mdp uses models bayesian nonparametric statistics. ﬁrst indian buffet process prior matrices potentially unbounded number columns generate samples prior ﬁrst beta bounded. then entry independently probability property distribution number nonzero columns columns popular others used rows. prior latent parameters relevant predicting certain transition output. second model gaussian process prior continuous functions prior probability outputs given inputs given multivariate gaussian mean function gaussian process covariance matrix elements positive deﬁnite kernel function additive mixtures gaussian processes model transition function. focus learning dynamics assume reward function ﬁxed across instances denote instance domain. hidden parameter markov decision process posits variation dynamics different instances captured hidden parameters hip-mdp described tuple sets states actions reward function. dynamics instance depends value hidden parameters θb). denote possible parameters prior parameters. thus hip-mdp describes class tasks; particular instance class obtained ﬁxing parameter vector speciﬁcally assume drawn beginning task instance change beginning next instance. special cases able derive analytic expressions affects dynamics example manipulation domain might able derive kinematic equations respond force given certain volume liquid. however situations simpliﬁcations required derive analytical forms dynamics brittle best. ibp-gp prior hip-mdp presented section describes semiparametric approach modeling dynamics places assumptions form transition function still maintaining computational tractability. pointed similar setting consider hip-mdp type pomdp hidden state parameters however hip-mdp makes assumptions stronger pomdp. first instance hip-mdp mdp—conditioned transition reward functions obey markov property. thus could always learn solve hip-mdp instance distinct mdp. second parameter vector ﬁxed duration task thus hidden state dynamics. considerably simpliﬁes procedure inferring hidden parametrization. weights values associated latent parameters instance ﬁlter parameters zkad denote whether latent parameter relevant making predictions dimension taking action task-speciﬁc basis functions fkad describe change latent factor affects dynamics. additivity assumption semi-parametric basis function regression allows learn latent elements model number factors weights ﬁlter parameters zkad form basis functions fkad. ibp-gp prior encodes assumption domain instance real world contains countably inﬁnite number independent scalar latent parameters wkb. however making predictions speciﬁc state dimension given action inﬁnite possible latent parameters relevant. thus need infer values ﬁnite number weights characterize dynamics ﬁnite number state dimensions ﬁnite actions. using prior ﬁlter parameters zkad implies expect latent factors relevant making predictions. moreover additional prediction tasks—such action dimension state space—can incorporated consistently. regression model ibp-gp model inﬁnite version semiparametric latent factor model batch inference focus scenarios agent given large amount batch observational data several domain instances tasked quickly performing well instances. batch inference procedure uses observational data ﬁlter parameters zkad basis functions fkad independent posterior weights gaussian given ﬁlter settings zkad means µwk. however marginalb number data collected instance instead represented function fkad pairs states support points various optimization procedures exist choosing support points found iteratively choosing support points existing points minimize maximum reconstruction error within batch best setting large errors result poor performance. given tuples task instance ﬁrst created tuples actions dimensions instances tuple predicted based data available action dimension pair using standard gaussian process prediction collection tuples action instance ks∗sab vector every d−sd. procedure repeated ksabsab matrix vector differences every task instance following preprocessing step proceeded infer ﬁlter parameters zkad weights values task-speciﬁc basis functions support points fkad using blocked gibbs sampler. given zkad posterior fkad gaussian. column vector concatenated fkad vectors column vector concatenated vectors. mean covariance given kronecker product ks∗s∗ matrix every write matrix elements zkad repeat process action dimension similarly given zkad fkad posterior also gaussian. instance mean vector latent parameter values instances combine likelihoods prior section sample zkad. initializing latent parameter k—that already values weights wkb— involves computing marginal likelihood |zkad fkad) intractable. approximate likelihood sampling sets weights wkb. given values weights instance compute likelihood basis function fkad marginalized using equations average likelihoods estimate marginal likelihood zkad case variance equation zkad samples weights samples based importance weight online filtering given values ﬁlter parameters zkad basis functions fkad posterior weights gaussian. thus write parametrized belief time information mean µwpw. update given experience tuple inverse covariance given diagonal noise matrix updates therefore inverse performed every time steps simply extend respectively. computing requires computing values basis functions point since values computed pseudo-input points standard prediction equation interpolate value point kss∗ vector every ks∗s∗ matrix fkad vector fkad. using mean value fkad ignores uncertainty basis function fkad. incorporating variance mathematically straight-forward—all updates remain gaussian—it adds additional computations online calculation. found using means already provided signiﬁcant gains learning practice. section describe results benchmark problems cartpole acrobot tests anisotropic squared-exponential kernel length scale parameters approximated action dimension cartpole cartpole task begins pole initially standing vertically cart. agent apply force either left right cart keep pole falling over. domain four-dimensional state space consisting cart’s position cart’s velocity pole’s angle pole’s angular velocity time step length system evolves according following equations varied pole mass pole length cartpole simple enough domain changing parameters change optimal policy—if pole falling left cart moved left; pole falling right cart moved right. however used cartpole demonstrate quality predictions describe latent parameters. training setting agent received batches data sarsa repetitions episodes episode iterations pole fell down. data seven settings reduced support points batch inference procedure repeated times iterations gaussian process hyper-parameters ﬁrst batch. next training points selected sarsa settings online inference procedure used estimate weights given ﬁlter parameters zkad basis functions fkad batch procedure. quality predictions evaluated test points. hip-mdp predictions compared using batch data single using training points current instance combining training points batch data together relative test log-likelihoods summarized ﬁgure table summarizes corresponding meansquared errors across runs. using training points instance highest error across dimensions ibp-gp approach performs similarly applying single batch data predicting change outputs dimensions depend previous values pole mass length approach signiﬁcantly outperforms baselines outputs whose changes depend parameters system. mcmc runs total latent parameters inferred. output dimensions consistently used ﬁrst feature—that ibp-gp’s predictions fact using single observation consistent cartpole dynamics observed prediction errors ﬁgure table second feature used positively correlated pole mass pole length equations many terms. third consistently discovered feature used positively correlated pole mass correlated pole length equations several terms depend fourth feature highest variability; generally higher values extreme length settings suggesting might correction complex nonlinear effects. acrobot acrobot domain features double-pendulum. agent apply positive negative neutral torque joint poles goal swing pendulum vertical. four-dimensional state space consists angle angular velocity ﬁrst segment angle angular velocity second segment batch training used mass settings settings employing sarsa first support points chosen minimize maximum prediction error within batch. next used approach section infer ﬁlter parameters zkad approximate map-estimates basis functions fkad. table shows mean-squared prediction errors using evaluation procedure cartpole. hipmdp slightly higher mean-squared errors angle predictions ibp-gp approach better overall lower mean-squared errors angular velocity predictions. predicting angular velocities critical planning acrobot task inaccurate predictions make agent believe reach swing-up position quickly physically possible. acrobat policy learned mass setting generally perform poorly another. settings repeated trials ﬁltered weight parameters episode updated policy episode. even mostly observed setting ﬁnding full bayesian solution pspace-complete ofﬂine approximation techniques active area current research avoid complexity performed planning using sarsa dynamics based mean weight parameters ﬁrst obtained initial value function using prior mean weights interleaved model-based planning reinforcement learning using simulated episodes planning episode interaction. results acrobot shown ﬁgure compare performance planning using true model planning using average model obtain initial value function. average hip-mdp model reach performance near quite high using true model. however hip-mdp model already near performance episode. using learned bases batches well learning weights ﬁrst episode lets quickly snap near similar hidden-parameter setting treated pomdp perform bayesian planning; assume model given whereas task learn hip-mdp similar pomdps ﬁxed hidden states example pomdps used slot-ﬁlling dialogs difference however objective hip-mdp simply gather information perform well task transfer learning—the goal hip-mdp—has received much attention reinforcement learning. directly related spirit approach hidden-goal mdps hierarchical model-based transfer settings agent must determine current discrete possible mdps rather continuous parameter settings. related technical perspective representation transfer approaches typically learn basis functions sufﬁcient representation value function deﬁned speciﬁc state space transfer different representations task. contrast hip-mdp focuses modeling dimensions variation family related tasks. ibp-gp prior relates body work multiple-output gaussian processes though focused learning convolution kernel model several related outputs single task rather parameterizing several related tasks. gaussian process latent variable models used dimensionality reduction dynamical systems. multi-output models however gp-lvms time-varying lowdimensional representation single system characterize instance systems stationary low-dimensional vector. focus efforts also modeling rather control. extensive work scaling inference gaussian process models provides several avenues relaxing approximations made work settings initial batch data several instances fully bayesian treatment ﬁlter parameters zkad basis functions fkad might allow agent accurately navigate exploration-exploitation trade-offs. exploring uncertainties important model—and not—is important question future work. extensions within particular model include applying clustering sophisticated hierarchical methods group together basis functions fkad thus share statistical strength. example might expect opposing actions cartpole could decomposed similar basis functions. machine learning approaches control often train agents expect repeated experiences domain. however accurate model agent experience repeated domains vary limited speciﬁc ways. setting traditional planning approaches typically rely models fail large inter-instance variation. contrast reinforcement learning approaches typically assume dynamics instance completely unknown fail leverage information related instances. hip-mdp model explicitly models inter-instance variation providing compromise standard paradigms control. hip-mdp model useful family domains parametrization small relative model objectives remains similar across domains. many applications—such handling similar objects driving similar cars even managing similar network ﬂows—ﬁt scenario batch observational data related tasks easy obtain advance. cases able generalize dynamics interactions operating regime step building controllers exhibit robust reliable decision making gracefully adapting situations.", "year": 2013}