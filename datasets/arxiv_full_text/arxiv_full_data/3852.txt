{"title": "Revisiting Activation Regularization for Language RNNs", "tag": ["cs.CL", "cs.NE"], "abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.", "text": "recurrent neural networks serve fundamental building block many sequence tasks across natural language processing. recent research focused recurrent dropout techniques custom cells order improve performance. require substantial modiﬁcations machine learning model underlying conﬁgurations. revisit traditional regularization techniques specifically regularization activations slowness regularization successive hidden states improve performance rnns task language modeling. techniques require minimal modiﬁcation existing architectures result performance improvements comparable superior complicated regularization techniques custom cell architectures. regularization techniques used without modiﬁcation optimized lstm implementations nvidia cudnn lstm. need effective regularization methods rnns seen extensive focus recent years. application dropout input output shown effective dropout destructive naively applied recurrent connections rnn. naive dropout applied recurrent connections almost impossible retain information long periods time. given fundamental issue substantial work gone understanding improving dropout applied recurrent connections. techniques shall broadly refer recurrent dropout speciﬁc variations gained popular usage. variational rnns drop network units timestep opposed dropping different network units timestep. performing dropout units timestep destructive loss hidden state avoided information masked timestep. rather dropping units another tactic drop upsemeniuta dates given network units. perform dropout input gate lstm allow forget gate discard portions existing hidden state. zoneout prevents hidden state updates occurring setting randomly selected subset network unit activations equal previous activations prevent updates hidden state preserving existing content. extreme work also done restrict recurrent matrices order limit computational capacity. architectures allow element-wise interactions removing recurrent matrix entirely others restrict capacity parameterizing recurrent matrix forms regularization explicitly upon activations batch normalization recurrent batch normalization layer normalization introduce additional training parameters complicate training process increasing sensitivity model. norm stabilization penalizes model norm rnn’s hidden state changes substantially timesteps achieving strong results character language modeling phoneme recognition. work revisit regularization form activation regularization temporal activation regularization applied modern baselines contain recurrent dropout normalization techniques achieve comparable superior results. require modiﬁcations cell complex model changes require substantial modiﬁcations model. enables applied optimized implementations cudnn lstm many times faster na¨ıve ﬂexible lstm implementations. applied output dense layer penalizes activations substantially away encouraging activations remain small. acting implicitly rather explicitly similarities various batch layer normalization techniques. adding prior minimizes differences states explored past. broad concept falls broad concept slowness regularization attempts minimize loss function describing distance arbitrary mapping function. penalizes large changes hidden state timesteps encouraging model keep output consistent possible. lstm hidden state regularized long term memory though could optionally regularized similar manner. benchmark activation regularization temporal activation regularization applied strong nonvariational lstm baseline. experiment uses preprocessed version penn treebank wikitext- hyperparameters including optimized validation dataset. best found hyperparameters determined validation results test set. penn treebank small dataset preventing overﬁtting considerable importance major focus research. almost competitive models rely upon form recurrent dropout ensure overﬁt drastic changes hidden state. aggressive dropout techniques performing dropout embedding layer entire words dropped sequence also frequently used. wikitext- dataset approximately twice large vocabulary three times larger. text also tokenized processed manner similar datasets used machine translation using moses tokenizer table single model perplexity results penn treebank. models noting tied weight tying embedding softmax weights. section contain models without bottom section containing equivalent models using them. experiment details experiments model containing layer rnn. loss applied output ﬁnal layer layers. majority experiments follow medium model size zaremba layer hidden units layer. training model stochastic gradient descent without momentum used epochs. learning rate began divided four time validation perplexity failed improve. weight regular used weights model ization gradients norm rescaled. batches consist examples example containing timesteps. loss averaged examples timesteps. embedding weights uniformly initialized interval weights initialized between evaluating independently understand potential investigate impact language model perplexity used independently table table result substantial reduction perplexity results strongest improvement achieves evaluating jointly used together found best result achieved decreasing likely model table present over-regularized otherwise. results three different model sizes comparing models without ar/tar both. model sizes chosen comparable size published results. smallest model improvement baseline model. improvements continue larger size models though gains fall model size increased. largest lstm train achieves comparable results recurrent highway network human developed custom architecture approximately double number parameters. although lstm uses twice many parameters runs cell times timestep resulting computation. would likely result slower larger lstm model training prediction especially factoring optimized lstm implementations nvidia’s cudnn lstm. zaremba lstm zaremba lstm ghahramani variational lstm ghahramani variational lstm ghahramani variational lstm ghahramani variational lstm charcnn merity pointer sentinel-lstm inan variational lstm augmented loss inan variational lstm augmented loss zilly variational zoph cell zoph cell table single model perplexity validation test sets penn treebank language modeling task. models noting tied weight tying embedding softmax weights. also compare neural architecture search cell zoph report hyperparameters type dropout used penn treebank result note performed extensive hyperparameter search learning rate weight initialization dropout rates decay epoch order produce best performing model. possible large contributor improved result tuned hyperparameters compare cell results standard variational lstm cell subjected extensive hyperparameter search. largest lstm results perplexity higher comparison undergone extensive hyperparameter search additional regularization techniques recurrent embedding dropout custom cell. wikitext- results compare wikitext- results inan introduced weight tying embedding softmax weights. perform hyperparamter search coefﬁcient values instead using best results still quite effective. baseline lstm already achieves perplexity improvement variational lstm models inan including uses augmented loss modiﬁes standard cross entropy temperature divergence based loss. parameters optimized used perplexity falls additional perplexity. strong improvement seen dataset increased complexity dataset tanh traditionally used language modeling wanted generality types cells. applied best values lstm cell tanh without search table values likely quite suboptimal sufﬁcient illustrative purposes. perplexity improved baseline. positive sign given impact regularization techniques quite different lstm. lstm subjected leaving long term memory unregularized uses output timestep hidden state input next timestep. tanh model train acceptable levels without application tar. tanh likely forced recurrent matrix learn identity function order ensure could produce ht+. would important given weights model randomly initialized suggests acts implicit identity initialization constraint work revisit regularization form activation regularization temporal activation regularization simple implement activity regularization temporal activity regularization comtable single model perplexity results penn treebank tanh gru. neither cell traditionally used language modeling demonstrates generality values taken best lstm model search. models noting tied weight tying embedding softmax weights. petitive complex regularization techniques offer equivalent better results. improvements techniques provide likely combined regularization techniques variational lstm lead improvements performance well especially subjected extensive hyperparameter search. generating text samples words sampled using standard generation script contained pytorch word level language modeling example. wikitext- used given larger vocabulary realistic looking text. neither heosi token hunki allowed selected. paragraph separate sample text tokens following moses joining words dot-decimal split token. forces nation area moved sarajevo troops despatched national register historic places summer establishment full political social parties polish language protected soviet union ﬁrst polish continental conﬂict newly formed union north america polish front last polish communist party", "year": 2017}