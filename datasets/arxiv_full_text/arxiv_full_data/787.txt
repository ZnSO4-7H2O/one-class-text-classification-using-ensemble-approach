{"title": "Programming with a Differentiable Forth Interpreter", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.", "text": "given practice training data scarce small problems core question incorporate prior knowledge model. paper consider case prior procedural knowledge neural networks knowing program traverse sequence local actions performed step. present end-to-end differentiable interpreter programming language forth enables programmers write program sketches slots ﬁlled behaviour trained program input-output data. optimise behaviour directly gradient descent techniques user-speciﬁed objectives also integrate program larger neural computation graph. show empirically interpreter able effectively leverage different levels prior program structure learn complex behaviours sequence sorting addition. connected outputs lstm trained jointly interpreter achieves state-of-the-art accuracy end-to-end reasoning quantities expressed natural language stories. central goal artiﬁcial intelligence creation machines learn effectively human instruction data. recent important step towards goal invention neural architectures learn perform algorithms akin traditional computers using primitives memory access stack manipulation architectures trained standard gradient descent methods enable machines learn complex behaviour input-output pairs program traces. context role human programmer often limited providing training data. however training data scarce resource many tasks. cases programmer department computer science university college london london department computer science university oxford oxford department theoretical applied linguistics university cambridge cambridge correspondence matko boˇsnjak <m.bosnjakcs.ucl.ac.uk>. partial procedural background knowledge know rough structure program implement several subroutines likely necessary solve task. example programming demonstration query language programming user establishes larger conditions data model needs details. scenarios question becomes exploit various types prior knowledge learning algorithms. address question present approach enables programmers inject procedural background knowledge neural network. approach programmer speciﬁes program sketch traditional programming language. sketch deﬁnes part neural network behaviour. part learned using training data. core insight enables approach fact programming languages formulated terms abstract machine executes commands language. implement machines neural networks constraining parts networks follow sketched behaviour. resulting neural programs consistent prior knowledge optimised respect training data. paper focus programming language forth simple powerful stack-based language facilitates factoring abstraction. underlying forth’s semantics simple abstract machine. introduce implementation machine differentiable respect transition executes time step well distributed input representations. sketches users write deﬁne underspeciﬁed behaviour trained backpropagation. neural programming tasks introduced previous work present forth sketches capture different degrees prior knowledge. example deﬁne general recursive structure sorting problem. show given input-output pairs learn sketch generalise well problems unseen size. addition apply task solving word algebra problems. show provided basic algorithmic scaffolding trained jointly upstream lstm able learn read natural language narratives extract important numerical quantities reason these ultimately answering corresponding mathematical questions without need explicit intermediate representations used previous work. contributions work follows present neural implementation dual stack machine underlying forth introduce forth sketches programming partial procedural background knowledge iii) apply forth sketches procedural prior learning algorithms data introduce program code optimisations based symbolic execution speed neural execution using forth sketches obtain state-of-the-art end-to-end reasoning quantities expressed natural language narratives. forth simple turing-complete stack-based programming language chose forth host language work established general-purpose high-level language relatively close machine code promotes highly modular programs branching loops function calls thus bringing good balance assembly higher level languages importantly iii) abstract machine simple enough straightforward creation continuous approximation. forth’s underlying abstract machine represented state contains stacks data evaluation pushdown stack holds values manipulation return address pushdown stack assists return pointers subroutine calls. accompanied heap random memory access buffer program counter forth program sequence forth words w...wn. role word varies encompassing language keywords primitives user-deﬁned subroutines word deﬁnes transition function machine states therefore program deﬁnes transition function simply applying word current program counter current state. although usually considered part heap consider forth programs separately ease analysis. example bubble sort algorithm implemented forth shown listing everything except lines b-c. execution starts line literals pushed data stack sort called. line executes main loop sequence. lines denote bubble procedure comparison stack numbers recursive call detailed description program executed forth abstract machine provided appendix notice forth provides common control structures looping branching always reduced low-level code uses jumps conditional jumps likewise think sub-routine deﬁnitions labelled code blocks invocation amounts jumping code block respective label. programmer writes forth program deﬁne sequence forth words i.e. sequence known state transition functions. words programmer knows exactly computation proceed. accommodate cases developer’s procedural background knowledge incomplete extend forth support deﬁnition program sketch. case forth programs sketches sequences transition functions. however sketch contain transition functions whose behaviour learned data. learn behaviour transition functions within program would like machine output differentiable respect functions enables choose parameterised transition functions neural networks. introduce tensorflow implementation differentiable abstract machine continuous state representations differentiable words sketches. program execution modelled recurrent neural network parameterised transition functions time step. figure left neural forth abstract machine. forth sketch translated low-level code slots {...} substituted parametrised neural networks. slots learnt input-output examples differentiable machine whose state comprises low-level code program counter data stack return stack heap right bilstm trained word algebra problems. output vectors corresponding representation entire problem well context representations numbers numbers solve tasks. entire system end-to-end differentiable. symbolic machine state continuous representation differentiable stacks data stack return stack heap attention vector indicating word sketch executed current time step. figure depicts machine together elements. three memory structures data stack return stack heap based differentiable memory buffers m∈{drh} drh∈ rl×v stack size value size differentiable read operation akin neural turing machine memory element-wise multiplication address pointer. addition memory buffers data stack return stack contain pointers current top-of-the-stack element dr∈rl respectively. allows implement pushing writing value incrementing pointer finally program counter vector that one-hot points single word program length equivalent vector symbolic state machine. denote space continuous representations neural forth words straightforward convert forth words deﬁned functions discrete machine states functions operating continuous space example consider word duplicates data stack. differentiable version ﬁrst calculates value address shifts stack pointer writes using writed. complete description implemented forth words differentiable counterparts found appendix deﬁne forth sketch sequence continuous transition functions here either corresponds neural forth word trainable transition function call trainable functions slots correspond underspeciﬁed slots program code need ﬁlled learned behaviour. produces latent representation current machine state using multi-layer perceptron decoder consumes representation produce next machine state. hence wdec wenc. slots within forth program code introduce notation reﬂects decomposition. particular slots deﬁned syntax encoder decoder encoder decoder speciﬁcations corresponding slot parts described below. linear sigmoid tanh represent chained transformations enable multilayer perceptron architecture. linear projects dimensions sigmoid tanh apply same-named functions elementwise. choose w...wm chooses forth words w...wm. takes input vector length produce hiwi. manipulate ...em directly manipulates machine state elements writing appropriately reshaped softmaxed output encoder machine state elements writem. model execution using produces state conditioned previous state ﬁrst passing current state function program weighing produced next states component program counter vector corresponds program index effectively using attention vector code. formally have clearly recursion ﬁnal state differentiable respect program code inputs. furthermore differentiable forth programs ﬁnal state correspond ﬁnal state symbolic execution figure segment execution forth sketch blue listing pointers values one-hot state program counter maintains uncertainty. subsequent states discretised clarity. here slot {...} learned optimal behaviour. requires one-time step transition. time step program counter either incremented decremented explicitly popped stack. turn machine state calculated executing words program weighting result states program counter. expensive advisable avoid full steps wherever possible. strategies avoid full steps signiﬁcantly speed-up symbolic execution interpolation if-branches. symbolic execution whenever sequence forth words contains branch entry exit points collapse sequence single transition instead naively interpreting words one-by-one. symbolically execute sequence forth words calculate machine state. difference initial state derive transition function sequence. example sequence swap swaps elements data return stack yields symbolic state rd...dl. dr...rl. comparing initial state derive single neural transition needs swap elements interpolation if-branches cannot apply symbolic execution code branching points branching behaviour depends current machine state cannot resolve symbolically. however still collapse if-branches involve function calls loops executing branches parallel weighing output states value condition. if-branch contain function calls loops simply fall back execution words weighted program counter. training procedure assumes input-output pairs machine start states only. output deﬁnes target memory data stack additionally mask indicates components stack included loss denote ﬁnal state steps execution using initial state deﬁne loss function cross-entropy loss parameters slots program backpropagation variant gradient descent optimise loss function. note point would possible include supervision intermediate states done neural program interpreter also test difﬁcult task answering word algebra problems. show standalone solver problems bypassing intermediary task producing formula templates must executed also outperform previous work trained data. speciﬁc transduction tasks discretise memory elements testing. effectively allows trained model generalise sequence length correct sketch behaviour learned. also compare seqseq baseline. full details experimental setup found appendix permute. sketch specifying elements stack return stack must permuted based values former value comparison permutation behaviour must learned. core sketch depicted listing sketch explained detail appendix compare. sketch provides additional prior procedural knowledge model. contrast permute comparison elements stack must learned core sketch depicted listing results bubble sort quantitative comparison models bubble sort task provided table given test sequence length vary training lengths illustrate model’s ability generalise sequences longer observed training. quickly learns correct sketch behaviour able generalise perfectly sort sequences elements observing sequences length three training. comparison seqseq baseline falters attempting similar generalisations performs close chance tested longer sequences. sketches perform ﬂawlessly trained short sequence lengths under-perform trained sequences length arising computational difﬁculties discuss issue section sorting sequences digits hard task rnns fail generalise sequences even marginally longer ones trained investigate several strong priors based bubble sort transduction task present sketches listing enable learn sorting hundred training examples next applied problem learning n-digit numbers. rely standard elementary school addition algorithm goal iterate pairs aligned digits calculating yield resulting sum. complication arises digits two-digit number requiring correct extra digit carried subsequent column. assume aligned pairs digits input carry least signiﬁcant digit length respective numbers. sketches deﬁne high-level operations recursion leaving core addition learned data. speciﬁed high-level behaviour includes recursive call template halting condition recursion underspeciﬁed addition operation must take three digits previous call digits previous carry produce single digit resultant carry introduce sketches inducing behaviour manipulate. sketch provides little prior procedural knowledge directly manipulates machine state ﬁlling carry result digits based three elements data stack depicted listing green. incorporating additional prior information choose exactly speciﬁes results computation namely output ﬁrst slot carry output second result digit conditioned digits carry data stack. depicted listing blue. quantitative evaluation addition experiments analogous evaluation bubble sort demonstrate performance addition task examining test sequence lengths varying lengths training instances seqseq model fails generalise longer sequences observed training. comparison choose sketch less structured manipulate sketch learn correct sketch behaviour generalise test sequence lengths additional experiments able successfully train choose manipulate sketches sequences input length tested sequence length conﬁrming perfect training generalisation capabilities. word algebra problems often used assess numerical reasoning abilities schoolchildren. questions short narratives focus numerical quantities culminating question. example previous work cast waps transduction task mapping question template mathematical formula thus requiring manuall labelled formulas. instance formula used correctly answer question example previous work local classiﬁers hand-crafted grammars recurrent neural models used perform task. predicted formula templates marginalised training evaluated directly produce answer. contrast approaches able learn both soft mapping text algebraic operations execution without need manually labelled equations explicit symbolic representation formula. bilstm reader reads text problem produces vector representation word concatenated forward backward pass bilstm network. resulting word vectors corresponding numbers text numerical values numbers vector representation whole problem initialise heap done end-to-end fashion enabling gradient propagation bilstm vector representations. process depicted figure sketch depicted listing dictates differentiable computation. first copies values heap word vectors return stack numbers data stack second contains four slots deﬁne space possible operations four operators three operands conditioned vector representations return stack. slots permutation elements data stack choosing ﬁrst operator iii) possibly swapping intermediate result last operand choice second operator. ﬁnal commands simply empties return stack slots deﬁne space possible operations however model needs learn utilise operations order calculate correct result. results evaluate model common core dataset introduced roth notable diverse equation patterns consisting four operators three operands. compare three baseline systems local classiﬁer hand-crafted features seqseq baseline model data generation component bouchard baselines trained predict best equation executed outside model obtain answer. contrast trained end-to-end input-output pairs predicts answer directly without need intermediate symbolic representation formula. outperform classiﬁer-based approach. method slightly outperforms seqseq baseline achieving highest reported result dataset without data augmentation. bridges traditional programming language modern machine learning architecture. however seen evaluation experiments faithfully simulating underlying abstract machine architecture introduces unique challenges. challenge additional complexity performing even simple tasks viewed terms operations underlying machine state. illustrated table sketches effectively trained small training sets generalise perfectly sequences length. however difﬁculty arises training sequences modest lengths. even dealing relatively short training length sequences program code optimisations employed underlying machine unroll problematically large number states. problems whose machine execution quadratic like sorting task observe signiﬁcant instabilities training backpropagating long sequences consequent failures train. comparison addition problem easier train comparatively shorter underlying execution rnns. higher degree prior knowledge provided played important role successful learning. example compare sketch provides structure achieves higher accuracies trained longer sequences. similarly employing softmax directly manipulated memory elements enabled perfect training manipulate sketch addition. furthermore encouraging trained jointly upstream lstm provide strong procedural prior knowledge solving real-world task. related work program synthesis idea program synthesis artiﬁcial intelligence long history computer science whereas large body work focused using genetic programming induce programs given inputoutput speciﬁcation also various inductive programming approaches aimed inducing programs incomplete speciﬁcations code implemented tackle problem sketching case sketches neural networks able learn slot behaviour. probabilistic bayesian programming work closely related probabilistic programming languages church allow users inject random choice primitives programs deﬁne generative distributions possible execution traces. sense random choice primitives languages correspond slots sketches. core difference lies train behaviour slots instead calculating posteriors using probabilistic inference estimate parameters using backpropagation gradient descent. similar in-kind terpret’s fmgd algorithm employed code induction backpropagation. comparison model optimises slots neural networks surrounded continuous approximations code enables combination procedural behaviour neural networks. addition underlying programming probabilistic paradigm programming languages often functional declarative whereas approach focuses procedural discriminative view. using end-to-end differentiable architecture easy seamlessly connect sketches neural input output modules lstm feeds machine heap. neural approaches recently surge research program synthesis execution deep learning increasingly elaborate deep models. many models based differentiable versions abstract data structures abstract machines differentiable neural computers neural gpus models able induce algorithmic behaviour training data. work differs differentiable abstract machine allows seemingly integrate code neural networks train neural networks speciﬁed slots backpropagation. related efforts also autograd enables automatic gradient computation work neural approximations abstract structures machines naturally leads elaborate machinery able induce call code code-like behaviour. neelakantan learned simple sql-like behaviour–—querying tables natural language simple arithmetic operations. although sharing similarities high level primary goal model induction code injection. learn compose neural modules produce desired behaviour visual task. neural programmer-interpreters learn represent execute programs operating different modes environment able incorporate decisions better captured neural network many lines code users inject prior procedural knowledge training program traces hence require full procedural knowledge. contrast enable users partial knowledge sketches. neural approaches language compilation also researched compiling language neural networks building neural compilers adaptive compilation however line research perceive neural interpreters compilers means injecting procedural knowledge did. best knowledge ﬁrst working neural implementation abstract machine actual programming language enables inject priors straightforward manner. presented differentiable abstract machine forth programming language showed used complement programmers’ prior knowledge learning unspeciﬁed behaviour forth sketches. successfully learns sort solve word algebra problems using program sketches program input-output pairs. believe larger paradigm helps establish useful addressing complex problems low-level representations input necessary higher-level reasoning difﬁcult learn potentially easier specify. future work plan apply problems domain like machine reading knowledge base inference. long-term integration non-differentiable transitions exciting future direction sits intersection reinforcement learning probabilistic programming. thank guillaume bouchard danny tarlow dirk weissenborn johannes welbl anonymous reviewers fruitful discussions helpful comments previous drafts paper. work supported microsoft research scholarship allen distinguished investigator award marie curie career integration award. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu ghemawat sanjay goodfellow harp andrew irving geoffrey isard michael yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh man´e monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay vi´egas fernanda vinyals oriol warden pete wattenberg martin wicke martin yuan zheng xiaoqiang. tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. andreas jacob rohrbach marcus darrell trevor proceedings klein dan. neural module networks. ieee conference computer vision pattern recognition bouchard guillaume stenetorp pontus riedel sebastian. learning generate textual data. proceedings conference empirical methods natural language processing goodman noah mansinghka vikash daniel bonawitz keith tenenbaum joshua church proceedings language generative models. conference uncertainty artiﬁcial intelligence graves alex wayne greg reynolds malcolm harley danihelka grabska-barwi´nska agnieszka colmenarejo sergio g´omez grefenstette edward ramalho tiago agapiou john hybrid computing using neural network dynamic external memory. nature grefenstette edward hermann karl moritz suleyman mustafa blunsom phil. learning transduce unbounded memory. proceedings conference neural information processing systems joulin armand mikolov tomas. inferring algorithmic patterns stack-augmented recurrent nets. proceedings conferences neural information processing systems bunel rudy desmaison alban kohli pushmeet torr philip kumar pawan. adaptive neural compilation. proceedings conference neural information processing systems kitzelmann emanuel. inductive programming survey international program synthesis techniques. workshop approaches applications inductive programming gaunt alexander brockschmidt marc singh rishabh kushman nate kohli pushmeet taylor jonathan tarlow daniel. terpret probabilistic programming language program induction. arxiv preprint arxiv. koncel-kedziorski hajishirzi hannaneh sabharwal ashish etzioni oren siena. parsing algebraic word problems equations. transactions association computational linguistics solar-lezama armando rabbah rodric bod´ık rastislav ebcio˘glu kemal. programming sketching proceedings programbit-streaming programs. ming language design implementation solar-lezama armando tancau liviu bodik rastislav seshia sanjit saraswat vijay. combinatorial sketching finite programs. sigplan notices volume kurach karol andrychowicz marcin sutskever ilya. neural random-access machines. proceedings international conference learning representations kushman nate artzi yoav zettlemoyer luke barzilay regina. learning automatically solve algebra word problems. proceedings annual meeting association computational linguistics tessa wolfman steven domingos pedro weld daniel learning repetitive text-editing procedures smartedit. wish command morgan kaufmann publishers inc. maclaurin dougal duvenaud david adams ryan gradient-based hyperparameter optimization reversible learning. proceedings international conference machine learning neelakantan arvind quoc sutskever ilya. neural programmer inducing latent programs proceedings international gradient descent. conference learning representations neelakantan arvind vilnis luke quoc sutskever ilya kaiser lukasz kurach karol martens james. adding gradient noise improves learning deep networks. arxiv preprint arxiv. appendix forth words implementation implemented small subset available forth words table words together descriptions given table implementation given table commands roughly divided groups. groups line-separated table data stack operations {num} swap over drop heap operations comparators return stack operations control statements if..else..then begin..while..repeat do..loop subroutine control {sub} macro variable creation variable create..allot table forth words descriptions. denotes top-of-stack denotes next-on-stack dstack denotes data stack rstack denotes return stack heap denotes heap. description pushes {num} dstack. increments dstack decrements dstack duplicates dstack tos. swaps nos. copies pushes tos. pops consumes dstack tos. returns operator tos. consumes dstack tos. returns respectivelly otherwise. pushes dstack rstack removes dstack. pushes rstack dstack removes rstack. copies rstack dstack tos. consumes dstack equals non-zero number executes commands else. otherwise executes commands else then. continually executes commands repeat code begin evaluates non-zero number consumes assumes limit current index. increases index equal nos. every increment executes commands loop. denotes subroutine followed word deﬁning subroutine invocation puts program counter rstack sets subroutine address. subroutine exit. consumest rstack sets treats subroutine macro function. table implementation forth words described table note variable creation words implemented ﬁxed address allocation macro words implemented inlining. example forth program implements bubble sort algorithm shown listing provide description ﬁrst iteration algorithm executed forth abstract machine prepare next call bubble move back return stack data stack swap next element back decrease invoke bubble again. notice accumulate analysed part sequence recursively taken back. sorter measuring performance model number training instances varies observe beneﬁt additional prior knowledge optimisation process. stronger prior knowledge provided model quickly maximises training accuracy. providing less structure results lower testing accuracy initially however sketches learn correct behaviour generalise equally well seeing training instances. additionally worth noting permute sketch always able converge result correct length sketches trivial train. comparison seqseq baseline able generalise sequence trained training sequence length testing much longer sequence length seqseq baseline able achieve accuracy. figure accuracy models varying number training examples trained input sequence length addition task. manipulate choose seqseq tested sequence lengths seqseq tested sequence length measure runtime bubble sort sequences varying length without optimisations described section results repeated runs shown figure demonstrate large relative improvements symbolic execution interpolation if-branches compared non-optimised code. figure accuracy models varying number training examples trained input sequence length bubble sort task. compare permute seqseq tested sequence lengths seqseq tested sequence length adder tested models train datasets increasing size addition task. results depicted table show choose manipulate sketch able perfectly generalise examples trained sequence lengths tested comparison seqseq baseline achieves trained examples tested input length test seqseq tested sketches unable achieve listing deﬁnes bubble word sketch capturing several types prior knowledge. section describe permute sketch. assume bubble involves recursive call terminates length next bubble call takes input function current length stack elements. trained addition choose manipulate sketches presented table randomly generated train development test sets sizes respectively. batch size used initial learning rate common core dataset partitioned train test containing questions respectively. batch size used initial learning rate bilstm word vectors initialised randomly vectors length stack width stack size figure visualise program counter traces. trace follows single example start middle training process. beginning training program counter starts deviate one-hot representation ﬁrst steps iterations sort fails correctly determine next word. training epochs learns better permutations enable algorithm take crisp decisions halt correct state. point programmer knows decision must made based data stack elements return stack precise nature decision unknown limited variants permutation elements output produce input state decrement recursive bubble call culmination call output learned slot behaviour moved onto data stack using execution proceeds next step. figure illustrates portions sketch executed rnn. program counter initially resides indicated vector next program data return stacks partially ﬁlled show content horizontal one-hot vectors corresponding integer values vectors point stacks one-hot state well. execution trace slot line already showing optimal behaviour remembers element return stack larger executes bubble remaining sequence counter subtracted parameters sketch trained using adam gradient clipping gradient noise tuned learning rate batch size parameters gradient noise random search development variant task. training procedure models consists epochs adam optimisation batch size learning rate gradient clipping norm model parameters exceeded vary size training test data observe indication models failing reach convergence training conditions. figure program counter traces single example different stages training bubblesort listing last element last halting command gets executed learning correct slot behaviour. word algebra problem sketch described listing core model problems. however additional words core took care copying data heap data return stacks ﬁnally emptying return stack. full sketch given listing deﬁne question variable denote address question vector heap. lines create repr buffer buffer variables denote occupy four sequential memory slots heap store representation vectors numbers respectively. lines create variables repr denote addresses current representations numbers heap. lines store repr buffer repr buffer essentially setting values variables repr starting addresses allotted lines lines create macro functions step step repr increment repr values call. macro functions used iterate heap space. lines deﬁne macro functions current fetching current number current repr fetching representation values. lines essentially copy values numbers heap data stack using current step macros. line pushes question vector lines push word representations numbers return stack. following that deﬁne core operations sketch. line permutes elements data stack function elements return stack line chooses operator execute elements data stack line executes possible swap elements data stack conditioned return stack. finally line chooses last operator execute data stack conditioned return stack. address question variable question allotting representations numbers create repr_buffer allot create num_buffer allot addresses first representation number variable repr variable permute stack elements based question number representations observe permute choose first operation observe choose choose whether swap intermediate result bottom number observe choose swap choose second operation observe choose", "year": 2016}