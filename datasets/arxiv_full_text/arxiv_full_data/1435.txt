{"title": "Improvements to context based self-supervised learning", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear tests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability.", "text": "develop methods improve results self-supervised learning using context. start baseline patch based arrangement context learning there. methods address overt problems chromatic aberration well potential problems spatial skew mid-level feature neglect. prevent problems testing generalization common self-supervised benchmark tests using different datasets development. results methods combined yield scores standard selfsupervised benchmarks including classiﬁcation detection pascal segmentation pascal linear tests imagenet csail places datasets. obtain improvement baseline method percentage points transfer learning classiﬁcation tests. also show results different standard network architectures demonstrate generalization well portability. data models programs available https//gdo-datasci. ucllnl.org/selfsupervised/. self-supervised learning opened intriguing avenue unsupervised learning. intellectually satisfying resembles gestalt-like mechanisms learning visual cortical formation. also appealing fact implemented standard off-the-shelf neural networks toolkits. self-supervised learning methods create protocol whereby computer learn teach supervised task. instance convolutional neural network taught learn arrangement patches image. learning relative position patches network would forced also learn features semantics underlie image. although network trained learn patch positions ﬁnal goal generalize learned representation solve tasks. instance self-supervised network trained transfer task classify objects pascal dataset compared trained supervised task learning classify imagenet dataset self-supervised network learned good generalizations image features semantics perform well supervised network transfer learning. last years several methods selfsupervised learning introduced. instance trained recognize transformation performed image. since then methods introduced context arrangement image patches image completion image colorization motion segmentation motion frame ordering object counting multi-task ensemble many models method relative strengths weaknesses. instance uses customized split-brain architecture makes less off-the-shelf solutions frequently standard network alexnet motion cues downside constrained video data. many patch based methods siamese network architecture incurs extra memory demands however every selfsupervised method suffers drawback still short supervised methods terms transfer learning performance. intent work improve performance self-supervised learning. present variety techniques hope applicable approaches well. demonstrate generalizability techniques running several different neural network architectures many kinds datasets tasks. instance discuss dataset wish corpus standard self-supervised tests caltech-ucsd birds excellent ﬁnding potential short comings techniques designed address well-known chromatic aberration problem show importance color patterns bird classiﬁcation patch/context approach issue selfsupervised learning popular method means active path inquiry. patch/context approaches work creating arrangement image patches either space time. distinct arrangement assigned class label network predicts correct arrangement patches solving supervised classiﬁcation problem. network typical supervised network alexnet googlenet resnet order view multiple patches time siamese network frequently used patch independent network path path shares weights. used system patches ﬁnite eight possible spatial conﬁgurations. created extension using many nine patches puzzle conﬁguration. temporal ordering patches also used. instance shufﬂed four consecutive video frames create classes prediction. another temporal method queried networks ability determine patch came object later time similar different object. considered meta self-learner since leverages pre-selfsupervised learner determine object similarity. patch based methods advantage easy understand network architecture agnostic frequently straightforward implement. also tend perform well standard measures transfer learning. instance performer pascal detection even among large number arrivals. almost tied score pascal classiﬁcation score pascal detection second highest score pascal segmentation desired semantic structure ﬁnding incidental clues reveal location patch. example chromatic aberration occurs naturally result camera lensing different frequencies light exit lens slightly different angles. radially offsets colors magenta green modestly center image. looking offset relation different color channels network determine angular location patch image. aberration varies lenses it’s imperfect none-the-less exists. common remedy withhold color data channels using channel-dropping channel replication conversion gray scale primary difﬁculty approaches color becomes decorrelated since colors observed together. makes difﬁcult learn color opponents patterns emerge supervised training sets imagenet. another approach jitter color channels similar effect blurring image might affect sharpness learned wavelet features. often-cited worry patch/context works relates trivial low-level boundary pattern completion neural network learn alignment patches based desirable semantic information instead matching bottom part simple line segments. common approaches provide large enough patches randomly jitter patches. last technique dubious since convolutional neural network align simple patterns arbitrary offsets. issue also implicitly addressed non--connected adjacent patches. half patches arranged diagonally make resistant trivial low-level boundary pattern completion. also note would want self-supervised learner cheat time could used help form level features. somewhat unclear much problem might another possible problem self-supervised networks general mid-layers network train well early later layers. instance created self-supervised network using ensemble different methods. created automated lasso grab layers network useful task. lasso tended grab layers early late network. suggests many self-supervised tasks information middle network layers essential training. another piece evidence comes csail places linear test shows well layer network performs transfer learning. many self-supervised networks perform well better supervised imagenet trained network ﬁrst secfigure examples patches taken imagenet used self-supervised training. original example chroma blurring. frequently difﬁcult distinguish blurred original images humans spatially sensitive variation color. chroma blurring sometimes result loss color saturation color bleeding saturated regions notice original gold image signs strong chromatic aberration blended effectively chroma blurring switches green aberration ﬁsh’s color. approach comprised three parts. ﬁrst collection tools enhancements part transferable self-supervised methods. second part utilize datasets make experiments diverse general. third part demonstration several different neural networks verify generalization demonstrate portability. general approach start baseline using two-patch spatial conﬁguration paradigm. approach gives good results easy implement understand. augment approach using various techniques. technique vigorously test effects empirically justify usage. address problem chromatic aberration removing cues image aberration allowing color patterns opponents least partially preserved. note human visual system sensitive spatial frequency chroma much better discerning detail shifts intensity however even lack spatial acuity color still discern meaningful color patterns. such balance tradeoff between decreasing color spatial information removing chromatic aberration cues. preserve intensity information reduce aberration start converting images color space. apply blurring operation chroma channels. case ﬁlter. times size convolution ﬁlter original googlenet target network. luminance channel left untouched convert image back rgb. figure shows patch/context methods apply random jitter patches different patches jittered different amounts different directions. issue applying random jitter might distort skew spatial understanding network. example head animal observed patch feet other true spatial extent items would difﬁcult discern given random jitter patches might pixels apart might hand patches maintained ﬁxed spacing reasoning extent object patches would easier. thus network might make better inferences larger shape object beyond patch itself. yoking patch jitter. patch randomly jittered create random crop effect jittered amount direction. might make prone trivial low-level boundary pattern completion mentioned suspect partially addressed non--connect patches. also unclear well random jitter address problem since features need aligned order recognized cnn. additionally certain amount low-level boundary pattern completion problem since enhance learning simple features. sized patches since receptive ﬁeld convolution many cnns. popular networks ﬁvelayer topology layer groups layer group half dimension preceding image. order labeled patch right column shows patches process create ﬁnal patches neural network. layer half-scale networks). dimensional variation caused omission padding layers pixel layer maps extent pixels input image. since networks tend convolutions last layers patch size justiﬁed cover full ﬁeld. three patches set. two-patch conﬁguration patches cover area useful information. instance half image covered ocean. address problem using patches. uses puzzle system nine patches. however want sized patches able train larger contemporary networks feasible train properly. hand patch create triple siamese network smaller single network traditional image size. makes easy move larger network. test four patches. extra patches conﬁgurations added several conﬁgurations patches seen figure adding patterns creates orthogonal unique patterns covers image mixes scales prevent simple pattern completion creates natural cover image multiple scales training. taken image. taken center aspect preserved image reducing smallest dimension size. patches evenly spaced aligned image corners cover image patches taken image overlap patches taken image. eight patterns similar patterns l-shaped four them. combine patches overlap patches create hybrid patch sets. hybrid scale patches allow semantic reasoning preventing easy matching simple features patches since different scales. processed patches network batches. ﬁnal patches network sized note figure shows typical combinations sample image. exact patterns extracted images imagenet training set. obtain training patch sets imagenet. mentioned evidence middle layers selfsupervised network neglected. approach would create bias towards neurons. general ﬁve-layer topology fourth group layer receptive ﬁeld given ﬁlter. alexnet would include layers conv conv conv. googlenet would include layers figure grayed-out area apertured size patch. aperture smallest size use. left image shows pixel arrangement group layer four. least region directly interfered aperture. however right layer pixel fully uncovered. spatial interactions layer involve least occluded region. ideally would create inhibition layer forming meaningful spatial associations perhaps bias towards layer can. note description simpliﬁed somewhat imprecise since image information propagate laterally consecutive layers. create aperture could create patch doesn’t cover extent group layers cover extent group layers ﬁlters. could bias layers learning since cannot whole patch. ideally would emphasis learning layers. figure example this. random aperture three patches created. idea behind leaving patch un-apertured don’t completely bias group layer still want learn. aperture square sample randomly sized minimum size since smallest size guarantee least convolution unobstructed fourth layer. position aperture also randomized must inside patch never viewable area less area outside aperture ﬁlled imagenet mean rgb. size position aperture patches yoked. patches apertured randomized sample. patch patch/context model simply contain part much larger object. general intent patch/context approach. might help parts understood different orientations? instance seen upside-down roof better understand triangular yield sign funnel. additionally humans ability conditionally recognize upsideparts embedded whole image. illustrated famous thatcher illusion reason self-supervised learning might beneﬁt exposure upside-down patches would help make figure left example famous thatcher illusion demonstrates conditional sensitivity upside-down features image background. used mostly inspiration. left house image network tell blue bordered area comes upper left corner based chromatic aberration alone. however right image rotation classiﬁcation makes tell patch inverted comes lower right corner. uses chromatic aberration would wrong time. network identify patches right-side-up upsidedown. ﬂipping whole image patches ﬂipped. then double number classes giving upside-down image class. instance classes patch arrangements upside-down images classes. also explore degree rotations. yields total classes. forcing network classify patches upside-down also reduces strength clues generated chromatic aberration. aberration radiates center image. without rotating image downward sloping arch green/magenta left indicates patch comes upper left-hand corner. however ﬂipped image pattern indicates lower right corner instead. trying guess upper left-hand corner chromatic aberration pattern wrong time. four rotations wrong time. present experiments tricks found helpful varying degrees. method typical mixture label preserving transformations calling usual tricks involves augmentation randomly mirroring zooming cropping images. mirroring simple horizontal ﬂipping special classiﬁcation like since would likely prove confusing network. random zooming randomly scaled input patch extract random patch this. zoom crop location random sample yoked three input patches set. method rescaling ubt. three patches rescaled four randomly chosen rescale techniques random selection yoked patches. idea make harder match level statistics between patches call randomization rescaling methods also tried varying learning rate decay rate network layers increase learning middle layer weights. instance adjust ﬁrst layer learning rate center layer. linearly increase learning rate towards middle layer reduce rate back down. given nine layers siamese alexnet would learning rates here conv layer learning rate multiplier last fully connected layer call weight varying development testing techniques generally requires ﬁshing results. such avoid using target dataset testing idea. fishing leads solutions specialized towards speciﬁc dataset rather general solution. self-supervised learning test metrics based pascal imagenet csail places commonly used. therefore test techniques datasets certain amount overlap possess differences conﬁdent generalization. validation combination birds compcars call combination cub/ccars sets training network self-supervised manner apply transfer learning ﬁne-tuning classiﬁcation. data sets grained respective class however major differences kinds features cars birds have. additionally birds dataset provides ideal test dealing chromatic aberration. four keys identifying birds size/shape habitat behavior color pattern trying control chromatic aberration alter image negatively affects color processing thus classiﬁcation birds. interested generalization solutions portability architectures. constrain selfsupervised learning mostly training protocol it’s easier train different networks. using many datasets using many networks also helps assure technique network speciﬁc works well designs. demonstrate results four different networks. standard caffenet type alexnet alexnet batch normalization resception network inception network triple siamese networks share weights between branches concatenated together fully connected layers. input three image patches processed patches taken imagenet training dataset. recall apply chroma blur operation ofﬂine train remove expense repeated conversion blurring. output softmax classiﬁcation patch arrangement pattern class classes. networks load list shufﬂed training testing patches. list reshufﬂed epoch. slightly different protocol training batch normalized networks training non-normalized caffenet. batch normalized networks train stochastic gradient descent iterations batch size initial learning rate step protocol used size gamma momentum weight decay caffenet google exponential style training train million iterations batch size initial learning rate train step size gamma momentum weight decay table basic ablation showing effect adding method time. scores birds compcars single class classiﬁcation accuracy. pascal uses mean average precision mean column ccars show mean ccars all. post classiﬁcation results fact well cub/ccars surrogate matches core self-supervised benchmark test. baseline patch protocol uses color dropping matches protocol gains cub/ccars appear correlate gains largest gains cub/ccars rotation classiﬁcation chroma blurring random aperture. also notice results compcars percentage point less imagenet pretrained network. †the imagenet pretrain uses conv conv. fully connected layers initialized new. bulk testing validation carried ﬁne-tuning self-supervise trained network cub/ccars datasets. sets split priori training testing sets authors. provided bounding boxes sets pre-crop images. details seen appendix perform ablation validation experiments custom batch normalized alexnet seen ﬁgure target network similar conv convb layers transfer layers self-supervise trained network. kept layers mostly diversity batch normalized alexnet somewhat different standard caffenet/alexnet perform benchmark tests again generalization important selfsupervise train transfer weights convolution batch norm layers non-siamese network initialize fully connected layers. ccars trained way. methods training established priori avoid over-tuning hyperparameters. ﬁne-tuning polynomial learning policy initial learning rate iterations batch size polynomial power momentum weight decay condition wished test trained three times took average testing accuracy reduce minor variation within condition results. show post results pascal classiﬁcation network condition well validation results target data sets. trained standard classiﬁcation method described results mean average precision finer details ablation seen appendix demonstrate results obtained compared self-supervised methods using suite standard benchmark tests. include classiﬁcation detection pascal segmentation pascal also include linear classiﬁer tests csail places imagenet note possible differences standard benchmark methodology here. detection multiscale training testing. common used authors segmentation authors surgery trained fully connected layers convolution layers seven. trained network correct number weights fully connected layers this. copy convolution layers initialize layers seven randomly. tests self-supervise train triple siamese caffenet type alexnet using non-batch normalized protocol previously described. pooling layer siamese structure custom batch normalized alexnet. leave batch normalization layers insert dropout layers joined joined table classiﬁcation detection segmentation test results pascal mean scores shown classiﬁcation detection well three segmentation score available bottom three results include methods except rwc. gives highest score three tests averaged conditions without give best scores detection. conserve space taken largest scores network weights rescaled *denotes estimate score based recent result different network alexnet. estimate computed adding gain reported work mutual baseline method alexnet result also appears table dropout ratio convolution weights layers transferred completely shelf caffenet. training testing performed standard deﬁned authors test results seen tables improvements yield results out-perform methods standard benchmark tests. trained networks demonstrate portability generalization. ﬁrst network resception googlenet like network batch normalization residual short-cutting convolutions group layer extend beyond self-supervised receptive layer. self-supervise trained replacing surrogate ﬁlters cannot table linear test imagenet data network ﬁne-tuned convolution layer shown. results bottom three rows. three self-supervised conditions used table methods presented except maximum score shown bold previous best result underlined. rotation classiﬁcation using degree rotations generally best performer. conv edges small margin table linear test csail places data. network ﬁne-tuned convolution layer shown. results bottom three rows. three selfsupervised conditions used table methods presented except maximum score shown bold previous best underlined. adding randomization rescaling method improves earlier layer results falls slightly behind condition later layers. also used standard inception network imagenet pre-train performed full imagenet labels. network required augmentation. weights copied self-supervised training except fully connected layer would discarded anyway. table shows results networks alexnet. compcars results tend within percentage point imagenet supervised training. however runs three six. results self-supervision seem enticingly close imagenet supervised there. authors would like thank several people contributed ideas conversation work carmen carrano alexei efros gerald friedland grathwohl brenda doug poland richard zhang. work performed auspices u.s. department energy lawrence livermore national laboratory under contract de-ac-na supported llnl-ldrd program project -si-.", "year": 2017}