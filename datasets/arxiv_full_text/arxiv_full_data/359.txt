{"title": "Learning to Play Othello with Deep Neural Networks", "tag": ["cs.AI", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Achieving superhuman playing level by AlphaGo corroborated the capabilities of convolutional neural architectures (CNNs) for capturing complex spatial patterns. This result was to a great extent due to several analogies between Go board states and 2D images CNNs have been designed for, in particular translational invariance and a relatively large board. In this paper, we verify whether CNN-based move predictors prove effective for Othello, a game with significantly different characteristics, including a much smaller board size and complete lack of translational invariance. We compare several CNN architectures and board encodings, augment them with state-of-the-art extensions, train on an extensive database of experts' moves, and examine them with respect to move prediction accuracy and playing strength. The empirical evaluation confirms high capabilities of neural move predictors and suggests a strong correlation between prediction accuracy and playing strength. The best CNNs not only surpass all other 1-ply Othello players proposed to date but defeat (2-ply) Edax, the best open-source Othello player.", "text": "abstract—achieving superhuman playing level alphago corroborated capabilities convolutional neural architectures capturing complex spatial patterns. result great extent several analogies board states images cnns designed particular translational invariance relatively large board. paper verify whether cnn-based move predictors prove effective othello game signiﬁcantly different characteristics including much smaller board size complete lack translational invariance. compare several architectures board encodings augment state-of-theart extensions train extensive database experts’ moves examine respect move prediction accuracy playing strength. empirical evaluation conﬁrms high capabilities neural move predictors suggests strong correlation prediction accuracy playing strength. best cnns surpass -ply othello players proposed date defeat edax best open-source othello player. neural networks particularly deep convolutional ones excel recognizing virtually kinds patterns naturally generalize arbitrary dimensionality makes suitable analyzing raster images time series video sequences volumetric data medical imaging properties make cnns powerful solving wide range challenging tasks unprecedented performance level ranging image classiﬁcation object localization object detection image segmentation even visual reinforcement learning environments structural analogy raster images board states games natural wonder exploited within neural paradigm past cases however neural nets served board evaluation functions employed searching game trees. recently massive computing efﬁcient learning algorithms deep cnns enabled direct move predictors capable achieving level play challenging games previously thought exclusive human players. gave rise line go-playing programs including paramount achievement alphago superhuman-level go-playing program authors institute computing science poznan university technology poznan poland. ja´skowski work staying idsia dalle molle institute artiﬁcial intelligence research manno switzerland; e-mails {pliskowski wjaskowski kkrawiec}cs.put.poznan.pl. cnns’ success possible known long time players rather performing mental tree search future game states rely heavily detecting patterns board states. this together high branching factor large board renders tree search approaches ineffective whether cnns practical potential games small branching factor much smaller board size minimax-style tree search algorithms perform well achieve human-level performance. consider learning cnn-based move predictors game othello long-standing benchmark problem diverges also different analysis images computer vision input rasters tiny free noise distortions typical real-world settings desirable invariances involve axial symmetries rather translation scaling. last least every single piece board matters inﬂuences decision. cnns hardly ever evaluated settings. contributions include experimental study different cnn-based architectures deep neural networks othello; state-of-the-art move prediction accuracy french othello league game dataset wthor; iii) state-of-theart -ply policy othello signiﬁcantly improves previous approaches; in-depth analysis characteristics trained policies including relationship move prediction accuracy winning odds strength weaknesses particular game stages confrontation opponents employ game tree search different depths. gathered evidence suggests several proposed neural architectures best-to-date move predictors othello. cnns introduced fukushima neocognitron occasionally used selected image analysis tasks next decades attempts applying broader classes images remained however largely unsuccessful. recent advent deep learning brought substantial conceptual breakthroughs enabling training deeper complex networks. this combined cheap computing power offered gpus revolutionized machine perception allowed achieving unprecedented classiﬁcation accuracy exempliﬁed chain ever-improving neural architectures assessed imagenet database structural analogy natural images game boards quite obvious humans could remain unnoticed long. early attempts applying cnns board games soon became clear deep learning paradigm powerful enough used together game tree search techniques. claim deepmind’s alphago ﬁrst superhuman-level playing system combined supervised learning policy gradient search monte carlo tree search. othello long time popular benchmark computational intelligence methods strong othello-playing programs variant minimax search board evaluation function. past research suggested gained improving latter former; recently focus mostly training look-ahead agents using either self-play ﬁxed opponents expert game databases multiple ways training agents proposed value-based temporal difference learning evolution hybrids thereof designing board evaluation function involves ﬁnding good function approximator. successful function approximators board games n-tuple networks. interestingly originally proposed optical character recognition reuse board games another sign analogy images piece arrangements board feasible. employed ﬁrst time othello name tabular value functions buro famous logistello program later popularized lucas neural networks also used othello function approximators e.g. best knowledge cnns never used purpose. aware published move predictors othello experimental part paper compare method representative sample -ply strategies. comparison state-of-the-art -ply players othello found among published works best -ply player date obtained coevolutionary cmaes systematic n-tuple networks. among multi-ply othello strategies contemporary leaders edax open source player uses tabular value functions stage game. edax highly optimized uses deep tree minimax-like search unfortunately description evaluation function obtained publicly available. date rl_iprefn trained using preference learning french othello league expert games dataset reportedly obtaining classiﬁcation accuracy. paper signiﬁcantly improve baseline. othello perfect information zero-sum two-player strategy game played board. identical pieces white side black other. game begins player pieces placed diagonally center board black player moves ﬁrst placing piece four shaded locations lead board state fig. move legal newly placed piece adjacent opponent’s piece causes opponent’s pieces become enclosed sides horizontal vertical diagonal line. enclosed pieces ﬂipped. players alternate placing pieces board. game ends neither player legal move player pieces board wins. players number pieces game ends draw. despite simple rules game othello trivial. number legal positions approximately game tree order nodes precludes exhaustive search method. othello also characterized high temporal volatility high number pieces ﬂipped single move dramatically changing board state. features fact game solved makes othello interesting test-bed computational intelligence methods. since othello deterministic fully-observable environment policy mapping space board states space actions correspond one-to-one board positions. represents desired move board state given training examples good moves formulate problem ﬁnding multi-class classiﬁcation problem. typical approach problems training parameterized probability function represents conﬁdence figure exemplary move prediction. board network two-channel binary image ones indicate locations player’s opponent’s pieces units feature receive data image network uses zero-padding edges maps layers clarity hidden layer feature maps. cited introduction). following brieﬂy describe making move state estimate needs architecture tailored move prediction minimize cross-entropy loss othello referring readers interested topic comprehensive studies clarity present core architecture augment extensions subsequent experimental section. otherwise. task training algorithm estimate optimal policy parameterization brings sufﬁciently close global optimum. found player chooses moves highest conﬁdence legal subset moves legal board state determines move make without explicit simulation future game course refer move predictor. one-to-one correspondence board locations moves othello greatly facilitates design move predictors. board games involve heterogeneous pieces move around board space moves complex requires sophisticated representation. simplest scenario board states could encoded ternary variables probability estimator. however generic probability estimators biased towards kind structure relates board state variables othello. structure exists enough imagine board variables randomly permuted would completely obfuscate game human players shows capturing spatial patterns player’s opponent’s pieces essential makes particularly appropriate take advantage learning systems capabilities i.e. treat board state image. composite multiple elementary processing units equipped weighted inputs output performing convolution input signals weights transforming outcome form nonlinearity. units arranged rectangular feature maps spatially aligned raster input image. spatial arrangement units primary characteristic makes cnns suitable processing visual information. feature maps arranged layers maps given layer fetch signals previous layer cnns take input representation current board state encoded binary matrices. indicate locations player’s pieces ﬁrst matrix opponent’s pieces second matrix. refer encoding pieces. encoding called vmoves third matrix mark player’s legal moves facilitate picking valid moves game playing. following recent research conducted deﬁne also ones encoding includes additional regularizing channel pieces encoding contains ones. board state represented forms binary image comprising three channels fetched feature maps ﬁrst layer cnn. unit feature receives data receptive ﬁeld small rectangle neighboring board locations however channels uses separate weight channel analogously subsequent layers unit fetches data corresponding feature maps previous layer treats separate weight sets. neighboring units offset stride. board size size stride together determine dimensions feature map. instance feature stride needs units applied board width each overlapping rows span entire board width local connectivity substantially reduces number weights facilitates capturing spatial patterns. units within feature share weights calculate local feature albeit different part board. reduces number parameters even makes extracted features equivariant. instance feature mentioned parameters b-channel input rectiﬁed linear units process convolution outcomes nonlinearity deﬁned max. relus make networks learn faster squeezing-function units particularly effective networks many layers effectively alleviating vanishing gradient problem gradient backpropagates undistorted positive excitations contrast small interval excitations squeezing units. basic architectures networks composed convolutional layers shown table architecture input passed stack convolutional layers composed feature maps stride since input small zeros grid border order preserve spatial resolution convolution. reason form pooling i.e. aggregating outputs multiple units means convolution little rationale small input grids consequently feature maps layers units. architectures stack convolutional layers followed fully-connected layers. hidden layer consists units relu activations output layer units corresponding possible move candidates. output corresponds single board location desired moves encoded using binary -of-k scheme output values interpreted probabilities must unity achieved using softmax transformation architectures presented table resulted series preliminary experiments found increasing network depth beyond eight convolutional layers improve performance signiﬁcantly lengthening training time. also small deeper networks turn section consider networks move predictors engage actual games. performance indicator prediction accuracy i.e. percentage correctly predicted moves. compare various conﬁgurations neural move predictors described section trained examples moves extracted games played humans datasets created wthor database contains records games played professional othello players various french tournaments. records extracted board states accompanied color player move move chosen player particular state. resulting triples referred original. removed duplicates obtaining unique triples referred unique dataset following. note datasets contain inconsistent examples i.e. board state associated different moves. perfect classiﬁer achieve accuracy original unique. color attribute used invert pieces boards white player move states dataset seen black player’s perspective. deep neural networks known perform particularly well trained large volumes data. order augment datasets take advantage othello’s invariance board symmetries. board state original unique datasets produce seven reﬂections rotations. results respectively symmetric datasets original-s unique-s latter cleared also duplicates resulting symmetric transformations. assess generalization capabilities move predictors partition dataset disjoint training testing sets. asymmetric datasets allocate examples testing. symmetric datasets much larger allocate examples testing test sets similar size. training training stochastic gradient descent momentum. units’ weights initialized using he’s method biases initialized zeroes. learning rate initially halved twice epoch. loss function regularized implementation based theano framework performs computation singleprecision arithmetic. experiments conducted intel core nvidia titan gpu. training time varied single hours dozens hours depending conﬁguration e.g. nearly hours conv trained unique-s. effect data augmentation table presents testset prediction accuracy three basic network architectures trained particular training sets pieces encoding. fairness networks compared data i.e. original test set. training datasets augmented symmetries systematically leads higher prediction accuracy. impact removing duplicates also consistently positive albeit strong. trends together render unique-s promising following experiments dataset only. table presents prediction accuracy three architectures trained unique-s various encodings board state. time networks tested uniquetest explains minor differences w.r.t. table original test used testing. accuracy clearly correlates number layers tempting reach even deeper architectures. however -layer architecture analogous conv failed improve prediction accuracy. concerning comparison encodings pieces ones seem perform former preferred require additional input layer/channel. relative underperformance vmoves puzzling hard giving learner hint moves’ validity could deteriorate performance. attempt investigate outcome evaluate networks respect ability discern valid invalid moves. table present test-set accuracy particular networks indicates network’s excited output always among valid moves. considered networks encodings bring indicator close perfection networks using vmoves making mistakes respect. suggests telling apart valid moves invalid ones easy regardless conﬁguration providing information explicitly vmoves unnecessary distract training process. therefore pieces encoding experiments follow. dropout consists disabling number randomly selected units individual batches training examples forces network form multiple alternative pathways inputs outputs. usually makes networks robust reduces overﬁtting particularly domains input data inherently redundant like natural images typically composed least thousands pixels. states othello board however different inputs every single piece counts choosing move make. virtually redundancy input. table suggests dropout prevents conv network capturing board state full hampers predictive accuracy. purpose batch normalization address internal covariate shift standardizing outputs individual network units batches examples demonstrated beneﬁcial many contexts. corroborated case percent gain accuracy consider conﬁguration baseline throughout rest paper. finally attempt reduce classiﬁer’s variance employing bootstrap aggregation train networks bootstrapped samples drawn training average corresponding outputs determine move make. applied conv+bn network bagging boosts performance price paid -fold increase training time finally recent work architectures dubbed widenet revealed number layers residual networks signiﬁcantly reduced little harm accuracy making residual blocks expressive. achieved increasing number convolution layers within block increasing number feature maps both. instance architecture comprises stacks residual blocks stack composed blocks. residual block contains convolutional layers. layers consecutive stacks equipped respectively feature maps network four times wider original resnet. last table demonstrates widenet conﬁguration fares best among skipconnections architectures considered here suggests aggregating multiple features various stages processing essential successful move prediction. however widenet- also slowest networks queried. architectures skip-connections recent research brought deep network architectures containing dozens layers became state-ofthe-art image classiﬁcation. training networks possible skip connections form shortcuts pathways layers typically adding representation learned given layer representation learned consecutive layers. this particularly repeated multiple times across network opens ways achieving training goal rather learning mapping network learns residual skip connections alleviates vanishing gradient problem gradient backpropagate unchanged many layers. table presents predictive accuracy several skipconnection architectures. resnet-n network consists residual blocks comprising nonlinear layer followed linear layer shortcut skip connection adds input ﬁrst layer output second one. originally designed image analysis resnets perform downsampling reduces input dimensionality; arguably little need small input consider variant stripped feature resnet-n-np. expected maintaining dimensionality input throughout learning improves accuracy smaller larger architecture. simple adding layer’s outputs possible dimensions. otherwise authors resnets propose implement skip connections linear mappings. resnet-n-np-p conﬁgurations table implement feature slightly improves predictions. resnets’ skip connections considered ﬁrst-order spans inputs outputs residual block. densenets skip connections introduced pair layers network layers them. trained densenet architectures. smaller managed perform networks considered section larger signiﬁcantly outperformed them almost reaching performance level baseline. board states typical initial game stages well occurring endgame. thus interesting difﬁcult network perform well particular stages game. here answer question terms prediction accuracy. fig. present top-k prediction accuracy baseline network conv+bn factored respect number move game number legal moves given state. given four pieces initial board state former factor vary however start ﬁfth move always othello symmetries color datapoint reﬂects probability correct move among indicated network. prediction accuracy remains largely consistent throughout game. game’s beginning network’s predictions less accurate attribute relatively small amount training data unique-s game stage aspect play suggested differences top- top- graphs large fraction inconsistent examples early game states many alternative paths success players tend follow accordingly personal habits preferences. high quality predictions later game stages particularly encouraging even single move towards game drastically change state board feature othello owes name prediction accuracy considered previous section surrogate measure performance since ultimately interested playing strength. section predictors play players. consider suite thirteen -ply players proposed previous research gathered compared former study suite consists players board evaluation functions encoded weighted piece counter n-tuple networks trained different methods including hand-design temporal difference learning evolution coevolution addition suite opponents consider also edax ver. strong open source highly optimized othello program. edax conﬁgured play given denote edax-n. since move predictors well opponents deterministic resort following technique order obtain robust estimates odds winning. rather playing single game starts initial board -ply opening positions prepared runarsson lucas starting state games played players switching roles. single game player obtain points selected representative sample best predictors section confronted opponents suite well edax-. table presents average winning rates players separately winning rates cocmaes-+x proved best edax-. networks turn signiﬁcantly stronger existing -ply look-ahead players achieving around winning rate them. interesting given move predictors essentially -ply strategies perform look-ahead. conv+bn achieves highest average winning rate slightly worse conv+bn+bagging playing cocmaes-+x widenet playing edax-. overall performance corroborates high prediction accuracy. roughly monotonous relationship prediction accuracy winning rate observed table lets whether tendency holds general. fig. plot latter former networks evaluated paper confronted models suite edax-. graph reveals strong linear dependency fitting linear models suggests considered range prediction accuracy percent point prediction accuracy brings percent points winning rate suite players percent points edax-. obviously trend cannot hold globally illustrated conv+bn+bagging although might outlier also suggest percent approximately point increases prediction accuracy translate better winning rates anymore. section found training stripped duplicates augmented symmetries gives rise best move prediction accuracy. however prediction accuracy proxy really care winning rate. thus obvious whether analogous claim holds playing strength search depth impacts mean time required edax make move report last column table. edax highly optimized able make moves rate comparable prediction time conv edax longer. hand however times reported neural predictors assessed batches board states common efﬁciency means computing. asked predict move single board state cnns times slower. table includes also edax prediction accuracy original dataset clearly grows depth search. however increasing depth improve accuracy despite associated increase headto-head matches conv+bn. also edax’s prediction accuracy signiﬁcantly lower demonstrated conv+bn. seems thus edax plays different human players cnns mimic. much performance -ply move predictors impressive previous experiments show deeper lookaheads essential achieving high winning rates. last experiment seek game stage beneﬁt supplementing move predictor game tree search split game four stages according intervals move numbers stage design hybrid strategy employs edax- stage using conv+bn policy make decisions remaining stages. hybrid strategies play edax-... results summarized fig. conﬁrm letting edax make moves game stage improves performance hybrid. gains largest demonstrated section vi-d smaller edax relatively easy beat already pure conv+bn extending edax- single game stage improve odds winning much. conversely opponent edax-n becomes strong hard increase odds winning either. graphs reveal also gain later stages game regardless opponent’s depth. suspect combinatorial nature game makes cnn’s pattern recognition capabilities less relevant success last game stage. also given othello games last moves search depth last game stage enables edax often reach ﬁnal game states obtain robust assessments potential moves. last least certain fraction moves derived wthor suboptimal might ‘deceived’ networks. figure networks’ move prediction accuracy winning rate opponent players edax table viii winning rates players suite basic neural predictors trained different datasets winning rates presented table viii corroborate outcomes terms prediction accuracy presented earlier table duplicate removal augmentations signiﬁcantly positive effects playing strength. correlation shows that certain intervals move prediction accuracy reliable predictor playing strength section vi-b revealed -ply neural move predictors outperform -ply opponents large margin interesting margin decreases opponents allowed deeper game-tree searches. investigate this confront conv+bn edax-n search depth gather results table remarkably predictor maintains superiority edax allowed -ply lookahead though odds winning roughly starting search depth conv+bn likely lose -ply search odds winning drop however still impressive given average branching arbitrary complex combinatorial patterns occurring entire board states. natural follow-up step thus considering fullyconnected networks suffer limitation; however also much costly training tend overﬁt. relation that interesting study regarding fully-connected networks othello hidden layers performed deep cnn-based move predictor nearly accuracy turned much better n-tuples-based preference learning achieved dataset. conv--bn wins games. obtained similar winning rate also cocmaes player strongest to-date othello player despite othello’s relatively branching factor small board cnns proved supreme move predictors managing efﬁciently capture expertise materialized millions humans’ moves. impressive given small discrete-valued board states arguably different natural images. seeing convolutional network perform well board game characterized little translation invariance intriguing main motivation using convolution precisely translation invariance. suggests certain patterns observed othello boards extent translation-invariant least detected using ‘higher-order convolutions’ implemented stack several convolutional layers intertwined nonlinearities. concerning limitations approach could improve upon result obtained conv--bn network using deeper networks skip connections. suggests works high-dimensional image data always succeeds low-dimensional problems othello. interesting observation concerning training large networks hardly overﬁt data. classiﬁcation accuracy training typically within percent point accuracy test suggesting networks considered cannot memorize training data entirely. conclude thus still room improving architectures. particular sign inherent limitation convolutional processing limited size insufﬁcient capture study brings evidence deep neural networks viable methodology training othello agents. require explicit knowledge game rules strategies extensive game-tree searches explicitly compare individual future board states actions. rather that achieve high level playing learning associations patterns observed board states desired moves. claim thus that extent trained neural predictors embody knowledge othello domain statement would hard defend game-tree search techniques power largely algorithmic efﬁciency. hand supervised approach proposed cannot deemed ‘knowledge-free’ sense instance evolutionary reinforcement learning methods. here essential know-how hidden expert games used training. cnns power lies ability extract incorporate knowledge system able generalize beyond training examples something method past shown well enough achieve expert level play othello. given supreme performance cnns work worth extending several directions. networks rely fully-connected layers mentioned section possibility. logistello edax different evaluation functions stage game ‘factorization game course’ could also lead better agent. another arguably simple extension information player move expect beneﬁcial given othello nonsymmetric game. last least given usefulness convolutional features evidenced here would interesting verify capabilities contexts supervised learning move predictors e.g. state state-action evaluation functions trained reinforcement learning methods. grant //n/st/ funded national science centre poland. ja´skowski acknowledges support ministry science higher education grant mobility plus /mob/iv//. krawiec supported national science centre poland grant //b/st/. simon lucas thomas runarsson. temporal difference learning versus co-evolution acquiring othello position evaluation. ieee symposium computational intelligence games pages ieee joseph redmon santosh divvala ross girshick farhadi. look once uniﬁed real-time object detection. proceedings ieee conference computer vision pattern recognition pages michiel marco wiering. reinforcement learning game othello learning ﬁxed opponent learning self-play. adaptive dynamic programming reinforcement learning ieee symposium pages p.s. rosenbloom. world-championship-level othello program. artiﬁ thomas runarsson simon lucas. preference learning move prediction evaluation function approximation othello. computational intelligence games ieee transactions spyridon samothrakis lucas runarsson david robles. coevolving game-playing agents measuring performance intransitivities. ieee transactions evolutionary computation david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature ioannis skoulakis michail lagoudakis. efﬁcient reinforcement learning adversarial games. ieee international conference tools artiﬁcial intelligence pages ieee maciej szkulmowski daniel ruminski paweł liskowski bartosz wieloch krzysztof krawiec bartosz sikorski maciej wojtkowski. retinal angiography using neural networks. investigative ophthalmology visual science marcin szubert wojciech ja´skowski krzysztof krawiec. learning board evaluation function othello hybridizing coevolution temporal difference learning. control cybernetics marcin szubert wojciech ja´skowski krzysztof krawiec. scalability generalization hybridization coevolutionary learning case study othello. ieee transactions computational intelligence games sjoerd dries marco wiering. neural-fitted td-leaf learning playing othello structured neural networks. ieee transactions neural networks learning systems november kevin binkley seehart masafumi hagiwara. study artiﬁcial neural network architectures othello evaluation functions. information media technologies michael buro. experiments multi-probcut high-quality evaluation function othello. h.j. herik iida editors games research pages university maastricht kunihiko fukushima miyake. neocognitron self-organizing neural network model mechanism visual pattern recognition. competition cooperation neural nets pages springer ross girshick jeff donahue trevor darrell jitendra malik. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition pages kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision pages kaiming xiangyu zhang shaoqing jian sun. deep proceedings ieee residual learning image recognition. conference computer vision pattern recognition pages sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv. wojciech ja´skowski. systematic n-tuple networks position evaluation exceeding othello league. technical report arxiv. institute computing science poznan university technology pozna´n poland wojciech ja´skowski paweł liskowski marcin szubert krzysztof krawiec. christian blum editor gecco’ proceedings annual conference genetic evolutionary computation pages amsterdam netherlands july acm. wojciech ja´skowski marcin szubert. coevolutionary cma-es knowledge-free learning game position evaluation. ieee transactions computational intelligence games michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. ieee conference computational intelligence games pages santorini greece ieee. krzysztof krawiec marcin grzegorz szubert. learning n-tuple networks othello coevolutionary gradient search. proceedings annual conference genetic evolutionary computation gecco pages york york press. simon lucas. learning play othello n-tuple systems. australian journal intelligent information processing systems special issue game technology", "year": 2017}