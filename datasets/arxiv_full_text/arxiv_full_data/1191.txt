{"title": "Learning Deep Object Detectors from 3D Models", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Crowdsourced 3D CAD models are becoming easily accessible online, and can potentially generate an infinite number of training images for almost any object category.We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.", "text": "figure propose train few-shot object detectors real images augmenting training data synthetic images generated freely available non-photorealistic models objects collected dwarehouse.sketchup.com. used successfully past afﬁne transformations training images recognize text even train detectors handful categories cars however demonstrated detection many categories modern dcnns. trained object detectors categories synthetic images used histogram-of-oriented gradient model signiﬁcantly less powerful dcnns object classiﬁcation detection main challenge training freely available models capture shape object frequently lack low-level cues object texture background realistic pose lighting etc. used simple rendering objects uniform gray texture white background showed hog-based models learn well data invariant color texture mostly retain overall shape object. however dcnn visualizations shown retain color texture mid-level patterns. therefore unknown would tolerate lack low-level cues training images sophisticated rendering process simulates cues needed. investigate missing low-level cues affect dcnns’ ability learn object detectors study precise nature invariances. given object category dcnn maps low-level cues contained image high-level category information represented layer activations deﬁne invariance ability network extract equivalent high-level category crowdsourced models becoming easily accessible online potentially generate inﬁnite number training images almost object category. show augmenting training data contemporary deep convolutional neural models synthetic data effective especially real training data limited well matched target domain. freely available models capture shape often missing level cues realistic object texture pose background. detailed analysis synthetic cad-rendered images probe ability dcnn learn without cues surprising ﬁndings. particular show dcnn ﬁne-tuned target detection task exhibits large degree invariance missing low-level cues pretrained generic imagenet classiﬁcation learns better low-level cues simulated. show synthetic dcnn training approach signiﬁcantly outperforms previous methods pascal dataset learning few-shot scenario improves performance domain shift scenario ofﬁce benchmark. deep models achieve state-of-the-art performance object detection heavily dependent largescale training data. unfortunately labeling images detection extremely time-consuming every instance every object must marked bounding box. even largest challenge datasets provide limited number annotated categories e.g. categories pascal coco imagenet wanted train detector novel category? feasible compile annotate extensive training covering possible intra-category variations. propose bypass expensive collection annotation real images using freely available models automatically generate synthetic training images synthetic data augmentation figure learn deep detectors real images non-photorealistic models? explore invariance deep features missing low-level cues shape pose texture context propose improved method learning synthetic data simulates cues. information despite missing low-level cues. expect network learn different invariances depending task trained quantifying invariances could help better understand dcnn models impove transfer domains e.g. non-photorealistic data. small number papers started looking problem many open questions remain dcnns invariant object color? texture? context? pose? invariance transferable tasks? help images synthetically rendered models design series experiments peer depths dcnns analyse invariance cues including ones difﬁcult isolate using real image data. make surprising discoveries regarding representational power deep features. particular show encode complex invariances cues pose color texture context previously accounted for. also quantify degree learned invariances speciﬁc training task. based analysis propose method zerofew-shot learning novel object categories generates synthetic data using models texture scene images related category. advantage approach drastically reduces amount human supervision traditional bounding-box labeling methods. could greatly expand available sources visual knowledge allow learning detectors millions models available web. present experiments pascal detection task show training data missing limited novel category method outperforms training real data synthetic method also demonstrate advantage approach setting real training data comes different domain target data using ofﬁce benchmark. object detection. flat hand-designed representations dominated object detection literature considerable invariance factors illumination contrast small translations. combination discriminative classiﬁers linear exemplar-based latent proved powerful learning localize global outline object. recently convolutional neural networks overtaken features clear front-runners many image understanding tasks including object detection. dcnns learn layered features starting familiar pooled edges ﬁrst layer progressing complex patterns increasing spatial support. extensions detection included sliding-window regions-cnn understanding deep cnns. increasing interest understanding information encoded highly nonlinear deep layers. reversed computation image patches highly activate isolated neuron. detailed study happens transfers network layers dataset another presented reconstruct image layer’s activations using image priors recover natural statistics removed network ﬁlters. visualizations conﬁrm progressively invariant abstract representation image formed successive layers analyse nature invariances. invariance simple transformations explored paper study complex invariances deconstructing image shape texture factors seeing speciﬁc combinations result high-layer representations discriminant object categories. synthetic data. synthetic data longstanding history computer vision. among earliest attempts used models primary source information build object models. recently used models source labeled data limited work categories like cars motorcycles. utilized synthetic data probe invariances features like sift etc. paper generate training data crowdsourced models noisy low-quality free available many categories. evaluate approach categories pascal dataset much larger realistic previous benchmarks. previous works designed special features matching synthetic object models real image data used features linear svms employ powerful deep convolutional images features demonstrate advantage directy comparing authors models show results detection pose estimation train multi-view detectors real images labeled pose. avoid expensive manual bounding pose annotation show results minimum real image labels. finally several approaches used synthetic training data tasks object detection. example recently proposed synthetic text generation engine perform text recognition natural scenes proposed technique improve novel-view synthesis images using structural information models. approach learns detectors objects training examples augmenting training data synthetic images. overview approach shown figure given models object generates synthetic image training dataset simulating various low-level cues extracts positive negative patches object synthetic images patch deep neural network computes feature activations used train ﬁnal classiﬁer deep detection method rcnn explore invariance networks trained different ways described section realistic object appearance depends many low-level cues reﬂectance location spectral distributions illumination sources properties background scene camera characteristics others. choose subset factors easily modeled using computer graphics techniques learning detection model category limited labeled real data choice whether simulate cues synthetic data depends invariance representation. example representation invariant color grayscale images rendered. study invariance dcnn representation parameters using synthetic data generated follows. models viewpoints crowdsourced models thousands objects becoming freely available online. start downloading models warehouse searching name desired object categories. category around models obtained experiments explore effect varying intra-class shape restricting number models experiments. original poses models arbitrary therefore adjust models’s viewpoint manually views best represent intraclass pose variance real objects. next manually speciﬁed model view generate several small perturbations adding random rotation. finally pose perturbation select texture color background image render virtual image include virtual training dataset. next describe detailed process factors. object/background color texture investigate various combinations color texture cues object background image. previous work shown learning detectors virtual data using features rendering natural backgrounds texture helpful equally good results obtained white background uniform gray object texture. explain fact hog-based classiﬁer focused learning outlines object shape invariant color texture. hypothesise case different dcnn representations neurons shown respond detailed textures colors mid-level patterns explore invariance dcnns factors. speciﬁally examine invariance dcnn representation types object textures realistic color textures uniform grayscale textures case background scenes examine invariance three types scenes namely real-image color scenes real-image grayscale scenes plain white background. examples texture background generation settings shown table real objects extract textures therein annotating bounding box. texture images stretched models. likewise order simulate realistic background scenes gathered real images scenes category likely appear generating virtual image ﬁrst randomly select background image available background pool project onto image plane. then select random texture image texture pool onto model rendering object. obtain deep feature representation images eight-layer alexnet architecture million parameters network ﬁrst achieved breakresults ilsvrc- image classiﬁcation remains studied widely used visual convnet. network trained fully supervised backpropagation takes image pixels ﬁxed size outputs object category labels. layer consists neurons linear weights input followed nonlinearity. ﬁrst layers network local spatial support convolutional ﬁnal three layers fullyconnected neuron previous layer thus include inputs entire image. network originally designed classiﬁcation applied ﬁne-tuned detection rcnn impressive gains popular object detection benchmarks. adapt alexnet detection rcnn applied network image sub-region proposed selective search method adding background label applied non-maximal suppression outputs. fine-tuning hidden layers resulted performance improvements. refer reader details. recall deﬁne invariance ability network extract high-level category information training images despite missing low-level cues object texture. test invariance create synthetic training sets without particular cue. extract deep features sets train object detectors compare performance real test data. hypothesis that representation invariant similar high-level neurons activate whether present input image leading similar category-level information training thus similar performance. hand features invariant missing result missing category information poorer performance. work extract last hidden layer pre-trained generic imagenet ilsvrc -way classiﬁcation task network additionally ﬁne-tuned pascal -category detection task case category labels ﬁne-tune imgnet network synthetic data obtain vcnn network ﬁne-tune entire network synthetic data backpropagating gradients lower learning rate. effect adapting hidden layer parameters synthetic data. also allows network learn information object categories synthetic data thus gain objectclass invariances. show essential good performance few-shot scenario. treating network activations ﬁxed features inferior learning capacity hidden layers ﬁnal classiﬁer. investigate degree presence different low-level cues affects well network learn synthetic data. ﬁrst evaluate variations low-level cues affect features generated imgnet pasc-ft networks pascal dataset. experiment follow steps select cues generate batch synthetic images cues sample positive negative patches class extract hidden dcnn layer activations patches features train classiﬁer object category test classiﬁers real pascal images report mean average precision determine optimal number synthetic training images computed function size training using rr-rr image generation setting results shown figure inobject color texture context experiment used pose perturbations view views category. trained series detectors several background object texture conﬁgurations results shown table first expected training synthetic data obtains lower mean training real data also imgnet network representation achieves lower performance pasc-ft network case real data however somewhat unexpected result generation settings rr-rr w-rr w-ug rgrr pasc-ft achieve comparable performance despite fact w-ug texture context. results real texture color background best. thus pasc-ft network learned invariant color texture object background. also note settings rr-ug rg-ug achieve much lower performance potentially uniform object texture well distinguished non-white backgrounds. imgnet network trend similar best performing methods rr-rr rg-rr. means adding realistic context texture statistics helps classiﬁer thus imgnet network less invariant factors least categories dataset. note imgnet network seen categories training part ilsvrc -way classiﬁcation task explains still fairly insensitive. combinations uniform texture real background also perform well here. interestingly rg-rr well networks leading conclusion networks learned associate right context colors objects. also variations across categories e.g. categories like sheep beneﬁt adding object texture cue. explore lower layers’ invariance color texture background visualize patches strongest activations pool units shown figure value receptive ﬁeld’s upper-left corner normalized dividing activation value units channel. results interesting. unit left subﬁgure ﬁres patches resembling tv-monitors real images; using synthetic data unit still ﬁres tv-monitors even though background texture removed. unit right ﬁres white animals green backgrounds real rr-rr images continues synthetic sheep simulated texture despite lack green background. however fails w-ug images demonstrating speciﬁcity object color texture. synthetic pose also analyse invariance features object pose. successive operations convolution max-pooling cnns built-in invariance translations scale. likewise visualizations learned ﬁlters early layers indicate built-in invariance local rotations. thus representation invariant slight translation rotations deformations remains unclear extent representation large rotations. experiment models three dominant poses front-view side-view intra-view shown table change number views used experiment keep total number synthetic training images exactly same generating random small perturbations around main view. results indicate networks adding side view front view gives boost improvement adding third view marginal. note adding views even hurt performance pascal test objects views. real image pose also test view invariance real images. interested objects whose frontal view presentation differs signiﬁcantly selected categories pascal training match criteria. held categories included rotationally invariant objects bottles tables. next split training data categories prominent side-view front-view shown table train classiﬁers exclusively removing view test resulting detector pascal test containing side front-views.we also compare random view sampling. results shown table point important surprising conclusions regarding representational power features. note drops less detectors exclusively trained removing either view tested pascal test set. detectors never presented second view also trained approximately half data. invariance large complex pose changes explained fact model trained views object present subsequently ﬁne-tuned views table detection results pascal test dataset. trained different background texture conﬁguration virtual data shown table. middle table dcnn trained imagenet ilsvrc classiﬁcation data ﬁnetuned pascal training data; bottom table network ﬁne-tuned pascal. figure regions strongest activations pool units using method overlay unit’s receptive ﬁeld drawn white normalized activation value shown upper-left corner. unit show results real pascal images rr-rr w-rr w-ug. text explanation. present level invariance nevertheless remarkable. last experiment reduce ﬁne-tuning training removing front-view objects note larger drop points much less expect. conclude that networks representation groups together multiple views object. shape finally experiment reducing intraclass shape variation using fewer models category. otherwise settings rr-rr condition pasc-ft. experiments decreases points using half models. shows signiﬁcant boost adding shape variation training data indicating less invariance factor. summarize conclusions previous section found dcnns learn signiﬁcant amount invariance texture color pose less invariance shape trained task. trained task degree invariance lower. therefore learning detection model category section experiment adapting deep representation synthetic data. available models views compare generation settings produced best results settings realistic backgrounds advantages detection. particular visualizations positive training data show white background around objects makes harder sample negative training data selective search interesting regions object. before simulate zero-shot learning situation number labeled real images novel category zero however also experiment small number labeled real images. every category randomly select positive training images form datasets sizes ﬁnal datasets note images contain positive bounding boxes. size virtual dataset always images. pre-train imagenet ilsvrc figure detection results proposed vcnn pascal. real annotated images limited available novel category vcnn performs much better rcnn fast adaptation method. ﬁne-tune vcnn network train classiﬁers rx+vk. baselines. datasets train rcnn model rx+vk train fast adaptation method described rcnn pre-trained imagenet ilsvrc however ﬁne-tuned detection data limited. results. results figure show number real training images limited method performs better traditional rcnn. vcnn also significantly outperfoms fast-adapt method based features. also conﬁrm proposed rrrr data synthesis methodology better simulating background texture. partcular ﬁne-tuning virtual rr-rr data boosts without using real training examples real images category absolute improvement rcnn. also notice results rg-rr much lower rr-rr unlike results ﬁxed-feature experiment. explained fact rg-rr selective search generates many sub-regions without color using regions ﬁne-tuning probably decreases cnn’s ability recognize realistic color objects. table detection results proposed vcnn object categories ofﬁce dataset. test data experiments images amazon domain. compare training real training images webcam domain model trained v-gray v-tx representing virtual images uniform gray texture real texture respectively. results clearly demonstrate real training data mismatched target domain synthetic training provide signiﬁcant performance boost real-image detection. much fewer annotated bounding boxes pascal training much easier collect texture images need bounding annotation. obtained comparable achieved trained full dataset. speaks power transferring deep representations suggests synthetic data promising avoid tedious annotation novel categories. emphasize signiﬁcant boost adapting features synthetic data ﬁne-tuning showing adapted features better ﬁxed features rr-rr generation settings. test images come different visual domain training images expect performance detector degrade dataset bias experiment evaluate beneﬁt using synthetic data improve performance novel real-image domains. part ofﬁce dataset categories common objects domain amazon images target testing domain webcam images training domain generate synthetic data categories ofﬁce dataset downloaded roughly models category. data generation method experiments pascal expcept original texture models experiment considering texture objects ofﬁce dataset simpler. compare generation settings v-gray v-tx representing virtual images uniform gray texture real texture respectively. background settings white match majority amazon domain backgrounds. generate images model producing images total. synthetic images train vcnn deep detector test amazon domain baseline train baseline real-image deep detector webcam domain also test images amazon domain. results results shown table mean vcnn v-tx versus deep detector trained webcam domain signiﬁcant boost performance. v-gray setting considerably worse. shows potential synthetic training dataset bias scenarios. figure show examples object detected detector trained webcam detected perfectly vcnn model. obtain results selected bounding highest score region proposals image. paper demonstrated synthetic training modern deep cnns object detectors successful real-image training data novel objects domains limited. investigated sensitivity convnets various low-level cues training data pose foreground texture color background image color. simulate factors used synthetic data generated models. results demonstrated popular deep convnet ﬁne-tuned detection task indeed largely invariant cues. training synthetic images simulated cues lead similar performance training synthetic images without cues. however network ﬁne-tuned task invariance diminished. thus novel categories adding synthetic variance along dimensions ﬁne-tuning layers proved useful. based ﬁndings proposed method learning object detectors categories avoids need costly large-scale image annotation. advantageous needs learn detector novel object category instance beyond available labeled datasets. also showed method outperforms detectors trained real images real training data comes different domain case domain shift. ﬁndings preliminary experiments domains necessary. rematas ritschel fritz tuytelaars. imagebased synthesis re-synthesis viewpoints guided models. ieee conference computer vision pattern recognition oral. yosinski clune bengio lipson. transferable features deep neural networks? ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages", "year": 2014}