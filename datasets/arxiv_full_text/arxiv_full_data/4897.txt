{"title": "Options Discovery with Budgeted Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "text": "hierarchical reinforcement learning framework options models sub-policies primitive actions. paper address problem discovering learning options scratch. inspired recent works cognitive science approach based budgeted learning approach options naturally arise minimize cognitive effort agent. case effort corresponds amount information acquired agent time step. propose budgeted option neural network model hierarchical recurrent neural network architecture learns latent options continuous vectors. respect existing approaches bonn need explicitly predeﬁne sub-goals priori deﬁne number possible options. evaluate model different classical problems showing quality resulting learned policy. introduction works cognitive science long emphasized human animal behavior seen hierarchical process solving task amounts sequentially solving subtasks examples simple maze environment door corridor; sub-tasks correspond sequences primitive actions sub-tasks. computer science domain works hierarchical reinforcement learning paradigm motivated idea makes discovery complex skills easier. line approaches consists modeling sub-tasks options giving rise main research questions choosing best suited option selecting actions apply environment based chosen option. challenges respectively mediated using high-level low-level controllers. reinforcement learning literature different models proposed solve questions. additional question discovering options rarely addressed hierarchical structure majority existing models manually constrained e.g. predeﬁning possible sub-goals learning automatic task decomposition without supervision remains open challenge ﬁeld difﬁculty discovering hierarchical policies emphasized fact well understood emerge behaviors. recent studies neurosciences suggest hierarchy emerge habits indeed distinguish goal-directed habitual action control; latter using information environment. underlying idea agent ﬁrst uses goal-directed control progressively switches habitual action control since goal-directed control requires higher cognitive cost process acquired information. based idea propose bonn architecture framework agent access different amounts information coming environment. precisely consider time step agent basic observation denoted used low-level controller classical problems. addition also choose acquire supplementary observation illustrated fig. extra observation provide information current state system used high-level controller higher cognitive cost. setting assume options naturally emerge reduce overall cognitive effort generated policy. words constraining amount high-level information used system bonn learn hierarchical structure resulting policy sequence low-cost sub-policies. setting show structure resulting policy seen sequence intrinsic options i.e. vectors latent space. contributions paper threefold propose assumption options discovery arguing consequence learning trade-off policy efﬁciency cognitive effort. hierarchical structure emerges reduce overall figure typical setting agent receives reward observation environment time step executes action setting used bonn model agent additional information deﬁne model called bonn implements idea hierarchical recurrent neural network which time step observations available different prices. model learned using policy gradient method budgeted objective. paper organized follows related works presented section deﬁne background recurrent policy gradient methods section introduce bonn model learning algorithm budgeted problem section experiments presented discussed section last section concludes opens perspectives. related work closest architecture bonn hierarchical multiscale recurrent neural network discovers hierarchical structures sequences. uses binary boundary detector learned straight-through estimator similar acquisition model bonn.i generally hierarchical reinforcement learning surge many different works last decade since deemed solution solve longrange planning tasks allow transfer knowledge tasks. many different models assume subtasks priori known e.g. maxq method concept option introduced architecture option consists initiation policy termination function deﬁnes probability ending option given certain state. concept options core many recent articles. example authors propose extension deep q-learning framework integrate hierarchical value functions using intrinsic motivation learn option policies. different models options manually chosen priori discovered learning process. still options framework discover options without supervision using respectively expectation maximization algorithm option-critic architecture. contribution differs last bonn model ﬁxed discrete number options rather uses intrinsic option represented latent vector. moreover clearly state options arise ﬁnding good trade-off efﬁciency cognitive effort. last articles propose hierarchical policies based different levels observations. ﬁrst category models open-loop policies i.e. observation environment every time step. propose model mixes open-loop closed-loop control considering sensing incurs cost. models focus problem learning macroactions case given state mapped sequence actions. ancategory models divides state space several components. instance abstract hidden markov model based discrete options deﬁned space region. low-level controller access proprioceptive information high-level controller access observations. similar idea factoring state space components learn stochastic neural network high-level controller. blind setting bonn model described section similar macro-actions open-loop policies rather limited complex environments. general bonn architecture comparable works using different observations however models learn high-level controller. denote markov decision process states discrete possible actions transition distribution reward function consider state associated observation partial view size observation space. moreover denote probability distribution possible initial states mdp. figure bonn architecture. arrows correspond dependencies dashed arrows correspond sampled values. note example model decides acquire computes option model keeps option. number sampled trajectories used approximating gradient using monte carlo sampling techniques variance reduction term time estimated learning consider future actions depend past rewards details recurrent policy gradients). budgeted option neural network bonn architecture typical setting agent uses observation environment every time step. contrast bonn architecture agent always uses low-level observation also choose acquire high-level observation provide relevant information illustrated figure situation corresponds many practical cases example robot acquires information camera sometimes decide make complete scan room user driving decide consult virtual agent taking decisions virtual world instructions human etc. note particular case empty classical observation environment. case agent basically decide whether wants current observation goal directed habits paradigm structure bonn close hierarchical recurrent neural network hidden states composed three components. acquisition model aims choosing whether observation acquired not. agent decides acquire option model uses formally describe three components. schema bonn architecture provided fig. complete inference procedure given algorithm note sake simplicity consider last chosen action included low-level observation often done reinforcement learning avoid explicitly write last chosen action equations. relevant representations learned neural network linear models case. following notations directly denote representations used inputs bonn architecture. acquisition model acquisition model aims deciding whether high-level observation needs acquired. draws according bernoulli distribution sigmoid). sampled policy note responds classical discount return. deﬁne reinforcement learning problem optimization problem optimal policy computed maximizing expected discounted return different learning algorithms maximizing case policy gradient techniques consider that sake simplicity also denotes parameters policy gradient objective approximated with options bonn high-level observation acquired option state updated based current observations then policy behave classical recurrent policy next high-level observation acquired. words acquiring high-level observation sub-policy chosen depending option state seen latent vector representing option chosen time represents usually called termination function option settings. bonn since option chosen directly according state environment need explicit initiation deﬁning states option begin. budgeted learning options discovery inspired cognitive sciences bonn considers discovering options aims reducing cognitive effort agent. case cognitive effort measured amount high-level observations acquired model solve task thus amount options vectors computed model complete episode. constraining model good trade-off policy efﬁciency number highlevel observations acquired bonn discovers extra information essential acquired thus start sub-policies. acquisition cost particular episode. propose integrate acquisition cost learning objective relying budgeted learning paradigm already explored different rl-based applications controls trade-off policy efﬁciency cognitive charge. associated discounted return denoted used objective maximize resulting following policy gradient update rule learning rate. note rule updates probabilities chosen actions probabilities seen internal actions decide option computed not. resulting variance reduction term deﬁned discovering discrete options previous sections considered option generated option model vector latent space. slightly different classical option deﬁnition usually considers agent given catalog possible sub-routines options ﬁnite discrete set. propose variant model model learns ﬁnite discrete options. denote number options wants discover. option associated embedding denoted option model store different possible options choose time option required. case option model considered stochastic model able sample option index denoted using multinomial distribution softmax computation. case option model computes stochastic choices policy gradient update rule integrate additional internal actions with blind setting given pomdp easiest design observations needed bonn model consider empty observation usual observation coming environment. case similar goal-directed habit action control paradigm corresponds case agent chooses either acquire observation. also corresponds macro-actions framework agent chooses sequence actions observation. several environments used evaluate bonn cartpole classical cart-pole setting environment implemented openai platform observations possible actions right reward every time step without failure. environment sizes networks used ngru lunarlander environment corresponds lunar lander environment proposed openai observations describe position velocity angle agent whether contact ground possible actions nothing left engine main engine right engine. reward landing ground crashing time main engine ﬁred. ngru k-rooms environment corresponds maze composed rooms doors agent always starts upper-left corner goal position chosen randomly episode room position position changes episode. reward function moving reaching goal different actions possible updownleft right. observation describes agent position position doors room goal position goal room agent note environment much difﬁcult -rooms problems latter possible goal position case goal results compare bonn recurrent policy gradient cells. note r-pg access observations every time step optimal policy environments bonn learns time time. illustrate quality bonn table versions environment deterministic stochastic movement agent fail probability case random transition applied. deterministic environments bonn model able perform well classical baselines acquiring observation times episode example cartpole environment agent needs observation time. results clearly show simple environments need receive feedback permanently single planning step generate several actions. however stochastic environments observations used often even performances degrade much more. indeed stochasticity possible agent deduce position using chosen action observation thus needs acquired often. demonstrates limits open-loop policies predictable environments justifying basic observation following. using low-level high-level observations seen above problem open-loop control control without feedback environment stochasticity environment cannot anticipated agent without receiving feedback. study setting provides simple light information provides richer information. motivation figure ×-rooms example trajectories generated agent. point corresponds position agent decides acquire generates option agent uses option room. options latent vectors visualized t-sne algorithm. similar colors mean goal similar areas room except points corresponds options used reach four possible doors goal near door. help decide action take without much cognitive effort using complete observation. that another version k-rooms environment contains agent position current room corresponds remaining information difference previous setting agent permanent access position. version used ngru stochastic environment bonn uses time even achieve cumulative reward fig. rewards w.r.t. cost curves obtained computing pareto front bonn models different cost levels note drop performance rooms position happens lower cost -rooms environment blind setting. indeed position agent knows time step able compensate stochasticity environment ﬁrst case position available like deterministic blind setting agent able discover meaningful options acquire relevant information room section experiment shows usefulness permanently using basic observation cognitive cost contrast temporary pure open-loop policies observation used. analysis option discovered figures illustrates trajectories generated agent ×-rooms environment positions options generated. agent learns observe room agent uses resulting option reaches another room. thus agent deducts must move another room reach goal current room. note agent directly goal room information seeing current room learns explore maze particular order reaching goal room. also visualized options latent vectors using t-sne algorithm similar colors mean options computed correspond observations goals similar areas. example green options close showing latent option space effectively captures relevant information options similarity. d-bonn model experimented rooms environment example generated trajectories given figure color corresponds learned discrete options. model still able learn good policy constraint ﬁxed number discrete options clearly decreases quality obtained policy. seems thus interesting continuous options instead discrete ones continuous options regrouped smooth clusters illustrated figure consider last setting information provided oracle classical observation. underlying idea agent choose action based observation information optimal model higher cost. study case consider maze environment maze randomly generated episode goal agent initial positions also randomly chosen. observation cases surrounding agent. observation vector action computed simple path planning algorithm access whole map. parameters model ngru note computation expensive leading idea higher cognitive cost. examples generated trajectories illustrated fig. figure illustrates generated trajectory learning ﬁnished. shows that certain point low-controller learned follow straight path acknowlegments work supported within labex smart supported french state funds managed within investissements d’avenir programme reference anr-labx-. figure examples trajectories maze. point corresponds position agent decides acquire generates option optimal learning used crossroad. incomplete learning agent also uses every time must change direction. needs instructions cross-road change direction needed. relevant options already emerged agent behavior optimal. learning ﬁnished agent instructions cross-roads decision essential reach goal. crossroads agent learns follow corridors corresponds intuitive realistic behavior. note future works done using outputs expensive models lieu high-level observation bonn model seems original study model free/model-based paradigm proposed neuroscience proposed model learning options pomdp agent chooses acquire informative costly observation time step. model learned budgeted learning setting acquisition additional information thus option cost. learned policy trade-off efﬁciency cognitive effort agent. setting options handled learned latent representations. experimental results demonstrate possibility reducing cognitive cost i.e. acquiring computing information without drastic drop performances. also show beneﬁt using different levels observation relevance extracted options. work opens different research directions. study bonn applied multi-task reinforcement learning problems another question would study problems many different observations acquired agent different costs many different sensors robot. finally promising perspective learning interact another expensive model. milos hauskrecht nicolas meuleau leslie pack kaelbling thomas dean craig boutilier. hierarchical solution markov decision processes using macroproceedings fourteenth conference unactions. certainty artiﬁcial intelligence pages morgan kaufmann publishers inc. nicolas heess greg wayne yuval tassa timothy lillicrap martin riedmiller david silver. learning transfer modulated locomotor controllers. arxiv preprint arxiv. tejas kulkarni karthik narasimhan ardavan saeedi joshua tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv preprint arxiv. volodymyr mnih john agapiou simon osindero alex graves oriol vinyals koray kavukcuoglu arxiv strategic attentive writer learning macro-actions. preprint arxiv. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. gabriella contardo ludovic denoyer thierry arti`eres. recurrent neural networks adaptive feature acquisition. international conference neural information processing pages springer international publishing amir dezfouli bernard balleine. actions action sequences habits evidence goal-directed habitual action control hierarchically organized. plos comput biol gabriel dulac-arnold ludovic denoyer philippe preux patrick gallinari. sequential approaches learning datum-wise sparse representations. machine learning gl¨ascher nathaniel peter dayan john o’doherty. states versus rewards dissociable neural prediction error signals underlying model-based model-free reinforcement learning. neuron", "year": 2016}