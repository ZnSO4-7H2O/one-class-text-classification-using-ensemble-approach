{"title": "Learning the Structure of Dynamic Probabilistic Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains.", "text": "dynamic probabilistic processes. sentation examine learn structure data. extend structure dard probabilistic networks show search structure ables hidden. finally technology classifying orderings ical results demonstrate methods domains. problem learning data. importantly learn dpns incomplete partial algorithm tional hidden variables) addition possible. particularly important cision making speech generation disease processes example-are introduction probabilistic networks works belief representations among several perhaps established probabilistic networks evolution significant kalman filters terior models whose parameterization tially show dpns outperform recognition crete random variables values finite denoted size iixill· capital letters variable denote specific ables denoted boldface sets values denoted given network note variables denote joint probability semi-infinite practice this notionally parents corresponding copy conditional similar manner. figure shows result network figure time slices. model joint distribution learning complete theory learning dpns section complete data. begin brief review learns standard pns. derive details score dpns finally discuss optimized upshot section uses techniques learning complete data. learning dpns quite applying methods unrolled constraint assuming ture. general quite different represent dependencies induced unobserved portion represents process -step uses currently parameters pected counts. likelihood true observed counts. behavior likelihood maximum. justing ever underlying structural structural completing based current re-estimating expected candidate complete-data shows large family scorings score score resulting must higher score original. even though expected structure convertthedpn join tree two-pass namic programming backwards algorithm compute probability tained join tree nodes need sophisticated currently variables convert number transitions states counts family. also maintain expected completion expected exact expected likely cussion allow learn larger models. procedure camera's attributes score. friedman reference learn procedure front since score linear. nonethe­ involved right. less friedman shows reasonable driving classes score expected several tasks. prime example arises batmobile loop. autonomous controller car. example driven across lanes might attempting leave freeway front you. since tracking eras readily available realistic models dition autonomous paramount importance models used also safety studies. experiments inferring driver behavior many tracking domains tive model behavior accurate useful predictions making. objects dpns learned correctly process also provide insight noisy causal systems provides noisy incomplete experiments typical tion. generate learn models data variety compare original models. experiments represent quired refrain since sufficient studying simple genetic pathway shows representation pathway model figure nature problem suggests provide parsimonious noisy-or node function parents child node state par­ probability state five-vertex used experiments ters entails network replacing also tried using gradient countered optimal test data different roughly bits needed encode time slice using network. numbers indicate ables improves also indicate training events traffic patterns. figures essential example \"relative tudinal encodes \"need take avoiding dpns powerful causal models stochastic useful many areas science including molecular biol­ often interested regulatory part genetic terms sequential made automatically circuits data however abstraction ministic probability eter determines fixing parameter tobs scale time high probability would otherwise learned nonetheless imply constrained given pair observations. number different generating tween adjacency logloss independent erating model figure shown results figure noisy-ors perform much better tabular data even missing examples all-but-two effectively shown). case even know time second observation switching shown). obviously important inference sufficient statistics. structure necessarily minimum size posterior slice generally approxi­ parents mations could speed inference method proposed mates posterior particularly appropriate models investigating. example variational ghahramani multilevel related linear gaussian petitive advantage case marginalizing efficient operation. case hybrid dpns discrete variables. state space models state variables represented domain separate observations crete hidden states overtake\"). hidden variables currently", "year": 2013}