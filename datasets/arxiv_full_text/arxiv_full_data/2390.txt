{"title": "Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed  Bandit Case", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The exploration/exploitation (E/E) dilemma arises naturally in many subfields of Science. Multi-armed bandit problems formalize this dilemma in its canonical form. Most current research in this field focuses on generic solutions that can be applied to a wide range of problems. However, in practice, it is often the case that a form of prior information is available about the specific class of target problems. Prior knowledge is rarely used in current solutions due to the lack of a systematic approach to incorporate it into the E/E strategy.  To address a specific class of E/E problems, we propose to proceed in three steps: (i) model prior knowledge in the form of a probability distribution over the target class of E/E problems; (ii) choose a large hypothesis space of candidate E/E strategies; and (iii), solve an optimization problem to find a candidate E/E strategy of maximal average performance over a sample of problems drawn from the prior distribution.  We illustrate this meta-learning approach with two different hypothesis spaces: one where E/E strategies are numerically parameterized and another where E/E strategies are represented as small symbolic formulas. We propose appropriate optimization algorithms for both cases. Our experiments, with two-armed Bernoulli bandit problems and various playing budgets, show that the meta-learnt E/E strategies outperform generic strategies of the literature (UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the robustness of the learnt E/E strategies, by tests carried out on arms whose rewards follow a truncated Gaussian distribution.", "text": "abstract. exploration/exploitation dilemma arises naturally many subﬁelds science. multi-armed bandit problems formalize dilemma canonical form. current research ﬁeld focuses generic solutions applied wide range problems. however practice often case form prior information available speciﬁc class target problems. prior knowledge rarely used current solutions lack systematic approach incorporate strategy. address speciﬁc class problems propose proceed three steps model prior knowledge form probability distribution target class problems; choose large hypothesis space candidate strategies; solve optimization problem candidate strategy maximal average performance sample problems drawn prior distribution. illustrate meta-learning approach diﬀerent hypothesis spaces strategies numerically parameterized another strategies represented small symbolic formulas. propose appropriate optimization algorithms cases. experiments two-armed bernoulli bandit problems various playing budgets show meta-learnt strategies outperform generic strategies literature also evaluate robustness learnt strategies tests carried arms whose rewards follow truncated gaussian distribution. exploration versus exploitation dilemmas arise many sub-ﬁelds science related ﬁelds artiﬁcial intelligence ﬁnance medicine engineering. simple version multi-armed bandit problem formalizes dilemma follows gambler coins step choose among slots allocate coins earns money depending response machine selected. response characterized unknown probability distribution constant time. goal gambler collect largest cumulated reward exhausted coins rational gambler knowing reward distributions arms would play every stage maximal expected reward maximize expected cumulative reward reward distributions unknown less trivial decide play optimally since contradictory goals compete exploration consists trying acquire knowledge expected reward exploitation consists using current knowledge decide play. balance eﬀort towards goals essence dilemma specially diﬃcult imposing ﬁnite number playing opportunities theoretical works multi-armed bandit problem focused design generic strategies provably optimal asymptotic conditions assuming unrestrictive conditions reward distributions among these strategies work computing every play quantity called upper conﬁdence index depends rewards collected selecting next play highest index. strategies called index-based policies initially introduced indices diﬃcult compute. easy compute indices proposed later index-based policies typically involve hyper-parameters whose values impact relative performances. usually reporting simulation results authors manually tuned values problems share similarities test problems running trial-and-error simulations actually used prior information problems select hyper-parameters. starting observations elaborated approach learning reproducible good policies playing multi-armed bandit problems ﬁnite horizons. approach explicitly models exploits prior information target multi-armed bandit problems. assume prior knowledge represented distribution multi-armed bandit problems draw number training problems. given distribution meta-learning consists searching chosen candidate strategies yields optimal expected performances. approach allows automatically tune hyper-parameters existing index-based policies. importantly opens door searching within much broader classes strategies optimal given problems compliant prior information. propose hypothesis spaces composed index-based policies ﬁrst index function linear function features whose meta-learnt parameters real numbers second function generated grammar symbolic formulas. empirically show case bernoulli arms number arms playing horizon fully speciﬁed priori learning enables obtain policies signiﬁcantly outperform wide range previously proposed generic policies even careful tuning. also evaluate robustness learned policies respect erroneous prior assumptions testing strategies learnt bernoulli arms bandits rewards following truncated gaussian distribution. ideas presented paper take roots previously published papers. idea learning multi-armed bandit policies using global optimization numerically parameterized index-based policies ﬁrst proposed searching good multi-armed bandit policies formula space ﬁrst proposed compared previous work adopt unifying perspective learning strategies prior knowledge. also introduce improved optimization procedure formula search based equivalence classes identiﬁcation pure exploration multi-armed problem formalization. paper structured follows. ﬁrst formally deﬁne multi-armed bandit problem introduce index-based policies section section formally states strategy learning problem. section section present numerical symbolic instantiation learning approach respectively. section reports experimental results. finally conclude present future research directions section denote arms bandit problem reward distribution expected value; played round obtained reward. vector gathers history ﬁrst plays denote algorithm processes play vector select index-based bandit policies based ranking index computes numerical value based sub-history responses gathered time policies sketched algorithm work follows. ﬁrst plays play sequentially machines perform initialization. subsequent plays policies compute every machine score index. proposed parameters refer reader detailed explanations parameters. note index function exploitation term give preference arms high reward mean exploration term aims playing arms gather information underlying reward distribution instead relying ﬁxed strategy solve given class problems propose systematic approach exploit prior knowledge learning strategies problem-driven way. state learning approach abstract terms. furthermore given problem computing e{rπ values cannot compute exactly general case. therefore propose approximate expected cumulative regret empirical mean order instantiate approach components provided hypothesis space optimization algorithm solve next sections describe diﬀerent instantiations components. deﬁne parametric family candidate policies index functions expressed linear combinations history features. index functions rely history feature function describes history w.r.t. given vector scalar features. given function history features describe aspect history including empirical reward moments current time step play counts combinations variables. features large avoid parameter estimation diﬃculties large enough provide support rich strategies. propose possibility deﬁning history feature function applied multi-armed problem shown perform well section compute ﬁrst compute following four variables i.e. square root logarithm current time step inverse square root number times played empirical mean standard deviation rewards obtained then variables multiplied diﬀerent ways produce features. number combinations controlled parameter whose default value given feature fijkl possible combinations terms feature possible polynomial degree using variables following denote power- policy learned using function parameter index function underlies policies written following discuss optimization equation case numerical parameterization. note objective function want optimize addition stochastic complex relation parameters slight change parameter vector lead signiﬁcantly diﬀerent bandit algorithm eda-based learning discrete bandit policy given number iterations imax given population size given number best elements given sample training bandit problems given history-features function episodes expected regret values. local optimization approaches thus appropriate here. instead suggest derivative-free global optimization algorithms. work powerful simple class global optimization algorithms known cross-entropy also known estimation distribution algorithms edas rely probabilistic model describe promising regions search space sample good candidate solutions. performed repeating iterations ﬁrst sample population candidates using current probabilistic model probabilistic model given best candidates. kind probabilistic model used inside eda. simplest form edas uses marginal distribution variable optimize known univariate marginal distribution algorithm adopted parameter although approach simple proved quite eﬀective experimentally solve equation full details eda-based policy learning procedure given algorithm initial distributions standard gaussian consider index functions given form small closed-form formulas. closed-form formulas several advantages easily computed formally analyzed easily interpretable. following consider operators constants provides good compromise high expressiveness cardinality binary operators considered paper includes four elementary mathematic operations operators {+−×÷ max}. unary operators contains square root logarithm abvariables constants chosen deﬁned figure summarizes grammar formulas gives examples index functions. length formula length number symbols occurring formula. example length subset formulas whose length {f|length index-based policies whose index functions deﬁned formulas discuss optimization equation case symbolic parameterization. first notice several diﬀerent formulas lead policy. example increasing function deﬁnes greedy policy always selects believed best. examples since useless evaluate equivalent policies multiple times propose following two-step approach. first partitioned equivalence classes formulas equivalent lead policy. partitioning task trivial given formula equivalent formulas obtained commutativity associativity operator-speciﬁc rules increasing transformation. performing step exactly involves advanced static analysis formulas believe diﬃcult solution implement. instead propose simple approximated solution consists discriminating formulas comparing rank random samples variables formally procedure following ﬁrst build space formulas length relatively samples suﬃcient reject high conﬁdence badly performing formulas. order exploit idea natural idea formalize search best formula another multi-armed bandit problem. formula associate arm. pulling consists selecting training problem running episode index-based policy whose index formula leads reward associated whose value quantity observed episode. purpose multi-armed bandit algorithms process sequence observed rewards select smart next formula tried budget pulls exhausted high-quality formula identiﬁed. formalization equation multi-armed bandit problem quality ﬁnally suggested matters. select arms identify best ﬁnite amount time known pure exploration multi-armed bandit problem shown index-based policies based upper conﬁdence bounds good policies solving pure exploration bandit problems. optimization procedure works follows bandit algorithm ucb-tuned given number steps return policy corresponds formula highest expected reward problem instances selected depending number times played step select training problem experiments estimate multi-armed bandit approach hundred thousand times faster naive monte carlo optimization procedure clearly demonstrates beneﬁts approach. note idea could also relevant numerical case. main diﬀerence corresponding multi-armed bandit problem relies continuous-arm space. although algorithms already proposed solve multi-armed bandit problems scale techniques problems hundreds thousands parameters still open research question. progresses ﬁeld could directly beneﬁt numerical learning approach. illustrate instances learning approach comparing learned policies number generic previously proposed policies setting prior knowledge available target problems. show cases learning enables obtain exploration/exploitation strategies signiﬁcantly outperforming tested generic policies. compare learned policies generic policies. distinguish untuned generic policies tuned generic policies. former either policies parameter-free policies used default parameters suggested training testing. illustrate approach consider scenario number arms playing horizon kind distributions known priori parameters distributions missing information. since learning policies care taken generalization issues. usual supervised machine learning training distinct testing set. training composed bandit problems sampled given distribution bandit probdistribution. study robustness policies w.r.t. wrong prior information also report performance problems drawn another distribution diﬀerent kinds distributions computing estimate regret problems averaging results overs runs. calculation thus involves simulating bandit episodes training expectations uniformly return bandit problem bernoulli arms expectations respectively. second distribution reward distributions changed gaussian distributions truncated interval order sample problem select mean standard deviation uniformly range rewards sampled using rejection sampling approach samples drawn corresponding gaussian distribution obtaining value belongs interval generic policies. consider following generic policies \u0001n-greedy policy described policies introduced ucb-tuned ucb-normal policy kl-ucb introduced policy ucb-v proposed except \u0001n-greedy policies belong family index-based policies discussed previously. ucb-tuned ucbnormal parameter-free policies designed bandit problems bernoulli distributions problems gaussian distributions respectively. policies hyper-parameters tuned improve quality policy. \u0001n-greedy parameters parameter kl-ucb parameter ucb-v learning numerical policies. learn policies using parameterizations power- power- described section note tuning generic policies particular case learning numerical parameters learned policies tuned generic policies make prior knowledge. make comparison kinds policies fair always training procedure algorithm imax iterations candidate policies iteration best elements number parameters optimize. linear dependency classical choice using edas note that cases optimization solved tens iterations. simulations shown imax careful choice ensuring optimization enough time properly converge. baseline policies default values advocated values initial expectation gaussians. otherwise initial gaussians centered zero. nothing done enforce respect constraints parameters practice automatically identiﬁes interesting regions search space respect constraints. distinct candidate strategies identify best distinct strategies apply ucb-tuned algorithm steps. experiments report best found policies denote formula- formula-. generic policies. already pointed seen ucbtuned particularly well ﬁtted bandit problems bernoulli distributions. also proves eﬀective bandit problems gaussian distributions making nearly always outperform untuned policies. tuning outperform ucb-tuned policy also sometimes happens ucb-v. however though used careful tuning procedure \u0001n-greedy never outperform ucb-tuned. learned policies. observe training horizon testing horizon learned policies systematically outperform generic policies. overall best results obtained power- policies. note that numerical nature large number parameters policies extremely hard interpret understand. results related symbolic policies show exist simple policies perform nearly well black-box policies. clearly shows beneﬁts hypothesis spaces numerical policies enable reach high performances symbolic policies provide interpretable strategies whose behavior easily analyzed. table mean expected regret untuned tuned learned policies bernoulli gaussian bandit problems. best scores categories shown bold. scores corresponding policies tested horizon horizon used training/tuning shown italics. interpretability/performance tradeoﬀ common machine learning identiﬁed several decades ﬁeld supervised learning. worth mentioning that among formula equivalence classes surprisingly large number strategies outperforming generic policies found obtain diﬀerent symbolic policies outperforming generic policies. robustness w.r.t. horizon expected learned policies give best performance training testing horizons equal. policies learned large training horizon prove work well also smaller horizons. however testing horizon larger training horizon quality policy quickly degrade robustness w.r.t. kind distribution. although truncated gaussian distributions signiﬁcantly diﬀerent bernoulli distributions learned policies time generalize well setting still outperform generic policies. word learned symbolic policies. worth noticing best index-based policies found largest horizons work similar ucb-type policies reported earlier literature. indeed also associate index positive term decreases however shortest time horizon policy found totally diﬀerent ucb-type policies. policy arms whose empirical reward mean higher given threshold positive index scores candidate selection i.e. making scores negative eﬀect kill arms. threshold index associated increase number times played decrease case policies. empirical means threshold equal reward means arms less played preferred. ﬁnding amazing since suggests optimistic paradigm multi-armed bandits upon policies based fact adapted context horizon small. percentage wins ucb-tuned. table gives policy percentage wins ucb-tuned trained horizon test horizon. compute percentage wins evaluate expected regret testing problems count number problems tested policy outperforms ucb-tuned. observe minimizing expected regret learned policies also reach high values percentage wins note that approach easy change objective function. real applicative maximize percentage wins ucb-tuned criterion could used directly policy optimization stage reach even better scores. used based implementation perform experiments. numerical case cores .ghz performing whole learning power- took hour hours symbolic case using single core .ghz performing whole learning took minutes less three hours note fact symbolic learning much faster explained reasons. first tuned algorithm careful sure high quality solution; observe using learning time already obtain close-to-optimal strategies. second factor symbolic learning algorithm saves time able rapidly reject strategies thanks multi-armed bandit formulation upon relies. approach proposed paper exploiting prior knowledge learning exploration/exploitation policies tested two-armed bandit problems bernoulli reward distributions knowing time horizon. learned policies found signiﬁcantly outperform policies previously published literature ucb-v kl-ucb \u0001n-greedy. robustness learned policies respect wrong information also highlighted evaluating two-armed bandits truncated gaussian reward distribution. opinion several research directions could investigated still improving algorithm learning policies proposed paper. example found problems similar problem overﬁtting supervised learning could occur considering large candidate polices. naturally calls studying whether learning approach could combined regularization techniques. along idea sophisticated optimizers could also thought identifying candidate policies predicted behave best. ucb-v kl-ucb \u0001n-greedy policies used comparison shown interesting bounds expected regret asymptotic conditions providing bounds learned policies. would certainly relevant investigate whether similar bounds could derived learned policies alternatively approach could adapted target policies oﬀering theoretical performance guarantees asymptotic conditions. example better bounds expected regret could perhaps obtained identifying candidate policies gives smallest maximal value expected regret rather gives best average performances. finally paper provided simulation results context simple multi-armed bandit setting exploration/exploitation policy meta-learning scheme also principle applied explorationexploitation problem. line research extension investigation markov decision processes studied suggests already approach meta-learning strategies successful much complex settings.", "year": 2012}