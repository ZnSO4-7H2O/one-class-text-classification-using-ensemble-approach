{"title": "Relevance-based Word Embedding", "tag": ["cs.IR", "cs.CL", "cs.LG", "cs.NE"], "abstract": "Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently attracted much attention in natural language processing and information retrieval tasks. The embedding vectors are typically learned based on term proximity in a large corpus. This means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. The primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. This is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with different objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classifies each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classification. Both query expansion experiments on four TREC collections and query classification experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.", "text": "introduction representation learning long-standing problem natural language processing information retrieval main motivation abstract away surface forms piece text e.g. words sentences documents order alleviate sparsity learn meaningful similarities e.g. semantic syntactic similarities dierent pieces text. learning representations words atomic components language also known word embedding recently aracted much aention communities. popular model learning word representation neural network-based language models. instance wordvec model proposed mikolov embedding model learns word vectors neural network single hidden layer. continuous words skip-gram implementations wordvec model. another successful trend learning semantic word representations employing global matrix factorization word-word matrices. glove example methods. theoretical relation discovered embedding models based neural network matrix factorization models demonstrated eective number tasks including query expansion query classication short text similarity document model estimation aforementioned embedding models typically trained based term proximity large corpus. instance wordvec model’s objective predict adjacent word given word context i.e. context window around target word. idea aims capture semantic syntactic similarities terms since semantically/syntactically similar words share similar contexts. however objective necessarily equivalent main objective many tasks. primary objective many methods model notion relevance paper revisit underlying assumption typical word embedding methods follows objective previously considered developing relevance models state-of-the-art relevance feedback approach. relevance models optimize objective given relevant documents given query indicator user’s information need. absence relevance information ranked documents retrieved response query assumed relevant. erefore relevance models general pseudo-relevance feedback models online seing obtain training data retrieving documents query abstract learning high-dimensional dense representation vocabulary terms also known word embedding recently aracted much aention natural language processing information retrieval tasks. embedding vectors typically learned based term proximity large corpus. means objective well-known word embedding algorithms e.g. wordvec accurately predict adjacent word given word context. however objective necessarily equivalent goal many information retrieval tasks. primary objective various tasks capture relevance instead term proximity syntactic even semantic similarity. motivation developing unsupervised relevance-based word embedding models learn word representations based query-document relevance information. paper propose learning models dierent objective functions; learns relevance distribution vocabulary query classies term belonging relevant non-relevant class query. train models used million unique queries ranked documents retrieved response query assumed relevant query. extrinsically evaluate learned word representation models using tasks query expansion query classication. query expansion experiments four trec collections query classication experiments dataset suggest relevance-based word embedding models signicantly outperform state-of-the-art proximity-based embedding models wordvec glove. reference format hamed zamani bruce cro. relevance-based word embedding. proceedings sigir august shinjuku tokyo japan pages. hp//dx.doi.org/./. permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. sigir august shinjuku tokyo japan acm. ----//.... hp//dx.doi.org/./. using retrieved documents order estimate relevance distribution. although relevance models proved eective many tasks retrieval query obtain training data estimating relevance distribution always practical real-world search engines. paper optimize similar objective oine seing enables predict relevance distribution without retrieval runs test time. consider retrieved documents millions training queries training learn embedding vectors term order predict words observed retrieved documents query. develop relevance-based word embedding models. relevance likelihood maximization model aims model relevance distribution vocabulary terms query second relevance posterior estimation model classies term relevant non-relevant query. provide ecient learning algorithms train models large amounts training data. note models unsupervised training data generated automatically. evaluate models performed sets extrinsic evaluations. focus query expansion task ad-hoc retrieval. experiments consider four trec collections including newswire collections large-scale collections results suggest relevance-based embedding models outperform state-of-the-art word embedding algorithms. model shows beer performance compared context query expansion since goal estimate probability term given query distribution directly learned model. second experiments focus query classication task using dataset. extrinsic evaluation relevance-based embedding models perform beer baselines. interestingly query classication results demonstrate model outperforms model reason task unlike query expansion task goal compute similarity query vectors learn accurate embedding vectors less training data. related work learning semantic representation text studied many years. latent semantic indexing considered early work area tries text semantic space using singular value decomposition well-known matrix factorization algorithm. subsequently clinchant perronnin proposed fisher vector document representation framework based continuous word embeddings aggregates non-linear mapping word vectors document-level representation. however number popular models language models signicantly outperform models based semantic similarities. recently extremely ecient word embedding algorithms proposed model semantic similarly words. word embedding also known distributed representation words refers machine learning algorithms learn high-dimensional real-valued dense vector representation vocabulary term denotes embedding dimensionality. glove wordvec well-known word embedding algorithms learn embedding vectors based idea using dierent machine learning techniques. idea words appear similar contexts similar other. algorithms accurately predict adjacent word given word context recently rekabsaz proposed exploit global context word embeddings order avoid topic shiing. word embedding representations also learned parameters end-to-end neural network model. instance zamani trained context-aware ranking model embedding vectors frequent n-grams learned using click data. recently dehghani trained neural ranking models weak supervision data learn word representations end-to-end ranking scenario. word embedding vectors successfully employed several tasks. kusner proposed word mover’s distance function calculating semantic distance between documents measures minimum traveling distance embedded vectors individual words document one. zhou introduced embeddingbased method question retrieval context community question answering. vuli´c moens proposed model learn bilingual word embedding vectors document-aligned comparable corpora. zheng callan presented supervised embedding-based technique re-weight terms existing models e.g. based well-dened structure language modeling framework information retrieval number methods introduced employ word embedding vectors within framework order improve performance tasks. instance zamani presented embedding-based query language models using query expansion pseudo-relevance feedback techniques benet word embedding vectors. expansion using word embedding also studied approaches based word embeddings learned based term proximity information. phrasefinder early work using term proximity information query expansion. mapping vocabulary terms space low-dimensional space compared vocabulary size used query modeling. widely known information retrieval literature dierence unigram distribution words sub-topics collection unigram distribution estimated whole collection. given phenomenon diaz recently proposed train word embedding vectors retrieved documents query. however model called local embedding always practical real-word applications since embedding vectors need trained query time. furthermore objective function local embedding based term proximity pseudo-relevant documents. paper propose models learning word embedding vectors specically designed information retrieval needs. aforementioned tasks section potentially benet vectors learned proposed models. relevance-based embedding typical word embedding algorithms wordvec glove learn high-dimensional real-valued embedding vectors based proximity terms training corpus i.e. cooccurrence terms context window. although approaches could useful learning embedding vectors capture semantic syntactic similarities vocabulary terms shown useful many tasks large learning objective needed many information retrieval tasks. example consider query expansion task assume user submied query dangerous vehicles. similar terms query based typical word embedding algorithms safe thus would high weight expanded query model. reason words dangerous safe share similar contexts. however expanding query word safe could lead poor retrieval performance since changes meaning intent query. example together many others motivated revisit objective used learning process word embedding algorithms order obtain word vectors beer match needs tasks. primary objective many tasks model notion relevance. several approaches relevance models proposed lavrenko proposed model relevance. given successes achieved models propose learn word embedding vectors based objective maers information retrieval. objective accurately predict terms observed relevant documents particular information need. following subsections describe neural network architecture explain build training learning relevance-based word embeddings. introduce models relevance likelihood maximization relevance posterior estimation dierent objectives using described neural network. neural network architecture simple eective feed-forward neural network single linear hidden layer. architecture neural network shown figure input model sparse query vector length denotes total number vocabulary terms. vector obtained projection function given vectors corresponding individual query terms. paper simply consider average projection function. denote one-hot hence vector representation term query length respectively. hidden layer network maps given query sparse vector query embedding vector follows figure relevance-based word embedding architecture. objective learn d-dimensional distributed representation words based notion relevance instead term proximity. denotes total number vocabulary terms. rd×n weight bias matrices estimating probability term. activation function discussed sections modeling relevance training relevance feedback shown highly eective improving retrieval performance relevance feedback relevant documents given query considered estimating accurate query models. since explicit relevance signals given query always available pseudo-relevance feedback assumes retrieved documents response given query relevant query uses documents order estimate beer query models. eectiveness various retrieval scenarios indicates useful information captured retrieved documents paper make well-known assumption train model. noted signicant dierence between proposed models feedback model estimated retrieved documents given query online seing. words retrieves documents initial query estimates feedback model using retrieved documents. paper propose train model oine seing. moving online oine seing would lead substantial improvements eciency because extra retrieval needed oine seing. learn model oine seing consider xed-length dense vector vocabulary term estimate vectors based information extracted retrieved documents large numbers training queries. note models changed optimization process cannot simply omit normalization term done estimating query embedding vectors based pre-trained word embedding vectors. make computations tractable consider hierarchical approximation somax function introduced morin bengio context neural network language models successfully employed mikolov wordvec model. hierarchical somax approximation uses binary tree structure represent vocabulary terms leaf corresponds unique word. exists unique path root leaf path used estimating probability word representing leaf. erefore complexity calculating somax probabilities goes height tree. leads huge improvement computational complexity. refer reader details calculating hierarchical somax approximation. relevance posterior estimation model alternative maximum likelihood estimation estimate relevance posterior probability. context pseudorelevance feedback zhai laery assumed language model retrieved documents estimated based mixture model. words assumed language models feedback relevance language model background noisy language model. used expectation-maximization algorithm estimate relevance language model. model make assumption order cast problem estimating relevance distribution classication task given pair word query come relevance distribution query instead model estimates boolean variable means given term-query pair comes relevance distribution parameters going learned training phase. erefore problem cast binary classication task modeled logistic regression sigmoid function) order address binary classication problem consider cross-entropy loss function. theory training query model learn model relevance terms appearing corresponding pseudo-relevant non-relevance vocabulary terms could impractical large number vocabulary terms. similar propose noise contrastive estimation hypothesizes achieve good model dierentiating data noise logistic regression model. main concept similar proposed divergence randomness model divergence minimization feedback model unsupervised. however explicit relevance data available click data without loss generality explicit implicit relevant documents considered training models. leave studying vectors learned based supervised signals future work. formally describe training data lett training training queries. element pair query corresponding pseudo-relevance feedback distribution. distributions estimated based retrieved documents query. distributions estimated using model proposed paper focus relevance model state-of-the-art model estimates relevance distribution relevance likelihood maximization model model goal learn relevance distribution given training data parameters order maximize likelihood generating relevance model probabilities whole training set. likelihood function dened follows learning parameters denotes relevance model distribution estimated query training denotes subset vocabulary terms appeared ranked documents retrieved query reason iterating terms appeared instead whole vocabulary probability equal zero terms somax function follows denotes learned embedding vector term query vector came output hidden layer network according somax modeling log-likelihood function following objective expwt computing objective function derivatives would computationally expensive expwt objective function). since word embedding vectors well query vector implemented trained models using tensorflow. networks trained based stochastic gradient descent optimizer using back-propagation algorithm compute gradients. model hyper-parameters tuned training model learning rate batch size selected respectively. also tuned number positive negative instances value swept parameter selected suggested experiments embedding dimensionality models including baselines. evaluation expansion subsection evaluate embedding models context query expansion ad-hoc retrieval task. following describe retrieval collections used experiments. explain experimental setup well evaluation metrics. nally report discuss query expansion results. data. four standard test collections experiments. collections consist thousands news articles considered homogeneous collections. robust previously used trec ad-hoc track trec robust track respectively. second collections large-scale collections containing heterogeneous documents. consists .gov domain pages crawled clueweb common crawl collection contains english pages. clueweb previously used trec terabyte track trec track respectively. statistics collections well corresponding trec topics reported table used title topics queries. experimental setup. cleaned clueweb collection ltering spam documents. spam ltering phase done using waterloo spam scorer threshold stopwords removed collections using standard inquery stopword list stemming performed. purpose query expansion consider language modeling framework estimate query language model based given word embedding vectors. expanded query language model denotes maximum likelihood estimation original query free hyper-parameter controls weight original query model expanded model. probability pw|q) calculated based trained word embedding vectors. model probability estimated using equation second model simply bayes rule given equation estimate probability. however since information probability notable although model learns embedding vectors queries words obvious calculate probability term given query; equation gives classication probability cannot simply bayes rule model perform well computing similarity terms queries query term. however model presented estimate query model using word embedding vectors calculate similarity query term. experiments section describe train relevance-based word embedding models. extrinsically evaluate learned embeddings using tasks query expansion query classication. note main compare proposed models existing word embedding algorithms state-of-the-art query expansion query classication models. training order train relevance-based word embeddings obtained millions unique queries publicly available query logs dataset contains sample search queries real users submied search engine within three-month period march used query strings session click information obtained dataset. ltered navigational queries containing substrings i.e. www. .com .net .org .edu. nonalphanumeric characters removed queries. applying constraints leads millions unique queries training query set. estimate relevance model distributions training considered retrieved documents target collection response query using galago implementation query likelihood retrieval model dirichlet prior smoothing table evaluating relevance-based word embeddings context query expansion. superscripts //// denote improvements mle/wordvec-external/wordvec-target/glove-external/glove-target statistically signicant. highest value marked bold. term given query uniform distribution. word embedding models standard method described models ignore terms whose embedding vectors available. retrieve documents expanded query language model using kl-divergence formula dirichlet prior smoothing retrieval experiments carried using galago toolkit evaluation metrics. evaluate eectiveness query expansion models report three standard evaluation metrics mean average precision ranked documents precision retrieved documents normalized discounted cumulative gain calculated retrieved documents statistically signicant dierences ndcg values based two-tailed paired t-test computed condence level results discussion. evaluate models consider following baselines standard maximum likelihood estimation query model without query expansion sets embedding vectors ectiveness employing high-dimensional word representations query expansion. similar observations made according results although wordvec performs slightly beer glove signicant dierences observed performances. according table relevance-based embedding models outperform baselines collections shows importance taking relevance account training embedding vectors. improvements statistically signicant compared baselines. relevance likelihood maximization model performs beer relevance posterior estimation model cases reason related objective function. learns relevance distribution terms learns classication probability relevance vocabulary terms sense learned embedding models table report expansion terms sample queries robust collection. according table terms added query wordvec model syntactically semantically related individual query terms expected. query indian american museum example terms history culture related query term museum terms united states related query term american. contrast looking expansion terms obtained relevance-based word embeddings relevant terms whole query selected. instance chumash heye smithsonian similar observation made sample query example word independence related whole query selected relevance-based word embedding models terms protestors protests protest protesting syntactically similar query term protesters considered wordvec model. believe dierences learning objective models. interestingly expansion terms added query relevance-based models look similar according table performances quite dierent. reason related weights given term models. weights given expansion terms close objective classify term terms classied high probability relevant. next experiments consider methods retrieved documents query expansion relevance model state-of-the-art pseudo-relevance feedback model local embedding approach recently proposed diaz general idea training word embedding models ranked documents retrieved response given query. similar wordvec model train table evaluating relevance-based word embedding pseudo-relevance feedback scenario. superscripts denote improvements rm/local embedding/erm local embedding statistically signicant. highest value marked bold. word embedding vectors documents. results reported table table refers embedding-based relevance model recently proposed zamani order make semantic similarities estimated based word embedding vectors pseudo-relevance feedback scenario. according table model uses relevance-based word embedding outperforms methods. improvements statistically signicant cases. comparing results obtained local embedding reported table observed substantial dierences results local embedding wordvec. similar reported diaz embedding vectors trained documents target collection similar seing. note relevance-based model also trained target collection. interesting observation tables performance robust close performance slightly beer collection. note needs retrieval runs uses retrieved documents needs retrieval run. important issue many real-world applications since eciency constraints always allow retrieval runs query. parameter sensitivity. next experiments study sensitivity best performing word embedding model table expansion parameters. figure plots sensitivity number expansion terms parameter according gure newswire collections method shows best performance queries expanded words. collection words needed method show best performance. figure plots sensitivity methods interpolation coecient number expansion terms according curves correspond robust original query language model needs interpolated model estimated using relevance-based word embeddings diaz showed precision-oriented tasks second retrieval restricted initial rank list improving eciency models. however recall-oriented metrics e.g. second retrieval helps lot. figure performance respect dierent amount training data terms map. equal weights shows quality estimated distribution learned embedding vectors. collection higher weight given original query model indicates original query plays role achieving good retrieval performance collection. also study performance best performing word embedding model query expansion respect embedding dimensionality. results shown figure query expansion performance generally improves increase embedding dimensionality. performances become stable dimension larger experiment suggests dimensions would enough relevance-based embedding model. large number parameters neural networks require large amounts training data achieve good performance. next experiments study much training data needed training best model. results ploed figure according gure increasing number training queries million four million queries performance signicantly increases becomes stable four million queries. table evaluating embedding algorithms query classication. superscripts denote improvements wordvec/glove signicant. highest value column marked bold. assigned number labels pre-dened training queries available label. supervised multi-label classication task lile training data. data. consider dataset introduced internet user search query categorization task previously used evaluating query embedding vectors. dataset contains queries submied real users randomly collected search logs. queries contain junk text non-english terms. queries labelled three human editors. categories pre-dened labels selected query editor. experimental setup. experiments performed -fold cross-validation queries reported results average obtained test folds. experiments spelling errors queries corrected pre-processing phase stopwords removed queries stemming performed. classify query consider simple knn-based approach proposed compute probability category/label given query select categories highest probabilities. probability computed follows denotes category. centroid vector query embedding vectors label training set. ignore query terms whose embedding vectors available. number labels assigned query tuned training query classication experiments trained relevance-based word embedding using robust collection. evaluation metrics. consider evaluation metrics also used precision fmeasure. since labels assigned three human editors dier cases label sets taken account. metrics computed described evaluating submied runs. statistically signicant dierences determined using two-tailed paired t-test computed condence level results discussion. compare models wordvec glove methods trained external collections described query expansion experiments. results reported table relevance-based embedding models signicantly outperform baselines terms metrics. interesting observation contrary query expansion experiments performs beer query classication. reason query expansion weight term considered order generate expanded query language model. erefore addition order terms weights also eective improving retrieval performance query expansion. query classication assign categories query thus long order categories correct similarity values queries categories maer. next experiments study performance relevance-based word embedding models respect embedding dimensionality. results ploed figure according gure performance generally improved increasing embedding dimensionality becomes stable dimension greater similar observation query expansion experiments. also study amount data needed training models figure according gure least million queries needed order learn accurate relevance-based word embeddings. seen figure needs training data compared order perform well increasing amount training data learning curves models closer. conclusions future work paper revisited underlying assumption typical word embedding models wordvec glove. instead learning embedding vectors based term proximity proposed learning embeddings based notion relevance primary objective many tasks. developed neural network-based models learning relevance-based word embeddings. model relevance likelihood maximization model aims estimate probability word relevance distribution query second relevance posterior estimation model classies term belonging relevant non-relevant class query. evaluated models using sets extrinsic evaluation query expansion query classication. query expansion experiments using four standard trec collections newswire large-scale collections suggested relevance-based word embedding models outperform state-of-the-art word embedding algorithms. showed expansion terms chosen models related whole query chosen typical word embedding models related individual query terms. query classication experiments also validated ndings investigated eectiveness models. future intend evaluate learned embedding models tasks query reformulation query intent prediction etc. also achieve accurate relevance-based embedding vectors considering clicked documents training query instead addition retrieved documents. acknowledgements. authors thank daniel cohen mostafa dehghani qingyao invaluable comments. work supported part center intelligent information retrieval. opinions ndings conclusions recommendations expressed material authors necessarily reect sponsor. references nasreen abdul-jaleel james allan bruce fernando diaz leah larkey xiaoyan donald metzler mark smucker trevor strohman howard turtle courtney wade. umass trec novelty hard. trec michael gutmann aapo hyv¨arinen. noise-contrastive estimation unnormalized statistical models applications natural image statistics. mach. learn. res. embeddings. cikm john laerty chengxiang zhai. document language models models risk minimization information retrieval. sigir victor lavrenko martin choquee bruce cro. cross-lingual xiaodong jianfeng xiaodong deng kevin ye-yi wang. representation learning using multi-task deep neural networks semantic classication information retrieval. naacl infoscale jerey pennington richard socher christopher manning. glove global vectors word representation. emnlp ponte bruce cro. language modeling approach information retrieval. sigir navid rekabsaz mihai lupu allan hanbury guido zuccon. generalizing translation models probabilistic relevance framework. cikm rocchio. relevance feedback information retrieval. smart retrieval system experiments automatic document processing. saracevic. notion relevance information science everybody knows relevance really? morgan claypool publishers. alessandro sordoni yoshua bengio jian-yun nie. learning concept embeddings expansion antum entropy minimization. aaai robust pseudo-relevance feedback. sigir ivan vuli´c marie-francine moens. monolingual cross-lingual information retrieval models based word embeddings. sigir jinxi bruce cro. expansion using local global document analysis. sigir", "year": 2017}