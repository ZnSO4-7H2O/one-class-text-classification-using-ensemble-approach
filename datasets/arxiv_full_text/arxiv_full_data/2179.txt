{"title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.", "text": "statistical performance bounds reinforcement learning algorithms critical high-stakes applications like healthcare. paper introduces framework theoretically measuring performance algorithms called uniform-pac strengthening classical probably approximately correct framework. contrast framework uniform version used derive high probability regret guarantees forms bridge setups missing literature. demonstrate beneﬁts framework ﬁnite-state episodic mdps algorithm uniform-pac simultaneously achieves optimal regret guarantees except factor horizon. recent empirical successes deep reinforcement learning tremendously exciting performance approaches still varies signiﬁcantly across domains requires user solve tuning problem ultimately would like reinforcement learning algorithms simultaneously perform well empirically strong theoretical guarantees. algorithms especially important high stakes domains like health care education customer service non-expert users demand excellent outcomes. propose framework measuring performance reinforcement learning algorithms called uniform-pac. brieﬂy algorithm uniform-pac high probability simultaneously selects ε-optimal policy episodes except number scales polynomially algorithms uniform-pac converge optimal policy high probability immediately yield high probability regret bounds makes superior algorithms come regret guarantees. indeed neither regret guarantees imply convergence optimal policies high probability; -pac algorithms ε/-suboptimal every episode; algorithms small regret maximally suboptimal inﬁnitely often. uniform-pac algorithms suffer none drawbacks. could hope existing algorithms regret guarantees might uniform-pac already analysis missing. unfortunately case modiﬁcation required adapt approaches satisfy performance metric. insight obtaining uniform-pac guarantees leverage time-uniform concentration bounds ﬁnite-time versions iterated logarithm obviates need horizon-dependent conﬁdence levels. provide optimistic algorithm episodic called ubev uniform pac. unlike predecessors ubev uses conﬁdence intervals based iterated logarithm hold uniformly time. allow tightly control probability failure events algorithm behaves poorly. analysis nearly optimal according traditional metrics linear dependence state space setting square root dependence regret. therefore ubev uniform algorithm bounds high probability regret bounds near optimal dependence length episodes optimal state action spaces cardinality well number episodes. knowledge ubev ﬁrst algorithm near-optimal regret guarantees. notation setup. consider episodic ﬁxed-horizon mdps time-dependent dynamics formalized tuple statespace actionspace ﬁnite sets cardinality agent interacts episodes time steps each. beginning time-step agent observes state chooses action based policy depend within-episode time step next state sampled transition kernel initial state agent receives reward drawn distribution depend mean determined reward function. reward distribution supported value function time step policy deﬁned compared optimal return notation value functions interpreted vectors length algorithm follows policy episode optimality episode bounded ∆max maxπ i{∆k number ε-errors regret episodes note number episodes total time steps episode index usually denotes time indices within episode. notation similar usual o-notation suppresses additional polylog-factors polynomial op)). deﬁnitions function polynomial arguments. notational conciseness often omit parameters context clear. different performance guarantees widely used high-probability regret expected regret space constraints discuss bayesian-style performance guarantees hold expectation respect distribution problem instances. shortly discuss limitations frameworks listed above ﬁrst formally deﬁne uniform-pac criteria deﬁnition algorithm uniform-pac performance metrics functions distribution sequence errors episodes k∈n. regret bounds integral sequence time random variable. expected regret expectation integral high-probability regret quantile. bounds quantile size superlevel ﬁxed level uniform-pac bounds like bounds hold simultaneously. limitations regret. since regret guarantees bound integral distinguish making severe mistakes many small mistakes. fact since regret bounds provably grow number episodes algorithm achieves optimal regret still make inﬁnitely many mistakes highly undesirable high-stakes scenarios. example drug treatment optimization healthcare would like distinguish infrequent severe complications frequent minor side effects fact even optimal regret bound could still serve inﬁnitely patients worst possible treatment. limitations pac. bounds limit number mistakes given accuracy level otherwise non-restrictive. means algorithm almost surely might still -pac. worse many algorithms designed -pac actually exhibit behavior explicitly halt learning ε-optimal policy found. less widely used bounds kwik guarantees suffer issueand conciseness discussed detail. advantages uniform-pac. criterion overcomes limitations regret guarantees measuring number ε-errors every level simultaneously. deﬁnition algorithms uniform-pac -pac soon algorithm non-trivial uniform-pac guarantee also small regret high probability. furthermore loss reduction algorithm optimal uniform-pac guarantees also optimal regret least episodic setting. sense uniform-pac missing bridge regret pac. finally algorithms based conﬁdence bounds uniform-pac guarantees usually obtained without much additional work replacing standard concentration bounds versions hold uniformly episodes sense think uniform-pac ‘gold-standard’ theoretical guarantees algorithms. existing theoretical analyses usually focus exclusively either regret framework. besides occasional heuristic translations proposition corollary results relating notion regret aware guarantees widely used average per-step regret superﬁcially bound hold inﬁnitely many time-steps exhibits limitations conventional regret bound. translation average loss comes additional costs discounted inﬁnite horizon setting. figure visual summary relationship among different learning frameworks expected regret preclude crossed arrows represent does-not-implies relationship. blue arrows represent imply relationships. details theorem statements. simultaneously two-armed multi-armed bandits bernoulli reward distributions. implies guarantees also cannot satisﬁed simultaneously episodic mdps. full proof appendix intuition simple. suppose two-armed bernoulli bandit mean rewards respectively second chosen times probability least easily show alternative bandit mean rewards non-zero probability second played ﬁnitely often bandit expected regret linear. therefore sub-linear expected regret possible pulled inﬁnitely often almost surely. theorem following statements hold performance guarantees episodic mdps algorithm satisﬁes -pac bound fpac satisﬁes speciﬁc fhpr bound. further algorithm satisﬁes -pac bound fpac regret means -pac bound fpac converted high-probability regret bound fhpr chosen fpac algorithm satisﬁes -pac bound fpac regret mdp. means -pac bound cannot converted sub-linear uniform high-probability regret bound. fuhpr fuhpr algorithm satisﬁes uniform high-probability regret bound makes inﬁnitely many mistakes sufﬁciently small accuracy level mdp. therefore high-probability regret bound cannot converted ﬁnite -pac bound. interesting problems including episodic mdps worst-case expected regret grows theorem shows establishing optimal high probability regret bound imply ﬁnite bound. bounds converted regret bounds resulting bounds necessarily severely suboptimal rate next theorem formalises claim uniform-pac stronger high-probability regret criteria. observe stronger uniform bounds lead stronger regret bounds episodic mdps optimal uniform-pac bound implies uniform regret bound. knowledge existing approaches regret guarantees uniform-pac. methods mbie mormax ucrl-γ ucfh delayed q-learning median-pac depend advance knowledge eventually stop improving policies. even disabling stopping condition methods uniform-pac conﬁdence bounds hold ﬁnitely many episodes eventually violated according iterated logarithms. existing algorithms uniform high-probability regret bounds ucrl ucbvi also satisfy number observed episodes number observations speciﬁc state action. presence causes algorithm action state inﬁnitely often. might begin wonder uniform-pac good true. algorithm meet requirements? demonstrate section answer showing ubev meaningful uniform-pac bounds. technique allows prove bounds ﬁnite-time iterated pseudo-code proposed ubev algorithm given algorithm episode follows optimistic policy computed backwards induction using carefully chosen conﬁdence interval transition probabilities state. line optimistic estimate q-function current state-action-time triple computed using empirical estimates expected next state value ˆvnext expected immediate reward plus conﬁdence bounds show lemma appendix policy update lines ﬁnds optimal solution maxp es∼p subject constraints width conﬁdence bound ˆpk|s msat) empirical transition probabilities empirical immediate rewards algorithm conceptually similar algorithms based optimism principle mbie ucfh ucrl ucrl-γ several differences instead using conﬁdence intervals transition kernel itself incorporate value function directly concentration analysis. ultimately saves factor sample complexity price difﬁcult analysis. previously mormax also used idea directly bounding transition value function different algorithm required discarding data less tight bound. similar technique used azar figure empirical comparison optimism-based algorithms frequentist regret bounds randomly generated actions time horizon states. algorithms parameters satisfy bound requirements. detailed description experimental setup including link source code found appendix many algorithms update policy less less frequently ﬁnitely often total. instead update policy every episode means ubev immediately leverages observations. azar scale ratelog/n number episodes played instead width ubev’s conﬁdence bounds scales rateln ln/n best achievable rate results signiﬁcantly faster learning. polylog function bounded polynomial logarithm polylog lnk+c. appendix provide lower bound sample complexity shows uniform-pac bound tight log-factors factor knowledge ubev ﬁrst algorithm near-tight high probability regret bounds well ﬁrst algorithm nontrivial uniform-pac bound. using theorem convergence regret bound follows immediately uniform bound. discussion different conﬁdence bounds allowing prove uniform-pac bounds provide short proof sketch uniform bound. bound jointly critical ubev continually make experience. ubev stopped leveraging observations ﬁxed number would able distinguish high probability among remaining possible mdps optimal policies sufﬁciently optimal mdps. algorithm therefore could potentially follow policy least ε-optimal inﬁnitely many episodes sufﬁciently small enable ubev incorporate observations conﬁdence bounds ubev must hold inﬁnite number updates. therefore require proof total probability possible failure events bounded order obtain high probability guarantees. contrast prior -pac proofs consider ﬁnite number failure events must bound probability inﬁnite possible failure events. choices conﬁdence bounds hold uniformly across sample sizes sufﬁciently tight uniform results. example recent work azar uses conﬁdence intervals shrink rate number episodes number samples pair particular time step. conﬁdence interval hold episodes intervals shrink sufﬁciently quickly even increase. simple approach constructing conﬁdence intervals sufﬁcient uniform guarantees combine bounds ﬁxed number samples union bound allocating failure probability failure case samples. results conﬁdence intervals shrink rate/n interestingly bounds shrink better rate of/n bounds sparked recent interest bounds much tighter than/n sequential decision making best knowledge ﬁrst leverage prove several general bounds appendix explain results analysis appendix bounds sufﬁcient ensure uniform optimal respect yields highest return mdps given constraints eqs. deﬁne failure event complement true event true desired higher optimal value function true therefore optimality bounded right hand side expression decomposed standard identity probability following policy true encounter quantities model parameters optimistic sake conciseness ignore second term following bounded ﬁrst. decompose ﬁrst term sa∈ltk also holds nice episodes signiﬁcant probability also signiﬁcant probability past wti. substituting careful pidgeon-hole argument laid lemma appendix show term bounded nice episodes. using pidgeon-hole argument show episodes nice. combining bounds optimality except episodes. decompose failure event multiple components. addition events triple observed times compared visitation probabilities past i.e. well conditional version statement failure event contains events empirical estimates immediate rewards expected optimal value successor states individual transition probabilites true states l-distance empirical transition probabilities true probabilities episode large show using uniform version popular bound weissman prove appendix show similar manner events small probability uniformly episodes together yields uniform bound thm. using second term min. reﬁned analysis avoids hölder’s inequality stronger notion nice episodes called friendly episodes obtain bound ﬁrst term min. however since similar analysis recently released defer discussion appendix. bound ubev theorem never worse improves similar mbie algorithm factor bound linear dependence size state-space depends tighter dependence horizon mormax’s best sample-complexity bound linear dependency far. comparing ubev’s regret bound ones ucrl regal requires care measure regret entire episodes transition dynamics time-dependent within episode effectively increases state-space factor converting bounds ucrl/regal setting yields regret bound order here diameter state space increases time-dependent transition dynamics additional gained stating regret terms episodes instead time steps. hence ubev’s bounds better factor bound matches recent regret bound episodic azar terms azar regret bounds optimal algorithm uniform characteristics outlined section uniform-pac framework strengthens uniﬁes high-probability regret performance criteria reinforcement learning episodic mdps. newly proposed algorithm uniform-pac side-effect means ﬁrst algorithm sublinear regret. besides this law-of-the-iterated-logarithm conﬁdence bounds algorithms mdps provides practical theoretical boost cost terms computation implementation complexity. work opens several immediate research questions future work. deﬁnition uniform-pac relations regret notions directly apply multi-armed bandits contextual bandits special cases episodic inﬁnite horizon reinforcement learning. extension non-episodic settings highly desirable. similarly version ubev algorithm inﬁnite-horizon linear state-space sample complexity would interest. broadly theory ever something useful practical algorithms large-scale reinforcement learning deal unrealizable function approximation setup major long-standing open challenge. references vincent françois-lavet raphaël fonteneau damien ernst. discount deep reinforcement learning towards dynamic strategies. nips workshop deep jiang akshay krishnamurthy alekh agarwal john langford robert schapire. contextual decision processes bellman rank pac-learnable. international conference machine learning alekh agarwal daniel satyen kale john langford lihong robert schapire. taming monster fast simple algorithm contextual bandits. journal machine learning research volume jean yves audibert rémi munos csaba szepesvári. exploration-exploitation tradeoff using variance estimates multi-armed bandits. theoretical computer science tsachy weissman erik ordentlich gadiel seroussi sergio verdu marcelo weinberger. inequalities deviation empirical distribution. technical report http//www.hpl.hp.com/techreports//hpl--r.pdf? origin=publicationdetail. peter bartlett tewari. regal regularization based algorithm reinforcement learning weakly communicating mdps. proceedings twenty-fifth conference uncertainty artiﬁcial intelligence pages stephane boucheron gabor lugosi pascal massart. concentration inequalities nonasymptotic theory independence. oxford university press isbn ---. proof. episodic mdps essentially -armed bandits hard distinguish prove statement. mdps state horizon actions ﬁxed rewards bernoulli distributed actions mdps. playing action gives bernoulli rewards action gives bernoulli rewards. assume algorithm nonzero probability plays suboptimal action times total i.e. number times action played therefore regret large enough hence algorithm ensure sublinear regret play suboptimal action inﬁnitely often probability however implies algorithm cannot satisfy ﬁnite bound accuracy proof. bound high-probability regret bound consider ﬁxed bound fpac following algorithm satisﬁes bound. algorithm uses worst possible policy optimality episodes event ﬁrst episodes complimentary event remaining episodes follows policy optimality probability regret algorithm min{t c/ε}h min{t event regret algorithm least takes minimum positive value hence therefore bound rate implies best high-probability regret bound order tight furthermore looking equation ﬁxed algorithm uniform high-probability regret bound uniform high-probability regret bound consider ﬁxed bound fpac evaluates value parameter algorithm uses worst possible policy optimality episodes event ﬁrst episodes uniform high-probability regret bound bound consider least suboptimal policy exists optimality nondecreasing function fuhpr algorithm plays optimal policy except episodes l/ε. algorithm satisﬁes regret bound makes inﬁnitely many ε/-mistakes probability uniform high-probability regret bound expected regret bound consider least suboptimal policy exists optimality consider algorithm probability always plays suboptimal policy probability always plays optimal policy. algorithm satisﬁes uniform high-probability regret bound suffers regret proof. convergence optimal policies convergence optimal policies follows directly using deﬁnition limits sequence outcome high-probability event bound holds. -pac sub-additivity probabilities bound assumes worst case ﬁrst algorithm makes worst mistakes possible regret subsequently less less severe mistakes controlled mistake bound. better intuition figure rewards deterministic value probability uniformly random otherwise. construction results mdps concentrated non-deterministic transition probabilities sparse rewards. since algorithms proposed assuming rewards known fair comparison assumed algorithms immediate rewards known adapted algorithms accordingly. example ubev term replaced true known rewards parameter scaled accordingly since concentration result immediate rewards necessary case. used adapted mormax ucrl ucfh mbie medianpac delayed q-learning episodic setting time-dependent transition dynamics using allowing learn time-dependent dynamics ﬁnite-horizon planning. adapt conﬁdence intervals re-derive constants algorithm. doubt opted smaller constants typically resulting better performance competitors. replaced range value function observed range optimistic next state values conﬁdence bounds. also reduced number episodes used delays factor mormax delayed q-learning ucfh would otherwise performed single policy update even within million episodes considered. scaling violates theoretical guarantees least shows methods work principle. performance reported figure expected return current policy algorithm averaged episodes. ﬁgure shows single randomly generated results representative. reran experiments different random seeds consistently obtained qualitatively similar results. source code experiments including concise efﬁcient implementations algorithms available https//github.com/chrodan/finiteepisodicrl.jl. note lower bound sample complexity method episodic mdps timedependent dynamics applies arbitrary ﬁxed bound therefore immediately stronger uniform-pac bounds. theorem proved theorem jiang standard construction involving careful layering difﬁcult instances multi-armed bandit problem. simplicity omitted dependency failure probability using techniques proof theorem strehl lower bound order obtained. lower bound shows small sample complexity ubev given theorem optimal except factor logarithmic terms. conﬁdence bound ˆpk|s t)/n empirical transition probabilities empirical average rewards. proof. since ˜vh+ initialized never changed immediately optimal value ﬁxed optimal values ˜vt+. plugging computation computation details analysis analysis denote value planning iteration ntk. denote probability sampling state slight abuse notation denotes probability vector pk|s conditional probability given optimistic computed optimistic planning steps iteration also following deﬁnitions proof theorem corollary ensures failure event probability outside failure event lemma ensures polylog episodes friendly. finally lemma shows friendly episodes except polylog ε-optimal. second bound follows replacing second term. furthermore outside failure event lemma ensures polylog episodes nice. finally lemma shows nice episodes except section deﬁne failure event cannot guarantee performance ubev. show event occurs probability. arguments based general uniform concentration measure statements prove section following argue apply setting ﬁnally combine concentration results failure event deﬁned proof. consider denote sigma-ﬁeld induced ﬁrst episodes k-th episode st+. deﬁne index episode observed time time. note stopping times respect deﬁne ﬁltration value episode markov property martingale difference sequence respect ﬁltration further since |xi| t+)] condit+)/-subgaussian hoeffding’s lemma i.e. satisﬁes e|gi−] tionally t+)/). therefore apply lemma conclude proof. consider ﬁrst denote number times triple encountered total algorithm. deﬁne random sequence follows. indicator whether next state encountered time bernoulli|s drawn i.i.d. construction sequence i.i.d. bernoulli random variables mean proof. consider deﬁne sigma-ﬁeld induced ﬁrst episodes indicator whether observed episode probability whether measurable hence apply lemma second statement consider denote sigma-ﬁeld induced ﬁrst episodes k-th episode su+. deﬁne index episode observed time time. note stopping times respect deﬁne ﬁltration indicator whether observed episode note probablity a)i{τi gi-measureable. markov property martingale difference sequence respect ﬁltration therefore apply lemma using union deﬁne notion nice stronger friendly episodes. nice episodes states either probability occuring probability occuring previous episodes large enough outside failure event guarantee exceed chosen threshold next section bound optimality episode terms form results derived bound number nice episodes algorithm follow ε-suboptimal policy. together bound number non-nice episodes obtain sample complexity ubev shown theorem similarly reﬁned analysis optimality friendly episodes together lemma obtain tighter sample complexity linear-polylog deﬁnition episode nice following conditions hold wmin times total. episode friendly either nice since left-hand side inequality increases least time happens right hand side stays constant happen times total. therefore denote cas|t|r monotonically decreasing satisﬁes llnp+d condition cannot satisﬁed since time condition satisﬁed increases least wmin holds happen section decompose optimality bound term individually. finally rate lemmas presented previous section used determine bound number nice friendly episodes optimality larger decomposition following lemma simpler version bounding number ε-suboptimal nice episodes eventually lead ﬁrst bound theorem lemma good event holds proof. ﬁrst inequality follows simply deﬁnition optimal value function since outcome consider event know true transition probabilities optimal policy optimal policy feasible solution optimistic planning problem lemma ubev solves. therefore follows immediately last equality follows fact difference distributions since centered bernoulli variable /-subgaussian satisﬁes exp]. since martingale nonnegative sub-martingale apply maximal inequality bound ﬁxed deﬁne stopping time min{t sequence min{t applying convergence theorem nonnegative supermartingales limt→∞ well-deﬁned almost surely. therefore well-deﬁned even optional stopping theorem nonnegative supermartingales applying fatou’s lemma obtain using markov’s inequality ﬁnally bound", "year": 2017}