{"title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization  Techniques", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.", "text": "present seboost technique boosting performance existing stochastic optimization methods. seboost applies secondary optimization process subspace spanned last steps descent directions. method inspired sesop optimization method large-scale problems adapted stochastic learning framework. applied existing optimization method need tweak internal algorithm. show method able boost performance different algorithms make robust changes hyper-parameters. boosting steps seboost applied large sets descent steps additional subspace optimization hardly increases overall computational burden. introduce hyper-parameters control balance baseline method secondary optimization process. method evaluated several deep learning tasks demonstrating promising results. stochastic gradient descent based optimization methods widely used many different learning problems. given objective function want optimize vanilla gradient descent method would simply take ﬁxed step direction current gradient. many learning problems objective loss function averaged given training examples. scenario calculating loss entire training would expensive therefore approximated small batch resulting stochastic algorithm requires relatively calculations step. simplicity efﬁciency algorithms made standard choice many learning tasks speciﬁcally deep learning although vanilla memory previous steps usually utilized example using momentum alternatively adagrad method uses previous gradients order normalize component gradient adaptively adam method uses estimate adaptive moment work utilize knowledge previous steps spirit sequential subspace optimization framework nature sesop allows easily merged existing algorithms. several extensions introduced years different ﬁelds pcd-sesop ssf-sesop showing state-of-the-art results matching ﬁelds core idea method follows. every outer iteration ﬁrst perform several steps baseline stochastic optimization algorithm summed inner cumulative stochastic step. afterwards minimize objective function afﬁne subspace spanned cumulative stochastic step several previous outer steps optional directions. subspace optimization boosts performance baseline algorithm therefore method called sequential subspace optimization boosting method many different large-scale optimization problems applying complex optimization methods practical. thus popular optimization methods problems usually based stochastic estimation gradient. minx∈rn minimization problem gradient general stochastic approach applies following optimization rule result iteration learning rate approximation obtained using small subset training data. stochastic descent methods proved many different problems speciﬁcally context deep learning algorithms providing combination simplicity speed. notice vanilla algorithm memory previous iterations. different optimization methods based usually utilize previous iterations order make informed descent process. sequential subspace optimization method optimization technique used large scale optimization problems. core idea sesop perform optimization objective function subspace spanned current gradient direction directions obtained previous optimization steps. following notations section subspace structure sesop usually deﬁned based following directions sesop formulation current gradient last step mandatory used enrich subspace. theoretical point view enrich subspace nemirovsky directions weighted average previous gradients direction starting point. provide optimal worst case complexity method denoting directions iteration sesop algorithm would solve minimization problem thus sesop reduces optimization problem subspace spanned iteration. means instead solving optimization problem dimensionality subspace governed size controlled. explained section dealing large-scale optimization problems stochastic learning methods usually better ﬁtted task many involved optimization methods. however applied correctly methods still used boost optimization process achieve faster convergence rates. propose start algorithm baseline apply sesop-like optimization method alternating manner. subspace sesop algorithm arises descent directions baseline utilizing previous iterations. description method given algorithm note subset training data used secondary optimization step isn’t necessarily baseline step shown section also note step last added direction changed done order incorporate step performed secondary optimization subspace. perform steps baseline stochastic optimization method direction cumulative step subspace dimension exceeded limit remove oldest direction optimization subspace clear seboost offers attractive balance baseline stochastic steps costly subspace optimizations. firstly number stochastic steps grows effect subspace optimization result subsides taking reduces algorithm back baseline method. secondly dimensionality subspace optimization problem governed size reduced parameters required. notice also seboost added baseline stochastic optimization method require internal changes made original algorithm. thus applied method minimal implementation cost potentially boosting base method. enriching subspace although core elements optimization subspace directions last external steps stochastic cumulative direction many elements added enrich subspace. anchor points last directions saved subspace subspace knowledge recent history optimization process. subspace might beneﬁt directions dependent preceding directions well. example could think overall descent achieved algorithm possible direction descent second half optimization process formulate idea deﬁning anchor points. anchors points locations chosen throughout descent process update rarely. anchor point direction added subspace. different techniques chosen setting changing anchors. formulation point associated parameter describes number boosting steps update point. every steps corresponding point initialized back current control number iterations anchor point becomes irrelevant initialized again. algorithm shows anchor points added algorithm incorporating step momentum similarly idea momentum methods save weighted average previous updates optimization subspace. denoting current momentum momentum updated µ·mk last step hyper-parameter regular momentum. note also found useful normalize anchor directions. figure results experiment baseline parameters lrsgd lradagrad provided good convergence. seboost’s parameters ﬁxed function evaluations secondary optimization. following recent rise interest deep learning tasks focus evaluation different neural networks problems. start small challenging regression problem proceed known problems mnist autoencoder cifar- classiﬁer. problem compare results baseline stochastic methods boosted variants showing seboost give signiﬁcant improvement base method. note purpose work directly compete existing methods rather show seboost improve learning method compared its’ original variant preserving original qualities algorithms. chosen baselines momentum nesterov’s accelerated gradient adagrad conjugate gradient used subspace optimization. algorithm implemented evaluated using torch framework publicly available. main hyper-parameters altered experiments were experiments weight decay momentum ﬁxed nag. unless stated otherwise number function evaluations baseline method used mini-batch size subspace optimization applied mini-batch size note subspace optimization applied signiﬁcantly larger batch. stochastic step canceled next ones single secondary step bigger effect overall result therefore requires better approximation gradient. boosting step applied large sets base method added cost hinder algorithm. experiment different architecture deﬁned. notation denote classic linear layer inputs outputs followed non-linear tanh function. notice presenting results show different graphs. right always shows error function number passes baseline algorithms data left shows error function actual processor time taking account additional work required boosted algorithms. start evaluating method small regression problem. dataset question values simulating continuous function dataset divided training examples test examples. problem solved using tiny neural network architecture although network size small resulting optimization problem remains challenging gives clear indication seboost’s behavior. figure shows optimization process different methods. examples boosted variant converged faster. note different variants seboost behave differently governed corresponding baseline. classic neural network formulation autoencoder network tries learn efﬁcient representation given data. autoencoder usually composed parts encoder takes input produces compact representation decoder takes representation tries reconstruct original input. experiment mnist dataset used training images size test images. encoder deﬁned three layer network architecture form matching decoder figure shows optimization process autoencoder problem. similar trend seen experiment seboost able signiﬁcantly improve shows improvement adagrad although noticeable. nice byproduct working autoencoding problem visualize quality reconstructions function iterations. figure shows change reconstructions quality sesop-sgd shows boosting achieved signiﬁcant terms actual results. classiﬁcation purposes standard benchmark cifar- dataset. dataset composed images size different classes class different images. images used training testing. order check seboost’s ability deal large modern networks resnet architecture winner ilsvrc classiﬁcation task used. figure shows optimization process achieved accuracy resnet depth note manually tweak learning rate done original paper. adagrad boosted experiment achieve signiﬁcant boosting reach better minimum. boosting step applied every epoch applying frequent boosting steps resulted less stable optimization higher minima applying infrequent steps also lead higher minima. experiment shows similar results mnist discusses them. seboost introduces hyper-parameters number baseline steps subspace optimization number directions use. purpose following experiments measure effect parameters achieved result give intuition meaning. experiments based mnist autoencoder problem deﬁned section first consider parameter controls balance baseline algorithm involved optimization process. taking small values results steps secondary optimization process however direction subspace composed fewer steps stochastic algorithm making less stable. furthermore recalling secondary optimization costly regular optimization steps applying often would hinder algorithm’s performance. hand taking large values weakens effect seboost baseline algorithm. figure shows affects optimization process. applying subspace optimization frequently increases algorithm’s runtime reaches higher minimum variants expected. although taking large value reaches better minimum taking value large slows algorithm. experiment taking balances correctly trade-offs. consider effect governs size subspace secondary optimization applied. although taking large values allows hold directions apply optimization larger subspace also makes optimization process involved. figure shows affects optimization process. interestingly lower faster algorithm starts descending. however larger values tend reach better minima. algorithm reaches minimum starts descent process faster making good choice experiment. conclude introduced hyper-parameters affect overall boosting effect achieved seboost. parameters incorporate different trade-offs optimization problem considered using algorithm. experiments show good initialization would algorithm runs twice epoch components seboost structure subspace optimization applied. purpose following experiments changes baseline algorithm addition directions affect algorithm. experiments based mnist autoencoder problem deﬁned section basic formulation seboost subspace composed directions baseline algorithm. section choosing different baselines affect algorithm. another experiment interest algorithm inﬂuenced changes hyperparameters baseline algorithm. figure shows effect learning rate baseline algorithms boosted variants. seen change original baseline affects algorithm however impact noticeably smaller showing algorithm robustness original learning rate. section additional directions added subspace deﬁned directions possibly enrich subspace improve optimization process. figure shows inﬂuence directions overall result. seboost-anchors anchor points added values seboost-momnetum momentum vector used. seen using proposed anchor directions signiﬁcantly boost algorithm. momentum direction less useful giving small boost actually slightly hinders performance used conjunction anchor directions. paper presented seboost technique boosting stochastic learning algorithms secondary optimization process. secondary optimization applied subspace spanned preceding descent steps extended additional directions. evaluated seboost different deep learning tasks showing achieved results methods compared original baselines. believe ﬂexibility seboost could make useful different learning tasks. easily change frequency secondary optimization step ranging frequent risky steps stable step epoch. changing baseline algorithm structure subspace allows alter seboost’s behavior. although focus work interesting research direction seboost parallel computing. similarly look framework composed single master workers worker optimizes local model master saves global parameters based workers. inspired seboost take descent directions workers apply subspace optimization spanned subspace allowing master take efﬁcient step based information workers. another interesting direction future work investigation pruning techniques. work subspace fully occupied oldest direction simply removed. might consider advanced pruning techniques eliminating direction contributed least secondary optimization step even randomly removing subspace directions. good pruning technique potentially signiﬁcant effect overall result. ideas researched future work. overall believe seboost provides promising balance popular stochastic descent methods involved optimization techniques. research leading results received funding european research council european unions seventh framework program grant agreement supported intel collaborative research institute computational intelligence jeffrey dean greg corrado rajat monga chen matthieu devin mark andrew senior paul tucker yang quoc large scale distributed deep networks. advances neural information processing systems pages michael elad boaz matalon michael zibulevsky. coordinate subspace optimization methods linear least squares non-quadratic regularization. applied computational harmonic analysis ross girshick jeff donahue trevor darrell jitendra malik. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition pages alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages ilya sutskever james martens george dahl geoffrey hinton. importance initialization momentum deep learning. proceedings international conference machine learning pages", "year": 2016}