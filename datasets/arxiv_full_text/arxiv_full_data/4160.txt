{"title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "tag": ["cs.CV", "cs.AI"], "abstract": "Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.", "text": "central idea represent test sample linear combination samples available training set. sparsity manifests non-zeros correspond bases whose memberships test sample. therefore ideal case object expected class subspace class subspaces non-overlapping. concretely given classes dictionary comprising training samples class sample class represented dcxc. consequently express using dictionary dx+···+ dcxc +···+dcxc active elements located hence coefﬁcient vector expected sparse. matrix form samples comprises class coefﬁcient matrix would sparse. ideal case block diagonal shown learning dictionary training samples instead using dictionary enhance performance src. existing classiﬁcation-oriented dictionary learning methods learn discriminative class-speciﬁc dictionaries either imposing block-diagonal constraints encouraging incoherence class-speciﬁc dictionaries. based k-svd model general sparse representations discriminative k-svd label-consistent k-svd learn discriminative dictionaries encouraging projection sparse codes close sparse matrix non-zeros satisfying block diagonal structure figure dfdl yang fddl apply fisher-based ideas dictionaries sparse coefﬁcients respectively. recently combined fisher-based idea introduced low-rank constraint sub-dictionary. claim model would reduce negative effect noise contained training samples. abstract— despite fact different objects possess distinct class-speciﬁc features also usually share common patterns. observation exploited partially recently proposed dictionary learning framework separating particularity commonality inspired this propose novel method explicitly simultaneously learn common patterns well class-speciﬁc features classiﬁcation intuitive constraints. dictionary learning framework hence characterized shared dictionary particular dictionaries. shared dictionary enforce low-rank constraint i.e. claim spanning subspace dimension coefﬁcients corresponding dictionary similar. particular dictionaries impose well-known constraints stated fisher discrimination dictionary learning further develop fast accurate algorithms solve subproblems learning step accelerating convergence. said algorithms could also applied fddl extensions. efﬁciencies algorithms theoretically experimentally veriﬁed comparing complexities running time well-known dictionary learning methods. experimental results widely used image datasets establish advantages method state-of-the-art dictionary learning methods. sparse representations emerged powerful tool range signal processing applications. applications include compressed sensing signal denoising sparse signal recovery image inpainting image segmentation representations signals expressed linear combination bases taken dictionary. based theory sparse representation-based classiﬁer initially developed robust face recognition. thereafter adapted numerous signal/image classiﬁcation problems ranging medical image classiﬁcation hyperspectral image classiﬁcation synthetic aperture radar recaptured image recognition video anomaly detection several others assumption made discriminative dictionary learning methods i.e. non-overlapping subspaces unrealistic practice. often objects different classes share common features e.g. background scene classiﬁcation. problem partially addressed recent efforts namely dlsi copar csdl however dlsi explicitly learn shared features since still hidden sub-dictionaries. copar csdl explicitly learn shared dictionary suffer following drawbacks. first contend subspace spanned columns shared dictionary must rank. otherwise class-speciﬁc features also represented shared dictionary. worst case shared dictionary span include classes greatly diminishing classiﬁcation ability. second coefﬁcients corresponding shared dictionary similar. implies features shared training samples different classes shared dictionary. paper develop low-rank shared dictionary learning framework satisﬁes aforementioned properties. framework basically generalized version well-known fddl additional capability capturing shared features resulting better performance. also show practical merits enforcing constraints signiﬁcant. typical strategy optimizing general dictionary learning problems alternatively solve subproblems sparse coefﬁcients found ﬁxing dictionary vice versa. discriminative dictionary learning models matrices furthermore comprise several small classspeciﬁc blocks constrained complicated structures usually resulting high computational complexity. traditionally solved block-by-block convergence. particularly block solved ﬁxing blocks although greedy process leads simple algorithm produces inaccurate solutions also requires huge computation. paper mitigate drawbacks proposing efﬁcient accurate algorithms allows directly solve fundamental discriminative dictionary learning methods fddl dlsi algorithms also applied speed-up proposed lrsdl copar related works. low-rank shared dictionary learning framework automatically extracting discriminative shared bases several widely used image datasets presented enhance classiﬁcation performance dictionary learning methods. framework simultaneously learns class-dictionary class extract discriminative features shared features classes contain. shared part impose intuitive constraints. first shared dictionary must low-rank structure. otherwise shared dictionary also expand contain discriminative features. second contend sparse coefﬁcients corresponding shared dictionary almost similar. words contribution shared dictionary reconstruct every signal close together. experimentally show constraints crucial shared dictionary. accurate efﬁcient algorithms selected existing proposed dictionary learning methods. present three effective algorithms dictionary learning sparse coefﬁcient update fddl using fista address main challenge algorithm calculate gradient complicated function effectively introducing simple function block matrices lemma dictionary update fddl support simple procedure using another lemma. extension fddl proposed lrsdl also beneﬁts aforementioned efﬁcient procedures. iii) dictionary update dlsi simple admm procedure requires matrix inversion instead several matrix inversions originally proposed subsequently show proposed algorithms performance computational beneﬁts. complexity analysis. derive computational complexity numerous dictionary learning methods terms approximate number operations needed. also report complexities experimental running time aforementioned efﬁcient algorithms original counterparts. reproducibility. numerous sparse coding dictionary learning algorithms manuscript reproducible user-friendly toolbox. toolbox includes implementations lc-ksvd efﬁcient dlsi efﬁcient copar efﬁcient fddl proposed lrsdl. toolbox provided hope usage future research comparisons peer researchers. remainder paper organized follows. section presents proposed dictionary learning framework efﬁcient algorithms subproblems efﬁcient procedure updating dictionaries dlsi copar. complexity analysis several well-known dictionary learning methods included section iii. section show given function sets variables deﬁne function variables ﬁxed. greek letters represent positive regularization parameters. given block matrix deﬁne function follows convex continuously differentiable lipschitz continuous gradient. fista iterative method requires calculate gradient iteration. paper focus calculating gradient fddl used broadly technique exploiting structured dictionary learning discriminative coefﬁcient. speciﬁcally discriminative dictionary sparse coefﬁcient matrix learned based minimizing following cost function fisher-based discriminative coefﬁcient term lnorm encouraging sparsity coefﬁcients. last term means small contribution representation last term cost function becomes convex respect shared dictionary needs satisfy following properties generativity common part important property shared dictionary represent samples classes words expected well represented collaboration particular figure lrsdl idea with brown items shared; green blue items class-speciﬁc. notation. discriminative ﬁdelity constraint class-c sample mostly represented fisher-based discriminative coefﬁcient constraint. classiﬁcation accuracies lrsdl widely used datasets comparisons existing methods literature reveal merits proposed lrsdl. section concludes paper. mean columns respectively. given matrix natural number deﬁne matrix columns column mean vector columns ignored implicitly number columns mean matrices. number columns depends context e.g. writing mc−m mean ‘mean vectors’ illustrated figure dictionary part expressed remaining dictionary expected contain classspeciﬁc features excluded classiﬁcation. shared code considered contribution shared dictionary representation even shared dictionary already low-rank contributions class might different illustrated figure row. case different contributions measured convey class-speciﬁc features avoid. naturally regularization term added proposed objective function force close mean vector constraint fisher-based discriminative coefﬁcient term extended deﬁned ηd∗. minimizing objective function jointly class speciﬁc shared dictionaries. notice shared dictionary become respectively becomes lrsdl reduces fddl. efﬁcient fddl dictionary update recall dictionary update step divided subproblems updates class-speciﬁc dictionary others ﬁxed. process repeated convergence. approach highly time consuming also inaccurate. generativity property also seen figure ﬁgure intersection different subspaces representing class subspace visualized light brown region. class subspace instance class well represented ideal shared atoms corresponding class-speciﬁc atoms low-rankness stated generativity property necessary condition atoms qualify shared dictionary. note atoms inside shaded ellipse figure also satisﬁes generativity property along remaining squares atoms well represent class subspace; observed class worst case including atoms also satisfy generativity property undesirable case would discriminative features remaining classspeciﬁc dictionaries. low-rankness hence necessary prevent shared dictionary absorbing discriminative atoms. constraint natural based observation subspace spanned shared dictionary dimension. concretely nuclear norm regularization convex relaxation rank force shared dictionary low-rank. contrast work existing approaches employ shared dictionaries i.e. copar incorporate crucial constraint. using admm method singular value thresholding algorithm admm procedure follows. first choose positive initialize alternatively solve following subproblems convergence lrsdl sparse coefﬁcients update preliminary work proposed method effectively solving alternatively combine problems solving following optimization problem j-th algorithm highly computational since requires matrix inversion columns propose admm procedure update requires matrix inversion referred edlsi-d. first letting i−th columns i−th element diagonal dominant computational task compute requires operators. since columns algorithm requires iterations complexity odl-d qdk. original update updating sub-dictionary solved main step algorithm stated dominant computational part matrix inversion complexity matrix-vector multiplication vector normalization ignored here. since columns algorithm requires iterations complexity o-dlsi-d algorithm cqkd. efﬁcient algorithm requires matrix inversion. later paper theoretically experimentally show e-dlsi-d much efﬁcient o-dlsi-d note algorithm beneﬁcial subproblems updating common dictionary particular dictionary copar well. compare computational complexity efﬁcient algorithms corresponding original algorithms. also evaluate total complexity proposed lrsdl competing dictionary learning methods dlsi copar fddl complexity algorithm estimated number multiplications required iteration simplicity assume number training samples number dictionary bases class same means number bases dictionary comparable number training samples class much less signal dimension i.e. iii) iterative algorithm requires iterations convergence. consistency changed notations methods denoting training sample sparse code. efﬁcient update based section ii-d complexity e-fddl-x mainly comes equation recall function require much computation. computation also neglected since required calculation column columns same. total complexity algorithm e-fddl-x roughly cost function copar another dictionary learning method also considers shared dictionary using notation lrsdl rewrite cost function copar following form update copar dictionary update algorithm requires solve problems form o-copar-d uses method o-dlsi-d proposed e-copar-d takes advantages e-dlsi-d presented section ii-e. therefore total complexity o-copar-d roughly cqkd similar complexity e-fddl-x. recall supposed number classes update compare e-fddl-d lrsdl-d requires computation y−dx then complexity lrsdl-d update algorithm lrsdl-d presented section ii-d main computation comes shrinkage thresholding operator requires matrix multiplications. total complexity lrsdl-d also assume table shows three proposed efﬁcient algorithms require less computation original versions signiﬁcant improvements speeding dlsi-d. table demonstrates interesting fact. lrsdl least expensive computationally compared original dictionary learning algorithms e-fddl lower complexity expected since fddl cost function special case lrsdl cost function. copar found expensive computationally. table table show ﬁnal complexity analysis proposed efﬁcient algorithm original counterparts. table compares lrsdl state-of-the-art methods. pick typical parameters classes training samples class bases sub-dictionary shared dictionary data dimension iterations iterative method. concretely comparing methods datasets present experimental results applying methods diverse datasets extended yaleb face dataset face dataset gender dataset oxford flower dataset multi-class object category caltech dataset dense sift descriptor. dsift descriptor extracted patch densely sampled dense grid pixels. extract sparse coding spatial pyramid matching feature concatenation vectors pooled words extracted dsift descriptor. dimension words pooling technique used pooling grid setup dimension scspm feature followed dimension reduction using pca. coil- dataset contains various views objects different lighting conditions. object images captured equally spaced views. similar work randomly choose views object training rest used test. obtain feature vector image ﬁrst convert grayscale resize pixel vectorize matrix -dimensional vector ﬁnally normalize unit norm. evaluate improvement three efﬁcient algorithms proposed section apply efﬁcient algorithms original versions training samples face dataset verify convergence speed algorithms. example number classes randomface feature dimension number training samples class number atoms particular dictionary e-fddl-d e-fddl-x figure shows cost functions running time iterations different versions fddl original fddl combination o-fddl-x e-fddl-d combination e-fddl-x o-fddl-d efﬁcient fddl ﬁrst observation o-fddl converges quickly suboptimal solution best cost obtained e-fddl. addition o-fddl requires seconds dataset caltech coil- example images datasets shown figure compare results using state-of-theart dictionary learning methods lc-ksvd dlsi fddl copar dlrd srrs regularization parameters methods chosen using ﬁve-fold cross-validation experiment different randomly split training test sets report averaged results. face datasets feature descriptors random faces made projecting face image onto random vector using random projection matrix. dimension random-face feature extended yaleb dimension face samples datasets shown figure gender dataset ﬁrst choose non-occluded subset face dataset consists males females conduct experiment gender classiﬁcation. training images taken ﬁrst males females test images comprises samples remaining males females. used reduce dimension image samples dataset shown figure oxford flower dataset collection images ﬂowers drawn species images class totaling images. feature extraction based impressive results presented choose frequent obtain feature local histogram feature extractor e-dlsi-d e-copar-d figure compare convergence rates dlsi copar algorithms. cost function value improves slightly time efﬁcient algorithms reduces signiﬁcantly. based beneﬁts cost function value computation rest paper efﬁcient optimization algorithms instead original versions obtaining classiﬁcation results. demonstrate behavior dictionary learning methods dataset presence shared features create example figure classiﬁcation problem classes whose basic class-speciﬁc elements shared elements visualized figure basis element dimension pixel× pixel. elements generate samples class linearly combining classspeciﬁc elements shared elements followed noise added; samples class used training remaining images used testing. samples class shown figure figure show sample learned bases using dlsi shared features still hidden class-speciﬁc bases. lcksvd bases shared features found classiﬁed bases class class diminishing classiﬁcation accuracy since test samples classiﬁed class phenomenon happens fddl bases best classiﬁcation results happen three shared dictionary learnings figure figure proposed lrsdl figure shared bases extracted gathered shared dictionary. however copar shared features still appear class-speciﬁc dictionaries shared dictionary also includes class-speciﬁc features. lrsdl class-speciﬁc elements shared elements nearly perfectly decomposed appropriate dictionaries. reason behind phenomenon low-rank constraint shared dictionary lrsdl. thanks constraint lrsdl produces perfect results simulated data. perform experiment study effect shared dictionary size overall classiﬁcation results three shared dictionary methods copar lrsdl gender dataset. experiment images class used training. number shared dictionary bases varies lrsdl regularization parameter attached low-rank term consider three values i.e. low-rank constraint different degrees emphasis. results shown figure observe performance copar heavily depends choice results worsen size shared dictionary increases. reason large copar tends absorb class-speciﬁc features shared dictionary. trend associated lrsdl even low-rank constraint ignored small) lrsdl another constraint forces coefﬁcients corresponding shared dictionary similar. additionally increase overall classiﬁcation lrsdl also gets better. observations conﬁrm proposed constraints shared dictionary important lrsdl exhibits robustness parameter choices. also observe performance robust shared dictionary size results good lrsdl. table shows overall classiﬁcation results various methods presented datasets terms mean standard deviation. evident cases three dictionary learning methods shared features proposed lrsdl) outperform others highest values presenting proposed lrsdl. note method represents query sample class class. also extend method representing query sample whole dictionary residual classiﬁcation src. extended version called jdl*. real-world classiﬁcation tasks often contend lack availability large training sets. understand training dependence various techniques present comparison overall classiﬁcation accuracy function training size different methods. figure overall classiﬁcation accuracies reported ﬁrst datasets corresponding various scenarios. readily apparent lrsdl exhibits graceful decline training reduced. addition lrsdl also shows high performance even training datasets. figure shows performance lrsdl face dataset different values parameters ﬁxed. ﬁrst three parameters vary parameter others ﬁxed. observe performance robust different values accuracies greater cases. also shows lrsdl achieves best performance finally compare training test time sample different dictionary learning methods oxford flower dataset. note that efﬁcient fddl dlsi copar experiment. results shown table result consistent complexity analysis reported table training time lrsdl around half hour times faster copar also better low-rank models i.e. srrs paper primary contribution development discriminative dictionary learning framework introduction shared dictionary crucial constraints. first shared dictionary constrained low-rank. second sparse coefﬁcients corresponding shared dictionary obey similarity constraint. conjunction discriminative model proposed leads ﬂexible model shared features excluded classiﬁcation. important beneﬁt model robustness framework size regularization parameter shared dictionary. comparison state-of-the-art algorithms developed speciﬁcally tasks lrsdl approach offers better classiﬁcation performance average. section ii-d ii-e discuss efﬁcient algorithms fddl dlsi ﬂexibly apply sophisticated models. thereafter section iv-b theoretically practically show proposed algorithms indeed signiﬁcantly improve cost functions time speeds different dictionary learning algorithms. complexity analysis also shows proposed lrsdl requires less computation competing models. proposed lrsdl model learns dictionary shared every class. practical problems feature belong classes. recently researchers begun address issue future work investigate design hierarchical models extracting common features among classes. mousavi monga dfdl discriminative feature-oriented dictionary learning histopathological image classiﬁcation proc. ieee international symposium biomedical imaging mousavi monga histopathological image classiﬁcation using discriminative featureoriented dictionary learning ieee transactions medical imaging vol. march srinivas mousavi monga hattel jayarao simultaneous sparsity model histopathological image representation classiﬁcation ieee transactions medical imaging vol. nasrabadi tran task-driven dictionary learning hyperspectral image classiﬁcation structured sparsity constraints ieee transactions geoscience remote sensing vol. zhang nasrabadi zhang huang multi-view automatic target recognition using joint sparse representation ieee trans. aerospace electronic systems vol. thongkamwitoon muammar p.-l. dragotti image recapture detection algorithm based learning dictionaries edge proﬁles ieee transactions information forensics security vol. monga bala adaptive sparse representations video video anomaly detection circuits systems technology ieee transactions vol. mousavi srinivas monga tran multi-task image classiﬁcation collaborative hierarchical spikeand-slab priors proc. ieee conf. image processing zhang zhang nasrabadi huang jointstructured-sparsity-based classiﬁcation multiple-measurement transient acoustic signals systems cybernetics part cybernetics ieee transactions vol. nguyen patel nasrabadi chellappa design non-linear kernel dictionaries object recognition ieee transactions image processing vol. yang zhang yang zhang metaface learning sparse representation based face recognition image processing ieee international conference ieee mousavi monga adaptive matching pursuit sparse signal recovery international conference acoustics speech signal processing yang zhang feng zhang fisher discrimination dictionary learning sparse representation proc. ieee international conference computer vision nov. yang zhang feng zhang sparse representation based ﬁsher discrimination dictionary learning image classiﬁcation int. journal computer vision vol. ramirez sprechmann sapiro classiﬁcation clustering dictionary learning structured incoherence shared features ieee conf. comp. vision pattern recog. ieee zhou jointly learning visually correlated dictionaries large-scale visual recognition applications ieee transactions pattern analysis machine intelligence vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern analysis machine int. vol. fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding vol. wang xiao zhou sparse representation face recognition based discriminative low-rank dictionary learning proc. ieee conf. computer vision pattern recognition. ieee tiep received b.s. degree electrical engineering hanoi university science technology vietnam currently pursuing ph.d. degree information processing algorithm laboratory pennsylvania state university. research interests broadly areas statistical learning signal image analysis computer vision pattern recognition image classiﬁcation recovery retrieval. vishal monga tenured associate professor school electrical engineering computer science main campus pennsylvania state university university park xerox research undergraduate work completed indian institute technology guwahati doctoral degree electrical engineering obtained university texas austin mongas research interests broadly signal image processing. research group penn state focuses convex optimization approaches image classiﬁcation robust signal hashing radar signal processing computational imaging. currently serves associate editor ieee transactions image processing ieee signal processing letters ieee transactions circuits systems video technology. prof. monga recipient national science foundation career award monkowski early career award college engineering penn state joel ruth spira teaching excellence award.", "year": 2016}