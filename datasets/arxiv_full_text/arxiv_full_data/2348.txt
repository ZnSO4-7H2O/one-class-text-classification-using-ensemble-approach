{"title": "Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A novel method for estimating Bayesian network (BN) parameters from data is presented which provides improved performance on test data. Previous research has shown the value of representing conditional probability distributions (CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network (BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local distributions,each having a different set of parents.This increases the space of possible structures which can be considered,enabling the CPDs to have finer-grained dependencies.The resulting estimation procedure induces a modelthat is better able to emulate the underlying interactions occurring in the data than conventional conditional Bernoulli network models.The results for artificially generated data indicate that overfitting is best reduced by restricting the complexity of candidate mixture substructures local to each node. Furthermore, mixtures of very simple substructures can perform almost as well as more complex ones.The BMN is also applied to data collected from an online adventure game with an application to keyhole plan recognition. The results show that the BMN-based model brings a dramatic improvement in performance over a conventional BN model.", "text": "sented provides improved performance test data. previous research shown value representing conditional distributions decision trees bernoulli explicitly represents crete nodes mixtures local distri­ butions different par­ ents. increases structures cpds finer-grained resulting model better able emulate underlying interactions occurring data onal bernoulli net­ conventional conditi work models. results gen­ erated data indicate overfitting reduced restricting complexity didate mixture substructures node. furthermore substructures perform almost well complex ones. also online adven­ plied data collected keyhole plan ture game application results show bmn­ recognition. based model brings dramatic improvement performance conventional tional bernoulli model. attention recent years common approach struc­ ture estimation selects single lection typically model maximum likelihood alternatively prior weights chosen penalise complex structures model maximum posteri­ likelihood found instead approach reasonable large amount data small number nodes condition often hold true case model chosen might processes. data generalise average many networks typi­ cally parameters integrated data struc­ marginal likelihood ture. bayesian model averaging takes step summing individual struc­ tures gives better average predictions difficulty approaches marginal likelihood pute. particular mixtures used marginal likelihood closed form large-sample often used. approximations however typically accurate. method proposed paper concerned rather structure estimation obtain good estimates con­ structure given ditional probability bayesian network. single could obtained could specified advance domain expert. bernoullimixture model based general novel idea mcmichael node treated distributions ents. previous tions cpds included noisy-or cision approaches estimated methods approximate known cpds distributions also special relationship mbn. recall joint distributions global node orderings whereas averages local structures general work structures stricted alent parameters estimated observed data handled distributed expectation-maximisation algorithm estimation briefly below. recall model given equation thus allowing data likelihood case n;.km; niikm; e�=l oim;. problems first expert reasonably specify prior counts specify mixture general many submodel cope. second problem nijkm; ambiguous much reason. instance specifies specify used nodes. problems design philosophy tional model count family data counts summing states parents included mith submodel. like­ wise computed prior family counts supplied family data counts containing conventional following model known struc­ sibility ture solely data generates. perfor­ mance various restrictions submodels compared usual method model selection. reveals apparent intended sys­ properties address issues. experiments tematically required show subtle properties dependence number nodes number local mixtures. artificially-created model chosen experiment shown figure nodes three four discrete states respectively. andomly initialised resulting used randomly generate ntrain cases training data ntest complete cases data well testing data poorly indicating also another band better per­ overfitting. forming models roughly linear correspondence tween scores empty true low-performance end. high-performance relative performances models training different possible testing data shown figure using log­ divided number data nor­ likelihood empty model malised score. completely independent nodes true model full model com­ pletely dependent nodes. full model special embodies exact rule conditional plete training data lection ini­ figure performance tially improve towards true degrades match full ultimately number iterations increases. however restricting substructure structure) degradation. limiting substructures three possible parents. performance restricted true interestingly parent substructures spite extreme simplicity. tions grade performance towards empty remain fixed observations scribe player completing single quest. dbns converted suitable pre-processing data included clustering thousands possible states node. previous section shown improved performance conventional cho­ model selection data. here performance analysed \"real\" data observed online adventure game known multi-user plan recognition task predict current action location quest player given knowledge vious action location quest. represent simplifications hence based main model structure. model structure ideal vehicle comparison four models since essentially mixture models plus models struc­ tures described here. mixture sub­ models greatest estimated weights interpreted although ously predicting state dividual conventional current location diction performances location model much accurate mixed results conditional predictions ment comprised player entered runs exited mud. relative performances shown figure observe roughly testing training data least contrast almost well main model greater data models. accuracy testing interestingly weights mixture average quasi-action model shown figure similar property individual sumably bias underfitting cpts.", "year": 2013}