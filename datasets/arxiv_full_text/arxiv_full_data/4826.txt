{"title": "Unsupervised Domain Adaptation Using Approximate Label Matching", "tag": ["cs.LG", "cs.AI"], "abstract": "Domain adaptation addresses the problem created when training data is generated by a so-called source distribution, but test data is generated by a significantly different target distribution. In this work, we present approximate label matching (ALM), a new unsupervised domain adaptation technique that creates and leverages a rough labeling on the test samples, then uses these noisy labels to learn a transformation that aligns the source and target samples. We show that the transformation estimated by ALM has favorable properties compared to transformations estimated by other methods, which do not use any kind of target labeling. Our model is regularized by requiring that a classifier trained to discriminate source from transformed target samples cannot distinguish between the two. We experiment with ALM on simulated and real data, and show that it outperforms techniques commonly used in the field.", "text": "general-purpose sentiment classiﬁer access highly-biased training data book reviews online store. similarly model trained photos taken webcam likely generalize poorly photos taken higher-resolution dslr camera. ability improve generalizability ﬁtted models crucial real-world effectiveness data-hungry methods like deep learning. domain adaptation holds promise allowing models ﬁtted datasets labeled examples abundant used make predictions separate dataset labeled samples much scarce generated distribution different samples network originally trained. domain adaptation also connected wide array important machine learning tasks including counterfactual inference off-policy reinforcement learning domain adaptation comes three speciﬁc forms. supervised domain adaptation given several fully-labeled sources goal learn model stronger trained sources alone. semi-supervised domain adaptation fully-labeled sources also partially-labeled target domain. setting interested classifying unlabeled target data well making available information. article consider unsupervised domain adaptation given labeled source examples access labels target examples. coerce model trained domain performing well another domain adaptation methods often learn transformation makes source samples statistically similar target samples. correspondingly difﬁculty domain adaptation lies learning high-quality transformation. previous methods typically learn transformation considering source target samples sometimes source labels make target labels since provided version problem. work propose approach unsupervised domain adaptation called approximate label matching method constructs rough labeling target data exploit dramatically improve domain adaptation addresses problem created training data generated socalled source distribution test data generated signiﬁcantly different target distribution. work present approximate label matching unsupervised domain adaptation technique creates leverages rough labeling test samples uses noisy labels learn transformation aligns source target samples. show transformation estimated favorable properties compared transformations estimated methods kind target labeling. model regularized requiring classiﬁer trained discriminate source transformed target samples cannot distinguish two. experiment simulated real data show outperforms techniques commonly used ﬁeld. intuitively intelligent agents able improve task learned similar kind task. human example might capable understanding italian learned spanish playing tennis after learned play badminton. goal domain adaptation endow machine intelligence sort capability. usual classiﬁcation regression framework assume training test data generated underlying distribution. assumption hold test performance signiﬁcantly worse training performance. problem comes many areas machine learning particularly natural language understanding computer vision example want build concrete illustration show synthetic unsupervised domain adaptation problem discussed section highlights potential importance target label information domain adaptation tasks place three circular domains three vertices equilateral triangle. points inside triangle’s perimeter assigned negative labels outside assigned positive labels. despite obvious similarities domain without somehow incorporating target labels would clearly impossible uncover rotation translation aligns target data either available sources. performs well adversarial situation methods struggle. example might seem contrived show experiments real data similar properties. figure synthetic dataset includes sources target. blue points positive source samples orange points negative source samples purple green points positive negative target samples respectively. target domain labels shown clarity classiﬁer access labels target dataset. sections introduce provide intuitive justiﬁcation method. section discuss means model overﬁt provide simple regularization scheme prevents behavior. finally sections present comparative results applied simulated real data show outperforms related methods domain adaptation. domain adaptation approached distinct ways. ﬁrst training samples re-weighted make resulting hypothesis better suited classiﬁcation test set. kernel mean matching example domain adaptation technique falls category re-weights source points effort make means source target datasets close possible reproducing kernel hilbert space circumvents need approximate source target distributions. like kernel methods vanilla implementation requires construction matrix square number samples target dataset causing potential difﬁculty working large amounts data. alternative approach domains transformed space better match other. representation classiﬁer trained source expected perform well unlabeled target samples. simple effective algorithm takes approach subspace alignment ﬁnds matrix minimizes bregman divergence transformed source target pca-based solution extremely efﬁcient. unlike make label information either source target datasets. like learns transformation match source target domain restricted learning linear transformations whereas able estimate nonlinear transformations. recently several deep learning-based solutions domain adaptation proposed popular example domain adversarial neural networks consist three parts transformation module label classiﬁcation module domain classiﬁcation module goal like uncover transformation projects source target data canonical space model trained source data generalize well unlabeled target. transformation module embeds source target datasets label classiﬁcation module uses representation distinguish labeled source samples. domain classiﬁcation module attached transformation module gradient-reversal layer trained distinguish source target examples. gradient reversal causes transformation module learn confuse domain classiﬁcation module ideally forcing transformed source transformed target data similar distributions. dann modules trained simultaneously. architecture consists three neural network modules similar used danns methods differ number important ways. signiﬁcant difference procedures rough better-than-guessing hypothesis target samples allows uncover transformations techniques cannot. subtle difference methods transformation learned applied target source addition classiﬁer suppose availability approximate labeling target samples rough labeling could come variety simple learning procedures outline options section estimate transformation leverage approximate labeling speciﬁcally ideal decision boundary optimally-transformed target labeling target usually correct able uncover optimal transformation function causes best agree fs’s predictions trans)). formed target samples minimize squared error refer procedure treats source separately approximate label matching. multiple sources available create multiple predictors take simple average discussed above approximate label matching requires better-than-guessing estimate target domain labels order effective. section overview ways approximate labels acquired. ﬁrst approach obtain rough labeling treating sources aggregate larger training ﬁtting classiﬁer would ordinary supervised setting. could predictions classiﬁer target samples call pseudo-supervised rough labeling. works particularly well multiple sources available resulting model able leverage features generalize well across different sources also generalize reasonably well unobserved target domain. important preventing overﬁtting alm. also unlike danns classiﬁcation module trained ofﬂine source dataset make true discriminator rather gradient reversal layer gradient reversal cause instability training approximate label matching somewhat reminiscent co-training developed method classifying data training examples available idea create different hypotheses training classiﬁers different representations data. test samples hypotheses strongly agree added training process repeated. like co-training takes advantage candidate hypotheses rough labeling obtained ofﬂine alternate labeling created passing transformed target samples classiﬁer trained source dataset. unsupervised domain adaptation predictor given source datasets source target sequence dataset includes input sequence label sequence binary classiﬁcation problem every elements. corresponds label d-dimensional inys target sequence goal classiﬁer predict label throughout article notation like denote sequence predictions produced assume that within source labeled examples generated according distribution likewise target. assume domain generated different distribution similar enough model trained transferred degree another. namely discussed section suppose target domain made resemble ﬁxed source domain transformation transformed samples target domain could classiﬁed using model trained s-th source domain. question transformation. natural starting point ﬁrst train classiﬁer label samples source dataset distributions generating source target different accuracy target tend much worse hope predictions obtained transforming target samples classiﬁcation— fs))—will yield favorable results. conﬁdence transformed target sample target sample. minimizing conﬁdence corresponds confusing discriminator effectively making transformed target samples resemble source samples. ﬁtted needs match labeling well possible also aligning source adversarial regularization makes source transformed target samples appear relatively similar without need approximate generative distributions deﬁne explicit distance metric them. implement functions using neural networks. speciﬁcally neural network arbitrary architecture network estimate solving already approximated ﬁxed. transformation represented adding additional network layers existing input layer already-trained network. discriminator output which described earlier iteratively trained distinguish source samples transformed target samples. figure diagram ﬁxed source orange modules trained together blue modules trained ofﬂine ﬁxed. solid arrows correspond standard connections dashed arrows adversarial connections. training aggregate network labels backpropagation mean squared error criterion update weights layers corresponding compositionality backpropagation allows easily solve eqn. even though nested within illustrate algorithm works demonstrate performance artiﬁcial dataset described section difﬁcult classify conceptually simple. data consists three isotropic gaussian domains distributed three vertices equilateral triangle points inside triangle’s perimeter assigned negative labels outside assigned another approach estimating target domain labels preferred source dataset available simply obtain hypothesis alternative unsupervised domain adaptation algorithm produce rough labeling target samples. using conjunction kind rough labeling call overall technique reﬁnement since sense reﬁning labeling produced given domain adaptation algorithm. approximate label matching attempts align target source however transformation highly expressive contorts target samples match rough labeling without aligning observed phenomenon anecdotally frequently experiments; detailed example given section form overﬁtting sense ﬁnding hypothesis agrees precisely underconstrained optimization. order prevent type overﬁtting propose adversarial form regularization attempts force transformed target samples look enough like source samples could confuse classiﬁer trained distinguish two. adding discriminating function trained jointly machinery. step learning updated better distinguish source samples transformed target samples updated deteriorate performance solve eqn. regularization technique similar traditional generative adversarial networks instead generating samples random noise generate source-looking samples target samples. discriminating function takes input either source samples transformed target samples trained predict either source sample target sample. loss minimized simply binary cross-entropy misclassiﬁed source target samples figure visualization synthetic dataset domain adaptation techniques. left original dataset shown figure right source target transformed bottom left transformation learned without adversarial regularization applied target data. bottom right transformation learned adversarial regularization applied target data. example includes sources obtained training multi-layer perceptron sources simultaneously. classiﬁer able correctly predict samples held-out target domain. trained without adversarial regularization overﬁts exactly described section learns make predictions almost exactly matches rough labeling contorts resemble adversarial regularization included learned transformation aligns fairly well yielding accuracy. unsupervised domain adaptation simulation demonstrates alm’s ability transformations algorithms cannot. speciﬁc example recovering correct translation target source inputs straightforward even without using target labeling recovering correct domain rotation challenging circular shape datasets. algorithm make target prediction cannot expected perform well task. target labeling lumps source target together appear distribution cannot uncover necessary target domain rotation accordingly perform better guessing adversarial simulation. turn next empirical evaluation compared leading domain adaptation algorithms. evaluate methods several domain adaptation problems described detail below. begin discussing experimental setup. domain-adaptation problem consists several domains. experiments domain target another source. repeat possible source-target pairs report accuracies each. experiment chose includes three layers allowing nonlinear transformations hidden nodes input output sizes equal dimensionality data. similarly chose predictor used produce pseudo-supervised rough labelings mlps three layers hidden layer size nodes many output dimensions classes problem. also three layers nodes hidden layer. experiments dropout applied help training networks except compare danns datasets. algorithm takes source target inputs returns canonical feature representation. make reasonable comparison space neural network architecture used classiﬁcation. similarly danns transformation label classiﬁcation domain classiﬁcation modules architecture respectively. architectures certainly designed optimal application kept ﬁxed control highlight relative performance various domain adaptation approaches generality solution. also include accuracies approximate labeling well applied target directly without transformation finally report results target only experiment target domain used training testing accuracies computed using twenty-fold cross validation. purpose design show comparison well model perform training test examples domain adaptation techniques used here especially danns hyperparameter sensitive. unsupervised approximation kinds values active area research outside scope work. obtain accuracy values algorithm times randomly-chosen hyperparameters sourcetarget pair reported best results. aside notice value ds)) discriminator’s conﬁdence transformed target samples indeed target samples tends correlate strongly accuracy dann exploiting observation hyperparameter tuning would interesting direction future research. neural networks implemented torch optimized using adam variant sentiment dataset provided product reviews amazon.com reviews come items either amazon’s book electronics kitchen departments data department contains samples; department separate domain. review represented term frequencies speciﬁed vocabulary. goal distinguish positive negative reviews. multiple sources available problem used pseudo-supervised rough labeling alm. however choice ﬁxed source-target pair implicitly uses information danns information single source. thus also present results reﬁnement dann labelings. that task dann generally outperforms sometimes large margin consistently achieves higher accuracy rough labeling used optimization regardless procedure used generate labeling. experiment supplied standard mnist digits well handwritten binary digits extracted usps parcels. datasets look similar classiﬁer trained still performs significantly worse other. digits usps domain pixels smaller dimensions mnist images zero-pad pixels side preprocessing. table percent accuracies sentiment classiﬁcation task. here represent book electronics kitchen appliances domains respectively. pairs letters correspond using domain source labeling books domain used target. almp alms almd show result performing using pseudo-supervised rough labeling reﬁnement reﬁnement dann respectively. average accuracies shown last row. dimensions. done training convolutional network lenet- architecture source data taking activations layer output layer representation source target results show even situation source target appear similar domain adaptation could used improve test accuracy. achieves favorable result produced techniques digit datasets. stanford ofﬁce dataset consists images different objects typically found around ofﬁce composed three domains pictures taken low-quality webcam high-quality dslr camera product images amazon.com table percent accuracies digit classiﬁcation task. ﬁrst results correspond treating usps images target second corresponds using mnist images target. average accuracies shown last row. table percent accuracies object classiﬁcation task. here represent amazon dslr webcam domains respectively. paired notation corresponds using dslr domain source labeling target amazon domain. average accuracies shown last row. images respectively. state-ofthe-art image classiﬁers typically require much data train available here network alexnet architecture trained imagenet dataset feature extractor. previous experiment pass image network activations last hidden layer lower-dimensional representation data. section reﬁnement rough labeling figure example images classes ofﬁce dataset. classes left bottom right ring binder printer laptop computer helmet paper notebook trash speaker desk chair projector desktop computer bookshelf mobile phone monitor stapler phone mouse keyboard cabinet letter tray. rows images webcam domain tend look overexposed washed-out balanced images third fourth rows generated dslr domain. last rows images correspond amazon product images generally feature white background. results challenging source-target pairs involve amazon domain unlike webcam dslr domains amazon images could contain single object object resemble typical items found around ofﬁce. even difﬁcult situations performs well better methods. note results somewhat worse reported original dann article. work pretrained alexnet reﬁned training transformation module rather used produce image embedding training transformation module scratch although explored work would expect similar boost performance using alterantive approach alm. magnetoencephalography machines magnetic ﬁelds measure brain’s electrical activity frequently used experiments neuroscience psychology. speciﬁc problem consider originally part online competition data come sixteen different people shown either pictures human faces non-faces brain activities recorded using meg. subject’s data comprises single domain objective classify instances held-out subject looking face subject looking non-face domain adaptation problem comes frequently brain-computer interface problems general dealing data. advancing classiﬁcation methods space could improve existing techniques areas like fmri analysis neuropsychiatric disease diagnosis prosthetics human-computer interaction locations neurons voxels often person-speciﬁc represent data according method discussed barachant maps time-series data dimensions. source labels used obtain representation held-out subject operates different space. figure shows example representation embedded dimensions. data least embedded space seems visual properties similar synthetic example discussed earlier. domains appear rotated translated respect other geometric overlap them. suggests might particularly well-suited problem. unlike previous experiments instead reporting accuracy technique performs domain adaptation ﬁxed source target average hypotheses resulting running technique available source ﬁxed target. average hypothesis described eqn. problem pseudosupervised rough labeling figure visualization data embedded dimensions using t-sne blue points positive source samples orange points negative source samples purple green points positive negative target samples respectively. data shows distinct clusters corresponding subject making clear domain adaptation problem. target domain labels shown clarity classiﬁer access labels target dataset. also compare stacked generalization literature often used solve problems hierarchical approach separate base classiﬁer trained available sources. predictions classiﬁers available source samples aggregated used k-dimensional feature space higher-level classiﬁer. again models mlps hidden layer composed nodes. table percent accuracies classiﬁcation task. presents results corresponding holding denoted subject target averaging hypotheses resulting performing domain adaptation available source. experiment also include results stacked generalization shown average accuracies shown last row. results found performance never worse rough labeling used optimization. several cases able improve slightly highlighting difﬁculty unsupervised domain adaptation task. still approach used conjunction domain adaptation technique. results outperformed state-of-the-art domain adaptation methods sometimes substantial margin. separately might useful apply algorithm iteratively performance continues improve. setting output current iteration would approximate labeling used next iteration alm. although address unsupervised domain adaptation article approach could straightforwardly modiﬁed accommodate supervised semi-supervised situations well. supervised domain adaptation could learn transformation simply setting true labels target. similarly semi-supervised situations available target labels could used supplement source data learning references arjovsky bottou towards principled methods intertraining generative adversarial networks. national conference learning representations volume blitzer dredze pereira biographies bollywood boom-boxes blenders domain adaptation sentiment classiﬁcation. association computational linguistics volume fernando habrard sebban tuytelaars unsupervised visual domain adaptation using subspace alignment. ieee international conference computer vision ganin ustinova ajakan germain larochelle laviolette marchand lempitsky domain-adversarial training neural networks. journal machine learning research ghifary bastiaan k.w. zhang balduzzi domain generalization object recognition multitask autoencoders. ieee international conference computer vision glorot bordes bengio domain adaptation large-scale sentiment classiﬁcation deep learning approach. proceedings international conference machine learning henson r.n. wakeman d.g. litvak friston k.j. parametric empirical bayesian framework eeg/meg inverse problem generative models multisubject multi-modal integration. frontiers human neuroscience lorbert ramadge p.j. guntupalli j.s. haxby j.v. regularized hyperalignment multi-set statistical signal processing workshop fmri data. ieee ieee goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems", "year": 2016}