{"title": "Deep Tracking on the Move: Learning to Track the World from a Moving  Vehicle using Recurrent Neural Networks", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abstract": "This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.", "text": "fig. typical training sequence. unoccluded occupancy predicted directly input grid data allowing objects tracked occlusion future frames. observed false positives therefore beneﬁcial. comparison visible ground truth shows model able capture dynamics moving vehicle accurately predicts track. advance work address problem tracking moving platform. extend neural network architecture proposed account egomotion sensor frame moves world frame demonstrate improved tracking accuracy compared previous work. demonstrate system laser point cloud data collected busy urban environment array static dynamic objects including cars buses pedestrians cyclists. model bypasses sequence handengineered steps typical traditional tracking approaches empirically shown successfully predict future evolution objects environment even completely occluded. abstract— paper presents end-to-end approach tracking static dynamic objects autonomous vehicle driving crowded urban environments. unlike traditional approaches tracking method learned end-to-end able directly predict full unoccluded occupancy grid laser input data. inspired recently presented deeptracking approach employ recurrent neural network capture temporal evolution state environment propose spatial transformer modules exploit estimates egomotion vehicle. results demonstrate ability track range objects including cars buses pedestrians cyclists occlusion moving stationary platforms using single learned model. experimental results demonstrate model also predict future states objects current inputs greater accuracy previous work. safe effective operation autonomous vehicle depends ability interpret surroundings track predict state environment time. many tracking systems employ multiple hand-engineered stages order represent state evolution world however tasks assigned robots become complex approach becomes increasingly infeasible. recent advances machine learning particularly deep neural networks demonstrated ability capture complex structure real world signiﬁcant improvements numerous computer vision natural language processing applications approaches would however typically require large taskspeciﬁc corpora annotated ground-truth labels master desired task. becomes difﬁcult learning model environment without access corresponding ground truth often case object tracking crowded urban environments. recent work took alternative approach presented end-to-end fully efﬁciently trainable framework learning model world dynamics building original deeptracking work considered speciﬁc problem learning track classify moving objects complex partially-observable realworld scenario viewed static sensor. here highlights related work section summarises problem deﬁnition deeptracking framework ﬁrst presented section describes models used perform tracking real-world scenarios considering static dynamic sensors. section presents empirical evaluation methods section concludes paper discusses future implications ﬁndings. number previous works explored problem model-free tracking objects environment autonomous vehicle typically approaches follow traditional paradigm multi-component pipeline separate components parametrise detect objects associate measurements existing tracks estimate state individually tracked object. multiple stages framework cumbersome introduces extra unnecessary failure modes tracking algorithm. recent work proposes replace multiple stages end-to-end learning framework known deeptracking leveraging neural networks directly learn mapping laser input unoccluded occupancy grid even relatively small amounts data. approach utilises architecture using gated recurrent units capture state evolution world sequence laser scan frames. another work considers recurrent networks takes different angle predicting occupancy dynamic environments. explicitly encode range velocities hidden layers recurrent network bayesian optimization learn network parameters update velocity estimation occupancy prediction. however model explicitly track objects occlusion. deeptracking shares similarities deep learning approaches predictive video modelling trained predict future state world based current input data. particularly important order successfully capture future location dynamic objects scene model must implicitly store position velocity object internal memory state. eliminates need design individual components hand model assumes static vehicle extending problem moving platform challenging task introduces array complex relative motions vehicle objects environment. deeptracking ignores motion vehicle model forced learn possible motion interactions vehicle environment vehicle stationary. moving platform leverage estimates egomotion proxy vehicle motion. scale rnn-based models proposed real-world application dynamic vehicles exploit spatial transformer modules allow internal memory state representations spatially transformed according estimated egomotion. problem address paper uncover true state world terms occupancy grid given sequence partially observed states environment computed sensor measurements. particular solve probability true unoccluded state world time given sequence partial observations previous time steps. formulation also used predict future states solving given empty input xt+t+n. element latent state update decoding step produce output modelled parts single neural network trained jointly. equations performed repeatedly building block recurrent neural network continuously updates belief state uses network memory predict makes suitable online stream ﬁltering sensor input. output ground-truth readily available case real-world scenarios network trained self-supervised fashion. made possible considering predicting movement objects occlusion time similar predicting future state yt+n lack input observation equates complete occlusion scene. observable ground truth available reduce problem predicting yt+n predicting directly observable input xt+n. training network predict equivalent computing backpropagating errors observable parts scene. refer reader details training procedure. input observation }×m×m represented pair discretised binary grids size parallel ground locally built around sensor. ﬁrst matrix encodes whether cell directly observable sensor time second matrix encodes whether cell observed free occupied static scenario dynamics scene viewed world frame coherent viewed local sensor frame. tracking moving vehicle however spatial update information within latent representation would additionally need account sensor’s egomotion affect position object relative sensor frame. although major drawback baseline deeptracking architecture compensated transforming memory state accordance egomotion. words static obstacle situated position {it− jt−} sensor frame moved position sensor frame time that network decoupling egomotion object motion introducing spatial transformer module hidden state original work introduced learnable module actively spatially transforming feature maps tasks classiﬁcation distorted datasets. however context tracking moving platform egomotion readily available used forward transform hidden feature maps centred sensor source frame time sensor destination frame time using transformation ttt− static dynamic cases network presented input sequence trained predict next input frames xt+t+n. binary cross-entropy loss calculated backpropagated visible part output achieved simply masking prediction xtvis multiplying resulting grid element-wise occupancy part grid xtocc. using loss fig. proposed network architecture features dilated convolutions gated recurrent units spatial transformer module outputs cell occupancy sensor’s surroundings. spatial transformer module utilised moving vehicle scenario. refer matrices xtvis xtocc visibility occupancy grids respectively. output wish obtain occlusion-free state world }m×m represented occupancy matrix similar input occupancy grid time step partially observed grid used input network computed laser scans tracing. cells measurement ends marked occupied cells sensor origin marked free cells beyond marked unobserved. input processed multi-layer network illustrated figure spatial transformer module utilised moving vehicle scenario. architecture originally proposed scaled dilated convolutions variant gated recurrent units allow simultaneous tracking different-sized objects cars pedestrians. layer time updated considering activations time layer time thus implementing recurrence. allows network extract remember information past prediction occluded objects future states world. additional static memory also utilised network able learn individual pixel-wise biases output every convolutional operation. output ﬁnal layer converted output cell occupancy simple convolutional decoder. explicited results section also experiment architectures decode entire hidden state output. static sensor scenario error backpropagated ground truth available visible part space. case moving sensor however additional constraint needs imposed account fact robot moves future frames discover space falls outside ﬁeld view current frame. given fact model falsely penalised failing accurately guess objects located within space input blanked out. similar nature static case input grid also represents frontier robot perceive understand scene unknown world outside ﬁeld view. address this apply additional mask training time cost computation error backpropagation represent predictable space future frames. accounting ﬁeld view drift crucial terms tracking performance corrects objective function otherwise skewed towards incredibly difﬁcult task predicting objects outside ﬁeld view. mask overlaid transparency ground truth comparison outputs figure indicates predictable free space shrinking future timesteps. section perform experimental validation baseline deeptracking stm-based variant proposed paper. stationary vehicle spatial transform necessary identical. static vehicle static case consider architecture three hidden layers feature maps each. computation hidden state consists convolutional ﬁlters applied three layers strides pixels receptive ﬁelds respectively. consider input ﬁeld view discretised cells translates input grid. hidden state consisting feature maps additional static memory contributes total parameters network. evaluation dataset consists minute collected stationary hokuyo umt-lx laser scanner positioned middle busy urban intersection. area features dense trafﬁc composed buses cars cyclists pedestrians results extensive amounts occlusion. consequently point time complete scene fully observable. dataset subsampled split minute training minute testing occupancy prediction. full model trained nvidia tesla convergence using unsupervised training procedure described section iii. training split minibatch sequences frames every minibatch network shown frames trained predict fig. scores network architectures attempting predict future occupancy scene second time horizon. measure computed threshold considering cell predicted occupied free. next frames leading sequences frame mini-batch. length sequence covers typical lengths occlusions observed tuned accordingly. look quantify gain performance achieved scaling original deeptracking network proposed architecture. compare number different architectures ranging original proposed model compare performance task predicting observable nearfuture scene occupancy given input sequence predicted output occupancy compared xt+nocc corresponds observed occupancy world time threshold used determine whether cell predicted occupied free measure computed frame. figure compares measures computed blacked future frames given frames past. model predictions decrease time would expected uncertainty state world increases prediction horizon. notable step change performance neural architectures compared state-of-the model-free tracking pipeline approach increase capacity original architecture feature maps obtain marginal performance increase. comparison replacing standard hidden unit state three layers units provides signiﬁcant improvement prediction ability. afﬁx signiﬁes decode last hidden unit output. incorporating dilated convolutions place traditional dense convolutions achieves comparable performance grudilconv similar output receptive ﬁeld model dilated convolutions additionally requires less computation nearly third fewer model parameters dense counterpart grudilconv further experiment grudilconv decodes full hidden state output. performance model maintained fig. example outputs produced system along selection activations hidden layers. highlighted colour-coded circles network able propagate assumed motion objects even complete occlusion. sample hidden layer activations shown highlight fact lower layers hidden units capture motion small slow moving objects pedestrians static background whereas higher level layer learns detect moving vehicles illustrated figure departure traditional architectures information different scales hidden units directly passed output. assist positively tasks semantic labelling scale information essential. lastly performance full model added static biases remains commensurate grudilconv grudilconv learned static bias values convey useful information static background layout. consider qualitative results better understand network learned also qualitatively analyse typical test sequence length seconds grudilconv along network output selected hidden state feature maps figure network able track pedestrians full occlusion unobserved hallucinated tracks represented blue output sequence. seen feature maps appear learned capture static background activations remain stationary sequence track pedestrians highlighted pink circles feature also captures motion pedestrians moving upwards left while interestingly unit seems activate appears right frame provides empirical support dilated convolutions allow model capture patterns increasing receptive ﬁelds hidden state units requiring fewer parameters traditional dense convolutional kernels. general observe information regarding objects scene captured hidden state moves spatially feature maps according motion object input. problematic extending tracking moving platform object motion vehicle motion coupled. address concern following section. compare baseline model grudilconv equivalent architecture incorporates spatial transformer module hidden belief state static case evaluate ability predict future frames given blacked input illustrate achieved occlusion-free tracking performance series examples selected test set. evaluation dataset collected minute period moving vehicle equipped velodyne hdle lasers resulting degree ﬁeld view. point clouds reduced scan considering range points within meters height ground. network trained mini-batches sequences frame rate alternating inputs shown inputs hidden. higher frequency better adapted moving vehicle case given input ﬁeld view vehicle mean velocity miles hour. longer occluded sequences would lead increased loss useful memory aforementioned drift ﬁeld view. finally performed split data training test overlap location trained model nvidia tesla convergence. architecture implemented torch uses spatial transformer gpu-implementation quantitative results figure represents measure computed baseline fig. positive contribution spatial transformer network’s ability correctly predict future occupancy scene time horizon. baseline surprisingly well attribute benign test vehicle mostly evolves constant velocity straight roads. baseline identical exception egomotion taken account. words additional mask applied cost computation backpropagation hidden states forward transformed next sensor frame. egomotion information might expect measure baseline poor. surprisingly however well illustrated figure suggest explanations this. firstly posit dataset relatively benign terms vehicle motion patterns. driving occurs straight roads relatively constant velocity baseline learned constant velocity model could used correct hidden state update. secondly measure less informative dataset dominated static objects walls buildings. such large fraction score attained learning vehicle’s motion merely capturing static scenes. nonetheless offers clear improvement baseline future frames. qualitative results qualitatively evaluate model show selection sequences model well poorly. dataset contain many occlusions long static setting vehicle driving urban environment look ability network predict happens occlusion maintaining blacking input every frames test time. figure shows compelling example accurately tracking dynamic static objects occlusion. particular model accurately predicts position static objects different sizes future frames blacked also able maintain tracks objects fully occluded. figure illustrates accurately predicts tracks moving vehicle occluded pedestrians whereas baseline model fails. latter predicted track vehicle gradually shifts ground truth complete failure frame fails separate fig. left right bottom example sequence dynamic static object tracking occlusion stm. output network evaluated visible ground truth. highlight several objects interest correctly tracked complete occlusion. fig. left right output sequence dynamic static object tracking occlusion predicted spatial transformer model baseline model show colour-coded comparison visible ground truth false negatives false positives blue true positives green. spatial transformer network consistently accurately tracks pedestrians occluded moving vehicle baseline model fails capture dynamics vehicle pedestrians. particularly visible last frame area around actual location objects showing false negative occupancy prediction baseline model. paper proposed approach perform object tracking mobile robot travelling crowded urban environments building previously proposed deeptracking framework crucially unlike classical techniques employ multi-stage pipeline approach learned end-to-end limited architectural choices. employing spatial transformer module model able exploit noisy estimates visual egomotion proxy true vehicle motion. experimental results demonstrate method performs favourably deeptracking terms accurately predicting future states show model capture location motion cars pedestrians cyclists buses even complete occlusion. authors would like gratefully acknowledge support work engineering physical sciences research council doctoral training programme programme grant dfr- well advanced research computing services university oxford providing access computing cluster. ondr´uˇska posner deep tracking seeing beyond seeing using recurrent neural networks thirtieth aaai conference artiﬁcial intelligence phoenix arizona february ondr´uˇska dequaire wang posner end-to-end tracking semantic segmentation using recurrent neural networks arxiv preprint arxiv. t.-d. aycard appenrodt online localization mapping moving object tracking dynamic outdoor environments intelligent vehicles symposium ieee june dahl deng acero context-dependent pretrained deep neural networks large-vocabulary speech recognition audio speech language processing ieee transactions vol. merri¨enboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation arxiv preprint arxiv. xingjian chen wang d.-y. yeung w.-k. wong w.-c. convolutional lstm network machine learning approach precipitation nowcasting advances neural information processing systems", "year": 2016}