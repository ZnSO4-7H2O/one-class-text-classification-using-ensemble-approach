{"title": "Correlational Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.", "text": "sarath chandar mitesh khapra hugo larochelle balaraman ravindran university montreal. apsarathchandargmail.com research india. mikhaprain.ibm.com university sherbrooke. hugo.larochelleusherbrooke.ca indian institute technology madras. ravicse.iitm.ac.in common representation learning wherein different descriptions data embedded common subspace receiving attention recently. popular paradigms canonical correlation analysis based approaches autoencoder based approaches. based approaches learn joint representation maximizing correlation views projected common subspace. based methods learn common representation minimizing error reconstructing views. approaches advantages disadvantages. example based approaches outperform based approaches task transfer learning scalable latter. work propose based approach called correlational neural network explicitly maximizes correlation among views projected common subspace. series experiments demonstrate proposed corrnet better mentioned approaches respect ability learn correlated common representations. further employ corrnet several cross language tasks show representations learned using corrnet perform better ones learned using state approaches. however views always available. example many movie clips audio video available subtitles available. recently interest learning common representation multiple views data useful several downstream applications views missing. consider four applications motivate importance learning common representations reconstruction missing view transfer learning matching corresponding items across views improving single view performance using data views. ﬁrst application learned common representations used train model reconstruct views data model would allow reconstruct subtitles even audio/video available. example transfer learning consider case profanity detector trained movie subtitles needs detect profanities movie clip video available. common representation available different views detectors/classiﬁers trained computing common representation relevant view test time common representation computed available view representation trained model prediction. third consider case items view need matched corresponding items another view project items views common subspace common representations corresponding items views correlated. match items across views based correlation projections. finally consider case interested learning word representations language. access translations words another language translations provide context disambiguation lead learning better word representations. words jointly learning representations word language translation language lead better word representations motivated importance common representation learning formally deﬁne task. consider data {zi}n views data point represented concatenation views work interested learning functions projections respectively common subspace given pair canonical correlation analysis commonly used tool learning common representations two-view data deﬁnition aims produce correlated common representations suffers drawbacks. first easily scalable large datasets. course approaches make scalable scalability comes cost performance. further since explicitly focus reconstruction reconstructing view might result quality reconstruction. finally cannot beneﬁt additional non-parallel single-view data. puts severe disadvantage several real world situations addition parallel two-view data abundant single view data available views. recently multimodal autoencoders proposed learn common representation views/modalities. idea train autoencoder perform kinds reconstruction. given view model learns self-reconstruction cross-reconstruction makes representations learnt predictive other. however noticed explicit learning signal encouraging share capacity common hidden layer views. words could develop units whose activation dominated single view. makes suitable transfer learning since views guaranteed projected common subspace. indeed veriﬁed results reported show performs better deep task transfer learning. approaches complementary characteristics. hand variants produce correlated common representations lack reconstruction capabilities. hand aims self-reconstruction cross-reconstruction guarantee correlated common representations. paper propose correlational neural network method learning common representations combines advantages approaches described above. main characteristics proposed method summarized follows unlike training objective used corrnet ensures common representations views correlated. particularly useful applications need match items view corresponding items view. corrnet trained using gradient descent based optimization methods. particularly dealing large high dimensional data stochastic gradient descent mini-batches. thus unlike easy scale corrnet. procedure used training corrnet easily modiﬁed beneﬁt additional single view data. makes corrnet useful many real world applications additional single view data available. approaches. particular evaluate ability self/cross reconstruct ability produce correlated common representations usefulness transfer learning. setup left right halves digit images views. next corrnet transfer learning task views data come different languages. speciﬁcally corrnet project parallel documents languages common subspace. employ common representations task cross language document classiﬁcation show perform better representations learned using state approaches. third corrnet task transliteration equivalence match name written using script language name written using script another language again demonstrate ability produce better correlated common representations corrnet performs better mae. finally employ corrnet bigram similarity task show jointly learning words representations languages leads better words representations. speciﬁcally representations learnt using corrnet help improve performance bigram similarity task. would like emphasize unlike models tested mostly scenarios demonstrate effectiveness corrnet different scenarios. remainder paper organized follows. section describe architecture corrnet outline training procedure learning parameters. section propose deep variant corrnet. section brieﬂy discuss related models learning common representations. section present experiments analyze characteristics corrnet compare kcca mae. section empirically compare deep corrnet deep methods. sections report results obtained using corrnet tasks cross language document classiﬁcation transliteration equivalence detection bigram similarity respectively. finally present concluding remarks section highlight possible future work. described earlier learn common representation views data that single view reconstructed common representation single view predicted representation another view like representations learned views correlated. ﬁrst goal achieved conventional autoencoder. ﬁrst second achieved together multimodal autoencoder guaranteed project views common subspace. propose variant autoencoders work views data explicitly trained achieve goals. following sub-sections describe model training procedure. start proposing neural network architecture contains three layers input layer hidden layer output layer. conventional single view autoencoder input output layers number units whereas hidden layer different number units. illustration consider twoview input discussions denotes concatenated vector size projection matrix projection matrix bias vector. function non-linear activation function example sigmoid tanh. output layer tries reconstruct hidden representation computing reconstruction matrix reconstruction matrix output bias vector. vector reconstruction function activation function. architecture illustrated figure parameters model next sub-section outline procedure learning parameters. reconstruction error scaling parameter scale fourth term respect remaining three terms mean vector hidden representations ﬁrst view mean vector hidden representations second view. dimensions input data take binary values cross-entropy reconstruction error otherwise squared error loss reconstruction error. simplicity shorthands note representations based single view. data point views means computing hidden representation using x-view. words equation choice notation deﬁning. certainly guarantied identical though training gain making various reconstruction terms. correlation term objective function calculated considering hidden representation random vector. words objective function decomposes follows. ﬁrst term usual autoencoder objective function helps learning meaningful hidden representations. second term ensures views predicted shared representation ﬁrst view alone. third term ensures views predicted shared representation second view alone. fourth term interacts objectives make sure hidden representations highly correlated encourage hidden units representation shared views. stochastic gradient descent optimal parameters. experiments used mini-batch sgd. fourth term objective function approximated based statistics minibatch. approximating second order statistics using minibatches training also used successfully batch normalization training method ioffe szegedy model four hyperparameters number units hidden layer mini-batch size learning rate. ﬁrst hyperparameter dependent speciﬁc task hand tuned using validation second hyperparameter ensure correlation term objective function range reconstruction errors. easy approximate based given data. third hyperparameter approximates correlation entire dataset larger minibatches preferred smaller mini-batches. ﬁnal hyperparameter learning rate common neural network based approaches. parameters learned corrnet compute representations views potentially generalize across views. speciﬁcally given data instance view available compute corresponding representation observed observed) data representation. practice often case abundant single view data comparatively little two-view data. example context text documents languages typically amount monolingual data available language much larger parallel data available given abundance single view data desirable exploit order improve learned representation. corrnet achieve this using single view data improve self-reconstruction error explained below. also access single view data training addition using explained before also suitably modifying objective function matches conventional autoencoder. speciﬁcally could minimize similarly experiments access three types data construct sets mini-batches sampling data respectively. feed mini-batches random order model perform gradient update based corresponding objective function. obvious extension corrnets allow multiple hidden layers. main motivation deep correlational neural networks better correlation views data might achievable non-linear representations. modify corrnet model ﬁrst input view connects hidden layer using weights bias similarly connect second view hidden layer using weights bias decoupled common hidden layer view would like point could followed procedure described chandar training deep corrnet. chandar learn deep representation view separately along shallow corrnet learn common representation. however feeding non-linear deep representations shallow corrnet makes harder train corrnet. also chose deep training procedure described ngiam since objective function used pre-training training different. speciﬁcally pretraining objective minimize self reconstruction error whereas training objective minimize self cross reconstruction error. contrast stacking procedure outlined above objectives training pre-training aligned. current training procedure deep corrnet similar greedy layerwise pretraining deep autoencoders. believe procedure faithful global training objective corrnet works well. strong empirical evidence superior methods described chandar ngiam less parallel data using method described chandar makes sense method advantages. leave detailed comparison different alternatives deep corrnet future work. canonical correlation analysis variants regularized de-facto approaches used learning common representation different views literature kernel another variant uses standard kernel trick pairs non-linear projections views. deep deep version also introduced issue easily scalable. even though several works scaling approximations hence lead decrease performance. also trivial extend multiple views. however recent work along line require complex computations. lastly conventional based models work parallel data. however real life situations parallel data costly compared single view data. inability leverage single view data acts drawback many real world applications. representation constrained model beneﬁt single view data multiview data. effectively uses weighted combination minimizing self-reconstruction errors maximizing correlation. corrnet contrast minimizes self cross reconstruction error maximizing correlation. rccca also considered linear version dccae proposed wang hsieh earliest neural network based model nonlinear cca. method uses three feedforward neural networks. ﬁrst neural network double-barreled architecture networks project views single unit projections maximally correlated. network ﬁrst trained maximize correlation. inverse mapping view learnt corresponding canonical covariate representation minimizing reconstruction error. clear differences neural model corrnet. first corrnet single neural network trained single objective function neural three networks trained different objective functions. second neural correlation maximization self-reconstruction whereas corrnet correlation maximization self-reconstruction cross-reconstruction time. multimodal autoencoder another neural network based approach. even though architecture similar corrnet clear differences training procedure used two. firstly aims minimize following three errors error reconstructing error reconstructing error reconstructing speciﬁcally unlike fourth term objective function objective function used contain term forces network learn correlated common representations. secondly difference manner terms considered training. unlike corrnet considers terms time. words given instance ﬁrst tries minimize updates parameters accordingly. tries minimize followed empirically observed training procedure considers three loss terms together performs better considers separately deep canonical correlation analysis recently proposed neural network approach cca. dcca employs deep networks view. model trained ﬁnal layer projections data views maximally correlated. dcca maximizes correlation whereas corrnet maximizes both correlation reconstruction ability. deep canonically correlated auto encoders extension dcca considers self reconstruction correlation. unlike corrnet consider cross-reconstruction. ability reconstruct view ability reconstruct view given ability learn correlated common representations views usefulness learned common representations transfer learning. used standard mnist handwritten digits image dataset experiments. data consists train images test images. image matrix pixels; pixel representing grayscale values. treated left half image view right half image another image. thus view contains dimensions. split train images sets. ﬁrst contains images used training. second contains images used validation tuning hyper-parameters four models described above. among four models listed above corrnets explicitly trained construct view well view. subsection consider models. table shows mean squared errors self cross reconstruction left half image used input. table suggests corrnet higher self reconstruction error alcross reconstruction error mae. unlike corrnet emphasis maximizing correlation common representations views. goal captured fourth term objective function obviously interferes goal self reconstruction. next sub-section embeddings learnt corrnet views better correlated even though self-reconstruction error sacriﬁced process. figure reconstruction right half image given left half. first block shows original images second block shows images right half reconstructed corrnet third block shows images right half reconstructed mae. mentioned above corrnet emphasize learning highly correlated representations views. show indeed case follow calculate total/sum correlation captured dimensions common representations learnt four models described above. training validation test sets used experiment described section results reported table next check whether indeed case change number dimensions. this varied number dimensions plotted correlation model models tuned hyperparameters used hyper-parameters dimensions. demonstrate transfer learning take task predicting digits half image. ﬁrst learn common representation views using images mnist training data. training instance take half image compute dimensional common representation using models described above. train classiﬁer using representation. test instance consider half image compute common representation. feed representation classiﬁer prediction. linear implementation provided classiﬁer experiments. models considered experiment representation learning done using train images best hyperparameters chosen using images validation set. chosen model report -fold cross validation accuracy using images available standard test mnist data. report accuracy settings left right right left single view corresponds classiﬁer trained tested view. upper bound performance transfer learning algorithm. again corrnet performs signiﬁcantly better models. verify holds even decrease data learning common representation images. results reported table show even less data corrnet perform betters models. face seem corrnet differ objective functions. speciﬁcally remove last correlation term objective function corrnet would become equivalent mae. verify this conducted experiments using corrnet without last term using train networks found performance almost similar. however advanced optimization technique like rmsprop corrnet starts performing better mae. results reported table experiment sheds light corrnet better mae. even though objective corrnet same tries solve stochastic adds noise. however corrnet performs better since actually working combined objective function stochastic version section analyze importance terms loss function. this training consider different loss functions contain different combinations terms. addition consider four loss terms analysis. this ﬁrst learn common representations using different loss functions listed ﬁrst column table repeated transfer learning experiments using common representations learned models. example sixth table shows results following loss function used learning common representations. even numbered table reports performance correlation term used addition terms immediately pair-wise comparison numbers even numbered immediately suggests correlation term loss function clearly produces representations lead better transfer learning. section evaluate performance deep extension corrnet. already compared previous section focus evaluation comparison dcca models trained using images mnist training dataset computed correlation transfer learning accuracy models. transfer learning linear implementation provided experiments -fold cross validation using images mnist test data. report results settings left right right left results summarized table table model-x-y means model units ﬁrst hidden layer units second hidden layer. example corrnet--- deep corrnet three hidden layers containing units respectively. third layer containing units used common representation. deep corrnets clearly perform better corresponding dcca. notice transfer learning tasks -layered corrnet performs better -layered corrnet correlation -layered corrnet better -layered corrnet. section learn bilingual word representations using corrnet representations task cross language document classiﬁcation. experiment three language pairs show approach achieves state-of-the-art performance. discuss bilingual word representations consider task learning word representations single language. consider language containing words vocabulary. represent sentence language using binary bag-of-words representation speciﬁcally dimension vocabulary word present sentence otherwise. wish learn k-dimensional vectorial representation word vocabulary training sentence bags-ofwords {xi}n propose achieve using corrnet works single view data effectively view corrnet encoding input bag-of-words columns corresponding words present followed non-linearity. thus view matrix whose columns vector representations word. let’s assume sentence bag-of-words source language associated bag-of-words sentence translated target language human expert. assuming training pairs we’d like learn representations languages aligned pairs translated words similar representations. corrnet allow achieve this. indeed effectively learn word representations informative words present sentences language also ensure representations’ space aligned language required cross-view reconstruction terms correlation term. note that since binary bags-of-words high-dimensional reconstructing binary bag-of-word slow. since later training millions sentences training individual sentence bag-of-words expensive. thus propose simple trick exploits bag-of-words structure input. assuming performing mini-batch training simply propose merge bags-of-words mini-batch single bag-of-words perform update based merged bag-of-words. resulting effect update efﬁcient stochastic gradient descent number updates training epoch divided minibatch size. we’ll experimental section trick produces good word representations sufﬁciently reducing training time. note that additionally could used stochastic approach proposed dauphin reconstructing binary bag-of-words representations documents improve efﬁciency training. importance sampling avoid reconstructing whole -dimensional input vector. learn language speciﬁc word representation matrices described above construct document representations using columns word vector representations. given document represent tf-idf weighted words’ representations wtf-idf language vtf-idf language tf-idf tf-idf weight vector document recent work considered problem learning bilingual representations words usually relied word-level alignments. klementiev propose train simultaneously neural network languages models along regularization term encourages pairs frequently aligned words similar word embeddings. thus regularization term requires ﬁrst obtain word-level alignments parallel corpora. similar approach different form regularizer neural network language models work speciﬁcally investigate whether method rely word-level alignments learn comparably useful multilingual embeddings context document classiﬁcation. looking generally neural networks learn multilingual representations words phrases mention work showed useful linear mapping separately trained monolingual skip-gram language models could learned. however rely speciﬁcation pairs words languages align. mikolov also propose method training neural network learn useful representations phrases context phrasebased translation model. case phrase-level alignments required. recently hermann blunsom proposed neural network architectures margin-based training objective that work rely word alignments. brieﬂy discuss work experiments section. tree based bilingual autoencoder similar objective function also proposed embeddings using task cross-language document classiﬁcation. followed closely setup used klementiev compare method word representations publicly available. follows. labeled data documents language available train classiﬁer however interested classifying documents different language test time. achieve this leverage bilingual corpora labeled document-level categories. bilingual corpora used learn document representations coherent languages hope thus successfully apply classiﬁer trained document representations language directly document representations language following setup performed experiments data sets language pairs english/german english/french english/spanish contains roughly million parallel sentences. considered language pairs. used pre-processing used klementiev tokenized sentences using nltk removed punctuations lowercased words. remove stopwords. labeled document classiﬁcation data sets extracted sections reuters rcv/rcv corpora pairs considered experiments. following klementiev consider documents assigned exactly level categories topic hierarchy documents also pre-processed using similar procedure used europarl corpus. used vocabularies used klementiev models trained epochs using data described earlier. used mini-batch stochastic gradient descent. results word embeddings size klementiev further speed training corrnet merged adjacent sentence pairs single training instance described earlier. language pairs hyperparameters tuned task using training/validation split using performance validation averaged perceptron trained smaller training portion compare models following approaches klementiev model uses word embeddings learned multitask neural network language model regularization term encourages pairs frequently aligned words similar word embeddings. embeddings document representations computed described section here test documents translated language training documents using standard phrase-based system moses trained using en/de language pairs directly report results klementiev pairs used embeddings available online performed classiﬁcation experiment ourselves. similarly generated baseline ourselves. table summarizes results. obtained using training examples. report results directions i.e. language vice versa. best performing method pairs except corrnet. particular corrnet often outperforms approach klementiev large margin. last table also include results recent work hermann blunsom proposed neural network architectures learning word document representations using sentence-aligned data only. instead autoencoder paradigm propose margin-based objective aims make representation aligned sentences closer non-aligned sentences. trained embeddings publicly available report results en/de classiﬁcation experiments representations size trained en/de sentence pairs. best model setting reaches accuracies respectively tasks. clear advantage model unlike model additional monolingual data. indeed train corrnet en/de sentence pairs plus monolingual documents accuracies still improving best model. monolingual data corrnet’s performance worse still competitive finally without constraining using additional french data best results hermann blunsom later being knowledge current state-of-the-art summarized figure figure observe corrnet clearly outperforms models almost data sizes. importantly performs remarkably well data sizes suggesting learns meaningful embeddings though method still beneﬁt labeled data table also illustrates properties captured within across languages en/de pair. english words words closest word representations shown english german. observe words form translation pair close also close words within language syntactically/semantically similar well. excellent performance corrnet suggests merging several sentences single bags-of-words still yield good word embeddings. words need rely word-level alignments exact sentence-level alignment also essential reach good performances. experimented merging adjacent sentences single bag-of-words. results shown table suggest merging several sentences single bags-of-words necessarily impact quality word embeddings. thus conﬁrm exact sentence-level alignment essential reach good performances well. table cross-lingual classiﬁcation accuracy different pairs languages merging bag-of-words different numbers sentences. results based labeled examples. previous section showed application corrnet cross language learning setup. addition cross language learning corrnet also used matching equivalent items across views. case study consider task determining transliteration equivalence named entities wherein given word written using script language word written using script language goal determine whether transliterations other. several approaches proposed task related work approach uses determining transliteration equivalence. condider english-hindi language pair transliteration equivalence needs determined. learning common representations used approximately transliteration pairs news english-hindi training represent hindi word bigram characters. forms ﬁrst view similarly represent english word bigram characters. forms second view pair serves training instance corrnet. testing consider standard news transliteration mining test test contains approximately wikipedia english hindi title pairs. original task deﬁnition follows. given english title containing words corresponding hindi title containing words identify pairs form transliteration pair. speciﬁcally title pair consider word pairs identify correct transliteration pairs. test contains word pairs transliteration pairs. every word pair obtain dimensional common representation using trained corrnet. calculate correlation representations correlation threshold mark word pair equivalent. threshold tuned using additional pairs provided training data news transliteration mining task. seen table corrnet clearly performs better methods. note achieve state performance task compare quality shared representations learned using different methods considered paper. section consider dataset/application compare performance corrnet state methods. speciﬁcally task hand calculate similarity score bigram pairs english based representations. bigram representations calculated word representations learnt using english german word pairs. motivation german word provides context disambiguating english word hence leads better word representations. task already considered mitchell lapata wang follow similar setup wang dataset. english german words ﬁrst represented using -dimensional monolingual word vectors trained latent semantic indexing monolingual news corpora. used english-german monolingual word vector pairs common representation learning. pair consisting english german word thus acts training instance corrnet. common representation learnt project english words common subspace word embeddings computing similarity bigram pairs english. bigram similarity dataset initially used mitchell lapata consider adjective-noun verb-object subsets bigram similarity dataset. tuning test splits size subset. vector representation bigram computed simply adding vector representations words bigram. following previous work compute cosine similarity vectors bigram pair order pairs similarity report spearman’s correlation model’s ranking human rankings. following wang dimensionality vectors hyperparameters tuned using tuning data. results reported table compare corrnet different methods proposed wang corrnet performs better previous state-of-the-art average score. best results obtained using corrnet--. experiment suggests apart multiview applications transfer learning reconstructing missing view matching items across views corrnet also employed exploit multiview data improve performance single view task paper proposed correlational neural networks method learning common representations views data. proposed model capability reconstruct view ensures common representations learned views aligned correlated. training procedure also scalable. further model beneﬁt additional single view data often available many real world applications. employ common representations learned using corrnet downstream applications viz. cross language document classiﬁcation transliteration equivalence detection. tasks show representations learned using corrnet perform better methods. believe possible extend corrnet multiple views. could useful applications varying amounts data available different views. example typically would easy parallel data english/german english/hindi harder parallel data german/hindi. data languages projected common subspace english could pivot language facilitate cross language learning hindi german. intend investigate direction future work. would like thank alexander klementiev ivan titov providing code classiﬁer data indices cross language document classiﬁcation task. would like thank janarthanan rajendran valuable discussions.", "year": 2015}