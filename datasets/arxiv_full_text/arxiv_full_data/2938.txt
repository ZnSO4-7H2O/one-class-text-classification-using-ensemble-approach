{"title": "Learning to cluster in order to transfer across domains and tasks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not. This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets. Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.", "text": "paper introduces novel method perform transfer learning across domains tasks formulating problem learning cluster. insight that addition features transfer similarity information sufﬁcient learn similarity function clustering network perform domain adaptation cross-task transfer learning. begin reducing categorical information pairwise constraints considers whether instances belong class similarity category-agnostic learned data source domain using similarity network. present novel approaches performing transfer learning using similarity function. first unsupervised domain adaptation design loss function regularize classiﬁcation constrained clustering loss hence learning clustering network transferred similarity metric generating training inputs. second cross-task learning propose framework reconstruct estimate number semantic clusters using clustering network. since similarity network noisy robust clustering algorithm show formulation robust alternative constrained unconstrained clustering approaches. using method ﬁrst show state results challenging cross-task problem applied omniglot imagenet. results show reconstruct semantic clusters high accuracy. evaluate performance cross-domain transfer using images ofﬁce- svhn-mnist tasks present accuracy datasets. approach doesn’t explicitly deal domain discrepancy. combine domain adaptation loss shows improvement. supervised learning made signiﬁcant strides past decade substantial advancements arising deep neural networks. however large part success come existence extensive labeled datasets. many situations practical obtain data amount effort required task data distributions change dynamically. deal situations ﬁelds transfer learning domain adaptation explored transfer learned knowledge across tasks domains. many approaches focused cases distributions features labels changed task cross-task transfer learning strategies hand widely adopted especially computer vision community features learned deep neural network large classiﬁcation task applied wide variety tasks prior cross-task transfer learning works however require labeled target data learn classiﬁers task. labels target data absent little choice apply unsupervised approaches clustering target data pre-trained feature representations. paper focus question transferred support cross-domain cross-task transfer learning. address learned similarity function fundamental component clustering. clustering realized using neural idea formulate clustering objective learnable term proposed work similarity prediction function. proposed objective function easily combined deep neural networks optimized end-to-end. features clustering optimized jointly hence taking advantage side information robust way. using method show unsupervised learning beneﬁt learning performed distinct task demonstrate ﬂexibility combining classiﬁcation loss domain discrepancy loss. summary make several contributions. first propose predictive pairwise similarity knowledge transferred formulate learnable objective function utilize pairwise information fashion similar constrained clustering. provide methodologies deploy objective function cross-task cross-domain scenarios deep neural networks. experimental results cross-task learning omniglot imagenet show achieve state clustering results predicted similarities. standard domain adaptation benchmark ofﬁce- dataset demonstrate improvements state-of-art even performing explicit domain adaptation improvements finally another domain adaptation task svhn-to-mnist approach using omniglot auxiliary dataset achieves performance large margin. transfer learning transfer learning aims leverage knowledge source domain help learn target domain focusing performance target domain. type transferred knowledge includes training instances features model parameters relational knowledge pairwise similarity meta-knowledge propose transfer falls last types. similarity prediction function neural network learned parameters output simpliﬁed form relational knowledge i.e. considering pairwise semantic similarity. cross-task transfer learning features learned trained imagenet classiﬁcation boosted performance variety vision tasks supervised setting. example classiﬁcation tasks object detection semantic segmentation image captioning translated learning unsupervised setting similar ours focuses transferring features across tasks. work explores learning could beneﬁt transferring pairwise similarity unsupervised setting. cross-domain transfer learning also known domain adaptation recently large body work dealing domain shift image datasets minimizing domain discrepancy address problem complementary transfers extra information auxiliary dataset show larger performance boost gains using additional domain discrepancy loss. constrained clustering constrained clustering algorithms categorized utilize constraints ﬁrst work constraints learn distance metric. example itml skms skkm sklr group approaches closely relates metric learning needs clustering algorithm k-means separate stage obtain cluster assignments. second group work constraints formulate clustering loss. example cosc third group uses constraints metric learning clustering objective mpckmeans cecm fourth group constraints all. generic clustering algorithms k-means lpnmf belong category. long list associated works summarized survey papers e.g. davidson basu figure overview transfer scheme learnable clustering objective pairwise similarity components approach described section dashed rectangles light gray arrows available cross-domain transfer. details described section dinler tural proposed clustering strategy belongs third group. constrained clustering methods applied semi-supervised setting groundtruth constraints sparsely available. unsupervised setting ground-truth unavailable predicted constraints densely available. include four groups algorithms comparison show advantages third group. deﬁne transfer learning problem addressed work follow notations used yang goal transfer knowledge source data data instances corresponding categorical labels target data noted learning unsupervised since unknown. scenario divided cases. {yt} {ys} means categories hence transfer across tasks. second case {yt} {ys} domain shift. words marginal probability distributions input data different i.e. latter cross-domain learning problem also called transductive learning. domain adaptation approaches gained signiﬁcant attention recently belong second scenario. align common benchmarks evaluating transfer learning performance notion auxiliary dataset split source data present cross-domain transfer scheme {yt}. auxiliary dataset large amount labeled data potentially categories well contain categories cross task scenario unlabeled included cross-domain transfer involves following sections notations describe transfer learning tasks detail. figure illustrates approach relates tasks. transfer across tasks target task different categories cannot directly transfer classiﬁer source target labeled target data ﬁne-tuning transferred features. propose ﬁrst reduce categorization problem surrogate same-task problem. directly apply transductive transfer learning transformed task. cluster structure target data reconstructed using predictions transformed task. ﬁgure illustration. source involves labeled auxiliary dataset unlabeled target dataset target must inferred. scenario categories {ya} {yt} unknown. ﬁrst transform problem categorization pairwise similarity prediction problem. words {}∀ij specify transformation function yij. figure concept reconstructing clusters unseen categories. proposed approach follows arrows counter-clockwise direction converts cross-task transfer learning cross-domain transfer learning. colors dots represent data different categories. hollow circle cross symbol represent similar dissimilar data pairs. function cluster reconstruction components diagram. applying obtain ytij. last step infer {ytij}∀ij solved using constrained clustering algorithms. note since actual unknown algorithm infers indices categories could arbitrary order. resulting clusters expected contain coherent semantic categories. transfer across domains problem setting consider unsupervised domain adaptation. following standard evaluation procedure labeled datasets imagenet domain ofﬁce- dataset. unlabeled another domain ofﬁce-. goal enhance classiﬁcation performance utilizing together. approach design learning objective predicted pairwise similarities inspired constrained clustering involves using pairwise information loss function. pairwise information called must-link/cannot-link constraints similar/dissimilar pairs note information binarized zero similar dissimilar pairs accordingly. although many constrained clustering algorithms developed scalable respect number pairwise relationships. further none easily integrated deep neural networks. inspired work kira construct contrastive loss clustering probability outputs softmax classiﬁer. however output node ﬁxed category instead output node represents probabilistic assignment data point cluster. assignment output nodes clusters formed stochastically optimization guided pairwise similarity. similar pair output distribution similar vice-versa. speciﬁcally pair-wise kl-divergence evaluate distance cluster assignment distributions data instances predicted similarity construct contrastive loss. given pair data corresponding output distributions deﬁned neural network. cost similar pair described cost symmetric w.r.t. alternatively assumed constant. kl-divergence factor dkl||q) becomes unary function whose gradient simply ∂dkl||q)/∂q. comes pair dissimilar output distributions expected different deﬁned hinge-loss function refer equation lco. function learnable part utilizes prior knowledge trained auxiliary dataset optimizing lco. particular characteristics clustering criterion worth mentioning need deﬁne cluster centers. predeﬁned metric applied feature representation. instead divergence calculated directly cluster assignment; therefore feature representation clustering jointly optimized using back-propagation deep neural networks. although restriction constructed choose deep convolution neural networks efﬁciency vision tasks. design network architecture inspired zagoruyko komodakis predict image patch similarity predict image-level semantic similarity. however siamese architecture used zagoruyko komodakis efﬁcient training inference especially pairwise information dense. therefore instead using siamese architecture keep single column backbone pair-enumeration layer feature extraction network. pair-enumeration layer enumerates pairs feature vectors within mini-batch concatenates features. suppose input layer mini-batch size feature dimension output pair-enumeration layer architecture illustrated ﬁgure hidden fully connected layer enumeration layer binary classiﬁer end. standard cross-entropy loss train end-to-end. supervision training obtained converting ground-truth category labels binary similarity i.e. samples class label similar otherwise dissimilar. inference also end-to-end outputs predictions among similarity pairs mini-batch. output probability means similar. binarize obtain discrete similarity predictions. following sections simpliﬁed notation pairwise similarity prediction network learned works static function experiments. since pairwise prediction mini-batch densely obtained efﬁciently utilize pair-wise information without forwarding data multiple times also combine pairenumeration layer described section equation case outputs softmax figure constrained clustering network transfer learning across tasks. input unlabeled target data cluster assignment block contains fully connected layers number output nodes equal described section backbone network plus cluster assignment block. optimize full pipeline diagram used. optimization uses another forward propagation obtain ﬁnal cluster assignment. figure network transfer learning across domains. input architecture direct extension ccn. ccn+ represent mandatory parts implements ccn++ includes domain adaptation method standalone deep neural networks reconstruct semantic clusters transfer learning across tasks call architecture constrained clustering network cross-domain case additionally labeled source data. enables using classiﬁcation loss overall training procedure similar previous domain adaptation approaches source target data mixed mini-batch different losses applied. denote source domain images mini-batch target domain images dense pairs loss function cross-domain transfer mini-batch formulated lcluster lcls share outputs network although lcluster force mapping clusters categories lcls does. therefore learned classiﬁer applied target data directly picks maximum probability predicted class. note loss function term explicitly match feature distribution source target; merely transfers knowledge form constraints regularize learning classiﬁer. also requirement architecture utilize hidden layers. therefore approach large ﬂexibility combined domain adaptation strategies. figure illustrates architectures ccn+ ccn++ used cross-domain experiments. section contains evaluations four image datasets covers cross-task crossdomain schemes. details described below differences experimental settings illustrated appendix omniglot dataset contains different handwritten characters images drawn different people. characters different alphabets separated background sets omniglotbg evaluation sets omnigloteval author. omniglotbg auxiliary dataset omnigloteval target data total number characters omniglotbg regarded number categories available learn semantic similarity. goal cluster omnigloteval reconstruct semantic categories without ever labels. function backbone neural network four convolution layers followed max-pooling stride hidden layers followed batch normalization rectiﬁed linear unit. prepare inputs training images omniglotbg resized normalized zero mean unit standard deviation. mini-batch size sampled random characters make sure amount similar pairs reasonable. pair enumeration pairs subject loss function two-class cross entropy loss. ground-truth similarity obtained converting categorical labels. loss optimized stochastic gradient descent part trained supervised manner section. constrained clustering network used reconstruct semantic clusters target dataset using outputs network four convolution layers followed max-pooling stride hidden fully-connected layer cluster assignment layer also fully connected. number output nodes last layer equal number potential clusters target data. output distribution enumerated pairs sending lco. network randomly initialized trained end-to-end optimized stochastic gradient descent randomly sampled images mini-batch. note function used ﬁxed optimization. input data preparation above except data omnigloteval. speciﬁcally training mini-batch given ccn. dense pairwise similarity predictions sent fully utilized. hyper-parameter experiments. omnigloteval contains alphabets used standalone dataset. target datasets contain varied number characters. therefore evaluate reconstruction performance varied number ground-truth clusters. tested situations number character alphabet known unknown. known target number clusters clustering algorithm equal true number characters. unknown common practice large number data different categories forced cluster. experiment merely much larger largest dataset constrained clustering algorithms used reconstruct semantic clusters problem. since mis-predictions robustness noise important factor. include four types constrained clustering algorithms introduced section baselines. provide full pairwise constraints algorithms including ours. words given alphabet characters contains images predicted similarities full pairwise similarities presented algorithms random order empirically found noticeable effect results. pick baseline approaches based code availability scalability concerning number pairwise constraints. therefore shown results k-means lpnmf itml skkm sklr skms mpck-means baselines. default parameters algorithm provided table unsupervised cross-task transfer omniglotbg omnigloteval. performance averaged across alphabets letters. without brackets number clusters equal ground-truth. means algorithms characteristics algorithm utilizes pairwise constraints marked \"constraints column metric stands metric learning feature representation. evaluation uses clustering metrics. ﬁrst normalized-mutual information widely used clustering. second clustering accuracy metric ﬁrst ﬁnds one-to-one matching predicted clusters ground-truth labels calculates classiﬁcation accuracy based mapping. data outside matched clusters regarded mis-predictions. high algorithm generate coherent clusters cluster includes data category; otherwise score drops quickly. therefore provides better discrimination evaluate whether semantic clusters reconstructed well. report average performance alphabets table approach achieved performance metrics. demonstrates strong robustness challenging scenario unknown achieved average accuracy. compared known relatively small drop. compared second best algorithm outperforms large gap. classical approach mpck-means works surprisingly well number clusters known performance dropped dramatically performance breakdown individual alphabets achieved clustering accuracy church slavonic cyrillic characters therefore results show feasibility reconstructing semantic clusters using noisy similarity predictions. semantic similarity? experiments table show clear trend utilizing pairwise constraints jointly metric learning minimizing clustering loss achieves best performance including mpck-means ccn. case unknown number clusters algorithms constraints optimize clustering loss better robustness example ccn. group constraints metric learning signiﬁcantly outperform group however performance still behind ccn. results conﬁrm importance jointly optimizing metric clustering. robustness noisy similarity prediction factor enable cross-task transfer framework. best knowledge table ﬁrst comprehensive robustness comparisons using predicted constraints learned real data instead converting ground-truth labels. accuracy experiment shown appendix table demonstrates reasonable performance matching-net binarizing prediction probability similar pair precision similar pair recall dissimilar pair precision dissimilar pair recall among characters accordingly. binarized predictions better uniform random guess still noisy. therefore challenging constrained clustering. visualization robustness range provided appendix shows robustness related density pairs involved mini-batch. hypothesize optimization gradients wrongly predicted pairs canceled correctly predicted pairs. therefore overall gradient still moves solution towards better clustering result. predict inferring number clusters hard problem pairwise similarity information becomes feasible. evaluation compute difference number dominant clusters true number categories dataset. naive deﬁnition number clusters size larger iverson bracket size cluster example alphabet images average difference calculated dataset smaller adif adif indicates better estimate achieves score compare baseline approach skms require given supports pipeline estimate automatically skms gets furthermore datasets ccn’s prediction difference smaller equal shows feasibility estimating predicted similarity. demonstrate scalability approach applied scheme imagenet dataset. -class dataset separated -class -class subsets random split vinyals imagen classes randomly sampled imagen difference section resnet- weights backbone pre-trained imagen since number pairs high feasible feed constrained clustering algorithms compare k-means lpnmf output average pooling layer resnet- input clustering algorithms. gives performance average known number clusters unknown outperforms second large margin. full comparison appendix table performance provided appendix table ofﬁce- images categories ofﬁce objects. images obtained three domains amazon dslr webcam dataset standard benchmark evaluating domain adaptation performance computer vision. experiment combinations target report average accuracy based random experiments setting. function learns semantic similarity function auxiliary dataset imagenet categories. backbone network resnet- weights initialized imagenet classiﬁcation. training process section except images resized follow standard protocols using deep neural networks unsupervised domain adaptation. backbone network ccn+ pre-trained imagenet. appendix ﬁgure illustrates scheme. training source data target data used. mini-batch constructed labeled samples source unlabeled samples target. since target dataset labels could randomly sampled crucial sufﬁcient mini-batch size ensure similar pairs sampled. loss function used approach equation optimized stochastic gradient descent. ccn+/++ dann resnet backbone implemented torch. code original author jan. dann -dimension bottleneck feature layer. results summarized table approach demonstrates strong performance boost unsupervised cross-domain transfer problem. reaches average accuracy gained points source-only baseline. although approach merely transfers information auxiliary dataset outperforms strong approach dann state-of-the-art combining dann performance boosted. indicates helps mitigate transfer problem certain orthogonal minimizing domain discrepancy. observe trend using deeper backbone network i.e. resnet-. case average accuracy achieved source-only ccn+ ccn++ respectively though used exactly indicates information carried similarity predictions equivalent transferring features deeper networks. discussions appendix performance provided appendix table show although prediction precision similar pairs approach still beneﬁts dense similarity predictions. also evaluated ccn+ another widely compared scenario uses color street view house numbers images gray-scale hand-written digits learn omniglotbg train networks section scratch. experimental setting similar sener achieve performance accuracy. performance gain source-only approach wins large margin compared full comparison presented appendix table paper demonstrated usefulness transferring information form pairwise similarity predictions. information transferred function utilized loss formulation inspired constrained clustering implemented robustly within neural network jointly optimize features clustering outputs based noisy predictions. experiments cross-task cross-domain transfer learning show strong beneﬁts using semantic similarity predictions resulting state results across several datasets. true even without explicit domain adaptation cross-domain task domain discrepancy loss beneﬁts increase further. factors determine performance proposed framework. ﬁrst robustness constrained clustering second performance similarity prediction function. show robustness empirically explore situations learning similarity function harder. example cases arise small number categories source large domain discrepancy source target. idea deal situation learning domain adaptation strategies. leave aspects future work. saket anand sushil mittal oncel tuzel peter meer. semi-supervised kernel mean shift clustering. ieee transactions pattern analysis machine intelligence mikhail bilenko sugato basu raymond mooney. integrating constraints metric learning semi-supervised clustering. proceedings twenty-ﬁrst international conference machine learning konstantinos bousmalis nathan silberman david dohan dumitru erhan dilip krishnan. unsupervised pixel-level domain adaptation generative adversarial networks. ieee conference computer vision pattern recognition wenyuan yuqiang chen rong qiang yang yong translated learning transfer learning across different feature spaces. advances neural information processing systems jason davis brian kulis prateek jain suvrit inderjit dhillon. information-theoretic metric learning. proceedings international conference machine learning jeff donahue yangqing oriol vinyals judy hoffman ning zhang eric tzeng trevor darrell. decaf deep convolutional activation feature generic visual recognition. icml volume yaroslav ganin evgeniya ustinova hana ajakan pascal germain hugo larochelle françois laviolette mario marchand victor lempitsky. domain-adversarial training neural networks. journal machine learning research ross girshick jeff donahue trevor darrell jitendra malik. rich feature hierarchies accurate object detection semantic segmentation. computer vision pattern recognition kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition mingsheng long jianmin wang michael jordan. learning transferable features deep adaptation networks. proceedings international conference machine learning icml lille france july mingsheng long jianmin wang michael jordan. deep transfer learning joint adaptation networks. proceedings international conference machine learning icml sydney australia august james macqueen methods classiﬁcation analysis multivariate observations. proceedings ﬁfth berkeley symposium mathematical statistics probability volume oakland usa. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision kuniaki saito yoshitaka ushiku tatsuya harada. asymmetric tri-training unsupervised domain adaptation. proceedings international conference machine learning icml sydney australia august ozan sener hyun song ashutosh saxena silvio savarese. learning transferrable representations unsupervised domain adaptation. advances neural information processing systems oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image caption generator. proceedings ieee conference computer vision pattern recognition eric xing michael jordan stuart russell andrew distance metric learning application clustering side-information. advances neural information processing systems yang dong feiping shuicheng yueting zhuang. image clustering using local discriminant models global integration. ieee transactions image processing sergey zagoruyko nikos komodakis. learning compare image patches convolutional neural networks. proceedings ieee conference computer vision pattern recognition werner zellinger thomas grubinger edwin lughofer thomas natschläger susanne samingerplatz. central moment discrepancy domain-invariant representation learning. iclr table list datasets involved experiment. learns similarity function dataset ccn* optimized dataset t∪s’ ccn* means cross-task transfer ccn+/++ cross-domain transfer. rows network initialization indicate whether network weights initialized training classiﬁcation task speciﬁed dataset. weights randomly initialized speciﬁed. table list loss functions used training networks. similarity prediction function uses cross-entropy loss classes training constrained clustering network involves combinations learnable clustering objective cross-entropy domain adaptation loss figure diagram depicting cross-task transfer experiment. experiments follow omniglotbg omnigloteval imagen imagen exclusive classes source target domain. imagenet experiment backbone network initialized weights pre-trained imagen figure comparison domain adaptation approaches transferring semantic similarity auxiliary data minimizing domain discrepancy. diagram uses ofﬁce- benchmark scenario transferring. table breakdown results alphabet omnigloteval. unsupervised cross task transfer experiment described section table shows clustering accuracy simulate situation unknown number clusters. alphabet angelic atemayar_qelisayer atlantean aurek_besh avesta ge_ez glagolitic gurmukhi kannada keble malayalam manipuri mongolian old_church_slavonic_cyrillic oriya sylheti syriac_serto tengwar tibetan ulog average alphabet angelic atemayar_qelisayer atlantean aurek_besh avesta ge_ez glagolitic gurmukhi kannada keble malayalam manipuri mongolian old_church_slavonic_cyrillic oriya sylheti syriac_serto tengwar tibetan ulog table performance similarity prediction function used section leverage n-way test commonly used one-shot learning evaluation. similarity learned omniglotbg n-way test omnigloteval mnist. experimental settings follow vinyals probability output used nearest exemplar n-way test. table unsupervised cross-task transfer learning imagenet. values average three random subsets imagen subset classes. \"acc\" \"acc sets methods features resnet- pre-trained imagen classiﬁcation. table performance similarity prediction function applied three subsets imagen subset contains random classes imagen predictions binarized calculate precision recall. random* expected performance classes uniformly distributed make uniform random guess similarity. approximation since number images class roughly equal imagenet. sampled pairs collect statistics. sampling pairs noticeable change values. experiment shows simply using deeper pre-trained networks produces signiﬁcant performance boost. speciﬁcally using resnet- increases performance points alexnet surpasses points gained using state-of-the-art domain adaptation algorithms regard pre-training deeper networks transferring information approach similar aspect since transfer information auxiliary dataset. case memory limitations precluded application models multi-gpu implementations problem area future work. table performance unsupervised transfer across domains ofﬁce- dataset. backbone networks comparison different numbers convolutional layers. alexnet layers resnets layers. abbreviation source-only simply trains directly applies classiﬁer ﬁrst rows directly copied long features learned deeper networks generalize better across domains. table unsupervised transferring across domains without pre-trained backbone network weights. setup similar sener ganin therefore similar source-only performance. quickly explore large combination factors affect clustering small dataset small network convolution layers followed fully connected layers. mnist dataset dataset handwritten digits contains training testing images size training used section pixels normalized zero mean unit standard deviation networks directly. networks randomly initialized clustering training times combination factors; show best ﬁnal results usual random restart regime. mini-batch size thus pairs presented mini-batch using full density mini-batches epoch optimization proceeded epochs. clustering loss minimized stochastic gradient descent learning rate momentum predicted cluster assigned forwarding samples clustering networks. best result runs reported. simulate different performance similarity prediction label pairs ﬂipped according designated recall. example simulate recall similar pair ground truth similar pair mini-batch ﬂipped. precision similar/dissimilar pairs function recall type pairs thus controlling recall sufﬁcient evaluation. recalls similar dissimilar pairs gradually reduced zero intervals resulting performance w.r.t different values recall density number clusters visualized figure bright color means high score desired. larger bright region robust clustering noise similarity prediction. score shows almost trend thus shown here. similarity prediction affect clustering? looking top-left heat ﬁgure clusters observed score robust similar pair recall even lower recall dissimilar pairs effect recall divided value clustering performance robust noise dissimilar pairs recall greater however completely fail recall similar pairs clustering works wide range recalls recall dissimilar pairs high. practical terms robustness recall similar pairs desirable much easier predict dissimilar pairs similar pairs real scenarios. dataset categories e.g. cifar- easily recall dissimilar pairs purely random guess number classes known recall similar pairs density constraints affect clustering? argue density pairwise relationships factor improving robustness clustering. density means every pair mini-batch utilized clustering loss. density means possible constraints used. could regard higher density better utilization pairwise information mini-batch thus learning instances contribute gradients once. consider scenario sample associated true similar pairs false similar pairs. case gradients introduced false similar pairs higher chance overridden true similar pairs within mini-batch thus loss converge faster less figure clustering performance different pairwise density number clusters. bright color means score close black corresponds density deﬁned ratio compared total number pair-wise combinations mini-batch. number clusters deﬁnes ﬁnal softmax output dimensionality. sub-ﬁgure show scores change w.r.t. similar pair recall dissimiliar pair recall. implementation enumerating full pairwise relationships introduces negligible overhead computation time using gpu. although overhead memory consumption limited vector predicted distributions enumerated calculating clustering loss. effect varying number clusters mnist experiments number categories augment softmax output number rows ﬁgure show even number output categories signiﬁcant larger number true object categories e.g. clustering performance score degrades slightly.", "year": 2017}