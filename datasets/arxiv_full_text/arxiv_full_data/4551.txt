{"title": "Feature Construction for Relational Sequence Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We tackle the problem of multi-class relational sequence learning using relevant patterns discovered from a set of labelled sequences. To deal with this problem, firstly each relational sequence is mapped into a feature vector using the result of a feature construction method. Since, the efficacy of sequence learning algorithms strongly depends on the features used to represent the sequences, the second step is to find an optimal subset of the constructed features leading to high classification accuracy. This feature selection task has been solved adopting a wrapper approach that uses a stochastic local search algorithm embedding a naive Bayes classifier. The performance of the proposed method applied to a real-world dataset shows an improvement when compared to other established methods, such as hidden Markov models, Fisher kernels and conditional random fields for relational sequences.", "text": "abstract. tackle problem multi-class relational sequence learning using relevant patterns discovered labelled sequences. deal problem ﬁrstly relational sequence mapped feature vector using result feature construction method. since eﬃcacy sequence learning algorithms strongly depends features used represent sequences second step optimal subset constructed features leading high classiﬁcation accuracy. feature selection task solved adopting wrapper approach uses stochastic local search algorithm embedding na¨ıve bayes classiﬁer. performance proposed method applied real-world dataset shows improvement compared established methods hidden markov models fisher kernels conditional random ﬁelds relational sequences. sequential reasoning fundamental task intelligence. indeed sequential data found contexts every life. computer science point view sequential data found many applications video understanding planning computational biology user modelling speech recognition etc. sequences simplest form structured patterns diﬀerent methodologies proposed face problem sequential pattern mining ﬁrstly introduced capturing existent maximal frequent sequences given database. many problems investigated concerns assigning labels sequences objects. however environments involve complex components features. thus classical existing data mining approaches look patterns single data table extended multi-relational data mining approaches look patterns involving multiple tables relational database. exploitation powerful knowledge representation formalism ﬁrst-order logic. propositional language hidden markov models allowing simple model representation eﬃcient algorithm; hand probabilistic relational systems able elegantly handle complex structured descriptions where contrary atomic representation could make problem intractable propositional sequence learning techniques. paper propose probabilistic algorithm relational sequence learning tackle task learning discriminant functions relational learning corresponds reformulate problem attribute-value form applying propositional learner reformulation process obtained adopting feature construction method mining frequent patterns successfully used boolean features since eﬃcacy learning algorithms strongly depends features used represent sequences feature selection task useful. feature selection optimal subset input features leading high classiﬁcation performance generally carry classiﬁcation task optimum way. however search variable subset np-hard problem. therefore optimal solution cannot guaranteed acquired except performing exhaustive search solution space. stochastic local search procedure allows obtain good solutions without explore whole solution space. algorithms feature selection divided categories wrapper ﬁlter methods feature selection algorithm embeds classiﬁer selects subsets features guided predictive power predicted classiﬁer using wrapper approach. ﬁlter approach selects features adopting preprocessing step using heuristics based intrinsic characteristic data ignoring learner. paper propose algorithm named lynx relational sequence learning ﬁrst step adopts classical feature construction approach. following features considered boolean able associate probability constructed feature. second step system adopts wrapper feature selection approach uses stochastic local search procedure embedding na¨ıve bayes classiﬁer select optimal subset constructed features. particular optimal subset patterns searched using greedy randomised search procedure search guided predictive power selected subset computed using na¨ıve bayes approach. hence focus paper combining probabilistic feature construction feature selection relational sequence learning. show proposed approach comparable purposely designed probabilistic approaches relational sequence learning. outline paper follows. discussing related work section present lynx algorithm section particular brieﬂy present description language followed description feature conalready pointed problem sequential pattern mining central data mining applications many eﬀorts done order propose purposely designed methods face works restricted propositional patterns patterns involving ﬁrst order predicates. early domains highlights need describe structural information sequences bioinformatics. thus need represent many real world domains structured data sequences became unceasing consequently many eﬀorts done extend existing propose methods manage sequential patterns ﬁrst order predicates involved. related works divided categories. ﬁrst category work belonging inductive logic programming area reformulate initial relational problem attribute-value form using frequent patterns boolean features applying propositional learners. second category belong systems purposely designed tackle problem relational sequence analysis falling speciﬁc statistical relational learning area probabilistic models combined relational learning. work correlated authors presented ﬁrst inductive logic programming feature construction method. ﬁrstly construct features adopting declarative language constraint search space discriminant features. then features used learn classiﬁcation model propositional learner. presented logic language seqlog mining sequences logical atoms inductive mining system mineseqlog combines principles level-wise search algorithm version space order patterns satisfy constraint using optimal reﬁnement operator seqlog. seqlog logic representational framework adopts operators represent sequences indicate atom direct successor another atom occurs somewhere another. furthermore based language notion subsumption entailment point semantic given. work even correlated work tackle account feature construction problem only. here however combine feature construction process feature selection algorithm maximising predictive accuracy probabilistic model. systems similar approach combine probabilistic models relational description logical hidden markov models fisher kernels logical sequences relational conditional random ﬁelds purposely designed relational sequences learning. proposed extension classical fisher kernels working sequences alphabets order make able model logical sequences i.e. sequences alphabet logical atoms. fisher kernels developed combine generative models kernel methods shown promising results combinations support vector machines hidden markov models bayesian networks. successively authors proposed algorithm selecting lohmms data. popular methods analysing sequential data exploited handle sequence ﬂat/unstructured symbols. proposed logical extension overcomes weakness handling sequences structured symbols means probabilistic framework. finally extension conditional random ﬁelds logical sequences proposed. case sequence labelling task crfs better alternative hmms makes relatively easy model arbitrary dependencies input space. crfs undirected graphical models instead learning generative model hmms learn discriminative model designed handle non-independent input features. authors lifted crfs relational case representing potential functions relational regression trees learnt relational regression tree learner. section ﬁrstly brieﬂy reports framework mining relational sequences introduced manage patterns dimension taken account. framework used lynx general logic formalism representing mining relational sequences. framework lynx implements probabilistic pattern-based classiﬁer. particular introducing representation language lynx system along feature construction capability adopted pattern-based classiﬁcation model feature selection approach presented. representation language used ﬁrst-order logic brieﬂy review. ﬁrst-order alphabet consists constants variables function symbols non-empty predicate symbols. function symbols predicate symbols natural number assigned term constant symbol variable symbols n-ary function symbol applied terms clauses literals terms said ground whenever contain variables. datalog clause clause function symbols non-zero arity; variables constants used predicate arguments. lynx includes multi-dimensional relational framework corresponding pattern mining algorithm reported brieﬂy recall. -dimensional relational sequence deﬁned ordered list considering sequence ordered succession events dimension ﬂuents used indicate atom true given event. general case n-dimensional sequences operator introduced express multi-dimensional relations. speciﬁcally denotes event successor event dimension hence multi-dimensional relational sequence deﬁned datalog atoms concerning dimensions event related another event means operators order represent multi-dimensional relational patterns following dimensional operators introduced. given dimensions indicates direct successor dimension encodes transitive closure calculates n-th direct successor. hence multi-dimensional relational pattern deﬁned datalog atoms regarding dimensions non-dimensional atoms event related another event means operators background knowledge contains deﬁnitions operators used prove dimensional operators appearing patterns. given multi-dimensional relational sequence following indicate datalog clauses ground atoms order calculate frequency pattern sequence important deﬁne concept sequence subsumption. sldoi-deduction sld-deduction object identity object identity framework within clause terms denoted diﬀerent symbols must distinct i.e. must represent diﬀerent objects domain. quent multi-dimensional relational pattern mining based idea generic level-wise search method known data mining apriori algorithm level-wise algorithm makes breadth-ﬁrst search lattice patterns ordered specialization relation search starts general patterns level lattice algorithm generates candidates using lattice structure evaluates frequencies candidates. generation phase patterns taken using monotonicity pattern frequency generation frequent patterns based top-down approach. algorithm starts general patterns. then step tries specialise potential frequent patterns discarding non-frequent patterns storing ones whose length equal user speciﬁed input parameter maxsize. furthermore reﬁned pattern semantically equivalent patterns detected using θoi-subsumption relation discarded. specialization phase specialization operator θoi-subsumption used. basically operator adds atoms pattern. background knowledge algorithm uses background knowledge containing sequence constraints similar deﬁned seqlog must satisﬁed generated patterns. particular constraint included details) type mode denote type input/output mode predicate’s arguments respectively. used specify language bias indicating predicates used patterns formulate constraints binding variables; reﬁnement step reﬁnement patterns obtained using reﬁnement operator maps pattern specialisations pattern i.e. {p′|p means general subsumes particular given dimensions ﬂuent atoms non-ﬂuent atoms reﬁnement operator specialising patterns deﬁned follows specialisation level start next reﬁnement step lynx records obtained patterns. hence could happens ﬁnal pattern subsumes patterns set. however subsumed patterns diﬀerent support contributing diﬀerent classiﬁcation model. identiﬁed frequent patterns task features order correctly classify unseen sequences. input space relational sequences denote ﬁnite possible class labels. given training single relational sequence label associated goal learn function predicts label unseen instance. constructed features obtained ﬁrst step lynx system sequence build d-component vector-valued random variable pattern subsumes sequence otherwise. given discriminant functions classiﬁer said assign vector class taking maximum discriminant function corresponds maximum posteriori probability. minimum error rate classiﬁcation following discriminant function used here considering multi-class classiﬁcation problem involving discrete features multi-class problem components vector binaryvalued conditionally independent. particular component vector binary valued deﬁne components statistically independent model feature gives yes/no answer pattern however expect i-th pattern subsume sequence frequently class factors estimated training examples frequency counts follows constructed features viewed probabilistic features expressing relevance pattern determining classiﬁcation assuming conditional independence write product probabilities components given assumption particularly convenient writing class-conditional probabilities follows recall decide magnitude weight indicates relevance subsumption pattern determining classiﬁcation probabilistic characteristic features obtained feature construction phase opposed boolean feature. constructed features presented method features classify unseen sequences problem optimal subset features optimise prediction accuracy. optimisation problem selecting subset features superior classiﬁcation performance formulated follows. constructed original patterns function scoring selected subset exhaustive approach problem would require examining possible subsets feature making impractical even values |p|. stochastic local search procedure allows obtain good solutions without explore whole solution space. graspfs consider combinatorial optimisation problem given discrete solutions objective function minimised seeks solution method high-quality solutions combinatorial problem steps approach consisting greedy construction phase followed perturbative local search greedy construction method starts process empty candidate solution construction step adds best ranked component according heuristic selection function. then perturbative local search algorithm searching local neighborhood used improve candidate solution thus obtained. advantages search method much better solution quality fewer perturbative improvement steps reach local optimum. greedy randomised adaptive search procedures solve problem limited number diﬀerent candidate solutions generated greedy construction search method randomising construction method. grasp iterative process combining iteration construction local search phase. construction phase feasible solution built neighbourhood explored local search. algorithm reports graspfs procedure included lynx system perform feature selection task. iteration computes solution using randomised constructive search procedure applies local search procedure yielding improved solution. main procedure made components constructive phase local search phase. constructive search algorithm used graspfs iteratively adds solution component randomly selecting according uniform distribution named restricted candidate list highly ranked solution components respect greedy function probabilistic component graspfs characterised randomly choosing best candidates rcl. case greedy function corresponds error function errd previously reported particular given errd heuristic function feasible solutions parameter controls amounts greediness randomness. value corresponds greedy construction procedure produces random construction. reported grasp ﬁxed nonzero parameter asymptotically convergent global optimum. solution make algorithm asymptotically globally convergent could randomly select parameter value continuous interval beginning iteration using value entire iteration implemented graspfs. hence starting empty ﬁrst iteration subsets containing exactly pattern considered best selected specialisation. iteration working patterns reﬁned trying pattern belonging improve solution generated construction phase local search used. works iteratively replacing current solution better solution taken neighbourhood current solution better solution neighbourhood. order build neighbourhood solution neigh following operators used. given patterns solution particular given solution elements neighborhood neigh solutions obtained applying elementary modiﬁcation local search starts initial solution iteratively generates series improving solutions k-th iteration neigh searched improving solution errd errd. solution found made current solution. otherwise search ends local optimum. experiments conducted protein fold classiﬁcation important problem biology since functions proteins depend fold dataset already used made logical sequences secondary structure protein domains. task predict populated scop folds alpha beta proteins beta/alphabarrel nad-binding rossmann-fold domains ribosomal protein cysteine hydrolase phosphotyrosine protein phosphatases i-like class proteins consists proteins mainly parallel beta sheets overall class distribution sequences class used round robin approach treating pair classes separate classiﬁcation problem overall classiﬁcation example instance majority vote among pairwise classiﬁcation problems. table reports experimental results -fold cross-validated accuracy lynx. experiments conducted conﬁdence level equal conﬁdence level particular given training data imposed conf experiment lynx applied data without feature selection. particular applied classiﬁcation test instances without applying graspfs order baseline accuracy value. indeed accuracy grows graspfs optimises feature proving validity method adopted feature selection task. furthermore accuracy level grows mine patterns conﬁdence level equal corresponding save jumping emerging patterns only. proves jumping patterns discriminative power greater emerging patterns second experiment compared lynx data statistical relational learning systems whose cross-validated accuracies summarised table particular lohhms able achieve predictive accuracy fisher kernels achieved accuracy tildecrf reaches accuracy value lynx obtains accuracy hence conclude lynx performs better established methods real-world data. paper considered problem multi-class relational sequence learning using relevant patterns discovered labelled sequences. ﬁrstly applied feature construction method order relational sequence feature vector. then feature selection algorithm optimal subset constructed features leading high classiﬁcation accuracy applied. feature selection task solved adopting wrapper approach uses stochastic local search algorithm embedding na¨ıve bayes classiﬁer. performance proposed method applied real-world dataset shows improvement compared established methods.", "year": 2010}