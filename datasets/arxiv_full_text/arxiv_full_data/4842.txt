{"title": "Adaptive Neural Compilation", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.", "text": "paper proposes adaptive neural-compilation framework address problem eﬃcient program learning. traditional code optimisation strategies used compilers based applying pre-speciﬁed transformations make code faster execute without changing semantics. contrast work involves adapting programs make eﬃcient considering correctness target input distribution. approach inspired recent works diﬀerentiable representations programs. show possible compile programs written low-level language diﬀerentiable representation. also show programs representation optimised make eﬃcient target distribution inputs. experimental results demonstrate approach enables learning speciﬁcally-tuned algorithms given data distributions high success rate. algorithm design often requires making simplifying assumptions input data. consider instance computational problem accessing element linked list. without knowledge input data distribution specify algorithm runs time linear number elements list. however suppose linked lists encountered practice ordered memory. would advantageous design algorithm speciﬁcally task lead constant running time. unfortunately input data distribution real world problem cannot easily speciﬁed simple example. best hope obtain samples drawn distribution. natural question arises observations adapt generic algorithm computational task using samples unknown input data distribution? process ﬁnding eﬃcient implementation algorithm received considerable attention theoretical computer science code optimisation community. recently conditionally correct superoptimization proposed method leveraging samples input data distribution beyond semantically equivalent optimisation towards data-speciﬁc performance improvements. underlying procedure based stochastic search space possible programs. additionally restrict applications reasonably small loop-free programs thereby limiting impact practice. work take inspiration recent wave machine-learning frameworks estimating programs. using recurrent models graves introduced fully diﬀerentiable representation program enabling gradient-based methods learn program examples. many models published recently build improve early work graves unfortunately models usually complex train need rely methods curriculum learning gradient noise reach good solutions shown neelakantan moreover interpretability limited. learnt model complex underlying algorithm recovered transformed regular computer program. main focus machine-learning community thus learning programs scratch little emphasis running time. however nearly computational problems feasible design generic algorithms worst-case. argue pragmatic goal machine learning community design methods adapting existing programs speciﬁc input data distributions. propose adaptive neural compiler design compiler capable mechanically converting algorithms diﬀerentiable representation thereby providing adequate initialisation diﬃcult problem optimal program learning. present method improve compiled program using data-driven optimisation alleviating need perform wide search possible programs. show experimentally framework capable adapting simple generic algorithms perform better given datasets. idea compiling programs neural networks previously explored literature. siegelmann described build neural network would perform operations given program. compiler designed gruau targeting extended version pascal. complete implementation achieved neto wrote compiler netdef language based occam programming language. methods allow obtain exact representation program neural network lend optimisation improve original program. indeed formulation elementary step program expressed group neurons precise topology weights biases thereby rendering learning gradient descent infeasible. performing gradient descent parameter space would result invalid operations thus unlikely lead improvement. recent work reed freitas neural programmer-interpreters also seen compile program neural network learning model mimic program. ﬂexible previous approaches unable improve learned program dependency non-diﬀerentiable environment. another approach learning problem taken code optimisation community. exploring space possible programs either exhaustively stochastic manner search programs results eﬃcient. work sharma broadens space acceptable improvements data-speciﬁc optimisations opposed provably equivalent transformations previously ones considered. however method still reliant nongradient-based methods eﬃcient exploration space. representing everything diﬀerentiable manner obtain gradients guide exploration. recently graves introduced learnable representation programs called neural turing machine uses lstm controller outputs commands executed deterministic diﬀerentiable machine. examples input/output sequences manage learn controller model becomes capable performing simple algorithmic tasks. extensions model proposed memory tape replaced diﬀerentiable versions stacks lists. kurach modiﬁed introduce notion pointers making amenable represent traditional programs. parallel works using reinforcement learning techniques reinforce algorithm q-learning able work diﬀerentiable versions mentioned models. models trained loss based diﬀerence output model expected output. weak supervision leads complex training. instance neural requires high number random restarts converging correct solution even using best hyperparameters obtained large grid search. work ﬁrst show design neural compiler whose target controller-machine model. makes compiled model amenable learning examples. moreover initialisation learning procedure allowing complex task ﬁnding eﬃcient algorithm. model model composed parts controller charge specifying executed; machine following commands controller. start describing global architecture model. sake simplicity general description present non-diﬀerentiable version model. section explain modiﬁcations required make model completely diﬀerentiable. detailed description model provided appendix general model ﬁrst deﬁne timestep memory tape contains integer values instruction register contain single value irt. also deﬁne instructions executed whose main role perform computations using registers. example values contained registers. also deﬁne side eﬀect action involves elements input output values instruction. interaction memory example side eﬀect. instructions computations side eﬀects detailed figure seen figure execution model takes input initial memory tape outputs ﬁnal memory tape steps. step controller uses instruction register compute command machine. command -tuple ﬁrst element instruction executed machine enumerated integer. elements specify registers used arguments given instruction. last element speciﬁes register output instruction written. example command {add means value ﬁrst register change following add. machine execute command updating values memory registers instruction register. machine always performs operations apart required instruction. outputs stop allows model decide stop execution. also increments instruction register iteration. diﬀerentiability model presented simple execution machine diﬀerentiable. order able train model end-to-end loss deﬁned ﬁnal memory tape need make every intermediate operation diﬀerentiable. achieve this replace every discrete value model multinomial distribution possible values could taken. moreover hard choice would non-diﬀerentiable replaced continuous soft choice. henceforth bold letters indicate probabilistic version value. first memory tape replaced matrix corresponds taking value change applied registers probability replacing matrix taking value finally instruction register also transformed single value vector size noted i-th element represents probability take value machine contain learnable parameter execute given command. make diﬀerentiable machine takes input four probability distributions distribution instructions distributions registers. compute argument values argt argt convex combinations diﬀerent registers function associated k-th instruction presented table since executed instruction controlled probability output written register number instructions. value stored registers performing soft-write parametrised special case associated stop signal. executing model keep track probability program terminated iteration based probability associated iteration speciﬁc instruction controls ﬂag. probability goes threshold ηstop execution halted. applied techniques make side-eﬀects diﬀerentiable presented appendix controller learnable part model. ﬁrst learnable part initial values registers instruction register second learnable part parameters controller computes required distributions using matrix matrices. representation matrices found figure controller deﬁned composed four independent fully-connected layers. section complexity suﬃcient model able represent program. henceforth denote {rir learnable parameters model. adaptative neural compiler present adaptive neural compiler. goal best weights given dataset model perform correct input/output mapping eﬃciently can. begin describing learning objective details. subsequent sections focus making optimisation learning objective computationally feasible. objective function goal solve given algorithmic problem eﬃciently. algorithmic problem deﬁned input/output pairs. also access generic program able perform required mapping. example accessing elements linked list transformation would consist writing desired value speciﬁed position tape. program given would iteratively elements linked list desired value write desired position. exists bias would allow traversal faster expect program exploit approach problem construct diﬀerentiable objective function mapping controller parameters loss. deﬁne loss based states memory tape outputs controller step execution. precise mathematical formulation term loss given appendix present motivation behind them. halting prevent programs take inﬁnite amount time without stopping deﬁned maximum number iterations tmax execution halted. moreover penalty loss controller didn’t halt limit. conﬁdence term penalise probability stopping current state memory expected one. correctness term considered nothing would encourage learnt algorithm halt soon ﬁnished. correctness halting considered program halt early possible. conﬁdence enables algorithm evaluate better stop. loss weighted four above-mentioned terms. denote loss i-th training sample given parameters learning objective speciﬁed parameters outputs controller initial values register instruction register probability distributions. optimisation highly non-convex problem. able solve using standard gradient descent based methods ﬁrst need transform unconstrained problem. also know result optimisation non-convex objective function strongly dependent initialisation point. rest section ﬁrst present small modiﬁcation model remove constraints. present neural compiler provide good initialisation solve problem. reformulation order gradient descent methods without project parameters alter formulation controller. softmax layer linear layer ensuring constraints controller’s output respected. also apply softmax initial values registers instructions register ensuring also respect original constraints. transform constrained-optimisation problem unconstrained allowing standard gradient descent methods. discussed works kind model hard train requires high number random restarts converging good solution. present neural compiler provide good initialisations help problem. neural compiler goal neural compiler convert algorithm written unambiguous program parameters. parameters controller reproduce exact steps algorithm. similar problem framed reed freitas show accomplish without learning. figure example compilation process. program written perform listk task. given pointer head linked list integer target cell linked list write target cell k-th element list. intermediary representation program. corresponds instruction random access machine would need perform execute program. representation weights encodes intermediary representation. matrix correspond state/line. initial value registers also parameters model omitted here. diﬀerent steps compilation illustrated figure ﬁrst step written version program equivalent list level instruction. step seen going figure figure illustrative example uses fairly low-level language traditional features programming languages loops if-statements supported using instruction. constants arguments values handled introducing registers hold values. value required passed target position instruction resolved compile time. obtained intermediate representation generating parameters straightforward. seen figure line contains instruction input registers output register corresponds command controller output. ensure dirac-delta distribution given value matrix-vector product equivalent selecting weight matrix. incremented iteration controller outputs rows matrix order. thus one-to-one mapping lines intermediate representation rows weight matrix. example matrices found figure weight matrix rows corresponding number lines code intermediate representation. ﬁrst line matrix corresponding ﬁrst argument ﬁfth element value linked ﬁrst line code ﬁrst argument read operation ﬁfth register. number rows weight matrix linear number lines code original program. output command must able index line instruction register means largest representable number machine needs greater number lines program. moreover program written regular assembly language rewritten restricted instructions. done ﬁrst conditionals assembly language expressed combination arithmetic instructions. secondly arithmetic operations represented combination simple arithmetic operations loops statements. means program regular computer ﬁrst rewritten restricted instructions compiled weights model. even though models lstm controller showed controller composed simple linear functions expressive enough. advantage simpler model easily interpret weights model would possible recurrent network controller. straightforward leverage results compilation initialise controller weights obtained compilation generic algorithm. account extra softmax layer need multiply weights produced compiler large constant output dirac-delta distributions. results associated technique found section however initialise exactly sharp parameters training procedure able move away initialisation gradients associated softmax region small. instead initialise controller non-ideal version generic algorithm. means choice highest probability output controller correct probability choices zero. seen section allows controller learn gradient descent algorithm diﬀerent original lower loss ideal version compiled program. performed sets experiments. ﬁrst shows capability neural compiler perfectly reproduce given program. second shows neural compiler adapt improve performance programs. present results data-speciﬁc optimisation carried show decreases runtime algorithms additionally algorithms show runtime diﬀerent computationalcomplexity class altogether. code required reproduce experiments available online compilation compiler described section allows program written using instruction weights controller. illustrate point implemented simple programs solve tasks introduced kurach shortest path problem. implementations found figure others available appendix programs written speciﬁc language transformed neural compiler parameters model. expected resulting models solve original tasks exactly generalise input sequence. experiments addition able reproduce given program done reed freitas possibility optimising resulting program further. exhibit compiling program model optimising performance. eﬃciency gains tasks come either ﬁnding simpler equivalent algorithms exploiting bias data either remove instructions change underlying algorithm. identify three diﬀerent levels interpretability model ﬁrst type corresponds weights containing dirac-delta distributions exact one-to-one mapping lines weight matrices lines assembly code. second type probabilities dirac-delta except ones associated execution instruction recover exact algorithm statements enumerate diﬀerent cases arising conditional jump. third type operation executed soft soft argument possible recover program eﬃcient learned one. present brieﬂy considered tasks biases report reader appendix detailed encoding input/output tape. table average numbers iterations required solve instances problems original program best learned program ideal algorithm biased dataset. also include success rate reaching eﬃcient algorithm across multiple random restarts. access given value array return biased version value always same address required element stored constant. similar optimisation known constant folding. swap given array pointers swap elements biased version always reading avoided. increment given array increment element biased version array ﬁxed size elements array value don’t need read going array. listk given pointer head linked list number linked list value k-th element. biased version linked list organised order memory would array address k-th value computed constant time. example developed figure addition values written tape summed. data bias introduced starting algorithm non-eﬃcient performs addition series increment operation. eﬃcient operation would numbers. tasks perform grid search loss parameters hyperparameters. training performed using adam choose best hyperparameters optimisation diﬀerent random seeds. consider program successfully optimised conditions fulﬁlled. first needs output correct solution test cases presenting bias. second average number iterations taken solve problem must lower algorithm used initialisation. note cared ﬁrst criterion methods presented section would already provide success rate without requiring training. results presented table tasks manage faster algorithms. simple cases access swap optimal algorithm presented datasets obtained. exploiting bias data successful heuristics incorporated algorithm appropriate constants stored initial value registers. learned programs tasks always ﬁrst case interpretability means recover eﬃcient algorithm learned weights. listk addition lower success rates improvements original learned algorithms still signiﬁcant. initialised iterative algorithms complexities. managed constant time algorithms solve given problems making runtime independent input. achieving means equivalence approaches identiﬁed similar optimising compilers operate. moreover listk task learned programs corresponds second type interpretability. indeed programs soft jumps condition execution value even though program would generalise values learned programs task achieve type interpretability study learned algorithm reveal generalise value finally increment task achieves unexpected result. indeed able outperform best possible algorithm. looking learned program actually leveraging possibility perform soft writes multiple elements memory time reduce runtime. case learned program associated third type interpretability. ideal algorithm would give conﬁdence output algorithm unable high enough conﬁdence considered correct algorithm. practice simple tasks observe optimisation possible useless instructions remain present. transformations controller indeed diﬃcult achieve local changes operated gradient descent algorithm. analysis failure modes algorithm found appendix motivates envision approaches gradient descent address issues. work presented ﬁrst step towards adaptive learning programs. opens several interesting directions future research. exemple deﬁnition eﬃciency considered paper ﬂexible. chose look average number operations executed generate output input. leave study potential measures kolmogorov complexity sloc name future works. shown experiment section current method good ﬁnding eﬃcient solutions simple programs. complex programs solution close initialisation found. even though training heuristics could help tasks considered here would likely scale real applications. indeed main problem identiﬁed gradient-descent based optimisation unable explore space programs eﬀectively performing local transformations. future work want explore diﬀerent optimisation methods. approach would global local exploration improve quality solutions. ambitious plan would leverage structure problem techniques combinatorial optimisation solve original discrete problem.", "year": 2016}