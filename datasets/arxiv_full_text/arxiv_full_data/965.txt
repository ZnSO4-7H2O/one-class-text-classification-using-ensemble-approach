{"title": "Provable Methods for Training Neural Networks with Sparse Connectivity", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent.", "text": "provide novel guaranteed approaches training feedforward neural networks sparse connectivity. leverage techniques developed previously learning linear networks show also effectively adopted learn non-linear networks. operate moments involving label score function input show factorization provably yields weight matrix ﬁrst layer deep network mild conditions. practice output method employed effective initializers gradient descent. paradigm deep learning revolutionized ability perform challenging classiﬁcation tasks variety domains computer vision speech. however complete theoretical understanding deep learning lacking. training deep-nets highly non-convex problem involving millions variables exponential number ﬁxed points. viewed naively proving guarantees appears intractable. paper contrary show guaranteed learning subset parameters possible mild conditions. propose novel learning algorithm based method-of-moments. notion using moments learning distributions dates back pearson paradigm seen recent revival machine learning applied unsupervised learning variety latent variable models survey). basic idea develop efﬁcient algorithms factorizing moment matrices tensors. underlying factors sparse ℓ-based convex optimization techniques proposed before employed learning dictionaries topic models linear latent bayesian networks paper employ ℓ-based optimization method learn deep-nets sparse connectivity. however method theoretical guarantees linear models. develop novel techniques prove correctness even non-linear models. technique stein’s lemma statistics taken together show effectively leverage algorithms based method-of-moments train deep non-linear networks. present theoretical framework analyzing neural networks learnt efﬁciently. demonstrate method-of-moments yield useful information weights neural network also cases even recover exactly. practice output method used dimensionality reduction back propagation resulting reduced computation. show feedforward neural network relevant moment matrix consider crossmoment matrix label score function input data classical stein’s result states matrix yields expected derivative label stein’s result essentially obtained integration parts employing stein’s lemma show span moment matrix label input score function corresponds span weight vectors ﬁrst layer natural non-degeneracy conditions. thus singular value decomposition moment matrix used rank approximation ﬁrst layer weight matrix back propagation number neurons less input dimensionality. note since ﬁrst layer typically number parameters rank approximation results signiﬁcant improvement performance computational requirements. show exactly recover weight matrix ﬁrst layer moment matrix weights sparse. argued sparse connectivity natural constraint lead improved performance practice show weights correctly recovered using efﬁcient optimization approach. approaches earlier employed linear models dictionary learning topic modeling here establish method also successful learning non-linear networks alluding stein’s lemma. thus show cross-moment matrix label score function input contains useful information training neural networks. result intriguing connection shown denoising auto-encoder approximately learns score function input. analysis provides theoretical explanation pretraining lead improved performance back propagation interaction score function label back propagation results correctly identifying span weight vectors thus leads improved performance. score functions improved classiﬁcation performance popular framework fisher kernels however fisher kernel deﬁned derivative respect model parameter consider derivation respect input refer score function. note fisher kernel respect location parameter notions equivalent. here show considering moment label score function input lead guaranteed learning improved classiﬁcation. note various efﬁcient methods computing score function instance sasaki point score function estimated efﬁciently non-parametric methods without need estimate density function. fact solution closed form hyper-parameters tuned easily cross validation. number score matching algorithms goal good terms score function employ obtain accurate estimations score functions. since employ method-of-moments approach assume label generated feedforward neural network input data fed. addition make mild non-degeneracy assumptions weights derivatives activation functions. assumptions make learning problem tractable whereas general learning problem np-hard. expect output moment-based approach provide effective initializers back propagation procedure. successfully improve performance reducing computations moreover notion using moment matrices dimension reduction popular statistics dimension reducing subspace termed central subspace present based convex optimization technique learn weights ﬁrst layer assuming sparse. note different convex approaches learning feedforward neural network. instance bengio show boosting approach learning neural networks convex optimization problem long number hidden units selected algorithm. however typically neural network architecture ﬁxed case optimization non-convex. work ﬁrst show guaranteed learning feedforward neural network incorporating label input. arora considered auto-encoder setting learning unsupervised showed weights learnt correctly conditions. assume hidden layer decoded correctly using hebbian style rule binary states. present different approach learning using moments label score function input. ﬁrst consider feedforward network hidden layer. subsequently discuss much extended. label vector generated neural network feature vector. assume well-behaved continuous probability distribution score function exists. network depicted figure setup applicable multiclass multilabel settings. multiclass classiﬁcation softmax function multilabel classiﬁcation elementwise sigmoid function. recall multilabel classiﬁcation refers case instance label hope information weight matrix using moments label input. question possible guarantees. study moments start simple problem. linear network whitened gaussian input ylinear order learn form label-score function correlation matrix therefore dimensional project span perform classiﬁcation lower dimension. stein’s lemma gaussian random vector states function satisfying mild regularity conditions difﬁcult problem generalized linear model gaussian rnx. case nonlinear activation function satisﬁes mild regularity conditions. using stein’s lemma therefore assuming ex)] full column rank obtain span gaussian random vector provides sufﬁcient statistic information loss. thus project input span obtain dimensionality reduction. gaussian distribution assumption restrictive assumption. challenging problem random vector general probability distribution network hidden layers. deal instance? provide method learn problems. random vector probability density function output label corresponding network described equation general probability distribution score function random vector provides sufﬁcient statistics algorithm learning weight matrix ﬁrst layer neural network input labeled samples estimate score function using auto-encoder score matching. compute ˆa=sparse dictionary learning proposition random vector joint density function suppose score function exists. consider continuously differentiable function entries zero boundaries support then proof follows integration parts; result scalar scalar-output functions provided remark theorem provides nice closedb diag)] full column rank obtain space form. deep networks auto-encoder shown approximately learn score function input shown pre-training results better performance. here using correlation matrix labels score function obtain span weights. auto-encoder appears estimating score function. therefore method provides theoretical explanation pre-training helpful. remark whitened gaussian random vector projecting input onto rowspace sufﬁcient statistic. empirically even non-gaussian distribution lead improvements moment method presented paper presents low-rank approximation train neural networks. showed recover span retrieve matrix without assumptions problem identiﬁable. reasonable assumption sparse. case pose problem learning given span. problem arises number settings learning sparse dictionary topic modeling. next using idea presented discuss done. identiﬁablity ﬁrst natural identiﬁability requirement full rank. spielman show bernoulli-gaussian entries relative scaling parameters impose sparsest vectors row-span rows vector space generated linear combination rows intuition random sparsity combination different sparse rows cannot make sparse row. identiﬁability condition need solve optimization problem optimization order come tractable update spielman convex relaxation norm relax nonzero constraint constraining afﬁne hyperplane {r⊤w therefore algorithm includes solving following linear programming problem ﬁnally note exist sophisticated analysis algorithms problem ﬁnding sparsest vectors subspace. anandkumar provide deterministic sparsity version result. barak require computation even quasi-polynomial time solve problem denser settings. elementwise ﬁrst layer elementwise function. nondegeneracy ex)a diag)] full column rank. score function score function exists. sufﬁcient input dimension positive constant sparse connectivity weight matrix bernoulli-gaussian. positive assumption satisﬁed full-rank diag) nondegenerate. case number classes large i.e. imagenets. future plan consider setting small number classes using methods like tensor methods. non-degeneracy assumption reason assume functions least linear i.e. ﬁrst order derivatives nonzero. true activation function models deep networks sigmoid function piecewise linear rectiﬁer softmax function last layer. assumption hold learn scaling bias back propagation. nevertheless since row-normalized provides directions number parameters back propagation reduced signiﬁcantly. therefore instead learning dense matrix need scaling sparse matrix. results signiﬁcant shrinkage number parameters back propagation needs learn. theorem assumptions a.−a. hold nonlinear neural network algorithm uniquely recovers row-normalized version exponentially small probability failure. proof remark optimization efﬁcient algorithm implement. algorithm involves solving optimization problems. traditionally minimization formulated linear programming problem. particular minimization problems written inequality constraints equality constraint. since computational complexity method often high large scale problems approximate methods gradient projection iterative-shrinkage thresholding proximal gradient noticeably faster remark learning encode ﬁrst layer perform softmax regression learn remark results work proposed random setting i.i.d. bernoulli-gaussian entries matrix assumed. general results presented terms deterministic conditions anandkumar show model identiﬁable full column rank following expansion condition holds elementwise function applicable multiclass mutlilabel settings. multiclass classiﬁcation softmax function multilabel classiﬁcation elementwise sigmoid function. network learn ﬁrst layer using idea presented earlier section learn ﬁrst layer. stein’s lemma matrix eadσ′d−ad−σ′d−ad− diag)] full column rank. assumption aihi denotes input i-th layer. theorem assumptions hold nonlinear deep neural network then algorithm uniquely recovers row-normalized version exponentially small probability failure. deep network ﬁrst layer includes parameters layers consist small number parameters since small number neurons. therefore result prominent progress learning deep neural networks. remark ﬁrst result learn subset deep networks general nonlinear case supervised manner. idea presented auto-encoder setting whereas consider supervised setting. also arora assume hidden layer decoded correctly using hebbian style rule binary states. addition remark order full column rank intermediate layers square weight matrices. however want learn middle layers requires number rows weight matrices smaller number columns speciﬁc manner therefore cannot full column rank. future hope investigate methods help overcoming challenge. introduced paradigm learning neural networks using method-of-moments. literature method restricted unsupervised setting. here bridged employed discriminative learning. opens interesting research directions future investigation. first note considered input continuous distribution score function exists. question whether learning parameters neural network possible discrete data. although stein’s lemma form discrete variables clear leveraged learn network parameters. next worth analyzing beyond relaxation provide guarantees cases. another interesting problem arises case small number classes. note non-degeneracy condition require number classes bigger number neurons hidden layers. therefore method work cases addition order learn weight matrices intermediate layers need number rows smaller number columns sufﬁcient input dimension. hand non-degeneracy assumption requires weight matrices square matrices. hence learning weights intermediate layers deep networks challenging problem. seems tensor methods highly successful learning wide range hidden models topic modeling mixture gaussian community detection problem provide overcome last challenges. boaz barak fernando brandao aram harrow jonathan kelner david steurer yuan zhou. hypercontractivity sum-of-squares proofs applications. proceedings forty-fourth annual symposium theory computing pages ingrid daubechies michel defrise christine mol. iterative thresholding algorithm linear inverse problems sparsity constraint. communications pure applied mathematics m´ario figueiredo robert nowak stephen wright. gradient projection sparse reconstruction application compressed sensing inverse problems. selected topics signal processing ieee journal seung-jean kwangmoo michael lustig stephen boyd dimitry gorinevsky. interior-point method large-scale -regularized least squares. selected topics signal processing ieee journal ker-chau principal hessian directions data visualization dimension reduction another application stein’s lemma. journal american statistical association charles stein persi diaconis susan holmes gesine reinert exchangeable pairs analysis simulations. stein’s method pages institute mathematical statistics kevin swersky david buchman nando freitas benjamin marlin autoencoders score matching energy based models. proceedings international conference machine learning pages", "year": 2014}