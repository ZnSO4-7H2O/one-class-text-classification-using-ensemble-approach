{"title": "A Unified Approach for Multi-step Temporal-Difference Learning with  Eligibility Traces in Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Recently, a new multi-step temporal learning algorithm, called $Q(\\sigma)$, unifies $n$-step Tree-Backup (when $\\sigma=0$) and $n$-step Sarsa (when $\\sigma=1$) by introducing a sampling parameter $\\sigma$. However, similar to other multi-step temporal-difference learning algorithms, $Q(\\sigma)$ needs much memory consumption and computation time. Eligibility trace is an important mechanism to transform the off-line updates into efficient on-line ones which consume less memory and computation time. In this paper, we further develop the original $Q(\\sigma)$, combine it with eligibility traces and propose a new algorithm, called $Q(\\sigma ,\\lambda)$, in which $\\lambda$ is trace-decay parameter. This idea unifies Sarsa$(\\lambda)$ (when $\\sigma =1$) and $Q^{\\pi}(\\lambda)$ (when $\\sigma =0$). Furthermore, we give an upper error bound of $Q(\\sigma ,\\lambda)$ policy evaluation algorithm. We prove that $Q(\\sigma,\\lambda)$ control algorithm can converge to the optimal value function exponentially. We also empirically compare it with conventional temporal-difference learning methods. Results show that, with an intermediate value of $\\sigma$, $Q(\\sigma ,\\lambda)$ creates a mixture of the existing algorithms that can learn the optimal value significantly faster than the extreme end ($\\sigma=0$, or $1$).", "text": "results implies fundamental trade-off problem reinforcement learning estimates value function adopting pure-expectation algorithm full-sampling algorithm? although pure-expectation approach lower variance needs complex larger calculation hand full-sampling algorithm needs smaller calculation time however worse asymptotic performance multi-step ﬁrstly attempts combine pure-expectation full-sample algorithms however multi-step temporaldifference learning expensive training. paper combine algorithm eligibility trace create algorithm called uniﬁes sarsa algorithm algorithm varies changes continuously sarsa paper also focus trade-off between pure-expectation full-sample control task experiments show intermediate value achieve better performance extreme case. framework notation standard episodic reinforcement learning framework often formalized markov decision processes framework considers -tuples form indicates states indicates actions indicates staterecently multi-step temporal learning algorithm called uniﬁes n-step tree-backup n-step sarsa introducing sampling parameter however similar multi-step temporal-difference learning algorithms needs much memory consumption computation time. eligibility trace important mechanism transform off-line updates efﬁcient on-line ones consume less memory computation time. paper develop original combine eligibility traces propose algorithm called trace-decay parameter. idea uniﬁes sarsa furthermore give upper error bound policy evaluation algorithm. prove control algorithm converge optimal value function exponentially. also empirically compare conventional temporal-difference learning methods. results show that intermediate value creates mixture existing algorithms learn optimal value signiﬁcantly faster extreme introduction reinforcement learning experiences sequences states actions rewards generated agent interacts environment. agent’s goal learning experiences seeking optimal policy delayed reward decision system. fundamental mechanisms studied temporal-difference learning method combination monte carlo method dynamic programming eligibility trace short-term memory process function states. learning combining eligibility trace provides bridge one-step learning monte carlo methods trace-decay parameter k-th expected error. expected-sarsa off-policy learning algorithm example greedy respect expected-sarsa restricted q-learning trajectory generated expected-sarsa on-policy algorithm λ-return algorithm one-step learning algorithm generalized multistep bootstrapping learning method. λ-return algorithm particular many multi-step learning algorithms weighting n-step returns proportionally λn−. rithm consider trajectory {}t≥ γtrt n-step returns initial state-action pair term n=λngn called λ-returns based fact ﬁxed point remains ﬁxed point equal usual bellman operator evaluation well-known trades bias bootstrapping transition probability state state taking action indicates expected reward transition discount factor. paper denote {}t≥ trajectory state-reward sequence episode.a policy probability distribution stationary policy policy change time. consider state-action value maps γ-contraction operator sup-norm r|s|×|a| fact ﬁxed point contraction operator unique value iteration converges initial unfortunately system solved directly fact environment usually unknown. practical model reinforcement learning available called model free. one-step learning algorithms learning algorithm signiﬁcant algorithms model free reinforcement learning idea bootstrapping critical learning evluation value function used targets learning process. given target policy learned behavior policy generates trajectory {}t≥ learning called on-policy learning otherwise off-policy learning. based fact γ-contraction operators convex combination operators thus γ-contraction. upper error bound policy evaluation section discuss ability policy evaluation iteration theorem results show sufﬁciently close ability policy evaluation iteration increases gradually decreases lemma sequence {ak}∞ satisﬁes mixed-sampling operator section present mixed-sampling operator contribution ﬂexible analysis algorithm later. introducing sampling parameter mixed-sampling operator varies continuously pure-expectation method full-sampling method. section analysis contraction ﬁrstly. introduce λ-return vision mixed-sampling operator denoting finally give upper error bound corresponding policy evaluation algorithm. contraction mixed-sampling operator deﬁnition mixed sampling operator r|s|×|a| r|s|×|a| parameter also degree sampling intrduced algorithm extreme deduce n-step returns k-th expected error. multi-step sarsa another extreme every intermediate value create mixed method varies continuously pure-expectation full-sampling call λ-return version deﬁne λ-version denote determined reinms maxa forcement learning system independent maxs∈s given policy constant determined learning system denote remark proof theorem strictly dependent assumption smaller never zero bound discrepancy behavior policy target policy ability prediction policy evaluation iteration dependent discussed contraction mixed-sampling operator introduced control algorithm. iteration theorem theorem version ofﬂine. section give on-line version discuss convergence. off-line learning expensive learning process must carried episode however on-line learning updates value function lower computational cost better performance. simple interpretation equivalence off-line learning on-line learning means that episode total updates forward view equal total updates backward view view equivalence on-line learning seen implementation ofﬂine algorithm inexpensive manner. another interpretation online learning provided learning accumulate trace comes approximate everyvisit monte-carlo method learning replace trace comes approximate ﬁrst-visit monte-carlo method. iterations theorem theorem verpractice access sion expectations trajectory {}t≥. statistical approaches utilize trajectory estimate value function. algorithm corresponds online form λ-returns time pair superscript visited k-th trajectory emphasizes forward update. denotes times pair visited k-th trajectory. deﬁne residual convenience expression give notations ﬁrstly. vector obtained iterations k-th trajectory superscript emphasizes online learning. denote k-th trajectory {}t≥ sampled policy online update rules expressed follows based theorem theorem greedy respect algorithm converge probability one. remark conclusion similar theorem update different develop assumption experiments experiment prediction capability section test prediction abilities -state random walk environment one-dimension environment widely used reinforcement learning agent state action left right taking action equal probability. compare root-mean-square error function episodes varies dynamically steps results figure show performance increases gradually decreases veriﬁes upper error bound theorem. figure root-mean-square error state values function episodes -state random walk consider accumulating trace replacing trace. plot shows prediction ability dynamically varying ﬁxed experiment control capability test control capability classical episodic task mountain because state space environment continuous tile coding function approximation version sutton’s tile coding software tilings. figure plot shows average return episode. rightcentered moving average window successive episodes employed order smooth results. right plot shows result comparing left plot shows result comparing sarsa step-size right part figure collect data varing steps results show signiﬁcantly converges faster left part figure results show intermediate value outperform sarsa. table table summarize average return episodes. order gain insight nature results steps take statistical method according lower upper conﬁdence interval bounds provided validate results. average return episodes could interpreted measure initial performance whereas average return episodes shows well algorithm capable learning. results show intermediate value best ﬁnal performance. conclusion paper presented method called uniﬁes sarsa solved upper error bound ability policy evaluation. furthermore proved convergence control algorithm conditions. proposed approach compared one-step multi-step learning methods results demonstrated effectiveness. richard sutton. generalization reinforcement learning successful examples using sparse coarse coding. advances neural information processing systems pages harm seijen hado hasselt shimon whiteson marco wiering. theoretical empirical analysis expected sarsa. adaptive dynamic programming reinforcement learning adprl’. ieee symposium pages ieee harm seijen rupam mahmood patrick pilarski marlos machado richard sutton. true online temporal-difference learning. journal machine learning research r´emi harutyunyan. offpolicy corrections. algorithmic learning theory international conference bari italy october proceedings volume page springer tommi jaakkola michael jordan satinder singh. convergence stochastic iterative dynamic programming algorithms. advances neural information processing systems pages r´emi munos stepleton anna harutyunyan marc bellemare. safe efﬁcient offpolicy reinforcement learning. advances neural information processing systems pages", "year": 2018}