{"title": "Weakly-supervised Semantic Parsing with Abstract Examples", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Semantic parsers translate language utterances to programs, but are often trained from utterance-denotation pairs only. Consequently, parsers must overcome the problem of spuriousness at training time, where an incorrect program found at search time accidentally leads to a correct denotation. We propose that in small well-typed domains, we can semi-automatically generate an abstract representation for examples that facilitates information sharing across examples. This alleviates spuriousness, as the probability of randomly obtaining a correct answer from a program decreases across multiple examples. We test our approach on CNLVR, a challenging visual reasoning dataset, where spuriousness is central because denotations are either TRUE or FALSE, and thus random programs have high probability of leading to a correct denotation. We develop the first semantic parser for this task and reach 83.5% accuracy, a 15.7% absolute accuracy improvement compared to the best reported accuracy so far.", "text": "figure overview setup visual reasoning. given image rendered utterance goal parse correct program results denotation training data includes triplets. zettlemoyer training examples correspond utterance-denotation pairs. denotation result executing program target knowledge-base naturally collecting denotations much easier because performed non-experts. training semantic parsers denotations raises fundamental learning challenges search training denotations algorithm must learn search huge space programs ones execute correct denotations. difﬁcult search problem combinatorial nature search space. incorrect programs easily lead right denotation thus learner astray based programs. mentioned problems spuriousness attracted relatively less attention semantic parsers translate language utterances programs often trained utterance-denotation pairs only. consequently parsers must overcome problem spuriousness training time incorrect program found search time accidentally leads correct denotation. propose small well-typed domains semi-automatically generate abstract representation examples facilitates information sharing across examples. alleviates spuriousness probability randomly obtaining correct answer program decreases across multiple examples. test approach cnlvr challenging visual reasoning dataset spuriousness central denotations either true false thus random programs high probability leading correct denotation. develop ﬁrst semantic parser task reach accuracy absolute accuracy improvement compared best reported accuracy far. introduction goal semantic parsing language utterances executable programs. early work statistical learning semantic parsers utilized supervised learning training examples included pairs language utterances programs. however collecting training examples scale quickly turned difﬁcult expert annotators familiar formal languages required. signiﬁcant body work weakly-supervised semantic parsing task comes ﬂavors input image input image synthesized. images fairly simple vision problem translating image structured relatively simple. given input easy view cnlvr semantic parsing problem goal translate language utterances programs executed determine correctness return values easy generate incorrect programs execute right denotation thus problem spuriousness signiﬁcant compared previous datasets. paper present ﬁrst semantic parser cnlvr dataset. develop formal language visual reasoning inspired johnson semantic parser based recent work main insight small well-typed domain manually cluster small number lexical items generate abstract representations utterances programs. representations alleviate challenges search spuriousness inherent weakly-supervised semantic parsing. creating abstract examples identify similar examples reward programs correct multiple examples thereby substantially reducing problem spuriousness. also done partial programs search time helps combat search challenge. moreover generate thousands utterance-program pairs automatically small number manually created abstract examples train supervised parser provide good starting point search process weakly-supervised setup. method allows train semantic parser weak supervision obtain accuracy absolute accuracy improvement compared state-of-the-art. code publicly available https//github.com/ udinaveh/nlvr_tau_nlp_final_proj. problem statement folgiven training examples lows language utterance describing objects image {true false} denotes whether utterance true false context goal learn semantic parser utterances-kb pairs program executed correct denotation programming language original cnlvr dataset describe image three sets objects object color shape size location absolute coordinates. deﬁne programming language amenable spatial reasoning inspired work clevr dataset programming language provides access functions allow check size shape color object check whether touching wall obtain sets items certain items etc. formally program sequence tokens describing possibly recursive sequence function applications preﬁx notation. token either function ﬁxed arity constant variable term used deﬁne boolean functions. functions constants variables following atomic types bool item size shape color side composite type func. valid programs type bool. table provides examples utterances correct program context image. note provide commas parenthesis table readability part language. provide full description program tokens arguments return types appendix unlike clevr cnlvr requires substantial set-theoretic reasoning required extending language described johnson include operators lambda abstraction. manually search explained searching large space programs fundamental challenge weakly-supervised semantic parsing. combat techniques. first beam search decoding time training weak supervision done prior work decoding step maintain beam program preﬁxes length expand exhaustively programs length keep program preﬁxes highest model probability. second utilize semantic typing system construct programs syntactically valid substantially prune program search space maintain stack keeps track expected semantic type decoding step. stack initialized type bool. then decoding step tokens return semantic type stack allowed stack popped decoded token function semantic types arguments pushed stack. dramatically reduces search space guarantees syntactically valid programs produced. figure illustrates state stack decoding program input utterance. program re-ranking model locally normalized model provides distribution every token emitted decoder. noticed often programs generated ﬁnal beam cover well words occur base model semantic parser presented work standard encoder-decoder architecture deﬁne distribution utterance encoded bidirectional lstm creates contextualized representation every utterance token decoder feed-forward network combined attention mechanism encoder outputs feed-forward decoder takes input last tokens decoded. formally probability program product probability tokens given probability decoded token computed follows. first utterance representation computed bilstm encoder table example mappings utterance tokens program tokens seven clusters used abstract representation. rightmost column counts number mapping cluster resulting total mappings. utterance. therefore optionally apply reranker re-ranks beam programs according following heuristic. every program count many utterance tokens program covers based small lexicon maps partial programs utterance phrases. then return program best coverage. section show results small boost ﬁnal performance parser. future plan train globally normalized re-ranker re-rank programs beam replace heuristic. main premise work small well-typed domains visual reasoning main challenge handling language compositionality utterances complex. conversely problem mapping lexical items functions constants programming language limited substantially reduced using handful manually deﬁned rules. thus abstract away actual utterance general representation easily generalize across examples datasets small number examples. consider utterances therefore deﬁne abstract representation utterances logical forms suitable spatial reasoning. deﬁne seven abstract clusters correspond main spatial concepts domain. then associate cluster small lexicon contains language-program token pairs associated cluster. table shows seven clusters example utterance-program token pair cluster number mappings cluster. total mappings used deﬁne abstract representations. show abstract examples employed develop rule-based semantic parser supervised semantic parser combat spuriousness search challenges weaklysupervised semantic parser. rule-based semantic parser examine diversity dataset created abstract representations utterances training examples mapping utterance tokens cluster label. counted many distinct abstract utterances exist found abstract utterances cover roughly half training examples original training set. create rule-based parser manually annotated abstract utterances corresponding abstract program aforementioned utterances mapped abstract program c-quantmod isc-shape istouchingwall))))). spuriousness central problem previously explained. alleviate spuriousness problem utilize interesting property data utterance appears times different images. program spurious likely yield wrong denotation images. thus re-deﬁne training example utterance paired different denotations utterance respect kbs. then maximize maximizing objective above except denotation correct four kbs. dramatically reduces problem spuriousness chance randomly obtaining correct denotation goes approach reminiscent method proposed pasupat liang random permutations wikipedia tables shown crowdsourcing workers eliminate spurious programs tables. approach tackling spuriousness described hinges fact utterance appears dataset multiple times. however explained above many examples similar another abstract level even identical. thus natural idea combat spuriousness sharing information across utterances abstract representation. construct cache maps abstract utterances abstract programs. obtaining beam ﬁnal programs every training example utterance cache every abstract utterance-program pair record whether obtained positive reward. thus every pair estimate probability whether obtains positive reward. construct abstract example utterance-program pair beam perform following procedure. first create abstract utterance replacing utterance tokens cluster label rule-based semantic parser. then every program token replace abstract cluster utterance contains token mapped program token according mappings table also provides alignment absupervised semantic parser data augmentation rule-based semantic parser high precision gauges amount structural variance data cannot generalize beyond observed examples. clusters small semantically coherent generate correct examples abstract examples sampling pairs utteranceprogram tokens cluster. equivalent synchronous context-free grammar rule generating manuallyannotated abstract utterance-program pair rules generating synchronously utterance program tokens seven clusters. generated examples using method trained standard supervised semanθ parser maximizing model described above. goal examine whether training dataset generalize beyond rule-based semantic parser. importantly probability obtaining denotation given deﬁned binary reward function correct. semantic parsers deﬁned correct executing results correct denotation goes long often probability generating correct denotation randomly generated low. however cnlvr observe binary denotations full program retrieval every utterance construct abstract utterance retrieve cache top-d abstract programs based compute programs using alignments program tokens utterance tokens programs ﬁnal beam. program preﬁx retrieval every decoding step beam decoded programs step beam decoded programs computed step every utterance top-d cashed programs former variant. step every abstract program exhaustively search possible continuations well constructing zt+. allows parser potentially construct programs cache already subsequent steps. combats spuriousness also search challenge beam promising program preﬁxes might fallen early decoding steps. experimental evaluation experimental details encoder bilstm hidden state dimension lstm decoder feed-forward network hidden layer dimension obtains last decoded tokens input utterance tokens embeddings dimension beam size retrieve programs cache. initialize word embeddings running cbow algorithm training data optimize end-to-end. weakly-supervised parser encourage exploration training time using meritocratic gradient updates weight program correct denotation interpolation model probability distribution uniform distribution. using standard policy gradient resulted slightly lower performance. models implemented tensorflow. supervised parser parameters word embeddings randomly initialized weakly-supervised parser initialize parameters using learned parameters supervised model warm-start model. adam optimizer learning rate mini-batch size examples trained epochs supervised model epochs weakly-supervised model. pre-processing number utterances relatively small training neural model take following steps reduce sparsity. lowercase utterance tokens also lemmatized form. also spelling correction replace words contain typos. last replace domain-speciﬁc words frequent synonym according training after pre-processing replace every word occurs less times symbol. evaluation evaluate public development test cnlvr well hidden test submitting code authors cnlvr. standard evaluation metric accuracy many examples correctly classiﬁed. addition report consistency measure proportion utterances resulting program correct denotation images/kbs. believe important measure captures better whether model consistently produce correct answer utterance. baselines compare models majority baseline always picks majority class also compare best results reported suhr taking input maximum entropy classiﬁer main results table describes main results. weakly-supervised semantic parser combined global re-ranking beam obtains accuracy consistency test improving accuracy points compared state-of-the-art. model evaluated hidden test according rules cnlvr evaluation. rule-based parser rule able handle non-negligible part data accuracy less points lower maxent baseline development public test set. training supervised parser improves accuracy public test showing generalizing generated examples better memorizing manually-deﬁned patterns. weakly-supervised parser signiﬁcantly improves sup. reaching accuracy test re-ranking. sup. weaksup. re-ranking substantially improves accuracy consistency. abstract examples already improves approaches programming language able overcome challenges search spuriousness develop weakly supervised semantic parser substantially improves state-of-the-art. examine importance abstract examples weakly-supervised parser scratch without pre-training supervised parser abstract examples without using cache abstract examples parser unable overcome challenges training weak supervision. training development accuracies remain throughtraining thus stopped training epochs seeing performance close majority baseline. examine importance cache itself train weakly-supervised parser starting initialized supervised parser without cache abstract examples weaklysupervised parser performs worse supervised parser initialized points. still unable improve learning training denotations. last beam cache full program retrieval programs added beam after decoding terminates rather every decoding step already results good performance substantially higher sup. still points worse best performing model development set. scaling semantic parsers since beginning decade. early work focused traditional loglinear models recently denotations used train neural semantic parsers well visual reasoning attracted considerable attention release datasets clevr advantage cnlvr language utterances natural compositional. treating problem visual reasoning end-to-end semantic parsing problem previously done clevr method generating training examples resembles ideas data re-combination suggested recently examples generated automatically replacing entities categories. spuriousness central semantic parsing denotations informative relatively little work explicitly tackling pasupat liang used manual rules prune unlikely programs wikitablequestions dataset later utilized crowdsourcing eliminate spurious programs. proposed randomer method increasing exploration handling spuriousness adding randomness beam search proposing meritocratic weighting scheme gradients. work found random exploration beam search improve results meritocratic updates slightly improved performance. work presented ﬁrst semantic parser cnlvr dataset working structured representations input. main insight small well-typed domains generate abstract examples help combat difﬁculties training parser delayed supervision. first abstract examples semiautomatically generate utterance-program pairs help warm-start parameters thereby reducing difﬁcult search challenge ﬁnding correct programs random parameters. second focus abstract representation examples allows tackle spuriousness alleviate search sharing information promising programs different examples. semantic parser substantially improves results compared previous state-of-the-art cnlvr dataset. future work plan investigate training globally-normalized re-ranker beam programs generated locally-normalized sequence-to-sequence model also train parser simpler programming language closer representation target artzi zettlemoyer. weakly supervised learning semantic parsers mapping instructions actions. transactions association computational linguistics glorot bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics. zettlemoyer collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. uncertainty artiﬁcial intelligence pages zettlemoyer collins. online learning relaxed grammars parsing logempirical methods natural lanical form. guage processing computational natural language learning pages andreas rohrbach darrell saenko. learning reason end-toend module networks visual question answerinternational conference computer viing. sion johnson hariharan maaten fei-fei zitnick girshick. clevr diagnostic dataset compositional language elementary visual reasoning. computer vision pattern recognition johnson hariharan maaten hoffman fei-fei zitnick girshick. inferring executing programs visual international conference comreasoning. puter vision krishnamurthy mitchell. weakly emsupervised training semantic parsers. pirical methods natural language processing computational natural language learning pages logical function exist filter count greaterthan lessthan greaterequal lessequal equal querycolor queryshape querysize getabove getbelow gettouching isyellow isblack isblue iscircle istriangle issquare isbig ismedium issmall istop isbottom istouchingwall union select allsame constants items boxes color.blue color.yellow color.black shape.circle shape.triangle shape.square side.top side.bottom side.left side.right side.any", "year": 2017}