{"title": "Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted  Least Squares Minimization", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "This work presents a general framework for solving the low rank and/or sparse matrix minimization problems, which may involve multiple non-smooth terms. The Iteratively Reweighted Least Squares (IRLS) method is a fast solver, which smooths the objective function and minimizes it by alternately updating the variables and their weights. However, the traditional IRLS can only solve a sparse only or low rank only minimization problem with squared loss or an affine constraint. This work generalizes IRLS to solve joint/mixed low rank and sparse minimization problems, which are essential formulations for many tasks. As a concrete example, we solve the Schatten-$p$ norm and $\\ell_{2,q}$-norm regularized Low-Rank Representation (LRR) problem by IRLS, and theoretically prove that the derived solution is a stationary point (globally optimal if $p,q\\geq1$). Our convergence proof of IRLS is more general than previous one which depends on the special properties of the Schatten-$p$ norm and $\\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets demonstrate that our IRLS is much more efficient.", "text": "|mij|; ||mj|| -norm column matrix) linear mapping. work consider nonconvex schatten-p norm ||m||p |mij|p p-norm ||m||p pursuing lower rank sparser solutions. problem general involves wide range problems lasso group lasso trace lasso matrix completion robust principle component analysis low-rank representation work propose general solver ease discussion focus following representative problems related works sparse rank minimization problems solved various methods semi-deﬁnite programming accelerated proximal gradient alternating direction method however complexity sized matrix unbearable large scale applications. requires least term objective function lipschitz continuous gradient. assumption violated many problems e.g. problem compared widely used one. usually requires introducing several auxiliary variables corresponding non-smooth terms. auxiliary variables slow convergence even lead divergence many variables. linearized reduce number auxiliary variables suffer convergence issue. work proposes accelerated ladm adaptive penalty lower periteration cost. however accelerating trick special problem. thus general problems. another drawback many rank minimization solvers perform soft singular value thresholding work presents general framework solving rank and/or sparse matrix minimization problems involve multiple non-smooth terms. iteratively reweighted least squares method fast solver smooths objective function minimizes alternately updating variables weights. however traditional irls solve sparse rank minimization problem squared loss afﬁne constraint. work generalizes irls solve joint/mixed rank sparse minimization problems essential formulations many tasks. concrete example solve schatten-p norm q-norm regularized low-rank representation problem irls theoretically prove derived solution stationary point convergence proof irls general previous depends special properties schatten-p norm q-norm. extensive experiments synthetic real data sets demonstrate irls much efﬁcient. problems research topics lead broad applications computer vision machine learning face recognition collaborative ﬁltering background modeling subspace segmentation norm nuclear norm popular choices sparse rank matrix minimizations theoretical guarantees competitive performance practice. models formulated joint rank sparse matrix minimization problem follow permitted. however permission material purposes must obtained ieee sending request pubs-permissionsieee.org. research supported singapore national research foundation international research centre singapore funding initiative administered programme ofﬁce. supported china program china msra collaborative research program. twofold. first rank sparse minimization problem solving difﬁcult solving rpca family algorithms. easy extend irls rank plus sparse matrix recovery problems based example. second become important model various applications machine learning computer vision. fast solver important real applications. ||m||p denotes schatten-p norm ||m||q denotes q-norm solver handle case problem special case major challenge solving terms objective function non-smooth. simple smooth terms introducing regularization terms rn×n identity matrix ones vector. terms make objective function smooth model called smoothed work. solving smoothed problem instead brings several advantages. first smooth major difference smoothed lrr. usually smooth objective function makes optimization problem easier solve. second convex theorem convex w.r.t guarantees globally optimal solution also given convex w.r.t theorem easily proved using convexity schatten-p norm q-norm third equality holds work solve general problem withintroducing auxiliary variables also without computing svd. idea smooth objective function introducing regularization terms. propose iteratively reweighted least squares method solving relaxed smooth problem alternately updating variable weight. actually reweighting methods studied minimization problem several variants proposed much theoretical analysis usually irls converges exponentially fast numerical results indicated leads sparse solution better recovery performance. reweighting method also applied rank minimization recently however problems solved iteratively reweighted algorithm still limited. previous works able minimize single -norm nuclear norm squared loss afﬁne constraint. thus cannot solve whose objective function contains non-smooth terms robust matrix completion rpca also previous convergence proofs based special properties p-norm schatten-p norm general thus limit application irls. actually many different nonconvex surrogate functions -norm proposed e.g. logarithm fcuntion generalize irls solving problem general objective functions. contributions summary contributions paper follows. solving problem objective function rank sparse matrix minimization ﬁrst introduce regularization terms smooth objective function solve relaxed problem iteratively reweighted least squares method. actually future works mentioned take schatten-p norm q-norm regularized problem concrete example introduce irls algorithm theoretically prove obtained solution irls stationary point. globally optimal based general proof show problems also solved irls. numerical experiments demonstrate effectiveness proposed irls algorithm comparing state-of-the-art family algorithms. irls much efﬁcient since avoids completely. section illustrate smoothed rank sparse matrix recovery iteratively reweighted least squares take problem concrete example. reason choosing model application notice depend computed ﬁxed. weight matrices ﬁxed obtained solving fact motivates solve iteratively updating optimization method called iteratively reweighted least squares shown algorithm irls separately treats weight matrices correspond rank sparse terms respectively. easy per-iteration complexity irls smoothed problem cost ladm ladmap. solves approximated unconstraint problem lrr. thus solution optimal traditional guarantee converge three variables. ladm ladmap lead optimal solution lrr. convergence rates sublinear i.e. number iterations. usually irls converges much faster type methods avoids computing iteration. though convergence rate irls established experiments show tends converge linearly. state-of-the-art method accelerated ladmap costs predicted rank faster irls rank sufﬁciently low. however rank depends choice parameter usually tuned achieve good performance application. observed experiments shown later irls outperforms accelerated ladmap several real applications. worth mentioning though present irls also used many problems including structured lassos overlapping/nonoverlapping group lasso tree structured group lasso robust matrix completion rpca though difﬁcult give general irls algorithm problems. main idea quite similar. ﬁrst step smooth objective function like table shows smoothed versions popular norms. related norms e.g. overlapping group lasso smoothed similar way. able compute derivatives smooth functions. derivatives rewritten simple function main variable introducing auxiliary variable i.e. weight matrix shown table iii. make updating main variable much easier. iteratively updating main variable weight matrix leads irls algorithm guarantees converge. generally concave function e.g. logarithm function replance p-norm talbe iii. induced problems also solved irls. previous iteratively reweighted algorithm minimizes non-smooth term squared loss minimize non-smooth terms. section provide convergence analysis irls non-smooth optimization. though based algorithm solving problem proofs general. ﬁrst show lemmas prove convergence irls. smoothed problem close problem easy check proofs also hold worth mentioning irls algorithm convergence proofs much general extensions nontrivial. problems sparse rank minimization problems afﬁne constraints. work considers unconstrained sparse rank minimization problems squared loss. work considers unconstrained joint rank sparse minimization problem. need update variable weight variables previous irls methods update variable weight. note usually easy prove convergence updating variables difﬁcult updating variables. also proofs totally different. afﬁne constraints optimal solution written feasible solution lies kernel property critical proofs cannot used proof rely least square loss function plays important role convergence proof proof handle least non-smooth terms simultaneously. also previous irls methods special property based young’s inequality concavity lemma involves general functions. thus irls also used replaced concave functions e.g. log. section conduct numerical experiments synthetic real data demonstrate efﬁciency proposed irls algorithm. irls solve inductive robust principle component problems. compare previous convex solvers ﬁrst examine behaviour irls sensitivity regularization parameter compare performance irls state-of-the-art methods. selection regularization parameter irls converges fast leads accurate solution regularization parameter chosen appropriately. decrease µt/ρ initialized µc||x|| ||x|| spectral norm following proofs also applicable concave functions e.g. approximation norm lemma assume column rm×n rm×n nonzero. concave differentiable functions. fig. convergence curves irls algorithm synthetic data different regularization parameters model parameter shows convergence curves irls algorithm different ﬁxing shows convergence curves irls algorithm different ﬁxing thus choice depends conduct experiments examine sensitivity irls respectively. ﬁrst examine different values second examine different values experiments performed synthetic data set. synthetic data generated procedure generate independent subspaces {si}k computed random rotation matrix rd×r random orthogonal matrix. subspace rank data dimension sample data vectors subspace uiqi i.i.d matrix. randomly chose samples corrupted adding gaussian noise zero mean standard deviation .||x||. figures show convergence curves irls different values observed small value lead inaccurate solution iterations. large value delay convergence. similar phenomenon found choice large value lead fast convergence small value lead accurate solution. accurate solution converge fast. thus cannot small large. observe work well. section present numerical results irls state-of-the-art algorithms including ladm ladmap accelerated ladmap solve problem subspace segmentation. type methods propack fast computing. implement irls algorithm matlab without using third party package. ladmap maximum iteration number ladmap usually fast able converge within iterations cases. except this default parameters competed methods released codes lin’s homepage. irls µc||x|| .||x|| µt/ρ experiments intel core quad memory running windows matlab version synthetic data example synthetic data section v-a. emphasize performance different model parameter usually larger leads lower rank solution. experiment test sensitiveness competed methods different ranks solution. figure shows convergence curves corresponding respectively table shows detailed results including achieved minimum last iteration computing time number iterations. seen irls always faster ladm. irls also outperforms ladmap ladmap except linearized methods need iterations sequences data points drawn three motions. sequence data ﬁrst projected onto -dimensional subspace pca. performed projected subspace best model parameter table tabulates comparison methods. seen irls fastest method. ladmap competitive irls requires much iterations. inductive robust principal component analysis inductive robust principal component analysis aims ﬁnding robust projection remove possible corruptions data. done solving following nuclear norm regularized minimization problem µi)− diagonal matrix test irls comparing ladmap face recognition. projection learned solving training data remove corruption coming test data point. perform experiments face data sets. ﬁrst extended yale consists subjects images subject. randomly select images training rest test. face dataset contains facial images people. images acquired across different poses. converge increases. small enough rank solution small. case partial faster full hence using propack unstable. compared ladmap irls better choice small-sized high-rank problems completely avoids svd. face clustering test performance competed methods face clustering extended yale database example face images shown figure subjects database. conduct experiments using ﬁrst subjects face images form data subject face images. images resized projected onto -dimensional subspace subjects clustering problem dimensional subspace subjects clustering problem. afﬁnity matrix deﬁned t|)/ solution problem obtained different solvers. normalized used produce clustering results based afﬁnity matrix. model parameter leads best clustering accuracy. figure table show performance comparison methods. seen irls fastest accurate method. also works well needs iterations. linearized methods efﬁcient since converge within iterations. near frontal pose includes images. images resized subject randomly select images training rest test. support vector machine used perform classiﬁcation. recognition results shown figure seen recognition accuracies almost different solvers. running time lamdap much larger irls algorithm. figure plots test images recovered irpca obtained irls algorithm. seen irpca irls successfully removes shadow corruptions faces. iteratively reweighted least squares algorithm simply solved single sparse rank minimization problem. proposed general irls solve joint rank sparse matrix minimization problems. objective function ﬁrst smoothed introducing regularization terms. irls applied solving relaxed problem. provide general proof show solution irls stationary point irls also applied various optimization problems convergence guarantee. interesting future work irls solving nonconvex structured lasso problems p-norm regularized group lasso overlapping/non-overlapping group lasso tree structured group lasso proof theorem proof. problem convex. stationary point globally optimal. thus need prove converges stationary point problem sequence {zt} bounded theorem hence exists matrix subsequence {ztj} note solves i.e. markus weimer alexandros karatzoglou quoc viet alex smola coﬁrank-maximum margin matrix factorization collaborative ranking advances neural information processing systems emmanuel cand`es xiaodong john wright robust principal component analysis? journal vol. canyi jiashi feng zhouchen shuicheng correlation international kim-chuan sangwoon accelerated proximal gradient algorithm nuclear norm regularized linear least squares problems paciﬁc journal optimization vol. zhouchen minming chen augmented lagrange multiplier method exact recovery corrupted low-rank matrices uiuc technical report uilu-eng-- tech. rep. zhouchen risheng zhixun linearized alternating direction method adaptive penalty low-rank representation advances neural information processing systems canyi yunchao zhouchen shuicheng proximal iteratively reweighted algorithm multiple splitting nonconvex proceedings aaai conference sparsity optimization artiﬁcial intelligence simon foucart ming-jun sparsest solutions underdetermined linear systems q-minimization applied computational harmonic analysis vol. yun-bin zhao duan reweighted -minimization sparse solutions underdetermined linear systems siam journal optimization vol. ingrid daubechies ronald devore massimo fornasier sinan gunturk iteratively reweighted least squares minimization sparse recovery communications pure applied mathematics vol. canyi jinhui tang shuicheng zhouchen generalized nonconvex nonsmooth low-rank minimization ieee international conference computer vision pattern recognition ming-jun yangyang wotao improved iteratively reweighted least squares unconstrained smoothed minimization siam journal numerical analysis vol. bing-kun guangcan changsheng shuicheng inductive robust principal component analysis ieee transactions image processing vol. athinodoros georghiades peter belhumeur david kriegman from many illumination cone models face recognition variable lighting pose ieee transactions pattern analysis machine intelligence vol. can-yi zhong-qiu zhao de-shuang huang shuicheng robust efﬁcient subspace segmentation least squares regression european conference computer vision. springer. canyi received bachelor degree mathematics fuzhou university master degree pattern recognition intelligent system university science technology china currently ph.d. student department electrical computer engineering national university singapore. current research interests include computer vision machine learning pattern recognition optimization. winner microsoft research asia fellowship zhouchen received ph.d. degree applied mathematics peking university currently professor laboratory machine perception school electronics engineering computer science peking university. also chair professor northeast normal university guest professor beijing jiaotong university. march lead researcher visual computing group microsoft research asia. guest professor shanghai jiaotong university southeast university guest researcher institute computing technology chinese academy sciences. research interests include computer vision image processing computer graphics machine learning pattern recognition numerical computation optimization. associate editor ieee trans. pattern analysis machine intelligence international computer vision area chair cvpr senior member ieee. shuicheng currently associate professor department electrical computer engineering national university singapore founding lead learning vision research group yan’s research areas include machine learning computer vision multimedia authored/coauthored hundreds technical papers wide range research topics google scholar citation times h-index highly-cited researcher iapr fellow serving associate editor ieee tkde tcsvt transactions intelligent systems technology received best paper awards pcm’ icme icimcs’ runner-up prize ilsvrc’ winner prize ilsvrc detection task winner prizes classiﬁcation task pascal winner prize segmentation task pascal honourable mention prize detection task pascal voc’ tcsvt best associate editor award young faculty research award singapore young scientist award young researcher award.", "year": 2014}