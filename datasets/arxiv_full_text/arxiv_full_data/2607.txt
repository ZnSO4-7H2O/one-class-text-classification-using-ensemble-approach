{"title": "Inference Compilation and Universal Probabilistic Programming", "tag": ["cs.AI", "cs.LG", "stat.ML", "68T37, 68T05", "G.3; I.2.6"], "abstract": "We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do \"compilation of inference\" because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.", "text": "introduce method using deep neural networks amortize cost inference models family induced universal probabilistic programming languages establishing framework combines strengths probabilistic programming deep learning methods. call compilation inference method transforms denotational speciﬁcation inference problem form probabilistic program written universal programming language trained neural network denoted neural network speciﬁcation language. test time neural network observational data executed performs approximate inference original model speciﬁed probabilistic program. training objective learning procedure designed allow trained neural network used proposal distribution sequential importance sampling inference engine. illustrate method mixture models captcha solving show signiﬁcant speedups eﬃciency inference. figure approach compiled inference. given probabilistic program compilation automatically construct neural network architecture comprising lstm core various embedding proposal layers speciﬁed probabilistic program train using inﬁnite stream training data generated model. expensive compilation stage complete left artifact weights neural architecture specialized given probabilistic program. inference probabilistic program compilation artifact used sequential importance sampling procedure artifact parameterizes proposal distribution probabilistic programming uses computer programs represent probabilistic models probabilistic programming systems stan bugs infer.net allow eﬃcient inference restricted space generative models systems church venture anglican —which call universal—allow inference unrestricted models. universal probabilistic programming appearing proceedings international conference artiﬁcial intelligence statistics fort lauderdale flordia usa. jmlr w&cp volume copyright authors. systems built upon turing complete programming languages support constructs higher order functions stochastic recursion control ﬂow. spate recent work addressing production artifacts compiling away amortizing inference body work roughly organized camps. work lives arguably camp organized around wake-sleep oﬄine unsupervised learning observationparameterized importance-sampling distributions monte carlo inference algorithms. camp approach paige wood closest spirit; propose learning autoregressive neural density estimation networks oﬄine approximate inverse factorizations graphical models introduce inference compilation sequential importance sampling objective function neural network architecture. section demonstrates approach examples mixture models captcha solving followed discussion section probabilistic programs denote probabilistic generative models programs include sample observe statements sample observe functions specify random variables generative model using probability distribution objects argument observe addition speciﬁes conditioning random variable upon particular observed value second argument. observed values induce conditional probability distribution execution traces whose approximations expected values want characterize performing inference. execution trace probabilistic program obtained successively executing program deterministically except encountering sample statements point value generated according speciﬁed probability distribution appended execution trace. assume order observe statements encountered ﬁxed. hence denote observed values ﬁxed possible traces. depending probabilistic program values generated sample statements order execution encounters sample statements well number encountered sample statements diﬀerent trace another. therefore given scheme assigns unique address sample statement according lexical position probabilistic program represent execution trace probabilistic program sequence respectively sample value address instance entry given trace trace-dependent length. count number sample values obtained speciﬁc sample statement address time step trace sequence holds sampled values sample statements. joint probability density execution trace test time trained inference network starts values observed quantities progressively proposes parameters latent nodes original structured model. however inversion dependency structure impossible universal probabilistic program model family approach instead focuses learning proposals forward inference methods model dependency inversion performed. sense work seen inspired kulkarni ritchie program-speciﬁc neural proposal networks trained guide forward inference. though signiﬁcantly less model-speciﬁc. high level characterizes camp fact artifacts trained suggest sensible varied parameters given explicitly structured therefore potentially interpretable model. related camp emerging around variational autoencoder also amortizes inference manner describe additionally also simultaneously learns generative model within structural regularization framework parameterized non-linear transformation latent variables. approaches camp generally produce recognition networks nonlinearly transform observational data test time parameters variational posterior approximation albeit less conditional structure excepting recent work johnson chief advantage approach learned model opposed recognition network simultaneously regularized towards simple perform inference towards explaining data well. work concern performing inference generative models speciﬁed probabilistic programs recognizing alternative methods exist amortizing inference simultaneously learning model structure. contributions twofold work ways handle complexities introduced compiling inference class generative models induced universal probabilistic programming languages establish technique embed neural networks forward probabilistic programming inference methods sequential importance sampling develop adaptive neural network architecture comprising recurrent neural network core embedding proposal layers speciﬁed probabilistic program reconﬁgured on-the-ﬂy execution trace trained inﬁnite stream training data sampled generative model. establishes framework combining deep neural networks generative modeling universal probabilistic programs begin providing background information reviewing related work section section figure results counting localizing objects detected pascal dataset corresponding categories object detectors matconvnet implementation fast r-cnn detector output processed using high detection threshold summarized representing bounding detector output single central point. inference using single trained neural network able accurately identify number detected objects locations categories. results particles. dirac delta function. requires designing proposal distributions corresponding addresses sample statements probabilistic program instance values proposal execution trace built executing program usual except sample statement address encountered time proposal sample value sampled proposal distribution qatit given proposal sample values point. obtain proposal execution traces assign weights achieve inference compilation universal probabilistic programming systems proposal distribution adaptation approximating framework sis. assuming adapted proposals qatit joint close resulting inference algorithm remains unchanged described section except replacement qatit qatit. inference compilation amounts minimizing function speciﬁcally loss neural network architecture makes proposal distributions good sense specify section process generating training data neural network architecture generative model described section training obtain compilation artifact comprising neural network components— recurrent neural network core embedding proposal layers corresponding original model probability distribution speciﬁed sample statement address probability distribution speciﬁed observe statement. called prior conditional density given sample values obtained encountering sample statement. called likelihood density given sample values obtained encountering observe statement mapping index observe statement index last sample statement encountered observe statement execution program. inference models amounts computing approximation expected values many inference algorithms universal probabilistic programming languages focus algorithms importance sampling family context develop scheme amortized inference. related diﬀerent approaches adapt proposal distributions importance sampling family algorithms sequential importance sampling method performing inference execution traces probabilistic program whereby weighted samples used approximate posterior expectations functions minibatch size given trace sample values addresses instances respectively denoted values sampled distributions observe statements denoted compilation training minibatches generated on-the-ﬂy probabilistic generative model streamed stochastic gradient descent procedure speciﬁcally adam optimizing neural network weights minibatches inﬁnite stream training data discarded update; therefore notion ﬁnite training associated issues overﬁtting training data early stopping using validation sample validation remains ﬁxed training compute validation losses tracking progress training less noisy admitted training loss. compilation artifact collection neural network components trained weights specialized performing inference model speciﬁed given probabilistic program. neural network architecture comprises non-domain-speciﬁc recurrent neural network core domain-speciﬁc observation embedding proposal layers speciﬁed given program. denote combined parameters neural network components rnns popular class neural network architecture well-suited sequence-to-sequence modeling wide spectrum state-of-the-art results domains including machine translation video captioning learning execution traces rnns work owing ability encode dependencies time hidden state. particular long short-term memory architecture helps mitigate vanishing exploding gradient problems rnns overall architecture formed combining lstm core domain-speciﬁc observe embedding layer several sample embedding divergence kullback–leibler measure closeness achieve closeness many possible take expectation quantity distribution ignore terms excluding last equality objective function corresponds negative entropy criterion. individual adapted proposals qatit) qatit depend output neural network time step parameterized considering factorization neural network architecture must able variable number outputs incorporate sampled values sequential manner concurrent running inference engine. describe neural network architecture detail section here training trace generated running unconstrained probabilistic program corresponding original one. unconstrained probabilistic program obtained program transformation replaces observe statement original program sample ignores second argument. universal probabilistic programming languages support stochastic branching generate execution traces changing number random choices. must therefore keep track information addresses instances samples execution trace introduced distinct layers address–instance pair described section probabilistic program execution trace diﬀerent length composed diﬀerent sequence addresses instances. handle complexity deﬁne adaptive neural network architecture reconﬁgured encountered trace attaching corresponding embedding proposal layers lstm core creating layers on-the-ﬂy ﬁrst encounter pair. evaluation starts computing observe embedding embedding computed trace repeatedly supplied input lstm time step. another alternative supply embedding ﬁrst time step approach preferred karpathy fei-fei vinyals prevent overﬁtting time step input lstm constructed concatenation figure neural network architecture. observe embedding; at−it− sample embeddings; previous sample value; type one-hot encodings current address instance proposal type; lstm input; lstm output; proposal layers; proposal parameters. note lstm core possibly stack multiple lstms. artifact generate parameter proposal distribution qatit. parameter obtained proposal layer mapping lstm output corresponding proposal layer. lstm network capacity incorporate inputs hidden state. allows parametric proposal qatit) take account previous samples observations. training supply at−it− tual sample values embedding interested parameter or∂φ calculate per-sample gradient inference evaluation proceeds requesting proposal parameters artifact speciﬁc address–instance pairs encountered. value sampled proposal distribution previous time step. neural network artifact implemented torch uses zeromq-based protocol interfacing anglican probabilistic programming system setup allows distributed training inference support across many machines demonstrate inference compilation framework examples. ﬁrst example demonstrate open-universe mixture model. second demonstrate captcha solving probabilistic inference mixture modeling e.g. gaussian mixture model shown figure density estimation clustering counting. inference problems posed given vector observations identify many where clusters optionally data points belong cluster. investigate inference compilation twodimensional number clusters unknown. inference arises observing valfigure typical inference results isotropic gaussian mixture model number clusters ﬁxed shown panels kernel density estimation distribution maximum posteriori values means {maxµk independent runs. ﬁgure illustrates uncertainty estimate cluster means given number particles equivalently ﬁxed amount computation. shows that given computation inference expected slowly becomes less noisy expectation. contrast bottom shows proposal learned used inference compilation produces low-noise highly accurate estimate given even small amount computation. eﬀectively encoder learns simultaneously localize clusters highly accurately. inferring posterior number clusters cluster mean covariance parameters σk}k assume input data model translated origin normalized within dimensions. order make good proposals inference neural network must able count i.e. extract represent information many clusters conditioned that localize clusters. towards select convolutional neural network observation embedding whose input two-dimensional histogram image binned observed data presenting observational data assumed arise mixture model neural network important considerations must accounted for. particular symmetries mixture models must broken order training inference work. first ways label classes. second ways individual data points could permuted. even experiments like presents major challenge neural network training. break ﬁrst symmetry training time sorting clusters euclidian distance means origin relabeling points permutation labels points cluster nearest original coming ﬁrst cluster next closest second approximately symmetry breaking many diﬀerent clusters nearly distance away origin. second avoid symmetry predicting number means covariances clusters individual cluster assignments. eﬀect sorting proposal mechanism learn propose nearest cluster origin receives training data always sorted manner. figure number clusters shows able learn proposal makes inference dramatically eﬃcient sequential monte carlo figure shows kind application eﬃcient inference engine simultaneous object counting localization computer vision achieve counting setting prior number clusters uniform distribution figure pseudo algorithm sample trace facebook captcha generative process. variations include sampling font styles coordinates letter placement language-model-like letter identity distributions noise parameters part inference. test time observe image infer sults literature. captcha solving well suited generative probabilistic programming approach latent parameterization low-dimensional interpretable design. using conventional computer vision techniques problem previously approached using segment-and-classify pipelines state-of-the-art results obtained using deep convolutional neural networks cost requiring large labeled training sets supervised learning. start writing generative models types surveyed bursztein namely ebay yahoo wikipedia figure provides overall summary modeling approach. actual models include domain-speciﬁc letter dictionaries font styles various types renderer noise matching captcha style. particular implementing displacement ﬁelds technique simard proved instrumental achieving results. note parameters stochastic renderer noise inferred example figure experiments shown successfully train artifacts also extract renderer noise parameters excluding list addresses learn proposal distributions improves robustness testing data sampled model. corresponds well-known technique adding synthetic variations training data transformation invariance used simard varga bunke jaderberg many others. observe-embedding consisting volutions linear -maxpooling-maxpooling-convolution-maxpooling-linear-linear convolutions successively ﬁlters max-pooling layers step size resulting embedding vector length convolutions linear layers followed relu activation. depending particular style artifact approximately trainable parameters. artifacts trained end-to-end using adam initial learning rate hyperparameters minibatches size table reports inference results test images sampled model achieve high recognition rates across board. reported results obtained approximately training traces. resulting artifacts running inference test captcha takes whereas durations ranging reported segment-and-classify approaches. also compared approach mansinghka method slow since must anew captcha taking order minutes solve captcha implementation method. probabilistic program must also written amenable markov chain monte carlo inference auxiliary indicator random variables rendering letters overcome multimodality posterior. subsequently investigated trained models would perform captcha images collected web. identiﬁed wikipedia facebook major services still making textual captchas collected labeled test sets images each. initially obtaining recognition rates several iterations model modiﬁcations able achieve recognition rates real wikipedia facebook datasets considerably higher threshold needed deem captcha scheme broken fact tune priors highlights issues model bias synthetic training models synthetic data testing real data. experiments also investigated feeding observe embeddings lstm time steps versus ﬁrst time step. empirically veriﬁed methods produce equivalent results latter takes signiﬁcantly longer train. training end-toend scratch former setup results frequent gradient updates training trace. summary need write probabilistic generative model produces captchas suﬃciently similar would like solve. using inference compilation framework inference neural network architecture training data labels free. create instances captcha break explored making deep neural networks amortizing cost inference probabilistic programming. particular transform inference problem given form probabilistic program trained neural network architecture parameterizes proposal distributions sequential importance sampling. amortized inference technique presented provides framework within integrate expressiveness universal probabilistic programming languages generative modeling processing speed deep neural networks inference. merger addresses several fundamennote synthetic/real boundary always clear instance assume captcha results goodfellow closely correspond results synthetic test data authors access google’s true generative process recaptcha images synthetic training data. stark train test model synthetic data. challenges associated constituents fast scalable inference probabilistic programs interpretability generative model inﬁnite stream labeled training data ability correctly represent handle uncertainty. experimental results show that family models focused proposed neural network architecture successfully trained approximate parameters posterior distribution sample space nonlinear regression observe space. aspects architecture currently working reﬁning. firstly structure neural network wholly determined given probabilistic program invariant lstm core maintains long-term dependencies acts glue embedding proposal layers automatically conﬁgured address–instance pairs program traces. would like explore architectures tight correspondence neural artifact computational graph probabilistic program. secondly domain-speciﬁc observe embeddings convolutional neural network designed captcha-solving task hand picked range fully-connected convolutional recurrent architectures trained end-to-end together rest architecture. future work explore automating selection potentially pretrained embeddings. limitation comes learning generative model itself—as done models organized around variational autoencoder possibility model misspeciﬁcation section explains training setup exempt common problem overﬁtting training set. demonstrated fact needed alterations captcha model priors handling real data risk overﬁtting model. therefore need ensure generative model ideally close possible true data generation process remember misspeciﬁcation terms broadness preferable misspeciﬁcation narrow uncalibrated model. would like thank hakan bilen help matconvnet setup showing fast r-cnn implementation rainforth helpful advice. tuan supported epsrc google studentships. atılım güneş baydin frank wood supported darpa ppaml u.s. afrl cooperative agreement fa--- award number references arulampalam maskell gordon clapp. tutorial particle ﬁlters online nonlinear/non-gaussian bayesian tracking. ieee transactions signal processing burda grosse salakhutdinov. importance weighted autoencoders. international conference learning representations bursztein martin mitchell. text-based captcha strengths weaknesses. proceedings conference computer communications security pages carpenter gelman hoﬀman goodrich betancourt brubaker riddell. stan probabilistic programming language. journal statistical software cheng druzdzel. ais-bn adaptive importance sampling algorithm evidential reasoning large bayesian networks. journal artiﬁcial intelligence research cotter shamir srebro sridharan. better mini-batch algorithms accelerated gradient methods. advances neural information processing systems pages dean corrado monga chen devin aurelio ranzato senior tucker yang large scale distributed deep networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. doucet johansen. tutorial particle ﬁltering smoothing fifteen years later. handbook nonlinear filtering goodfellow bulatov ibarz arnoud shet. multi-digit number recognition street view imagery using deep convolutional neural networks. international conference learning representations hochreiter schmidhuber. long short-term memory. neural computation jaderberg simonyan vedaldi zisserman. synthetic data artiﬁcial neural networks johnson duvenaud wiltschko datta adams. structured vaes composing probabilistic graphical models variational autoencoders. arxiv preprint arxiv. karpathy fei-fei. deep visual-semantic alignments generating image descriptions. proceedings ieee conference computer vision pattern recognition pages kulkarni kohli tenenbaum mansinghka. picture probabilistic programming language scene perception. proceedings ieee conference computer vision pattern recognition mansinghka kulkarni perov tenenbaum. approximate bayesian image interpretation using generative probabilistic graphics programs. advances neural information processing systems pages rainforth naesseth lindsten paige j.-w. meent doucet wood. interacting particle markov chain monte carlo. proceedings international conference machine learning volume jmlr w&cp ritchie thomas hanrahan goodman. neurally-guided procedural models amortized inference procedural graphics programs using neural networks. advances neural information processing systems pages simard steinkraus platt. best practices convolutional neural networks applied visual document analysis. proceedings seventh international conference document analysis recognition volume icdar pages washington ieee computer society. starostenko cruz-perez uceda-ponga alarcon-aquino. breaking text-based captchas variable word character orientation. pattern recognition sutskever vinyals sequence sequence learning neural networks. advances neural information processing systems pages varga bunke. generation synthetic training data hmm-based handwriting recognition system. seventh international conference document analysis recognition pages ieee vedaldi lenc. matconvnet convolutional neural networks matlab. proceeding international conference multimedia venugopalan donahue rohrbach mooney saenko. translating videos blum hopper langford. captcha using hard problems security. international conference theory applications cryptographic techniques pages springer wingate stuhlmüller goodman. lightweight implementations probabilistic programming languages transformational compilation. proceedings international conference artiﬁcial intelligence statistics pages wood meent mansinghka. approach probabilistic programming inference. proceedings seventeenth international conference artiﬁcial intelligence statistics pages zhang jiang sigal agam. learning synthetic data using stacked multichannel autoencoder. ieee international conference machine learning applications pages ./icmla...", "year": 2016}