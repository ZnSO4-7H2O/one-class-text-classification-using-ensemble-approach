{"title": "A Conditional Random Field for Discriminatively-trained Finite-state  String Edit Distance", "tag": ["cs.LG", "cs.AI"], "abstract": "The need to measure sequence similarity arises in information extraction, object identity, data mining, biological sequence analysis, and other domains. This paper presents discriminative string-edit CRFs, a finitestate conditional random field model for edit sequences between strings. Conditional random fields have advantages over generative approaches to this problem, such as pair HMMs or the work of Ristad and Yianilos, because as conditionally-trained methods, they enable the use of complex, arbitrary actions and features of the input strings. As in generative models, the training data does not have to specify the edit sequences between the given string pairs. Unlike generative models, however, our model is trained on both positive and negative instances of string pairs. We present positive experimental results on several data sets.", "text": "need measure sequence similarity arises information extraction object identity data mining biological sequence analysis domains. paper presents discriminative string-edit crfs ﬁnitestate conditional random ﬁeld model edit sequences strings. conditional random ﬁelds advantages generative approaches problem pair hmms work ristad yianilos conditionally-trained methods enable complex arbitrary actions features input strings. generative models training data specify edit sequences between given string pairs. unlike generative models however model trained positive negative instances string pairs. present positive experimental results several data sets. parameterized string similarity models based string edits long history however methods learning model parameters training data even though tasks learning lead greater accuracy realworld problems. ristad yianilos proposed expectationmaximization-based method learning string edit distance generative ﬁnite-state model. approach training data consists pairs strings considered similar parameters probabilities certain edit operations. e-step highest probability edit sequence found using current parameters; m-step probabilities re-estimated using expectations determined e-step reduce cost edit sequences expected caused match. useful attribute method edit operations parameters associated states ﬁnite state machine however generative model model cannot tractably incorporate arbitrary features input strings cannot beneﬁt negative evidence pairs strings considered dissimilar. bilenko mooney extend ristad’s model include aﬃne gaps also present learned string similarity measure based unordered bags words training performed svm. cohen richman conditional maximum entropy classiﬁer learn weights several sequence distance features. survey string edit distance measures provided cohen however none methods combine expressive power markov model edit operations discriminative training. paper presents undirected graphical model string edit distance conditional-probability parameter estimation method exploits matching non-matching sequence pairs. based conditional random ﬁelds approach provides powerful capabilities long sought many application domains also demonstrates interesting example discriminative learning probabilistic model involving structured latent variables. training data consists input string pairs associated binary label indicating whether pair considered match mismatch. model parameters estimated positive negative examples unlike previous generative models models however necessary provide desired edit-operations alignments— alignments enable accurate discrimination discovered automatically procedure. thus model example interesting class graphical models trained conditionally latent variables latent variable parameters maximize discriminative performance. another recent example includes work crfs object recognition images structured ﬁnite-state machine model single initial state disjoint sets non-initial states transitions them. state transitions labeled edit operations. disjoint sets represents match condition mismatch condition. non-empty transition path starting initial state deﬁnes edit sequence wholly contained either match mismatch subsets machine. marginalizing edit sequences subset obtain probability match mismatch. cost transition function edit operation previous state state input strings starting ending position input strings. applications take full advantage ﬂexibility. example cost function examine portions input strings current match position examine domain knowledge lexicons depend rich conjunctions primitive features. ﬂexibility edit operations possibly even valuable. edits make arbitrarily-sized forward jumps input strings size jumps conditioned input strings current match points each previous state ﬁnite state process. example single edit operation could match three-letter acronym expansion string consuming three capitalized characters ﬁrst string consuming three matching words second string. cost operation could conditioned previous state ﬁnite state process well appearance consumed strings various lexicons words following acronym. present experimental results standard text data sets including short strings names addresses well longer complex strings bibliographic citations. show signiﬁcant error reductions data sets. strings symbol sequences. pair input strings associated output label indicating whether strings considered match mismatch explain model scores alignments whether match mismatch. alignment fourtuple consisting sequence edit operations sequences string positions sequence states. indicate sequence edit operations delete-one-character-in-x substituteone-character-in-x-for-one-character-in-y delete-allcharacters-in-x-up-to-its-next-nonalphabetic. edit operation sequence consumes either positions therefore corresponding non-decreasing sequences a.ix a.iy edit-operation positions classify alignments matches mismatches take edits transition labels non-deterministic state transitions initial state states disjoint sets transitions sets. addition edit sequence string position sequences associate alignment sequence consecutive destinations states labels allowed transition construction either alignments states supposed represent matches alignments states supposed represent mismatches. summary alignment speciﬁed fourtuple ha.e a.ix a.iy qki. convenience also write dummy initial edit. could also straightforwardly imagine diﬀerent regression-based scenario real-valued also ranking-based criteria pairs provided indicates pair strings considered closer. dynamic programming model ﬁlls threedimensional table table moderately large practice beam search eﬀectively used increase speed speech recognition even larger tables common. interesting examine alignments learned non-match portion model. attain high accuracy states attract string pairs dissimilar. even similar strings alignments example alignment ﬁrst deletes inserts fortunately ﬁnding dissimilar strings requires ﬁnding good alignment possible deciding alignment good. as-goodas-possible alignments exactly learning procedure discovers driven objective function aims maximize likelihood correct binary match/non-match labels model ﬁnds latent alignment paths enable maximize likelihood. model thus falls family interesting techniques involving discrimination among complex structured objects structure relationship among parts unknown latent choice high impact discrimination task. similar considerations core discriminative non-probabilistic methods structured problems handwriting recognition speech recognition recently computer vision object recognition discuss related work section model implemented part ﬁnite-state transducer classes mallet three-dimensional dynamic programming problems positions states mallet’s existing ﬁnite-state forward-backward viterbi implementations encoding position indices single index diagonal crossing pattern starts example singlecharacter delete operation would adjacent vertical horizontal original table longer one-dimensional jump encoding. addition standard edit operations also powerful edits naturally model delete-until-end-of-word delete-word-in-lexicon delete-word-appearing-in-other-string. potential function non-negative function arguments normalizer experiments parameterize potential functions exponential linear scoring function vector feature functions taking arguments consecutive states alignment sequence corresponding edits string positions allow feature functions depend context typical feature function combines predicate input orinput feature predicate alignment fortunately calculated eﬃciently dynamic programming. typically given edit operation starting positions input strings small number possible resulting ending positions. max-product inference also performed eﬃciently. parameters estimated penalized maximum likelihood training data. training data consists string pairs corresponding labels indicating whether pair match. zero-mean spherical gausmaximize likelihood estimating given current parameters estep maximizing complete penalized loglikelihood m-step. optimization mstep bfgs. unlike crfs without latent variables objective function local maxima. avoid getting stuck poor local maxima parameters initialized yield reasonable default edit distance. show experimental results synthetic real-world data sets used previous work evaluating string edit measures. ﬁrst data sets name address ﬁelds restaurant database. among records matches. last four data sets citation strings standard reasoning constraint reinforcement face sections citeseer data. ratios citations unique papers respectively. making problem challenging certain evaluations data sets strings segmented ﬁelds title author treated single unsegmented character sequence. also present results synthetic noise person names generated database generator. program produces perturbed names according modiﬁable noise parameters including probability error anywhere record probability single character insertion deletion swap probability word swap. main advantages model ability include non-independent input features extremely ﬂexible edit operations. input features used experiments include subsets following described acting cell dynamic programming table input strings learned parameters associated input features well state transitions fsm. transitions entering state share tied parameters diﬀerent parameters since states edit operations remember context previous edit actions. model exploits positive negative examples training. positive training examples include pairs strings referring object however total number negative examples quadratic number objects. time memory constraints well desire avoid overwhelming positive training examples sample negative string pairs attain ratio match mismatch pairs. order preferentially sample near misses ﬁlter negative examples ways remove negative examples dissimilar according suitable metric. citeseer datasets cosine metric measure similarity citations; datasets metric jaro bilenko mooney train/test split data repeat process folds interchanged. restaurant name restaurant address dataset algorithm diﬀerent choices features states random splits data. citeseer datasets results random splits data. give training reasonable starting point hand-set initial parameters somewhat arbitrary reasonable parameters. examined small held-out data verify initial parameters reasonable. parameters match portion provide good alignments; copy parameters mismatch portion model oﬀseting bringing values closer zero small constant. table averaged f-measure detecting matching ﬁeld values several standard data sets four rows results duplicated bilenko mooney bottom performance method introduced paper. equations section used calculate ﬁrst-order model. threshold predicts whether string pair match mismatch. bilenko mooney found transitive closure improve results; help not. precision calculated ratio number correctly classiﬁed duplicates total number duplicates identiﬁed. recall ratio correctly classiﬁed duplicates total number duplicates dataset. report mean performance across multiple random splits. experiments real-world data sets compare performance results recent benchmark paper bilenko mooney bilenko recently completely thesis work area. results summarized table four rows duplicated bilenko mooney bottom shows results method. entries average measure across folds. observe large performance improvements datasets. fact diﬀerence performance across trials typically around suggests strong statistical signiﬁcance. average face dataset less previous best. examples made errors generally large venue authors ﬁeld string other. stead forward-backward inference. except restaurant address dataset forwardbackward performs signiﬁcantly better viterbi datasets. restaurant address data contains positive examples large unmatched suﬃx strings lead inappropriate dilution probability amongst many alignments. average measures restaurant datasets using viterbi forward-backward shown table results shown table forward-backward probabilities. table shows restaurant data various edit operations added model denotes insert denotes delete denotes substitute paren denotes skip-parenthesized-word denotes skip-if-wordin-lexicon pres denotes skip-word-if-present-inother-string. same-alphabets diﬀerentalphabets input features. seen results adding skip edits improves performance. although skip-parenthesized-words gives better results smaller data used experiments table skip-if-word-in-lexicon produces higher accuracy larger data sets peculiarities restaurants name diﬀerent locations named data set. also second-order model performs less well presumably data sparseness. dalp stand same-alphabets diﬀerentalphabets features snum dnum stand same-numbers diﬀerent-numbers features. features diﬀerent salpdalpsnum dnum features weights learned former depend whether characters equal separate weights learned number match letter match. conjecture number mismatch address data needs penalized letter mismatch. separating diﬀerent features features letters numbers reduces error finally table demonstrates power crfs include extremely ﬂexible edit operations examine arbitrary pieces input strings. particular measure impact including skip-wordif-present-in-other-string operation train test synthetic name data error probability typo error probability swap ﬁrst last name probability diﬀerence performance dramatic bringing error less course arbitrary substring swaps expressible standard dynamic programs skip operation gives excellent approximation preserving eﬃcient ﬁnitestate inference. typical improved alignments operation skip matching swapped ﬁrst name proceed correct individual typographic errors last name. tion mismatch portion model indeed learns best possible latent alignments order measure distance salient features. example’s alignment score match portion higher. entries dynamic programming table correspond states reached operations insert deletesubstitute skip-word-in-lexicon skip-parenthesized-word respectively. symbol denotes null transition. string similarity metrics based edit distance widely used applications ranging approximate matching duplicate removal database records identifying conserved regions comparative genomics. levenshtein introduced leastcost editing based independent symbol insertion deletion substitution costs needleman wunsch extended method allow gaps. editing strings alphabet generalized transduction strings different alphabets instance letter-to-sound mappings speech recognition applications edit distance model derived heuristic means possibly including data-dependent tuning parameters. example monge elkan recognize duplicate corrupted records using edit distance tunable edit costs. hernandez stolfo merge records large databases using rules based domain-speciﬁc edit distances duplicate detection. cohen token-based tf-idf string similarity score compute ranked approximate joins tables derived pages. association rule mining check duplicate records per-ﬁeld exact levenshtein blast gapped alignment matching. cohen surveys edit common substring similarity metrics name record matching application various duplicate detection tasks. bioinformatics sequence alignment edit costs based evolutionary biochemical estimates common position-independent costs normally used general sequence similarity search position-dependent costs often used searching speciﬁc sequence motifs. basic edit distance cost individual edit operations independent string context. however applications often require edit costs change depending context. instance characters author’s ﬁrst name ﬁrst character likely deleted ﬁrst character. instead specialized representations dynamic programming algorithms instead represent contextdependent editing weighted ﬁnite-state transducers whose states represent diﬀerent types editing contexts. idea also expressed pair hidden markov models pairwise biological sequence alignment edit costs identiﬁed probabilities edit distance models certain weighted transducers interpreted generative models pairs sequences. pair hmms generative models deﬁnition. therefore expectation-maximization using appropriate version forward-backward algorithm used learn parameters maximize likelihood given training pairs strings according generative model bilenko mooney train probabilities simple edit transducer duplicate detection measures evaluate. eisner gives general algorithm learning weights transducers notes approach applies transducers transition scores given globally normalized log-linear models. models crfs pair hmms hmms. need given explicit alignments alignment latent variable. joachims gives generic maximummargin method learning score alignments positive negative examples training examples must include actual alignments. addition cannot solve problem exactly exploit factorizations problem yield polynomial number constraints eﬃcient dynamic programming search alignments. basic models algorithms expressed terms single letter edits practice convenient richer application-speciﬁc edit operations example name abbreviation. example brill moore edit operations designed spelling correction spelling correction model trained tejada edit operations abbreviation acronym record linkage. presented discriminative model learning ﬁnite-state edit distance postive negative examples consisting matching nonmatching strings. necessary provide sequence alignments training. experimental results show method outperform previous approaches. model interesting member family models discriminative objective function discover latent structure. latent edit operation sequences learning indeed alignments help discriminate matching nonmatching strings. described detail ﬁnite-state version model. context-free grammar version model could edit operations deﬁned trees handle swaps arbitrarily-sized substrings. thank charles sutton xuerui wang useful conversations mikhail bilenko helpful comments previous draft. work supported part center intelligent information retrieval national science foundation grants iis- iis- iis- defense advanced research projects agency department interior acquisition services division contract nbchd.", "year": 2012}