{"title": "Stochastic variance reduced multiplicative update for nonnegative matrix  factorization", "tag": ["cs.NA", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Nonnegative matrix factorization (NMF), a dimensionality reduction and factor analysis method, is a special case in which factor matrices have low-rank nonnegative constraints. Considering the stochastic learning in NMF, we specifically address the multiplicative update (MU) rule, which is the most popular, but which has slow convergence property. This present paper introduces on the stochastic MU rule a variance-reduced technique of stochastic gradient. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets.", "text": "nonnegative matrix factorization dimensionality reduction factor analysis method special case factor matrices low-rank nonnegative constraints. considering stochastic learning speciﬁcally address multiplicative update rule popular slow convergence property. present paper introduces stochastic rule variance-reduced technique stochastic gradient. numerical comparisons suggest proposed algorithms robustly outperform state-of-the-art algorithms across diﬀerent synthetic real-world datasets. entry non-negativity enables interpret meanings obtained matrices well. interpretation produces broad range applications machine learning signal processing text mining image processing data clustering name few. however problem non-convex optimization problem ﬁnding global minimum np-hard. problem seung proposed simple eﬀective calculation algorithm denotes component-wise product matrices ﬁnds local optimal solution rule designated multiplicative update rule estimate expressed product current estimate factor. global convergence stationary point guaranteed slightly modiﬁed update rules constraints nevertheless many eﬃcient algorithms developed rule accompanied slow convergence furthermore considering data online learning algorithm preferred terms computational burden memory consumption. designating former algorithms batch-nmf online-nmf investigated actively several studies robust variant also assessed however still exhibit slow convergence. stochastic gradient descent become method choice solving data optimization problems. although beneﬁcial constant cost iteration independent convergence rate also slower full even strongly convex case. issue various variance reduction approaches proposed recently achieved superior convergence rates convex non-convex functions. paper presents proposal novel stochastic multiplicative update technique svrmu. paper also explains extensions svrmu accelerated variant robust variant outliers. exhaustive comparisons suggest proposed svrmu algorithms robustly outperform state-of-the-art algorithms across diﬀerent synthetic real-world datasets. noteworthy discussion presented applicable distance functions. algorithm designated solve problem without nonnegative constraints begin eagerly sought machine learning ﬁeld. designating designating rightmost inner term cost function respectively full gradient decent stepsize straightforward approach corresponds eﬀective alternative stochastic gradient ∇fnt nt-th updates η∇fnt assumes unbiased estimator full gradient ent] apparently calculation cost iteration independent mini∇fnt samples size |st|. however accelerate rate variance reduction techniques explicitly implicitly exploit full gradient estimation reduce variance noisy stochastic gradient leading superior convergence properties. regard approach hybrid algorithm sgd. representative research among stochastic variance reduced gradient svrg ﬁrst keeps indexed epoch inner iterations. also sets initial value inner loop s-th epoch computing full gradient randomly selects computes modiﬁed stochastic gradient section ﬁrst describes stochastic multiplicative update designated smu. details proposed stochastic variance-reduced algorithm i.e. svrmu. problem setting indexed svrmu double loop structure. keeping outer loop inner iterations also setting initial value inner loop s-th outer loop compute components full gradient update notation simplicity. then update convergence analysis similar diﬀerent update rule speciﬁcally denoting rightmost term deﬁne empirical cost already calculated previous steps. consider expected cost limn→∞ expectation taken respect distribution samples. interest usually minimization expected cost almost surely instead empirical cost convergence analysis ﬁrst shows converge a.s. zero acts surrogate function proof show converges a.s. modiﬁed update rule here stepsize ratio guarantee convergence. showing convergence ﬁnally obtain below; theorem assume {v}∞ compact. initial positive deﬁnite strictly convex. iterates minimization problem close examination update rule reveals that whereas latter requires dominant calculation component-wise product last step former requires much lower latter therefore repeat calculation several times corresponds step algorithm computation although similar strategy also proposed batch-based proposed diﬀers diﬀerent update rule. noteworthy point stopping criteria maximum iteration number dynamic stop criteria. former speciﬁcally examines ratio calculation complexity dynamic stop criteria process stops change l-th predeﬁned ratio diﬀerence initial value algorithm outlier causes remarkable degradation approximation address issue robust batch-nmf robust online-nmf proposed. extension also tackles problem within svrmu framework. given outlier matrix used comparison incremental online asag-mu proposed algorithms include svrmu section accelerated variants i.e. smu-acc svrmu-acc section ﬁrst maximum epoch figure presents results convergence behavior terms optimality calculated using hals advance. addition figure presents results convergence behavior setting maximum epoch ﬁgures show superior performance svrmu terms number gradients time. robust variant r-onmf accelerated variant r-svrmu. batch-based variant r-onmf also evaluated. figure presents illustration generated basis representations apparent r-svrmu produces better bases r-onmf provides similar bases batch-based r-nmf. dataset contains subjects captured view points illumination conditions. also coil object image dataset includes images size classes. resizing size preprocess procedures performed previous experiment. here sample data used coil sets respectively. methods comparison methods similar. peak signal-to-noise ratio image denoising calculated max/mse) vmax maximum value pixels normalized mutual information purity metrics used clustering accuracy. table shows svrmu provides better quality psrn higher clustering accuracies others. figure portrays denoised classes images reveals superior performance proposed svrmu. present paper proposed novel stochastic multiplicative update variance reduction technique svrmu. numerical comparisons suggest svrmu robustly outperforms state-ofthe-art algorithms across diﬀerent synthetic real-world datasets.", "year": 2017}