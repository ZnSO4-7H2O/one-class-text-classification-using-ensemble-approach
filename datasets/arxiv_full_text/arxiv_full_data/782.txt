{"title": "Weight Normalization: A Simple Reparameterization to Accelerate Training  of Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "text": "present weight normalization reparameterization weight vectors neural network decouples length weight vectors direction. reparameterizing weights improve conditioning optimization problem speed convergence stochastic gradient descent. reparameterization inspired batch normalization introduce dependencies examples minibatch. means method also applied successfully recurrent models lstms noise-sensitive applications deep reinforcement learning generative models batch normalization less well suited. although method much simpler still provides much speed-up full batch normalization. addition computational overhead method lower permitting optimization steps taken amount time. demonstrate usefulness method applications supervised image recognition generative modelling deep reinforcement learning. recent successes deep learning shown neural networks trained ﬁrst-order gradient based optimization capable achieving amazing results diverse domains like computer vision speech recognition language modelling however also well known practical success ﬁrst-order gradient based optimization highly dependent curvature objective optimized. condition number hessian matrix objective optimum problem said exhibit pathological curvature ﬁrst-order gradient descent trouble making progress amount curvature thus success optimization invariant reparameterization multiple equivalent ways parameterizing model much easier optimize others. finding good ways parameterizing neural networks thus important problem deep learning. architectures neural networks differ widely across applications typically mostly composed conceptually simple computational building blocks sometimes called neurons neuron computes weighted inputs adds bias term followed application elementwise nonlinear transformation. improving general optimizability deep networks challenging task since many neural architectures share basic building blocks improving building blocks improves performance wide range model architectures could thus useful. several authors recently developed methods improve conditioning cost gradient general neural network architectures. approach explicitly left multiply cost gradient approximate inverse fisher information matrix thereby obtaining approximately whitened natural gradient. approximate inverse example obtained using kronecker factored approximation fisher matrix inverting using approximate cholesky factorization inverse fisher matrix whitening input layer neural network alternatively standard ﬁrst order gradient descent without preconditioning change parameterization model give gradients like whitened natural gradients methods. example raiko propose transform outputs neuron zero output zero slope average. show transformation approximately diagonalizes fisher information matrix thereby whitening gradient leads improved optimization performance. another approach direction batch normalization method output neuron normalized mean standard deviation outputs calculated examples minibatch. reduces covariate shift neuron outputs authors suggest also brings fisher matrix closer identity matrix. following second approach approximate natural gradient optimization propose simple general method called weight normalization improving optimizability weights neural network models. method inspired batch normalization deterministic method share batch normalization’s property adding noise gradients. addition overhead imposed method lower additional memory required additional computation negligible. method show encouraging results wide range deep learning applications. k-dimensional weight vector scalar bias term k-dimensional vector input features denotes elementwise nonlinearity rectiﬁer denotes scalar output neuron. associating loss function neuron outputs neural network commonly trained stochastic gradient descent parameters neuron. effort speed convergence optimization procedure propose reparameterize weight vector terms parameter vector scalar parameter perform stochastic gradient descent respect parameters instead. expressing weight vectors terms parameters using k-dimensional vector scalar ||v|| denotes euclidean norm reparameterization effect ﬁxing euclidean norm weight vector ||w|| independent parameters therefore call reparameterizaton weight normalization. idea normalizing weight vector proposed earlier work typically still performed optimization w-parameterization applying normalization step stochastic gradient descent. fundamentally different approach propose explicitly reparameterize model perform stochastic gradient descent parameters directly. improves conditioning gradient leads improved convergence optimization procedure decoupling norm weight vector direction weight vector speed convergence stochastic gradient descent optimization show experimentally section instead working directly also exponential parameterization scale i.e. log-scale parameter learn stochastic gradient descent. parameterizing parameter log-scale intuitive easily allows span wide range different magnitudes. empirically however advantage. experiments eventual test-set performance signiﬁcantly better worse results directly learning original parameterization optimization slightly slower. training neural network parameterization done using standard stochastic gradient descent methods. differentiate obtain gradient loss function respect parameters gives gradient respect weights used normally. backpropagation using weight normalization thus requires minor modiﬁcation usual backpropagation equations easily implemented using standard neural network software. provide reference implementations theano https//github.com/timsalimans/weight_ norm. unlike batch normalization expressions independent minibatch size thus cause minimal computational overhead. alternative write gradient projection matrix projects onto complement vector. shows weight normalization accomplishes things scales weight gradient g/||v|| projects gradient away current weight vector. effects help bring covariance matrix gradient closer identity beneﬁt optimization explain below. projecting away norm grows monotonically number weight updates learning neural network weight normalization using standard gradient descent without momentum denote parameter update necessarily orthogonal current weight vector since project away calculating since proportional update thus also orthogonal increases norm pythagorean theorem. speciﬁcally ||∆v||/||v|| c||v|| ||v||. rate increase depend variance weight gradient. gradients noisy high norm quickly increase turn decrease scaling factor g/||v||. norm stop increasing. norm gradients small using mechanism scaled gradient self-stabilizes norm. property strictly hold optimizers separate learning rates individual parameters like adam experiments using momentum. however qualitatively still effect hold. empirically ability grow norm ||v|| makes optimization neural networks weight normalization robust value learning rate learning rate large norm unnormalized weights grows quickly appropriate effective learning rate reached. norm weights grown large respect norm updates effective learning rate stabilizes. neural networks weight normalization therefore work well much wider range learning rates using normal parameterization. observed neural networks batch normalization also property also explained analysis. projecting gradient away weight vector also eliminate noise direction. covariance matrix gradient respect given covariance matrix gradient given mwcmw. empirically often dominant eigenvector covariance matrix removing eigenvector gives covariance matrix closer identity matrix speed learning. mean standard deviation pre-activations special case network single layer input features layer whitened statistics given ||v||. case normalizing pre-activations using batch normalization equivalent normalizing weights using weight normalization. convolutional neural networks usually much fewer weights pre-activations normalizing weights often much cheaper computationally. addition norm non-stochastic minibatch mean variance general high variance small minibatch size. weight normalization thus viewed cheaper less noisy approximation batch normalization. although exact equivalence usually hold deeper architectures still weight normalization method provides much speed-up full batch normalization. addition deterministic nature independence minibatch input also means method applied easily models like rnns lstms well noise-sensitive applications like reinforcement learning. besides reparameterization effect batch normalization also beneﬁt ﬁxing scale features generated layer neural network. makes optimization robust parameter initializations scales vary across layers. since weight normalization lacks property important properly initialize parameters. propose sample elements simple distribution ﬁxed scale experiments normal distribution mean zero standard deviation starting training initialize parameters minibatch statistics pre-activations network like batch normalization single minibatch data initialization. done efﬁciently performing initial feedforward pass network single minibatch data using following computation neuron like batch normalization method ensures features initially zero mean unit variance application nonlinearity. method holds minibatch initialization subsequent minibatches slightly different statistics experimentally initialization method work well. method also applied networks without weight normalization simply stochastic gradient optimization parameters directly initialization terms compare section independently work type initialization recently proposed different authors found data-based initialization work well standard parameterization terms downside initialization method applied similar cases batch normalization applicable. models recursion rnns lstms resort standard initialization methods. weight normalization introduced section makes scale neuron activations approximately independent parameters unlike batch normalization however means neuron activations still depend therefore also explore idea combining weight normalization special version batch normalization call mean-only batch normalization normalization method subtract minibatch means like full batch normalization weight vector parameterized using weight normalization minibatch mean pre-activation training keep running average minibatch mean substitute test time. gradient loss respect pre-activation calculated denotes operation taking minibatch mean. mean-only batch normalization thus effect centering gradients backpropagated. comparatively cheap operation computational overhead mean-only batch normalization thus lower full batch normalization. addition method causes less noise training noise caused gentle large numbers ensures approximately normally distributed. thus added noise much lighter tails highly kurtotic noise caused minibatch estimate variance used full batch normalization. show section leads improved accuracy test time. experimentally validate usefulness method using four different models varied applications supervised image recognition generative modelling deep reinforcement learning. test reparameterization method application supervised classiﬁcation consider cifar- data natural images model using based convpool-cnn-c architecture small modiﬁcations replace ﬁrst dropout layer layer adds gaussian noise expand last hidden layer units units max-pooling rather hyperparameter actively optimized chosen maximize performance network holdout examples using standard parameterization full description resulting architecture given table supplementary material. train network cifar- using adam epochs ﬁxed learning rate momentum ﬁrst epochs. last epochs momentum linearly decay learning rate zero. minibatch size evaluate different parameterizations network standard parameterization using batch normalization using weight normalization using weight normalization combined mean-only batch normalization using mean-only batch normalization normal parameterization. network parameters initialized using scheme section four cases identical parameters starting out. case pick optimal learning rate resulting error curves training found ﬁgure weight normalization batch normalization provide signiﬁcant speed-up standard parameterization. batch normalization makes slightly progress epoch weight normalization early although partly offset higher computational cost implementation training batch normalization slower compared standard parameterization. contrast weight normalization noticeably slower. later stage training weight normalization batch normalization seem optimize speed normal parameterization still lagging behind. optimizing network epochs using different parameterizations evaluate performance cifar- test set. results summarized table weight normalization normal parameterization mean-only batch normalization similar test accuracy batch normalization signiﬁcantly better error. mean-only batch normalization combined weight normalization best performance test error interestingly much better mean-only batch normalization combined normal parameterization suggests noise added batch normalization useful regularizing network model maxout network network deeply supervised convpool-cnn-c all-cnn-c mean-only b.n. weight norm. normal param. batch norm. ours w.n. mean-only b.n. figure training error cifar- using different network parameterizations. weight normalization batch normalization mean-only batch normalization show results using adam learning rate normal parameterization instead works best case. last epochs learning rate linearly decayed zero. reparameterization provided weight normalization full batch normalization also needed optimal results. hypothesize substantial improvement mean-only b.n. weight normalization regular batch normalization distribution noise caused normalization method training mean-only batch normalization minibatch mean distribution approximately gaussian noise added full batch normalization training much higher kurtosis. aware result mean-only batch normalization combined weight normalization represents state-of-the-art cifar- among methods data augmentation. next test effect weight normalization applied deep convolutional variational autoencoders trained mnist data images handwritten digits cifar- data small natural images. variational auto-encoders generative models explain data vector arising latent variables joint distribution form decoder speciﬁed using neural network. lower bound marginal likelihood obtained approximately inferring latent variables observed data using encoder distribution also speciﬁed neural network. lower bound optimized model data. follow similar implementation cvae modiﬁcations mainly encoder decoder parameterized resnet blocks diagonal posterior replaced auto-regressive variational inference. mnist encoder consists sequences resnet blocks each ﬁrst sequence acting feature maps others feature maps. ﬁrst sequences followed -times subsampling operation implemented using stride third sequence followed fully connected layer units. decoder similar architecture reversed direction. cifar- used neural architecture resnet units multiple intermediate stochastic layers. used adamax optimization combination polyak averaging form exponential moving average averages parameters approximately epochs. ﬁgure plot test-set lower bound function number training epochs including error bars based multiple different random seeds initializing parameters. seen parameterization weight normalization lower variance converges better optimum. observe similar results across different hyper-parameter settings. figure marginal likelihood lower bound mnist cifar- test sets convolutional training standard implementation well modiﬁcation weight normalization. mnist provide standard error bars indicate variance based different initial random seeds. next consider draw recurrent generative model draw variational auto-encoder generative model encoder similar model section encoder decoder consisting recurrent neural network comprised long short-term memory units. lstm units consist memory cell additive dynamics combined input forget output gates determine information ﬂows memory. additive dynamics enables learning long-range dependencies data. time step model draw uses weight vectors update cell states lstm units encoder decoder. recurrent nature process clear batch normalization could applied model normalizing cell states diminishes ability pass information. fortunately weight normalization applied trivially weight vectors lstm unit work well empirically. take theano implementation draw provided https//github.com/jbornschein/ draw model mnist data handwritten digits. make single modiﬁcation model apply weight normalization weight vectors. seen ﬁgure signiﬁcantly speeds convergence optimization procedure even without modifying initialization method learning rate tuned normal parameterization. figure marginal likelihood lower bound mnist test draw training standard implementation well modiﬁcation weight normalization. epochs sufﬁcient convergence model implementation using weight normalization clearly makes progress much quickly standard parameterization. next apply weight normalization problem reinforcement learning playing games atari learning environment approach deep q-network proposed application batch normalization well suited noise introduced estimating minibatch statistics destabilizes learning process. able batch normalization work without using impractically large minibatch size. contrast weight normalization easy apply context initialization method section stochastic gradient learning performed using adamax momentum search optimal learning rates generally ﬁnding work well weight normalization work well normal parameterization. also larger minibatch size found efﬁcient hardware apart changes follow closely possible terms parameter settings evaluation methods. however python/theano/lasagne reimplementation work adapted implementation available https//github.com/spragunr/deep_q_rl small additional differences implementation. figure shows training curves obtained using standard parameterization weight normalization space invaders. using weight normalization algorithm progresses quickly reaches better ﬁnal result. table shows ﬁnal evaluation scores obtained weight normalization four games average weight normalization improves performance dqn. figure maximum evaluation scores obtained using either normal parameterization using weight normalization. scores indicated mnih reported normal parameterization approximately equivalent method. differences scores caused small differences implementation. speciﬁcally difference score enduro reported might using play-time limit evaluation. figure evaluation scores space invaders obtained epoch training standard parameterization using weight normalization. learning rates cases selected maximize highest achieved test score. presented weight normalization simple reparameterization weight vectors neural network accelerates convergence stochastic gradient descent optimization. weight normalization applied four different models supervised image recognition generative modelling deep reinforcement learning showing consistent advantage across applications. reparameterization method easy apply computational overhead introduce dependencies examples minibatch making default choice development deep learning architectures. layer type input whitening gaussian noise conv leaky relu conv leaky relu conv leaky relu pool str. dropout conv leaky relu conv leaky relu conv leaky relu pool str. dropout conv leaky relu conv leaky relu conv leaky relu global average pool softmax output", "year": 2016}