{"title": "Better Text Understanding Through Image-To-Text Transfer", "tag": ["cs.CL", "cs.CV", "cs.LG"], "abstract": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.", "text": "generic text embeddings successfully used variety tasks. however often learnt capturing co-occurrence structure pure text corpora resulting limitations ability generalize. paper explore models incorporate visual information text representation. based comprehensive ablation studies propose conceptually simple well performing architecture. outperforms previous multimodal approaches well established benchmarks. also improve state-of-the-art results image-related text datasets using orders magnitude less data. problem ﬁnding good representations text data actively studied area research. many models able learn representations directly optimizing end-to-end task supervised manner. this however often requires enormous amount labeled data available many practical applications gathering data costly. common solution requires order magnitude less labeled examples reuse pretrained embeddings. large body work space focused training embeddings pure text data. however many types relations cooccurrences hard grasp pure text. instead appear naturally modalities images. particular similarity measure image space pairs images sentences similarity measure sentences induced illustrated figure work study build sentence embeddings text-image pairs good terms sentence similary metrics. extends previous works example propose conceptually simple well performing model call visually enhanced text embeddings takes advantage visual information images order improve quality sentence embeddings. model uses simple ingredients already exist combines properly. using pre-trained convolutional neural network image embedding sentence embeddings obtained normalized word embeddings. figure images close visual space quantiﬁed cnn. descriptions convey concept using entirely different vocabulary. method improves understanding text leveraging knowledge visual properties corresponding images. photo credit trained end-to-end aligned corresponding image embeddings aligned mismatching pairs optimizing pearson correlation. despite simplicity model signiﬁcantly outperforms pure-text models best multimodal model well established text similarity benchmarks semeval competition particular image-related datasets model matches state-of-the-art results substantially less training data. results indicate exploring image data signiﬁcantly improve quality text embeddings incorporating images source information result text representations effectively captures visual knowledge. also conduct detailed ablation study quantify effect different factors embedding quality image-to-text transfer setup. summary contributions work propose simple multimodal model outperforms previous image-text approaches wide variety text similarity tasks. furthermore proposed model matches state-ofthe-art results image-related semeval datasets despite trained substantially less data. many works study multimodal data particular pairs images text. explore pairs data leveraged tasks require knowledge modalities like captioning image retrieval line work interesting common embeddings directly applied captioning image retrieval tasks direct text embeddings tasks using images auxiliary training data less explored. propose extend skip-gram algorithm incorporate image data. also took similar approach before. original skip-gram algorithm word embedding optimized increase likelihood neighboring words conditioned center word. addition predicting contextual words models maximize similarity image word embeddings. precisely average embedding images paired word show augment word embeddings learnt large text sources visual information addition image labeling retrieval show embeddings perform better word similarity metrics. also margin loss co-embed images corresponding text. text embeddings different models depending task. image captioning image retrieval task figure overview vete model. images pre-trained representation transformed match dimension sentence embeddings. sentences encoded text embedding model finally embeddings paired ways matching pairs incorrect pairs. pairs compute similarity. loss signal comes pearson correlation matching pairs incorrect pairs. green shaded modules trained. photo credit employ lstm encoder. explore resulting word embedding properties particular arithmetics simpler word embedding model. interesting arithmetical properties embeddings demonstrated like image blue blue leading image car. however quantitative evaluation quality text embeddings terms text similarity. recent works investigate phrase embeddings trained visual signals quality terms text similarity metrics. example recurrent neural network language model order learn word embeddings combined create phrase embedding. propose three models. model similar setup captioning model decoder conditioned pre-trained embedding. reads text trying predict next token. initial state transformation last internal layer pre-trained vggnet vector vimage. model tries match ﬁnal state vimage. finally model develops multimodal skip-gram adding additional loss measuring distance word embeddings vimage. authors’ experiments show model performs best model baseline experiments. setup aims directly transferring knowledge image text representations main goal reusable sentence embeddings. make direct paired data consisting pictures text describing them. propose model consisting separate encoders images another text. overview archiecture presented figure text encoder consider three families models combine words text representations. bag-of-words model sentence embedding simply normalized vectors corresponding individual words. model create stacked recurrent neural network encoder finally model encoder includes convolutional layer followed fully connected layer described encoding images pre-trained inceptionv network provides dimensional feature vector image dataset eimg denote -dimensional embedding vector image etxt ndimensional embedding sentence produced {bow throughout paper refer cosine similarity vectors informally speaking training goal maximize etxt) sentence paired image minimize value otherwise. figure examples three datasets. shown images provided captions. notice language varies formal descriptions geared towards general audience informal posts formally {bow text encoding model. let’s deﬁne afﬁne transformation transforms -dimensional image embeddings dimensions. learnable parameters consist word embedding matrix internal parameters model well transformation matrix batch image-sentence pairs form randomly shufﬂe sentences following \"incorrect\" pairs batch random permutation denote experiments consider three training datasets coco pinterestm. described detail below. important note modify datasets contain multiple captions image keep caption. done prevent network cheating using image feature vector joining text pairs. best knowledge ﬁrst notice issue evaluation quality multimodal image-text models. known training similarity models directly text-text pairs yields good results want investigate effect knowledge transfer images. coco coco dataset contains image categories. image highquality captions provided. coco dataset using train/validation/test split imtxt tensorﬂow implementation initially train/validation/test sets contain k/k/k examples respectively. then ﬁlter sets keep caption image \"text part\" ﬁnal datasets times smaller. stony brook university dataset consists image-caption pairs collected flickr caption image. randomly split dataset train/validation/test sets sizes k/k/k respectively. pinterestm original pinterestm dataset contains images. however image urls released time submission. unfortunately images longer available able collect approx. images dataset. every image keep caption. then randomly split data .m/k/k train/validation/test sets respectively. training data datasets lowercased tokenized using stanford tokenizer also wrap sentences \"<s>\" \"</s>\" marking beginning sentence. performance every machine learning model highly depends choice hyperparameters. order fairly compare approach previous works follow hyperparameter search protocol models. choose average score semeval validation metric refer avg. hyperparameter similar meaning models ranges searched same. additionally ensured parameters reported authors included ranges. models train using adam optimizer epochs ﬁnal embeddings size vete models pearson loss goal create good text embeddings encode knowledge corresponding images. evaluate effect knowledge transfer images text textual semantic similarity datasets semeval competitions. unfortunately could compare models directly gold dataset introduced publicly released. also additional custom test sets coco-test pin-test. created coco pinterestm test datasets respectively randomly sampling semantically related captions non-related captions different images. opposed semeval datasets similarity score binary case. goal check performance task semeval data distribution words training data. table presents scores obtained models trained coco datasets. allows fairly compare algorithms data used. section analyze robustness methods additional datasets direct comparison implement odela described refer pinmodela. implementation uses pre-trained inceptionv network visual feature extraction vete models. understand impact adding information images text data also evaluate models trained purely text vete models outperform pure-text baselines pinmodela. similarly observed rnn-based encoders outperformed simpler model. also show holds cnn-based encoders. worth noting mostly domain adaptation issue encoders perform better coco-test data distribution training data. analyze effect changing text encoder section results context table compares methods trained much larger corpora. used word embeddings obtained three methods three approaches consider versions differ vocabulary allowed inference time. experiment done vocabulary restricted coco non-restricted version whole vocabulary given embeddings. vocabulary size signiﬁcant impact ﬁnal score pinterest-test benchmark tokens coco vocabulary. means sentences least missing token. finally also include best results semeval competition available. note obtained heavily tuned complex models trained without data restrictions. still vete model able match results. analyze impact different components architecture perform ablation studies employed text encoder loss type training dataset. also investigate effect training word sentence level. cases follow similar protocol section study impact different text encoders vete model. results summarized table rnn-gru rnn-lstm denote encoders lstm cells respectively. options either mean word embeddings. bag-of-words encoders perform better encoders although rnns slightly better test data distribution training data. table results applying different text encoders vete model. training data coco rnn-based models learned model distribution better. however generalizes better datasets. surrogate kendall pearson correlation takes account linear dependencies. mitigate this experimented kendall correlation rankdependent. unfortunately differentiable. therefore used differentiable approximation sktα deﬁned sktα study effect training dataset. results training model coco pinterestm dataset presented table cell table contains average score evaluation datasets quality image captions varies signiﬁcantly datasets seen figure however conclude relation models preserved regardless dataset used training pinmodela always worse vete-rnn turn worse vete-bow. previous methods transferring knowledge images text focused improving wordlevel embeddings. sentence representation could created combining them. work learn sentence embeddings whole best performing text encoder turned bow. raises following question could model perform equally well train word-level combine word embeddings inference? comparison approaches presented table clearly shows beneﬁt sentence-level training. effect studied further separately training word embeddings forces close corresponding images training sentence level gives opportunity word embeddings become complementary explaining part image capturing co-occurences. studied improve text embeddings leveraging multimodal datasets using pre-trained image model paired text-image datasets. showed vete simple approach directly optimizes phrase embeddings match corresponding image representations outperforms previous multimodal approaches sometimes complex optimize word embeddings opposed sentence embeddings. also showed even relatively complex similarity tasks sentence levels proposed models create competitive embeddings even compared sophisticated models trained orders magnitude text data especially vocabulary related visual concepts. initial surprise state-of-the-art encoder models like lstms performed signiﬁcantly worse much simpler encoders like bag-of-word models. achieve better results evaluated data distribution embeddings transfer well text distributions. general embeddings need robust distribution shifts applying techniques probably improve results. using multimodal approach order improve general text embeddings under-explored hope results motivate developments. example fact best models simple suggests large headroom direction.", "year": 2017}