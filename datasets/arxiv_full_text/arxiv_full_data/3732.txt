{"title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of  International Relations", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SI", "stat.AP"], "abstract": "We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form \"country $i$ took action $a$ toward country $j$ at time $t$.\" BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to \"topics\" of action types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.", "text": "introduce bayesian poisson tucker decomposition modeling country– country interaction event data. data consist interaction events form country took action toward country time bptd discovers overlapping country– community memberships including number latent communities. addition discovers directed community–community interaction networks speciﬁc topics action types temporal regimes. show bptd yields efﬁcient mcmc inference algorithm achieves better predictive performance related models. also demonstrate discovers interpretable latent structure agrees knowledge international relations. like inhabitants countries interact another consult negotiate trade threaten ﬁght. interactions seldom uncoordinated. rather connected fabric overlapping communities security coalitions treaties trade cartels military alliances. example opec coordinates petroleum export policies thirteen member countries laia fosters trade among latin american countries nato guarantees collective defense attacks external parties. single country belong multiple communities reﬂecting different identities. example venezuela— oil-producing country latin american country—is member opec laia. venezuela interacts countries sometimes opec member sometimes laia member. countries engage within-community betweencommunity interactions. example acting opec member venezuela consults opec countries trades non-opec oil-importing countries. moreover although venezuela engages betweencommunity interactions trading opec member engages within-community interactions trading laia member. understand predict countries interact must account community memberships memberships inﬂuence actions. paper take approach learning overlapping communities interaction events form country took action toward country time data interaction events represented either event tokens tensor event type counts series weighted multinetworks. models token representation naturally yield efﬁcient inference algorithms models tensor representation exhibit good predictive performance models network representation learn latent structure aligns well-known concepts communities. previous models interaction event data used subset representations. approach—bayesian poisson tucker decomposition —takes advantage three. represent data interaction events event tokens single token indicates sender country took action toward receiver country time step alternatively aggregate event tokens four-dimensional tensor element count number events type tensor sparse event types never actually occur practice. finally equivalently view count tensor series weighted multinetwork snapshots weight edge a−→j snapshot positive real num− bers. factors capture rates countries participate communities respectively; factor captures strength association between action topic captures well regime explains events time step collectively view country–community factors latent factor matrix represents country community memberships. similarly view action–topic factors time-step–regime factors latent factor matrices respectively. factor captures rate community takes tions associated topic toward community regime factors form core tensor interacts communities topics regimes. shape rate parameters speciﬁc country place uninformative gamma prior shape rate parameters hierarchical prior enables bptd express heterogeneity countries’ rates activity. example expect engage interactions burundi. bptd learns single embedding countries communities preserves traditional network-based notion community membership. sender–receiver asymmetry captured core tensor figure latent structure learned bptd country– country interaction events right community–community interaction network speciﬁc single topic action types temporal regime. topic places mass intend cooperate consult actions network represents cooperative community–community interactions. strongest between-community interactions left depicts overlapping community memberships single country. show countries whose strongest community membership either community ordered countries accordingly. countries strongly associated community highlighted red; countries associated community highlighted green; countries associated community highlighted purple. bottom right country colored according strongest community membership. latent communities strong geographic interpretation. bptd builds classic tucker decomposition factorize tensor event type counts three factor matrices four-dimensional core tensor factor matrices embed countries communities action types topics time steps regimes. core tensor interacts communities topics regimes. country–community factors enable bptd learn overlapping community memberships core tensor enables learn directed community–community interaction networks speciﬁc topics action types temporal regimes. figure illustrates structure. bptd leads efﬁcient mcmc inference algorithm achieves better predictive performance related models finally bptd discovers interpretable latent structure agrees knowledge international relations model closely related poisson-based model schein explicitly uses canonical polyadic tensor decomposition factorize count tensor four latent factor matrices. factor matrices jointly embed senders receivers action types time steps q-dimensional space although schein al.’s model expressed terms tensor event type counts relationship multinomial poisson distributions means also express terms event tokens. yields equation similar equation conversely dubois smyth’s model expressed tensor decomposition. equivalence analogous relationship poisson matrix factorization latent dirichlet allocation adding per-class weight reveals decomposition special case tucker decomposition cardinalities latent dimensions equal offdiagonal elements core tensor zero. dubois smyth’s schein al.’s models therefore highly constrained special cases bptd cannot capture dimension-speciﬁc structure communities countries topics action types. models require latent class jointly summarize information senders receivers action types time steps. requirement conﬂates communities countries topics action types thus forcing class capture potentially redundant information. moreover deﬁnition decomposition models cannot express between-community interactions cannot express sender–receiver asymmetry withlearning completely separate latent factor matrices view compression count tensor allowing on-diagonal elements denote offdiagonal elements non-zero core tensor represent withinbetween-community interactions. community positive weights capture rates withinbetweencommunity interaction respectively. topic positive weight regime positive weight place uninformative prior within-community interaction rates gamma shrinkc priors weights priors bias bptd toward learning latent structure sparse. finally assume drawn uninformative gamma prior topic weights corresponding gamma process similarly regime weights corresponding time-step–regime factors constitute draw another gamma process. withinbetween-community interaction weights corresponding country–community factors conθc marked gamma process mark associated atom view elements core tensor corresponding factors draw θcθdφkψr gamma process provided expected core tensor elements ﬁnite. multirelational gamma process extends relational gamma process zhou proposition limit expected core tensor elements ﬁnite equal connections previous work poisson decomposition dubois smyth developed model assigns event token latent classes class characterized three categorical distributions—θ ball al.’s model nonparametric introduced poisson–bernoulli distribution link binary data poisson likelihood principled fashion. model elements core matrix corresponding factors constitute draw relational gamma process. non-poisson tucker decomposition researchers sometimes refer poisson rate equation being bilinear equivalently written nickel introduced rescal— non-probabilistic bilinear model binary data achieves state-of-the-art performance relation extraction. nickel introduced several extensions extracting relations different types. bilinear models rescal extensions special cases tucker decomposition. given observed count tensor inference bptd involves inverting generative process obtain posterior distribution parameters conditioned hyperparameters posterior distribution analytically intractable; however approximate using posterior samples. draw samples using gibbs sampling repeatedly resampling value parameter conditional posterior given current values parameters. express parameter’s conditional posterior closed form using gamma–poisson conjugacy auxiliary variable techniques zhou carin provide conditional posteriors supplementary material. senders receivers. limitations make hard interpret models learning community memberships. inﬁnite relational models inﬁnite relational model kemp also learns latent structure speciﬁc dimension m-dimensional tensor; however unlike bptd elements tensor binary indicating presence absence corresponding event type. therefore uses bernoulli likelihood. schmidt mørup extended model tensor event counts replacing bernoulli likelihood poisson likelihood respective community assignments countries topic assignment action regime assignment time step model refer gamma–poisson allocates m-dimensional event types m-dimensional latent classes—e.g. allocates tokens type class constraint poisson rates equations equal. unlike bptd gpirm single-membership model. addition cannot express heterogeneity rates activity countries action types time steps. latter limitation remedied letting θizi θjzj φaza ψtzt positive real numbers. refer variant gpirm degree-corrected gpirm stochastic block models generalizes stochastic block model nowicki snijders learns latent structure binary networks. although originally speciﬁed using bernoulli likelihood karrer newman introduced alternative speciﬁcation uses poisson likelihood λc−→d positive real number. like gpirm single-membership model cannot express heterogeneity countries’ rates activity. airoldi addressed former limitation letting meanwhile karrer newman addressed latter limitation allowing θizi θjzj positive real numbers much like dcgpirm. ball simultaneously addressed limitations letting constrained λc−→d λd−→c. finally zhou extended figure compositional allocation. clarity show allocation process three-dimensional count tensor observed three-dimensional event tokens compositionally allocated three-dimensional latent classes m-dimensional latent class. figure illustrates process. decomposition models dubois smyth schein permit noncompositional allocation. example bptd allocates token four-dimensional latent class schein al.’s model allocates one-dimensional latent class cannot decomposed. therefore c×c×k×r bptd yields faster allocation inference algorithm schein al.’s model. data come integrated crisis early warning system boschee global database events language tone leetaru schrodt icews gdelt conﬂict mediation event observations hierarchy senders receivers actions. top-level cameo coding senders receivers country afﬁliation lower levels hierarchy incorporate speciﬁc attributes like sectors religious ethnic afﬁliations. studying international relations using cameo-coded event data researchers usually consider senders’ receivers’ countries. countries represented icews include nonuniversally recognized states occupied palestinian territory former states former yugoslav republic macedonia; countries gdelt. level actions analyses consists twenty action classes roughly ranked according overall sentiment. example negative —use unconventional mass violence. cameo divides actions quadclass scheme verbal cooperation material cooperation verbal conﬂict material conﬂict ﬁrst action neutral. token’s class assignment auxiliary latent variable. using representation computing latent sources simply involves allocating event tokens classes much like inference algorithm dubois smyth’s model aggregating using equation conditional posterior token’s class assignment computing normalizing constant na¨ıvely involves operations; however latent class composed four separate dimensions improve efﬁciency. instead compute compositional allocation using equations improves computational efﬁciency signiﬁcantly na¨ıve non-compositional allocation using equations practice large values approximate nonparametric interpretation bptd. example computing normalizing constant equation using equation requires times number operations implied equation proposition m-dimensional core tensor elements computing normalizing constant using non-compositional allocation requires times number operations required compute using compositional allocation. prove proposition supplementary material. bptd poisson-based models yield allocation inference algorithms take advantage inherent sparsity data scale number event tokens. contrast non-poisson tensor decomposition models lead algorithms scale size count tensor. allocation-based inference bptd especially efﬁcient compositionally allocates m-dimensional event token predictive analysis baseline models compared bptd’s predictive performance three baseline models described section gpirm dcgpirm bayesian poisson tensor factorization model schein three models poisson likelihood hyperparameters bptd—i.e. recommended gelman parameterization encourages elements core tensor sparse. implemented mcmc inference algorithm model. provide full generative process three models supplementary material. gpirm dcgpirm tucker decomposition models thus allocate events four-dimensional latent classes. cardinalities latent dimensions bptd’s—i.e. contrast bptf decomposition model thus allocates events one-dimensional latent classes. cardinality dimension total number latent factors bptf’s likelihood equal total number latent factors bptd’s likelihood—i.e. chose bptf bptd number latent classes—i.e. bptf permit non-compositional allocation mcmc inference becomes slow even moderate values decomposition models also tend overﬁt large throughout predictive experiments values well-supported data explain section experimental setup constructed twelve different observed tensors—six icews gdelt. five tensors source correspond one-year time spans monthly time steps starting ending sixth corresponds ﬁve-year time span monthly time steps spanning divided tensor training tensor train test tensor test divided test tensor held-out portion observed portion binary mask. experimented different masks treats elements involving active ﬁfteen countries held-out portion remaining elements observed portion opposite. ﬁrst mask enabled evaluate models’ reconstructions densest portion test tensor second mask enabled evaluate reconstructions complement. across entire gdelt database example elements involving active ﬁfteen countries—i.e. countries—account event tokens. moreover elements non-zero. non-zero elements highly dispersed variance-to-mean ratio contrast elements involving countries nonzero. elements variance-to-mean ratio combination four models twelve tensors masks iterations mcmc inference training tensor. clamped country–community factors action–topic factors core tensor inferred time-step–regime factors test tensor using observed portion running iterations mcmc inference. saved every tenth sample ﬁrst used sample along country– community factors action–topic factors core tensor compute poisson rate element held-out portion test tensor. finally averaged rates across samples used element’s average rate compute probability. combined heldelements’ probabilities taking geometric mean equivalently computing inverse perplexity. chose combination strategy ensure models penalized heavily making poor predictions non-zero elements rewarded excessively making good predictions zero elements. clamping country–community factors action–topic factors core tensor training experimental setup analogous used assess collaborative ﬁltering models’ strong generalization ability results figure illustrates results combination four models twelve tensors masks. contains results twelve experiments involving ﬁrst mask elements involving active ﬁfteen countries treated held-out portion. bptd outperformed baselines significantly. bptf—itself state-of-the-art model—performed better bptd experiment. general tucker decomposition allows bptd learn richer latent structure generalizes better held-out data. bottom contains results experiments involving second mask. models’ performance closer experiments probably large proportion easy-to-predict zero elements. bptd bptf performed indistinguishably experiments models outperformed gpirm dcgpirm. single-membership nature gpirm dcgpirm prevents expressing high levels heterogeneity countries’ rates activity. heldelements highly dispersed models sometimes made extremely inaccurate predictions. contrast mixed-membership nature bptd bptf allows better express heterogeneous rates activity. figure predictive performance. plot shows inverse perplexity four models gpirm dcgpirm bptf bptd experiments depicted treated elements involving active countries held-out portion; experiments depicted bottom treated remaining elements held-out portion. ease comparison scaled inverse perplexities zero one; give scales top-left corners plots. bptd outperformed baselines signiﬁcantly predicting denser portion test tensor used tensor icews events spanning monthly time steps explore latent structure discovered bptd. initially —i.e. latent classes— used shrinkage priors adaptively learn appropriate numbers communities topics regimes. found communities topics weights signiﬁcantly greater zero. provide plot community weights supplementary material. although three regimes non-zero weights much larger weight two. comparison schein used ﬁfty latent classes model data hoff used model similar tensor gdelt. topics action types show inferred action–topic factors heatmap left subplot ﬁgure ordered topics weights heatmap. inferred topics correspond closely cameo’s quadclass scheme. moving left right topics place mass increasingly negative actions. topics place mass verbal cooperation actions; topic places mass material cooperation actions neutral —make statement action; topic places mass verbal conﬂict actions —make statement action; topics place mass material conﬂict actions. topic-partitioned community–community networks right subplot ﬁgure visualize inferred community structure topic active regime bottom-left heatmap community–community interaction network top-left heatmap depicts rate country acts sender community c—i.e. similarly bottom-right heatmap depicts rate country acts receiver community. top-right heatmap depicts number times country took action associated topic toward country ak−→jd grouped countries strongest community memberships ordered communities withincommunity interaction weights smallest largest; thin green lines separate countries strongly associated community countries strongly associated adjacent communities. communities contain strongly associated countries. example community contains community contains china community contains russia belarus. communities mostly engage between-community interaction. larger communities communities mostly engage within-community interaction. communities strong geographic interpretation. moving upward bottom communities correspond eastern europe east africa south-central africa latin america australasia central europe central asia etc. community–community interaction network summarizes patterns top-right heatmap. topic dominated –consult action network symmetric; negative topics asymmetric community–community interaction networks. therefore hypothesize cooperation inherently reciprocal type interaction. provide visualizations topics supplementary material. presented bayesian poisson tucker decomposition learning latent structure international relations country–country interaction events form country took action toward country time unlike previous models bptd takes advantage three representations interaction event data event tokens tensor event type counts series weighted multinetwork snapshots. bptd uses poisson figure left action–topic factors. topics ordered right latent structure discovered bptd topic active regime including community–community interaction network rate country acts sender receiver community number times country took action associated topic toward country regime show active countries. likelihood respecting discrete nature data inherent sparsity. moreover bptd yields compositional allocation inference algorithm efﬁcient non-compositional allocation algorithms. bptd tucker decomposition model shares parameters across latent classes. contrast decomposition models force latent class capture potentially redundant information. bptd therefore does less. efﬁciency reﬂected predictive analysis bptd outperforms bptf—a decomposition model—as well baselines. bptd learns interpretable latent structure aligns well-known concepts networks literature. speciﬁcally bptd learns latent country– community memberships including number communities well directed community–community interaction networks speciﬁc topics action types temporal regimes. structure captures complexity country–country interactions revealing patterns agree knowledge international relations. finally although presented bptd context interaction events bptd well suited learning latent structure types multidimensional count data. thank abigail jacobs brandon stewart helpful discussions. work supported iis- iis-; n--; darpa fa--- n--c; adobe; john templeton foundation; sloan foundation; umass amherst center intelligent information retrieval. opinions ﬁndings conclusions recommendations expressed material authors’ necessarily reﬂect sponsors. cichocki zdunek phan amari nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separation. john wiley sons dubois smyth modeling relational events proceedings sixteenth latent classes. sigkdd international conference knowledge discovery data mining gerner schrodt abu-jabr yilmaz conﬂict mediation event observations event data framework analysis foreign policy interactions. working paper. gopalan ruiz ranganath blei bayesian nonparametric poisson factorization proceedings sevrecommendation systems. enteenth international conference artiﬁcial intelligence statistics volume kemp tenenbaum grifﬁths yamada ueda learning systems concepts inﬁnite relational model. proceedings twenty-first national conference artiﬁcial intelligence nickel murphy tresp gabrilovich relational machine learning knowledge graphs multi-relational link prediction automated knowledge graph construction. arxiv. tucker extension factor analysis threedimensional matrices. frederiksen gulliksen contributions mathematical psychology. holt rinehart winston inﬁnite tucker decomposition nonparametric bayesian models multiway data analysis. proceedings twenty-ninth international conference machine learning zhao zhang cichocki bayesian factorization incomplete tensors automatic rank determination. ieee transactions pattern analysis machine intelligence inﬁnite edge partition models overlapping proceedcommunity detection link prediction. ings eighteenth international conference artiﬁcial intelligence statistics", "year": 2016}