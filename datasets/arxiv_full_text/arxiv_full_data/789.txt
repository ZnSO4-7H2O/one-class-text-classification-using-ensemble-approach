{"title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and  Knowledge", "tag": ["cs.AI", "cs.LG", "cs.LO", "cs.NE"], "abstract": "We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.", "text": "abstract. propose logic tensor networks uniform framework integrating automatic learning reasoning. logic formalism called real logic deﬁned ﬁrst-order language whereby formulas truth-value interval semantics deﬁned concretely domain real numbers. logical constants interpreted feature vectors real numbers. real logic promotes well-founded integration deductive reasoning knowledge-base efﬁcient data-driven relational machine learning. show real logic implemented deep tensor neural networks google’s tensorflowtm primitives. paper concludes experiments applying logic tensor networks simple representative example knowledge completion. recent availability large-scale data combining multiple data modalities image text audio sensor data opened various research commercial opportunities underpinned machine learning methods techniques particular recent work machine learning sought combine logical services knowledge completion approximate inference goal-directed reasoning data-driven statistical neural network-based approaches. argue great possibilities improving current state machine learning artiﬁcial intelligence thought principled combination knowledge representation reasoning learning. guha’s recent position paper case point advocates model theory real-valued numbers. paper take inspiration recent work also less recent work area neuralsymbolic integration semantic attachment symbol grounding achieve vector-based representation shown adequate integrating machine learning reasoning principled way. ﬁrst author acknowledges mobility program supporting long term visit city university london. also acknowledges nvidia corporation supporting research donation gpu. paper proposes framework called logic tensor networks integrates learning based tensor networks reasoning using ﬁrst-order manyvalued logic implemented tensorflowtm enables ﬁrst time range knowledge-based tasks using rich knowledge representation ﬁrstorder logic combined efﬁcient data-driven machine learning based manipulation real-valued vectors. given data available form real-valued vectors logical soft hard constraints relations apply certain subsets vectors speciﬁed compactly ﬁrst-order logic. reasoning constraints help improve learning learning data revise constraints thus modifying reasoning. adequate vector-based representation logic ﬁrst proposed paper enables integration learning reasoning detailed follows. interested providing computationally adequate approach implementing learning reasoning integrated within idealized agent. agent manage knowledge unbounded possibly inﬁnite objects objects associated quantitative attributes represented n-tuple real values call grounding. example person grounding -tuple containing numerical representation person’s name height weight number friends social network. object tuples participate relations denotes arity relation presuppose existence latent relation numerical properties i.e. groundings partial relational structure starting partial knowledge agent required infer knowledge relational structure objects predict numerical properties class objects classes relations normally independent. example case object class related another object relation object class logic ∀x∃y whether holds depend application reasoning derive otherwise might evidence training examples only; learning need revise conclusion examples contrary become available. vectorial representation proposed paper permits reasoning learning exempliﬁed detailed next section. forms reasoning learning integrated unifying framework implemented within tensor networks exempliﬁed relational domains combining data relational knowledge objects. expected that adequate integration numerical properties relational knowledge differently immediate related literature framework introduced paper capable combining effective ﬁrst-order logical inference open domains efﬁcient relational multi-class learning using tensor networks. sentational power ﬁrst-order logic instantiates framework using tensor networks efﬁcient implementation shows proposed vector-based representation logic offers adequate mapping symbols real-world manifestations appropriate rich inference learning examples. paper organized follows. section deﬁne real logic. section propose learning-as-inference framework. section instantiate framework showing real logic implemented deep tensor neural networks leading logic tensor networks section contains example handles knowledge completion using data knowledge well-known smokers friends experiment. section concludes paper discusses directions future work. start ﬁrst order language whose signature contains constant symbols functional symbols predicate symbols. sentences used express relational knowledge e.g. atomic formula states objects related binary relation ∀xy. states symmetric relation variables; ∃y.r states object related object simplicity without loss generality assume logical sentences prenex conjunctive skolemised normal form e.g. sentence ∃yr) transformed equivalent clause function symbol. semantics deviate standard abstract semantics propose concrete semantics sentences interpreted tuples real numbers. emphasise fact interpreted real world term grounding denoted instead standard interpretation. example suppose documents deﬁned ﬁnite dictionary words. language contains binary function symbol concat denoting document resulting concatenation documents contain also binary predicate supposed true document deemed similar document example grounding associates document bag-of-words vector consequence natural grounding concat function would vectors predicate cosine similarity vectors. formally instance three documents john studies logic plays football mary plays football logic games john mary play football study logic together ={john mary football game logic play study together} following examples grounding terms atomic formulas clauses. previous deﬁniiton follows checking satisﬁable amounts seaching extension partial grounding space possible groundings instantiations clauses satisﬁed w.r.t. speciﬁed interval. clearly unfeasible practical point view. usual must restrict space grounding clause instantiations. consider turn check satisﬁability subset functions real numbers recall grounding capture latent correlation quantitative attributes object relational properties. particular interested searching within speciﬁc class functions paper based tensor networks although family functions considered. limit number clause instantiations general might inﬁnite since admits function symbols usual approach consider instantiations clause certain depth grounded theory inconsitent grounding satisﬁes interested ﬁnding grounding satisﬁes much possible ˆgi. want grounding minimizes satisﬁability error. error occurs grounding assigns value clause outside interval prescribed measure error deﬁned minimal distance points interval example whether document classiﬁed ﬁeld artiﬁcial intelligence depends bag-of-words grounding. language contains unary predicate standing paper grounding function bag-of-words vectors assign values close vectors close semantically furthermore vectors similar grounding similar. speciﬁc instances real logic obtained selectiong space groundings speciﬁc s-norm interpretation disjunction. section describe realization real logic space real tensor transformations order space function symbols interpreted linear transformations. precisely function symbol arity real vectors corresponding grounding terms written tensor rmn×mn×k matrix rk×mn vector sigmoid function. encoding grounding clause determined neural network ﬁrst computes grounding literals contained clause combines using speciﬁc s-norm. example tensor network shown figure architecture generalization structure proposed shown rather effective task knowledge compilation also presence simple logical constraints. tensor network formulation parameters learned minimizing loss function equivalently maximize satisﬁability clause logic tensor networks implemented python library called using google’s tensorflowtm test idea section well-known friends smokers example illustrate task knowledge completion ltn. people divided groups within group people complete knowledge smoking habits. ﬁrst group complete knowledge cancer. second group known persons. knowledge friendship relation complete within group symmetry friendship assumed. otherwise imcomplete known that e.g. friend known whether friend finally also general knowledge smoking friendship cancer namely smoking causes cancer friendship normally symmetric anti-reﬂexive relation everyone friend smoking propagates among friends. knowledge represented knowledge-bases shown figure facts contained knowledge-bases different degrees truth known. otherwise combined knowledge-base would inconsistent ¬s). main task complete knowledgebase degree truth facts contained truth-value missing facts e.g. grounding constant symbol answer grounding best normally probabilistic approach taken solve problem requires instantiating clauses remove variables essentially turning problem propositional one; takes different approach. approximates complete start assuming facts contained knowledge-base true show role background knolwedge learning-inference process experiments. ﬁrst seek complete consisting factual knowledge kexp i...n. second also include background knowledge kexp kexp confgure network follows constant real-valued features. number layers tensor network regularization parameter purpose illustration lukasiewicz t-norm s-norm harmonic mean aggregation operator. estimation optimal grounding obtained runs rmsprop learning algorithm available tensorflowtm results experiments reported table readability boldface truth-values greater truth-values facts listed knowledge-base highlighted background color knowledgebase figure values white background result knowledge completion produced learning-inference procedure. evaluate quality results check whether truth-values facts listed indeed close truth-values associated knowledge completion correspond expectation. initial analysis shows associated kexp produces facts kexp itself. words data. however also learns infer additional positive negative facts derivable kexp pure logical reasoning; example facts derived exploiting similarities groundings constants generated ltn. instance happen present high cosine similarity measure. result facts friendship relations affect friendship relations vice-versa instance level satisﬁability associated kexp indicates kexp classically satisﬁable. results second experiment show facts learned inclusion background knowledge. example predicts true. similarly symmetry friendship relation concludes friend expected. fact axioms generic background knowledge satisﬁed degree satisﬁability higher apart smoking causes cancer axiom responsible classical inconsistency since data smoke cancer degree satisﬁability recent note guha advocates need model theory distributed representations note sketches proposal terms predicates interpreted points/vectors n-dimensional real space. computation truth-value atomic formulae obtained comparing projections vector associated associated real logic shares idea terms must interpreted geometric space. however different interpretation functions predicate symbols. real logic general semantics proposed implemented within single layer since operation projection comparison necessary compute truth-value encoded within matrix constraint encoded easily ltn. real logic orthogonal approach taken markov logic networks variations mlns level truth formula determined number models satisfy formula models higher degree truth. hybrid mlns introduce dependency real features associated constants given learned. real logic instead level truth complex formula determined logical reasoning relations features different objects learned error minimization. another difference mlns work closed world assumption real logic open domain. much work done also neuro-fuzzy approaches essentially propositional real logic ﬁrst-order. bayesian logic open domain respect similar real logic ltns. instead taking explicit probabilistic approach ltns draw efﬁcient approach used tensor networks knowledge graphs already discussed. ltns probabilistic interpretation requirement. statistical probabilistic approaches lifted inference fall category including probabilistic variations inductive logic programming normally restricted horn clauses. metainterpretive together blog seem closer ltns concerns knowledge representation language explore beneﬁts tensor networks computational efﬁciency. approach embedding logical knowledge onto data purpose relational learning similar real logic presented real logic share idea interpreting logical alphabet n-dimensional real space. terminologically term grounding real logic corresponds embeddings however several differences. first uses function-free langauges provide also groundings functional symbols. second model used compute truth-values atomic formulas adopted special case general model proposed paper finally semantics universal existential quantiﬁers adopted based closed-world assumption i.e. universally quantiﬁed formulas reduced ﬁnite conjunctions possible instantiations; real logic make cwa. furthermore real logic assume speciﬁc t-norm. framework learning presence logical constraints. ltns share idea logical constraints training examples treated uniformly supervisions learning algorithm. introduces novelties ﬁrst existential quantiﬁers grounded ﬁnite disjunction scolemized. words required existentially quantiﬁed formulas satisﬁed individuals. second allows generate data prediction. instance grounded theory contains formula ∀x∃yr generates real function every vector returns feature vector intuitively interpreted features typical object takes part relation object features equal finally related work domain neural-symbolic computing neural network ﬁbring sought combine neural networks gain efﬁciency forms knowledge representation propositional modal logic logic programming. tightly-coupled approaches. contrast ltns richer language exploit beneﬁts knowledge compilation tensor networks within looselycoupled approach might even offer adequate representation equality logic. experimental evaluations comparison neural-symbolic approaches desirable though including latest developments ﬁeld good snapshot found proposed real logic uniform framework learning reasoning. approximate satisﬁability deﬁned learning task knowledge data being mapped onto real-valued vectors. inference-as-learning approach relational knowledge constraints state-of-the-art data-driven approaches integrated. showed real logic implemented deep tensor networks call logic tensor networks applied efﬁciently knowledge completion data prediction tasks. future work make implementation available tensorflowtm apply large-scale experiments relational learning benchmarks comparison statistical relational learning neural-symbolic computing inductive logic programming approaches.", "year": 2016}