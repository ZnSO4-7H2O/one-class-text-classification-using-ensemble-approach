{"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal  Abstraction and Intrinsic Motivation", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.", "text": "learning goal-directed behavior environments sparse feedback major challenge reinforcement learning algorithms. primary diﬃculty arises insuﬃcient exploration resulting agent unable learn robust value functions. intrinsically motivated agents explore behavior sake rather directly solve problems. intrinsic behaviors could eventually help agent solve tasks posed environment. present hierarchical-dqn framework integrate hierarchical value functions operating diﬀerent temporal scales intrinsically motivated deep reinforcement learning. top-level value function learns policy intrinsic goals lower-level function learns policy atomic actions satisfy given goals. h-dqn allows ﬂexible goal speciﬁcations functions entities relations. provides eﬃcient space exploration complicated environments. demonstrate strength approach problems sparse delayed feedback complex discrete stochastic decision process classic atari game ‘montezuma’s revenge’. learning goal-directed behavior sparse feedback complex environments fundamental challenge artiﬁcial intelligence. learning setting requires agent represent knowledge multiple levels spatio-temporal abstractions explore environment eﬃciently. recently non-linear function approximators coupled reinforcement learning made possible learn abstractions highdimensional state spaces task exploration sparse feedback still remains major challenge. existing methods like boltzmann exploration thomson sampling oﬀer signiﬁcant improvements \u0001-greedy limited underlying models functioning level basic actions. work propose framework integrates deep reinforcement learning hierarchical value functions agent motivated solve intrinsic goals exploration. goals provide eﬃcient exploration help mitigate sparse feedback problem. additionally observe goals deﬁned space entities relations help signiﬁcantly constrain exploration space data-eﬃcient learning complex environments. reinforcement learning formalizes control problems ﬁnding policy maximizes expected future rewards value functions central cache utility state achieving agent’s overall objective. recently value functions also generalized order represent utility state achieving given ﬁrst learn ways achieve intrinsically generated goals subsequently learn optimal policy chain together. value functions used generate policy terminates agent reaches goal state collection policies hierarchically arranged temporal dynamics learning planning within framework semi-markov decision processes high-dimensional problems value functions approximated neural networks propose framework hierarchically organized deep reinforcement learning modules working diﬀerent time-scales. model takes decisions levels hierarchy level module takes state picks goal lower-level module uses state chosen goal select actions either goal reached episode terminated. meta-controller chooses another goal steps repeat. train model using stochastic gradient descent diﬀerent temporal scales optimize expected future intrinsic extrinsic rewards demonstrate strength approach problems long-range delayed feedback discrete stochastic decision process long chain states receiving optimal extrinsic rewards classic atari game even longer-range delayed rewards existing state-of-art deep reinforcement learning approaches fail learn policies data-eﬃcient manner. learning operating diﬀerent levels temporal abstraction challenge tasks involving long-range planning. context reinforcement learning sutton proposed options framework involves abstractions space actions. step agent chooses either one-step primitive action multi-step action policy option deﬁnes policy actions terminated according stochastic function thus traditional setting extended semi-markov decision process options. recently several methods proposed learn options real-time using varying reward functions composing existing options value functions also generalized consider goals along states universal value function provides universal option approximately represents optimal behavior towards goal work inspired papers builds upon them. also work option discovery tabular value function setting recent work machado presented option discovery algorithm agent encouraged explore regions previously reach. however option discovery non-linear state approximations required still open problem. related work hierarchical formulations include model dayan hinton consisted managers taking decisions various levels granularity percolating atomic actions made agent. maxq framework built work decompose value function combinations value functions smaller constituent mdps guestrin factored formulation. hernandez-gardiol mahadevan combined hierarchical variable length short-term memory high-level decisions. work propose scheme temporal abstraction involves simultaneously learning options control policy compose options deep reinforcement learning setting. approach separate q-functions option instead treats option part input similar advantages shared nature origin ‘good’ intrinsic reward functions open question reinforcement learning. singh explored agents intrinsic reward structures order learn generic options apply wide variety tasks. using notion salient events sub-goals agent learns options events. another paper singh take evolutionary perspective optimize space reward functions agent leading notion extrinsically intrinsically motivated behavior. context hierarchical goel huber discuss framework subgoal discovery using structural aspects learned policy model. s¸im¸sek provide graph partioning approach subgoal identiﬁcation. schmidhuber provides coherent formulation intrinsic motivation measured improvements predictive world model made learning algorithm. mohamed rezende recently proposed notion intrinsically motivated learning within framework mutual information maximization. frank demonstrate eﬀectiveness artiﬁcial curiosity using information gain maximization humanoid robot. object-based representations exploit underlying structure problem proposed alleviate curse dimensionality diuk propose object-oriented using representation based objects interactions. deﬁning state value assignments possible relations objects introduce algorithm solving deterministic object-oriented mdps. representation similar guestrin describe object-based representation context planning. contrast approaches representation require explicit encoding relations objects used stochastic domains. recent advances function approximation deep neural networks shown promise handling high-dimensional sensory input. deep q-networks variants successfully applied various domains including atari games still perform poorly environments sparse delayed reward signals. strategies prioritized experience replay bootstrapping proposed alleviate problem learning sparse rewards. approaches yield signiﬁcant improvements prior work struggle reward signal long delayed horizon. exploration strategy suﬃcient agent obtain required feedback. nature origin intrinsic goals humans thorny issue notable insights existing literature. converging evidence developmental psychology human infants primates children adults diverse cultures base core knowledge certain cognitive systems including entities agents actions numerical quantities space social-structures intuitive theories even newborns infants seem represent visual world terms coherent visual entities centered around spatio-temporal principles cohesion continuity contact. also seem explicitly represent agents assumption agent’s behavior goal-directed eﬃcient. infants also discriminate relative sizes objects relative distances higher order numerical relations ratio object sizes. curiosity-driven activities toddlers knowledge generate intrinsic goals building physically stable block structures. order accomplish goals toddlers seem construct sub-goals space core knowledge putting heavier entity lighter entity order build tall blocks. knowledge space also utilized learn hierarchical decomposition spatial environments bottlenecks diﬀerent spatial groupings correspond sub-goals. explored neuroscience successor representation represents value function terms expected future state occupancy. decomposition successor representation yields reasonable sub-goals spatial navigation problems botvinick written general overview hierarchical reinforcement learning context cognitive science neuroscience. model consider markov decision process represented states actions transition function agent operating framework receives state external environment take action results state deﬁne extrinsic reward function objective agent agents eﬀective exploration mdps signiﬁcant challenge learning good control policies. methods \u0001-greedy useful local exploration fail provide impetus agent explore diﬀerent areas state space. order tackle this utilize temporal abstraction options deﬁne policies goal agent learns option policies simultaneously along learning optimal sequence goals follow. order learn agent also critic provides intrinsic rewards based whether agent able achieve goals temporal abstractions shown figure agent uses two-stage hierarchy consisting controller meta-controller. meta-controller receives state chooses goal denotes possible current goals. controller selects action using goal remains place next time steps either achieved terminal state reached. internal critic responsible evaluating whether goal reached providing appropriate reward controller. objective function controller maximize cumulative intrinsic −trt. similarly objective meta-controller −tft reward signals also view setup similar optimizing space optimal reward functions maximize ﬁtness case reward functions dynamic temporally dependent sequential history goals. figure provides illustration agent’s hierarchy subsequent time steps. figure overview agent produces actions receives sensory observations. separate deep-q networks used inside meta-controller controller. meta-controller looks states produces policy goals estimating value function controller takes states current goal produces policy actions estimating value function solve predicted goal internal critic checks goal reached provides appropriate intrinsic reward controller. controller terminates either episode ends accomplished. meta-controller chooses process repeats. note transitions generated slower time-scale transitions generated represent using non-linear function approximator parameters called deep q-network trained minimizing disjoint memory spaces respectively. denotes training iteration number maxaq following parameters previous iteration held ﬁxed optimising loss function. parameters optimized using gradient learning algorithm learn parameters h-dqn using stochastic gradient descent diﬀerent time scales experiences controller collected every time step experiences meta-controller collected controller terminates goal drawn \u0001-greedy fashion exploration probability annealed learning proceeds controller every time step action drawn goal using exploration probability dependent current empirical success rate reaching model parameters periodically updated drawing experiences replay perform experiments diﬀerent domains involving delayed rewards. ﬁrst discrete-state stochastic transitions second atari game called ‘montezuma’s revenge’. game setup consider stochastic decision process extrinsic reward depends history visited states addition current state. selected task order demonstrate importance intrinsic motivation exploration environments. possible states agent always starts agent moves left deterministically chooses left action; action right succeeds time resulting left move otherwise. terminal state agent receives reward ﬁrst visits reward going without visiting modiﬁed version reward structure adding complexity task. process illustrated figure epsgreedy execute obtain next state extrinsic reward environment obtain intrinsic reward internal critic store transition updateparamsd) updateparamsd) consider state possible goal exploration. encourages agent visit state hence learn optimal policy. goal agent receives positive intrinsic reward reaches corresponding state. results compare performance approach q-learning baseline terms average extrinsic reward gained episode. experiments parameters annealed steps. learning rate figure plots evolution reward methods averaged diﬀerent runs. expected q-learning unable optimal policy even epochs converging sub-optimal policy reaching state directly obtain reward contrast approach hierarchical q-estimators learns choose goals statistically lead agent visit going back therefore agent obtains signiﬁcantly higher average reward around figure illustrates number visits states increases episodes training. data point shows average number visits state last episodes. indicates model choosing goals reaches critical state often. game description consider ‘montezuma’s revenge’ atari game sparse delayed rewards. game requires player navigate explorer several rooms collecting treasures. order pass doors player ﬁrst pick key. player climb ladders right move left towards resulting long sequence actions receiving reward collecting key. this navigating towards door opening results another reward existing deep approaches fail learn environment since agent rarely reaches state non-zero reward. instance basic achieves score even best performing system gorila manages average. setup agent needs intrinsic motivation explore meaningful parts scene learn advantage getting itself. inspired developmental psychology literature object-oriented mdps entities objects scene parameterize goals environment. unsupervised detection objects visual scenes open problem computer vision although recent progress obtaining objects directly image motion data work built custom object detector provides plausible object candidates. controller figure sample screen atari game called ‘montezuma’s revenge’. architecture architecture controller similar architecture produces meta-controller practice networks could share lower level features enforce this. function conﬁgurations entities. experiments agent free choose entity. instance agent deemed completed goal agent entity reaches another entity door. note notion relational intrinsic rewards generalized settings. instance atari game ‘asteroids’ agent could rewarded bullet reaches asteroid simply ship never reaches asteroid. game ‘pacman’ agent could rewarded pellets screen reached. general case potentially model evolve parameterized intrinsic reward function given entities. leave future work. experience replay memories equal respectively. learning rate discount rate follow phase training procedure ﬁrst phase exploration parameter meta-controller train controller actions. eﬀectively leads pre-training controller learn solve subset goals. second phase jointly train controller meta-controller. results figure shows reward progress joint training phase evident model starts gradually learning reach open door reward around episode. shown figure agent learns choose often training proceeds also successful reaching training proceeds observe agent ﬁrst learns perform simpler goals slowly starts learning ‘harder’ goals bottom ladders provide path higher rewards. figure shows evolution success rate goals picked. training ’key’ ’bottom-left-ladder’ ’bottom-right-ladders’ chosen increasingly often. order scale-up solve entire game several ingredients missing automatic discovery objects videos goal parametrization considered ﬂexible short-term memory ability intermittently terminate ongoing options. figure results montezuma’s revenge plots depict joint training phase model. described section ﬁrst training phase pre-trains lower level controller million steps. joint training learns consistently high rewards additional million steps shown goal success ratio agent learns choose often training proceeds successful achieving goal statistics early phases joint training goals equally preferred high exploration training proceeds agent learns select appropriate goals bottom-left door. figure sample gameplay agent montezuma’s revenge four quadrants arranged temporally coherent manner beginning meta-controller chooses goal controller tries satisfy goal taking series level actions fails colliding skull meta-controller chooses bottom-right ladder next goal controller terminates reaching subsequently meta-controller chooses top-right door controller able successfully achieve goals. presented h-dqn framework consisting hierarchical value functions operating diﬀerent time scales. temporally decomposing value function allows agent perform intrinsically motivated behavior turn yields eﬃcient exploration environments delayed rewards. also observe parameterizing intrinsic motivation space entities relations provides promising avenue building agents temporally extended exploration. also plan explore alternative parameterizations goals h-dqn future. current framework several missing components including automatically disentangling objects pixels short-term memory. state abstractions learnt vanilla deep-q-networks structured suﬃciently compositional. recent work using deep generative models disentangle multiple factors variations pixel data. hope work motivates combination deep generative models images h-dqn. additionally order handle longer range dependencies agent needs store history previous goals actions representations. recent work using recurrent networks conjunction reinforcement learning order scale-up approach harder non-markovian settings necessary incorporate ﬂexible episodic memory module. would like thank vaibhav unhelkar ramya ramakrishnan gershman michael littman vlad firoiu whitney kleiman-weiner pedro tsividis critical feedback discussions. grateful receive support center brain machines minds openmind team.", "year": 2016}