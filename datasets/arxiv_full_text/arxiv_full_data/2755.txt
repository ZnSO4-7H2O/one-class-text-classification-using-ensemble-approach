{"title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer  Ordering", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Existing deep multitask learning (MTL) approaches align layers shared between tasks in a parallel ordering. Such an organization significantly constricts the types of shared structure that can be learned. The necessity of parallel ordering for deep MTL is first tested by comparing it with permuted ordering of shared layers. The results indicate that a flexible ordering can enable more effective sharing, thus motivating the development of a soft ordering approach, which learns how shared layers are applied in different ways for different tasks. Deep MTL with soft ordering outperforms parallel ordering methods across a series of domains. These results suggest that the power of deep MTL comes from learning highly general building blocks that can be assembled to meet the demands of each task.", "text": "existing deep multitask learning approaches align layers shared tasks parallel ordering. organization signiﬁcantly constricts types shared structure learned. necessity parallel ordering deep ﬁrst tested comparing permuted ordering shared layers. results indicate ﬂexible ordering enable effective sharing thus motivating development soft ordering approach learns shared layers applied different ways different tasks. deep soft ordering outperforms parallel ordering methods across series domains. results suggest power deep comes learning highly general building blocks assembled meet demands task. multitask learning auxiliary data sets harnessed improve overall performance exploiting regularities present across tasks. deep learning yielded state-ofthe-art systems across range domains increased focus developing deep techniques. techniques applied across settings vision natural language speech reinforcement learning although improve performance single-task learning settings approaches generally constrained joint training relatively and/or closely-related tasks. hand perspective kolmogorov complexity transfer always useful; pair distributions underlying pair tasks must something common principle even tasks superﬁcially unrelated vision beneﬁt sharing words sufﬁciently expressive class models inductive bias requiring model multiple tasks simultaneously encourage learning converge realistic representations. expressivity success deep models suggest ideal candidates improvement mtl. existing approaches deep restricted scope? based assumption learned transformations shared across tasks. paper identiﬁes additional implicit assumption underlying existing approaches deep sharing takes place parallel ordering layers. sharing tasks occurs aligned levels feature hierarchy implied model architecture. constraint limits kind sharing occur tasks. requires subsequences task feature hierarchies match difﬁcult establish tasks become plentiful diverse. paper investigates whether parallel ordering layers necessary deep mtl. alternative introduces methods make deep ﬂexible. first existing approaches reviewed context reliance parallel ordering. then foil parallel ordering permuted ordering introduced shared layers applied different orders different tasks. increased ability permuted ordering support integration information across tasks analyzed results used develop soft ordering approach deep mtl. figure classes existing deep multitask learning architectures. classical approaches task-speciﬁc decoder output core single-task model task; columnbased approaches include network column task deﬁne mechanism sharing columns; supervision custom depths adds output decoders depths based task hierarchy; universal representations adapts layer small number task-speciﬁc scaling parameters. underlying approaches assumption parallel ordering shared layers requires aligned sequences feature extractors across tasks. approach joint model learns apply shared layers different ways different depths different tasks simultaneously learns parameters layers themselves. suite experiments soft ordering shown improve performance single-task learning well ﬁxed order deep methods. importantly soft ordering simply technical improvement thinking deep mtl. learning different soft ordering layers task amounts discovering generalizable modules assembled different ways different tasks. perspective points future approaches train collection layers training tasks assembled novel ways future unseen tasks. striking structural regularities observed natural technological sociological worlds repeatedly observed across settings scales; ubiquitous universal. forcing shared transformations occur matching depths hierarchical feature extraction deep falls short capturing sort functional regularity. soft ordering thus step towards enabling deep realize diverse array structural regularities found across complex tasks drawn real world. designing deep system requires answering question learned parameters shared across tasks? landscape existing deep approaches organized based answer question joint network architecture level classical approaches. neural network ﬁrst introduced case shallow networks deep networks prevalent. idea output neurons predict auxiliary labels related tasks would regularizers hidden representation. many deep learning extensions remain close nature approach learning shared representation high-level layer followed task-speciﬁc decoders extract labels task approach extended task-speciﬁc input encoders underlying single-task model adapted ease task integration core network still shared entirety. column-based approaches. column-based approaches assign task layer task-speciﬁc parameters shared depth deﬁne mechanism sharing parameters tasks shared depth e.g. shared tensor factor across tasks allowing form communication columns observations negative effects sharing columnbased methods attributed mismatches features required depth tasks dissimilar. supervision custom depths. intuitive hierarchy describing tasks related. several approaches integrate supervised feedback task levels consistent hierarchy method sensitive design hierarchy tasks included therein approach learns task-relationship hierarchy training though learned parameters still shared across matching depths. supervision custom depths also extended include explicit recurrence reintegrates information earlier predictions although recurrent methods still rely pre-deﬁned hierarchical relationships tasks provide evidence potential learning transformations different function different tasks different depths i.e. case different depths unrolled time. universal representations. approach shares core model parameters except batch normalization scaling factors number classes equal across tasks even output layers shared small number task-speciﬁc parameters enables strong performance maintained. method applied diverse array vision tasks demonstrating power small number scaling parameters adapting layer functionality different tasks. observation helps motivate method developed section common interpretation deep learning layers extract progressively higher level features later depths natural assumption learned transformations extract features also tied depth learned. core assumption motivating regularities across tasks result learned transformations leveraged improve generalization. however methods reviewed section assumption subsequences feature hierarchy align across tasks sharing tasks occurs aligned depths call parallel ordering assumption. consider tasks learned jointly associated model suppose sharing across tasks occurs consecutive depths. ti’s task-speciﬁc encoder core sharable portion network inputs layer learned weights task shared depth optional nonlinearity. parallel ordering assumption implies approximate equality means shared depth applied weight tensors task similar compatible sharing. example learned parameters shared across closely-related tasks assumption reasonable constraint. however tasks added joint model difﬁcult layer represent features given depth tasks. furthermore distant tasks unreasonable expect task feature hierarchies match even tasks related intuitively. conjecture explored paper parallel ordering limits potential deep strong constraint enforces layer. foil parallel ordering assumption permuting shared layers consider common deep setting hard-sharing layers layer {wk}d shared entirety across tasks. baseline deep model task given figure fitting random tasks. dotted lines show permuted ordering samples well parallel linear networks; relu networks permuted ordering enjoys similar advantage. thus permuted ordering shared layers eases integration information across disparate tasks. setup satisﬁes parallel ordering assumption. consider alternative scheme equivalent above except learned layers applied different orders different task. task-speciﬁc permutation size ﬁxed training. sets tasks joint training model deﬁned achieves similar improved performance parallel ordering necessary requirement deep mtl. course formulation required applied order. section examples possible generalizations. note multitask permuted ordering differs approach training layers multiple orders single task. single-task case results model increased commutativity layers behavior also observed residual networks whereas result layers assembled different ways different tasks. fitting tasks random patterns. permuted ordering evaluated comparing parallel ordering tasks. randomly generated tasks disparate possible tasks share minimal information thus help build intuition permuting layers could help integrate information broad settings. following experiments investigate accurately model jointly tasks samples. data task drawn uniformly drawn uniformly shared learned afﬁne layers models permuted ordering given ﬁnal shared classiﬁcation layer. reference parallel ordering models deﬁned identically order tasks. note ﬁtting parallel model samples equivalent single-task model ﬁrst experiment although adding depth expressivity single-task linear case useful examining effects permuted ordering deep linear networks known share properties nonlinear networks second experiment relu. results shown figure remarkably linear case permuted ordering shared layers lose accuracy compared single-task case. similar performance seen nonlinear case indicating behavior extends powerful models. thus learned permuted layers able successfully adapt different orderings different tasks. looking conditions make result possible shed light behavior. instance consider tasks input output size optimal linear solutions respectively. matrices suppose exist matrices figure soft ordering shared layers. sample soft ordering network three shared layers. soft ordering generalizes eqs. learning tensor task-speciﬁc scaling parameters. learned jointly allow ﬂexible sharing across tasks depths. ﬁgure include shared weight layer nonlinearity. architecture enables learning layers used different ways different depths different tasks. case random matrices induced random tasks above traces equal expectation concentrate well dimensionality increases. restrictive effect expressivity permuted ordering negligible. adding small number task-speciﬁc scaling parameters. course real world tasks generally much structured random ones reliable expressivity permuted ordering might always expected. however adding small number task-speciﬁc scaling parameters help adapt learned layers particular tasks. observation previously exploited parallel ordering setting learning task-speciﬁc batch normalization scaling parameters controlling communication columns similarly permuted ordering setting constraint induced reduced adding task-speciﬁc scalars {si}t sigig constraint given reduces deﬁned importantly number task-speciﬁc parameters depend useful scalability well encouraging maximal sharing tasks. idea using small number task-speciﬁc scaling parameters incorporated soft ordering approach introduced next section. permuted ordering tests parallel ordering assumption still ﬁxes priori layer ordering task training. here ﬂexible soft ordering approach introduced allows jointly trained models learn layers applied simultaneously learning layers themselves. consider core network depth layers learned shared across tasks. soft ordering model task deﬁned follows drawn tensor learned scales task layer depth figure shows example resulting depth three model. motivated section previous work adds scaling parameters task notably function size constraint implemented softmax emphasizes idea soft ordering learned; particular formulation subsumes ﬁxed layer ordering learned jointly learnable parameters backpropagation. training initialized equal values reduce initial bias layer function across tasks. also helpful apply dropout shared layer. aside usual beneﬁts dropout shown useful increasing generalization capacity shared representations since trained layers used different tasks different locations dropout makes robust supporting different functionalities. ideas tested empirically mnist omniglot celeba data sets next section. experiments evaluate soft ordering ﬁxed ordering single-task learning. ﬁrst experiment applies intuitively related mnist tasks second superﬁcially unrelated tasks third real-world problem omniglot character recognition fourth large-scale facial attribute recognition. experiment single task parallel ordering permuted ordering soft ordering train equivalent core layers. permuted ordering order layers randomly generated task trial. appendix additional details including additional details speciﬁc experiment. experiment evaluates ability multitask methods exploit tasks intuitively related disparate input representations. binary classiﬁcation problems derived mnist hand-written digit dataset common test evaluating deep learning methods require multiple tasks here goal task distinguish distinct randomly selected digits. create initial dissimilarity across tasks multitask models must disentangle random frozen fullyconnected relu layer output size four core layers fully-connected relu layer units. unshared dense layer single sigmoid classiﬁcation output. figure mnist results. relative performance permuted soft ordering compared parallel ordering improves number tasks increases showing ﬂexibility order help scaling tasks. note cost savings multitask single task models terms number trainable parameters scales linearly number tasks. representative two-task soft order experiment layer-wise distance scalings tasks increases iteration scalings move towards hard ordering. ﬁnal learned relative scale shared layer depth task indicated shading strongest path drawn showing distinct soft order learned task results shown figure relative performance permuted ordering soft ordering compared parallel ordering increases number tasks trained jointly showing ﬂexibility order help scaling tasks. result consistent hypothesis parallel ordering increased negative effects number tasks increases. figure bshow soft ordering actually learns scalings tasks diverge layers specialize different functions different tasks. figure data sets results. tasks used joint training; varying types problems dataset characteristics show diversity tasks. mean test error tasks iteration. permuted parallel order show improvement ﬁrst iterations soft order decisively outperforms methods. next experiment evaluates ability soft ordering integrate information across diverse superﬁcially unrelated tasks i.e. tasks immediate intuition related. tasks taken popular classiﬁcation data sets descriptions tasks given figure inputs outputs priori shared meaning across tasks. learned fully-connected relu layer output size four core layers fully-connected relu layer units. unshared dense softmax layer given number classes. results figure show that parallel permuted show improvement error ﬁrst iterations soft ordering signiﬁcantly outperforms methods. ﬂexible layer ordering model eventually able exploit signiﬁcant regularities underlying seemingly disparate domains. omniglot dataset consists ﬁfty alphabets induces different character recognition task. deep approaches recently shown promise dataset useful benchmark large number tasks allows analysis performance function number tasks trained jointly clear intuition knowledge alphabets increase ability learn others. omniglot also good setting evaluating ability soft ordering learn compose layers different ways different tasks developed problem inherent composibility e.g. similar kinds strokes applied different ways draw characters different alphabets consequently used test deep generative models evaluate performance given number tasks single random ordering tasks created ﬁrst tasks considered. train/test splits created previous work using data testing. experiment scale-up previous experiments evaluates soft ordering convolutional layers. models made close possible architecture previous work allowing soft ordering applied. four core layers convolutional followed pooling. fully-connected softmax layer output size equal number classes. results show soft ordering able consistently outperform deep approaches improvements robust number tasks amount training data suggesting soft ordering task complexity model complexity responsible improvement. permuted ordering performs signiﬁcantly worse parallel ordering domain. surprising deep vision systems known induce common feature hierarchy especially within ﬁrst couple layers parallel ordering hierarchy built permuted ordering difﬁcult exploit. however existence feature hierarchy preclude possibility functions used produce hierarchy useful contexts. soft ordering allows discovery uses. figure shows layer used less different depths. soft ordering model learns soft figure omniglot results. error number tasks trained jointly. soft ordering signiﬁcantly outperforms single task ﬁxed ordering approaches number tasks; distribution learned layer usage depth across tasks soft order run. usage layer correlated depth. coincides understanding innate hierarchy convolutional networks soft ordering able discover. instance usage layer decreases depth increases suggesting primary purpose low-level feature extraction though still sees substantial deeper contexts; errors tasks different training sizes. ﬁrst methods previous deep results multitask tensor factorization methods shared parallel ordering. soft ordering signiﬁcantly outperforms approaches showing approach scales real-world tasks requiring specialized components convolutional layers. hierarchy layers layer distribution increased decreased usage depth. case usage layer correlated depth. instance usage layer decreases depth increases suggesting primary purpose low-level feature extraction though still sees substantial deeper contexts. section describes experiment investigates behavior single layer different contexts. although facial attributes high-level concepts intuitively exist level shared hierarchy rather concepts related multiple subtle overlapping ways semantic space. experiment investigates soft ordering approach component larger system exploit relationships. celeba dataset consists color images binary labels facial attributes experiment label deﬁnes task parallel soft order models based resnet- vision model also used recent state-of-the-art approaches celeba resnet- model truncated ﬁnal average pooling layer followed linear layer projecting embedding size shared across tasks. four core layers dense relu layer units. unshared dense sigmoid layer. parallel ordering soft ordering models compared. test robustness learning models trained without inclusion additional facial landmark detection regression task. soft order models also tested without inclusion ﬁxed identity layer depth. identity layer increase consistency representation across contexts ease learning layer also allowing soft ordering tune much total non-identity transformation individual task. especially relevant case attributes since different tasks different levels complexity abstraction. results given figure existing work used resnet- vision model showed using parallel order multitask model improved test error single-task learning faster training strategy added core layers parallel ordering model achieves test error soft ordering model yielded substantial improvement beyond demonstrating soft ordering value larger deep learning system. including landmark detection yielded marginal improvement parallel ordering degraded performance slightly indicating soft ordering robust joint training diverse kinds tasks. including identity layer improved performance though landfigure celeba results. layer usage depth without inclusion identity layer. cases layers lower usage lower depths higher usage higher depths vice versa. identity layer almost always sees increased usage; application increase consistency representation across contexts. soft order models achieve signiﬁcant improvement parallel ordering receive boost including identity layer. ﬁrst rows previous work resnet- show baseline improvement single task multitask. mark detection identity layer improvement slightly diminished. explanation degredation added ﬂexibility provided identity layer offsets regularization provided landmark detection. note previous work shown adaptive weighting task loss data augmentation ensembling larger underlying vision model also yield signiﬁcant improvements. aside soft ordering none improvements alter multitask topology beneﬁts expected complementary soft ordering demonstrated experiment. coupling soft ordering greater improvements possible. figures characterize usage layer learned soft order models. like case omniglot layers used less lower depths used higher depths vice versa giving evidence models learn soft hierarchy layer usage. identity layer included usage almost always increased training allows model smaller specialized proportions nonlinear structure individual task. success soft layer ordering suggests layers learn functional primitives similar effects different contexts. explore idea qualitatively following experiment uses generative visual tasks. goal task learn function pixel coordinate brightness value normalized task deﬁned single image drawn mnist dataset; pixels used training data. tasks trained using soft ordering four shared dense relu layers units each. linear encoder shared across tasks global average pooling decoder. thus task models distinguished completely learned soft ordering scaling parameters visualize behavior layer depth task predicted image task generated across varying magnitudes results ﬁrst tasks ﬁrst layer shown table similar function observed contexts suggesting layers indeed learn functional primitives. table example behavior soft order layer. task depth effect increasing activation particular layer expand left side manner appropriate functional context results layers similar suggesting layers implement functional primitives. interest clarity soft ordering approach paper developed relatively small step away parallel ordering assumption. develop practical specialized methods inspiration taken recurrent architectures approach extended layers general structure applied training understanding general functional building blocks. connections recurrent architectures. deﬁned recursively respect learned layers shared across tasks. thus soft-ordering architecture viewed type recurrent architecture designed speciﬁcally mtl. perspective figure shows unrolling soft layer module different scaling parameters applied different depths unrolled different tasks. since type recurrence induced soft ordering require task input output sequential methods recurrence setting particular interest recurrent methods also used reduce size e.g. recurrent hypernetworks finally section demonstrated soft ordering shared learned layers fully-connected convolutional; also straightforward extend soft ordering shared layers internal recurrence lstms setting soft ordering viewed inducing higher-level recurrence. generalizing structure shared layers. clarity paper core layers given setup shape. course would useful generalization soft ordering could subsume modern deep architecture many layers varying structure. given soft ordering requires shape inputs element-wise depth. reshapes and/or resampling added adapters tensors different shape; alternatively function could used. example instead learning weighting across layers depth probability applying module could learned manner similar adaptive dropout sparsely-gated mixture experts furthermore idea soft ordering layers extended soft ordering modules general structure succinctly capture recurring modularity. training generalizable building blocks. used different ways different locations different tasks shared trained layers permuted soft ordering learned general functionality layers trained ﬁxed location single task. natural hypothesis likely generalize future unseen tasks perhaps even without training. ability would especially useful small data regime number trainable parameters limited. example given collection layers trained previous tasks model task could learn apply building blocks e.g. learning soft order keeping internal parameters ﬁxed. learning efﬁcient generalizable layers would akin learning functional primitives. functional modularity repetition evident natural technological sociological worlds functional primitives align well complex real-world models. perspective related recent work reusing modules parallel ordering setting different ways different tasks learn modules also help shed light tasks related especially seem superﬁcially disparate thus assisting discovery real-world regularities. paper identiﬁed parallel ordering shared layers common assumption underlying existing deep approaches. assumption restricts kinds shared structure learned tasks. experiments demonstrate direct approaches removing assumption ease integration information across plentiful diverse tasks. soft ordering introduced method learning apply layers different ways different depths different tasks simultaneously learning layers themselves. soft ordering shown outperform parallel ordering methods well single-task learning across suite domains. results show deep improved generating compact multipurpose functional primitives thus aligning closely understanding complex real-world processes. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. experiments keras deep learning framework chollet using tensorﬂow backend experiments used adam optimizer default parameters unless otherwise speciﬁed. iteration multitask training random batch task processed results combined across tasks single update. compared alternating batches tasks processing tasks simultaneously simpliﬁed training procedure faster lower ﬁnal convergence. encoders shared inputs samples batch across tasks. cross-entropy loss used classiﬁcation tasks. overall validation loss task validation losses. experiment single task parallel ordering permuted ordering soft ordering trained equivalent core layers. permuted ordering order layers randomly generated task trial. several trials setup produce conﬁdence bounds. input pixel values normalized training test sets task mnist train test sets restricted selected digits. dropout rate applied output core layer. setup trained iterations batch consisting samples task. randomly selecting pairs digits deﬁne tasks digits selected without replacement within task replacement across tasks possible tasks possible sets tasks size tasks input feature scaled task training validation data created random split. split ﬁxed across trials. dropout rate applied output core layer. enable soft ordering output shared layers must shape. comparability models made close possible architecture previous work models four sharable layers three convolutions followed max-pooling kernels. experiment evaluate soft ordering convolutional layers four core layers convolutional layer relu activation kernel size convolutional layer followed maxpooling layer. number ﬁlters convolutional layer makes number total model parameters close possible reference model. dropout rate applied output core layer. omniglot dataset consists black-and-white images. ﬁfty alphabets characters twenty images character. compatible shapes shared layers input zero-padded along third dimension shape i.e. ﬁrst slice containing image data remainder zeros. evaluate approaches tasks random ordering ﬁfty tasks created ﬁxed across trials. trial ﬁrst tasks ordering trained jointly iterations training batch containing random samples task. ﬁxed ordering tasks follows angelic malay sanskrit cyrillic anglo-saxon futhorc syriac ge’ez japanese keble manipuri alphabet magi gurmukhi korean early aramaic atemayar qelisayer tagalog mkhedruli inuktitut tengwar hebrew n’ko grantha latin syriac tiﬁnagh balinese mongolian ulog futurama malayalam oriya ojibwe avesta kannada bengali japanese armenian aurekbesh glagolitic asomtavruli greek braille burmese blackfoot atlantean]. celeba experiments training validation test splits provided used. images training validation testing. dataset contains images approximately celebrities. images given celebrity occur three dataset splits models must also generalize human identities. weights resnet- initialized pre-trained imagenet weights provided keras framework chollet image preprocessing done default keras image preprocessing function including resizing images output facial landmark detection task dimensional vector indicating locations landmarks normalized mean squared error used training loss. landmark detection included target metric still attribute classiﬁcation error. aligned celeba images used accurate landmark detection challenge including additional task still provide additional regularization multitask model. dropout rate applied output core layer. experiments used batch size validation loss converges adam models trained rmsprop learning rate similar approach used günther produce resulting image ﬁxed model predictions pixel locations generated denormalized mapped back pixel coordinate space. loss used experiment mean squared error since pixels task image used training sense generalization unseen data within task. result dropout used experiment. task models distinguished completely learned soft ordering scaling parameters joint model viewed generative model generates different varying values visualize behavior layer depth task output model task visualized sweeping across enable sweeping keeping rest model behavior ﬁxed softmax task depth replaced sigmoid activation. note global avgerage pooling decoder altering weight single layer observable effect depth four.", "year": 2017}