{"title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "abstract": "In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.", "text": "work propose novel interpretation residual networks showing seen collection many paths differing length. moreover residual networks seem enable deep networks leveraging short paths training. support observation rewrite residual networks explicit collection paths. unlike traditional models paths residual networks vary length. further lesion study reveals paths show ensemble-like behavior sense strongly depend other. finally surprising paths shorter might expect short paths needed training longer paths contribute gradient. example gradient residual network layers comes paths layers deep. results reveal characteristics seem enable training deep networks residual networks avoid vanishing gradient problem introducing short paths carry gradient throughout extent deep networks. modern computer vision systems follow familiar architecture processing inputs lowlevel features task speciﬁc high-level features. recently proposed residual networks challenge conventional view three ways. first introduce identity skip-connections bypass residual layers allowing data layers directly subsequent layers. stark contrast traditional strictly sequential pipeline. second skip connections give rise networks orders magnitude deeper previous models many layers. contrary architectures like alexnet even biological systems capture complex concepts within half dozen layers. third initial experiments observe removing single layers residual networks test time noticeably affect performance. surprising removing layer traditional architecture leads dramatic loss performance. work investigate impact differences. address inﬂuence identity skipconnections introduce unraveled view. novel representation shows residual networks viewed collection many paths instead single deep network. further perceived resilience residual networks raises question whether paths dependent whether exhibit degree redundancy. perform lesion study. results show ensemble-like behavior sense removing paths residual networks deleting layers corrupting paths reordering layers modest smooth impact performance. finally investigate depth residual networks. unlike traditional models paths residual networks vary length. distribution path lengths follows binomial distribution meaning majority paths network layers layers deep. moreover show gradient training comes paths even shorter i.e. layers deep. reveals tension. hand residual network performance improves adding layers however hand residual networks seen collections many paths effective paths relatively shallow. results could provide ﬁrst explanation residual networks resolve vanishing gradient problem preserving gradient throughout entire depth network. rather enable deep networks shortening effective paths. short paths still seem necessary train deep networks. paper make following contributions perform lesion study show paths strongly depend other even though trained jointly. moreover exhibit ensemble-like behavior sense performance smoothly correlates number valid paths. sequential hierarchical computer vision pipeline visual processing long understood follow hierarchical process analysis simple complex features. formalism based discovery receptive ﬁeld characterizes visual system hierarchical feedforward system. neurons early visual areas small receptive ﬁelds sensitive basic visual features e.g. edges bars. neurons deeper layers hierarchy capture basic shapes even deeper neurons respond full objects. organization widely adopted computer vision machine learning literature early neural networks neocognitron traditional hand-crafted feature pipeline malik perona convolutional neural networks recent strong results deep neural networks general perception depth neural networks govern expressive power performance. work show residual networks necessarily follow tradition. residual networks neural networks layer consists residual module skip connection bypassing since layers residual networks comprise multiple convolutional layers refer residual blocks remainder paper. clarity notation omit initial pre-processing ﬁnal classiﬁcation steps. input output block recursively deﬁned sequence convolutions batch normalization rectiﬁed linear units nonlinearities. figure shows schematic view architecture. recent formulation residual networks deﬁned weight matrices denotes convolution batch normalization max. formulations typically composed operations differ order. idea branching paths neural networks new. example regime convolutional neural networks models based inception modules among ﬁrst arrange layers blocks parallel paths rather strict sequential order. choose residual networks study simple design principle. highway networks residual networks viewed special case highway networks output layer highway network deﬁned figure residual networks conventionally shown natural representation equation expand formulation equation obtain unraveled view -block residual network circular nodes represent additions. view apparent residual networks implicit paths connecting input output adding block doubles number paths. follows structure equation highway networks also contain residual modules skip connections bypass them. however output path attenuated gating function learned parameters dependent input. highway networks equivalent residual networks case data ﬂows equally paths. given omnipotent solver highway networks could learn whether residual module affect data. introduces parameters complexity. investigating neural networks several investigative studies seek better understand convolutional neural networks. example zeiler fergus visualize convolutional ﬁlters unveil concepts learned individual neurons. further szegedy investigate function learned neural networks small changes input called adversarial examples lead large changes output. within stream research closest study work yosinski performs lesion studies alexnet. discover early layers exhibit little co-adaptation later layers co-adaptation. papers along ours common thread exploring speciﬁc aspects neural network performance. study focus investigation structural properties neural networks. ensembling since early days neural networks researchers used simple ensembling techniques improve performance. though boosting used past simple approach arrange committee neural networks simple voting scheme ﬁnal output predictions averaged. performers several competitions technique almost afterthought generally characteristic ensembles smooth performance respect number members. particular performance increase additional ensemble members gets smaller increasing ensemble size. even though strict ensembles show residual networks behave similarly. dropout hinton show dropping individual neurons training leads network equivalent averaging ensemble exponentially many networks. similar spirit stochastic depth trains ensemble networks dropping entire layers training. work show need special training strategy stochastic depth drop layers. entire layers removed plain residual networks without impacting performance indicating strongly depend other. better understand residual networks introduce formulation makes easier reason recursive nature. consider residual network three building blocks input output equation gives recursive deﬁnition residual networks. output stage based combination subterms. make shared structure residual network apparent unrolling recursion exponential number nested terms expanding layer figure deleting layer residual networks test time equivalent zeroing half paths. ordinary feed-forward networks alexnet deleting individual layers alters viable path input output. illustrate expression tree graphically figure subscripts function modules indicating weight sharing graph equivalent original formulation residual networks. graph makes clear data ﬂows along many paths input output. path unique conﬁguration residual module enter skip. conceivably unique path network indexed binary code input ﬂows residual module skipped. follows residual networks paths connecting input output layers. classical visual hierarchy layer processing depends output previous layer. residual networks cannot strictly follow pattern inherent structure. module residual network data mixture different distributions generated every possible conﬁguration previous residual modules. compare strictly sequential network alexnet depicted conceptually figure networks input always ﬂows ﬁrst layer straight last single path. written output three-layer feed-forward network networks worthwhile note ordinary feed-forward neural networks also unraveled using thought process level individual neurons rather layers. renders network collection different paths path unique conﬁguration neurons layer connecting input output. thus paths ordinary neural networks length. however paths residual networks varying length. further path residual network goes different subset layers. based observations formulate following questions address experiments below. paths residual networks dependent exhibit degree redundancy? paths strongly depend other behave like ensemble? paths varying lengths impact network differently? figure deleting individual layers residual network cifar-. performance drops random chance layers deleted deleting individual modules residual networks minimal impact performance. removing downsampling modules slightly higher impact. figure results dropping individual blocks residual networks trained imagenet similar cifar results. however downsampling layers tend impact imagenet. time cifar- experiments imagenet show comparable results. train residual networks standard training strategy dataset augmentation learning rate policy cifar- experiments train -layer residual network modules pre-activation type contain batch normalization ﬁrst step. imagenet layers important note special training strategy adapt network. particular perturbations stochastic depth training. motivating experiment show transformations within residual network necessary deleting individual modules neural network fully trained. remove residual module single building block leaving skip connection untouched. change yi−. measure importance building block varying residual module remove. compare conventional convolutional neural networks train network layers setting number channels layers allow removal layer. unclear whether neural network withstand drastic change model structure. expect break dropping layer drastically changes input distribution subsequent layers. results shown figure expected deleting layer reduces performance chance levels. surprisingly case residual networks. removing downsampling blocks modest impact performance block removal lead noticeable change. result shows extent structure residual network changed runtime without affecting performance. experiments imagenet show comparable results seen figure residual networks resilient dropping layers not? expressing residual networks unraveled view provides ﬁrst insight. shows residual networks seen collection many paths. illustrated figure layer removed number paths reduced leaving half number paths valid. contains single usable path input output. thus single layer removed viable path corrupted. result suggests paths residual network strongly depend although trained jointly. figure error increases smoothly randomly deleting several modules residual network. error also increases smoothly re-ordering residual network shufﬂing building blocks. degree reordering measured kendall correlation coefﬁcient. results similar would expect ensembles. depends smoothly number members. collection paths behave like ensemble would expect test-time performance residual networks smoothly correlate number valid paths. indeed observe deleting increasing numbers residual modules increases error smoothly figure implies residual networks behave like ensembles. deleting residual modules network originally length number valid paths decreases example original network started building blocks deleting blocks leaves paths. though collection factor roughly original size still many valid paths error remains around previous experiments dropping layers effect removing paths network. experiment consider changing structure network re-ordering building blocks. effect removing paths inserting paths never seen network training. particular moves high-level transformations low-level transformations. re-order network swap randomly sampled pairs building blocks compatible dimensionality ignoring modules perform downsampling. graph error respect kendall rank correlation coefﬁcient measures amount corruption. results shown figure corruption increases error smoothly increases well. result surprising suggests residual networks reconﬁgured extent runtime. seen many paths residual networks necessarily depend other investigate characteristics. distribution path lengths paths residual networks length. example precisely path goes modules paths single module. reasoning distribution possible path lengths residual network follows binomial distribution. thus know path lengths closely centered around mean figure shows path length distribution residual network modules; paths modules. vanishing gradients residual networks generally data ﬂows along paths residual networks. however paths carry amount gradient. particular length paths network affects gradient magnitude backpropagation empirically investigate effect vanishing gradients residual networks perform following experiment. starting trained network blocks sample individual paths certain length measure norm gradient arrives input. sample path length ﬁrst feed batch forward whole network. backward pass randomly sample residual figure much gradient paths different lengths contribute residual network? ﬁrst show distribution possible path lengths follows binomial distribution. second record much gradient induced ﬁrst layer network paths varying length appears decay roughly exponentially number modules gradient passes through. finally multiply functions show much gradient comes paths certain length. though many paths medium length paths longer modules generally long contribute noticeable gradient training. suggests effective paths residual networks relatively shallow. blocks. blocks propagate residual module; remaining blocks propagate skip connection. thus measure gradients single path length sample measurements length using random batches training set. results show gradient magnitude path decreases exponentially number modules went backward pass figure effective paths residual networks relatively shallow finally results deduce whether shorter longer paths contribute gradient training. total gradient magnitude contributed paths length multiply frequency path length expected gradient magnitude. result shown figure surprisingly almost gradient updates training come paths modules long. effective paths even though constitute paths network. moreover comparison total length network effective paths relatively shallow. validate result retrain residual network scratch sees effective paths training. ensures long path ever used. retrained model able perform competitively compared training full network know long paths residual networks needed training. achieve training subset modules mini batch. particular choose number modules distribution paths training aligns distribution effective paths whole network. network modules means sample exactly modules training batch. then path lengths training centered around modules well aligned effective paths. experiment network trained effective paths achieves error rate whereas full model achieves error rate. statistically signiﬁcant difference. demonstrates indeed effective paths needed. removing residual modules mostly removes long paths deleting module residual network mainly removes long paths network. particular deleting residual modules network length fraction paths remaining path length given figure illustrates fraction remaining paths deleting modules module network. becomes apparent deletion residual modules mostly affects long paths. even deleting residual modules many effective paths modules long still valid. since mainly effective paths important performance result line experiment shown figure performance drops slightly removal residual modules however removal modules observe severe drop performance. figure impact stochastic depth resilience layer deletion. training stochastic depth improves resilience slightly indicating plain residual networks already don’t depend individual layers. compare fig. connection highway networks highway networks multiplexes data residual skip connections means paths used equally. highway networks wild observe empirically gates commonly deviate particular tend biased toward sending data skip connection; words network learns short paths. similar results reinforces importance short paths. effect stochastic depth training procedure recently alternative training procedure residual networks proposed referred stochastic depth approach random subset residual modules selected mini-batch training. forward backward pass performed modules. stochastic depth affect number paths network paths available test time. however changes distribution paths seen training. particular mainly short paths seen. further selecting different subset short paths mini-batch encourages paths produce good results independently. training procedure signiﬁcantly reduce dependence paths? repeat experiment deleting individual modules residual network trained using stochastic depth. result shown figure training stochastic depth improves resilience slightly; dependence downsampling layers seems reduced. surprising know plain residual networks already don’t depend individual layers. reason behind residual networks’ increased performance? recent iteration residual networks provide hypothesis obtain results simple essential concept—going deeper. true deeper previous approaches present complementary explanation. first unraveled view reveals residual networks viewed collection many paths instead single ultra deep network. second perform lesion studies show that although paths trained jointly strongly depend other. moreover exhibit ensemble-like behavior sense performance smoothly correlates number valid paths. finally show paths network contribute gradient training shorter expected. fact deep paths required training contribute gradient. thus residual networks resolve vanishing gradient problem preserving gradient throughout entire depth network. insight reveals depth still open research question. promising observations provide lens examine neural networks. would like thank kwak theofanis karaletsos insightful feedback. also thank reviewers nips constructive helpful feedback suggesting paper title. work partly funded connected experiences laboratory graduate research fellowship award google focused research award", "year": 2016}