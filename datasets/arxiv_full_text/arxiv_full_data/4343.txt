{"title": "Zero-Shot Learning via Semantic Similarity Embedding", "tag": ["cs.CV", "stat.ML"], "abstract": "In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (\\eg attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.", "text": "figure proposed method source/target domain data displayed leftmost/rightmost ﬁgures respectively. light blue corresponds unseen classes colors depict seen class data. light-blue data unavailable training. test-time unseen source domain data revealed along arbitrary unseen instance target domain presented identify unseen class label. unseen class source domain data expressed histograms seen class proportions. seen class proportions estimated target instance compared source domain histograms. classes revealed. goal test time predict target domain instance seen/unseen classes associated with. idea proposed method depicted fig. view target data instances arising seen instances attempt express source/target data mixture seen class proportions. algorithm based postulate mixture proportion target domain similar source domain must arise class. leads learning source target domain embedding functions using seen class data arbitrary source target domain data mixture proportions seen classes. propose parameterized-optimization problems learning semantic similarity embedding functions training data jointly optimize predeﬁned parameters using cross validation held-out seen class data. method necessitates fundamentally design choices requiring learn class-dependent feature transforms because components embedding must account contribution seen class. source domain embedding based subspace clustering literature known resilient noise. target domain embedding based margin-based framework using intersection function rectiﬁed linear unit paper consider version zero-shot learning problem seen class source target domain data provided. goal test-time accurately predict class label unseen target domain instance based revealed source domain side information unseen classes. method based viewing source target data mixture seen class proportions postulate mixture patterns similar instances belong unseen class. perspective leads learning source/target embedding functions arbitrary source/target domain data semantic space similarity readily measured. develop max-margin framework learn similarity functions jointly optimize parameters means cross validation. test results compelling leading signiﬁcant improvement terms accuracy benchmark datasets zero-shot recognition. signiﬁcant progress large-scale classiﬁcation recent years lack sufﬁcient training data every class increasing difﬁculty ﬁnding annotations large fraction data might impact improvements. zero-shot learning increasingly recognized deal difﬁculties. version zero shot learning based so-called source target domains. source domain described single vector corresponding class based side information attributes language words/phrases even learned classiﬁers assume collected easily. target domain described joint distribution images/videos labels training time given source domain attributes target domain data corresponding subset classes call seen classes. test time source domain attributes unseen attempts align seen class source domain data corresponding seen class target domain data instances. finally employ cross validation technique based holding seen class data matching held-out seen classes optimize parameters used optimization problems source target domain. jointly optimize parameters best align mixture proportions held-out seen classes provide basis generalizing unseen classes. results several benchmark datasets zero-shot learning demonstrate method signiﬁcantly improves current state-of-the-art results. related work existing zero-shot learning methods rely predicting side information classiﬁcation. proposed semantic output code classiﬁer utilizes knowledge base semantic properties. proposed several probabilistic attribute prediction methods. proposed designing discriminative categorylevel attributes. proposed optimization formulation learn source domain attribute classiﬁers attribute vectors jointly. proposed learning classiﬁers unseen classes linearly combining classiﬁers seen classes. proposed label embedding method embed class attribute vector space. directly learned mapping functions feature vectors source target domains deep learning. methods suffer noisy side information data bias leading unreliable prediction. recent work proposed overcome issues above. proposed propagated semantic transfer method exploiting unlabeled instances. discussed projection domain shift problem proposed transductive multi-view embedding method. investigated attribute unreliability issue proposed random forest approach. proposed simple method introducing better regularizer. important conceptual difference distinguishes method existing works methods interpreted learning relationships source attributes target feature components method based leveraging similar class relationships source target domains requiring class dependent feature transform. leads complex scoring functions cansimpliﬁed linear bilinear forms semantic similarity embedding widely used model relationships among classes quite insensitive instance level noise. proposed learning mapping functions embed input vectors classes dimensional common space based class taxonomies. proposed label embedding tree method large multiclass tasks also embeds class labels dimensional space. proposed analogy-preserving semantic deﬁnition seen classes number seen classes indexes seen unseen classes simplex r|s| dimensional space source domain attribute vector class normalization i.e. training data target feature class number training samples source/target domain feature embedding functions target domain class dependent feature transformation entry vector learned source domain embedded histogram ∆|s| class learned target domain reference vector class vector seen class learned target domain weight vector learned structured scoring function relating target domain sample class label embedding method multi-class classiﬁcation. later proposed uniﬁed semantic embedding method incorporate different semantic information learning. recently proposed semantic embedding method zero-shot learning embed unseen class convex combination seen classes heuristic weights. proposed semantic ranking representation based semantic similarity aggregate semantic information multiple heterogeneous sources. embedding represent class mixture seen classes domains. method based expressing source/target data mixture seen class proportions using seen class data learn source target domain embedding functions respectively. construct functions take arbitrary source vectors target vectors inputs embed ∆|s| observe components corresponding seen class denote proportion class instance test-time source domain vectors unseen classes revealed. presented arbitrary target instance predict unseen label maximizing semantic similarity histograms. letting zero-shot recognition rule deﬁned follows precisely letting unseen seen classconditional target feature distributions respectively priori approximate mixture py’s i.e. ¯πypy perror various approaches context) denotes mixture weight class analogously also decompose source domain data mixture source domain seen classes. leads associate mixture proportion vector unseen class zuycy ∆|s|. postulate target domain instance must average similar mixture pattern source domain pattern correspond unseen label namely average equal postulate essentially postulate also motivates margin-based approach learning note since single source domain vector class natural constraint require empirical mean mixture corresponding example class target domain aligns well source domain mixture. empirically consistent postulate. letting seen class labels denote average mixture class target domain requirement guarantee i{·} denotes binary indicator function returning condition holds otherwise note empirical mean embedding corresponds kernel empirical mean embedding valid rkhs kernel pursue point paper. nevertheless alignment constraint generally insufﬁcient capture shape underlying sample distribution. augment misclassiﬁcation constraints seen sample svms account shape. source domain embedding recall fig. sec. embedding aims source domain attribute vectors histograms seen class proportions i.e. ∆|s|. propose parameterized optimization problem inspired sparse coding follows given source domain vector optimize parameters globally using held-out seen class data. summarize learning scheme below. source domain embedding function embedding function realized means parameterized optimization problem related sparse coding. target domain embedding function model consists constant weight vector class dependent feature transformation propose margin-based optimization problem jointly learn weight vector feature transformation. note parameterization yield negative values normalized incorporated additional constraints ignore issue optimization objectives. cross validation embedding functions parameter dependent. choose parameters employing cross validation technique based holding seen class data. first learn embedding functions remaining seen class data different values predeﬁned parameters. jointly optimize parameters source/target embedding functions minimize prediction error held-out seen classes. re-train embedding functions entire seen class data. salient aspects proposed method decomposition method seeks decompose source target domain instances mixture proportions seen classes. contrast much existing work interpreted learning cross-domain similarity source domain attributes target feature components. class dependent feature transformation decomposition perspective necessitates fundamentally design choices. instance component corresponding class must dependent implies must choose class dependent feature transform constant vector agnostic class. joint optimization generalization unseen classes method jointly optimizes parameters embedding functions best align source target domain histograms held-out seen classes thus providing basis generalizing unseen classes. even ﬁxed parameters embedding functions nonlinear maps since parameters jointly optimized learned scoring function couples seen source target domain together rather complex way. cannot reduce linear bilinear setting recall method based viewing unseen source target instances histogram seen classes proportions. fig. suggests target instance viewed arising mixture seen classes mixture components dependent location instance. entry-wise operators. note intersection function captures data patterns thresholds relu captures data patterns thresholds. sense features generated functions complementary. reason choose functions demonstrate robustness method. test-time target instance compute arbitrary unseen label source attribute vector revealed note highly non-convex cannot reduce bilinear functions used existing works denotes structural loss groundtruth class predicted class predeﬁned regularization parameters {ξiy} {\u0001ys} slack variables vector paper deﬁne respectively. note learning access utilize data seen classes. figure cosine similarity matrices among seen unseen classes apascal ayahoo dataset. brighter color depicts larger values. type data used compute matrix shown corresponding matrix. observe training/testing source/target domain embedding preserves inter-class relationships originally deﬁned source domain attribute vectors. also indicates target domain embeddings manage align well target domain distributions source domain attribute vectors. even though simplex embeddings always. note embedding general nonlinear function. indeed account simplex constraint small values vector zeroed solve quadratic programming. large-scale cases adopt efﬁcient proximal gradient descent methods. note many alternate ways embedding similarity rescaling subspace clustering sparse learning rank representation long embedding simplex. tried different methods simplex constraint learn embeddings current solution works best. believe probably goal methods subspace clustering goal noise resilient embedding good generalization unseen class classiﬁcation. optimize parameter globally cross validation. parameter identiﬁed seen classes used embedding function. note small coordinate vector essentially amounts coding multi-class classiﬁcation useful unseen class generalization. conceptually learn tuning parameters predict well held-out seen classes general close zero. demonstrate class afﬁnity matrices embedding seen unseen classes fig. obtained cross validation. training testing source domain embeddings preserve afﬁnities among classes attribute space. denote entries vectors respectively. rdt|s| vector concatenation denote summations convex concave terms respectively. i.e. functions. using cccp relax constraint +∇g)t denotes solution iteration denotes subgradient operator. similarly perform cccp relax constraint letting denote minimization problem using cccp write subgradient iteration follows subgradient descent update equivalently learning simple algebra show entry class equivalent entry order guarantee monotonic decrease objective extra checking step iteration. cross validation seen class data scoring function obtained solving turn depend parameters propose learning parameters means cross validation using held-out seen class data. speciﬁcally deﬁne held-out s\\s. learn collection embedding functions source target domains using range parameters suitably discretized space. parameter choice obtain scoring function depends training subset well parameter choice. compute prediction error namely number times held-out target domain sample misclassiﬁed parameter choice. repeat procedure different randomly selected subsets choose parameters minimum average prediction error. parameters obtained plug back re-learn scoring function using seen classes. figure illustration three different constraints learning target domain semantic embedding function. different shapes denote differnt classes ﬁll-in shapes denote source domain embeddings green crosses denote empirical means target domain data embeddings. method takes account zero-shot learning based distribution alignment instance classiﬁcation. correspond discussion sec. hand care alignment condition likely many misclassiﬁed training data samples illustrated fig. hand conventional classiﬁcation methods consider separating data instances tight shape unable align distributions lack constraint training introducing constraints able learn target domain embedding function well scoring function produce clusters well aligned separated illustrated fig. similarly learn predeﬁned parameters cross validation step optimizes prediction held-out seen classes. parameters determined re-learn classiﬁer seen data. fig. depicts class afﬁnity matrices target domain semantic embedding real data. method manages align source/target domain data distributions. learning ﬁxing step collect constraints plugging {}v{cy}y∈s solve linear learn respectively. learning ﬁxing using concave-convex procedure note constraints test method benchmark image datasets zero-shot recognition i.e. cifar- apascal ayahoo animals attributes caltech-ucsd birds-- attribute datasets utilize matconvnet imagenet-vgg-verydeep- pretrained model extract -dim feature vector image verydeep features work well since lead good class separation required class dependent transform similar features used previous work zero-shot learning. denote variants general method sse-int sse-relu respectively. note terms experimental settings main difference method competitors features. report top- recognition accuracy averaged trials. cross validation. iteration randomly choose seen classes validation alg. initialization speeding computation. simply small number much less important others recognition. cifar- dataset consists color images resolution pixels classes. enriched binary attributes -dim semantic word vectors real numbers class. follow settings precisely take cat-dog plane-auto auto-deer deer-ship cattruck test categories zero-shot recognition respectively rest classes seen class data. training testing performed split training test data provided dataset respectively. ﬁrst summarize accuracy method table clearly method outperforms signiﬁcantly sse-int sse-relu perform similarly. observe cat-dog method performs similarly others method easily achieve high accuracy. show class afﬁnity matrix fig. using binary attribute vectors turns high similarity. similarly word vectors provide discrimination attribute vectors still much less others. better understand learning method visualize target domain features well learned features using t-sne fig. different seen classes learned functions embeddings fig. fig. different. fig. features seem form clusters different classes overlaps small embeddings figure class afﬁnities classes using source domain binary attribute vectors. t-sne visualization different features attributes samples class test selected randomly color denotes class. shows -dim original target domain features. show -dim learned features sse-int tested auto-deer cat-dog respectively. embeddings produced sse-relu similar patterns. imals artifacts. contrast features guided source domain attribute vectors indeed preserve afﬁnities classes attribute space. words learning algorithm manages align target domain distributions corresponding source domain embeddings space well discriminating target domain instance wrong classes. gaps animals artifacts much clearer fig. fig. fig. however still large overlap space leading poor recognition. overall sample distributions fig. fig. similar preserve class afﬁnities. detail dataset please refer original paper. ap&y cub-- attribute datasets take means attribute vectors classes generate source domain data. dataset utilize real-number attribute vectors since discriminative. utilize training/testing splits zero-shot recognition ap&y others. cub- follow bird spices seen classes training left spices unseen classes testing. attribute follow table zero-shot recognition accuracy comparison cifar-. compared numbers best estimated fig. notice methods utilize deep features represent images target domain. table zero-shot recognition accuracy comparison ap&y cub-- attribute respectively form mean±standard deviation. except results rest numbers cited original papers. note experimental settings differ ours. attribute method farhadi mahajan wang rohrbach akata mensink lampert jayaraman grauman romera-paredes torr akata lampert romera-paredes torr sse-int sse-relu test features dataset. show distribution comparison using decaf features vggverydeep- features fig. large difference distributions decaf features clusters slightly separated still cluttered overlaps among different classes. vgg-verydeep- features contrast form crisp clusters different classes useful zero-shot recognition. also plot cosine similarity matrices created using different features fig. matrix vgg-verydeep- features similar source domain attribute vectors demonstrates learning method vgg-verydeep- features align target domain distribution source domain attribute vectors. attribute fact need class dependent feature transform good separation seen classes. implementation based unoptimized matlab code. however return prediction results datasets within minutes using multi-thread starting loading features. instance cifar- manage ﬁnish running code less minutes. figure t-sne visualization comparison distributions using features testing data. method works well good separation classes verydeep features particularly useful. classes unseen classes testing take rest seen classes training. summarize comparison table blank spaces indicate proposed methods tested datasets original papers. still performance difference sse-int sse-relu. datasets method works best except cub--. hand specifically targets ﬁne-grained zero-shot recognition dataset aims general zero-shot learning. hand suspect source domain projection function work well ﬁne-grained recognition investigate future work. understand method better different features figure cosine similarity matrices created using different features testing data. numbers brackets mean accuray achieved using corresponding features. learning method performs best vgg-verydeep- features. attribute fact need class dependent feature transform good separation seen classes. random chance notice even though classes seen classes cannot guarantee testing results similar traditional classiﬁcation methods source domain attribute vectors guide method learning. less discriminative e.g. attribute vectors cifar- recognition performance worse. summarize method performs well stably attribute small seen classes relatively large unseen classes. therefore believe method suitable large-scale zero-shot recognition. proposed learning semantic similarity embedding method zero-shot recognition. label semantic meanings using seen classes project source domain attribute vectors onto simplex space class represented probabilistic mixture seen classes. learn similarity functions embed target domain data semantic space source domain empirical mean embeddings seen class data distributions aligned corresponding source domain embeddings also data instance classiﬁed correctly. propose learning variants using intersection function rectiﬁed linear unit method benchmark datasets including large-scale attribute dataset signiﬁcantly outperforms state-of-art methods. future work would like explore applications method person re-identiﬁcation zero-shot activity retrieval thank anonymous reviewers useful comments. material based upon work supported part u.s. department homeland security science technology directorate ofﬁce university programs grant award -st--ed grant contract fa--c-. views conclusions contained document authors interpreted necessarily representing social policies either expressed implied u.s. test generalization ability method attribute dataset large-scale zero-shot recognition. design experimental settings like benchmark comparison randomly select classes seen classes training among rest classes also randomly select classes unseen classes testing; randomly select classes seen classes training categorize data sample rest unseen classes classes. fig. shows results correspond settings respectively. fig. seen classes achieve reasonably good performance unseen classes few. however increase number unseen classes curve drops rapidly changes slowly number large. unseen classes performance reduced increase number seen classes performance improving especially number unseen classes small. unseen classes performance increases using seen classes respectively. improvement marginal already sufﬁcient number seen classes instance seen classes. fig. generally speaking seen classes performance better better chance preserve semantic afﬁnities among classes source domain. seen classes method achieve mean accuracy much better", "year": 2015}