{"title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty.", "text": "recent work shown deep neural networks highly susceptible well-designed small perturbations input layer so-called adversarial examples. taking images example distortions often imperceptible result mis-classiﬁcation state dnn. study structure adversarial examples explore network topology pre-processing training strategies improve robustness dnns. perform various experiments assess removability adversarial examples corrupting additional noise pre-processing denoising autoencoders daes remove substantial amounts adversarial noise. however stacking original resulting network attacked adversarial examples even smaller distortion. solution propose deep contractive network model end-to-end training procedure includes smoothness penalty inspired contractive autoencoder increases network robustness adversarial examples without signiﬁcant performance penalty. deep neural networks recently signiﬁcant improvement countless areas machine learning speech recognition computer vision krizhevsky dahl taigman zhang dnns achieve high performance deep cascades nonlinear units allow generalize non-locally data-speciﬁc manifolds bengio ability automatically learn non-local generalization priors data strength dnns also creates counter-intuitive properties. particular szegedy showed seminal paper engineer small perturbations input data called adversarial examples make otherwise high-performing misclassify every example. image datasets perturbations often imperceptible human thus creating potential vulnerabilities deploying neural networks real environments. example could envision situations attacker knowledge parameters could adversarial examples attack system make fail consistently. even worse cross-model cross-dataset generalization properties adversarial examples szegedy attacker might generate adversarial examples independent models without full knowledge system still able conduct highly successful attack. indicates still signiﬁcant robustness machine human perception despite recent results showing machine vision performance closing human performance taigman formally challenge design train deep network generalizes abstract manifold space achieve good recognition accuracy also retains local generalization input space? main result szegedy smoothness assumption underlies many kernel methods support vector machines hold deep neural networks trained backpropagation. points possible inherent instability deterministic feed-forward neural network architectures. practice svms used replace ﬁnal softmax layer classiﬁer neural networks leading better generalization tang applying manifold space guarantee local generalization input space. recently duvenand categorize distributions deep neural networks deep gaussian process show stacked architectures capacity network captures fewer degrees freedom layers increase. propose circumvent connecting inputs every layer network. without trick input locality hardly preserved higher layers complexity nonlinear mapping cascades. framework leveraging approaches random recursive vinyals recursively solves whose input combines input data outputs previous layer randomly projected dimension input data. rsvm avoids solving nonconvex optimization recursively solving demonstrates generalization small datasets. however performance suboptimal compared state-of-the-art dnns possibly lack end-to-end training vinyals tang another work inspired recursive nature human perceptual system deep attention selective network stollenga dynamically ﬁne-tunes weight convolutional ﬁlter recognition time. speculate robustness human perception complex hierarchies recursions wirings human brain felleman essen douglas since recursions provide multiple paths input data could retain locality information multiple levels representation. intuition also partially supported recent state-ofthe-art models object classiﬁcation detection involving multi-scale processing szegedy since modeling recursions dnns notoriously hard often relies additional techniques reinforcement learning stollenga mnih ﬁrst investigate explicit inclusion input generalization additional objective standard training process. important note adversarial examples universal unavoidable deﬁnition could always engineer additive noise input make model misclassify example also problem shallow models logistic regression szegedy question much noise needed make model misclassify otherwise correct example. thus solving adversarial examples problem equivalent increasing noticeability smallest adversarial noise example. paper investigate training procedures adversarial examples generated based szegedy higher distortion distortion measured adversarial data original data respectively. first investigate structure adversarial examples show contrary small distortion difﬁcult recover classiﬁcation performance additional perturbations gaussian additive noises gaussian blur. suggests size blind-spots fact relatively large input space volume locally continuous. also show adversarial examples quite similar szegedy autoencoder trained denoise adversarial examples network generalizes well denoise adversarials generated different architectures. however also found classiﬁer stacked resulting network attacked creating adversarial examples even smaller distortion. this conclude ideal architectures trained end-to-end incorporate input invariance respect ﬁnal network output. ideas denoising autoencoder contractive autoencoder recently marginalized denoising autoencoder provide strong framework training neural networks robust adversarial noises rifai alain bengio chen propose deep contractive networks incorporate layer-wise contractive penalty show adversarials generated networks signiﬁcantly higher distortion. believe initial results could serve basis training robust neural networks misdirected substantial noise attuned human perception performs. follow procedure outlined szegedy generating adversarial examples classiﬁer neural network slight modiﬁcation computational efﬁciency. using notation denote classiﬁer {...k} associated continuous loss function {...k} then given image whose pixels normalized target label {...k} minimal adversarial noise approximated optimizing following problem given subject instead ﬁnding minimum line-search example constant evaluation given dataset model architecture. sufﬁciently large subset data size minimized subject constraint mean prediction error rate greater chosen throughout experiments. since interested macro-scale evaluation adversarial examples dataset model setting sufﬁciently simpliﬁes speeds procedure allowing quantitative analysis results. perform experiements mnist dataset using number architectures lecun cortes krizhevsky hinton table summarizes experimental settings baseline error rate adversarial examples’ error rate average adversarial distortion. weight decay applied except convolutional layers. mnist convnet convolutional layers fully-connected layer softmax layer. order gain insight properties adversarial noises explore three pre-processing methods aiming recovering adversarial noise presented following sections. given tiny nature adversarial noise investigate recovery strategy based additional corruptions hope move input outside network blind-spots initially assumed small localized. experiment additive gaussian noise gaussian blurring. gaussian noise averaged predictions feed-forward runs reduce prediction variance. results gaussian additive noises summarized figure shows trade-off adversarial examples recovered clean examples misclassiﬁed varies amount additive noises added input layer input plus hidden layers. results gaussian blurring summarized table gaussian additive noises refers noise applied input layer; refers noise applied input layer plus hidden layers. gaussian blurring applied input layer. results show convolution seems help recovering adversarial examples. example convnet model applying gaussian blur kernel size input data recover adversarial examples expense increase test error clean data addition convnet model adding gaussian noise input layer plus hidden layers allow model recover similar small loss model performance clean data however neither gaussian additive noises blurring effective removing enough noise error adversarial examples could match error clean data. assess structure adversarial noise trained three-hidden-layer autoencoder mapping adversarial examples back original data samples. important detail also train model original training data back itself autoencoder preserves original data non-adversarial data samples allows stack several autoencoders. train autoencoder using adversarial examples training only test generalization capabilities adversarial examples test across different model topologies. table shows generalization performance autoencoders trained adversarial examples different models. columns indicate whose adversarial data autoencoder trained rows indicate whose adversarial test data autoencoder used denoise. entries correspond error rates outputs autoencoder model identiﬁed labels. observe autoencoders generalize well adversarial examples different models. autoencoders able recover least adversarial errors regardless model originates. successful experiment found drawback autoencoder corresponding classiﬁer stacked form feed-forward neural network adversarial examples generated stacked network. last table figure show stacked network adversarial examples signiﬁcantly smaller distortion adversarial examples original classiﬁer network suggesting autoencoder effectively recovers weaknesses original classiﬁer network stacked network even susceptible adversarial noises. possible explanation since autoencoder trained without knowledge classiﬁcation objective blind-spots retable cross-model autoencoder generalization test. error-rates adversarial test data. last rows shows error-rates clean test data average minimal distortion adversarial examples generated stacked network respectively. error-rates adversarial test data without autoencoder preprocessing approximately shown table section standard denoising autoencoder trained without knowledge adversarial noise distribution. maps corrupted input data clean input data. training batch pixel input data corrupted adding independent gaussian noise mean standard deviation. table summarizes results indicating standard denoising autoencoder still recover signiﬁcant portion adversarial noises. particular denoising auto-encoder gaussian noise could denoise adversarial examples almost well autoencoder trained actual adversarials noises shown table however model also suffers deﬁciency section stacked network susceptible adversarials. case deﬁciency likely arises imperfect training itself. experiments shown adversarial noise fairly robust local perturbations additive gaussian noise suggesting size blind-spots relatively large. image data case effect adversarial examples signiﬁcantly reduced low-pass ﬁltering gaussian blurring suggesting adversarial noise mostly resides high-frequency domain. moreover success autoencoder denoising autoencoder experiment shows adversarial noise simple structure easily exploitable. observation adversarial examples unavoidable intrinsic property feed-forward architecture. pre-processing always possible backpropagate error signal additional functions adversarial examples deterministic pre-processing steps gaussian blurring autoencoders. surprisingly experiments show distortion adversarials stacked network even lower distortion adversarials original classiﬁer network. furthermore table also hints even simple gaussian additive noise often used data augmentation effectively creates invariant regions around input data points. based results observations thus postulate solving adversarial problem correspond ﬁnding training procedures objective functions increase distortion smallest adversarial examples. therefore section formulate model could propagate input invariance toward ﬁnal network outputs trained end-to-end. section formulate deep contractive network imposes layer-wise contractive penalty feed-forward neural network. layer-wise penalty approximately minimizes network outputs variance respect perturbations inputs enabling trained model achieve ﬂatness around training data points. contractive autoencoder variant autoencoder additional penalty minimizing squared norm jacobian hidden representation respect input data rifai standard consists encoder decoder. encoder maps input data hidden representation decoder attempts reconstruct input hidden representation. formally given input hidden representation encoder parametrized matrix bias vector decoder parametrized matrix bias vector output given non-linear activation functions encoder decoder respectively. given training data points trained ﬁnding model parameters minimize following objective function deep contractive network generalization contractive autoencoder feed-forward neural network outputs target network hidden layers denote function computing hidden representation rdhi hidden layer ...h ideally model penalize following objective table error-rates clean test data average distortion adversarial examples generated original model model contractive penalty error-rates adversarial examples table error-rates clean test data average distortion adversarial examples models different training conditions. refers gaussian additive noise training. however penalty computationally expensive calculating partial derivatives layer standard back-propagation framework. therefore simpliﬁcation made approximating objective following layer-wise contractive penalty enables partial derivatives computed contractive autoencoder easily incorporated backpropagation procedure. objective guarantee global optimality solution also limits capacity neural network. however computationally efﬁcient greedily propagate input invariance deep network. note neural network additive gaussian noise added input training training objective equivalent alain bengio however stochastic penalties require many passes data train model effectively. efﬁciency decided employ deterministic penalty instead rifai alain bengio chen experiments involve applying contractive penalty models table models trained achieved nearly accuracy original models lacked contractive penalty. adversarial examples generated following method deﬁned section table shows contractive penalty successfully increases minimum distortion adversarial noises. table shows comparison deep contractive network penalty stochastic noise addition. deep contractive networks robust standard neural network trained gaussian input noise easily augmented adding gaussian input noise increase minimum distortion adversarial noises. results show deep contractive networks successfully trained propagate contractivity around input data deep architecture without signiﬁcant loss ﬁnal accuracies. model improved augmenting layer-wise contractive penalty based higher-order contractive autoencoders rifai marginalized denoising autoencoders chen paper deep contractive networks used framework alleviating effects adversarial examples also provide suitable framework probing invariance properties learnt deep neural networks. study conducted evaluate performance loss layer-wise penalties opposed global contractive objectives deﬁned addition exploring non-euclidean adversarial examples e.g. small afﬁne transformation images varying contractivity higher layers network could lead insights semantic attributes features learned high levels representation. example explicitly learning instantiation parameters previously attempted models transforming autoencoder hinton work also bridges supervised learning unsupervised representation learning introducing penalty standard dnn. penalty acts practical regularizers also highly efﬁcient learn obvious information training data local generalization input space. recent progress deep neural networks driven end-to-end supervised training various modes unsupervised feature learning krizhevsky bengio thus believe merge could likely enable milestones ﬁeld. tested several denoising architectures reduce effects adversarial examples conclude simple stable structure adversarial examples makes easy remove autoencoders resulting stacked network even sensitive adversarial examples. conclude neural network’s sensitivity adversarial examples related intrinsic deﬁciencies training procedure objective function model topology. crux problem come appropriate training procedure objective function efﬁciently make network learn invariant regions around training data. propose deep contractive networks explicitly learn invariant features layer show positive initial results. dahl george dong deng acero alex. context-dependent pre-trained deep neural networks large-vocabulary speech recognition. audio speech language processing ieee transactions rifai salah mesnil gr´egoire vincent pascal muller xavier bengio yoshua dauphin yann glorot xavier. higher order contractive auto-encoder. machine learning knowledge discovery databases springer rifai salah vincent pascal muller xavier glorot xavier bengio yoshua. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning stollenga marijn masci jonathan gomez faustino schmidhuber juergen. deep networks internal selective attention feedback connections. arxiv preprint arxiv. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv. zhang ning paluri manohar ranzato marc’aurelio darrell trevor bourdev lubomir. panda pose aligned networks deep attribute modeling. arxiv preprint arxiv.", "year": 2014}