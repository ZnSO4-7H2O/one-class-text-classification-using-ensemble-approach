{"title": "Q-Learning with Basic Emotions", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Q-learning is a simple and powerful tool in solving dynamic problems where environments are unknown. It uses a balance of exploration and exploitation to find an optimal solution to the problem. In this paper, we propose using four basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent. Simulations show that the proposed affective agent requires lesser number of steps to find the optimal path. We found when affective agent finds the optimal path, the ratio between exploration to exploitation gradually decreases, indicating lower total step count in the long run", "text": "ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines paper incorporates higher level emotions fear anger sadness agent learning specifically qlearning partially based korsten taylor’s model main purpose investigate difference similarity number exploration steps normal q-learning agent versus agent whose decision based circumstances mimic sadness fear and/or anger. main performance indicators explored average number steps episode average number steps optimal path found. current state action taken; also next state quality value calculated value correction. correction based learning rate determines information reward value observed performing discount factor determines future rewards maximum state-action pairs next state. used -greedy algorithm stated probability instead select action random. learning rate discount rate held constant rate -greedy parameter made vary since govern action proposed agent changes normal agent’s number steps vary change four basic emotions abstract—q-learning simple powerful tool solving dynamic problems environments unknown. uses balance exploration exploitation find optimal solution problem. paper propose using four basic emotions sadness fear anger influence qlearning agent. simulations show proposed affective agent requires lesser number steps find optimal path. found affective agent finds optimal path ratio exploration exploitation gradually decreases indicating lower total step count long run. robots indistinguishable other. robots interact like normal people wouldn’t nice feel emotions turn exhibit emotions own? although future steps made direction. several researches done combining computer learning affect even early experiments usually controlled physical environment mechanical robot goals moved environment using sort reinforcement learning procedure. robots either accept additional rewards human intervention actions. today similar experiments done discrete environments artificial virtual agents. robots commonplace today industries assist humans work humans could never physically lifting heavy parts working hostile environments. robots leave realm industry homes workplaces important interact efficiently comfortable humans agents. humans interact information also emotion. fact negative emotion enhances memory accuracy positive emotion broaden scope attention different affect models used past incorporate sort emotion learning process robots agents. arousal pleasure factors used influence agent’s movement. ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines researcher also proposed affect model based valence arousal. valence positive current choice expected give higher average reward negative otherwise. arousal function uncertainty. valence arousal define intrinsic reward qlearning algorithm marinier laird environment characterized high/low suddenness high/low pleasantness high/low relevance high/low conductiveness. based characteristics appraisal mechanism provides valenced feeling intensity serves reward signal agent learns. sequeira proposed novelty motivation valence control appraisal dimensions. appraisals turned scalars intrinsic reward calculated linear combination values. recently kuremoto used simplified circumplex model affect russell proposed adopting arousal conventional reinforcement learning improve performance reinforcement learning multi-agent systems. suggested fundamental affect factors arousal pleasure used produce emotion function emotion function combined function state-action value function q-learning constitute motivation function. motivation function implemented stochastic policy function reinforcement learning instead function agents select behaviors according states observed environment also internal affective responses experiments either done using physical autonomous robot virtual autonomous agent. notable robots khepera roco khepera preferred experimental robot gadanho hallam khepera robot small robot wheels eight infrared sensors allow sense object proximity light. sensors located front rear robot. robot’s task move around environment find different food sources extract energy food source. made four identical experiments using different controllers hand-crafted event-triggered intervaltriggered random. controller responsible making robot move. experiments showed emotions used reinforcement event detector. emotion-dependent event detector allows drastic cuts number triggering learning controller. learns task much less iterations needs lesser control. another study observed emotional controller lesser number events collisions. conclude emotional association powerful ability cover states explanatory power introduce errors over-generalization. work novel approach incorporate emotion q-learning developed. work novel accounts. first first time basic emotions incorporated q-learning. second first time basic emotions made influence agent’s behavior terms direction speed. work el-nasr user feedback used provide action values. broekens used human’s emotional expressions. expressions analyzed real time converted additional reinforcement signal used robot; positive expressions result reward negative expressions punishment. took affect mean positiveness versus negativeness situation object etc. user feedback form extrinsic motivation. researches though focused intrinsic motivation. intrinsic means dealing internal reward rather external input. gadanho used complex model receives input environment sensors. input processed feelings module outputs feelings hunger pain restlessness temperature eating smell warmth proximity. turn serves input emotion module outputs sadness fear anger. component emotions back feelings module hormone component include suggestion dominant emotion selected either good emotion. value observation learning function. learning function chooses three behaviors namely; avoidobstacles seek-light wall-follow. previous work researchers proposed model goal system emotions modelled explicitly goal system although often inspired them. goal system built homoeostatic variables tries maintain within fixed bounds. output goal system provides reinforcement reward learning algorithm. picard proposed system motivation learning decision making influenced cognition emotion internal rewards external rewards environment. matrices represented cognition emotion internal value function two. ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines picard used physical autonomous robot named roco. roco collaboration media lab’s affective computing group robotic life group. roco’s goal solicit attention pleasure user el-nasr used simulated named peteei. peteei acronym evolving emotional intelligence. users interact predefined actions provides feedback predefined sounds. user provided survey form rate believability pet. concluded emotion improved believability pet. separate study found introduction learning improved system created closer simulation behavior real pet. study broekens simulated robot live continuous grid world environment consisting wall food path patches. features world observable agent. agent cannot walk walls walk path food. walls path neutral food reinforcement cell grid assumed object. experiment made compare social reinforcement learning normal on-social reinforcement learning. experiment showed affective interaction humanin-the-loop learning provide significant benefit efficiency reinforcement learning robot continuous grid world. picard simulated single-step decision making two-armed bandit type gambling task maze task. showed addition affective anticipatory reward used improving effectiveness learning decision making. study marinier laird maze made agent navigate. three agent types tested standard reinforcement learning agent agent emotions mood full agent included mood. showed agents emotion learn fast relative standard reinforcement learning agent. kuremoto made computer simulations pursuit problems verify effectiveness proposed method. environment grids prey move position hunters find capture prey. hunter starts another starts experiments made compare proposed method plain q-learning method. results shown learning efficiency enhanced proposed method compared conventional q-learning. fig. shows conceptual framework proposed agent. explanation starts bottommost square. position agent relative target gives rise certain emotions table shows value comparisons resulting emotion. represents actual value norm represents average represents expected value based current stimuli. proposed agent subset shown table include four basic emotions namely sadness fear anger. although ekman listed includes disgust surprise argued disgust drive rather emotion strongly related body state similarly drives like pain hunger jack also shows humans four basic emotions include surprise disgust. variable along x-axis. function based function scaled logarithmically power regression line shows best far. expected number steps initially equal average value increments every step past average. actual value shortest known path current position target. event unknown position maintains value previous actual path. study participants normal emotion processing engaged card-drawing task. drawing dangerous decks consequently experiencing losses associated negative emotions subsequently made safer lucrative choices. words experiencing negative affect sadness anger fear humans tend prefer safer higher rewards choices. individuals motivated implicit goal reward acquisition angry people tend process heuristically thinking alternate solutions hand happy individuals likely optimize sacrifice decision making rather maximizing achieve best outcome emotions found strongly affect kinematics locomotion particularly walking speed. barliya found anger happiness energetic sadness fear also relation speed. observation also mirrors study crane gross says happy angry people tend move faster frightened ones. table summarizes effect emotions speed. ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines compare performance standard q-learning agent proposed affective agent simulations pursuit problems executed. discrete environment consisting grid. goal prey situated center position autonomous agent position start. setup adaptation kuremoto’s test fig. shows test environment. agent simulations episodes each value parameter varied increments steps. number steps taken measured episode taking note episode optimal path target found. standard agent using ε-greedy action selection algorithm select next position whether down left right current position. proposed agent decision select next position depends whether agent happy case randomly chooses next position goes highest reward otherwise. standard agent move constant step at-a-time fashion proposed agent either move step afraid steps angry happy. decision number steps take chosen random. table doesn’t show significant difference agents terms number steps episode fig. shows epsilon standard agent lower step count episode proposed agent performs better. proposed agent doesn’t take ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines analysis affective agent’s affect profile show dominant emotion comes play anger dominant emotion earlier episodes would superseded anger latter episodes. fig. shows behavior. means proposed agent explore earlier episodes exploit latter episodes. table shows statistical comparison total number steps optimal path found. t-test shows finding optimal path proposed affective agent performs better. evident fig. shows proposed agent lesser number steps except throughout simulation starting high reaching decreasing near end. also interesting note proposed affective agent average number episodes required find optimal path value ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines cognitive neuroscientific perspectives. global gadanho hallam emotion-driven learning animat control proceedings fifth international conference simulation adaptive behavior animals animats vol. picard affective-cognitive learning decision making motivational reward framework affective agents affective computing intelligent interaction. springer jack rachael schyns dynamic facial expressions emotion transmit evolving hierarchy signals time current biology vol. http//www.cell.com/currentbiology/abstract/s-- investigate influence emotions q-learning agent affective agent proposed paper. results show proposed agent requires less number steps finding exploration/exploitation ratio. gradual decrease exploration/exploitation ratio indicates long total number steps affective agent lower standard q-learning agent. study done area especially multiple agents either competitive cooperative mode. another avenue study would test theory targets move. finally study done incorporating behavior proposed agent compare performance optimistic agent pessimistic one. research partially supported philippine council industry energy emerging technology research development department science technology philippine higher education research network russell circumplex model affect. journal personality social psychology vol. kuremoto tsurusaki kobayashi mabu obayashi improved reinforcement learning system using affective factors robotics vol. ieee international conference humanoid nanotechnology information technology communication control environment management institute electrical electronics engineers inc. philippine section november hotel centro puerto princesa palawan philippines barliya omlor giese berthoz flash expression emotion kinematics locomotion experimental brain research vol. crane gross motion capture emotion affect detection whole body movement affective computing intelligent interaction ser. lecture notes computer science paiva prada picard eds. springer berlin heidelberg vol. available http//dx.doi.org/./ ---- raghunathan pham negative moods equal motivational influences anxiety sadness decision making organizational behavior human decision processes vol.", "year": 2016}