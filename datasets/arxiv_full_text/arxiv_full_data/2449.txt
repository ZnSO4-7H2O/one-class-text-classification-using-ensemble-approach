{"title": "Learning from networked examples", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Many machine learning algorithms are based on the assumption that training examples are drawn independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in this networked setting, providing significantly improved results. An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.", "text": "many machine learning algorithms based assumption training examples drawn independently. however assumption hold anymore learning networked sample training examples share common objects hence share features shared objects. show classic approach ignoring problem potentially harmful effect accuracy statistics consider alternatives. independent examples discarding information. however clearly suboptimal. analyze sample error bounds networked setting providing signiﬁcantly improved results. important component approach formed efﬁcient sample weighting schemes leads novel concentration inequalities. recently increasing interest network-structured data data social networks economic networks citation networks chemical interaction networks important challenge data concerning related objects cannot assumed independent. precisely assumption made many approaches ﬁeld statistics machine learning observations drawn independently identically ﬁxed distribution. however assumption hold anymore observations extracted network. many practical approaches supervised learning networks ignore problem learn models classic machine learning techniques. work extent supported well-developed theory provides generalization guarantees i.i.d. case function number training examples. consequence miss opportunities learn numerous dependencies training examples. introduce framework explicitly representing dependencies among examples. framework allows encoding domain knowledge form basis studies preparing machine learning effort networked data. show concentration inequalities networked examples demonstrate applicability results learning theory upgrading empirical risk minimization result networked data. process improve earlier concentration inequalities janson improve earlier learning guarantees even original unweighted erm. propose novel learning scheme based appropriate weighting networked examples efﬁciently implementable solving linear program satisﬁes monotonicity ensures always better networked dataset becomes larger usunier deﬁne interdependent training data closely related networked examples make similar assumptions show generalization error bounds classiﬁers trained type data. pac-bayes bounds classiﬁcation non-i.i.d data established based fractional colorings graphs results also hold speciﬁc learning settings ranking learning stationary βmixing distributions. establishes concentration inequalities based idea fractional chromatic numbers entropy methods fractionally sub-additive fractionally self-bounding functions dependent variables. results based janson’s work dependency graphs represent examples relations. paper different representation networked examples improve concentration inequalities potentially improve existing learning results. besides propose better learning scheme existing works. wang considered similar setting networked examples. work assumes bounded covariance pairs examples connected edge assumed model. model show learning guarantees wang shows corrections bias statistical hypothesis tests. mixing conditions also used model non-i.i.d. samples. example learning performance regularized classiﬁcation regression algorithm using non-i.i.d. sample investigated assumption training sample satisﬁes mixing conditions. modha masry presented bernstein-type inequality stationary exponentially α-mixing processes based effective number bernstein-type inequalities networked random variables assigns weights examples. however assumptions training sample different main techniques distinct. refer interested readers references therein details mixing conditions. e.g. learning local conditional probability functions directed models probabilistic relational models logical bayesian networks relational bayesian networks huge literature learning features existence edges graph refer reader survey rossi section introduce networked examples. basic intuition networked example combines number objects carry information every object shared among several examples network. sharing information also makes explicit dependencies examples. feature vectors suitable data structures. usual supervised learning example pair input target value. input example collection features objects hypergraph-based representation together weaker independence assumption powerful expressive ability cover many existing models. example many previous works treat ranking problem binary classiﬁcation networked data. learning rank consider pairwise document ranking problem extensively studied information retrieval hypergraph vertex documents every hyperedge contains document vertices every document described feature vector could words; every training example contains documents target value example order documents i.e. means ﬁrst document ranked higher following example make simpliﬁed version problem applying machine contrast example above every learning techniques decide projects invest. hyperedge example investment vertices cardinality every hyperedge different other. investment organizations banks research councils provide ﬁnancial-related support e.g. loans grants business projects research projects. organizations machine learning techniques help deciding projects invest. hypergraph-structured every training example contains group people take charge project. target value example return investment. common people involved several different projects hypergraph. model results also applied important subclass hypergraphs movie rating consider problem movie rating. tripartite hypergraph vertex cinemas. every hyperedge contains person vertex movie vertex cinema vertex. feature space also three parts every person described feature vector gender nationality every movie described feature vector genre actor popularity every cinema vertex described feature vector location equipments then space feature vectors complete examples consisting concatenation person feature vector movie feature vector cinema feature vector. target value example rating person gave movie concerned cinema e.g. though networked examples independent still need assume weaker form independence examples. would make assumption dependence examples could strong perfectly correlate. situation possible learn. worth pointing weak independence assumption hold realworld situations already better approximation classic i.i.d. assumptions. instance consider problem learning rank weak independence assumption usually satisﬁed whether know target value speciﬁc pair documents depends whether collected data independent features documents. besides similar assumptions also made several previous works important aspect theory presented paper understood better estimation effective sample size networked datasets. generally speaking deﬁne effective sample size dataset particular statistical task estimating number examples i.i.d. dataset would need allow estimating accurately done networked dataset paper consider mean value estimation concentration inequalities references. classical concentration inequalities bernstein inequality bennett inequality chernoff-hoeffding inequalities used analyze generalization errors learning problems i.i.d. samples. deﬁnition given hypergraph call {ξi}n g-networked random variables exist functions {φv}v∈v independent random variables indexed vertices feature example deﬁnition b-fold hyperedge-coloring hypergraph assignment colors every hyperedge adjacent hyperedges color common. b-fold hyperedge-chromatic number hypergraph smallest number colors needed obtain b-fold hyperedge-coloring hyperedges fractional hyperedge-chromatic number though janson’s result variations foundation existing theoretical work related learning networked examples begin technical discussion following examples show necessary possible improve janson’s result denote effective sample size janson’s method cess i.e. cess n/χ∗g hypergraph result reasonable networked random variables highly controlled shared vertex. however networked random variables highly correlated intuitively variables cannot correlated networked random variables hence effective sample size would larger indeed case show corollary effective sample size cannot obtained fractional-coloring-based approaches methods consider examples dependent details dependent. details matter example shows subset networked random variables overlap fractional-coloring-based approaches deal worst case networked random variables share common vertex ignore details hypergraph. natural question whether only mess beat cess. particular cases without issue details mess still larger cess? following example shows answer yes. example consider cycle hyepredges i.e. hyperedge intersect hyperedges fractional hyperedge-chromatic number cess corollary improve result mess note example exist issue details mentioned example mess cess still exists. examples hypergraphs considered symmetric structures equally weighted estimator seems work well. following example shows simple estimator always good satisfy monotonicity roughly speaking given i.i.d. examples sample size larger would expect accurate estimator. similarly super-hypergraph effective sample size g′-networked examples smaller g-networked examples. intuition data learn better else select subsample learn. example above don’t ignore hyperedges obtain better effective sample size cess? idea generalized select maximum matching hypergraph examples corresponding matching. easy section show concentration bounds random variables allowed nonidentical weights. concentration inequalities cannot obtained janson’s method prove lemmas. next section results bound generalization errors learning networked examples. remark weighting scheme also matching method every weight must either i.e. vectors correspond matchings hypergraphs. original concentration inequalities applied directly weighting scheme since examples weight mutually independent. besides since larger matchings provide tighter bounds tend maximum matching whose called matching number denoted however general np-hard problem maximum matching hypergraphs moreover maximum matching problem also apxcomplete problem would expect efﬁcient algorithm achieve good approximation either. also derive concentration inequalities networked analogues bernstein chernoff-hoeffding inequalities proofs appendix. inequalities used next section provide learning guarantees. maximum called maximum fractional matching optimal value linear program called fractional matching number hypergraph denoted ν∗g. deﬁned maximum fractional matching vector. efﬁcient solvers including simplex method efﬁcient practice recent interior-point methods interior-point method solves time number decision variables number constraints. practice usually every hyperedge connect many vertices vertex incident many hyperedges usually sparse. almost solvers perform signiﬁcantly better sparse lps. using theorem also improve janson’s inequalities weighting scheme. fractional matching satisfy requires maxv∈vg maximum degree following corollary. hyperedge-chromatic number i.e. χ∗g. fact generally ensures inequalities corollary provide tighter bounds theorem addition number exist hypergraphs χ∗g/ωg hence improvement corollary theorem arbitrarily large. example consider projective planes order known exists projective plane order whenever prime power. fractional hyperedge-chromatic number subhypergraph projective plane equal hyperedge number. datasets janson’s inequalities fail offer useful bounds corollary provides signiﬁcantly betdifferent hypergraphs possible n/ωg n/ωg n/ωg therefore whether scheme provides tighter bounds scheme depends hypergraph. however n/ωg smaller ν∗g. hence scheme always gives better concentration bounds schemes. hoeffding presented concentration inequalities u-statistics important class statistics networked random variables. results also improve concentration inequalities. example consider one-sample u-statistics. prove inequality construct hypergraph one-sample u-statistic. hypergraph vertices consider independent random variables {si}m features vertices. statistic equally weighted sample mean networked random variables deﬁned hyperedges. inequality proved letting projective planes truncated projective planes theoretical interest. fact special cases block designs studied ﬁeld experimental design ﬁeld studies points feature space measure maximize certain experimental objectives diversity independence training data. section results obtained previous section show generalization performance guarantees learning networked examples making relaxed assumptions previous sections. context speciﬁc framework principles applied many paradigms learning theory e.g. structural risk minimization showed weighting scheme provides clearly better properties classical approaches. networked examples hypergraph combination features vertices hyperedge target value hyperedge case dataset called consider least square regression goal minimizer expected risk minimization taken measurable functions unfortunately since probability distribution unknown cannot computed directly. good approximation sample. empirical risk minimization principle minimizer empirical risk properly selected hypothesis space i.e. error need choose proper hypothesis space. complexity hypothesis space usually measured terms covering number entropy number vc-dimension etc. covering numbers deﬁned vc-dimensions shown evgeniou pontil vc-dimension suitable real-valued function classes section random variables presented real-valued functions therefore measure complexity hypothesis space uniform covering number. consider three weighting schemes different sample error bounds related different important parameters hypergraphs. ﬁrst weighting schemes straightforward upper bound point view waste information provided networked examples. third weighting scheme reaches better sample error bound solving linear program discussed section ﬁrst consider weighting scheme learns networked examples i.i.d. i.e. without weighting function network structure. corollary bound sample error scheme result shows bound sample error relies sample size also depends maximum degree larger sample result poorer sample error bound since also become larger. remember almost previous works deal scheme results depend n/χ∗g theorem depends n/ωg improves results signiﬁcantly. paper considered problem learning networked data. proposed several schemes weighting training examples allow using available training data large extent mitigating dependency problem. particular weighting scheme allows generalizing large fraction existing statistical learning theory. weights weighting schemes computed efﬁciently. presented theory forms step towards statistically sound theory learning networks. assumption weaker assumption studied already general still sufﬁciently powerful properly model range real-world scenarios. ﬁrst step direction would develop measure assess strength dependency features vertices inﬂuence learning task hand. design active learning methods structured data another useful topic i.e. study query strategies choose objects examples network perform experiments order learn good predictor minimal cost. also interesting work study implications theory algorithms learning settings tasks e.g. cross-validation bootstrapping", "year": 2014}