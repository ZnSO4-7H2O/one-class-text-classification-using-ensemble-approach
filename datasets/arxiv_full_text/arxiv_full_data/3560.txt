{"title": "Eye In-Painting with Exemplar Generative Adversarial Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons.", "text": "paper introduces novel approach in-painting identity object remove change preserved accounted inference time exemplar gans exgans type conditional utilize exemplar information produce high-quality personalized in-painting results. propose using exemplar information form reference image region in-paint perceptual code describing object. unlike previous conditional formulations extra information inserted multiple points within adversarial network thus increasing descriptive power. show exgans produce photo-realistic personalized in-painting results perceptually semantically plausible applying task closedto-open in-painting natural pictures. benchmark dataset also introduced task inpainting future comparisons. every around pictures captured shared social networks large percentage featuring people-centric content. little doubt realistic face retouching beautiﬁcation algorithms growing research topic within computer vision machine learning communities. examples include red-eye ﬁxing blemish removal patch matching poisson blending used create plausible-looking results full manipulation face appearance like beautiﬁcation attribute transferral face frontalization synthetic make-up also becoming popular. however humans sensitive small errors facial structure specially faces well-known moreover so-called uncanny valley difﬁcult impediment cross manipulating facial features. recently deep convolutional networks produced high-quality results in-painting missing regions pictures showing natural scenery particular problem facial transformations learn preserve features global lighting skin tone also encode notion semantic plausibility. given training sufﬁcient size network learn human face should look like in-paint accordingly preserving overall structure face image. paper focus particular problem in-painting. dnns produce semanticallyplausible realistic-looking results deep techniques preserve identity person photograph. instance could learn open pair closed eyes guarantee encoded model eyes correspond original person’s speciﬁc ocular structure. instead dnns insert pair eyes correspond similar faces training leading undesirable biased results; person distinguishing feature reﬂected generated part. generative adversarial networks speciﬁc type deep network contain learnable adversarial loss function represented discriminator network gans successfully used generate faces scratch in-paint missing regions face particularly well-suited general facial manipulation tasks discriminator uses images real faces guide generator network producing samples appear arise given ground-truth data distribution. variation conditional-gans constrain generator extra information used generate images based user generated tags however type personalization described previously considered within literature. paper extends idea using extra conditional information introduces exemplar gans type cgan extra information corresponds directly identifying traits entity interest. furthermore assume extra information available inference time. believe reasonable assumption since multiple images objects readily available. exemplar data restricted images prove perceptually-coded version object also used exemplar. motivation exemplar data twofold. first utilizing extra information exgans hallucinate textures structure scratch still retain semantics original image. second output images automatically personalized. instance inpaint pair eyes generator another exemplar instance eyes ensure identity retained. finally exgans differ original formulation cgan extra information used multiple places; either form perceptual loss hint generator discriminator. propose general framework incorporating extra exemplar information. direct application show using guided examples training gans perform in-painting produces photo-realistic identity-preserving results. previous approaches opening closed eyes photographs generally used example photos burst photographs subject similar pose lighting conditions produced ﬁnal results mixture patch matching blending however technique take full advantage semantic structural information image global illumination pose subject. small variations lighting incorrect gaze direction produce uncanny results seen fig. besides classic computer vision techniques recent research focused using deep convolutional networks perform variety facial transformations. speciﬁcally within body work applications gans faces numerous many gans able generate photo-realistic faces single lowdimensional vector pushing results uncanny valley realm reality. fader networks figure comparison commercial state opening algorithm adobe photoshop elements proposed exgan technique exemplar original images shown respectively. expand idea training make element low-dimensional noise vector correspond speciﬁc face attribute beards glasses. directly manipulating elements parts transferred changed demand including opening closing mouth changing frown smile. however identity preserved technique. in-painting studied extensively without deep networks exemplar in-painting iterative algorithm decomposes image structural textured components holes reconstructed combination in-painting texture synthesis. technique used remove large objects images effectiveness compared deep methods shown exemplar in-painting struggles complex structured inpainting recently cgans used success in-painting natural images using extra information remaining portions image in-paint. generator network learns missing regions image discriminator network learns judge difference in-painted real images take advantage discontinuities inpainted original regions. forces generator produce in-painted results smoothly transition original photograph directly sidestepping need pixel blending. besides general case in-painting scenes gans also used in-paint regions face inference time gans must rely information present training incapable personalized face in-paintings unless particular face also exists training set. finally particular relevance work multi-view face synthesis speciﬁcally approaches attempt preserve identity given face. face identiﬁcation regime pose invariance particularly important previous work focused developing various identitypreserving objectives. approach inputs training images containing multiple views face attempts generate similar views separate input face inference time. identity-preserving loss proposed uses perceptual distance faces manifold outlined objective minimized. however unlike aforementioned approaches make assumption reference image available inference time. like approaches perceptual code generated reference face also propose providing generator reference image also help identity preservation. instead relying network generate images based data seen training introduce exgans second source related information guide generator creates image. datasets developed images made available online reasonable assume second image particular object exists inference time. example in-painting face reference information could second image person taken different time different pose. however instead directly using exemplar information produce image network learns incorporate information semantic guide produce perceptually-plausible results. consequently learns utilize reference data still retaining characteristics original photograph. propose separate approaches exgan inpainting. ﬁrst reference-based in-painting reference image used generator guide discriminator additional information determining generated image real fake. second approach code-based in-painting perceptual code created entity interest. in-painting code stores compressed version person’s eyes vector also used several different places within generative discriminator networks. formally approaches deﬁned two-player minimax game objective conditioned extra information similar extra information original image patches removed combination these. additional content loss term added objective. framework general potentially applied tasks inpainting. assume image training exists corresponding reference image therefore training deﬁned tuples in-painting image person potentially taken different pose. patches removed produce learning objective deﬁned better generalization reference images corresponding given also utilized expands training tuples comprised cartesian product image-to-be-in-painted reference image rn}. code in-painting code-based in-painting datasets number pixels image assume exists compressing function r|i| |i|. then image in-painted corresponding reference image code generated using given codiﬁed exemplar information deﬁne adversarial objective compressing function deterministic function auto-encoder general deep network projects example onto manifold. ﬁnal term optional loss measures distance generated image original reference image nator developed discriminator processes whole face zoomed-in portion image contains eyes global adversarial loss enforces overall semantic consistency local adversarial loss ensures detail sharpness generated output. outputs global local convolutional branches concatenated passed ﬁnal sigmoid. additional global branch added discriminator reference image used input case outputs three branches concatenated. next possibility generator network could take input tested alternative architecture fully-convolutional generator. generator uses encoder-decoder architecture downsampling upsampling layers dimensional fully-connected bottleneck layer. bottleneck concatenated code resulting overall dimensionality output bottleneck. code also used perceptual loss term furthermore code appended penultimate ﬁxed-size output discriminator. dimensions code much larger outputs original discriminator experimented feeding global local outputs code small two-layer fully-connected network ﬁnal sigmoid order automatically learn best weighting code convolutional discriminator. remainder paper reference code-based exgans used architecture. generate trained separate auto-encoder compressing function non-standard architecture. training encoder took single input decoder portion autoencoder split left right branch separate targets left right eyes. forces encoder learn duplicate features common eyes also encode distinguishing features general encoded dimensional ﬂoat vector codes combined form dimensional code. unless otherwise speciﬁed activations used convolution layers. also implemented onesided label smoothing probability full listing model architectures given supplemental material. exgans require dataset contain pairs images object types datasets common. observed require large number unique identities sufﬁcient generalization. high resolution images taken variety environments lighting condifigure general architecture exemplar gan. overall training summarized mark eyes input image; in-paint image reference image code guide; compute gradient generator’s parameters content/reconstruction loss input image in-painted image; compute gradient discriminator’s parameters in-painted image another real ground truth image reference image code; backpropagate discriminator error generator. optionally generator’s parameters also updated perceptual loss. reference-based exemplar gans compressing functions identity function. perceptual space. deep network corresponds measuring distance generated reference images low-dimensional manifold. note generator originally fully-convolutional takes input architecture must modiﬁed handle arbitrary scalar vector. overall layout codereference-based exgans depicted fig. experiments used standard convolutional generator bottleneck region containing dilated convolutions similar generator proposed smaller channel count interior layers network generating eyes restricted domain general in-painting. input generator image portions in-paint removed stacked one-channel binary mask indicating regions ﬁll. generator could take additional four channels values reference image another -channel mask indicating locations. locations detected prior training stored dataset. objective optimized using adam parameters order emphasize viability exemplar in-painting distance used reconstruction loss binary cross-entropy used adversarial loss. learning rate tricks gradient regularization controltheory approach hyperparameters swept included learning rates relative weight discriminator network weight assigned perceptual loss points network reference image code. full table various results experiments given supplemental material. order best judge effects codereference-based exgans avoided mixing codes reference images single network. throughout section compare contrast results three models non-exemplar architecture identical global/local adversarial difference smaller channel count generator best reference image exemplar best code-based exemplar gan. tried multiple architectures model introduced produced best non-exemplar results. note comparison base architecture hyperparemters exception code-based uses encoder-decoder style generator. interestingly learning rate could used types generators likely similar number parameters depth. particular setup perceptual loss little overall effect ﬁnal quality generator output; instead better results generated using code directly generator itself. fig. show effect exemplars overreconstruction loss. addition codes content loss non-exemplar decreased adding reference images decreased loss training models overall content loss least decreasing adversarial loss tended produce best results. training runs learning rate generator discriminator resulted well-behaved loss decrease time. however in-painting determined content loss entirely representative ﬁnal perceptual quality issue discussed section tions permits exgan able in-paint eyes wide variety input photographs. addition including images without distractors non-extreme poses improved quality sharpness generated eyes. able utilize celeba dataset contains unique identities. furthermore photos celeba usually taken unnatural environments carpet photographs movie premieres. megaface dataset provides suitable training many images contain human faces include faces sunglasses extreme poses. desired ﬁner-grained control certain aspects dataset ensuring image group contained individual high conﬁdence distracting objects face. order circumvent limitations pre-existing datasets developed internal training roughly million d-aligned images around individuals. individual least images present dataset. every image training contained person eyes opened force network learn in-paint open eyes. external replication purposes developed in-painting benchmark high-quality images celebrities scraped web. contains around individual identities total images least photographs celebrity. additional created publicly-available benchmark called celeb-id. note network trained internal dataset thereby making impossible network overﬁt images shown paper celebrity images training. training epoch individual image inpainted second different image used either example network used generate generator discriminator trained variable finally figs. show additional qualitative results celebrity validation generated exgan uses code-based exemplar generator discriminator perceptual loss. local global in-painted images shown along reference image used in-painting. evident network matches original shape accounts pose lighting conditions in-painted image. cases fig. network match iris color exactly likely mismatch shape would incur higher content adversarial loss. describe solutions problem section general content adversarial losses oneto-one proxies perceptual quality discussed many cases network content loss produced training results looked perceptually worse another network slightly higher content loss. example refer fig. includes output network different values losses. although effect simply example overﬁtting also observed poor results lower loss values training set. observation justiﬁes fact perceived quality pixel difference correlated certain point. order combat effect stopped training early form regularization. addition measured several perceptual metrics course model’s training including msssim inception score score neither ms-ssim score inception score correlated strongly perceptual quality. believe inception score speciﬁcally correlate based scores interior layers googlenet trained image classiﬁcation. generated images belong class network activations vary enough ﬁne-grained details around eye. figure comparison ground truth nonexemplar exemplar-based results. exgan uses reference image generator discriminator shown column exgan uses code shown column figure comparison ground-truth image results model trained epoch loss results model epoch loss despite lower content loss model trained longer produces blurrier results. score better metric perceptual quality; model score epoch epoch score correlate strongly perceived quality. images fig. score increased along blurriness image. therefore postulate in-painting general best metric compare models score accurately corresponds sharpness deﬁnition around generated eye. list metrics three best models given table order verify method performed perceptual test judge quality obtained results. test presented pairs images person pair contained reference image real image pair contained reference image different in-painted image. photographs selected internal dataset offered variety pose lighting generic celebrity datasets. participants asked pick pair images in-painted. time participants either picked generated image unsure real image pair. common cause failure occlusions glasses hair covering eyes original reference images. suspect training variable sized masks could alleviate issue. figure results generated code-based exemplar gan. columns represent reference image image in-paint ground-truth global image in-painted global image ground-truth local image in-painted local image. exemplar gans provide useful solution image generation in-painting region image sort identifying feature. provide superior perceptual results incorporate identifying information stored reference images perceptual codes. clear example capabilities demonstrated in-painting. exemplar gans general framework extended tasks within computer vision even domains. future wish combinations reference-based code-based exemplars using reference generator code discriminator. work kept approach separate order show approaches viable highlight differences results models using references codes. observed in-painting quality sensitive mask placement size future masks square generator utilize remaining context around eye. addition believe assigning higherweighted loss color iris tracking result generated color closely matches reference image. finally believe applying exemplar gans in-painting tasks ﬁlling missing regions natural uniquely identiﬁable scene lead superior results. figure additional closed-eye-opening results generated reference-based exemplar gan. column reference image column in-painted version images column generated exemplar gan.", "year": 2017}