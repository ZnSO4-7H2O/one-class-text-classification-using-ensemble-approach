{"title": "Universal adversarial perturbations", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.", "text": "figure added natural image universal perturbation image causes image misclassiﬁed deep neural network high probability. left images original natural images. labels shown arrow. central image universal perturbation. right images perturbed images. estimated labels perturbed images shown arrow. given state-of-the-art deep neural network classiﬁer show existence universal small perturbation vector causes natural images misclassiﬁed high probability. propose systematic algorithm computing universal perturbations show state-of-the-art deep neural networks highly vulnerable perturbations albeit quasiimperceptible human eye. empirically analyze universal perturbations show particular generalize well across neural networks. surprising existence universal perturbations reveals important geometric correlations among high-dimensional decision boundary classiﬁers. outlines potential security breaches existence single directions input space adversaries possibly exploit break classiﬁer natural images. single small image perturbation fools state-of-the-art deep neural network classiﬁer natural images? show paper existence quasi-imperceptible universal perturbation vectors lead misclassify natural images high probability. specifically adding quasi-imperceptible perturbation natural images label estimated deep neural network changed high probability perturbations dubbed universal imageagnostic. existence perturbations problematic classiﬁer deployed real-world environments exploited adindeed perturbation versaries break classiﬁer. process involves mere addition small perturbation natural images relatively straightforward implement adversaries real-world environments relatively difﬁcult detect perturbations small thus signiﬁcantly affect data distributions. surprising existence universal perturbations reveals insights topology decision boundaries deep neural networks. summarize main contributions paper follows show existence universal image-agnostic perturbations state-of-the-art deep neural networks. propose algorithm ﬁnding perturbations. algorithm seeks universal perturbation training points proceeds aggregating atomic perturbation vectors send successive datapoints decision boundary classiﬁer. show universal perturbations remarkable generalization property perturbations computed rather small training points fool images high probability. show perturbations universal across images also generalize well across deep neural networks. perturbations therefore doubly universal respect data network architectures. explain analyze high vulnerability deep neural networks universal perturbations examining geometric correlation different parts decision boundary. robustness image classiﬁers structured unstructured perturbations recently attracted attention despite impressive performance deep neural network architectures challenging visual classiﬁcation benchmarks classiﬁers shown highly vulnerable perturbations. networks shown unstable small often imperceptible additive adversarial perturbations. carefully crafted perturbations either estimated solving optimization problem step gradient ascent result perturbation fools speciﬁc data point. fundamental property adversarial perturbations intrinsic dependence datapoints perturbations specifically crafted data point independently. result computation adversarial perturbation data point requires solving data-dependent optimization problem scratch uses full knowledge classiﬁcation model. different universal perturbation considered paper seek single perturbation vector fools network natural images. perturbing datapoint involves mere addition universal perturbation image finally emphasize notion universal perturbation differs generalization adversarial perturbations studied perturbations computed mnist task shown generalize well across different models. instead examine existence universal perturbations common data points belonging data distribution. formalize section notion universal perturbations propose method estimating perturbations. denote distribution images deﬁne classiﬁcation function outputs image estimated label main focus paper seek perturbation vectors fool classiﬁer almost datapoints sampled seek vector coin perturbation universal represents ﬁxed image-agnostic perturbation causes label change images sampled data distribution focus case distribution represents natural images hence containing huge amount variability. context examine existence small universal perturbations norm misclassify images. goal therefore satisﬁes following constraints algorithm. images sampled distribution proposed algorithm seeks universal perturbation fooling data points algorithm proceeds iteratively data points gradually builds universal perturbation illustrated fig. iteration minimal perturbation sends current perturbed point decision boundary classiﬁer computed aggregated current instance universal perturbation. details provided current universal perturbation fool data point seek extra perturbation minimal norm allows fool data point solving following optistandard classiﬁer several efﬁcient approximate methods devised solving problem following approach efﬁcency. noticed objective algorithm smallest universal perturbation fools data points sampled distribution rather perturbation sufﬁciently small norm. particular different random shufﬂings naturally lead diverse universal perturbations satisfying required constraints. proposed algorithm therefore leveraged generate multiple universal perturbations deep neural network ﬁrst experiment assess estimated universal perturbations different recent deep neural networks ilsvrc validation report fooling ratio proportion images change labels perturbed universal perturbation. results reported respectively numerical values chosen order obtain perturbation whose norm signiﬁcantly smaller image norms perturbation quasi-imperceptible added figure schematic representation proposed algorithm used compute universal perturbations. illustration data points super-imposed classiﬁcation regions shown different colors. algorithm proceeds aggregating sequentially minimal perturbations sending current perturbed points outside corresponding classiﬁcation region then update rule given ppξ. several passes data performed improve quality universal perturbation. algorithm terminated empirical fooling rate perturbed data exceeds target threshold stop algorithm whenever detailed algorithm provided algorithm interestingly practice number data points need large compute universal perturbation valid whole distribution particular much smaller number training points natural images. results listed table result reported used compute perturbation well validation observe networks universal perturbation achieves high fooling rates validation set. speciﬁcally universal perturbations computed caffenet vgg-f fool validation words natural image validation mere addition universal perturbation fools classiﬁer times result moreover speciﬁc architectures also universal perturbations cause googlenet resnet classiﬁers fooled natural images probability edging results element surprise show existence single universal perturbation vectors cause natural images misclassiﬁed high probability albeit quasiimperceptible humans. verify latter claim show visual examples perturbed images fig. googlenet architecture used. images either taken ilsvrc validation captured using mobile phone camera. observe cases universal perturbation quasi-imperceptible powerful image-agnostic perturbation able misclassify image high probability state-of-the-art classiﬁers. refer supp. material original images well ground truth labels. also refer video supplementary material real-world examples smartphone. visualize universal perturbations corresponding different networks fig. noted universal perturbations unique many different universal perturbations generated network. fig. visualize different universal perturbations obtained using different random shufﬂings observe universal perturbations different although exhibit similar pattern. moreover conﬁrmed computing normalized inner products pairs perturbation images normalized inner products exceed shows diverse universal perturbations. universal perturbations computed images training examine inﬂuence size quality universal perturbation. show fig. fooling rates obtained validation different sizes googlenet. note example containing images fool images validation set. result signiﬁcant compared number classes imagenet shows fool large unseen images even using containing less image class universal perturbations computed using algorithm therefore remarkable generalization power unseen data points computed small training images. cross-model universality. computed perturbations universal across unseen data points examine cross-model universality. study extent universal perturbations computed speciﬁc architecture also valid another architecture table displays matrix summarizing universality perturbations across different architectures. architecture compute universal perturbation report fooling ratios architectures; report rows table observe that architectures universal perturbations generalize well across architectures. example universal perturbations computed vgg- network fooling ratio tested architectures. result shows universal perturbations extent doubly-universal generalize well across data points different architectures. noted that adversarial perturbations previously shown generalize well extent across different neural networks mnist problem. results however different show generalizability universal perturbations across different architectures imagenet data set. result shows perturbations practical relevance generalize well across data points architectures. particular order fool image unknown neural network simple addition universal perturbation computed vgg- architecture likely misclassify data point. figure examples perturbed images corresponding labels. ﬁrst images belong ilsvrc validation last images taken mobile phone camera. supp. material original images. visualization effect universal perturbations. gain insights effect universal perturbations natural images visualize distribution labels imagenet validation set. speciﬁcally build directed graph whose vertices denote labels directed edges indicate majority images class fooled label applying universal perturbation. existence edges therefore suggests preferred fooling label images class construct graph googlenet visualize full graph supp. material space constraints. visualization graph shows peculiar topology. particular graph union disjoint components edges component mostly connect target label. fig. illustration connected components. visualization clearly shows existence several dominant labels universal perturbations mostly make natural images classiﬁed figure diversity universal perturbations googlenet architecture. perturbations generated using different random shufﬂings note normalized inner products pair universal perturbations exceed highlights diversity perturbations. table generalizability universal perturbations across different networks. percentages indicate fooling rates. rows indicate architecture universal perturbations computed columns indicate architecture fooling rate reported. images. vgg-f architecture ﬁne-tune network based modiﬁed training universal perturbations added fraction training samples training point universal perturbation added probability original sample preserved probability account diversity universal perturbations pre-compute pool different universal perturbations perturbations training samples randomly pool. network ﬁne-tuned performing extra epochs training modiﬁed training set. assess effect ﬁne-tuning robustness network compute universal perturbation ﬁne-tuned network report fooling rate network. extra epochs fooling rate validation shows improvement respect original network despite improvement ﬁne-tuned network remains largely vulnerable small universal perturbations. therefore labels. hypothesize dominant labels occupy large regions image space therefore represent good candidate labels fooling natural images. note dominant labels automatically found algorithm imposed priori computation perturbations. ﬁne-tuning experiment slightly modiﬁed notion universal perturbations direction universal vector ﬁxed data points magnitude adaptive. data point consider perturbed point x+αv smallest coefﬁcient fools classiﬁer. observed feedbacking strategy less prone overﬁtting strategy universal perturbation simply added training points. repeated procedure obtained fooling ratio general repetition procedure ﬁxed number times yield improvement fooling ratio obtained step ﬁnetuning. hence ﬁne-tuning network leads mild improvement robustness observed simple solution fully immune small universal perturbations. goal section analyze explain high vulnerability deep neural network classiﬁers universal perturbations. understand unique characteristics universal perturbations ﬁrst compare perturbations types perturbations namely random perturbation adversarial perturbation computed randomly picked sample iii) adversarial perturbations mean images perturbation depict phase transition graph fig. showing fooling rate validation respect norm perturbation. different perturbation norms achieved scaling accordingly perturbation multiplicative factor target norm. note universal perturbation computed also scaled accordingly. versal perturbation computed using algorithm achieves fooling rate norm constrained perturbations achieve much smaller ratios comparable norms. particular random vectors sampled uniformly sphere radius fool validation set. large difference universal random perturbations suggests universal perturbation exploits geometric correlations different parts decision boundary classiﬁer. fact orientations decision boundary neighborhood different data points completely uncorrelated norm best universal perturbation would comparable random perturbation. note latter quantity well understood norm random perturbation required fool speciﬁc data point precisely behaves distance data point decision boundary considered imagenet classiﬁcation task data points quantity equal least order magnitude larger universal perturbation substantial difference random universal perturbations thereby suggests redundancies geometry decision boundaries explore. image validation compute minr s.t. easy normal decision boundary classiﬁer vector hence captures local geometry decision boundary region surrounding data point quantify correlation normal vectors decision boundary vicinity data points validation set. binary linear classiﬁers decision boundary hyperplane rank normal vectors collinear. capture generally correlations decision boundary complex classiﬁers compute singular values matrix singular values matrix computed caffenet architecture shown fig. further show ﬁgure singular values obtained columns sampled uniformly random unit sphere. observe that latter singular values slow decay singular values decay quickly conﬁrms existence large correlations redundancies decision boundary deep networks. precisely suggests existence subspace dimension contains normal vectors decision boundary regions surrounding natural images. hypothesize existence universal perturbations fooling natural images partly existence low-dimensional subspace captures correlations among different regions decision boundary. fact subspace collects normals decision boundary different regions perturbations belonging subspace therefore likely fool datapoints. verify hypothesis choose random vector norm belonging subspace spanned ﬁrst singular vectors compute fooling ratio different images perturbation fool nearly images thereby showing random direction well-sought subspace signiﬁcantly outperforms random perturbations fig. illustrates subspace captures correlations decision boundary. noted existence dimensional subspace explains surprising generalization properties universal perturbations obtained fig. build relatively generalizable universal perturbations images. unlike experiment proposed algorithm choose random vector subspace rather chooses speciﬁc direction order maximize overfooling rate. explains fooling rates obtained random vector strategy algorithm showed existence small universal perturbations fool state-of-the-art classiﬁers natural images. proposed iterative algorithm generate universal perturbations highlighted several properties s.-m. moosavi-dezfooli fawzi frossard. deepfool simple accurate method fool deep neural networks. ieee conference computer vision pattern recognition nguyen yosinski clune. deep neural networks easily fooled high conﬁdence predictions unrecogieee conference computer vision nizable images. pattern recognition pages rodner simon fisher denzler. fine-grained recognition noisy wild sensitivity analysis convolutional neural networks approaches. british machine vision conference russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision szegedy sermanet reed anguelov erhan vanhoucke rabinovich. ieee conference going deeper convolutions. computer vision pattern recognition szegedy zaremba sutskever bruna erhan goodfellow fergus. intriguing properties neural networks. international conference learning representations taigman yang ranzato wolf. deepface closing human-level performance face veriﬁcation. ieee conference computer vision pattern recognition pages perturbations. particular showed universal perturbations generalize well across different classiﬁcation models resulting doubly-universal perturbations explained existence perturbations correlation different regions decision boundary. provides insights geometry decision boundaries deep neural networks contributes better understanding systems. theoretical analysis geometric correlations different parts decision boundary subject future research.", "year": 2016}