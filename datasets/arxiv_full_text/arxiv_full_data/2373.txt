{"title": "Efficient Learning of Generalized Linear and Single Index Models with  Isotonic Regression", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints. However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We also provide an empirical study, demonstrating their feasibility in practice.", "text": "generalized linear models single index models provide powerful generalizations linear regression target variable assumed -dimensional function linear predictor. general problems entail non-convex estimation procedures practice iterative local search heuristics often used. kalai sastry recently provided ﬁrst provably eﬃcient method learning sims glms assumptions data fact generated certain monotonicity lipschitz constraints. however obtain provable performance method requires fresh sample every iteration. paper provide algorithms learning glms sims computationally statistically eﬃcient. also provide empirical study demonstrating feasibility practice. input namely vector assume generalized linear models links conditional expectation linear manner i.e. review). simple assumption immediately leads many practical typically link function assumed known parameter estimated using iterative procedure. even setting known aware classical estimation procedure computationally eﬃcient achieves good statistical rate provable guarantees. standard procedure iteratively reweighted least squares based newton-ralphson single index models unknown. here face challenging question jointly estimating come large non-parametric family monotonic functions. issues here statistical rate achievable simultaneous estimation computationally eﬃcient algorithm joint estimation? regards former mild lipschitz-continuity restrictions possible characterize eﬀectiveness joint empirical risk minimization procedure. however issue computationally eﬃciently estimating delicate focus work. note trivial problem general joint estimation problem highly non-convex despite signiﬁcant body literature problem existing methods usually based heuristics guaranteed converge global optimum note recently presented kernel-based method allow learning certain types glm’s sim’s even agnostic setting assumptions made underlying distribution. side formal computational complexity guarantee degrades super-polynomially norm show provably unavoidable setting. recently proposed isotron algorithm provides ﬁrst provably eﬃcient method learning glms sims common assumption monotonic lipschitz assuming data corresponds model. algorithm attained polynomial sample computational complexity sample size dependence depend explicitly dimension. algorithm variant gradient-like perceptron algorithm added twist update isotonic regression procedure performed linear predictions. recall isotonic regression procedure ﬁnds best monotonic dimensional regression function. here well-known pool adjacent violator algorithm provides computationally eﬃcient method task. unfortunately cursory inspection isotron algorithm suggests that computationally eﬃcient wasteful statistically iteration algorithm throws away previous training data requests examples. intuition underlying technical reasons fact algorithm need return function bounded lipschitz constant. furthermore empirically clear deleterious issue work seeks address issues theoretically practically. present algorithms glm-tron algorithm learning glms known monotonic lipschitz l-isotron algorithm general problem learning sims unknown monotonic lipschitz algorithms practical parameter-free provably eﬃcient statistically computationally. moreover easily kernelizable. addition investigate algorithms empirically show feasible approaches. furthermore results show original isotron algorithm perhaps also eﬀective several cases even though algorithm lipschitz constraint. generally interesting note statistical assumption data fact generated leads eﬃcient estimation procedure despite non-convex problem. without making assumptions i.e. agnostic setting problem least hard learning parities noise. setting assume data sampled i.i.d. distribution supported unit ball d-dimensional euclidean space. algorithms analysis also apply case unit ball high -dimensional kernel feature space. assume ﬁxed vector non-decreasing -lipschitz function note plays role generalized linear models since norm arbitrary -lipschitz algorithms work iteratively constructing hypotheses form non-decreasing -lipschitz function linear predictor. algorithmic analysis provides conditions small using statistical arguments guarantee would small well. simplify presentation results standard notation always hides begin simpler case transfer function assumed known problem estimating properly. present simple parameter-free perceptron-like algorithm glm-tron eﬃciently ﬁnds close-to-optimal predictor. note algorithm works arbitrary non-decreasing lipschitz functions thus covers generalized linear models. pseudo-code appears algorithm analyze performance algorithm show algorithm suﬃciently many iterations predictors obtained must nearly-optimal compared bayes-optimal predictor. present l-isotron algorithm applicable harder setting transfer function unknown except non-decreasing -lipschitz. corresponds semi-parametric setting single index models. algorithm present simple parameter-free. main diﬀerence compared glm-tron algorithm transfer function must also learned algorithm keeps track transfer function changes iteration iteration. algorithm also rather similar isotron algorithm main diﬀerence instead applying procedure arbitrary monotonic function iteration diﬀerent procedure lpav lipschitz monotonic function. diﬀerence allows make algorithm practical maintaining non-trivial guarantees followed method empirical studies. running time method proposed slow large-scale datasets remind reader one-dimensional ﬁtting problem thus highly accurate achieved randomly subsampling data turn formal analysis algorithm. formal guarantees parallel previous subsection. however rates achieved somewhat worse additional diﬃculty simultaneously estimating plausible rates sharp information-theoretic reasons based -dimensional lower bounds case thm. easily satisﬁes theorem’s conditions running l-isotron algorithm suﬃciently many iterations choosing hypothesis minimizes based hold-out set. ﬁnding values lpav obtains entire function interpolating linearly points. assuming sorted order formulated quadratic problem following constraints also another result require additional notation. iteration l-isotron algorithm lpav procedure based training sample current direction non-decreasing lipschitz function deﬁne proof uses following proposition relates values proof proposition somewhat lengthy requires additional technical machinery therefore relegated appendix proof lemma uses covering number argument shown part general lemma supplementary material. lemma applies particular combining bound using union bound result well. section present empirical study glm-tron l-isotron algorithms. ﬁrst experiment performed synthetic meant highlight diﬀerence l-isotron isotron algorithm particular show attempting transfer function without lipschitz constraints cause isotron overﬁt complementing theoretical ﬁndings. second experiments comparison glm-tron l-isotron several competing approaches. goal experiments show algorithms perform well real-world data even distributional assumption required theoretical guarantees precisely hold. discussed earlier l-isotron algorithm similar isotron algorithm main diﬀerences first apply lpav iteration best lipschitz monotonic function data apply procedure monotonic function. second diﬀerence theoretical guarantees case isotron required working fresh training sample iteration. theoretical guarantees algorithm computationally eﬃcient might wonder well performs practice. later actually performs quite well datasets examined. however subsection provide simple example shows sometimes repeated ﬁtting non-lipschitz function done isotron algorithm cause overﬁt thus hurt performance compared ﬁtting lipschitz function done l-isotron algorithm. synthetic example construct ﬁrst attribute relevant attribute. however random noise values isotron tends overﬁt using irrelevant attributes. data points observed. hand l-isotron predicts correctly close lipschitz constraint. figure shows link functions predicted l-isotron isotron dataset. repeating experiment times error l-isotron normalized variance values table performance comparison l-isotron algorithms. values reported diﬀerence normalized squared errors negative values indicate better performance l-isotron. several algorithms. include standard logistic regression linear regression simple heuristic algorithm single index models along lines standard iterative maximumlikelihood procedures types problems algorithm works iteratively ﬁxing direction ﬁnding best transfer function ﬁxing optimizing gradient descent. algorithms performed -fold cross validation using fold time test report averaged results across folds. table shows mean squared error algorithms across folds normalized variance values. table shows diﬀerence squared errors algorithms across folds. results indicate performance l-isotron glm-tron comparable regression techniques many cases also slightly better. suggests algorithms work well practice enjoying non-trivial theoretical guarantees. also illustrative transfer functions found algorithms l-isotron isotron figure plot transfer function concrete communities. compare other. plots illustrate fact isotron repeatedly non-lipschitz function resulting piecewise constant function less intuitive smoother lipschitz transfer function found l-isotron algorithm.", "year": 2011}