{"title": "Learning Loss for Knowledge Distillation with Conditional Adversarial  Networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student.", "text": "hand empirical studies suggest performance shallow networks improved learning large networks following student-teacher strategy approaches student networks forced mimic output probability distribution teacher networks transfer knowledge embedded soft targets. intuition dark knowledge contains relative probabilities incorrect answers provided deep wide networks informative representative. example image mistakenly recognized sheep small probability seldom recognized car; soft target output distribution categories image contains information hard target one-hot vector previous works train shallow wide student networks potentially parameters deep teacher networks; ensemble networks teacher train student network similar architecture capacity; particularly train small deep thin network replace shallow wide network acceleration given best teacher time shallow wide vggnet since then design network architecture advanced. resnet signiﬁcantly deepened networks introducing residual connections wide residual networks suggest widen networks better performance. unclear whether dark knowledge state-of-the-art networks based residual connections deep wide help train shallow and/or thin network acceleration. paper focus practical approach improve performance shallow thin modern network learning dark knowledge deep wide network. student teacher networks convolutional neural networks residual connections student network shallow thin much faster teacher network inference. instead forcing output student network exactly mimic soft targets produced teacher increasing interest accelerating neural networks real-time applications. study studentteacher strategy small fast student network trained auxiliary information provided large accurate teacher network. conditional adversarial networks learn loss function transfer knowledge teacher student. proposed method particularly effective relatively small student networks. moreover experimental results show effect network size modern networks used student. empirically study trade-off inference time classiﬁcation accuracy provide suggestions choosing proper student. deep neural networks achieve massive success artiﬁcial intelligence substantially improving stateof-the-art performance various applications. core applications computer vision large-scale image classiﬁcation accuracy reached dnns become comparable humans several benchmark datasets. recent progress towards impressive accomplishment largely driven exploring deeper wider network architectures. despite clear performance boost modern dnns heavy computation memory cost deep wide networks makes difﬁcult directly deploy trained networks embedded system real-time applications. meantime demands cost networks increasing applications mobile devices autonomous cars. dnns really need deep wide? early theoretical studies suggest shallow networks approximate arbitrary functions. recent theorems show depth indeed beneﬁcial expressive capacity networks moreover overparameterized redundant networks easily memorize overﬁt training data surprisingly generalize well practice various explanations investigated secret deep wide networks remains open problem. network introduce conditional adversarial networks transfer dark knowledge teacher student. empirically show loss learned adversarial training advantage hand-engineered loss student-teacher strategy especially relatively small student used. learning loss approach inspired recent success conditional adversarial networks various imageto-image translation applications show generative adversarial nets capable beneﬁting task different image generation. student-teacher strategy help preserve multi-modal nature output distribution. take soft targets image example soft targets predict correct label dog. fact teacher network trained multiple times would produce different correct soft targets randomness training. forcing student network exactly mimic soft targets difﬁcult student smaller capacity teacher. introducing discriminator learning loss approach transfers correlation classes i.e. dark knowledge teacher also preserves multi-modality. network acceleration attracted increasing interest because needs real-time applications artiﬁcial intelligence growing techniques roughly divided three categories precision pruning factorization knowledge distillation. precision methods limited number bits store operate network weights extreme case binary networks -bit represent number acceleration methods sometimes conceptual precision support common still limited. networks also directly modiﬁed pruning factorizing redundant weights either post-processing training ﬁne-tuning stage methods often assume network weights sparse and/or rank efﬁcient networks similar architecture reduced number weights. knowledge distillation principled approach train small neural networks acceleration. slightly abuse name knowledge distillation represent methods train student networks transferring knowledge teacher networks. pioneered approach model compression. trained shallow wide student learning deep teacher primarily designed acceleration. generalized previous methods introducing metric output distribution teacher student well tuning parameter. variants knowledge distillation also applied many different tasks semantic segmentation pedestrian detection face recognition metric learning reinforcement learning regularization. recent preprint presented promising preliminary results cifar- learning small resnet large resnet. another line research focuses transferring intermediate features instead soft targets teacher student approach complement methods directly following design metric output distribution teacher student adversarial networks used learn metric replace hand-engineering. generative adversarial networks extensively studied recent years since trains neural networks generator discriminator adversarial learning process alternatively updates networks. apply conditional setting generator conditioned input images. unlike previous works focus generating editing images target learning loss knowledge distillation requires quite different architecture choices generator discriminator. section introduce learning loss approach based conditional adversarial networks. start recap modern network architectures describe dark knowledge transferred teacher student networks ganbased approach learning loss detailed section figure blocks residual connection convolutional neural networks multi-layer perceptron represents output block. block composed batch normalization activation relu weight layer dropout. higher temperature produces softer probability categories. detailed euclidean distance teacher student logits special case large regular softmax classiﬁcation uses image-label pairs provided cross-entropy loss supervised training neural network represented figure gan-based architecture learn loss knowledge distillation. deep wide teacher pretrained ofﬂine. student network discriminator updated alternatively discriminator aims distinguish logits student teacher networks student aims fool discriminator. additional supervised loss added student discriminator. overview. main idea learning loss transferring knowledge teacher student presented figure instead forcing student exactly mimic teacher minimizing kl-divergence equation knowledge transferred teacher student discriminator gan-based approach. zagoruyko komodakis basic components build deep neural networks achieve stateof-the-art performance. student teacher networks paper based residual convolutional blocks shown figure ﬁrst layer ﬁlters convolution followed stack layers number residual blocks block contains convolution layers equipped batch normalization relu dropout output feature subsampled twice number ﬁlters doubled subsampling shown table widen factor used increase number ﬁlters residual block. last residual block global average pooling fully-connected layer softmax. following sections architecture wide residual networks represented wrn-d-m total depth teacher network deep wide large student network shallow thin small output neural networks image classiﬁcation probability distribution categories. probability generated softmax layer logits represents output last fully connected layer. dimension logits student teacher networks equal number categories. rich information embedded output probability teacher network logits transfer knowledge student network review method provides metric student teacher logits generalized previous methods knowledge distillation. logits vector generated pre-trained teacher network input image represented dimension vector number categories consider training student network generate student logits introducing parameter called temperature generalized softmax layer converts logits vector probability distribution output discriminator dimensional vector entry corresponds either label real/fake. experiments optimizing forms gan-based loss ˜lgan lgan achieves almost identical performance. hence always following sections simplicity compactness discriminator. note equation form auxiliary classiﬁer gans adversarial training becomes much stable discriminator reconstructs category labels besides real/fake. moreover discriminator provide category-level alignment outputs student teacher. student outputs image likely learn teacher outputs predict dogs. provide instance-level information discriminator investigate conditional discriminators input discriminators logits concatenate conditional vector. tried following conditional vectors image convolutional embedding; label one-hot vector embedding; extracted teacher logits. embedding includes several weight layers outputs vector size logits. however turns conditional vectors easy ignored training discriminator. conditional discriminator help practice introduce direct instancelevel alignment training student network below. student update. update student network updating discriminator iteration. updating student network fool discriminator ﬁxing discriminator minimizing adversarial loss meantime student network also trained satisfy auxiliary classiﬁer discriminator lds. besides category-level alignment provided introduce instance-level alignment teacher student outputs discriminator trained distinguish output logteacher student student adversarially trained fool discriminator i.e. output logits similar teacher logits discriminator distinguish. several beneﬁts proposed method. first learned loss effective already demonstrated several image image translation tasks moreover gan-based approach relieves pain hand-engineering loss. though parameter tuning hand-engineering loss replaced handengineering discriminator networks sense empirical study shows performance less sensitive discriminator architecture temperature parameter knowledge distillation. second beneﬁt closely related multi-modality network output. consider previous example classifying image labels relationship categories looks like captured discriminator trained multi-modal logits teacher. however look individual output valid outputs plausible student small capacity exactly produce either outputs. student still beneﬁt knowledge transferred discriminator suggests output similar vectors different vector like discriminator update. describe proposed method rigorous way. student discriminator figure alternatively updated gan-based approach. ﬁrst look update discriminator trained distinguish teacher student logits. multi-layer perceptron discriminator stack residual block shown figure number nodes layer dimension logits i.e. number categories representing discriminator predicts binary value real/fake ﬁxing student network maximize log-likelihood known binary cross-entropy loss adversarial loss knowledge distillation follows original faces major issues. first adversarial training difﬁcult. even replace log-likelihood advanced techniques wasserstein least squares training still slow unstable experiments. second discriminator captures high-level statistics teacher student outputs low-level alignment missing. student outputs learn completely unrelated teacher sample optimizing means image mapped logits vector predicts cat. multi-layer preceptron discriminator gan-based approach. number nodes layer length logits i.e. number categories. -layer used experiments except section study effect depth discriminator. logits teacher generated ofﬂine stored memory. logits pass batch normalization layer mlp. dropout also used discriminator. ﬁrst show proposed method good transferring knowledge teacher student. table presents error rate classiﬁcation three benchmark datasets. teacher deep wide wrn--. student much shallower thinner wrn-- cifars wrn-- imagenet. choose larger student network imagenet dataset contains training samples categories. discussion wisely choosing student architecture section ﬁrst rows table present performance pure supervised learning student teacher networks without knowledge transfer student-teacher strategy. compare gan-based approach knowledge distillation proposed reviewed section choose temperature parameter following gan-based approach detailed section parameter tuned. several observations table deep wide teacher performs much better shallow thin student pure supervised learning. error rate small network trained student-teacher strategy lower bounded teacher performance expected. baseline method helps training small networks cifars help imagenet. difference capacity student small learn knowledge distillation imagenet samples categories. temperature parameter introduced combination learned loss knowledge distillation supervised loss neural network look complicated ﬁrst glance. however component loss relatively simple. moreover since student discriminator learned explicit parameters tuned loss function. experiments next section suggest performance reasonably insensitive discriminator architecture proposed method replace hand-engineering loss knowledge distillation. present experimental results section. implementation details experimental settings provided section beneﬁts proposed method training small student network help large teacher network presented section analyze different components proposed methods section effect depth width student network presented section followed discussion trade-off classiﬁcation accuracy inference acceleration section last section show qualitative visualization output distribution student teacher knowledge distillation. experimental setting consider three benchmark datasets image classiﬁcation imagenet cifar- cifar- imagenet downsampled version imagenet challenge dataset contains training images validation images classes; images downsampled cifar datasets contain training images validation images classes respectively. images also experiments light data augmentation horizontal padding cropping used input images wide residual networks student teacher networks. residual blocks shown figure network architectures table wrn-d-m represents network depth widen factor. teacher network always wrn-- student network changes depth width different experiments. dropout used wrns. stochastic gradient descent optimizer initial learning rate momentum weight decay vinyals dean useful. cifars performs better large extent performs similarly. proposed method improves performance small network three datasets outperforms discuss proposed method detail section. figure presents training curve small student network wrn-- cifar- dataset. loss discriminator gradually decreasing suggests adversarial training steadily makes progress. error rates gan-based method training testing data decreasing. testing error rate ganbased method consistently better pure supervised training student model looks stable epoch surprisingly training error rate pure supervised learning slightly better gan-based method suggests knowledge transfer beneﬁcial generalization. next look different components ganbased approach shown table optimizing adversarial loss category-level knowledge transfer learned loss lgan performs reasonably well. however indirect knowledge provided lgan alone good pure supervised learning category-level knowledge transfer lgan instance-level knowledge transfer improve performance training finally present effect depth discriminator table error rate relatively insensitive depth discriminator. error rate slightly decreases depth increases discriminator generally shallow. discriminator becomes deeper error rate increases adversarial training becomes unstable. decreasing learning rate discriminator sometimes helps introduce parameter tuning proposed method. -layer works reasonably well used experiments keep gan-based method simple. asked similar question convolutional neural networks claimed network least layers convolutions. study modern architecture residual blocks. empirical study suggests even modern architecture network deep wide extent. table presents results pure supervised learning knowledge distillation gan-based approach different student networks cifar-. ﬁrst depth change widen factor minimum depth architecture depth width increase depth parameter size millions inference time seconds minibatch samples cpu. student small wrn-- difﬁcult transfer knowledge teacher student because student limited network capacity. student large wrn-- knowledge distillation gan-based approach improve performance almost good figure error rate inference time parameter size. ﬁgure generated table networks wrn-m labeled circles wrn-d- labeled crosses gan-based approach. largest student smaller faster teacher wrn--. teacher. advantage proposed method obvious relatively small student wrn-. increasing depth effective increasing width wrn. example wrn-- less parameter wrn-- achieves lower error rate. shallow thin network much easier deploy practice. present trade-off error rate inference time parameter size figure ﬁgure generated table changing architecture student network. larger student network accurate also slower. network similar size wrn- wrn-- deep network achieves lower error rate wide network runs slightly faster. studentteacher strategy help improve classiﬁcation performance student network. student network relatively large wrn-- student network trained gan-based approach achieve competitive error rate comparing teacher wrn--. wrn-- smaller faster wrn-- ganbased approach decreases absolute error rate visualization distribution last section experimental results present qualitative visualization gan-based approach. figure presents scaled histogram prediction category cifar-. histogram counted testing samples samples category labeled positive labeled negative histogram scaled positive negative respectively. three plots represent distribution predicted student network trained pure supervised learning teacher network student network trained ganbased approach. histogram middle similar histogram right suggests gan-based approach transfers knowledge teacher student. study student-teacher strategy network acceleration paper. propose gan-based approach learn loss transferring knowledge teacher student. empirically show gan-based approach improve training student network especially student network shallow thin. moreover empirically study effect capacity modern network student provide guidelines wisely choosing student balance error rate acceleration. speciﬁc setting train student smaller faster teacher without loss accuracy. gan-based approach stable easy implement applying several advanced techniques literature. current implementation uses stored logtis teacher network save memory computation. generating teacher logits dropout reliable adversarial training. last ganbased approach naturally extended ensemble networks teacher. logits multiple teacher networks discriminator better performance. investigate ideas future work.", "year": 2017}