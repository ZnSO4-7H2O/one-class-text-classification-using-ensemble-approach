{"title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.", "text": "explore deep reinforcement learning methods multi-agent domains. begin analyzing difﬁculty traditional algorithms multi-agent case q-learning challenged inherent non-stationarity environment policy gradient suffers variance increases number agents grows. present adaptation actor-critic methods considers action policies agents able successfully learn policies require complex multiagent coordination. additionally introduce training regimen utilizing ensemble policies agent leads robust multi-agent policies. show strength approach compared existing methods cooperative well competitive scenarios agent populations able discover various physical informational coordination strategies. reinforcement learning recently applied solve challenging problems game playing robotics industrial applications emerging practical component large scale systems data center cooling successes single agent domains modelling predicting behaviour actors environment largely unnecessary. however number important applications involve interaction multiple agents emergent behavior complexity arise agents co-evolving together. example multi-robot control discovery communication language multiplayer games analysis social dilemmas operate multi-agent domain. related problems variants hierarchical reinforcement learning also seen multi-agent system multiple levels hierarchy equivalent multiple agents. additionally multi-agent self-play recently shown useful training paradigm successfully scaling environments multiple agents crucial building artiﬁcially intelligent systems productively interact humans other. unfortunately traditional reinforcement learning approaches q-learning policy gradient poorly suited multi-agent environments. issue agent’s policy changing training progresses environment becomes non-stationary perspective individual agent presents learning stability challenges prevents straightforward past experience replay crucial stabilizing deep q-learning. policy gradient methods hand usually exhibit high variance coordination multiple agents required. alternatively modelbased policy optimization learn optimal policies back-propagation requires model world dynamics assumptions interactions agents. applying methods competitive environments also challenging optimization perspective evidenced notorious instability adversarial training methods work propose general-purpose multi-agent learning algorithm that leads learned policies local information execution time assume differentiable model environment dynamics particular structure communication method agents applicable cooperative interaction competitive mixed interaction involving physical communicative behavior. ability mixed cooperative-competitive environments critical intelligent agents; competitive training provides natural curriculum learning agents must also exhibit cooperative behavior execution time. adopt framework centralized training decentralized execution allowing policies extra information ease training long information used test time. unnatural q-learning without making additional assumptions structure environment function generally cannot contain different information training test time. thus propose simple extension actor-critic policy gradient methods critic augmented extra information policies agents actor access local information. training completed local actors used execution phase acting decentralized manner equally applicable cooperative competitive settings. since centralized critic function explicitly uses decision-making policies agents additionally show agents learn approximate models agents online effectively policy learning procedure. also introduce method improve stability multi-agent policies training agents ensemble policies thus requiring robust interaction variety collaborator competitor policies. empirically show success approach compared existing methods cooperative well competitive scenarios agent populations able discover complex physical communicative coordination strategies. simplest approach learning multi-agent settings independently learning agents. attempted q-learning perform well practice show independently-learning policy gradient methods also perform poorly. issue agent’s policy changes training resulting non-stationary environment preventing naïve application experience replay. previous work attempted address inputting agent’s policy parameters function explicitly adding iteration index replay buffer using importance sampling deep q-learning approaches previously investigated train competing pong agents. nature interaction agents either cooperative competitive many algorithms designed particular nature interaction. studied cooperative settings strategies optimistic hysteretic function updates assume actions agents made improve collective reward. another approach indirectly arrive cooperation sharing policy parameters requires homogeneous agent capabilities. algorithms generally applicable competitive mixed settings. surveys multi-agent learning approaches applications. concurrently work proposed similar idea using policy gradient methods centralized critic test approach starcraft micromanagement task. approach differs following ways learn single centralized critic agents whereas learn centralized critic agent allowing agents differing reward functions including competitive scenarios consider environments explicit communication agents combine recurrent policies feed-forward critics whereas experiments feed-forward policies learn continuous policies whereas learn discrete policies. recent work focused learning grounded cooperative communication protocols agents solve various tasks however methods usually applicable communication agents carried dedicated differentiable communication channel. method requires explicitly modeling decision-making process agents. importance modeling recognized reinforcement learning cognitive science communities stressed importance robust decision making process agents others building bayesian models decision making. incorporate robustness considerations requiring agents interact successfully ensemble possible policies agents improving training stability robustness agents training. markov games work consider multi-agent extension markov decision processes called partially observable markov games markov game agents deﬁned states describing possible conﬁgurations agents actions ...an observations ...on agent. choose actions agent uses stochastic policy πππθi produces next state according state transition function agent obtains rewards function state agent’s action receives private observation correlated state initial states determined distribution agent aims maximize discount factor time horizon. q-learning deep q-networks q-learning popular methods reinforcement learning previously applied multi-agent settings q-learning makes action-value function policy qπππ function recursively rewritten qπππ γea∼πππ a)]]. learns action-value function corresponding optimal policy minimizing loss target function whose parameters periodically updated recent helps stabilize learning. another crucial component stabilizing experience replay buffer containing tuples q-learning directly applied multi-agent settings agent learn independently optimal function however agents independently updating policies learning progresses environment appears non-stationary view agent violating markov assumptions required convergence q-learning. another difﬁculty observed experience replay buffer cannot used setting since general πππn policy gradient algorithms. policy gradient methods another popular choice variety tasks. main idea directly adjust parameters policy order maximize objective es∼pπππa∼πππθ taking steps direction ∇θj. using function deﬁned previously gradient policy written pπππ state distribution. policy gradient theorem given rise several practical algorithms often differ estimate qπππ. example simply sample γi−tri leads reinforce algorithm alternatively could learn approximation true action-value function qπππ e.g. temporal-difference learning qπππ called critic leads variety actor-critic algorithms policy gradient methods known exhibit high variance gradient estimates. exacerbated multi-agent settings; since agent’s reward usually depends actions many agents reward conditioned agent’s actions exhibits much variability thereby increasing variance gradients. below show simple setting probability taking gradient step correct direction decreases exponentially number agents. proposition consider agents binary actions a=···=an assume uninformed scenario agents initialized then estimating gradient cost policy gradient have baselines value function baselines typically used ameliorate high variance problematic multi-agent settings non-stationarity issues mentioned previously. deterministic policy gradient algorithms. also possible extend policy gradient framework deterministic policies µµµθ particular certain conditions write gradient objective es∼pµµµ] since theorem relies ∇aqµµµ requires action space continuous. deep deterministic policy gradient variant policy critic qµµµ approximated deep neural networks. ddpg off-policy algorithm samples trajectories replay buffer experiences stored throughout training. ddpg also makes target network deep deterministic policy gradient variant policy critic qµµµ approximated deep neural networks. ddpg off-policy algorithm samples trajectories replay buffer experiences stored throughout training. ddpg also makes target network argued previous section naïve policy gradient methods perform poorly simple multi-agent settings supported experiments section goal section derive algorithm works well settings. however would like operate following constraints learned policies local information execution time assume differentiable model environment dynamics unlike assume particular structure communication method agents fulﬁlling desiderata would provide general-purpose multi-agent learning algorithm could applied cooperative games explicit communication channels competitive games games involving physical interactions agents. similarly accomplish goal adopting framework centralized training decentralized execution. thus allow policies extra information ease training long information used test time. unnatural q-learning function generally cannot contain different information training test time. thus propose simple extension actor-critic policy gradient methods critic augmented extra information policies agents. centralized action-value function takes input actions qπππ agents addition state information outputs q-value agent simplest case could consist observations agents however could also include additional state information available. since qπππ learned separately agents arbitrary reward structures including conﬂicting rewards competitive setting. extend idea work deterministic policies. consider continuous policies µµµθi w.r.t. parameters gradient written |ai=µµµi] {µµµθ shown section centralized critic deterministic policies works well practice refer multi-agent deep deterministic policy gradient provide description full algorithm appendix. primary motivation behind maddpg that know actions taken agents environment stationary even policies change since πππn case explicitly condition actions agents done traditional methods. note require policies agents apply update knowing observations policies agents particularly restrictive assumption; goal train agents exhibit complex communicative behaviour simulation information often available agents. however relax assumption necessary learning policies agents observations describe method section remove assumption knowing agents’ policies required agent additionally maintain approximation ˆµµµφj true policy agent µµµj. approximate policy learned maximizing probability agent actions entropy regularizer note optimized ˆµµµ completely online fashion updating qµµµ centralized function take latest samples agent replay buffer perform single gradient step update note also that equation input action probabilities agent directly rather sampling. previously mentioned recurring problem multi-agent reinforcement learning environment non-stationarity agents’ changing policies. particularly true competitive settings agents derive strong policy overﬁtting behavior competitors. policies undesirable brittle fail competitors alter strategies. obtain multi-agent policies robust changes policy competing agents propose train collection different sub-policies. episode randomly select particular sub-policy agent execute. suppose policy µµµi ensemble different sub-policies sub-policy denoted µµµθ agent maximizing ensemble objective since different sub-policies executed different episodes maintain replay buffer sub-policy agent accordingly derive gradient ensemble objective respect perform experiments adopt grounded communication environment proposed consists agents landmarks inhabiting two-dimensional world continuous space discrete time. agents take physical actions environment communication actions broadcasted agents. unlike assume agents identical action observation spaces according policy πππ. also consider games cooperative competitive environments require explicit communication agents order achieve best reward environments agents perform physical actions. provide details environment below. figure illustrations experimental environment tasks consider including cooperative communication predator-prey cooperative navigation physical deception. webpage videos experimental results. cooperative communication. task consists cooperative agents speaker listener placed environment three landmarks differing colors. episode listener must navigate landmark particular color obtains reward based distance correct landmark. however listener observe relative position color landmarks know landmark must navigate conversely speaker’s observation consists correct landmark color produce communication output time step observed listener. thus speaker must learn output landmark colour based motions listener. although problem relatively simple show section poses signiﬁcant challenge traditional algorithms. cooperative navigation. environment agents must cooperate physical actions reach landmarks. agents observe relative positions agents landmarks collectively rewarded based proximity agent landmark. words agents ‘cover’ landmarks. further agents occupy signiﬁcant physical space figure comparison maddpg ddpg single policy maddpg ensemble maddpg competitive environments. cluster shows normalized score competing policies higher score better agent. cases maddpg outperforms ddpg directly pitted similarly ensemble single maddpg policies. full results given appendix. penalized colliding other. agents learn infer landmark must cover move avoiding agents. keep-away. scenario consists landmarks including target landmark cooperating agents know target landmark rewarded based distance target adversarial agents must prevent cooperating agents reaching target. adversaries accomplish physically pushing agents away landmark temporarily occupying adversaries also rewarded based distance target landmark know correct target; must inferred movements agents. physical deception. here agents cooperate reach single target landmark total landmarks. rewarded based minimum distance agent target however lone adversary also desires reach target landmark; catch adversary know landmarks correct one. thus cooperating agents penalized based adversary distance target learn spread cover landmarks deceive adversary. predator-prey. variant classic predator-prey game slower cooperating agents must chase faster adversary around randomly generated environment large landmarks impeding way. time cooperative agents collide adversary agents rewarded adversary penalized. agents observe relative positions velocities agents positions landmarks. covert communication. adversarial communication environment speaker agent must communicate message listener agent must reconstruct message end. however adversarial agent also observing channel wants reconstruct message alice penalized based eve’s reconstruction thus alice must encode message using randomly generated known alice bob. similar cryptography environment considered implement maddpg algorithm evaluate environments presented section unless otherwise speciﬁed policies parameterized two-layer relu units layer. support discrete communication messages gumbelsoftmax estimator evaluate quality policies learned competitive settings pitch maddpg agents ddpg agents compare resulting success agents adversaries environment. train models convergence evaluate averaging various metrics figure comparison maddpg ddpg cooperative communication physical deception environments small dark circles indicate landmarks. grey agent speaker color listener indicates target landmark. blue agents trying deceive adversary covering target landmark maddpg learns correct behavior cases speaker learns output target landmark color direct listener agents learn cover landmarks confuse adversary. ddpg struggles settings speaker always repeats utterance listener moves middle landmarks agent greedily pursues green landmark othe agent scatters. video full trajectories. iterations. provide tables details results environments appendix summarize here. ﬁrst examine cooperative communication scenario. despite simplicity task traditional methods actorcritic ﬁrst-order implementation trpo ddpg fail learn correct behaviour practice observed listener learns ignore speaker simply moves middle observed landmarks. plot learning curves episodes various approaches figure hypothesize primary reason failure traditional methods multi-agent settings lack consistent gradient signal. example speaker utters correct symbol listener moves wrong direction speaker penalized. problem exacerbated number time steps grows observed traditional policy gradient methods learn objective listener simply reconstruct observation speaker single time step initial positions agents landmarks ﬁxed evenly distributed. indicates many multi-agent methods previously proposed scenarios short time horizons generalize complex tasks. conversely maddpg agents learn coordinated behaviour easily centralized critic. cooperative communication environment maddpg able reliably learn correct listener speaker policies listener often able navigate target. similar situation arises physical deception task cooperating agents trained maddpg able successfully deceive adversary covering landmarks around time furthermore adversary success quite especially adversary trained ddpg contrasts sharply behaviour learned figure effectiveness learning approximating policies agents cooperative communication scenario. left plot reward number iterations; maddpg agents quickly learn solve task approximating policies others. right divergence approximate policies true policies. cooperating ddpg agents unable deceive maddpg adversaries scenario even deceive ddpg agents cooperative navigation predator-prey tasks less stark divide success failure cases maddpg agents outperform ddpg agents. cooperative navigation maddpg agents slightly smaller average distance landmark almost half average number collisions episode compared ddpg agents ease coordination. similarly maddpg predators successful chasing ddpg prey converse covert communication environment found trained maddpg ddpg out-performs terms reconstructing alice’s message. however trained maddpg achieves larger relative success rate compared ddpg further alice trained maddpg encode message achieves near-random reconstruction accuracy. learning curve shows oscillation competitive nature environment often cannot overcome common decentralized methods. emphasize tricks required cryptography environment including modifying eve’s loss function alternating agent adversary training using hybrid ‘mix transform’ feed-forward convolutional architecture. evaluate effectiveness learning policies agents cooperative communication environment following hyperparameters previous experiments setting results shown figure observe despite ﬁtting policies agents perfectly learning approximated policies able achieve success rate using true policy without signiﬁcant slowdown convergence. focus effectiveness policy ensembles competitive environments including keep-away cooperative navigation predator-prey. choose sub-policies keep-away cooperative navigation environments predator-prey. improve convergence speed enforce cooperative agents policies episode similarly adversaries. evaluate approach measure performance ensemble policies single policies roles agent adversary. results shown right side figure observe agents policy ensembles stronger single policy. particular pitting ensemble agents single policy adversaries ensemble agents outperform adversaries large margin compared roles reversed proposed multi-agent policy gradient algorithm agents learn centralized critic based observations actions agents. empirically method outperforms traditional algorithms variety cooperative competitive multi-agent environments. improve performance method training agents ensemble policies approach believe generally applicable multi-agent algorithm. downside approach input space grows linearly number agents could remedied practice example modular function considers agents certain neighborhood given agent. leave investigation future work. authors would like thank jacob andreas smitha milli jack clark jakob foerster others openai berkeley interesting discussions related paper well jakub pachocki yura burda joelle pineau comments paper draft. thank tambet matiisen providing code base used early experiments associated paper. ryan lowe supported part vanier scholarship samsung advanced institute technology. finally we’d like thank openai fostering engaging productive research environment. boutilier. learning conventions multiagent stochastic domains using likelihood estimates. proceedings twelfth international conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. busoniu babuska schutter. comprehensive survey multiagent reinforcement learning. ieee transactions systems cybernetics part applications reviews chalkiadakis boutilier. coordination multiagent reinforcement learning bayesian approach. proceedings second international joint conference autonomous agents multiagent systems pages goodfellow pouget-abadie mirza warde-farley ozair courville bengio. generative adversarial nets. advances neural information processing systems wellman. online learning agents dynamic multiagent system. proceedings second international conference autonomous agents agents pages york acm. lauer riedmiller. algorithm distributed reinforcement learning cooperative multi-agent systems. proceedings seventeenth international conference machine learning pages morgan kaufmann littman. markov games framework multi-agent reinforcement learning. proceedings eleventh international conference machine learning volume pages matignon laurent fort-piat. hysteretic q-learning algorithm decentralized reinforcement learning cooperative multi-agent teams. intelligent robots systems iros ieee/rsj international conference pages ieee matignon laurent fort-piat. independent reinforcement learners cooperative markov games survey regarding coordination problems. knowledge engineering review silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature sutton mcallester singh mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems pages experiments adam optimizer learning rate updating target networks. size replay buffer update network parameters every samples added replay buffer. batch size episodes making update except trpo found batch size lead better performance train random seeds environments stark success/ fail conditions random seeds environments. details experimental results shown following tables. table percentage episodes agent reached target landmark average distance target cooperative communication environment episodes. note percentage targets reached different policy learning success rate figure indicates percentage runs correct policy learned even correct behavior learned agents occasionally hover slightly outside target landmark episodes conversely agents learn middle landmarks occasionally stumble upon correct landmark. table average number prey touches predator episode predator-prey environments prey slightly faster signiﬁcantly faster policies experiment -layer unit mlps. analyze variance policy gradient methods multi-agent settings consider simple cooperative scenario agents binary actions deﬁne reward actions otherwise. simple scenario table evaluations adversary agent w./w.o. policy ensembles trials different scenarios including keep-away physical deception predator-prey denotes agents single policy. denotes agents policy ensembles. figure competitive environments ‘covert communication’ reward oscillate signiﬁcantly agents adapt other. ddpg often unable overcome this whereas maddpg algorithm much greater success. temporal component agents must simply learn either always output always output time step. despite this show probability taking gradient step correct direction decreases exponentially number agents proposition consider agents binary actions a=···=an assume uninformed scenario agents initialized then estimating gradient cost policy gradient have somewhat artiﬁcial example serves illustrate simple environments become progressively difﬁcult policy gradient methods number agents grows. particularly true environments sparse rewards described above. note example policy gradient variance actually decreases grows. however expectation policy gradient decreases well signal noise ratio decreases corresponding decreasing probability correct gradient direction. intuitive reason centralized critic helps reduce variance gradients remove source uncertainty; conditioned agent’s actions signiﬁcant variability associated actions agents largely removed using actions input critic.", "year": 2017}