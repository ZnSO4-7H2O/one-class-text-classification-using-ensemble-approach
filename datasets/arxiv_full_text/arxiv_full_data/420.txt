{"title": "Geometric GAN", "tag": ["stat.ML", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.", "text": "generative adversarial nets represent important milestone effective generative models inspired numerous variants seemingly different other. main contributions paper reveal uniﬁed geometric structure variants. speciﬁcally show adversarial generative model training decomposed three geometric steps separating hyperplane search discriminator parameter update away separating hyperplane generator update along normal vector direction separating hyperplane. geometric intuition reveals limitations existing approaches leads propose formulation called geometric using separating hyperplane maximizes margin. theoretical analysis shows geometric converges nash equilibrium discriminator generator. addition extensive numerical results show superior performance geometric gan. recently inspired success deep discriminative models goodfellow proposed novel generative model training method called generative adversarial nets formulated minimax game generative network maps random vector data space discriminative network trying distinguish generated samples real samples. unlike classical generative models variational auto-encoders minimax formation transfer success deep discriminative models generative models resulting signiﬁcant improvement generative model performance speciﬁcally original form solves following minmax game sample distribution; discriminator takes input outputs scalar generator maps sample drawn distribution input space meaning generator tries fool discriminator discriminator wants maximize differentiation power true generated samples. authors showed training indeed approximate minimization symmetric jensen-shannon divergence idea generalized authors f-divergences. moreover maximum mean discrepancy objective training also proposed well-known training difﬁcult. particular authors identiﬁed following sources difﬁculties discriminator becomes accurate gradient generator vanishes popular ﬁxation using generator gradient updating ez∼pz unstable singularity denominator discriminator accurate. main motivation wasserstein therefore introduce weight clipping address above-described limitations. fact wasserstein special instance minimizing integral probability metric mroueh recently generalized w-gan wider function classes proposed mean feature matching and/or covariance feature matching using minimization framework inspired mcgan propose novel geometric generalization called geometric gan. speciﬁcally geometric inspired novel observation mcgan composed three geometric operations feature space geometric interpretation proves general applied existing variants. indeed main differences algorithms come construction separating hyperplanes linear classiﬁer feature space geometric scaling factors feature vectors. based observation provide geometric interpretations f-gan eb-gan w-gan terms separating hyperplanes geometric scaling factors discuss limitations. furthermore propose novel geometric using support vector machine separating hyperplane maximal margin classes separable data numerical experiments clearly showed proposed geometric outperforms existing gans data set. mean feature matching bounded real valued functions sample space suppose probability distributions then integral probability metric function space deﬁned follows ex∼p denotes expectation respect probability distribution easily show non-negative symmetric satisﬁes triangle inequality. used distance measure probability space. example deﬁned collection functions ﬁnite lipschitz constant wasserstein distance earth mover’s distance forms basis wasserstein mcgan generator network maps random input target shown block fig. function space study deﬁned follows learning rate. authors also used projection onto unit ball weight clipping updates respectively meet constraints. using updated generator update given speciﬁcally designing linear classiﬁer class classiﬁcation problems known normal vector separating hyperplane mean difference classiﬁer shown fig. separating hyperplane deﬁned udpate update discriminator parameters true fake samples maximally separately away separating hyperplane parallel normal vector. hand udpate using update generator parameters make fake samples approach separating hyperplane along normal vector direction recall linear classiﬁer deﬁned normal vector separating hyperplane offset thus comparing direction classiﬁers means comparing normal vector directions. shown appendix aside geometric scaling factors existing variants mainly differ deﬁnition normal vector separating hyperplane. based observation next section discuss optimal separating hyperplane generative model training maximal margin. adversarial training feature space discriminator interested discriminating true samples {φζ}n practice minibatch size much smaller dimension feature space type classiﬁcation problem often called high-dimension low-sample size problem fact mean difference classiﬁer popular methods hdlss. speciﬁcally classiﬁer selects hyperplane lies half class means. particular normal vector seperating hyperplane given difference class means note variables ﬁrst mean centered scaled standard deviation mean difference equivalent naive bayes classiﬁer furthermore hdlss always exists maximal data piling direction mulitple points class identical projection line spanned normal vector. hand support vector machine many variants widely used well studied classiﬁcation algorithms robustness also proven hdlss setup although aforementioned classiﬁcation algorithms motivated ﬁtting statistical distribution data motivated geometric heuristic leads directly optimization problem maximize margin classes separable data. addition soft-margin balances competing objectives maximize margin penalizing points wrong side margin. recently carmichael investigated karush-kuhn-tucker conditions provide rigorous mathematical proof insights behaviour soft-margin large small tuning parameter regimes hdlss. revealed small tuning parameter number data classes direction becomes exactly direction. addition sufﬁciently large tuning parameter authors showed soft margin equivalent hard margin data separable hard-margin data piling. generality soft-margin proposed geometric designed based soft-margin linear classiﬁer. note soft-margin designed adding tunning parameter slack variables allows points wrong side margin problem classifying true samples versus fake samples primal form soft-margin formulated goal optimization maximize margin classes. implies discriminator update also easily incorporated update goal discriminator update also regarded maximize margin classes. speciﬁcally optimization problem given nonzero support vectors support vectors includes data points margin boundary well wrong side margin boundary speciﬁcally deﬁne region margin boudaries shown fig. cost function value dependent feature vectors outside margin boundaries fully determined supporting vectors accordingly discriminator update given following updates another word update discriminator parameters need push supporting vectors toward margin boundaries. hand generator update requires geometric intuition. shown fig. generator update tries move fake feature vectors toward normal vector direction separating hyperplane classiﬁed true feature vectors. means generator update given following minimization problem note updates strikingly similar mcgan. aside different choice separating hyperplane discriminator update additional geometric scaling factors shown appendix appearance geometric scaling factors recurrent theme geometric interpretation variants believe fundamental account geometry classiﬁers. max{ dwbζ linear discriminator parameterized here denote probability density functions distribution respectively. similarly generator cost function becomes suppose optimal solution aforementioned adversarial training pair then prove following convergence result. theorem suppose minimizer alternating minimization then almost everywhere following example provide speciﬁc example discriminator generator cost function close form expressions also intuitive meaning minimum value theorem particular consider example learning parallel lines original wasserstein paper example denote uniform distribution unit interval. distribution uniform straight vertical line passing origin. suppose generator sample given single real parameter. easily separating hyperplane given coincides results theorem example true fake samples lies separating hyperplane. informs nash equilibrium problem true samples fake samples separable desired property training. however theorem necessary condition make true fakes sample non-separable. proof sufﬁciency condition would interesting beyond scope current paper. order evaluate proposed geometric perform comparative studies three representative types gans; jenson-shannon mean difference mean difference here behavior maximum margin separating hyperplane geometric empirically analyzed aforementioned approaches. addition evaluate dependency variants lipschitz continuity constraints lipschitz constraints suggested also applied adversarial training approach. speciﬁcally parameters ﬁnal linear layer discriminator determined represent aforementioned hyperplane properties whereas lipschitz constraints applied network parameters paper consider lipschitz density constraints follow weight decay generators feature space mapping discriminators. test four hyperplane searching approaches discriminators well complementary generator losses dimensional synthetic data. synthetic data consists data points generated mixture gaussians akin data used describing mode collapsing behaviors gans speciﬁcally means gaussians evenly spaced grid along axis standard deviation normal distribution sampled data true distribution seen figure discriminator generator multi-layered fully-connected neural network architecture used described below. rmsprop used train networks except vanilla vanilla adam momentum used. base learning rate weight clipping applied parameters feature mapping clipped within range weight projection unit norm applied following rule min{ /p}× described used update parameter every iteration. weight decay weight decaying parameter batch size experiment. number discriminator update generator update i.e. figure generated samples variants trained lipschitz density constraints suggested mixture gaussians sample true data distribution. training weight decay applied addition hyperplane constraints. results experiment mixture gaussians illustrated figure amongst variants experiment geometric demonstrated least mode collapsing behavior independently lipschitz continuity regularization constraints. shown fig. lipschitz density constraints linear hyperplane approaches demonstrated less mode collapsing behaviors virtue consistent gradients unlike nonlinear separating hyperplane original gan. however mean difference-driven hyperplanes wasserstein mcgan generators mean arbitrary number modes true distributions since characteristics mean difference. hand geometric generally showed robust consistent convergence behavior towards true distributions. order analyze proposed method large-scale dataset geometric empirically analyzed well-studied datasets context adversarial training; mnist celeba lsun datasets. since consistent quantitative measures still debate perform qualitative comparisons generated samples learned generators propsed method results previous literatures. favor fair comparisons adversarial training methods adopt settings previous literatures except hyperparameters stochastic optimizations tuning parameter proposed method. dcgan neural network architectures used including batch normalization generator. note currently known adversarial training methods demonstrated stable learning without batch normalization resorted lipschitz constraints; therefore also applied adversarial training criterions including geometric order train batch normalization-free generators. pixel value input image rescaled dataset including mnist dataset. training mini-batch size stochastic gradient update training rmsprop used number generator’s updates discriminator’s update learning rate discriminators generators tuning parameter discriminator speciﬁcally mnist dataset input images resized pixels order dcgan network architecture number epochs training celeba dataset input images resized pixels center-cropped pixels number epochs training lsun dataset bedroom dataset used input image resized pixels. number epochs training lsun dataset. results figure clearly show geometric generates realistic images without mode collapsing divergent behaviours. paper proposed novel geometric using separating hyperplane based geometric intuitions revealed previous adversarial training approaches. geometric based separating hyperplanes maximal margins classes. compared existing approaches based statistical design criterion geometric derived based geometric intuition similar derivation svm. extensive numerical experiments showed proposed method demonstrated less mode collapsing stable training behavior. moreover theoretical results showed proposed algorithm converges nash equilibrium discriminator generator also geometric meaning.", "year": 2017}