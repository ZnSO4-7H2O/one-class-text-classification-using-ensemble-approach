{"title": "Incorporating Domain Knowledge in Matching Problems via Harmonic  Analysis", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Matching one set of objects to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem (QAP). The QAP is known to be notoriously hard, both in theory and in practice. Here, we investigate if this difficulty can be mitigated when some additional piece of information is available: (a) that all QAP instances of interest come from the same application, and (b) the correct solution for a set of such QAP instances is given. We propose a new approach to accelerate the solution of QAPs based on learning parameters for a modified objective function from prior QAP instances. A key feature of our approach is that it takes advantage of the algebraic structure of permutations, in conjunction with special methods for optimizing functions over the symmetric group Sn in Fourier space. Experiments show that in practical domains the new method can outperform existing approaches.", "text": "deepti pachauri† maxwell collins† risi kondor§ vikas singh‡† †dept. computer sciences ‡dept. biostatistics med. informatics university wisconsin madison. §dept. computer science dept. statistics university chicago. {pachaurimcollins}cs.wisc.edu risiuchicago.edu vsinghbiostat.wisc.edu limitation take account relationships xi’s other. example matching feature points images want landmarks image matched similar landmarks image also want distances landmarks ﬁrst image similar distances corresponding landmarks second one. leads general optimization problem matching objects another ubiquitous task machine learning computer vision often reduces form quadratic assignment problem known notoriously hard theory practice. here investigate diﬃculty mitigated additional piece information available instances interest come application correct solution instances given. propose approach accelerate solution qaps based learning parameters modiﬁed objective function prior instances. feature approach takes advantage algebraic structure permutations conjunction special methods optimizing functions symmetric group fourier space. experiments show practical domains method outperform existing approaches. matching objects another fundamental problem computer science. computer vision arises context ﬁnding correspondence multiple images scene taken bioinformatics diﬀerent viewpoints must align sequences genes amino acids. machine learning often needs align examples meaningful similarity measure computed unfortunately np–hard also notoriously hard approximate solve heuristically. combinatorial search methods branch bound almost never manage solve real world instances polynomial time convex optimization methods thwarted fact feasible region exponential number faces. purely empirical point view successful approaches ant-methods like heuristic algorithms optimality guarantees whatsoever. figure images object. assuming images related simple transformation homography matching seeks assign feature points image similar feature point image. typically also cost associated alignment corresponding edges. natural whether side information make easier. here propose approach learning modiﬁed objective function prior training instances. criteria must satisfy vector parametrizes determined methods similar structural risk minimization. form risk minimization strategy structural svms applied training allows parameter generalize unseen examples well. note structural svms well understood original inference problem poly-time solvable unfortunately original graph matching problem requires ﬁnding fortunately observe interest arbitrary objects rather constitute symmetric group opens door look properties speciﬁc sub-classes and/or provide useful insights eﬃcient solution strategies. speciﬁcally view allows leverage entire spectrum tools abstract algebra including non-commutative harmonic analysis fast fourier transforms groups develop eﬃcient algorithms real promise approach lies generality setting matching problems broader algebraic framework potential serve basis developing matching algorithms exploit representation theory groups better inmodel developed applicable learning version scenarios make exposition concrete restrict attention problem learning parameters graph matching. learning graph matching problem seeks solve parameters compatibility functions solution approximate solution matches permutation matrix provided user best possible. since main objects interest permutation matrices makes ideal sandbox develop present main ideas. graph matching related work. graph matching problem ﬁnding correspondences nodes graphs maximize alignment. model write quadratic assignment problem among well-studied combinatorial optimization problems literature many alternative approaches graph matching also known context learning graph matching interested following question optimal correspondence nodes pair graphs known knowledge learn correspondences another pair graphs extracted similar conditions. notion conditions reﬂects properties application study instance uses example image pairs acquired similar illumination airport surveillance camera matching task refers aligning feature points images. determined parameter corresponds weights appropriately adjust joint feature node edge compatibilities match found solver agrees user provided solution. learning graph matching serves another important need restricting focus speciﬁc domain tuning weights best reﬂect practical considerations application less sophisticated approach still able obtain good quality solutions ﬁxed amount time memory implications situations training data available. algorithm uses nice structure learning formalization problem. ﬁnding violated constraint precisely construction involves solving approximately estimated permutations leaves branch corresponds coset modulation edge node compatibility functions consists solving simple convex optimization models deﬁned fourier space. broadly approach performs stochastic descent-like weight updates induce margin path leads correct user-provided relative training example. contribution paper parameter learning framework class combinatorial problems solution candidate symmetric group show representation theory makes procedure computationally tractable branch bound schemes modiﬁed learn information relevant problem instances coming application interest. aspect approach exploits permutations need optimize over group called symmetric group letters. recall means respect natural notion multiplication permutations satisfy following axioms another ingredient proposed approach technology fast fourier transforms going back work clausen recall given subgroup permutation called h–cosets two-sided cosets ffts generally work ﬁrst computing fourier transforms restricted small cosets recursively assembling small transforms ever large ones reach fourier transform entire group. solver structure search employing inverse fast fourier transform restrict various cosets bound illustrate coset tree figure components leads branch bound type optimization algorithm runs time branch visited competitive conventional exact solvers unfortunately many practical problems resulting algorithm still takes exponential amount time simply needs visit many branches. observe qaps hard objective function relatively according reasonable metrics diameter small compared size. practical problems however sometimes overcome seemingly insurmountable barriers making inventive side information. particular take approach using similar instances learn modiﬁed objective function eﬀectively drive algorithm correct solution. since solving scratch modiﬁcation parameters clearly intractable option main goal adapt tools noncommutative harmonic analysis recast problem sidesteps burden. framework described next formalizes idea. ﬁrst write objective function graph matching. function deﬁned adjacency matrices graph pair band-limited structure observed next incorporate domain information within objective. present framework learning parameters given multiple instances related graphs. instances derived vision problems typically rely analytic form approximating perceptual similarity feature points. instance similar pair shape features extracted diﬀerent images? node node match perceptually correct turns marginally better many incorrect matches necessarily suggests features discriminative. practical consequence branch bound type search need work much harder global solution. core strategy incorporate domain information objective. this simple idea composing various qaps write base objective certain desirable properties. since features available applications seems logical additional information design function used inform base objective. function might perfect noisy features standard learning algorithms used learn shared structure observing multiple instances drawn speciﬁc domain. using training data able learn parameters learnt parameters induce domain friendly instances biasing search towards interesting matches ﬁrst simultaneously suppressing inﬂuence misleading features. replaced learning correct parameters. turns ﬁxed easily computable entirely fourier space without perform costly full fourier transform. exact form interacts depends problem formulation. keep presentation simple formulate objective follows various ways extracting features vision. model neighborhood relationship vertices based similarity features. list include edge features delaunay triangulation euclidean distance interest points shape context features etc. consider representations disposal encodes corresponding graph neighborhood. representations generate adjacency matrices graph. entry squared distance feature values vertex according representation similarly deﬁne write objective using want match edge assigned edge similar length simultaneously encoded adjacency matrices. however features noisy discriminative lead wrong assignments. strategy parameterize write parameterized objective learning loss function regularizer optimal permutation example note corresponds solving objective given objective modulated parameter goal learn performing stochastic descent level tree cosets. experiments implemented loss. observe many yield loss; regularization used seeks minimal diﬀerence previous step path/node leading preferable node level small margin. exact algorithm discussed next. demonstrate proof principle results proposed algorithm learnt parameters compatibility functions context graph matching. numerous applications setting considers task aligning images using local features extracted image data including interest points shape context features. edge–features performed delaunay triangulation interest points generate unweighted edges. provides unweighted adjacency matrix. distance–features used coordinates interest points calculate euclidean distance points. extracted weighted adjacency matrices various scales using distances. shape context–features also included shape context features brieﬂy feature vector descriptor log-polar space describes localized shape node. generated weighted adjacency matrices using normalized histogram diﬀerences subsets shape context features. dataset setup performed experimental evaluation datasets. used subset landmark points provided. experimental dataset closely follow setup described graph pair instances generated graphs separated varying baseline training data include multiple qap’s pair images. algorithm learnt suitable performed standard –fold cross-validation train/test data oﬀset. number pairs train/test splits varied based oﬀset. summarize results various datasets below. hotel/house dataset considered house dataset contains frames video sequence house. landmark points identiﬁed hand–labeled frame. quantitative results corresponding matches found test shown present accuracy matches oﬀset varies. compared results greedy matching feature settings expected no-learngreedy approach perform poorly features informative small oﬀsets nolearn-greedy takes advantage fact problem instance easy shape context features useful performance gradually deteriorates oﬀset increases greedy assignment using learnt weights still performs well. note test phase perform backtracking employ stochastic gradient descent approach similar upper bounds deﬁned training phase know node node assignment make know bounds corresponding vertex want largest. random training example node selected compared correct sibling n−k. reduce objective term taking gradient step replaced bounds update takes form like structural parameters model predicts instead bound correct node coset trees training greater incorrect siblings margin. learnt parameters goodness measures individual graph correlation function contributing figure results method compared no-learn baseline. colors represent learning no-learning delaunay distance uninformative features. delaunay distance shape context features. house hotel silhouette. branch bound. performed similar experiment hotel dataset contains frames video sequence hotel. again good overall agreement ground truth using learnt parameters. quantitative results shown qualitative results corresponding learnt matches test shown silhouette dataset second experiment used silhouette database. applied horizontal shear twice width transformed images synthetically. results experiment shown introducing uninformative features makes matching problem diﬃcult shape context features quite useful learning setting still outperforms no-learning oﬀset variations. finally analysed learnt parameters setup includes delaunay distance–based adjacency matrices scales along random graph pairs. learning induced parameters corresponding delaunay distance. shows example average weights produced –fold cross-validation setup indicating reduced weights uninformative features paper shows parameter learning general class hard combinatorial optimization problems performed eﬃciently solution primary objective function member present framework performing weight updates nodes coset tree. observe number leaves tree still functions discussed here bandlimited nature fourier transform makes process tractable. algorithm inspired recent results showing concepts harmonic analysis topics machine learning. procedure generalizes problems cast appropriate functions provides complementary approach number problems typically tackled using structure learning. believe additional structure explicitly leveraged current model instance evaluating computational beneﬁts performing weight updates frequency components instead features. exploring properties provide strategies seemingly unrelated problems. instance recently related ideas independently investigated submodular functions implementation available http//pages.cs.wisc.edu/~pachauri/. acknowledgments authors thank shamgar gurevich nigel boston jerry various suggestions. work supported part grants uw-ictr collins supported cibm training program", "year": 2012}