{"title": "Compression of Deep Neural Networks on the Fly", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Thanks to their state-of-the-art performance, deep neural networks are increasingly used for object recognition. To achieve these results, they use millions of parameters to be trained. However, when targeting embedded applications the size of these models becomes problematic. As a consequence, their usage on smartphones or other resource limited devices is prohibited. In this paper we introduce a novel compression method for deep neural networks that is performed during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve significantly larger compression rates than state-of-the-art methods.", "text": "abstract. thanks state-of-the-art performance deep neural networks increasingly used object recognition. achieve best results millions parameters trained. however targetting embedded applications size models becomes problematic. consequence usage smartphones resource limited devices prohibited. paper introduce novel compression method deep neural networks performed learning phase. consists adding extra regularization term cost function fully-connected layers. combine method product quantization trained weights higher savings storage consumption. evaluate method data sets achieve signiﬁcantly larger compression rates state-of-the-art methods. deep convolutional neural networks become stateof-the-art object recognition image classiﬁcation. matter fact recently proposed systems using architecture global trend arise questions import cnns embedded platforms including smartphones data storage bandwidth limited. today size typical often large smartphone users. purpose paper propose techniques compressing deep neural networks without sacriﬁcing performance. work focus compressing cnns used vision although methodology taking advantage particular application ﬁeld expect perform similarly types learning tasks. typical stateof-the-art visual recognition contains several convolutional layers followed several fully connected layers. challenging datasets layers require hundred millions parameters trained order eﬃcient. recently works focus compressing neural network specially reduce storage network. works generally different categories focus compressing fully connected layers others compressing convolutional layers. authors focus compressing densely connected layers. work signal processing vector quantization methods k-means product quantization authors focus compressing fully connected layers multi-layer perceptron using hashing trick cost hash function randomly group connection weights hash buckets value parameters bucket. authors propose compressing convolutional layers using discrete cosinus transform applied convolutional ﬁlters followed hashing trick fully connected layers. interesting point showed typical sate-of-the-art storage taken densely connected layers whereas running time taken convolutional layers. order compress size mainly focus compressing densely connected layers. instead using post-learning method compress network approach consists modifying regularization function used learning phase order favor quantized weights layers especially output ones. achieve this idea originally proposed order compress furthermore obtained networks also described afterwards. perform experiments multi-layer perceptrons convolutionnal neural networks. outline paper follows. section discuss related work. section introduced methodology compressing layers deep neural networks. section experiments celebrated databases. section conclusion. already mentioned introduction densely connnected layers state-of-the-art usually involve hundreds millions parameters thus requiring important storage hard obtain practice. several works published speeding prediction speed. authors tricks cpus speed execution cnn. authors show carrying convolutionnal operations fourrier domain lead speed-up recent works linear matrix factorization methods speeding convolutions obtain speed-up gain almost loss classiﬁcation accuracy. previously mentionned works mainly focus speeding feedforward operations. recently several works devoted compressing size. authors demonstrate overparametrization neural network parameters. indeed show parameters enough accurately predict remaining ones. results motivate apply vector quantization methods beneﬁt redundancy compress network parameters. compression allows obtain results similar able achieve compression rate without sacriﬁcing accuracy. paper tackle model size issue applying trained weights. able achieve good balance storage test accuracy. imagenet challenge ilsvrc achieve compression rate whole network loss accuracy using state-of-the-art cnn. ﬁrst time learn-based method proposed compress neural networks. method based hashing trick allows eﬃcient compression rates. particular show compressing large neural network eﬃcient directly training smaller example able divide loss using eight times larger neural network compressed eight times. authors also propose compress ﬁlters convolutional layers arguing size convolutional layers stateof-the-art’s increasing year year. using fact learned ﬁlters often smooth discrete cosinus transform followed hashing tricks allows compress whole neural network without loosing much accuracy. consists partitioning parameters space disjoint sub-spaces performing quantization them. term product refers fact quantized points original parameter space cartesian product quantized points sub-space. performs increasingly better redundancy subspace grows. need store nearest centroid indexes sub-vector. codebook negligible therefore compression rate kn). ﬁxed segment size increasing lead decreasing compression rate. recall training neural network generally performed thanks minimization cost function using derivative gradient descent algorithm. order attract network weights binary values binarization cost learning phase. added cost pushes weights binary values. result solutions minimization problem expected binary almost binary depending scaling parameter added cost respect initial one. idea although work applying deep learning literature. choice regularization term greatly inspired scaling parameter representing importance binarization cost respect initial cost. note possible values binary weights empirically explored centered best results. finding good value tricky small value results failure binarization process large value results creation local minima prevent network successfully training. facilitate selection barrier method consists starting small values incrementing regularly help quantization process. experiments iteration multiply constant observed layers typically well quantized learning phase whereas others still binary. reason binarize layers all. again selection made exploring empirically possibilities example using results depicted figure evaluate diﬀerent methods image classiﬁcation datasets mnist cifar. parameters used product quantizer segment size varying number cluster varying mnist mnist database handwritten digits training examples test examples. subset larger available nist. digits size-normalized centered ﬁxed-size image. neural network mnist lenet. lenet convolutional neural network introduced cifar cifar database training examples test examples. subset larger available million tiny images dataset. consists colour images partitioned classes. cifar database convolutional neural network made four convolutional layers followed fully connected layers. network introduced layers quantify ﬁrst experiments depicted table shows inﬂuence quantiﬁed layers performance. observe performance strongly depends layers quantized. precisely experiment shows quantize layers output input rather contrary. result surprising input layers often described similar wavelet transforms intrinsically analog operators whereas output layers often compared high level patterns detection image often enough good classiﬁcation results. binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised binarised fig. performance classiﬁcation task depending layers network quantized mnist database. layer input layer whereas layer output one. performance comparison second experiment shows comparison previous work. results depicted figure note cases compared networks exact architecture. proposed method concerned choose compress outputs layers fully connected. since sizes distinct able coeﬃcients twice. note layer contains almost weights therefore chose investigate role parameters. observe added regularization cost allows signiﬁcantly improve performance. example mnist database want respect loss compression rate single whereas learn-based method leads compression rate compression rate concern output layers. howewer output layers contains almost weights still signiﬁcant compression speciﬁc example using proposed method memory used store network weights fall forces weights become almost binary. order compress even network apply product quantization combination allows reach performance state-of-the-art methods. also demonstrate inﬂuence depth binarized layer performance. ﬁndings particular interest motivation explore connections actual biological data deep neural systems. future work consider applying method larger datasets datasets typically require larger networks leading increased interest obtaining good compression factors. addition network expected deeper thus allow studying thoroughly impact binarization depending deepness layers. also consider exploring complex regularization functions particular order extend work q-ary values layer-dependent determined finally next step consists making activities neurons also binary. connections activities binary could propose optimized digital implementations networks leading higher throughput lesser energy consumption lesser memory usage conventional implementations.", "year": 2015}