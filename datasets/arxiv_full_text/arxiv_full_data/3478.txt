{"title": "Improved Fixed-Rank Nyström Approximation via QR Decomposition:  Practical and Theoretical Aspects", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The Nystr\\\"om method is a popular technique for computing fixed-rank approximations of large kernel matrices using a small number of landmark points. In practice, to ensure high quality approximations, the number of landmark points is chosen to be greater than the target rank. However, the standard Nystr\\\"om method uses a sub-optimal procedure for rank reduction mainly due to its simplicity. In this paper, we highlight the drawbacks of standard Nystr\\\"om in terms of poor performance and lack of theoretical guarantees. To address these issues, we present an efficient method for generating improved fixed-rank Nystr\\\"om approximations. Theoretical analysis and numerical experiments are provided to demonstrate the advantages of the modified method over the standard Nystr\\\"om method. Overall, the aim of this paper is to convince researchers to use the modified method, as it has nearly identical computational complexity, is easy to code, and has greatly improved accuracy in many cases.", "text": "popular approach tackle challenges obtained eigenvalue decomposition rank. here columns rn×r span r-dimensional eigenspace diagonal matrix rr×r contains eigenvalues. since kernel matrix spsd have target rank small chosen independently beneﬁts ranktakes approximation twofold. first store matrix linear number samples second rank-r approximation leads substantial computational savings within learning process. example approximating means matrix kernel ridge regression inversion calculated using sherman-morrison-woodbury formula time compared done naively. examples kernel k-means performed matrix step k-means algorithm runs time proportional kernel-based spectral clustering ﬁxed-rank kernel approximations used reduce computation time although shown ﬁxed-rank approximation kernel matrices promising approach trade-off accuracy scalability eigenvalue decomposition least quadratic time complexity takes space. address issue line prior work centered around efﬁcient techniques approximating best rank-r approximation ready access survey. however typically unknown kernel methods cost form using standard kernel functions extremely expensive large high-dimensional data sets. reason nyström method popular technique computing ﬁxed-rank approximations kernel-based learning eliminates need access every entry full kernel matrix. nyström method works selecting small bases referred landmark points computes kernel similarities input data points landmark points. abstract—the nyström method popular technique computing ﬁxed-rank approximations large kernel matrices using small number landmark points. practice ensure high quality approximations number landmark points chosen greater target rank. however standard nyström method uses sub-optimal procedure rank reduction mainly simplicity. paper highlight drawbacks standard nyström terms poor performance lack theoretical guarantees. address issues present efﬁcient method generating improved ﬁxed-rank nyström approximations. theoretical analysis numerical experiments provided demonstrate advantages modiﬁed method standard nyström method. overall paper convince researchers modiﬁed method nearly identical computational complexity easy code greatly improved accuracy many cases. index terms—kernel methods large-scale learning nyström kernel methods widely used various learning problems. well-known examples include support vector machines kernel principal component analysis feature extraction kernel clustering kernel ridge regression main idea behind kernel-based learning input data points feature space pairwise inner products mapped data points computed nonlinear kernel function satisﬁes mercer’s condition thus kernel methods allow linear algorithms feature space correspond nonlinear algorithms original space. reason kernel machines received much attention effective tool tackle problems complex nonlinear structures. data points inner products feature space calculated using nonlinear kernel function def= kernel-induced feature map. popular choice gaussian kernel function expxi parameter kernel machines pairwise inner products stored symmetric positive semideﬁnite kernel matrix rn×n. standard nyström method generates rank-r approximation using landmark points practice common choose greater obtaining higher quality rank-r approximations since accuracy nyström method depends number selected landmark points selection procedure. landmark points sampled respect uniform nonuniform distribution input data points moreover recent techniques utilize out-of-sample landmark points generating improved nyström approximations e.g. centroids found k-means clustering input data points ﬁxed landmark points rn×m rm×m matrices entries then rank-m nyström approximation form cw†ct pseudoinverse ﬁxed-rank case standard nyström method restricts rank inner matrix despite simplicity rank reduction process standard nyström method main downside structure completely disregarded. standard nyström method generates rank-r approximation gnys solely based ﬁltering smaller size compared matrix size result selection landmark points standard nyström method guarantee improved rank-r approximations kernel matrices. example experimental results section reveal increase number landmark points even produce less accurate rank-r approximations poor rank reduction process remarks paper considers fundamental problem rank reduction nyström method. particular present efﬁcient technique computing rank-r approximation form gopt comparable standard nyström method. modiﬁed method utilizes thin decomposition matrix computing accurate rank-r approximation cw†ct compared gnys moreover unlike standard nyström method results show theoretically empirically modiﬁed nyström produces accurate rank-r approximations number landmark points increases. work make following contributions algorithm present efﬁcient method generating improved rank-r nyström approximations. modiﬁed method computes best rank-r approximation cw†ct i.e. gopt respect sample size theorem shown gopt always produces accurate rankr approximation compared gnys respect trace norm greater landmark points selected input data set. remark shows necessarily true frobenius norm. provide counter-examples remarks showing equivalent theorem cannot hold standard nyström method. example shows situation modiﬁed nyström method arbitrarily better standard method respect trace frobenius norms. remark gives insight expect standard modiﬁed methods differ. theorem shows that certain conditions theoretical results also applicable recent landmark selection techniques based out-of-sample extensions input data centroids found k-means clustering. knowledge modiﬁed nyström method discussed literature preliminary preprint though derivation straightforward suspect previously derived unpublished work. importance rank reduction nyström method recent works independently study selected input data set. however principal differences work aforementioned references. first main focus paper directly compare standard modiﬁed nyström methods provide theoretical experimental evidences effectiveness modiﬁed nyström. second present theoretical experimental results important class out-of-sample landmark points often lead accurate nyström approximations section present notation give brief review matrix decomposition low-rank approximation techniques. section reviews standard nyström method computing rank-r approximations kernel matrices explain process obtaining approximate eigenvalues eigenvectors. section present efﬁcient modiﬁed method computing improved rank-r approximations kernel matrices. main theoretical results given section present experimental results comparing modiﬁed standard nyström methods section section provides brief conclusion. denote column vectors lower-case bold letters matrices upper-case bold letters. in×n identity matrix size matrix zeros. vector denote euclidean norm diag represents diagonal matrix elements main diagonal. entry denoted transpose trace operator. write range denote column space matrix rank min{n admits factorization form uσvt rn×ρ rm×ρ orthonormal matrices known left singular vectors right singular vectors respectively. diagonal matrix diag σρ]) contains singular values descending order i.e. kernel matrix meaning generated assume kernel function satisﬁes mercer’s condition therefore symmetric positive semideﬁnite rn×n spsd matrix rank similar singular value decomposition matrix factorized uλut rn×ρ contains orthonormal eigenvectors i.e. iρ×ρ diag λρ]) diagonal matrix contains nonzero eigenvalues descending order. factorization known eigenvalue decomposition matrices partitioned target rank rr×r contains leading eigenvalues columns rn×r span r-dimensional eigenspace λρ−r uρ−r contain remaining eigenvalues eigenvectors. best rankk matrices rn×n another matrix factorization technique paper decomposition. matrix decomposed product matrices rn×m orthonormal columns i.e. im×m rm×m upper triangular matrix. sometimes called thin decomposition distinguish full decomposition ﬁnds rn×n zero-pads accordingly. nyström method generates ﬁxed-rank approximation spsd kernel matrix rn×n selecting small bases referred landmark points. simplest common selection technique uniform sampling without replacement case data point data sampled probability i.e. advantage technique computational complexity associated sampling landmark points. however uniform sampling take account nonuniform structure many data sets resulting kernel matrices. therefore sampling mechanisms respect nonuniform distributions proposed address problem. line work requires computation statistical leverage scores expensive uniform sampling addition leverage score sampling often requires computing entire kernel matrix negates principal beneﬁts nyström method. comprehensive review comparison uniform nonuniform landmark selection techniques found recently generating landmark points using out-ofsample extensions input data shown effective high quality nyström approximations. line work originates work zhang based observation nyström approximation error depends quantization error encoding data landmark points. hence landmark points selected centroids found k-means clustering. machine learning pattern recognition k-means clustering wellestablished technique partition data clusters trying minimize total squared euclidean distances point closest cluster center general assume landmark points denoted rp×m given. consider matrices rn×m rm×m nyström method uses construct approximation kernel matrix following form rank time complexity nyström method form lnys takes construct matrices takes time perform partial eigenvalue decomposition represents cost matrix multiplication cvr. thus computation cost form rank-r approximation kernel matrix linear data size eigenvalues eigenvectors estimated using rank-r approximation fact approach provides exact eigenvalue decomposition gnys ﬁrst step eigenvalue decomposition matrix overall procedure estimate leading eigenvalues/eigenvectors summarized algorithm time complexity approximate eigenvalue decomposition addition cost computing lnys mentioned earlier. thus total cost computing approximate eigenvalue decomposition linear previous section explained nyström method computing rank-r approximations spsd kernel matrices based selecting small landmark points. although ﬁnal goal approximation rank greater often preferred select landmark points restrict resultant approximation rank main intuition selecting landmark points restricting approximation lower rank-r space regularization effect lead accurate approximations example landmark points chosen centroids k-means clustering landmark points lead smaller quantization error data thus higher quality nyström approximations. standard nyström method presented algorithm rank matrix restricted computing best rank-r approximation inner matrix gnys since inner matrix representation gnys rank main beneﬁt technique computational cost performing exact eigenvalue decomposition relatively small matrix size however standard nyström method totally ignores structure matrix rank reduction process. fact since rank-r approximation gnys utilize full knowledge selection landmark points guarantee improved low-rank approximation standard nyström method remarks solve problem present efﬁcient method compute best rank-r approximation matrix cw†ct given matrices rn×m rm×m. contrast standard nyström method modiﬁed approach takes advantage matrices begin consider best rank-r approximation matrix unitarily invariant norm frobenius norm trace norm follows decomposition rn×m; rn×m rm×m. eigenvalue decomposition matrix rw†rt computed rw†rt vσvt diagonal matrix rm×m contains eigenvalues descending order main diagonal columns rm×m corresponding eigenvectors. moreover note columns rn×m orthonormal orthonormal columns. thus decomposition )σ)t contains eigenvalues orthonormal eigenvectors nyström approximation cw†ct hence best rank-r approximation cw†ct rr×r computed using leading eigenvalues rn×r given corresponding eigenvectors thus estimates eigenvalues eigenvectors kernel matrix nyström approximation cw†ct obtained follows precise best approximation unique nonzero eigenvalues cw†ct assume unique simplicity presentation unique method returns particular optimal solution. algorithm nyström decomposition input data landmark points kernel function target rank output estimates leading eigenvectors eigenvalues rr×r modiﬁed method estimating leading eigenvalues/eigenvectors kernel matrix presented algorithm time complexity method represents cost form matrices complexity decomposition takes time compute eigenvalue decomposition rw†rt finally cost compute matrix multiplication compare computational complexity nyström decomposition standard nyström method since focus paper large-scale data sets large consider terms involving lead dominant computation costs. based previous discussion takes cnys time compute eigenvalue decomposition using standard nyström method cost modiﬁed technique copt thus data even moderate dimension dominant term cnys copt means increase computation cost noticeable typically term dominates algorithms hence signiﬁcant increase cost case runtime example shown fig. example rest section present simple example gain intuition superior performance modiﬁed technique compared standard nyström. present theoretical results show modiﬁed nyström method provides improved rank-r approximations spsd kernel matrices. particular given ﬁxed matrices rn×m rm×m generated in-sample landmark points nyström method decomposition guaranteed generate improved rank-r approximation compared standard nyström method terms trace norm. addition present theorem shows number landmark points increases modiﬁed nyström method generates accurate rank-r approximation kernel matrix terms trace norm. important advantage nyström decomposition since standard nyström method generate lower quality rank-r approximations increasing sub-optimal rank restriction step section order compare accuracy modiﬁed nyström standard nyström method ﬁrst provide alternative formulation methods. assume landmark points in-sample meaning selected fashion among input data points matrix rn×m contains columns kernel matrix column selection process viewed forming sampling matrix rn×m exactly nonzero entry column location corresponds index selected landmark point. then matrix product rn×m contains columns sampled kernel matrix rm×m intersection columns corresponding rows deﬁne def= rn×m means moreover consider singular value decomposition fsnt columns rn×m left singular vectors rm×m. thus eigenvalue decomposition matrix nsnt simplicity presentation assume full rank though results still hold long rank greater equal next present alternative formulation rankr approximation gopt terms left singular vectors modiﬁed nyström method ﬁnds best rank-r approximation cw†ct observe that following present theorem shows modiﬁed nyström method generates improved rank-r approximation kernel matrix compared standard nyström method fair comparison assumed methods access ﬁxed matrices theorem rn×n spsd kernel matrix target rank. matrix rn×m rm×m. then have example showed modiﬁed method outperforms standard nyström method. essential structural feature example presence large-magnitude block kernel matrix denoted below. following remark shows block zero methods perform same. remark theorem showed nyström decomposition generates improved rank-r approximation kernel matrices respect trace norm. however property always satisﬁed terms frobenius norm. example consider following spsd kernel matrix k−gnys example standard nyström method slightly better performance terms frobenius norm. section present comprehensive experimental results real-world data sets show nyström decomposition outperforms standard nyström method terms trace norm frobenius norm. theoretical result quality rank-r nyström approximations number landmark points increased. formal ﬁrst sample landmark points input data points generate rank-r approximation using modiﬁed nyström sample additional method namely gopt number landmark points modiﬁed nyström method leads improved rank-r approximation. theorem rn×n spsd kernel matrix target rank. consider sampling matrix ﬁxed matrix rn×m rn×m rm×m. then modiﬁed nyström method generates rank-r approximation gopt concatenating matrix pnew =cw†ctr. increase number landmark points i.e. rn×m. resulting matrix used form rn×m rm×m modiﬁed nyström method generates gopt =cw†ctr. approximation gopt rn×m andf rn×m left singular vectors rn×m. remark theorem true standard nyström method. consider kernel matrix example sampling ﬁrst columns standard nyström method gave relative errors trace frobenius norms. sampled ﬁrst column standard nyström method would returned approximation thus relative error norms meaning adding additional landmark points leads worse approximation. also remark experiments. theorem consider distinct data points rp×n gaussian kernel function form expxi leads kernel matrix rn×n. rp×m arbitrary form rn×m rm×m then exists matrix rn×m rm×m represents approximation error empirical kernel deﬁned assuming def= positive deﬁnite matrix then proof. mentioned earlier kernel matrix using gaussian kernel function full rank. thus exists matrix empirical kernel deﬁned recall singular value decomposition fsnt eigenvalue decomposition nyström method written mentioned earlier assumption holds true landmark points selected input data points since sampling matrix consisting columns identity matrix. however recent landmark selection techniques utilize out-of-sample extensions input data points improve accuracy nyström method e.g. centroids found k-means clustering. case matrix rn×m necessarily contain columns thus cannot hope sampling matrix satisﬁes show that certain conditions theoretical results applicable case out-ofsample landmark points. formal consider distinct data points i.e. rp×n gaussian kernel form expxi−xj leads kernel matrix rn×n. rp×m cluster centroids kmeans clustering form rn×m rm×m based kernel matrix deﬁned distinct data points using gaussian kernel function full rank. thus deﬁning rn×m write sampling matrix follow show consider empirical kernel deﬁned input data points therefore out-of-sample landmark points gaussian kernel function nyström method exists matrix satisﬁes matrix rm×m represents approximation error. known relative amount error small close another eigenvalues eigenvectors perturbed proportional relative error however work goal prove nyström approximations cw†ct close another relative amount error small. demonstrate importance result note invertible matrices identity m)m−. thus small norm cannot directly used conclude close another. following present error bound difference nyström approximations i.e. cw†ct terms relative amount error caused empirical kernel map. holds even assumptions exactly satisﬁed highlight beneﬁts modiﬁed method. order illustrate effectiveness modiﬁed nyström compare accuracy standard nyström method target rank varying number landmark points provide baseline comparison report accuracy best rank- approximation obtained eigenvalue decomposition requires computation storage full kernel matrices hence impractical large data sets. parameter chosen averaged squared distances data points sample mean. consider landmark selection techniques uniform sampling landmark points selected uniformly random without replacement data points; out-of-sample landmark points obtained k-means clustering original data perform k-means clustering algorithm matlab’s built-in function kmeans maximum number iterations matlab implementation modiﬁed standard nyström available online. used convergence neumann series continuity norms ﬁrst inequality submultiplicativity property spectral norm second. ﬁnish relate difference nyström approximations cw†ct gain intuition theorem present numerical experiment pendigits data used section here gaussian kernel function employed parameter chosen averaged squared distances data points sample mean. standard k-means clustering algorithm performed input data points select landmark points various values value form matrices also compute w−we; calculating impractical larger data sets support theorem. ect/k fig. reports mean cw†ct trials varying number landmark points. ﬁgure also plots mean theoretical bound theorem i.e. observe cw†ct provide similar nyström approximations kernel matrix relative error respect spectral norm less values furthermore clear theoretical bound theorem accurate provides meaningful upper bound relative error nyström approximations. based theorem closeness nyström approximations cw†ct respect spectral norm function quantity note measures relative amount perturbation eigenvalues eigenvectors therefore small cw†ct lead similar low-rank approximations kernel matrix. particular goes zero converges cw†ct hence expect theoretical results rank-r nyström approximations valid case outof-sample landmark points small values present experimental results ﬁxed-rank approximation kernel matrices using standard modiﬁed nyström methods show evidence corroborate theory section suggest theory still respect trace norm i.e. gnys gopt perform baseline. fig. mean standard deviation relative error trials reported four data sets varying number landmark points modiﬁed nyström standard nyström method identical performance rank restriction step involved case. number landmark points increases beyond rank parameter nyström decomposition always generates better rank- approximations kernel matrix standard method does. observation consistent theorem states landmark points gopt constant relative error). even out-of-sample landmark points modiﬁed method drastically better. right plot fig. modiﬁed method achieves mean error landmark points standard method higher mean error even using landmark points. exempliﬁes importance effectiveness precise rank restriction step nyström decomposition method. accuracy either nyström method depends crucially landmark selection procedure. four data sets landmark points obtained k-means clustering lead accurate approximations compared uniform sampling technique; note vertical axis scale varies plot plot. fact using k-means centroids landmark points employing modiﬁed nyström method yield accurate rank- approximations close best rank- approximation. furthermore improvement better sampling pronounced modiﬁed nyström method standard method. fig. generated fig. error reported frobenius norm. pattern behavior similar fig. even though lack theoretical guarantees. fact neither method dominates kernel matrices practical data sets modiﬁed method always performs better. remark remark showed trace frobenius norms standard nyström method perform worse sample additional landmark points. figs. show similar effect happens standard nyström method out-of-sample landmark points selected k-means example according fig. mean relative error standard nyström increased increase landmark points selected k-means centroids; fig. similar effect respect frobenius norm. optimal restriction procedure standard nyström. theorem proves modiﬁed nyström method suffer effect terms trace norm in-sample landmark points figs. show evidence effect even switch frobenius norm consider out-of-sample landmark points. finally demonstrate efﬁciency nyström decomposition plotting averaged running time trials e-tfidf. running time results omitted remaining data sets average running time less second. fig. shows modiﬁed nyström method runs time comparable standard nyström method. thus discussed section practical cases increase computation cost nyström decomposition negligible provides improved ﬁxed-rank kernel approximations. seek given accuracy approximation modiﬁed nyström needs fewer landmark points case could fact argue efﬁcient standard nyström method. paper presented modiﬁed technique important process rank reduction nyström method. theoretical analysis shows that modiﬁed method provides improved ﬁxed-rank approximations compared standard nyström respect trace norm; quality ﬁxed-rank approximations generated modiﬁed method improves number landmark points increases. theoretical results accompanied illustrative numerical experiments comparing modiﬁed method standard nyström. also showed modiﬁed method almost computational complexity standard nyström makes suitable large-scale kernel machines. suykens vandewalle least squares support vector machine classiﬁers neural processing letters vol. schölkopf smola müller nonlinear component analysis kernel eigenvalue problem neural computation vol. chitta havens jain approximate kernel k-means solution large scale kernel clustering proceedings sigkdd international conference knowledge discovery data mining figure mean standard deviation relative kernel approximation error respect trace norm four data sets. subﬁgure left plot uses in-sample landmark points right plot uses out-of-sample points. modiﬁed nyström method consistently generates improved ﬁxed-rank approximations compared standard nyström. figure mean standard deviation relative kernel approximation error respect frobenius norm four data sets. modiﬁed nyström method consistently generates improved ﬁxed-rank approximations. schölkopf smola learning kernels support vector machines regularization optimization beyond. press bach sharp analysis low-rank kernel matrix approximations zhang wang moerchen scaling kernel limited resources low-rank linearization approach international conference artiﬁcial intelligence statistics zhao review nyström methods largescale machine learning information fusion vol. bishop pattern recognition machine learning. springer wang zhao large scale online kernel learning journal machine learning research vol.", "year": 2017}