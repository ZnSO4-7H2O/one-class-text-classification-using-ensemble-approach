{"title": "Memory Networks", "tag": ["cs.AI", "cs.CL", "stat.ML"], "abstract": "We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.", "text": "describe class learning models called memory networks. memory networks reason inference components combined long-term memory component; learn jointly. long-term memory read written goal using prediction. investigate models context question answering long-term memory effectively acts knowledge base output textual response. evaluate large-scale task smaller complex task generated simulated world. latter show reasoning power models chaining multiple supporting sentences answer questions require understanding intension verbs. machine learning models lack easy read write part long-term memory component combine seamlessly inference. hence take advantage great assets modern computer. example consider task told facts story answer questions subject. principle could achieved language modeler recurrent neural network models trained predict next word output read stream words. however memory typically small compartmentalized enough accurately remember facts past rnns known difﬁculty performing memorization example simple copying task outputting input sequence read situation similar tasks e.g. vision audio domains long term memory required watch movie answer questions work introduce class models called memory networks attempt rectify problem. central idea combine successful learning strategies developed machine learning literature inference memory component read written model trained learn operate effectively memory component. introduce general framework section present speciﬁc implementation text domain task question answering section discuss related work section describe experiments ﬁnally conclude section convert internal feature representation update memories given input compute output features given input memory finally decode output features give ﬁnal response process applied train test time distinction phases memories also stored test time model parameters updated. memory networks cover wide class possible implementations. components potentially existing ideas machine learning literature e.g. make favorite models component component make standard pre-processing e.g. parsing coreference entity resolution text inputs. could also encode input internal feature representation e.g. convert text sparse dense feature vector. function selecting slot. updates index parts memory remain untouched. sophisticated variants could back update earlier stored memories based evidence current input input character word level could group inputs store chunk memory slot. memory huge needs organize memories. achieved slot choosing function described example could designed trained store memories entity topic. consequently efﬁciency scale need operate memories operate retrieved subset candidates explore simple variant experiments. memory becomes full procedure forgetting could also implemented chooses memory replaced e.g. could score utility memory overwrite least useful. explored experimentally yet. components component typically responsible reading memory performing inference e.g. calculating relevant memories perform good response. component produces ﬁnal response given example question answering setup ﬁnds relevant memories produces actual wording answer e.g. could conditioned output hypothesis without conditioning memories perform poorly. particular instantiation memory network components neural networks. refer memory neural networks section describe relatively simple implementation memnn textual input output. basic architecture module takes input text. ﬁrst assume sentence either statement fact question answered system text stored next available memory slot original form i.e. returns next empty memory slot module thus used store memory memories updated. sophisticated models described subsequent sections. core inference lies modules. module produces output features ﬁnding supporting memories given procedure generalizable larger highest scoring supporting memory retrieved with finally needs produce textual response simplest response return i.e. output previously uttered sentence retrieved. perform true sentence generation instead employ rnn. experiments also consider easy evaluate compromise approach limit textual responses single word ranking them example task given figure order answer question where milk now? module ﬁrst scores memories i.e. previously seen sentences retrieve relevant fact left milk case. then would search memory second relevant fact given travelled ofﬁce finally module using would score words given output ofﬁce. matrix number features embedding dimension. role original text d-dimensional feature space. simplest feature space choose words representation choose i.e. every word dictionary three different representations depending whether words input arguments actual input supporting memories modeled differently. similarly used well. different weight matrices technically using embedding model represent text could store incoming input using learned embedding vector memory instead. downside choice learning embedding parameters changing hence stored vectors would stale. however test time storing embedding vectors could make sense faster reading original words embedding repeatedly. experiments single dictionary linear embeddings performed worse order model single dictionary could consider deeper networks transform words dependent context. leave future work. figure example story statements questions answers generated simple simulation. answering question location milk requires comprehension actions picked left. questions also require comprehension time elements story e.g. answer where ofﬁce?. training train fully supervised setting given desired inputs responses supporting sentences labeled training data training know best choice functions training performed margin ranking loss stochastic gradient descent speciﬁcally given question true response supporting sentences minimize model parameters case employing component memnn replace last term standard likelihood used language modeling task sequence test time output prediction given contrast absolute simplest model using outputting located memory response would ﬁrst term train. input word rather sentence level words arrive stream already segmented statements questions need modify approach described. hence segmentation function learned takes input last sequence words segmented looks breakpoints. segmenter ﬁres write sequence memory proceed before. segmenter modeled similarly components embedding model form wseg vector sequence input words represented words using separate dictionary. margin sequence recognised segment. memnn learning component write operation. consider segmenter ﬁrst proof concept course could design something much sophisticated. details training mechanism given appendix stored memories large prohibitively expensive score equations instead explore hashing tricks speed lookup hash input buckets score memories buckets. investigated ways hashing hashing words; clustering word embeddings. construct many buckets words dictionary given sentence hash buckets corresponding words. problem memory considered shares least word input method tries solve clustering instead. training embedding matrix k-means cluster word vectors thus giving buckets. hash given sentence buckets individual words fall into. word vectors tend close synonyms cluster together thus also score similar memories well. exact word matches input memory still scored deﬁnition. choosing controls speed-accuracy trade-off. extend model take account memory slot written important answering questions ﬁxed facts important answering questions story e.g. figure obvious implement extra features representations encode index given memory assuming follows write time however requires dealing absolute rather relative time. success empirically following procedure instead scoring input candidate pairs above learn function triples uses three features take value whether older older older model prefers prefers argmax replaced loop memories keeping winning memory step always comparing current winner next memory procedure equivalent argmax time features removed. details given appendix even humans read text words continuously introduced. example ﬁrst time word boromir appears lord rings machine learning model deal this? ideally work seen example. possible would language model given neighboring words predict word assume word similar that. proposed approach takes idea incorporates networks rather separate step. concretely word store words co-occurred with left context right. unknown word represented features. hence increase feature representation model contexts model learns deal words training using kind dropout technique time pretend seen word before hence n-dimensional embedding word represent context instead. instead. words matching score learned embedding score another related propose stay n-dimensional embedding space extend feature representation matching features e.g. word. matching feature indicates word occurs score φx⊤u actually built conditionally words match words matching features unseen words modeled similarly using matching features context words. gives feature space classical methods documents kind memory information retrieval methods answers e.g. references therein. recent methods instead create graph facts knowledge base memory questions logical queries neural network embedding approaches also recently explored compared recent knowledge base approaches memory networks differ apply two-stage strategy apply information extraction principles ﬁrst build followed inference instead extraction useful information answer question performed on-the-ﬂy memory stored text well choices embedding vectors. potentially less brittle ﬁrst stage building already thrown away relevant part original data. classical neural network memory models associative memory networks provide content-addressable memory i.e. given vector output value vector e.g. haykin references therein. typically type memory distributed across whole network weights model rather compartmentalized memory locations. memory-based learning nearest neighbor hand seek store examples compartments memory uses ﬁnding closest labels. memory networks combine compartmentalized memory neural network modules learn read write memory e.g. perform reasoning iteratively read salient facts memory. however notable models attempted include memory read write operations particular designed differentiable push actions called neural network pushdown automaton. work schmidhuber incorporated concept neural networks fast changing weights potentially used memory. schmidhuber proposed allow network modify weights selfreferentially also seen kind memory addressing. finally relevant works discern model script processing memory narx recurrent networks modeling long term dependencies work submitted arxiv neural turing machine work graves relevant related methods. method also proposes perform prediction using large addressable memory read written experiments memory size limited locations whereas consider much larger storage experimental setups notably quite different also whereas focus language reasoning tasks paper focuses problems sorting copying recall. hand problems require considerably complex models memory network described section hand problems known algorithmic solutions whereas language problems not. recent related works. rnnsearch method machine translation uses learned alignment mechanism input sentence representation predicting output order overcome poor performance long sentences. work performs handwriting recognition dynamically determining alignment text locations learns decide character write next. view particular variants memory networks case memory extends back single sentence character sequence. perform experiments dataset introduced fader consists statements stored triples stored memories memnn model. triples reverb extractions mined clueweb corpus cover diverse topics following fader bordes training combines pseudo-labeled pairs made question associated triple pairs paraphrased questions wikianswers like wrote winnie pooh books? poohs creator?. performed experiments framework re-ranking returned candidate answers several systems measuring score test following bordes answers annotated right wrong humans whereas answers ignored test time know label. used memnn model section supporting memory ends similar approach bordes also tried adding words features section well. time unseen word modeling used. results given table results show memnns viable approach large scale terms performance. however lookup linear size memory facts slow. therefore implemented memory hashing techniques section using hashing words clustered embeddings. latter tried clusters. results given table show signiﬁcant speedups maintaining similar performance using cluster-based hash. string hash hand loses performance answers share words longer matched. similar approach bordes also built simple simulation characters objects rooms characters moving around picking dropping objects. actions transcribed text using simple automated grammar labeled questions generated similar way. gives task simple stories figure overall difﬁculty task multiple statements used inference asking object e.g. answer milk figure understand meaning actions picked left inﬂuence relative order. generated statements questions simulator training identical number testing compare memnns rnns lstms task. control complexity task setting limit number time steps past entity question last mentioned. experiments using limit i.e. limit pick random sentence time steps past. chosen sentence mentions actor e.g. bill kitchen generate question where bill? where bill kitchen?. sentence mentions object e.g. bill dropped football question where football?. answers consider options single word answers; simple grammar generating true answers sentence form e.g. kitchen kitchen believe details dataset generation given appendix note object case supporting statements necessary deduce answer last sentences e.g. example answer depends sentences bill actually dropped football. fact dataset generated necessary supporting statements sentences reason also conducted types experiments questions actors actors objects also consider actor-based questions without before questions simplest possible task went garden fred picked milk; moved bathroom fred dropped milk moved living room. dan? living room believe joe? bathroom fred moved bedroom went kitchen took milk journeyed bedroom; discarded milk. milk milk kitchen now? think bedroom took milk there mike travelled ofﬁce went living room next went back kitchen travelled ofﬁce. now? think ofﬁce baseline lstm systems perform language modeling backpropagation time backprop answer words. optimized hyperparameters size hidden layer bptt steps learning rate dataset. memnns ﬁxed embedding dimension learning rate margin epochs training experiments. figure example story questions correctly answered memnn. memnn trained simulation described section never seen many words before e.g. bilbo frodo gollum. bilbo travelled cave. gollum dropped ring there. bilbo took ring. bilbo went back shire. bilbo left ring there. frodo ring. frodo journeyed mount-doom. frodo dropped ring there. sauron died. frodo went back shire. bilbo travelled grey-havens. end. ring? mount-doom bilbo now? grey-havens frodo now? shire figure example dialogue memnn system trained simulation data large-scale data. system able answer questions general world knowledge speciﬁc story-based statements dialogue. fred went kitchen. fred picked milk. fred travelled ofﬁce. milk ofﬁce milk come milk come type female cattle cattle found cattle farm become widespread brazil milk taste like milk taste like milk milk well milk coffee fred ofﬁce kitchen before) perform worse questions even worse difﬁculty tasks. demonstrates poor performance failure encode long-term memory. would likely deteriorate even higher difﬁculty levels lstms however better rnns expected designed sophisticated memory model still trouble remembering sentences past. memnns memory limitation mistakes instead incorrect usage memory wrong statement picked time features necessary good performance questions difﬁculty otherwise pick statement person’s whereabouts since moved. finally results harder actor+object task indicate memnn also successfully perform -stage inference using whereas memnns without inference rnns lstms fail. also tested memnns multi-word answer setting similar results whereby memnns outperform rnns lstms detailed appendix example test prediction output demonstrating model setting given figure tested ability memnns deal previously unseen words test time using unseen word modeling approach sections trained memnn simulated dataset test story given figure story generated using similar structures simulation data except nouns unknowns system training time. despite never seeing lord rings speciﬁc words memnns able correctly answer questions. memnns discover simple linguistic patterns based verbal forms successfully generalize meaning instantiations using unknown words perform -stage inference. without unseen word modeling described section completely fail task. combining simulated world learning real-world data might show power generality models design. implemented naive setup towards goal took models sections trained large-scale simulated data respectively built ensemble two. present input systems question simply output response choices highest score. allows perform simple dialogues combined memnn system. system capable answering general knowledge questions speciﬁc statements relating previous dialogue. example dialogue trace given fig. answers appear whereas others nonsensical. future work combine models effectively example multitasking directly tasks single model. paper introduced powerful class models memory networks showed instantiation future work develop memnns text further evaluating harder open-domain machine comprehension tasks example large scale tasks require multi-hop inference webquestions also tried berant complex simulation data could also constructed order bridge e.g. requiring coreference involving verbs nouns sentences structure requiring temporal causal understanding. sophisticated architectures also explored order deal tasks e.g. using sophisticated memory management sophisticated sentence representations. weakly supervised settings also important explored many datasets supervision form question answer pairs supporting facts well used here. finally believe class models much richer speciﬁc variant detail here currently explored speciﬁc variant memory networks. memory networks applied text tasks domains vision well. berant jonathan srikumar vivek chen pei-chun huang brad manning christopher vander linden abby harding brittany clark peter. modeling biological processes reading comprehension. proc. emnlp sreerupa giles guo-zheng. learning context-free grammars capabilities limitations recurrent neural network external stack memory. proceedings fourteenth annual conference cognitive science society. indiana university iyyer mohit boyd-graber jordan claudino leonardo socher richard daum´e. neural network factoid question answering paragraphs. proceedings conference empirical methods natural language processing weston jason bengio samy usunier nicolas. wsabie scaling large vocabulary image annotation. proceedings twenty-second international joint conference artiﬁcial intelligence-volume volume three aaai press wen-tau xiaodong meek christopher. semantic parsing single-relation question answering. proceedings acl. association computational linguistics june http//research.microsoft.com/apps/pubs/default.aspx?id=. firstly currently encompasses small part kind language understanding want model learn move towards full language understanding believe prerequisite models perform well kind task work real-world environments. currently tasks within simulation restricted question answering tasks location people objects. however envisage tasks possible including asking learner perform actions within simulation asking learner describe actions executing actions asking questions using underlying actions constraints model deﬁnes actors act. currently simple make random valid action moment restricted drop depending types experiments running actor; actor object. write actions text form gives simple story executable simulation e.g. kitchen; fred kitchen; milk; ofﬁce; drop milk; bathroom. example corresponds story given figure system questions state simulation e.g. milk? joe? ofﬁce? easy calculate true answers questions access underlying world. remains convert statements questions look like natural language. simple grammar generating language order produce natural looking text lexical variety built simple automated grammar. verb assigned synonyms e.g. simulation command replaced either picked grabbed took drop replace either dropped left discarded down. similarly object actor replacement synonyms well although currently ambiguity experiments simply articles not. lexical variation questions e.g. where john where john joining statements finally word sequence training setting join statements compound sentences. simply take statements join randomly following then then later that then next. example output seen figure issues great many aspects language modeled. example currently coreference modeled similarly compound noun phrases seem easy simulation. hope adding complexities help evaluate models controlled within simulated environment hard real data. course substitute real data models applied well serve useful testbed. wseg vector already fully supervised setting question training given answer supporting facts input stream also supervision segmenter well. known supporting fact bill kitchen question where bill? wish segmenter statement unﬁnished statements bill the. thus write training criterion segmentation minimization training procedure take account modeling write time slightly different described section write time features important memnn knows memory written hence knows ordering statements comprise story dialogue. note different time information described text statement tense statement statements containing time expressions e.g. went ofﬁce yesterday. cases write time features directly necessary could modeled directly text. model prefers prefers argmax replaced loop memories keeping winning memory step always comparing current winner next memory inference time model functions replaced deﬁned algorithm below. uses three features take value whether older older older ﬁnding second supporting memory encode whether older older older capture relative ﬁrst supporting memory w.r.t. second ﬁrst features. note ﬁnding ﬁrst supporting memory ﬁrst features useless last thing memory hence always older. last term ﬁnal ranking words return response remains unchanged terms replace eqs. considering triples directly. need terms considering second third argument appear either side inference before every step sample rather compute whole training example. computed test accuracy memnns varying amounts training data training questions. results given table results compared rnns lstms full data comparing figure example difﬁculty actor actor object tasks memnns outperform lstms even using times less training examples. conducted experiments input sentence-level data already presegemented statements questions input memnn results comparing rnns memnns given table conclusions similar word level section memnns outperform rnns inference ﬁnds supporting statements time features necessary actor object task. conducted experiments simulation data case answers sentences single word answer model longer used simply compare memnns using either rnns lstms response module baselines still rnns lstms standard setting words including statements question word stream. contrast memnn lstms effectively output module experiments consider difﬁculty actor+object setting case memnns iterations means module features modules run. sentence generation performed test data evaluation chose follows. correct generation contain correct location answer optionally contain subject correct pronoun referring example question where bill? allows correct answers kitchen kitchen bill kitchen kitchen think bill kitchen. however incorrect answers contain incorrect location subject reference example kitchen kitchen bill bathroom believe. measure percentage text examples correct using metric. numerical results given table example output given figure results indicate memnns lstms perform quite strongly outperforming memnns using rnns. however memnn variant outperform rnns lstms distance.", "year": 2014}