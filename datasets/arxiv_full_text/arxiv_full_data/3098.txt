{"title": "Deep Predictive Coding Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.", "text": "quality data representation deep learning methods directly related prior model imposed representations; however generally used ﬁxed priors capable adjusting context data. address issue propose deep predictive coding networks hierarchical generative model empirically alters priors latent representations dynamic contextsensitive manner. model captures temporal dependencies time-varying signals uses top-down information modulate representation lower layers. centerpiece model novel procedure infer sparse states dynamic network used feature extraction. also extend feature extraction block introduce pooling function captures locally invariant representations. applied natural video data show method able learn high-level visual features. also demonstrate role topconnections showing robustness proposed model structured noise. performance machine learning algorithms dependent data represented. methods quality data representation dependent prior knowledge imposed representation. prior knowledge imposed using domain speciﬁc information sift etc. learning representations using ﬁxed priors like sparsity temporal coherence etc. ﬁxed priors became particularly popular training deep networks spite success general purpose priors capable adjusting context data. hand several advantages model actively adapt context data. achieving empirically alter priors dynamic context-sensitive manner. main focus work emphasis visual perception. propose predictive coding framework deep locally-connected generative model uses top-down information empirically alter priors used lower layers perform bottom-up inference. centerpiece proposed model extracting sparse features time-varying observations using linear dynamical model. propose novel procedure infer sparse states dynamical system. extend feature extraction block introduce pooling strategy learn invariant feature representations data. line deep learning methods basic building blocks construct hierarchical model using greedy layer-wise unsupervised learning. hierarchical model built output layer acts input layer above. words layers arranged markov chain states layer dependent representations layer above independent rest model. overall goal dynamical system layer make best prediction representation layer using top-down information layers temporal information previous states. hence name deep predictive coding networks dpcn proposed closely related models proposed predictive coding used statistical model explain cortical functions mammalian brain. similar proposed model construct hierarchical generative models seek infer underlying causes sensory inputs. ballard update rule similar kalman ﬁlter inference friston proposed general framework considering higher-order moments continuous time dynamic model. however neither models capable extracting discriminative information namely sparse invariant representation image sequence helpful high-level tasks like object recognition. unlike models propose efﬁcient inference procedure extract locally invariant representation image sequences progressively extract abstract information higher levels model. methods used building deep models like restricted boltzmann machine autoencoders predictive sparse decomposition also related model proposed here. models constructed similar underlying principles like ours also greedy layer-wise unsupervised learning construct hierarchical model layer consists encoder decoder. models learn encoding decoding concurrently denoising weight sharing building deep network feed forward model using encoder. idea approximate latent representation using feed-forward encoder avoiding decoder typically requires expensive inference procedure. however dpcn encoder. instead dpcn relies efﬁcient inference procedure accurate latent representation. show below reciprocal top-down bottom-up connections make proposed model robust structured noise recognition also allows perform low-level tasks like image denoising. scale large images several convolutional models also proposed similar deep learning paradigm inference models applied entire image rather small parts input. dpcn also extended form convolutional network discussed here. section begin brief description general predictive coding framework proceed discuss details architecture used work. basic block proposed model pervasive across layers generalized state-space model form data functions parameterized terms called unknown causes. since usually interested obtaining abstract information observations causes encouraged non-linear relationship observations. hidden states mediate inﬂuence cause output endow system memory terms stochastic model uncertainty. several state-space models stacked output acting input layer above form hierarchy. l-layered hierarchical model time described terms form stochastic ﬂuctuations higher layers enter layer independently. words model forms markov chain across layers simplifying inference procedure. notice causes lower layer form observations layer causes form link layers states link dynamics time. important point design higher-level predictions inﬂuence lower levels’ figure shows single layered network group small overlapping patches input video. green bubbles indicate group inputs bubbles indicate corresponding states blue bubbles indicate causes pool states within group. shows two-layered hierarchical model constructed stacking several basic blocks. visualization overlapping shown image patches here overlapping patches considered actual implementation. inference. predictions higher layer non-linearly enter state space model empirically altering prior causes. summary top-down connections temporal dependencies state space inﬂuence latent representation layer. following sections ﬁrst describe basic computational network particular form functions speciﬁcally consider linear dynamical model sparse states encoding inputs state transitions followed non-linear pooling function infer causes. next discuss stack learn hierarchical model using several basic networks. also discuss incorporate top-down information inference hierarchical model. begin with consider dynamic network extract features small part video sequence. ...} -dimensional sequence patch extracted location across frames video process this network consists distinctive parts feature extraction pooling ﬁrst part sparse coding used conjunction linear state space model inputs time onto over-complete dictionary k-ﬁlters sparse states keep track dynamics latent states linear function state-transition matrix rk×k. formally inference features performed ﬁnding representation minimizes energy function take advantage spatial relationships local neighborhood small group states ...n} represents contiguous patches w.r.t. position image space added together. pooling states lead local translation invariance. this d-dimensional causes inferred pooled states obtain representation invariant complex local transformations like rotation spatial frequency etc. line invariant function learned capture dependencies components pooled states. speciﬁcally causes inferred constant. notice multiplicatively interacts accumulated states modeling shape sparse prior states. essentially invariant matrix adapted component connects group components accumulated states co-occur frequently. words whenever component active lowers coefﬁcient components making likely active. since co-occurring components typically share common statistical regularity activity typically leads locally invariant representation learn parameters alternatively minimize using procedure similar block co-ordinate descent. ﬁrst infer latent variables keeping parameters ﬁxed update parameters keeping variables ﬁxed. done parameters converge. discuss separately inference procedure update parameters using gradient descent method ﬁxed variables. jointly infer using proximal gradient methods taking alternative gradient descent steps update holding ﬁxed. words alternate updating using single update step minimize respectively. however updating relatively involved. keeping aside causes ﬁrst focus inferring sparse states alone back discuss joint inference states causes. inferring states inferring sparse states given parameters linear dynamical system forms crux model. performed ﬁnding solution minimizes energy function respect states priors states temporal dependence sparsity term. although energy function convex presence non-smooth terms makes hard standard optimization techniques used sparse coding alone. similar problem solved using dynamic programming homotopy bayesian sparse coding however optimization used models computationally expensive large scale problems like object recognition. overcome this inspired method proposed structured sparsity propose approximate solution consistent able efﬁcient solvers like fast iterative shrinkage thresholding alogorithm approach ﬁrst nestrov’s smoothness method approximate non-smooth state transition term. resulting energy function convex continuously differentiable function sparsity constraint hence efﬁciently solved using proximal methods like fista. begin ketk idea smooth approximation function notice that since linear function approximation also smooth w.r.t. re-write using dual norm joint inference showed thus inferred respective energy functions using ﬁrst-order proximal method called fista. however joint inference minimize combined energy function alternately updating holding ﬁxed using single fista update step iteration. important point internal fista step size parameters maintained iterations. procedure equivalent alternating minimization using gradient descent. although procedure longer guarantees convergence optimal solution simulations lead reasonably good solution. please refer algorithm. details. note that alternating update procedure inﬂuenced feed-forward observations temporal predictions feedback connections causes. ﬁxed update parameters minimizing respect since inputs time-varying sequence parameters updated using dual estimation ﬁltering i.e. additional constraint parameters follow state space equation form gaussian transition noise parameters. keeps track temporal relationships. along constraint update parameters using gradient descent. notice ﬁxed parameter matrices updated independently. matrices column normalized update avoid trivial solution. mini-batch update faster convergence parameters updated performing inference large sequence inputs instead every time instance. batch signals sophisticated gradient methods like conjugate gradient used hence lead accurate faster convergence. discussion focused encoding small part video frame using single stage network. build hierarchical model single stage network basic building block arrange form tree structure learn hierarchical model adopt greedy layer-wise procedure like many deep learning methods speciﬁcally following strategy learn hierarchical model. ﬁrst layer learn dynamic network described group small patches video. take learned network replicate several places larger part input frames outputs replicated networks considered inputs layer above. similarly second layer inputs grouped together used train another dynamic network. similar procedure followed build higher layers. emphasis model learned layer-wise manner i.e. top-down information learning network parameters. also note that pooling states layers receptive ﬁeld causes becomes progressively larger depth model. parameters ﬁxed shift focus inference hierarchical model top-down information. discussed above layers hierarchy arranged markov chain i.e. variables layer inﬂuenced variables layer layer above. speciﬁcally states ideally perform inference inﬂuenced hierarchical model states causes updated simultaneously depending present state layers model reaches equilibrium however procedure slow practice. instead propose approximate inference procedure requires single top-down information single bottom-up inference using top-down information. notice additional term involving compared comes top-down information call top-down prediction causes layer using previous states layer speciﬁcally arrival observation time layer ﬁrst propagate likely causes layer using state previous time instance predicted causes predicted causes substituted single layer-wise bottominference performed described section combined prior imposed causes similar elastic prior discussed leading firstly would like test ability proposed model learn complex features higher-layers model. train layered network natural video. frame video ﬁrst contrast normalized described then train ﬁrst layer model overlapping contiguous pixel patches video; layer dimensional states dimensional causes. causes pool states related patches. separation overlapping patches pixels implying receptive ﬁeld causes ﬁrst layer pixels. similarly second layer trained causes ﬁrst layer obtained overlapping pixel patches video. separation patches pixels implying receptive ﬁeld causes second layer pixels. second layer contains dimensional states dimensional causes pools states related patches. figure visualization receptive ﬁelds invariant units learned layer layer trained natural videos. receptive ﬁelds constructed weighted combination dictionary ﬁlters bottom layer. primitive features localized orientation position whereas causes second layer represent complex features like corners angles etc. ﬁlters consistent previously proposed methods like zeiler section show role top-down information inference particularly presence structured noise. video sequences consisting objects three different shapes constructed. objective classify frame coming three different classes. this several pixel frame long sequences made using objects shape bouncing walls. several sequences concatenated form long sequence. train layer network using sequence. first divided frame patches neighboring patches overlapping pixels; frame divided patches. bottom layer trained patches used inputs encoded using dimensional state vector. contiguous neighboring patches pooled infer causes dimensions. second layer trained ﬁrst layer causes inputs inferred contiguous overlapping blocks video frames. states dimensional long causes dimensions. important note receptive ﬁeld second layer causes encompasses entire frame. test performance dpcn conditions. ﬁrst case frames clean video frames shape constructed described above. consider single video without considering discontinuities. second case corrupt clean video structured noise randomly pick number objects three shapes poisson distribution frame independently random locations. correlation consecutive frames regarding noisy objects added first consider clean video perform inference bottom-up inference i.e. inference consider figure shows scatter plot three dimensional causes layer. clearly clusters recognizing three different shape video sequence. figure shows scatter plot procedure applied noisy video. observe shapes clearly distinguished. finally top-down information along bottom-up inference described section noisy data. argue that since second layer learned class speciﬁc information top-down information help bottom layer units disambiguate noisy objects true objects. figure shows scatter plot case. clearly top-down information spite largely corrupted sequence dpcn able separate frames belonging three shapes figure shows scatter plot dimensional causes top-layer clean video bottom-up inference corrupted video bottom-up inference corrupted video top-down along bottom-up inference. point shape marker indicates true shape object frame. paper proposed deep predictive coding network generative model empirically alters priors dynamic context sensitive manner. model composes main components linear dynamical models sparse states used feature extraction top-down information adapt empirical priors. dynamic model captures temporal dependencies reduces instability usually associated sparse coding task speciﬁc information layers helps resolve ambiguities lower-layer improving data representation presence noise. believe approach extended convolutional methods paving implementation high-level tasks like object recognition etc. large scale videos images.", "year": 2013}