{"title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence  Learning", "tag": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.", "text": "ping∗ kainan peng∗ andrew gibiansky∗ sercan arık∗ ajay kannan sharan narang baidu research {pingwei pengkainan gibianskyandrew sercanarik kannanajay sharan}baidu.com jonathan raiman∗† openai raimanopenai.com present deep voice fully-convolutional attention-based neural textto-speech system. deep voice matches state-of-the-art neural speech synthesis systems naturalness training order magnitude faster. scale deep voice dataset sizes unprecedented training eight hundred hours audio thousand speakers. addition identify common error modes attention-based speech synthesis networks demonstrate mitigate them compare several different waveform synthesis methods. also describe scale inference million queries single server. text-to-speech systems convert written language human speech. systems used variety applications human-technology interfaces accessibility visuallyimpaired media entertainment. traditional systems based complex multi-stage hand-engineered pipelines typically systems ﬁrst transform text compact audio representation convert representation audio using audio waveform synthesis method called vocoder. recent work neural demonstrated impressive results yielding pipelines simpler features fewer components higher quality synthesized speech. consensus optimal neural network architecture tts. however sequence-to-sequence models shown promising results. paper propose novel fully-convolutional architecture speech synthesis scale large audio data sets address several real-world issues arise attempting deploy attention-based system. speciﬁcally make following contributions propose fully-convolutional character-to-spectrogram architecture enables fully parallel computation trains order magnitude faster analogous architectures using recurrent cells several recent works tackle problem synthesizing speech neural networks including deep voice deep voice tacotron charwav voiceloop samplernn wavenet deep voice retain traditional structure pipelines separating grapheme-to-phoneme conversion duration frequency prediction waveform synthesis. contrast deep voice deep voice employs attentionbased sequence-to-sequence model yielding compact architecture. similar deep voice tacotron charwav propose sequence-to-sequence models neural tts. tacotron neural text-to-spectrogram conversion model used grifﬁn-lim spectrogram-to-waveform synthesis. charwav predicts parameters world vocoder uses samplernn conditioned upon world parameters waveform generation. contrast charwav tacotron deep voice avoids recurrent neural networks speed training. deep voice makes attention-based feasible production system compromise accuracy avoiding common attention errors. finally wavenet samplernn neural vocoder models waveform synthesis. also numerous alternatives highquality hand-engineered vocoders literature straight vocaine world deep voice adds novel vocoder potential integrated different waveform synthesis methods slight modiﬁcations architecture. automatic speech recognition datasets often much larger traditional corpora tend less clean typically involve multiple microphones background noise. although prior work applied methods datasets deep voice best knowledge ﬁrst system scale thousands speakers single model. sequence-to-sequence models encode variable-length input hidden states processed decoder produce target sequence. attention mechanism allows decoder adaptively select encoder hidden states focus generating target sequence attention-based sequence-to-sequence models widely applied machine translation speech recognition text summarization recent improvements attention mechanisms relevant deep voice include enforced-monotonic attention training fully-attentional non-recurrent architectures convolutional sequenceto-sequence models deep voice demonstrates utility monotonic attention training domain monotonicity expected. alternatively show simple heuristic enforce monotonicity inference standard attention mechanism work well even better. deep voice also builds upon convolutional sequence-to-sequence architecture gehring introducing positional encoding similar used vaswani augmented rate adjustment account mismatch input output domain lengths. section present fully-convolutional sequence-to-sequence architecture architecture capable converting variety textual features variety vocoder parameters e.g. mel-band spectrograms linear-scale magnitude spectrograms fundamental frequency spectral envelope aperiodicity parameters. vocoder parameters used inputs audio waveform synthesis models. figure deep voice uses residual convolutional layers encode text per-timestep value vectors attention-based decoder. decoder uses predict mel-scale magnitude spectrograms correspond output audio. hidden states decoder converter network predict vocoder parameters waveform synthesis. appendix details. encoder fully-convolutional encoder converts textual features internal decoder fully-convolutional causal decoder decodes learned representation overall objective function optimized linear combination losses decoder converter separate decoder converter apply multi-task training makes attention learning easier practice. speciﬁc loss mel-spectrogram prediction guides training attention mechanism attention trained gradients mel-spectrogram prediction besides vocoder parameter prediction. multi-speaker scenario trainable speaker embeddings arık used across encoder decoder converter. next describe components data preprocessing detail. model hyperparameters available table within appendix text preprocessing crucial good performance. feeding text yields acceptable performance many utterances. however utterances mispronunciations rare words yield skipped words repeated words. alleviate issues normalizing input text follows uppercase characters input text. remove intermediate punctuation marks. every utterance period question mark. replace spaces words special separator characters indicate duration pauses inserted speaker words. four different word separators indicating slurred-together words standard pronunciation space characters short pause words long pause words. example sentence either shoot slowly long pause short pause shoot would written either way%you shoot/very slowly%. representing long pause representing short pause encoding convenience. pause durations obtained either manual labeling estimated text-audio aligner gentle single-speaker dataset labeled hand multispeaker datasets annotated using gentle. model directly convert characters acoustic features hence learns implicit grapheme-to-phoneme model. implicit conversion difﬁcult correct model makes mistakes. thus addition character models also train phoneme-only models mixed character-and-phoneme models allowing phoneme input option explicitly. models identical character-only models except input layer encoder sometimes receives phoneme phoneme stress embeddings instead character embeddings. phoneme-only model requires preprocessing step convert words phoneme representations mixed character-and-phoneme model requires similar preprocessing step except words phoneme dictionary. out-of-vocabulary words input characters allowing model implicitly learned grapheme-to-phoneme model. training mixed character-and-phoneme model every word replaced phoneme representation ﬁxed probability training iteration. improves pronunciation accuracy minimizes attention errors especially generalizing utterances longer seen training. importantly models support phoneme representation allow correcting mispronunciations using phoneme dictionary desirable feature deployed systems. figure convolution block consists convolution gated linear unit residual connection. denotes dimensionality input. convolution output size split equal-sized portions gate vector input vector. providing sufﬁciently large receptive ﬁeld stacked convolutional layers utilize long-term context information sequences without introducing sequential dependency computation. convolution block depicted fig. main sequential processing unit encode hidden representations text audio. convolution block consists convolution ﬁlter gated-linear unit learnable nonlinearity residual connection input scaling factor gated linear unit provides linear path gradient alleviates vanishing gradient issue stacked convolution blocks retaining non-linearity. introduce speaker-dependent control speaker-dependent embedding added bias convolution ﬁlter output softsign function. softsign nonlinearity limits range output also avoiding saturation problem exponentialbased nonlinearities sometimes exhibit. initialize convolution ﬁlter weights zero-mean unit-variance activations throughout entire network. convolutions architecture either non-causal causal preserve sequence length inputs padded timesteps zeros left causal convolutions timesteps zeros left right non-causal convolutions convolution ﬁlter width. dropout applied inputs prior convolution regularization. encoder network begins embedding layer converts characters phonemes trainable vector representations embeddings ﬁrst projected fully-connected layer embedding dimension target dimensionality. then processed series convolution blocks described section extract time-dependent text information. lastly projected back embedding dimension create attention vectors attention value vectors computed attention vectors text embeddings jointly consider local information long-term context information vectors used attention block compute attention weights whereas ﬁnal context vector computed weighted average value vectors decoder generates audio autoregressive manner predicting group future audio frames conditioned past audio frames. since decoder autoregressive must causal convolution blocks. choose mel-band log-magnitude spectrogram compact low-dimensional audio frame representation. similar wang empirically observed decoding multiple frames together yields better audio quality. decoder network starts multiple fully-connected layers rectiﬁed linear unit nonlinearities preprocess input mel-spectrograms then followed series causal convolution attention blocks. convolution blocks generate queries used attend encoder’s hidden states lastly fully-connected layer output next group audio frames also binary ﬁnal frame prediction dropout applied fully-connected layer prior attention blocks except ﬁrst one. loss computed using output mel-spectrograms binary cross-entropy loss computed using ﬁnal-frame prediction. dot-product attention mechanism similar vaswani attention mechanism uses query vector per-timestep vectors encoder compute attention weights outputs context vector computed weighted average value vectors. observe empirical beneﬁts introducing inductive bias attention follows monotonic progression time. thus positional encoding query vectors. positional encodings chosen k/d) timestep index channel index positional training attention weights dropped out. ﬁxed ratio output timesteps input timesteps multi-speaker datasets computed query speaker embedding speaker sine cosine functions form orthonormal basis initialization yields attention distribution form diagonal line initialize fully-connected layer weights used compute hidden attention vectors values query projection projection. positional encodings used attention blocks. context normalization gehring fully-connected layer applied context vector generate output attention block. overall positional encodings improve convolutional attention mechanism. production-quality systems tolerance attention errors. hence besides positional encodings consider additional strategies eliminate cases repeating skipping words. approach substitute canonical attention mechanism monotonic attention mechanism introduced raffel approximates hard-monotonic stochastic decoding soft-monotonic attention training expectation. despite improved monotonicity strategy yield diffused attention distribution. cases several charthe paper raffel also proposes hard monotonic attention process sampling. aims improve inference speed attending states selected sampling thus avoiding compute future states. work beneﬁt speedup observed poor attention behavior cases e.g. stuck ﬁrst last character. acters attended time high quality speech couldn’t obtained. attribute unnormalized attention coefﬁcients soft alignment potentially resulting weak signal encoder. thus propose alternative strategy constraining attention weights inference monotonic preserving training procedure without constraints. instead computing softmax entire input instead compute softmax ﬁxed window starting last attended-to position going forward several timesteps initial position zero later computed index highest attention weight within current window. strategy also enforces monotonic attention inference shown fig. yields superior speech quality. converter network takes inputs activations last hidden layer decoder applies several non-causal convolution blocks predicts parameters downstream vocoders. unlike decoder converter non-causal non-autoregressive future context decoder predict outputs. grifﬁn-lim vocoder grifﬁn-lim algorithm converts spectrograms time-domain audio waveforms iteratively estimating unknown phases. raising spectrogram power parametrized sharpening factor waveform synthesis helpful improved audio quality suggested wang loss used prediction linear-scale log-magnitude spectrograms. world vocoder world vocoder based vocoder parameters predict boolean value value spectral envelope aperiodicity parameters. cross-entropy loss voiced-unvoiced prediction losses predictions wavenet vocoder separately train wavenet used vocoder treating melscale log-magnitude spectrograms vocoder parameters. vocoder parameters input external conditioners network. wavenet trained using ground-truth mel-spectragrams audio waveforms. architecture besides conditioner similar wavenet described arık wavenet arık conditioned linear-scale log-magnitude spectrograms observed better performance mel-scale spectrograms corresponds compact representation audio. addition loss mel-scale spectrograms decode loss linear-scale spectrogram also applied grifﬁn-lim vocoder. table attention error counts single-speaker deep voice models -sentence test given appendix mispronunciations skips repeats count single mistake utterance. phonemes characters refers model trained joint character phoneme representation discussed section include phoneme-only models test contains out-of-vocabulary words. models grifﬁn-lim vocoder. section present several different experiments metrics evaluate speech synthesis system. quantify performance system compare recently published neural systems. data single-speaker synthesis internal english speech dataset containing approximately hours audio sample rate khz. multi-speaker synthesis vctk librispeech datasets. vctk dataset consists audios speakers total duration hours. librispeech dataset consists audios speakers total duration hours. sample rate vctk librispeech. fast training compare deep voice tacotron recently published attention-based system. system single-speaker data average training iteration time seconds using opposed seconds tacotron indicating ten-fold increase training speed. addition deep voice converges iterations three datasets experiment tacotron requires iterations suggested wang signiﬁcant speedup fully-convolutional architecture deep voice exploits parallelism training. attention error modes attention-based neural systems several error modes reduce synthesis quality including repeated words mispronunciations skipped words. reason attention-based model impose monotonically progressing mechanism. order track occurrence attention errors construct custom -sentence test includes particularly-challenging cases deployed systems attention error counts listed table indicate model joint representation characters phonemes trained standard attention mechanism enforced monotonic constraint inference largely outperforms approaches. naturalness demonstrate choice waveform synthesis matters naturalness ratings compare published neural systems. results table indicate wavenet neural vocoder achieves highest followed world grifﬁn-lim respectively. thus show natural waveform synthesis done neural vocoder basic spectrogram inversion techniques match advanced vocoders high quality single speaker data. wavenet vocoder sounds natural world vocoder introduces various noticeable artifacts. lower inference latency render world vocoder preferable heavily engineered wavenet implementation runs realtime core world runs realtime core table mean opinion score ratings conﬁdence intervals using different waveform synthesis methods. crowdmos toolkit batches samples models presented raters mechanical turk. since batches contained samples models experiment naturally induces comparison models. table ratings conﬁdence intervals audio clips neural systems multi-speaker datasets. also crowdmos toolkit; batches samples including ground truth presented human raters. multi-speaker tacotron implementation hyperparameters based arık proof-of-concept implementation. deep voice tacotron systems trained librispeech dataset prohibitively long time required optimize hyperparameters. multi-speaker synthesis demonstrate model capable handling multi-speaker speech synthesis effectively train models vctk librispeech data sets. librispeech apply preprocessing step standard denoising splitting long utterances multiple pause locations results presented table purposefully include ground-truth samples evaluated accents datasets likely unfamiliar north american crowdsourced raters. model world vocoder achieves comparable vctk contrast deep voice state-of-theart multi-speaker neural system using wavenet vocoder seperately optimized phoneme duration fundamental frequency prediction models. expect improvement using wavenet multi-speaker synthesis although substantially slow inference. librispeech lower compared vctk mainly attribute lower quality training dataset various recording conditions noticeable background noise. literature yamagishi also observes worse performance apply parametric method different datasets hundreds speakers. lastly learned speaker embeddings meaningful latent space optimizing inference deployment order deploy neural system cost-effective manner system must able handle much trafﬁc alternative systems comparable amount hardware. target throughput million queries queries second single-gpu server twenty cores comparable cost commercially deployed systems. implementing custom kernels deep voice architecture parallelizing world synthesis across cpus demonstrate model handle million queries day. provide details implementation appendix introduce deep voice neural text-to-speech system based novel fully-convolutional sequence-to-sequence acoustic model position-augmented attention mechanism. describe common error modes sequence-to-sequence speech synthesis models show successfully avoid common error modes deep voice show model agnostic waveform synthesis method adapt grifﬁn-lim spectrogram inversion wavenet world vocoder synthesis. demonstrate also architecture capable multispeaker speech synthesis augmenting trainable speaker embeddings technique described deep voice finally describe production-ready deep voice system full including text normalization performance characteristics demonstrate state-of-the-art quality extensive evaluations. future work involve improving implicitly learned grapheme-tophoneme model jointly training neural vocoder training cleaner larger datasets scale model full variability human voices accents hundreds thousands speakers. sercan arık mike chrzanowski adam coates gregory diamos andrew gibiansky yongguo kang xian john miller jonathan raiman shubho sengupta mohammad shoeybi. deep voice real-time neural text-to-speech. icml capes paul coles alistair conkie ladan golipour abie hadjitarkhani qiong nancy huddleston melvyn hunt jiangchuan matthias neeracher siri on-device deep learningguided unit selection text-to-speech system. interspeech kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp hideki kawahara ikuyo masuda-katsuse alain cheveigne. restructuring speech representations using pitch-adaptive time–frequency smoothing instantaneous-frequency-based extraction possible role repetitive structure sounds. speech communication soroush mehri kundan kumar ishaan gulrajani rithesh kumar shubham jain jose sotelo aaron courville yoshua bengio. samplernn unconditional end-to-end neural audio generation model. iclr masanori morise fumiya yokomori kenji ozawa. world vocoder-based high-quality speech synthesis system real-time applications. ieice transactions information systems aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv. vassil panayotov guoguo chen daniel povey sanjeev khudanpur. librispeech corpus based public domain audio books. acoustics speech signal processing ieee international conference ieee yuxuan wang skerry-ryan daisy stanton yonghui weiss navdeep jaitly zongheng yang ying xiao zhifeng chen samy bengio quoc yannis agiomyrgiannakis clark saurous. tacotron towards end-to-end speech synthesis. interspeech junichi yamagishi bela usabaev simon king oliver watts john dines jilei tian yong guan rile keiichiro oura yi-jian thousands voices hmm-based speech synthesis– analysis application systems built various corpora. ieee transactions audio speech language processing figure deep voice uses deep residual convolutional network encode text and/or phonemes per-timestep value vectors attentional decoder. decoder uses predict mel-band magnitude spectrograms correspond output audio. hidden state decoder gets converter network output linear spectrograms grifﬁn-lim parameters world used synthesize ﬁnal waveform. weight normalization applied convolution ﬁlters fully-connected layer weight matrices model. running inference tensorflow graph turns prohibitively expensive averaging approximately instead implement custom kernels deep voice inference. complexity model large number output timesteps launching individual kernels different operations graph impractical overhead launch cuda kernel approximately which aggregated across operations model output timesteps limits throughput approximately qps. thus implement single kernel entire model avoids overhead launching many cuda kernels. finally instead batching computation kernel kernel operates single utterance launch many concurrent streams streaming multiprocessors gpu. every kernel launched block expect schedule block allowing scale inference speed linearly number sms. single nvidia tesla achieve inference speed corresponds target million queries day. parallelize world synthesis across cpus server permanently pinning threads cpus order maximize cache performance. poor tensorflow performance overhead running graph evaluator hundreds nodes hundreds timesteps. using technology tensorflow could speed evaluation unlikely match performance hand-written kernel. parameter size window size shift audio sample rate reduction factor bands sharpening factor character embedding dim. encoder layers conv. width channels decoder afﬁne size decoder layers conv. width attention hidden size position weight initial rate converter layers conv. width channels dropout keep probability number speakers speaker embedding dim. adam learning rate anneal rate anneal interval batch size gradient norm gradient clipping max. value similar arık apply principal component analysis learned speaker embeddings analyze speakers based ground truth genders. fig. shows genders speakers space spanned ﬁrst principal components. observe clear separation male female genders suggesting low-dimensional speaker embeddings constitute meaningful latent space.", "year": 2017}