{"title": "Language modeling with Neural trans-dimensional random fields", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Trans-dimensional random field language models (TRF LMs) have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrating rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks (NNs), in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.", "text": "trans-dimensional random ﬁeld language models recently introduced sentences modeled collection random ﬁelds. approach shown advantages computationally efﬁcient inference lstm close performance able ﬂexibly integrate rich features. paper propose neural trfs beyond previous discrete trfs linear potentials discrete features. idea nonlinear potentials continuous features implemented neural networks framework. neural trfs combine advantages trfs. beneﬁts word embedding nonlinear feature learning larger context modeling inherited nns. time strength efﬁcient inference avoiding expensive softmax preserved. number technical contributions including employing deep convolutional neural networks deﬁne potentials incorporating joint stochastic approximation strategy training algorithm developed work enable successfully train neural lms. various evaluated terms speech recognition wers rescoring -best lists wsj’ test data. results show neural improve discrete also perform slightly better lstm ﬁfth parameters faster inference efﬁciency. statistical language models estimate joint probability words sentence form crucial component many applications automatic speech recognition machine translation recently neural network language models either feedforward recurrent shown surpass classical n-gram lms. rnns long short-term memory units particularly popular. remarkably n-gram follow directed graphical modeling approach represents joint probability terms conditionals. contrast trans-dimensional random ﬁeld recently introduced undirected graphical modeling approach sentences modeled collection random ﬁelds joint probability deﬁned terms local potential functions. shown signiﬁcantly outperform n-gram perform close lstm computationally efﬁcient inference discrete features used previous models limit performances. previous models thus referred discrete trfs. limitation clear comparing discrete lstm lms. first lstm associate word vocabulary real-valued feature vector. word embedding continuous vector space creates notion similarity words achieves level generalization hard discrete features. discrete trfs mainly rely word classing various orders discrete features smoothing parameter estimates. second lstm learn nonlinear interactions underlying features discrete basically log-linear models. third lstm models could model larger contexts using memory cells discrete models. despite differences discrete still achieves impressive performances close lstm lms. promising extension integrate framework thus eliminating limitation discrete trfs. transdimensional random ﬁelds paper. idea nonlinear potentials continuous features implemented framework. neural trfs combine advantages trfs. beneﬁts word embedding nonlinear feature learning larger context modeling inherited nns. time strength efﬁcient inference avoiding expensive softmax preserved. developed stochastic approximation algorithm called augmented markov chain monte carlo estimate model parameters normalizing constants discrete trfs. note log-likelihood discrete concave guaranteeing training convergence global maximum. fitting neural trfs non-convex optimization problem challenging. number technical contributions made work enable successfully train neural trfs. first employ deep convolutional neural networks deﬁne potential functions. cnns stacked represent larger larger context allows easier gradient propagation lstm rnns. second augsa training algorithm extended train neural trfs incorporating joint stochastic approximation strategy used successfully train deep generative models. strategy introduce auxiliary distribution serve proposal constructing mcmc operator target distribution. loglikelihood target distribution kl-divergence target distribution auxiliary distribution jointly optimized. resulting augsa plus algorithm crucial handling deep features signiﬁcantly reducing computation cost every iteration step also considerably improving training convergence. third several additional techniques found improve convergence training neural trfs includvarious evaluated terms speech recognition wers rescoring -best lists wsj’ test data. neural improves discrete reducing less parameters. compared state-of-theart lstm neural outperforms small lstm relative reduction performs slightly better medium lstm ﬁfth parameters. moreover inference neural times faster medium lstm average time cost rescoring -best list utterance wsj’ test second seconds using gpu. rest paper ﬁrst discuss related works section introduce neural model section training algorithm section presenting experimental results section conclusions made section research roughly divided tracks. directed graphical modeling approach includes classic n-gram various lms. undirected graphical modeling approach priori work except review tracks found knowledge work represents ﬁrst success using undirected graphical modeling approach language modeling. starting discrete trfs main features neural trfs proposed paper marriage random ﬁelds neural networks cnns feature extraction. following mainly comment related studies connection work. strength nonlinear feature learning ability. random ﬁelds powerful describing interactions among structured random variables. combining pursued models developed fact combining conditional random ﬁelds namely crfs+nns. conventional crfs node potentials edge potentials deﬁned linear functions using discrete indicator features. crfs+nns introduced times priori literature. termed conditional neural fields later neural conditional random fields slightly different speciﬁcation potentials. also appeared previously speech recognition literature recently increasing studies various types e.g. fnns rnns lstm rnns extract features input crfs. remarkably general idea crfs+nns models implement combination using represent potential functions spirit neural trfs. however algorithms developed crfs+nns studies applicable neural trfs sample spaces crfs much smaller trfs. worth pointing crfs used discriminative tasks e.g. sequence labeling structured prediction tasks. contrast language modeling generative modeling task. generative random ﬁeld model proposed potentials also deﬁned cnns modeling ﬁxed-size images besides great success computer vision cnns recently received attention language modeling. cnns language feature detectors stacked hierarchically capture large context like computer vision. shown applying convolutional layers fnns performs better conventional lstm lms. convolutional layers also used within rnns e.g. studied convolutional ﬁlters varying widths applied characters whose output upper lstm rnn. recently shown cnnfnns novel gating mechanism beneﬁt gradient propagation perform slightly better lstm lms. similarly pilot experiment shows using stacked structure cnns neural trfs allows easier model training using recurrent structure rnns. embedding projection. first word sentence mapped embedded vector projection layer rectiﬁed linear unit activation applied embedding vector reduce dimension i.e. cnn-bank. outputs projection layer cnn-bank module contains convolutional ﬁlters widths ranging ﬁlters explicitly model local contextual information denote rdp×k ﬁlter width rdp×l output projection layer. ﬁrst symmetrically zeros beginning make longer denoted rdp×. convolution performed ﬁlter output feature given i-th component vector i-to--th columns forbenius inner product. convolution padding scheme known half convolution. output feature maps multiple novel learning algorithm called augmented developed estimate model parameters normalizing constants discrete trfs section augsa algorithm extended train neural trfs. hypothesized value ratio respect namely log{zl/z} chosen reference value calculated exactly. speciﬁed length probability used model training. note prior length probability empirical length probability inference. denote training collection sentences length training set. maximum likelihood estimation parameter normalization constant found solving following simultaneous equations exact solving infeasible. augsa proposed stochastically solve framework iterates mcmc sampling parameter update. convergence studied various conditions mcmc sampling augsa implemented trans-dimensional mixture sampling algorithm simulate sentences different dimensions joint distribution sampling operations transms make computational bottleneck augsa. suppose gibbs sampling simulate sentence given length method used training discrete trfs. need calculate conditional distribution word position given ﬁlters varying widths spliced together followed max-pooling time width stride zeros also padded max-pooling preserve time dimensionality. suppose ﬁlters ﬁlter width. output cnn-bank module rwk×l. cnn-stack. cnn-bank module cnn-stack module consists stack convolutional layers extract hierarchical features layers. outputs convolutional layer weighted summarized similar rds×l denote output skip-connections j-th convolutional layer experiments reduce dimension employ half convolution layer. output cnn-stack module rds×l adam denotes adam optimizer learning rate empirical probability length remarkably gradient respect efﬁciently computed backpropagtion. update normalization constants trans-dimensional mixture sampling proposed extended applying augsa plus jsa. transms consists steps iteration local jump dimensions markov move given dimension. first auxiliary distribution introduced proposal distribution local jump markov move. second multiple-trial metropolis independence sampling used increase acceptance rate. denote length sequence sampling iteration algorithm described follows. step local jump. assuming ﬁrst draw length jump distribution deﬁned uniform neighborhood words x=i. computational expensive calculating needs enumerate possible values compute joint probability possible value denotes vocabulary. word classing introduced accelerate sampling means word assigned single class. applying metropolis-hastings within gibbs sampling ﬁrst sample class using reduced model proposal includes features depend class sample word. reduces computational cost |v|/|c| denotes number classes. however computation reduction using word classing neural trfs signiﬁcant discrete trfs deep potentials neural trfs involve much larger context makes sampling computation reduced model still expensive. apply augsa neural trfs borrow idea joint stochastic approximation used successfully train deep generative models. strategy introduce auxiliary distribution parameter serve proposal constructing mcmc operator target distribution log-likelihood target distribution kl-divergence target distribution auxiliary distribution jointly optimized. therefore augsa plus algorithm deﬁned stochastically solving three simultaneous equations namely together parameter auxiliary distribution iteration updated together parameter normalization constants used transms proposal distribution paper auxiliary distribution implemented lstm rnn. moreover several additional techniques used suit nonlinear potentials improve convergence training neural trfs. ﬁrst technique called training mini-batching iteration mini-batch sentences randomly selected training empirical expectation calculated mini-batch. crucial training neural trfs unlike discrete trfs gradient nonlinear potential function respect depends parameters second adam optimizer used update parameter saving computation cost estimating empirical variances. table comparison three different rescoring strategies lstm lms. lstm-×// indicates lstm hidden layers hidden units layer. denotes perplexity test set. wer-p wer-r wer-s correspond preserve reset shufﬂe strategy respectively. step markov move. step given current sequence ﬁxing length perform block gibbs sampling ﬁrst position last position block-size mtmis used proposal distribution local jump. experiments block-size multiple-trial number denote current length sequence local jump. positions markov move proceeds follows speech recognition wers various obtained using training development sets applied rescore -best lists recognizing wsj’ test data utterance -best list candidate sentences generated ﬁrst-pass recognition using kaldi toolkit dnn-based acoustic models. oracle -best list baseline include -gram modiﬁed kneserney smoothing three lstm hidden layers hidden units layer respectively called small medium large lstms reproduce three lstm rescoring experiments. -gram trained using srilm toolkit automatically adds beginning-token endtoken h/si beginning sentence. applying rescoring beginning-token endtoken also added sentence -best list. contrast training lstm tricky practice end-token sentence concatenate training sentences. fact treats whole training corpus single long sentence. rescoring end-token added sentence like training. three strategies initial hidden state conﬁgured rescoring. wers strategies shown table seen lowest achieved lstm-× hidden layer hidden units using preserve strategy. shufﬂe sentences increases signiﬁcantly even worse lstm×. preserve strategy candidate sentences utterance rescored successively. ﬁnal hidden state carries relevant information beneﬁt prediction next candidate sentence belongs testing utterance current candidate sentence. shufﬂing relation between adjacent candidate sentences broken. information preserved hidden states mislead prediction. following wer-r obtained reset strategy performance measure lstm since resulting wers independent processing order candidate sentences stable. moreover enables fair comparison neural since information across sentences. section compare neural different speech recognition. training corpus wall street journal portion penn treebank sections used training sections development section test vocabulary limited words including special token hunki denoting word vocabulary. setting used studies evaluation terms conﬁguration used neural models shown table augsa plus algorithm fig. used train neural training corpus. iteration random select sentences training corpus generate sentences various lengths using transms algorithm described section auxiliary distribution deﬁned lstm hidden layers hidden units. learning rates length distribution speciﬁed table performances various lms. perplexity test set. word error rate wsj’ test data. param number parameter numbers denotes log-linear interpolation equal weights lstms obtained using reset strategy. inference time denotes average time rescoring -best list utterance. parameters lstm initialized randomly within interval except word embedding initialized running wordvec toolkit training updated training. stop training smoothed log-likelihood development increase signiﬁcantly resulting iterations negative log-likelihood test kldivergence model distribution auxiliary distribution shown fig. model parameters neural estimated stochastically cache model parameters recent training epochs. training stopped calculate ppls test scores -best lists using cached model parameters. resulting ppls averaged ﬁnal scores models averaged used rescoring giving ﬁnal wer. similar model combination using sentence-level log-linear interpolation reduces variance stochastically estimated models. ppls wers various shown table several comments. first studied augsa tends underestimate perplexity test set. reported ppls table lower bound true ppls models. second neural achieves outperforms discrete using features w+c+ws+cs+wsh+csh+tied relative reduction compared lstm-x similar model size neural achieves relative reduction compared lstm-x neural achieves slightly lower ﬁfth parameters. large lstm-x performs slightly better neural times parameters. third examine neural lsmt complimentary other. probability sentences log-linearly combined equal interpolated weights interpolated neural lstm-x reduces achieves lowest moreover inference neural much faster lstm lms. time cost using neural trfs rescore -best list utterance second. compared lstm inference neural trfs times faster lstm-x lstm-x times faster lstm-x. worth pointing apart success language modeling neural models also applied sequential trans-dimensional data modeling tasks general also discriminative modeling tasks e.g. extending current crfs+nns models. language modeling integrating richer nonlinear structured features important future direction. tomas mikolov stefan kombrink lukas burget cernocky sanjeev khudanpur extensions recurrent neural network language model proc. international conference acoustics speech signal processing wang zhijian zhiqiang learning transdimensional random ﬁelds applications language modeling ieee transactions pattern analysis machine intelligence a¨aron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu wavenet generative model audio corr abs/. ronald rosenfeld stanley chen xiaojin wholesentence exponential language models vehicle linguisticstatistical integration computer speech language vol. john lafferty andrew mccallum fernando pereira conditional random ﬁelds probabilistic models segmenting labeling sequence data proc. international conference machine learning rohit prabhavalkar eric fosler-lussier backpropagation training multilayer conditional random ﬁeld based phone recognition acoustics speech signal processing ieee international conference ieee ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa natural language processing scratch journal machine learning research vol. kaisheng baolin peng geoffrey zweig dong xiaolong feng recurrent conditional random ﬁeld language understanding acoustics speech signal processing ieee international conference", "year": 2017}