{"title": "Reconstruction of Hidden Representation for Robust Feature Extraction", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "This paper aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretical summarize the general properties of all algorithms that are based on traditional Auto-Encoders: 1) The reconstruction error of the input or corrupted input can not be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input or corrupted input. 2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. 3) Minimizing the Frobenius norm of the Jacobian matrix has a deficiency and may result in a much worse local optimum value. 4) Minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible. We also show that for handling inessential features, our model is more robust than Denoising Auto-Encoders (DAEs). Comparative experiments illustrate that our model is significantly better for representation learning than the state-of-the-art models.", "text": "abstract—this paper aims develop robust approach feature representation. motivated success autoencoders ﬁrst theoretical summarize general properties algorithms based traditional auto-encoders reconstruction error input corrupted input lower lower bound viewed guiding principle reconstructing input corrupted input. reconstruction hidden representation achieving ideal situation necessary condition reconstruction input reach ideal state. minimizing frobenius norm jacobian matrix deﬁciency result much worse local optimum value. minimizing reconstruction error hidden representation robust minimizing frobenius norm jacobian matrix. based analysis propose model termed double denoising auto-encoders uses corruption reconstruction input hidden representation. demonstrate proposed model highly ﬂexible extensible. also show handling inessential features model robust denoising auto-encoders comparative experiments illustrate model signiﬁcantly better representation learning state-of-the-art models. developed important area machine learning research recent years. development also witnessed wide range successful applications ﬁelds computer vision speech recognition natural language processing reviews recent progresses found deep neural network usually deep architecture uses least layer learn feature representation given data. representation learning procedure applied discover multiple levels representation higher level abstract representation. shown performance deep neural networks heavily dependent multilevel representation data past years researchers endeavored design variety efﬁcient deep learning algorithms capture characteristics data-generating distribution. among algorithms traditional autoencoders perhaps received research attention conceptual simplicity ease t.r. h.m. chen school information science technology southwest jiaotong university chengdu china. e-mail zyugsu.edu {trli hmchen}swjtu.edu.cn department informatics university south carolina department computer science georgia state department computer science university illinois training inference training efﬁciency. used learn data-generating distribution input data minimizing reconstruction error encoder function decoder function reconstruction error. recently become promising approaches representation learning estimating data-generating distribution. since appearance auto-encoders many variants representation learning algorithms based auto-encoders proposed e.g. sparse auto-encoders denoising auto-encoders higher order contractive auto-encoders variational autoencoders marginalized denoising auto-encoders generalized denoising auto-encoders generative stochastic networks made laplacian auto-encoders adversarial auto-encoders ladder variational auto-encoders auto-encoder-based algorithm minimizing reconstruction error input encoder decoder function common practice feature learning. learned features usually applied subsequent tasks supervised classiﬁcation past years many research works shown reconstruction input encoder decoder function efﬁcient learning feature representation resulting representations also substantially help performance subsequent tasks. general lower value reconstruction error input better feature representation input. ideal situation value reconstruction error equal i.e. input completely reconstructed. paper show reconstruction error input every traditional auto-encoders based algorithm lower bound greater equal important method representation learning minimizing frobenius norm jacobian matrix hidden representation widely used deep learning models. ﬁrst application caes learn locally invariant features minimizing frobenius norm jacobian matrix hidden representation. that many frameworks based minimizing frobenius norm jacobian matrix hidden representation developed computer vision tasks. speciﬁcally developed multimodal feature learning model stacked caes video classiﬁcation. stable features schulz designed two-layer encoder regularized extension previous work caes. geng proposed novel deep supervised contractive neural network image classiﬁcation using idea minimizing frobenius norm jacobian matrix hidden representation. shao introduced enhancement deep feature fusion method rotating machinery fault diagnosis combination daes caes. however demonstrate minimizing frobenius norm jacobian matrix hidden representation deﬁciency learning feature representation. learn robust feature representation minimizing reconstruction error hidden representation also important efﬁcient. idea emphasized popular deep learning algorithms ladder networks target propagation networks order reconstruct hidden representation ladder networks need streams information reconstruct hidden representation used generate clean hidden representation encoder function; utilized reconstruct clean hidden representation combinator function ﬁnal objective function reconstruction errors hidden representation. noted reconstructing hidden representation layer needs information layers makes ladder networks difﬁcult trained layer-wise pre-training strategy. training deep learning model layer-wise manner known unsupervised learning approach many potential advantages. reconstruct hidden representation target propagation networks trained layer-wise manner. nevertheless target propagation networks reconstructing hidden representation decomposed separate targets trapped local optimum. best knowledge reconstructing hidden representation whole training layer-wise manner investigated far. design robust approach feature representation based properties. follow framework layer-wise pre-training consider idea reconstruction hidden representation. propose deep learning model takes advantages corruption reconstruction. model consists separate parts constraints input reconstruction hidden representation constraints part viewed traditional deep learning model auto-encoder variants. reconstruction part viewed explicitly regularizing hidden representation adding additional feedback pre-training stage. simplicity convenience constraints part build model. best results obtained utilizing corruption input hidden representation refer double denoising auto-encoders prove algorithms based traditional auto-encoders reconstruction error input lower lower bound sever guiding principle reconstruction input. also show necessary condition reconstruction input reach ideal state reconstruction hidden representation achieves ideal condition. input corrupted noises also demonstrate reconstruction error corrupted input also lower lower bound. validate minimizing frobenius norm jacobian matrix hidden representation deﬁciency result much worse local optimum value. also show minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobian matrix main reason proposed ddaes always outperform caes. propose approach learning robust feature representations input based evidences. compared existing methods ddaes following advantages ddaes ﬂexible extensible potentially better capability learning invariant robust feature representations. dealing noises inessential features ddaes robust daes. ddaes trained different pre-training strategies optimizing objective function combining separate manner respectively. rest paper organized follows. section introduces basic daes caes. section presents lower bound reconstruction error input necessary condition reconstruction input reach ideal state. section illustrates shown daes extract robust features injecting noises original input implicitly capture data-generating distribution input conditions reconstruction error squared error data continuous-valued gaussian corruption noise contractive auto-encoders extracting locally invariant features hidden representation extract locally invariant features caes penalize sensitivity adding analytic contractive penalty traditional auto-encoders. contractive penalty frobenius norm ﬁrst derivatives encoder function respect input compared daes caes least differences penalty analytic rather stochastic; hyper parameter allows control tradeoff reconstruction robustness. actually optimizing searching algorithm seems likely caes invariant features restricting step lengths small numbers search. lower bound reconstruction error input generally algorithm based traditional autoencoders smaller reconstruction error input better algorithm. ideally value reconstruction error input equal means algorithm completely reconstruct input. however paper prove reconstruction error input lower bound viewed criterion reconstruction input. also illustrate reconstruction hidden representation achieves ideal condition necessary condition reconstruction input reach ideal state. input corrupted noises also demonstrate reconstruction error corrupted input lower bound too. defect caes gives theoretical proof ddaes outstrip caes. section describes proposed ddaes framework. section compares performance ddaes relevant state-ofthe-art representation learning algorithms using various testing datasets. conclusions together studies summarized last section. preliminaries ddaes designed according traditional autoencoders learn feature representation minimizing reconstruction error. ease understanding reveal ddaes starting describe conventional auto-encoder variants notations. denoising auto-encoders extracting robust features reconstruction similar traditional auto-encoders denoising auto-encoders ﬁrstly encoder decoder procedure train one-layer neural network minimizing reconstruction error stack deep neural network trained layers. difference traditional auto-encoders daes daes train neural network corrupted input traditional auto-encoders original input. corrupted input usually obtained conditional distribution injecting noises original input typically widely-used noises simulations gaussian noise masking noise input components extract robust features ﬁrstly maps corrupted input hidden representation encoder function dh×dx connection weight matrix bias vector hidden representation activation function typically logistic sigmoid +e−τ that reversely maps hidden representation back reconstruction input decoder function dx×dh tied weight matrix i.e. bias vector activation function typically either identity sigmoid. finally learns robust features minimizing reconstruction error training x··· xn}. remark theorem summarizes general rule reconstruction input algorithms based traditional auto-encoders. reconstruction error input lower lower bound gives guiding principle reconstructing input. traditional view reconstructing input smaller reconstruction error input better algorithm. ideal situation value reconstruction error input i.e. algorithm completely reconstruct input. however theorem demonstrates ideal value reconstruction error input lower bound greater equal hence compared traditional view theorem gives accurate quantitative description reconstruction error input. remark theorem also provides necessary condition reconstruction input reach ideal state namely reconstruction hidden representation achieves ideal condition. reconstruction hidden representation achieve ideal condition reconstruction input also reach ideal state. nevertheless reconstruction hidden representation achieves ideal state theorem guarantee reconstruction input also obtains lower bound necessary condition present lower bound reconstruction error input rigorous theoretical analysis below. also reveal necessary condition reconstruction input reach ideal state. theorem squared error. clean input clean hidden representation reconstruct themselves proof clean input corresponding clean hidden representation reconstructed input respectively. reconstructed hidden representation. approximate encoder function taylor expansion around lagrange remainder term remark presents relationship reconstruction error input reconstruction error hidden representation well reconstruction error hidden representation objective function cae. remark theorem also main evidence minimizing reconstruction error hidden representation robust feature learning minimizing frobenius norm jacobia matrix hidden representation. since ddaes reconstruction hidden representation objective function caes learn features minimizing frobenius norm jacobia matrix hidden representation conclude ddaes robust feature representation caes. also main reason ddaes always outperform caes. lower bound corrupted input show input corrupted noises reconstruction error corrupted input also lower bound. theorem squared error. noises added original input remark theorem summarizes even though input corrupted noises reconstruction error corrupted input also lower lower bound guiding principle reconstructing corrupted input. however lower bound situation expectation. remark theorem also main evidence minimizing reconstruction error hidden representation robust feature representation minimizing frobenius norm jacobia matrix hidden representation confronted corrupted input. robustness hidden representation reconstruction section theoretically prove minimizing frobenius norm jacobian matrix hidden representation deﬁciency result much worse local optimum value. also show minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrix hidden representation. supplementary material present three examples illustrate them. fact defect appear situation maximizing frobenius norm jacobia matrix. however demonstrate deﬁciency minimizing frobenius norm jacobia matrix. similar results acquired case maximizing frobenius norm jacobia matrix. corresponding hidden representation clean input i.e. reconstructed input encoder function decoder function reconstruction error hidden representation frobenius norm jacobia matrix hidden representation respect input give proof inequation section theoretically show minimizing frobenius norm jacobia matrix hidden representation invalid situations. meanwhile also demonstrate consider three special optimization problems algorithm reaches areas ﬁrst derivatives equal constants; ﬁrst derivatives equal constants; ﬁrst derivatives equal constant frobenius norm jacobia matrix constant. ﬁrstly consider simple situation ﬁrst derivatives equal constants i.e. constant situation frobenius norm jacobia matrix hidden representation constant. means algorithm reaches areas minimizing frobenius norm jacobia matrix losts ability optimum value. algorithm stops early searches random direction even includes much worse direction away optimum value. therefore minimizing frobenius norm invalid insituation. however equation value reconstruction error hidden representation continues decrease. hence ﬁrst derivatives equal constants minimizing reconstruction error hidden representation works continues optimum value. also means reconstruction hidden representation robust minimizing frobenius norm jacobia matrix. second situation ﬁrst derivatives equal constants minimizing frobenius norm jacobia matrix hidden representation seems working. however theoretically show minimizing frobenius norm jacobia matrix encourage obtain much worse local optimum value. case show results ﬁrst derivatives equal constant similar results obtained multiple constants ﬁrst derivatives. current theorem optimum value. ﬁrst derivatives equal constant exists next value minimizing reconstruction error hidden representation work optimum value. third situation ﬁrst derivatives varying frobenius norm jacobia matrix constant similar ﬁrst situation. speciﬁcally encoder function frobenius norm jacobia matrix constant i.e. case also prove minimizing frobenius norm invalid reconstruction hidden representation robust minimizing frobenius norm jacobia matrix. discussion above conclude ﬁrst derivatives equal constants ﬁrst derivatives varying frobenius norm jacobia matrix constant minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrix. main reason ddaes always outperform caes experiments. double denoising auto-encoders shown necessary condition reconstruction input reach ideal state reconstruction hidden representation achieves ideal condition section also show minimizing frobenius norm jacobia matrix much worse local optimum value minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrixas illustrated section therefore paper consider decrease reconstruction error hidden representation better feature representation. idea reconstruction hidden representation daes propose deep learning model takes advantages corruption reconstruction. anticipate proposed model capability learn invariant robust feature representation. ddaes architecture previously stated ddae usually separate parts constraints input reconstruction hidden representation constraints part ddae. fact replace constraints part auto-encoder variant. example replace sparse auto-encoder cae. means ddae ﬂexibility. reconstruction part done ﬁrst corrupting hidden representation according conditional distribution mapping corrupted hidden representation intermediate reconstructed input sg˜h reconstruct hidden representation fig. illustrates schematic representation fig. ddae architecture. sample stochastically corrupted auto-encoder maps hidden representation attempts reconstruct decoding producing reconstruction reconstruction error measured loss meanwhile hidden representation also stochastically corrupted then mapped intermediate reconstructed input attempts example two-layer ddaes. hidden representation ﬁrst-layer ddae taken input secondlayer ddae. classiﬁer layer added stacked two-layer ddaes form multilayer perceptron classiﬁer. training classiﬁer stacked two-layer ddaes ﬁrstly pre-trained greedy layerwise manner. that classiﬁer initialized pre-trained parameters ﬁne-tuned utilizing back-propagation. part optimize separately convenience linear combination constraints part reconstruction part objective function ddae-com. parameters trained minimize reconstruction error training x··· xn}. objective function optimized stochastic gradient descent becomes reconstruction error reconstrucdae tion error hidden representation mathematical expectation obtained conditional distribution hyper parameter controls tradeoff constraints part reconstruction part. equation conclude ddae regarded general expression extends dae. hyper parameter ddae dae. also means special case proposed method i.e. ddae generalization basic algorithm. procedure. note reconstruction error hidden representation instead error intermediate reconstructed input original input complicated expressions combination intermediate reconstructed input sg˜h almost equal reconstruction input bx). result similar effect constraints part l)). two-layer ddaes. usually ddae used stack multiple layers form deep ddaes architecture output ddae used input next ddae. classiﬁer layer built stacked deep ddaes architecture form multi-layer classiﬁer. training multilayer classiﬁer stacked deep ddaes architecture ﬁrstly pre-trained greedy layer-wise manner. subsequently multi-layer classiﬁer initialized pre-trained parameters ﬁne-tuned utilizing back-propagation. training sample selected mini-batch reconstruction error selected sample. subsequently ddae-sep updates parameters optimized ﬁrst objective function minimizes second objective function dealing noises inessential features backgrounds robust daes. daes corrupting input reconstructing makes daes learn robust features. proposed model corrupts reconstructs input also thing hidden representation. feature representation corrupting hidden representation reconstructing partially reduce negative effects noises propagated input inessential features backgrounds daes deal noises inessential features. therefore compared daes proposed model robust deal noises inessential features. corresponding hidden representation updated parameters corresponding hidden representation reconstruction error hidden representation parameters updated objective functions ddae-sep train next mini-batch repeat procedure stopping criteria met. details implement ddae-com ddae-sep please refer algorithms properties ddaes please note corrupted hidden representation instead clean hidden representation reconstruct clean hidden representation equations main reasons although ddaes manifold learning extract robust features guarantee noises eliminated propagate hidden representation. even noises eliminated ddaes learn inessential features backgrounds. also reasons corrupting reconstructing hidden representation experiments evaluate ddaes twelve datasets nine image recognition datasets human genome sequence datasets compare performance competitive state-of-the-art models. several important parameters also experimentally evaluated. description dataset twelve datasets selected machine learning repository evaluate performance ddaes algorithms. datasets utilize -fold cross validation evaluate competing algorithms give average error rates runs. note datasets tested recent work deep support vector machine table summarizes basic information twelve datasets. nine image recognition datasets consist well-known mnist digits classiﬁcation problem eight benchmark datasets. mnist digits come gray-scale images handwritten digits. eight benchmark datasets consist tenclass problems modiﬁed mnist digits three experimental veriﬁcation section proved reconstruction hidden representation reach ideal situation reconstruction input obtain ideal value. section also shown minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrix hidden representation. however question still needs solved experimentally validate points? reconstruction hidden representation reconstruction input show robustness reconstruction hidden representation reconstruction input evaluate classiﬁcation performance minimizing reconstruction error hidden representation compare results minimizing reconstruction error input i.e. stacked fig. shows classiﬁcation error rates rehr nine image recognition datasets different layers. sake fairness results derived work vincent shown fig. rehr layers almost always gets better results nine image recognition datasets. fig. presents example images corresponding ﬁlters learned models stacked stacked daes stacked caes stacked rehr stacked ddaes. ﬁgure shows features learned ﬁrst layer models rect bg-img-rot datasets. compared rehr seems learn much more test data. two-class problems shape classiﬁcation. ten-class problems variants mnist digits smaller subset mnist digits random angle rotation digits random noise background digits random image background digits rotation image background three two-class problems shape classiﬁcation tasks white tall wide rectangles black background tall wide rectangular image overlayed different background images convex concave shape data sets also used works larochelle rifai vincent divided three parts training pre-training ﬁne-tuning validation choice hyper-parameters testing report result. details image recognition datasets listed table human genome sequence datasets standard benchmark fruitﬂy.org predicting gene splicing sites human genome sequences. ﬁrst dataset acceptor locations containing sequences features. second data donor locations including sequences features. acceptor dataset intron following exon. donor dataset exon following intron sequences consist four letters datasets need ﬁrstly transform four letters four fig. classiﬁcation error rates nine benchmark classiﬁcation tasks. results based minimizing reconstruction error hidden representation different layers. results sae- based minimizing reconstruction error input result comes vincent reconstruction hidden representation minimizing frobenius norm jacobia matrix conduct comparison experiments show robustness hidden representation reconstruction fig. example images corresponding ﬁlters learned different models rect bg-img-rot datasets. example images; filters learned sae; filters learned cae; filters learned dae; results rehr; results ddae. minimizing frobenius norm jacobia matrix compare results using reconstruction hidden representation feature representation minimizing frobenius norm jacobia matrix. also show results adding reconstruction input reconstruction hidden representation minimizing frobenius norm jacobia matrix. second situation illustrate comparison results ddaes caes practice. ﬁrst comparison experiment mnist testing dataset. shown table classiﬁcation error rate minimizing reconstruction error hidden representation feature representation. however classiﬁcation error rate minimizing frobenius norm jacobia matrix hidden representation. non-convergence problem appear minimizing frobenius norm jacobia matrix hidden representation feature representation. second comparison experiment show comparison results ddaes caes. tables ddaes always outperform caes. therefore conclude minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrix hidden representation also ddaes robust feature representation caes. parameter evaluation order illustrate effectiveness hidden representation reconstruction evaluate inﬂuence varying range hyper parameters fact difﬁcult optimal combination hyper parameters deep network. fortunately many researchers proposed various rules choosing hyper-parameters deep networks experiments refer strategies used initialize parameters random values hyper parameters perform grid search range hyper parameter utilizing mini-batch stochastic gradient descent. show inﬂuence hyper parameter ddae-com controls tradeoff constraints part reconstruction part compare ddaes daes adjustment comparison bg-img-rot testing dataset. hyper parameters models present classiﬁcation error rates bg-img-rot dataset shown fig. clearly ddaes perform better daes hyper parameter located proper scope. also contrast ddaes daes increasing number hidden layers number hidden units layer show inﬂuence double corruption. fig. shows comparative classiﬁcation error rates bg-img-rot dataset. results ddaes marked blue fig. illustrate increase number hidden layers classiﬁcation error rates gradually descend. could ddaes capability capture underlying data-generating distributions input hidden representation daes capture distribution input. fig. also shows ddaes outperform daes especially number hidden units layer low. much easy ddaes capture underlying data-generating distribution hidden representation number hidden representation layer low. comparisons state-of-the-art results show robustness hidden representation reconstruction feature representation compare performance state-of-the-art models fig. experimental results bg-img-rot dataset. classiﬁcation error rates varying hyper-parameter ddae-com. dae-b- hidden layers stacked daes masking noise result obtained vincent comparison ddaes daes increasing number hidden layers number hidden units layer. ﬁrst test classiﬁcation performance ddaes twelve datasets. utilizing ddae-com algorithm compare ddaes model stacked deep neural networks stacked deepsvms stacked caes stacked daes tied weights sigmoid activation function squared error reconstruction loss networks caes daes ddaes. ddaes model employ -layer architecture frequently used structure. number features small dataset corruption ﬁrst second layers daes ddaes input data. table presents classiﬁcation error rates hidden layers stacked ddaes masking noise compared model stacked deep neural networks layers stacked deepsvms hidden layers stacked caes hidden layers stacked daes masking noise general networks minimizing reconstruction error hidden representation perform better networks without constraint. possible ddaes reconstruction input hidden representation learn underlying data-generating distributions input hidden representation. digit image recognition testing small dataset classiﬁcation problem compare ddaes several stateof-the-art models unsupervised feature extraction models kernel stacked deep belief networks stacked deep boltzmann machines stacked stacked daes stacked caes stacked rectiﬁed factor networks ladder networks models also adopt tied weights sigmoid activation function encoder decoder cross-entropy reconstruction loss except dbns dbms rfns. dbns dbms optimize parameters using contrastive divergence rfns expectation-maximization algorithm. stochastic gradient descent applied optimization method models. classiﬁcation results ddaes models mnist dataset listed table using zeromasking corruption noises ddae-com algorithm ddaes layers achieve error rate traditional daes using gaussian corruption noises test error -layer ddaes reduces ddaecom algorithm ddae-sep algorithm. well-known trick technique dropout test error -layer ddaes reduce training ddae-com algorithm training ddae-sep algorithm. experiments -layer layer architecture tested. hyperparameters selected according performance validation set. dropout human genome sequence classiﬁcation order demonstrate effectiveness ddaes evaluate performance human genome sequence datasets. sequence datasets also -fold cross validation present average error rates runs. fig. illustrates performance stacked saes stacked rehr stacked caes stacked ddas masking noise stacked ddaes masking noise note models hidden layers. ddae-b- model obtain classiﬁcation error rates acceptor dataset donor dataset best results compared models experiments. methods rehr layers svmrbf ddae-com layers rfns ddae-com layers ddae-sep layers ddae-sep layers dropout ﬁnetuning dropout ﬁnetuning ddae-com layers dropout ﬁnetuning ddae-sep layers dropout ﬁnetuning using ddae-com algorithm also test model eight deep learning benchmark datasets layers table reports resulting classiﬁcation performance model together performance svms kernel hidden layers stacked deep belief networks hidden layers stacked daes masking noise hidden layers stacked caes stacked rectiﬁed factor networks table model works remarkably well datasets. better equivalent state-of-the-art models data sets three layers datasets four layers. pointed give corrupting noise level input hidden representation experiment. conclusion future work paper demonstrated reconstruction error input lower bound minimizing frobenius norm jacobia matrix hidden representation deﬁciency encourage much worse local optimum value. based evidences deep neural network ddaes unsupervised representation learning proposed using idea learning invariant robust features small change input hidden representation. idea implemented minimizing reconstruction error injecting noises input hidden representation. shown model ﬂexible extendible. also demonstrated minimizing reconstruction error hidden representation feature representation robust minimizing frobenius norm jacobia matrix hidden representation. comprehensive experiments indicate ddaes achieve performance superior existing state-of-the-art models twelve datasets human genome sequence datasets. nine image recognition datasets ddaes better equivalent state-of-the-art models. since reconstruction hidden representation always helps auto-encoder perform better competes improves upon representations learning design useful constraint operations hidden representation development efﬁcient representation learning model would interesting extension studies. socher pennington huang manning semi-supervised recursive autoencoders predicting sentiment distributions proceedings conference empirical methods natural language processing trends machine learning vol. xiang gilmore tang madabhushi stacked sparse autoencoder nuclei detection breast cancer histopathology images ieee transactions medical imaging vol. vincent larochelle bengio p.-a. manzagol extracting composing robust features denoising autoencoders international conference machine learning rifai mesnil vincent muller bengio dauphin glorot higher order contractive auto-encoder european conference machine learning knowledge discovery databases makhzani shlens jaitly goodfellow frey adversarial autoencoders arxiv preprint arxiv. sønderby raiko maaløe sønderby winther ladder variational autoencoders advances neural information processing systems rifai vincent muller glorot bengio contractive auto-encoders explicit invariance feature extraction international conference machine learning feng zhou multimodal video classiﬁcation stacked contractive autoencoders signal processing vol. wang tian zhang when ensemble learning meets deep learning deep support vector machine classiﬁcation knowledge-based systems vol. larochelle erhan courville bergstra bengio empirical evaluation deep architectures problems many factors variation international conference machine learning vincent larochelle lajoie bengio p.-a. manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research vol. bergstra bardenet bengio k´egl algorithms hyper-parameter optimization advances neural information processing systems srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research vol.", "year": 2017}