{"title": "SparseMAP: Differentiable Sparse Structured Inference", "tag": ["stat.ML", "cs.CL", "cs.LG", "68T50", "I.2.6; I.2.6"], "abstract": "Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP, a new method for sparse structured inference, together with corresponding loss functions. SparseMAP inference is able to automatically select only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, hence it is applicable even to problems where marginal inference is intractable, such as linear assignment. Moreover, thanks to the solution sparsity, gradient backpropagation is efficient regardless of the structure. SparseMAP thus enables us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.", "text": "structured prediction requires searching combinatorial number structures. tackle introduce sparsemap method sparse structured inference together corresponding loss functions. sparsemap inference able automatically select global structures situated inference picks single structure marginal inference assigns probability mass structures including implausible ones. importantly sparsemap computed using calls oracle hence applicable even problems marginal inference intractable linear assignment. moreover thanks solution sparsity gradient backpropagation efﬁcient regardless structure. sparsemap thus enables augment deep neural networks generic sparse structured hidden layers. experiments dependency parsing natural language inference reveal competitive accuracy improved interpretability ability capture natural language ambiguities attractive pipeline systems. introduction structured prediction involves manipulation discrete combinatorial structures trees sequences alignments structures arise naturally desired outputs machine learning tasks intermediate representations deep pipelines. core challenge structured prediction however prohibitively large possible structures. inference space generally difﬁcult often sidestepped greedy search factorization assumptions continuous relaxations figure left unstructured case softmax sparsemax interpreted regularized differentiable approximations; softmax returns dense solutions sparsemax favors sparse ones. right work extend view structured inference consists optimizing polytope convex hull possible structures introduce sparsemap structured extension sparsemax situated inference yields single structure marginal inference returns dense combination structures. sparsity structured representations. namely seek solutions explicitly expressed combination small enumerable global structures. framework departs common inference strategies structured prediction maximum posteriori inference returns highest-scoring structure marginal inference yields dense probability distribution structures. neither strategies fully satisfactory latent structure models marginal inference appealing since represent uncertainty unlike inference continuous differentiable hence amenable structured hidden layers neural networks however several limitations. first useful problems tractable marginal inference e.g. linear assignment even marginal inference available case-bycase derivation backward pass needed sometimes producing fairly complicated algorithms e.g. second-order expectation semirings finally marginal inference dense assigns non-zero probabilities r×k. denote columns matrix extension slice columns denoted indices denote canonical simplex indicator function predicate otherwise preliminaries regularized operators softmax sparsemax ties unique solution peaking index highest value ties set-valued. even assuming ties piecewise constant thus ill-suited direct within neural networks e.g. attention mechanism. instead common softmax continuous differentiable approximation seen entropy-regularized softmax sparsemax continuous differentiable almost everywhere; however sparsemax encourages sparsity outputs. corresponds euclidean projection onto simplex likely boundary magnitude increases. mechanisms well variants different penalties successfully used attention mechanisms mapping score vector d-dimensional normalized discrete probability distribution small choices. relationship softmax sparsemax illustrated figure sits foundation sparsemap. propose sparsemap framework sparse structured inference main idea illustrated figure sparsemap twofold generalization ﬁrst structured extension sparsemax transformation second continuous sparse relaxation inference. yields single structure marginal inference yields dense distribution structures. contrast sparsemap solutions sparse combinations small number often-overlapping structures. show compute sparsemap effectively requiring solver subroutine exploiting problem’s sparsity quadratic curvature. noticeably oracle arbitrary solver e.g. hungarian algorithm linear assignment permits tackling problems marginal inference intractable. derive expressions gradient backpropagation sparsemap inference which unlike differentiable almost everywhere backward pass fully general efﬁcient thanks sparsity solutions reusing quantities computed forward pass. introduce novel sparsemap loss structured prediction placing family loss functions generalizes structured losses inheriting desirable properties sparsemap inference sparsemap loss gradients computed efﬁciently provided access inference. experiments demonstrate sparsemap useful predicting structured outputs well learning latent structured representations. dependency parsing structured output networks trained sparsemap loss yield accurate models sparse interpretable predictions adapting ambiguity test examples. natural language inference learn latent structured alignments obtaining good predictive performance well useful natural visualizations concentrated small number structures. instances e.g. solvable dynamic programming entropic term regularizes toward uniform distributions resulting strictly dense solutions like case softmax interesting types structures experiments described section include following. sequence tagging. consider sequence items assigned possible tags. case global structure joint assignment tags matrix nm-by-mn–dimensional columns indicating assigned variable global structure nm-bymn–dimensional encoding transitions consecutive tags i.e. viterbi algorithm provides inference forwardbackward provides marginal inference non-projective dependency parsing. consider sentence length here structure dependency tree rooted spanning tree possible arcs column encodes tree assigning arcs. empty known arborescence polytope inference performed maximal arborescence algorithms matrixtree theorem provides perform marginal inference linear assignment. consider one-to-one matching sets nodes. global structure n-permutation column seen ﬂattening corresponding permutation matrix. again empty. birkhoff polytope inference performed e.g. hungarian algorithm jonker-volgenant algorithm noticeably marginal inference known p-complete makes open problem matchings latent variables. armed parallel structured inference regularized operators described ready introduce sparsemap novel inference optimization problem returns sparse solutions. length-n sequence spanning trees nodes oneto-one alignments sets. still write optimization problems maxd impractical enumerate possible structures turn specify scores structure instead structured problems often parametrized structured log-potentials rk×d matrix speciﬁes structure problem lower-dimensional parameter vector i.e. example factor graph algorithm considered prior work solving approximations marginal inference problem step must solve linearized subproblem. denote sparsemap objective equation mate solution. intuitively step seek high-scoring structure penalizing sharing variables structures already selected. vanilla simply adds structure active every iteration. pairwise away-step variants trade moving direction structure moving away already-selected structures. active method. importantly sparsemap problem equation quadratic curvature general algorithms optimally leverage. reason consider active method constrained proposed nocedal wright context structured prediction variation algorithm proposed quadratic subproblems algorithm active algorithm iteration updates estimate solution support adding removing constraint to/from active set; solves karush–kuhn–tucker system relaxed restricted current support. shown nocedal wright ﬁnite convergence meaning give exact solution enough iterations rather approximation converges optimum—this allows among things capturing sparsity pattern optimal solution. compare convergence aforementioned solvers dependency parsing instance random potentials. results figure show active algorithm substantially outperforms variants terms objective value well solution sparsity suggesting quadratic curvature sparsemap makes problem solvable high degree accuracy. thus active solver remainder paper. quadratic penalty replaces entropic penalty marginal inference pushes solutions strict interior marginal polytope. consequence sparsemap favors sparse solutions faces marginal polytope illustrated figure structured prediction problems mentioned section sparsemap would able return example sparse combination sequence labelings parse trees matchings. moreover strongly convex regularization ensures sparsemap unique solution differentiable almost everywhere see. turn question solving optimization problem equation although sparsemap quadratic program polytope note obvious solve efﬁciently exponentially large number vertices possibly constraints. even describing problem standard form infeasible preventing direct application e.g. generic differentiable solver amos kolter instead focus sparsemap solvers involve sequence problems subroutine— makes sparsemap widely applicable given availability implementations various structures. discuss methods based conditional gradient algorithm another based active method quadratic programming. provide full description methods appendix order sparsemap neural network layer trained backpropagation must compute products sparsemap jacobian vector computing jacobian optimization problem active research topic known argmin differentiation generally difﬁcult. fortunately show next argmin differentiation always easy efﬁcient case sparsemap. proof given appendix relies conditions sparsemap importantly zero outside support solution computing jacobian requires columns corresponding structures active set. moreover using active algorithm discussed matrix readily available byproduct forward pass. backward pass therefore computed approach gradient computation draws efﬁciency solution sparsity depend type structure considered. contrasted related lines research. ﬁrst unrolling iterative inference algorithms instance belief propagation stochastic gradient descent backward pass complexity scales number optimization iterations. second employed involves backpropagating marginal inference using second-order expectation semirings applicable situations marginal inference performed dynamic programming latter approach results backward pass complexity matching forward pass. another advantage approach neither forward backward passes involve logarithms exponentiations log-domain classes avoiding slowdown stability issues normally incurred. efﬁcient algorithms derived hand switch gears deﬁning sparsemap loss function. structured output prediction models typically trained minimizing structured loss measuring discrepancy between desired structure prediction induced log-potentials provide general family structured prediction losses make newly proposed sparsemap loss arise natural case. below leads natural deﬁning sparsemap losses plugging following equation sparsemap loss margin sparsemap well-known computing subgradients structured perceptron structured losses requires inference space deﬁned loss gradient requires marginal inference. similarly subgradients sparsemap loss computed sparsemap inference turn solely depends oracles. next proposition states properties structured fenchel-young losses including general connection loss corresponding inference method. table unlabeled attachment accuracy scores dependency parsing using bi-lstm model sparsemap margin version m-sparsemap produce best parser datasets. context include scores conll udpipe baseline trained conditions proof given appendix property suggests minimizing pushes models toward objective value true label. property shows compute subgradients provided access inference output dure described section makes sparsemap losses promising structured prediction. property suggests strength penalty adjusted simply scaling finally remark stronglyconvex seen smoothed perceptron loss; smoothed losses explored shalevshwartz zhang section experimentally validate sparsemap natural language processing applications illustrating main cases presented structured output prediction sparsemap loss structured hidden layers models implemented using dynet library table test accuracy scores natural language inference structured unstructured variants esim. parentheses percentage pairs words nonzero alignment scores. shared task isolate effect loss provided gold tokenization partof-speech tags. follow closely bidirectional lstm arc-factored parser kiperwasser goldberg using model conﬁguration; exception using externally pretrained embeddings. parameters trained using adam tuning experiment languages diverse terms family terms amount training data test results indicate sparsemap losses outperform losses languages considered. suggests sparsemap good middle ground map-based marginalbased losses terms smoothness gradient sparsity. moreover illustrated figure sparsemap loss encourages sparse predictions models converge towards sparser solutions train yielding ambiguous arcs. conﬁdent sparsemap predict single tree. otherwise small candidate parses returned easily visualized often indicating genuine linguistic ambiguities exempliﬁed figure property sparsemap valuable pipeline systems e.g. output dependency parser input downstream application error propagation diminished cases highest-scoring tree incorrect unlike k-best heuristics sparsemap dynamically adjusts output sparsity desirable realistic data instances easy. evaluate sparsemap losses commonly used structured losses. task focus non-projective dependency parsing structured output task consisting predicting directed tree grammatical dependencies words sentence annotated universal dependency data used conll section demonstrate sparsemap inferring latent structure large-scale deep neural networks. focus task natural language inference deﬁned classiﬁcation problem deciding given sentences whether premise entails hypothesis contradicts neutral respect figure example ambiguous parses english validation set. sparsemap selects small number candidate parses differing small number ambiguous dependency arcs. cases desired gold parse among selected trees highest-scoring one. figure distribution tree sparsity sparsity sparsemap solutions training chinese dataset. shown respectively number trees average number parents word non-zero probability. consider following structured replacements independent row-wise column-wise softmaxes sequential alignment. model alignment sequence tagging instance length possible tags corresponding words hypothesis. transition scores enable model capture continuity monotonicity alignments parametrize transitioning word binning distance groups less− more}. similarly parametrize initial alignment using bins more} ﬁnal alignment less−} allowing model express whether alignment starts align applying method direction different transition scores overall sequential alignment requires learning additional scalar parameters. matching alignment. seek symmetrical alignment directions simultaneously. cast alignment problem ﬁnding maximal weight bipartite matching. recall solution found hungarian algorithm maximal matchings represented permutation matrices words remain unaligned. sparsemap returns weighted average maximal matchings. method requires additional learned parameters. evaluate models alongside softmax baseline snli multinli datasets. models trained learning rate decay epochs validation accuracy best seen. tune learning rate figure latent alignments example snli validation correctly predicted neutral compared models. premise y-axis hypothesis x-axis. columns bottom rows matching alignment mechanism yields symmetrical alignment thus shown once. softmax yields dense alignment structures selected sequential alignment overlayed paths; selected matchings displayed right. best model either end. results table show structured alignments competitive softmax terms accuracy orders magnitude sparser. sparsity allows produce global alignment structures interpretable illustrated figure interestingly observe computational advantages sparsity. despite overhead memory copying training validation latent structure models take roughly time softmax become faster models grow certain. sake comparison report slow-down structured attention networks marginal inference. related work structured attention networks. structured hidden layers proposed works lapata take advantage tractability marginal inference certain structured models derive specialized backward passes. contrast approach general easier sparsemap forward pass requires inference backward pass efﬁciently computed based forward pass results. moreover unlike marginal inference sparsemap yields sparse solutions appealing property statistically computationally visually. k-best inference. returns small structures sparsemap brings mind k-best inference often used pipeline systems increasing recall handling uncertainty k-best inference solved tree-structured factor graphs certain types structures specialized algorithms well approximated general time proportional calls inference although terms inference. appeal sparsemap support size adapts dynamically instance rather ﬁxed priori. furthermore sparsemap yields sparse probability distribution k-best reveal posterior structures. regularized inference. ravikumar meshi martins proposed perturbations penalties various related ways goal solving lp-map approximate inference graphical models. contrast goal work sparse structured prediction considered aforementioned work. nevertheless formulations work share properties sparsemap; exploring connections interesting avenue future work. introduced framework sparse structured inference sparsemap along corresponding loss function. proposed efﬁcient ways compute forward backward passes sparsemap. experimental results illustrate cases sparse inference well-suited. structured prediction sparsemap loss leads strong models make sparse interpretable predictions good tasks local ambiguities common like many natural language processing tasks. structured hidden layers demonstrated sparsemap leads strong interpretable networks trained end-to-end. modular design sparsemap applied readily structured problem inference available including combinatorial problems linear assignment.", "year": 2018}