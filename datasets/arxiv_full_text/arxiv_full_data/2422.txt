{"title": "A Robust Independence Test for Constraint-Based Learning of Causal  Structure", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Constraint-based (CB) learning is a formalism for learning a causal network with a database D by performing a series of conditional-independence tests to infer structural information. This paper considers a new test of independence that combines ideas from Bayesian learning, Bayesian network inference, and classical hypothesis testing to produce a more reliable and robust test. The new test can be calculated in the same asymptotic time and space required for the standard tests such as the chi-squared test, but it allows the specification of a prior distribution over parameters and can be used when the database is incomplete. We prove that the test is correct, and we demonstrate empirically that, when used with a CB causal discovery algorithm with noninformative priors, it recovers structural features more reliably and it produces networks with smaller KL-Divergence, especially as the number of nodes increases or the number of records decreases. Another benefit is the dramatic reduction in the probability that a CB algorithm will stall during the search, providing a remedy for an annoying problem plaguing CB learning when the database is small.", "text": "paper considers bines ideas bayesian network inference testing bust test independence based learning method produces used test independence statistics. asymptotic calculate standard lows specification parameters database oretical synthetic fits empirically standard greedy bayesian even used noninformative priors results better recovery tures produces kl-divergence nodes increases creases. duction rithm stall search remedy annoying learning bayesian disadvantages relatively defined stopping bitrary unstable search cascading many errors present approaches ground knowledge forbidden forced added advantage rate users' probabilities eters network. dealing incorporating climbing approaches serious drawback fact structures several fits bayesian investigated performing space equivalence recently kocka castelo space dags using search operators consider equivalence class. gorithms based search uses resulting second-stage particular temporal algorithm dering nodes algorithm requires meek algorithm good starting space essential took opposite learning pattern independence greedy bayesian containing vnj· general denote objects face notation denote singletons possible. case symbols case symbols random variable denote range variable notation independent cooper suggested part approximate rithm. bayesian independence constructing according score together approach paper. technique constructed allowing contributions develop culate smoothed calculated tion priors space requirements contingency theoretical tion demonstrate cally improves using nxyz) chi-squared demonstrate number records algorithm space dags. find-independence-graph later search turn lead future decisions incorrect sets potential learned lead incorrectly reasons independence constraint-based methods missing defines joint probability lent model averaging thus bayesian expected independence perform classical hypothesis call \"pseudo-bayesian\" algorithm hybrid-it setx however less trivial ability worst case marginalization uz}. even calculation feasible perform bayesian table inner loop discovery network. however ered onto particular imposed obvious gency table gives information independence lowing theorems procedure pendencies alter outcome perfect std-it. second tion accomplished incomplete methods. work inference eral; however ments small particular informative priors improves dence tests data sets. section empirically. final experimental algorithm periments sets c<ijk criterion weighting variables except explicitly significance code used experiments library sion support experiments range number records ntrials ntrials results shown table columns labelled confi­ column indi­ dence intervals. cates recorded lower kl-divergence. results better however creases sometimes measurements level. high outperforms structural features number records gins make significant interesting highest measured errors adjacencies much greater average ish. ironic small chi-square non-smoothed dependence relations turn causes algorithm network arations later dense structure. algorithm work increase number conditioning result tractable reasonable time. exit whenever becomes integer maximum time elapsed. facts allowed cutoff time tied longer reliable algorithm. would unreasonable expect take times longer used much conservative greater i.e. procedure regimes majority comparisons nerable divergence fact selected reason favored standard less naive experiment separately tuned isons. thus happened benefited could explain results. values value began approach ever values overall graphs decreased vergence. tuned optimum value value achieved maximum. experiment configuration shown table shifting optimum significance demonstrated test used along empirically dence tests. demonstrated using consistently decreases networks structural tures accurately probability improvements outperform techniques number nodes increases. bayesian method induction probabilistic networks data. machine learning denver dash marek druzdzel. hybrid anytime algorithm construction sparse data. conference uncertainty {uai-) pages francisco morgan kaufmann publishers. marek druzdzel. smile structural ference learning engine genie devel­ opment environment graphical decision-theoretic models. proceedings ference artificial orlando friedman iftach nachman dana pe'er. learning bayesian network structure massive datasets \"sparse candidate\" algorithm. pro­ ceedings fifteenth annual conference certainty francisco publishers.", "year": 2012}