{"title": "Convergence of Online Mirror Descent Algorithms", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "In this paper we consider online mirror descent (OMD) algorithms, a class of scalable online learning algorithms exploiting data geometric structures through mirror maps. Necessary and sufficient conditions are presented in terms of the step size sequence $\\{\\eta_t\\}_{t}$ for the convergence of an OMD algorithm with respect to the expected Bregman distance induced by the mirror map. The condition is $\\lim_{t\\to\\infty}\\eta_t=0, \\sum_{t=1}^{\\infty}\\eta_t=\\infty$ in the case of positive variances. It is reduced to $\\sum_{t=1}^{\\infty}\\eta_t=\\infty$ in the case of zero variances for which the linear convergence may be achieved by taking a constant step size sequence. A sufficient condition on the almost sure convergence is also given. We establish tight error bounds under mild conditions on the mirror map, the loss function, and the regularizer. Our results are achieved by some novel analysis on the one-step progress of the OMD algorithm using smoothness and strong convexity of the mirror map and the loss function.", "text": "relaxing hilbert space structure using mirror capture geometric properties data banach space paper consider endowed norm might non-euclidean norm allowing capture non-euclidean geometric structures data introduce mirror descent online mirror descent algorithms assume mirror fr´echet diﬀerentiable strongly given diﬀerentiable convex objective function mirror descent algorithm approximates minimizer sequence {wt}t∈n deﬁned initial vector gradient descent method terms gradient {ηt} sequence positive numbers called step size sequence. gradient descent performed dual primal space since well-deﬁned invertible strong convexity useful instantiations mirror include choice p-norm divergence deﬁned sample t-th iteration mirror descent compute gradient respect variable leads online mirror descent algorithm extends classical online gradient descent algorithm replacing strongly smooth). lψ-strongly smooth examples strongly smooth mirror maps include mirror parameters deﬁned mirror plays important role mirror descent method speciﬁc choice gives convergence bounds logarithmic dependence dimension strongly convex norm takes p-norm norm equivalence following result example proved section denote transpose theorem assume supx∈x covariance matrix positive deﬁnite. consider algorithm denote either p-norm divergence strongly risk mirror necessary conditions shall assume continuous satisﬁes following incremental condition inﬁnity. deﬁnition satisﬁes incremental condition inﬁnity exists constant theorem assume constant l-strongly smooth almost every suppose pair satisﬁes around convex function satisfying step size sequence satisﬁes condition limt→∞ almost surely. subsection state explicit results convergence algorithm associated regularized loss function norm loss function lipschitz continuous derivative. common examples loss functions include least squares loss logistic loss log) -norm hinge loss huber loss deﬁned regularized loss function λ)-strongly smooth every objective function also λ)-strongly smooth λ-strongly convex. conclusion theorem replaced holds algorithm either p-norm divergence strongly smooth mirror map. main diﬃculty general mirror lack analysis one-step progress carried exploiting hilbert space structure special linearity caused least squares loss function. overcome proposition assume ∇w]∗] strongly smooth. assume also satisﬁes incremental condition inﬁnity. limt→∞ ez...zt−] continuous step size sequence satisﬁes proposition assume constant l-strongly smooth almost every suppose pair satisﬁes around convex function satisfying sequence satisﬁes limt→∞ ez...zt− proof theorem necessity. assumption l-strongly smooth almost every implies l-strong smoothness observe estimate derived proof proposition valid -strong smoothness σψ-strong convexity hence lemma ˜xt}t∈n sequences nonnegative random variables {ft}t∈n sequence random variable sets every suppose almost surely every sequence ˜xt} converges nonnegative random variable almost surely. proof theorem follow proof proposition apply since wt∇f implies know also stochastic process ˜xt}t≥t supermartingale. supermartingale convergence theorem lemma know sequence ˜xt}t≥t converges non-negative random variable almost surely. according fatou’s lemma limit limt→∞ proved assumption ∇w]∗] theorem assumption x|x∗] theorem theorem know replace statement holds true constant taken λminb statement norm square notice ﬁrst strong convexity know statement holds true.", "year": 2018}