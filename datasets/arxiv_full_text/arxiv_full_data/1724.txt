{"title": "Learning Multi-Relational Semantics Using Neural-Embedding Models", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "In this paper we present a unified framework for modeling multi-relational representations, scoring, and learning, and conduct an empirical study of several recent multi-relational embedding models under the framework. We investigate the different choices of relation operators based on linear and bilinear transformations, and also the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources. Our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on Freebase.", "text": "real-world entities often connected relations forming multirelational data. modeling multi-relational data important many research areas natural language processing biological data mining prior work multi-relational learning categorized three categories statistical relational learning markovlogic networks directly encode multi-relational graphs using probabilistic models; path ranking methods explicitly explore large relational feature space relations random walk; embedding-based models embed multi-relational knowledge low-dimensional representations entities relations tensor/matrix factorization bayesian clustering framework neural networks work focuses study neural-embedding models representations learned neural network architecture. shown powerful tools multi-relational learning inference high scalability strong generalization abilities. number techniques recently proposed learn entity relation representations using neural networks represent entities low-dimensional vectors represent relations operators combine representations entities. main difference among techniques lies parametrization relation operators. instance given entity vectors model neural tensor network represents relation bilinear tensor operator linear matrix operator. model transe hand represents relation single vector linearly interacts entity vectors. models report promising performance predicting unseen relationships knowledge bases. however directly compared terms different choices relation operators resulting effectiveness. neither design entity representations recent studies carefully explored. example ﬁrst shows beneﬁts representing entities average word vectors initializing word vectors pre-trained vectors large text corpora. idea promising pre-trained vectors tend capture syntactic semantic information natural language assist better generalization entity embeddings. however many real-world entities expressed non-compositional phrases meaning cannot composed constituent words. therefore averaging word vectors provide appropriate representation entities. paper examine compare different types relation operators entity vector representations general framework multi-relational learning. speciﬁcally derive several recently proposed embedding models including transe variants framework. empirically evaluate performance knowledge base completion task using various real-world datasets controlled experimental setting present several interesting ﬁndings. first models fewer parameters tend better complex models terms performance scalability. second bilinear operator plays important role capturing entity interactions. third model complexity multiplicative operations superior additive operations modeling relations. finally initializing entity vectors pre-trained phrase vectors signiﬁcantly boost performance whereas representing entity vectors average word vectors initialized pre-trained vectors hurt performance. ﬁndings inspired design simple knowledge base embedding model signiﬁcantly outperforms existing models predicting unseen relationships top- accuracy evaluated freebase. existing neural embedding models multi-relational learning derived general framework. input relation triplet describing certain relation output scalar measuring validity relationship. input entity represented high-dimensional sparse vector ﬁrst neural network layer projects input vectors dimensional vectors second layer projects vectors real value comparison relationspeciﬁc operator formally denote input entity ﬁrst layer neural network parameter. scoring function relation triplet written many choices form scoring function available. existing scorr bilinear functions literature uniﬁed based basic linear transformation transformation parametrized respectively. table summarize several popular scoring functions literature relation triplet reformulated terms functions. denote entity vectors. denote rm×n matrix vector parameters denote rn×n rn×n×m matrix tensor parameters linear transformation identity matrix. additional parameter bilinear transformation relation scoring function transe derived ||ye vr|| general framework relationship modeling also applies recent deep-structured semantic model learns relevance single relation pair word sequences. framework applies using multiple neural network layers project entities using relation-independent scoring function cosine scoring function special case neural network parameters models discussed learned minimizing margin-based ranking objective encourages scores positive relationships higher scores negative relationships usually positive triplets observed data. given positive triplets construct negative triplets corrupting either relation arguments training objective minimize margin-based ranking loss datasets evaluation metrics used wordnet freebase datasets introduced contains triplets entities relations consists triplets entities relations. also consider subset containing frequent relations results triplets entities relations. link prediction prediction task test triplet entity treated target entity predicted turn. scores computed correct entity corrupted entities dictionary ranked descending order. consider mean reciprocal rank hits mean average precision evaluation metrics. implementation details models implemented using gpu. training implemented using mini-batch stochastic gradient descent adagrad gradient step sampled positive triplet negative triplets corrupted subject entity corrupted object entity. entity vectors renormalized unit length gradient step relation parameters used standard regularization. models number minibatches dimensionality entity vector regularization parameter number training epochs fbk- learning rate initially adapted training adagrad. examine embedding models decreasing order complexity tensor slices bilinear+linear tensor slice without non-linear layer; transe norm special case bilinear+linear described bilinear; bilinear-diag special case bilinear relation matrix diagonal matrix. table shows results compared methods datasets. general observe performance increases complexity model decreases complex model provides worst performance suggests overﬁtting. compared previously published results transe implementation achieves much better results using evaluation metric attribute discrepancy mainly different choice optimization adagrad constant learning rate. also found bilinear consistently provides comparable better performance transe especially note contains much entities require parametrization relations expressive better handle richness entities. interestingly found simple variant bilinear bilinear-diag clearly outperforms baselines achieves comparable performance bilinear uses weighted element-wise product transe uses element-wise subtraction bias highlight difference distmult distadd refer bilinear-diag transe respectively. comparison models provide insights effect common choices compositional operations multiplication addition modeling entity relations. overall observed superior performance distmult datasets table table shows hits score four types relation categories fbk- predicting subject entity object entity respectively. distmult signiﬁcantly outperforms distadd almost categories. qualitative results found appendix. following examine learning entity representations introduce improvements using non-linear projection initializing entity vectors pre-trained phrase vectors. focus distmult baseline compare modiﬁcations distmulttanh distmult-tanh-ev-init fbk-. also reimplemented word vector representation initialization technique introduced entity represented average word vectors word vectors initialized using -dimensional pre-trained word vectors released wordvec. denote method distmult-tanh-wv-init. inspired design evaluation setting predicted entities automatically ﬁltered according entity types provides better understanding model performance entity type information provided. table distmult-tanh-ev-init provides best performance metrics. surprisingly observed performance drops distmult-tanh-wv-init. suspect word vectors appropriate modeling entities described non-compositional phrases promising performance distmult-tanh-ev-init suggests embedding model greatly beneﬁt pre-trained entity-level vectors. paper present uniﬁed framework modeling multi-relational representations scoring learning conduct empirical study several recent multi-relational embedding models framework. investigate different choices relation operators based linear bilinear transformations also effects entity representations incorporating unsupervised vectors pre-trained extra textual resources. results show several interesting ﬁndings enabling design simple embedding model achieves state-of-the-art performance popular knowledge base completion task evaluated freebase. given recent successes deep learning various applications; e.g. future work exploit deep structure including possibly tensor construct computing neural embedding vectors; e.g. extend current multi-relational neural embedding model deep version potentially capable capturing hierarchical structure hidden input data.", "year": 2014}