{"title": "Sylvester Normalizing Flows for Variational Inference", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "abstract": "Variational inference relies on flexible approximate posterior distributions. Normalizing flows provide a general recipe to construct flexible variational posteriors. We introduce Sylvester normalizing flows, which can be seen as a generalization of planar flows. Sylvester normalizing flows remove the well-known single-unit bottleneck from planar flows, making a single transformation much more flexible. We compare the performance of Sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets.", "text": "inference relies ﬂexible apvariational proximate posterior distributions. normalizing ﬂows provide general recipe construct ﬂexible variational posteriors. introduce sylvester normalizing ﬂows seen generalization planar ﬂows. sylvester normalizing ﬂows remove well-known single-unit bottleneck planar ﬂows making single transformation much ﬂexible. compare performance sylvester normalizing ﬂows planar ﬂows inverse autoregressive ﬂows demonstrate compare favorably several datasets. stochastic variational inference allows posterior inference increasingly large complex problems using stochastic gradient ascent. continuous latent variable models variational inference made particularly efﬁcient amortized inference inference networks amortize cost calculating variational posterior data point particularly successful class models variational autoencoder generative model inference network given neural networks sampling variational posterior efﬁcient noncentered parameterization also known reparameterization trick variational inference searches best posterior approximation within parametric family distributions. hence true posterior distribution recovered exactly happens chosen family. particular widely used simple variational families diagonal covariance gaussian distributions variational approximation likely insufﬁcient. complex variational families enable better posterior approximations resulting improved model performance. therefore designing tractable expressive variational families important problem variational inference rezende mohamed introduced general framework constructing ﬂexible variational distributions called normalizing ﬂows. normalizing ﬂows transform base density number invertible parametric transformations tractable jacobians complicated distributions. proposed classes normalizing ﬂows planar ﬂows radial ﬂows. effective small problems hard train often many transformations required good performance. planar ﬂows kingma argue fact transformation used acts bottleneck warping direction time. large number ﬂows makes inference network deep harder train empirically resulting suboptimal performance. kingma proposed inverse auto-regressive ﬂows achieving state results dynamically binarized mnist time publication. successful iafs require large number parameters. large number parameters iafs cannot amortize parameters. instead amortization achieved additional context vector step. paper contribution paper sylvester’s determinant identity introduce sylvester normalizing ﬂows family ﬂows generalization planar ﬂows removing bottleneck. compare number different variants snfs show compare favorably planar ﬂows iafs. show speciﬁc variant snfs related iafs requiring many fewer parameters direct amortization parameters. consider probabilistic model observations continuous latent variables model parameters generative modeling often interested performing maximum likelihood learning parameters latent-variable model requires marginalization unobserved latent variables unfortunately integration generally intractable. variational inference instead introduces variational approximation posterior learnable parameters construct lower bound marginal likelihood bound known evidence lower bound referred variational free energy. equation ﬁrst term represents reconstruction error second term kullbackleibler divergence approximate posterior prior distribution acts regularizer. paper consider variational autoencoders distributions whose parameters given neural networks. parameters generative model inference model respectively trained jointly stochastic minimisation made efﬁcient reparameterization trick equation better variational approximation posterior tighter elbo. simplest probably widely used choice variation distribution diagonal-covariance gaussians form σσσ) however simple variational distributions elbo fairly loose resulting biased maximum likelihood estimates model parameters harming generative performance. thus variational inference work well ﬂexible approximate posterior distributions needed. rezende mohamed propose construct ﬂexible posteriors transforming simple base distribution series invertible transformations easily computable jacobians. resulting transformed density after transformation follows strategy used variational inference follows ﬁrst stochastic variable drawn simple base posterior distribution diagonal gaussian sample transformed number ﬂows. applying ﬂows ﬁnal latent stochastic variables given corresponding log-density given normalizing ﬂows normally used amortized variational inference. instead learning variational parameters data point well parameters outputs deep neural network conditioned referred inference network. here suitable smooth activation function. rezende mohamed show tanh transformations kind invertible long matrix determinant lemma jacobian transformation given practice many planar transformations required transform simple base distribution ﬂexible distribution especially high dimensional latent spaces. kingma argue related term effectively acts single-neuron mlp. next section derive generalization planar ﬂows single-neuron bottleneck still maintaining property efﬁciently computable jacobian determinant. rd×m rm×d jacobian determinant transformation obtained using sylvester’s determinant identity generalization matrix determinant lemma. theorem rd×m rm×d computation determinant matrix thus reduced computation determinant matrix. using sylvester’s determinant identity jacobian determinant transformation given computed since also upper triangular. following theorem gives sufﬁcient condition transformation invertible. theorem upper triangular matrices. smooth function bounded positive derivative. then diagonal entries satisfy rii˜rii −/h∞ invertible transformation given invertible. orthogonality convenient property mathematically hard achieve practice. paper consider three different ﬂows based theorem various ways preserve orthogonality ﬁrst explicit differentiable constructions orthogonal matrices third variant assumes speciﬁc ﬁxed permutation matrix orthogonal matrix. orthogonal sylvester ﬂows. first consider sylvester using matrices orthogonal columns choose thus introduce ﬂexible bottleneck. similar ensure orthogonality recall one-dimensional real functions strictly positive derivatives invertible. columns orthonormal span subspace span{q denote orthogonal complement. decompose similarly decom⊥. clearly pose hence acts thus sufﬁces consider effect multiplying left gives vectors respective coordinates w.r.t. dimensions completely independent dimension transformed real function riih. consider single dimension since h∞rii˜rii thus invertible. since dimensions independent transformation invertible dimension hence write inverse using normalizing ﬂows amortized inference setting parameters base distribution well parameters functions data point figure shows diagram step amortization procedure. inference network takes datapoints input provides output mean variance several transformations applied producing ﬂexible posterior distribution parameters produced output inference network thus fully amortized. number invertible transformations tractable jacobians proposed recent years. rezende mohamed ﬁrst discussed transformations context stochastic variation inference coining term normalizing ﬂows. rezende mohamed proposed different parametric families transformations tractable jacobians planar radial ﬂows. effective small problems transformations hard scale large latent spaces often require large number transformations. transformation corresponding planar ﬂows given recently successful class ﬂows called inverse autoregressive flows introduced name suggests transformation seen inverse autoregressive transformation. consider following autoregressive transformation transformation models distribution variable autoregressive faci= since parameters transformation dependent procedure requires sequential steps samsufﬁcient condition convergence given here -norm matrix refers λmax λmax representing largest singular value experimental evaluations iterative procedure qq−if frobenius norm small convergence threshold. observed running procedure steps sufﬁcient ensure convergence respect threshold. minimize computational overhead introduced orthogonalization perform orthogonalization parallel ﬂows. since orthogonalization procedure differentiable allows calculation gradients respect backpropagation allowing standard optimization scheme stochastic gradient descent used updating parameters. householder sylvester ﬂows. second study householder sylvester ﬂows orthogonal matrices constructed products householder reﬂections. householder transformations reﬂections hyperplanes. reﬂection hyperplane orthogonal given worth noting performing single householder transformation cheap compute requires parameters. chaining together several householder transformations results general orthogonal matrices shown orthogonal matrix written product householder transformations. householder sylvester number householder transformations hyperparameter trades number parameters generality orthogonal transformation. note householder transformations forces since householder transformation result square matrices. figure different amortization strategies sylvester normalizing ﬂows inverse autoregressive flows. left inference network produces amortized parameters. strategy also employed planar ﬂows. right large number parameters introduces measure dependence context context acts additional input transformation. parameters independent inverse transformation longer dependent transformation hence transformation computed parallel )/¯σ. rewriting /¯σi −¯µ/¯σi yields transformation starting multiple transformations stacked produce ﬂexible probability distributions. depend linearly model full covariance gaussian distributions. order move away gaussian distributions ﬂexible distributions important nonlinear functions zt−. practice wide mades deep pixelcnn layers needed increase ﬂexibility transformations. results transformations large number parameters. shown figure amortization achieved context autoregressive networks additional input every step. kingma every transformation order reversed order ensure average dimensions warped equally. t-snf effect achieved using permutation matrix reverses order every transformation orthogonal matrix. however mean-only volume-preserving transformation i.e. determinant jacobian absolute value one. t-snf volume preserving nonzero elements diagonals note kingma shown difference performance mean-only general transformation negligible. important difference t-snf parameters amortized. t-snf directly amortized functions input equivalent amortizing made parameters mean-only iaf. input dependent made parameters allows ﬂexible transformations fewer parameters. householder sylvesters ﬂows also seen non-linear extension householder ﬂows householder ﬂows volumepreserving ﬂows transform variational posterior diagonal covariance matrix fullcovariance posterior. householder ﬂows special case h-snf identity matrix residual connection left out. number invertible transformations proposed context density estimation. note density estimation requires inverse tractable. provably invertible transformation thus goal transform complicated data distribution back simple distribution. general directions invertible transformations need tractable. hence methods developed density estimation generally directly applicable variational inference. nice transformation splits variables disjoint subsets subsets transformed left unchanged. next transformation different subset variables transformed. results transformation trivially invertible tractable jacobian. real uses fundamental idea. appealingly tractable inverse nice real generate data estimate density forward pass. however fact subset variables updated transformation many transformations needed practice. rezende mohamed compared nice planar ﬂows context variational inference found planar ﬂows empirically perform better. finally papamakarios showed ﬁtting seen ﬁtting implicit data distribution base distribution. however generating data density model requires passes making unappealing variational inference. here brieﬂy compare number parameters needed planar ﬂows three sylvester normalizing ﬂows. denote size stochastic variables number output units inference network planar ﬂows amortized parameters transformation. therefore number parameters related transformations equal implementation described section inference network needs produce context size denotes width made layers. total number related learnable parameters comes case orthogonal sylvester ﬂows bottleneck size require parameters. householder sylvester ﬂows householder reﬂections transformation parameters needed. finally triangular sylvester ﬂows parameters require optimization. planar ﬂows require smallest number parameters generally result worse results. iafs hand require number parameters quadratic width made layers. good results quite large. contrast snfs number parameters quadratic dimension latent space large still amortized. perform empirical studies performance sylvester ﬂows four datasets statically binarized mnist freyfaces omniglot caltech silhouettes. baseline model plain fully factorized gaussian distribution. furthermore compare planar ﬂows inverse autoregressive flows different sizes. annealing optimize lower bound prefactor divergence linearly increased epochs suggested bowman sønderby learning rate used experiments. order obtain estimates negative likelihood used importance sampling unless otherwise stated importance samples used. order assess performance different ﬂows properly base encoder decoder architecture models. gated convolutions transposed convolutions base layers encoder decoder architecture respectively. inference network consists several gated convolution layers produce hidden unit vector. ﬂattened hidden units input fully connected layers predict mean variance planar sylvester ﬂows ﬂattened hidden units passed separate linear layer output amortized parameters. ﬂattened hidden units also passed linear layer produce context figure negative evidence lower bound static mnist. results h-snf reﬂections orthogonal matrix left clarity similar results reﬂections. model evaluated times. shaded areas indicate standard deviation. following implementation transformation transformation ﬁrst applies made layer followed nonlinearity input upscaling hidden variable size point context vector hcontext added hidden units masked layers applied produce mean scale transformation table negative log-likelihood free energy static mnist. numbers produced runs model different random initializations. standard deviations different runs also shown. figure shows dependence negative evidence lower bound number ﬂows type static mnist. exact numbers corresponding ﬁgure shown section appendix. models performance improves functions number ﬂows. ﬂows difference between baseline planar ﬂows small. however planar ﬂows clearly beneﬁt transformations. three different widths made layers used surprisingly ﬂows widest hidden units outperformed hidden units made layers. expect fact model parameters therefore harder train indicated larger standard deviation model. three sylvester ﬂows outperform planar ﬂows. orthogonal sylvester ﬂows show results orthogonal vectors orthogonal matrix thus corresponding bottlenecks size respectively latent space size clearly larger bottleneck improves performance. householder sylvester ﬂows experimented householder reﬂections orthogonal matrix. since results nearly indistinguishable variants left curve avoid clutter. o-snf h-snf t-snf seem perform par. table negative evidence lower bound estimated negative log-likelihood shown baseline together models ﬂows. reported result made width o-snf model bottleneck table results freyfaces omniglot caltech silhouettes datasets. freyfaces dataset results reported bits dim. datasets results reported nats. model ﬂows used. made width used o-snf bottleneck used. h-snf householder reﬂections used construct orthogonal matrices. datasets runs model performed. h-snf contains householder reﬂections orthogonal matrix. again sylvester ﬂows outperform planar ﬂows terms free energy negative log-likelihood. discussed section t-snf closely related mean-only made parameters directly amortized. fact t-snf outperforms indicates amortizing parameters directly leads ﬂexible transformation compared taking wide made data dependent context additional input. freyfaces small dataset around faces. normalizing ﬂows increase performance planar ﬂows yielding best result closely followed triangular householder sylvester ﬂows. expect planar ﬂows perform best case since least sensitive overﬁtting. omniglot caltech silhouettes results clearer sylvester normalizing ﬂows family resulting best performance. h-snf t-snf perform better o-snf. could attributed fact o-snf bottleneck latent space size scores caltech surprisingly bad. expect could case large number parameters need trained iaf. therefore also evaluated present family normalizing ﬂows sylvester normalizing ﬂows. ﬂows generalize planar ﬂows maintaining efﬁciently computable jacobian determinant sylvester’s determinant identity. ensure invertibility ﬂows orthogonal triangular parameter matrices. three variants sylvester ﬂows investigated. first orthogonal sylvester ﬂows iterative procedure maintain orthogonality parameter matrices. second householder sylvester ﬂows householder reﬂections construct orthogonal matrices. third triangular sylvester ﬂows alternate ﬁxed permutation identity matrices orthogonal matrices. show triangular sylvester ﬂows closely related mean-only directly amortized made parameters. performing comparably planar ﬂows freyfaces dataset proposed family ﬂows improve signiﬁcantly upon planar ﬂows three datasets. would like thank christos louizos useful discussions helping implementation inverse autoregressive ﬂows. funded epsrc oxwasp grant ep/l/. funded european commission within msc. rvdb funded eric nalisnick lars hertel padhraic smyth. approximate inference deep latent gaussian mixtures. nips workshop bayesian deep learning george papamakarios iain murray theo pavlakou. masked autoregressive flow density estimation. nips pages aaron oord kalchbrenner lasse espeholt koray kavukcuoglu oriol vinyals alex graves. conditional image generation pixelcnn decoders. nips pages matthew hoffman david blei chong wang john paisley. stochastic variational inference. journal machine learning research michael jordan zoubin ghahramani tommi jaakkola lawrence saul. introduction variational methods graphical models. machine learning diederik kingma salimans rafal jozefowicz chen ilya sutskever welling. improved variational inference inverse autoregressive flow. nips pages inputs outputs l-th layer respectively weights l-th layer denote biases convolution operator sigmoid activation function. used following architecture encoder notice last layer acts fully-connected layer. eventually fully-connected linear layers used parameterized diagonal gaussian distribution amortized parameters ﬂow. omniglot dataset containing hand-written characters various alphabets. character represented images makes problem challenging. dataset split training datapoints test images. randomly pick training examples validation. training applied dynamic binarization data similarly dynamic mnist. caltech silhouettes contains images representing silhouettes object classes. image ﬁlled black polygon object white background. training images validation datapoints test examples. dataset characterized small training sample size many classes makes learning problem ambitious. frey faces dataset faces person different emotional expressions. dataset consists nearly gray-scaled images. randomly split training images validation images test images. repeated experiment times.", "year": 2018}