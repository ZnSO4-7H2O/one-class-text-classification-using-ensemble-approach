{"title": "Imaging Time-Series to Improve Classification and Imputation", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Inspired by recent successes of deep learning in computer vision, we propose a novel framework for encoding time series as different types of images, namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov Transition Fields (MTF). This enables the use of techniques from computer vision for time series classification and imputation. We used Tiled Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn high-level features from the individual and compound GASF-GADF-MTF images. Our approaches achieve highly competitive results when compared to nine of the current best time series classification approaches. Inspired by the bijection property of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on the GASF images of four standard and one synthesized compound dataset. The imputation MSE on test data is reduced by 12.18%-48.02% when compared to using the raw data. An analysis of the features and weights learned via tiled CNNs and DAs explains why the approaches work.", "text": "inspired recent successes deep learning computer vision propose novel framework encoding time series different types images namely gramian angular summation/difference fields markov transition fields enables techniques computer vision time series classiﬁcation imputation. used tiled convolutional neural networks standard datasets learn high-level features individual compound gasf-gadf-mtf images. approaches achieve highly competitive results compared nine current best time series classiﬁcation approaches. inspired bijection property gasf rescaled data train denoised auto-encoders gasf images four standard synthesized compound dataset. imputation test data reduced .%-.% compared using data. analysis features weights learned tiled cnns explains approaches work. introduction since techniques developed deep neural networks greatly impacted natural language processing speech recognition computer vision research successful deep learning architecture used computer vision convolutional neural networks cnns exploit translational invariance extracting features receptive ﬁelds learning weight sharing becoming state-of-the-art approach various image recognition computer vision tasks since unsupervised pretraining shown improve performance sparse coding topographic independent component analysis integrated unsupervised pretraining approaches learn diverse features complex invariances algorithms generative models deep belief networks denoised auto-encoders many deep generative models developed based energy-based model autoencoders. temporal autoencoding integrated restrict boltzmann machines improve generative models training strategy inspired recent work optimization-based learning proposed train complex neural networks imputation tasks generalized denoised auto-encoder extends theoretical framework applied deep generative stochastic networks inspired recent successes supervised unsupervised learning techniques computer vision consider problem encoding time series images allow machines visually recognize classify learn structures patterns. reformulating features time series visual clues raised much attention computer science physics. speech recognition systems acoustic/speech data input typically represented concatenating mel-frequency cepstral coefﬁcients perceptual linear predictive coefﬁcient recently researchers trying build different network structures time series visual inspection designing distance measures. recurrence networks proposed analyze structural properties time series complex systems build adjacency matrices predeﬁned recurrence functions interpret time series complex networks. silva extended recurrence plot paradigm time series classiﬁcation using compression distance another build weighted adjacency matrix extracting transition dynamics ﬁrst order markov matrix although maps demonstrate distinct topological properties among different time series remains unclear topological properties relate original time series since exact inverse operations. present three novel representations encoding time series images call gramian angular summation/difference field markov transition field applied deep tiled convolutional neural networks classify time series images standard datasets. experimental equation above time stamp constant factor regularize span polar coordinate system. polar coordinate based representation novel understand time series. time increases corresponding values warp among different angular points spanning circles like water rippling. encoding equation important properties. first bijective monotonic given time series proposed produces result polar coordinate system unique inverse map. second opposed cartesian coordinates polar coordinates preserve absolute temporal relations. discuss detail future work. rescaled data different intervals different angular bounds. corresponds cosine function cosine values interval fall angular bounds discuss later provide different information granularity gramian angular field classiﬁcation tasks gramian angular difference field rescaled data accurate inverse map. property actually lays foundation imputing missing value time series recovering images. transforming rescaled time series polar coordinate system easily exploit angular perspective considering trigonometric sum/difference point identify temporal correlation within different time intervals. gramian summation angular field gramian difference angular field deﬁned follows gafs several advantages. first provide preserve temporal dependency since time increases position moves top-left bottom-right. gafs contain temporal correlations represents relative correlation superposition/difference directions respect time interval main diagonal special case contains original value/angular information. main diagonal reconstruct time series high level features learned deep neural network. however gafs large size gramian matrix length time series reduce size figure illustration proposed encoding gramian angular fields. sequence rescaled time series ’fish’ dataset. transform polar coordinate system ﬁnally calculate gasf/gadf images eqs. example build gafs without smoothing gafs high resolution. results demonstrate approaches achieve best performance standard dataset compared previous current best classiﬁcation methods. inspired bijection property gasf rescaled data train denoised auto-encoder gasf images standard synthesized compound dataset. imputation test data reduced .%-.% compared using data. analysis features weights learned tiled cnns explains approaches work. imaging time series ﬁrst introduce frameworks encoding time series images. ﬁrst type image gramian angular field represent time series polar coordinate system instead typical cartesian coordinates. gramian matrix element actually cosine summation angles. inspired previous work duality time series complex networks main idea second framework markov transition field build markov matrix quantile bins discretization encode dynamic transition probability quasi-gramian matrix. gramian angular field given time series real-valued observations rescale values fall interval figure illustration proposed encoding markov transition fields. sequence time-series ’ecg’ dataset ﬁrst discretized quantile bins. calculate markov transition matrix ﬁnally build figure structure tiled convolutional neural networks. size receptive ﬁelds ﬁrst convolutional layer second convolutional layer. tica pooling layer pools block input units previous layer without warping around borders optimize sparsity pooling units. number pooling units exactly number input units. last layer linear classiﬁcation. construct network stacking tiled cnns maps tiling size markov transition field propose framework similar campanharo encoding dynamical transition statistics extend idea representing markov transition probabilities sequentially preserve information time domain. given time series identify quantile bins assign corresponding bins thus construct weighted adjacency matrix counting transitions among quantile bins manner ﬁrstorder markov chain along time axis. given frequency point quantile followed markov transition matrix. insensitive distribution temporal dependency time steps however experimental results demonstrate getting temporal dependency results much information loss matrix overcome drawback deﬁne markov transition field follows build markov transition matrix dividing data quantile bins. quantile bins contain data time stamp denotes transition probability spread matrix contains transition probability magnitude axis matrix considering temporal positions. time series. mij||i−j|=k denotes transition probability points time interval example mij|j−i= illustrates transition process along time axis skip step. main diagonal special case captures probability quantile time step make image size manageable computation efﬁcient reduce size averaging pixels non-overlapping patch blurring kernel m}m×m. aggregate transition probabilities subsequence length together. figure shows procedure encode time series mtf. apply tiled cnns classify time series using representations datasets different domains medicine entomology engineering astronomy signal processing others. datasets pre-split training testing sets facilitate experimental comparisons. compare classiﬁcation error rate gasf-gadf-mtf approach previously published results competing methods best approaches proposed recently early state-of-the-art classiﬁers based euclidean distance fast-shapelets classiﬁer based bag-of-patterns based vector space model classiﬁer based recurrence patterns compression distance treebased symbolic representation multivariate time series classiﬁer based bag-of-features representation represent datasets generated human motions ﬁgure shapes synthetically predeﬁned procedures remaining temporal signals respectively. approach numbers brackets optimal size quantile size. words adiac beef coffee faceall facefour point lighting lighting oliveoil osuleaf swedishleaf synthetic control trace patterns wafer yoga wins tiled convolutional neural networks tiled convolutional neural networks variation convolutional neural networks tiles multiple feature maps learn invariant features. tiles parameterized tile size control distance weights shared. producing multiple feature maps tiled cnns learn overcomplete representations unsupervised pretraining topographic sake space please refer details. structure tiled cnns applied paper illustrated figure experiment setting experiments size image regulated number bins sgaf given time series size divide time series sgaf adjacent non-overlapping windows along time axis extract means bin. enables construct smaller matrix gsgaf ×sgaf requires time series discretized quantile bins calculate markov transition matrix construct image mn×n afterwards. classiﬁcation shrink image size blurring kernel tiled trained image size {sgaf quantile size last layer tiled linear soft margin select -fold cross validation training set. input image size sgaf quantile size pretrain tiled full unlabeled dataset learn initial weights tica. train last layer selecting penalty factor cross validation. finally classify test using optimal hyperparameters lowest error rate training set. models prefer larger larger helps preserve information procedure larger encodes dynamic transition statistics detail. model selection approach provides generalization without overly expensive computationally. results discussion tiled cnns classify single gasf gadf images well compound gasf-gadf-mtf images datasets. sake space show full results single-channel images. generally approach prone overﬁtting relatively small difference training test errors. exception olive dataset approach test error signiﬁcantly higher. addition risk potential overﬁtting found generally higher error rates gafs. likely uncertainty inverse mtf. note encoding function rescaled time series gafs surjections. functions gafs produce image ﬁxed given time series because surjective mapping functions inverse image mapping functions ﬁxed. however recovering broken gasf images. training manually salt-and-pepper noise time series transform data gasf images. single layer denoised auto-encoder fully trained generative model reconstruct gasf images. note input layer noise broken gasf images. sigmoid function helps learn nonlinear features hidden layer. last layer compute mean square error original broken gasf images loss function evaluate ﬁtting performance. train models simple batch gradient descent applied back propagate inference loss. testing corrupt time series transform noisy data broken gasf trained helps recover image extract main diagonal reconstruct recovered time series. compare imputation performance also test standard time series data input recover missing values experiment setting models batch gradient descent batch size optimization iterations changed less threshold gasf time series. single hidden layer hidden neurons sigmoid functions. choose four dataset different types time series repository imputation task point swedishleaf explore statistical dependency learned generalized unknown data four datasets adiac dataset together train impute totally unknown test datasets patterns wafer randomness input randomly data among speciﬁc time series zero experiments imputation implemented theano control random initialization parameters randomness induced gradient descent repeated every experiment times report average mse. figure pipeline time series imputation image recovery. gasf broken gasf recovered gasf time series corrupted time series missing value predicted time series dataset swedishleaf mapping function gafs rescaled time series bijective. shown later section reconstruct time series diagonal gasf hard even roughly recover signal mtf. even rescaled data gafs smaller uncertainty inverse image mapping function randomness comes ambiguity hand much larger inverse image space results large variations recover signal. although encodes transition dynamics important features time series features alone seem sufﬁcient recognition/classiﬁcation tasks. note pixel denotes superstition/difference directions transition probability quantile quantile encodes static information depicts information dynamics. point view consider three orthogonal channels like different colors image space. thus combine gafs images size construct triple-channel image combines static dynamic statistics embedded time series posit able enhance classiﬁcation performance. experiments below pretrain tune tiled compound gasf-gadf-mtf images. then report classiﬁcation error rate test sets. table tiled classiﬁers gasf-gadfmtf images achieved signiﬁcantly competitive results state-of-the-art time series classiﬁcation approaches. previously mentioned mapping functions rescaled time series gafs surjections. uncertainty among inverse images come ambiguity however mapping functions rescaled time series bijections. main diagonal gasf i.e. {gii} {cos} allows precisely reconstruct original time series means unknown points among time series. interestingly data perform well whole sequence generally full imputation mse. time series known data much better predicting unknown data predicting missing value using gasf always achieves slightly higher full imputation reduced .%-.%. observe difference full imputation much smaller gasf data. interpolation gasf stable performance data. predicting missing values using gasf stable performance using time series? actually transformation maps gafs generally equivalent kernel trick. deﬁning inner product achieve data augmentation increasing dimensionality data. preserving temporal spatial information gasf images utilizes temporal spatial dependencies considering missing points well relations data explicitly encoded gasf images. entire sequence instead short subsequence helps predict missing value performance stable full imputation close. contrast cases cnns applied natural image recognition tasks neither gafs natural interpretations visual concepts like edges angles. section analyze features weights learned tiled cnns explain approach works. figure illustrates reconstruction results feature maps learned tiled cnns gasf tiled cnns extracts color patch essentially moving average enhances several receptive ﬁelds within nonlinear units different trained weights. simple moving average synthetic integration considering temporal dependencies among different time intervals beneﬁt gramian matrix structure helps preserve temporal information. observing orthogonal reconstruction layer feature maps clearly observe tiled cnns extract multi-frequency dependencies convolution pooling architecture images preserve trend addressing details different subphases. high-leveled feature maps learned tiled equivalent multi-frequency approximator original curve. experiments also demonstrates learned weight matrix constraint makes effective local orthogonality. tica pretraining provides built-in advantage function w.r.t parameter space likely ill-conditioned weight matrix quasi-orthogonal approaching without large magnitude. implies condition number approaches helps system well-conditioned. imputation gasf images concept angle edge actually learned different prototypes gasf images signiﬁcant noise ﬁlters misc dataset training relatively small better learn different ﬁlters. actually noisy ﬁlters patterns work like gaussian noise ﬁlter. conclusions future work created pipeline converting time series novel representations gasf gadf images extracted multi-level features using tiled classiﬁcation imputation. demonstrated approach yields competitive results classiﬁcation compared recently best methods. imputation using gasf achieved better stable performance data using analysis features learned tiled suggested tiled works like multi-frequency moving average beneﬁts temporal dependency preserved gramian matrix. features learned gasf shown different prototype correlated basis construct images. important future work involve developing recurrent neural nets process streaming data. also quite interested different deep learning architectures perform gafs images. another important future work learn deep generative models high-level features gafs images. apply time series models real world regression/imputation anomaly detection tasks. references fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop mustafa gokce baydogan george runger eugene tuv. bag-of-features framework classify time series. pattern analysis machine intelligence ieee transactions yoshua bengio guillaume alain pascal vincent. generalized denoising auto-encoders generative models. advances neural information processing systems pages reik donner yong jonathan donges norbert marwan j¨urgen kurths. recurrence networksa novel paradigm nonlinear time series analysis. journal physics reik donner michael small jonathan donges norbert marwan yong ruoxi xiang j¨urgen kurths. recurrence-based time series analysis means complex network methods. international journal bifurcation chaos dumitru erhan yoshua bengio aaron courville pierre-antoine manzagol pascal vincent samy bengio. unsupervised pre-training help deep learning? journal machine learning research rong-en kai-wei chang cho-jui hsieh xiang-rui wang chih-jen lin. liblinear library large linear classiﬁcation. journal machine learning research koray kavukcuoglu pierre sermanet y-lan boureau karol gregor micha¨el mathieu yann cun. learning convolutional feature hierarchies visual recogadvances neural information processing systems nition. pages eamonn keogh michael pazzani. scaling dynamic time warping datamining applications. proceedings sixth sigkdd international conference knowledge discovery data mining pages eamonn keogh xiaopeng chotirat ratanamahatana. time series classiﬁcation/clustering homepage. url= http//www. ucr. edu/˜ eamonn/time series data alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages jessica rohan khade yuan rotationinvariant similarity time series using bag-of-patterns representation. journal intelligent information systems jiquan ngiam zhenghao chen daniel chia pang quoc andrew tiled convolutional neural networks. advances neural information processing systems pages thanawin rakthanmanon eamonn keogh. fast shapelets scalable algorithm discovering time series shapelets. proceedings thirteenth siam conference data mining siam pavel senin sergey malinchik. sax-vsm interpretable time series classiﬁcation using vector space model. data mining ieee international conference pages ieee diego silva vinicius souza gustavo eapa batista. time series classiﬁcation using compression distance recurrence plots. data mining ieee international conference pages ieee pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pages", "year": 2015}