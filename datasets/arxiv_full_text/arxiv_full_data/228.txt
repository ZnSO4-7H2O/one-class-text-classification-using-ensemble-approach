{"title": "A Rotation and a Translation Suffice: Fooling CNNs with Simple  Transformations", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need for designing robust classifiers even in natural, benign contexts.", "text": "various settings. however methods quite sophisticated resulting perturbations tend fairly contrived. might thus suspect adversarial examples constitute problem fully adversarial setting unlikely arise benign settings. raises natural question work address question studying basic image transformations translations rotations. transformations appear natural human show alone cause signiﬁcant drop model’s performance. remains case even model trained using appropriate data augmentation visual information lost transformations figure illustration. show simple transformations namely translations rotations alone sufﬁcient fool neural network-based vision models signiﬁcant fraction inputs. sharp contrast previous work relied complicated optimization approaches unlikely appear outside truly adversarial setting. moreover fooling rotations translations easy require black-box queries target model. overall ﬁndings emphasize need designing robust classiﬁers even natural benign contexts. neural network models widely embraced cuttingedge solutions computer vision speech recognition text analysis however despite near sometimes beyond human-like results achieve benchmarks still understand true reliability robustness performance. prominent issue context existence so-called adversarial examples i.e. inputs almost indistinguishable natural data human cause state-of-the-art classiﬁers make incorrect predictions high conﬁdence. raises concerns neural networks contexts reliability dependability security matter. *equal contribution eecs massachusetts usa. correspondence logan engstrom <engstrommit.edu> brandon tran <btranmit.edu> dimitris tsipras <tsiprasmit.edu> ludwig schmidt <ludwigsmit.edu> aleksander ˛adry <madrymit.edu>. start standard models mnist cifar imagenet datasets. models achieve close state-of-the-art performance respective benchmarks. demonstrate however even small rotations translations alone cause models signiﬁcant drop classiﬁcation accuracy. drop ranges high additionally show even need direct access model misclassifying transformations. fact choosing worst random transformations sufﬁcient reduce accuracy models mnist cifar imagenet demonstrates existence adversarial examples robustness neural network–based models also concern outside safety/security contexts. thus need properly examine mitigate settings natural benign. examine possible ways alleviate vulnerabilities. natural ﬁrst step augment training procedure rotations translations. mitigate problem mnist models trained cifar imagenet still robust. thus propose natural methods increasing robustness models. methods based robust optimization aggregation random input transformations. offering signiﬁcant improvement expense considerable computational overhead. even then still sufﬁcient completely mitigate vulnerability. suggests obtaining models robust spatial transformations inputs remains challenge. finally examine interplay variant adversarial perturbations widely used ∞-based variant. observe variants seem orthogonal other. particular pixel-based robustness imply spatial robustness combining spatial ∞-bounded transformations seems cumulative effect. emphasizes need current pixelwise notions similarity used context adversarial examples signiﬁcantly broadened include natural spatial transformations rotations translations considered here. perform extensive experiments provide ﬁnegrained understanding vulnerabilities described wide spectrum datasets training regimes. summary show that simple attack based purely rotations translations effective state-of-the-art neural network models. remains case even model trained appropriate data augmentation image information lost transformation. take steps increase model’s robustness rotations translations cost increased training and/or inference runtime. however methods still sufﬁcient fully solve problem. recall that context vision models adversarial example given image classiﬁer image that hand causes classiﬁer output different label i.e. hand visually similar clearly notion visual similarity precisely deﬁned here. fact providing precise rigorous deﬁnition extraordinarily difﬁcult would implicitly require formally capturing notion human perception. consequently previous work settled assuming close small enough. arguably images close norm visually similar. however converse necessarily true. work expand range similarity measures considered include natural ones. focus rotations translations which changing images signiﬁcantly norms tend affect human perception. initial goal develop sufﬁciently strong methods generating adversarial examples type. context pixel-wise perturbations successful approach constructing adversarial examples optimization methods suitable loss function following approach parametrize attack method tunable parameters optimize parameters. perform optimization three distinct ways first-order method starting random choice parameters iteratively take steps direction loss function’s gradient. direction locally maximizes loss classiﬁer note grid search discretize parameter space exhaustively examine every possible parametrization attack causes classiﬁer give wrong prediction since parameter space small enough method computationally feasible p-based adversaries). worst-of-k randomly sample different attack parameter choices choose model performs worse. increase attack interpolates random choice grid search. observe ﬁrst-order attack requires full knowledge model order compute gradient loss respect input attacks not. simply require evaluating chosen input done even query access target model. need deﬁne exact range attacks want optimize over. case rotation translation attacks wish parameters rotating original image degrees around center translating pixels causes classiﬁer make wrong prediction. formally pixel position moved following position center image) implement transformation differentiable manner using spatial transformer blocks order deal pixels mapped non-integer coordinates transformer units include differentiable bilinear interpolation routine. since loss function differentiable respect input transformation turn differentiable respect parameters apply ﬁrst-order optimization method problem. loss function neural network correct label example since non-concave loss classiﬁer function images real numbers expresses performance network particular example natural idea design models robust rotations translations augment training random transformation. order improve model robustness propose novel defense mechanism modify training inference method model. robust optimization ﬁrst step instead performing standard empirical risk minimization train model utilize robust optimization methods. robust optimization rich history recently applied successfully context defending neural networks adversarial examples main barrier applying robust optimization improve spatial invariance lack efﬁcient procedure compute worst-case perturbation given example. performing grid search prohibitive would increase training time factor close grid size. moreover non-convexity landscape prevents ﬁrst-order methods discovering worst-case transformations efﬁciently given cannot fully optimize space transformations coarse approximation provided worst-of- adversary ﬁrst sample random transformations training example uniformly space allowed transformations. evaluate model transformations choose train highest loss. corresponds minimizing min-max formulation problem similar training adversary increases training factor roughly six. aggregating random transformations every example transformations don’t result misclassiﬁcation verify section motivates following inference procedure compute number random transformations given image output label predicted transformations constrain random transformations within input image size translation direction degrees rotations. observe adversary rotates image cifar imagenet datasets. order determine extent misclassiﬁcation caused non-sufﬁcient data augmentation training examine various data augmentation methods. model architecture mnist dataset simple convolutional neural network derived tensorflow tutorial order obtain fully convolutional version network replace fully-connected layer convolutional layers ﬁlters each followed global average pooling. cifar consider standard resnet model groups residual layers ﬁlter sizes residual units each. naturally ∞-adversarially trained models similar studied madry imagenet resnet- architecture using code tensorpack repository modify model architectures training procedures. make full experimental code available soon. attack space order maintain visual similarity images natural ones restrict space allowed perturbations relatively small. consider rotations degrees translations percent image size direction. corresponds pixels mnist cifar pixels imagenet grid search attacks consider values translation direction values rotations equally spaced. ﬁrst-order attacks steps projected gradient descent step size times standard natural training out-of-the-box training procedure. cifar includes random left-right ﬂipping random translations image standardization. imagenet includes random left-right ﬂipping random cropping similar various color space transformations. refer standard model. standard ∞-bounded adversarial training classiﬁer trained using ∞-bounded adversarial examples computed using projected gradient descent pixel space step. imagenet classiﬁer efﬁciently reliably trained adversary. refer model ∞-adversarially trained. random cropping order examine effects standard data augmentation training train cifar imagenet models without random cropping. still apply random left-right ﬂips rest transformations. refer model crop. random rotations translations perform uniformly random perturbation training examples attack space described training step. standard data augmentation translate rotate image choosing parameters uniformly random allowed space. refer model augmentation random rotations translations larger intervals augmentation instead select random perturbations superset attack space. sample random rotations degrees translations pixels mnist cifar pixels imagenet. refer model augmentation addition random rotations translations training greatly improves random adversarial accuracy classiﬁer mnist cifar less imagenet. note augmentation model trained transformations larger magnitude appear evaluation. problem signiﬁcant barrier. investigate issue visualizing loss landscape. random examples plot cross-entropy loss examples function translation rotation. figure show example dataset. additional examples visualized figure appendix. clearly observe loss landscape highly non-concave full low-value local maxima. appears low-dimensionality problem makes non-concavity crucial obstacle. even case mnist observe fewer local maxima large regions would prevent ﬁrst-order methods ﬁnding transformations high loss. investigate brittle networks analyze percentage grid points fooling example figure appendix majority perturbations benign signiﬁcant fraction fool classiﬁer. black canvas experiments might wonder reduced accuracy models cropping applied transformation. verify case padding image cifar imagenet datasets zeros. creates \"black canvas\" version dataset ensuring information original image lost translation rotation. show random adversarial examples case figure results full evaluation shown table table compare different attack methods across models datasets. observe worst-of- powerful adversary despite limited interaction model. ﬁrst-order adversary performs signiﬁcantly worse despite better random choice fails approximate ground-truth accuracy models. understanding failure first-order methods fact ﬁrst-order methods fail reliably adversarial examples sharp contrast previous work that case pixel-based perturbations optimization landscape well-behaved allowing ﬁrst-order methods consistently maxima good value. case spatial perturbations observe non-concavity relation black-box attacks performance worst-of- adversary impressive given limited interaction model. adversary performs random non-adaptive queries model able adversarial examples large fraction inputs. recently signiﬁcant interest designing black-box attacks neural network models. attacks rely black-box queries model without additional information minimize number queries required. results propose simple baseline strategy produces adversarial examples signiﬁcant fraction inputs queries. combining spatial ∞-bounded perturbations table observe models trained robust attacks achieve higher robustness spatial attacks. provides evidence families perturbation orthogonal other. investigate possibility considering combined adversary utilizes bounded perturbations rotations translations. results shown figure indeed observe that combined attack reduce classiﬁcation accuracy additive manner. table observe training worst-of- adversary greatly increases spatial robustness model. fact performs signiﬁcantly better simply augmenting training random transformations. believe using reliable methods compute worst-case transformations improve results. table accuracy different models natural evaluation rotation translation adversaries mnist cifar imagenet datasets. allowed transformations translations image pixels rotation. attack parameters chosen random sampling grid search rotations translations considered together separate. figure fine-grained dataset analysis. model visualize percent test fooled various methods. compute many examples fooled either translations rotations many fooled these many require combination fooled table comparison attack methods across datasets models. worst-of- effective signiﬁcantly reduces model accuracy despite limited interaction. ﬁrst-order adversary performs poorly despite large number steps allowed. figure loss landscape random example dataset performing left-right translations rotations. translations rotations restricted image pixels respectively. observe landscape signiﬁcantly non-concave making rendering methods adversarial example generation powerless. additional examples visualized figure appendix. original image constrained globally smooth optimized misclassiﬁcation probability. considers ∞–bounded pixel-wise perturbation version original image slightly rotated random pixels ﬂipped. methods require direct access attacked model compute gradient loss function respect model’s input. contrast attacks implemented using small number random nonadaptive transformations input. examined robustness state-of-the-art vision classiﬁers translations rotations. observed even small number randomly chosen perturbations input sufﬁcient considerably degrade model performance. vulnerability state-of-the-art neural networks simple naturally occurring spatial perturbations indicates adversarial robustness concern truly adversarial setting. ﬁnding misclassiﬁed perturbations constructed trying small number randomly chosen input transformations emphasizes classiﬁers indeed quite brittle. additional techniques need incorporated architecture training procedures modern classiﬁers achieve worst-case translational rotational robustness. also results underline need considering broader notions similarity pixel-wise distances studying adversarial misclassiﬁcation attacks. particular view combining pixel-wise distances rotations translations next step towards capturing right notion similarity context images. additionally classiﬁers trained robust ∞-bounded pixel-based perturbations exhibit additional spatial robustness suggesting orthogonal concepts. combining ∞-bounded attacks spatial transformations seems cumulative effect model. finally explored methods increase spatial robustness vision models. augmenting training data random transformations increases resulting classiﬁer’s robustness completely alleviate issue. observed randomly sampling transformations training point training worst signiﬁcantly improves model’s performance. furthermore making predictions based majority vote randomly perturbed versions input increase classiﬁcation accuracy adversarial setting. table majority defense. accuracy different models natural evaluation combined rotation translation adversary using aggregation multiple random transformations. image natural perturbed random transformations performed model outputs majority vote predicted labels. fact small rotations translation fool neural network-based classiﬁers mnist cifar ﬁrst observed main difference work focus ﬁnding minimum transformation required fool classiﬁer. measure qualitative comparison different architectures training procedures. contrast range small transformations study robustness model within space. recently observed independently possible various spatial transformations construct adversarial examples naturally adversarially trained models. main difference work show even simple transformations sufﬁcient break variety classiﬁers transformations employed involved. transformation based performing displacement individual pixels chen pin-yu zhang huan sharma yash jinfeng hsieh cho-jui. zeroth order optimization based black-box attacks deep neural networks without training substitute models. proceedings workshop artiﬁcial intelligence security collobert ronan weston jason. uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning graves alex mohamed abdel-rahman hinton geoffrey. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems madry aleksander makelov aleksandar schmidt ludwig tsipras dimitris vladu adrian. towards deep learning models resistant adversarial attacks. arxiv preprint arxiv. moosavi-dezfooli seyed-mohsen fawzi alhussein frossard pascal. deepfool simple accurate ieee method fool deep neural networks. conference computer vision pattern recognition cvpr vegas june papernot nicolas mcdaniel patrick goodfellow somesh celik berkay swami ananthram. practical black-box attacks machine learning. proceedings asia conference computer communications security raghunathan aditi steinhardt jacob liang percy. incertiﬁed defenses adversarial examples. ternational conference learning representations https//openreview.net/forum? id=bysob-rb. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge. international journal computer vision ./s---y. sharif mahmood bhagavatula sruti bauer lujo reiter michael accessorize crime real stealthy attacks state-of-the-art face recognition. proceedings sigsac conference computer communications security vienna austria october szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. proceedings ieee conference computer vision pattern recognition taigman yaniv yang ming ranzato marc’aurelio wolf lior. deepface closing human-level performance face veriﬁcation. proceedings ieee conference computer vision pattern recognition xiao chaowei jun-yan warren mingyan song dawn. spatially transformed adversarial examples. international conference learning representations https//openreview. net/forum?id=hyydrmzc-. figure mnist. successful adversarial examples models studied section rotations restricted within original image translations pixels direction example visualized along predicted label original perturbed versions. figure cifar. successful adversarial examples models studied section rotations restricted within original translations pixels directions example visualized along predicted label original perturbed version. figure imagenet. successful adversarial examples models studied section rotations restricted within original translations pixels directions example visualized along predicted label original perturbed version. figure visualizing angles fool classiﬁer random examples. dataset model visualize example row. corresponds misclassiﬁcation images. observe angles fooling models form highly non-convex set. figure cumulative density function plots. fraction grid points plot percentage correctly classiﬁed test examples fooled least grid points. instance ﬁrst plot mnist translations rotations approximately correctly classiﬁed natural examples misclassiﬁed grid points transformations. figure loss landscape random examples dataset performing left-right translations rotations. translations rotations restricted image pixels respectively. observe landscape signiﬁcantly non-concave making rendering methods adversarial example generation powerless. figure accuracy different classiﬁers ∞-bounded adversaries various values spatial transformations. value perform adversarial ∞-bounded perturbation. additionally combine random rotations translations grid search rotations translations order transformation combines adversarial way.", "year": 2017}