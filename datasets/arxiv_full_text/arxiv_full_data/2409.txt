{"title": "Hierarchical Affinity Propagation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Affinity propagation is an exemplar-based clustering algorithm that finds a set of data-points that best exemplify the data, and associates each datapoint with one exemplar. We extend affinity propagation in a principled way to solve the hierarchical clustering problem, which arises in a variety of domains including biology, sensor networks and decision making in operational research. We derive an inference algorithm that operates by propagating information up and down the hierarchy, and is efficient despite the high-order potentials required for the graphical model formulation. We demonstrate that our method outperforms greedy techniques that cluster one layer at a time. We show that on an artificial dataset designed to mimic the HIV-strain mutation dynamics, our method outperforms related methods. For real HIV sequences, where the ground truth is not available, we show our method achieves better results, in terms of the underlying objective function, and show the results correspond meaningfully to geographical location and strain subtypes. Finally we report results on using the method for the analysis of mass spectra, showing it performs favorably compared to state-of-the-art methods.", "text": "aﬃnity propagation exemplar-based clustering algorithm ﬁnds datapoints best exemplify data associates datapoint exemplar. extend aﬃnity propagation principled solve hierarchical clustering problem arises variety domains including biology sensor networks decision making operational research. derive inference algorithm operates propagating information hierarchy eﬃcient despite high-order potentials required graphical model formulation. demonstrate method outperforms greedy techniques cluster layer time. show artiﬁcial dataset designed mimic hiv-strain mutation dynamics method outperforms related methods. real sequences ground truth available show method achieves better results terms underlying objective function show results correspond meaningfully geographical location strain subtypes. finally report results using method analysis mass spectra showing performs favorably compared state-of-theart methods. exemplar-based clustering partitions data partition associated prototypical data point similarities datapoints exemplars exemplar points’ prior preference exemplars maximized. problems arise naturally diverse ﬁelds used visual scene analysis image segmentation analysis chemical-genetic interaction data vaccine design np-hard aﬃnity propagation algorithm used obtain good quality albeit approximate solutions. derived loopy max-sum algorithm factor-graph unlike metric-based methods aﬃnity propagation require pairwise negative similarities metrics applied successfully nonmetric spaces. unlike non-exemplar methods parametric nonparametric mixture models aﬃnity propagation arbitrarily complex similarity functions since need search integrate parameter space. example aﬃnity propagation used cluster ensembles proteins using jensen divergence pairwise similarity function. extend hierarchical structure data. appealing property hierarchical representation described extension standard representation ﬁnding exemplar hierarchy. representation layer chosen exemplars. example hierarchical image segmentation would consist tiny segments lowest layer successively larger segments higher layers corresponding objects. greedy approach ﬁrst described recursively apply exemplar found previous layer. demonstrate work greedy method suﬀers making hard decisions lower layers turn suboptimal higher layers examined since optimize global objective function accounts layers simultaneously. self) function enforces also enforces ‘exemplar consistency’ i.e. point must exemplar points choose exemplar. objective seen trading exemplar preferences pairwise similarities non-exemplars exemplars. thus preferences provide control knob total number clusters discovered. finding approximate solution nphard problem achieved running max-sum algorithm. shown number binary scalar operations needed iteration scales linearly number ﬁnite similarities. following sets messages calculated iteratively convergence move layers exemplar either remains exemplar chooses another exemplar exemplar relinquishing role cluster representative. since greedy layer layer solution incur lower preferences exemplar choices higher layers locally optimal globally suboptimal decisions made lower layers wish global solution. clustering objective function terms high-order factor-graph derive eﬃcient approximate loopy max-sum algorithms refer hierarchical aﬃnity propagation evaluate algorithm using diﬀerent types synthetic data show achieves better solutions greedy alternative constructs layer time. also demonstrate applicability biological tasks building hierarchies sequences identifying proteins mass spectra data. choosing point exemplar. goal select subset datapoints exemplars assign every non-exemplar point exactly exemplar maximize overall similarities points exemplars exemplar preferences. similarity thought intuitively negative euclidean distance datapoints need symmetric metric. note represented sjj; since explicitly preference variables indicates point chosen exemplar. following objective function expressed using additive factor graph shown dotted fig. using following function deﬁnitions modiﬁed function results following behavior point chosen exemplar layer point clustered layer alternatively point chosen exemplar layer must choose exemplar layer although explicitly require exemplars previous layers serve exemplars current layer implicitly guaranteed fact non-exemplar points layer constrained choose exemplar together exemplar consistency constraint allows point exemplar chosen itself. second diﬀerence manifested functions allow layer-speciﬁc pairwise similarities rest functions remain before appropriate layer superscripts. case standard restrictions form input pairwise similarities preferences. thus objective function wish maximize hierarchical stated clarity exposition focus formulation derivation hierarchical version problem. note problem highly related facility location problem well studied operational research problem. exemplars disjoint datapoints goal subset facilities utilize assign customer facility. relationship described addition possible deﬁne alternative objective function related models formulated modiﬁcations layer-speciﬁc constraints. property allows seamlessly ‘mix match’ layers exemplar-based clustering facility location principled manner deﬁning novel hierarchical clustering grouping tasks. full treatment given supp. material. similarly solutions problem described inferring approximate values hidden variables using max-sum algorithm. since function nodes layers within layers high-order naively seem require evaluation exponential number settings calculating maximization needed compute max-sum function-to-variable messages. however much like case single layer valid settings maximization restricted careful analysis valid settings shared computations allows compute functionto-variable messages eﬃciently resulting runtime observe larger number layers improvement greedy method pronounced improvements seven layers datapoints. further charts scatter plots show dataset size increases increasingly outperforms greedy method. observations consistent expectation hierarchical clustering problem becomes diﬃcult gains provided increase. small fraction cases converge appropriately greedy method achieves better solutions. practice good strategy methods pick solution better objective. experiments dataset found tends oscillate. therefore following procedure expedite runtime synthetic real data algorithm iterations variable assignment bottom-most layer ﬁxed repeat layers ﬁxed. procedure used toy-data rest experiments. also investigated alternative optimization strategy solving relaxation corresponding objective function. used provably convergent max-product like mplp algorithm found results poor comparison methods. additionally algorithm converges slowly. supp. material discussion experimental setup results. process evolution viewed stochastically generating hierarchy exemplars exemplar corresponds sequence. given population sequences useful task infer tree describes evolutionary relationships sequences. sec. hierarchically cluster sequences taken infected individuals. however since ground truth labelling available data ﬁrst explore synthetically generated dataset. starting single root sequence simulated evolution three generations form four-layer tree sequences. first randomly picked four-letter root sequence length generate children sequences ﬁrst sampled number children truncated geometric distribution mean then child begin constructing experiments synthetic data determine whether proposed method obtains better results terms objective function optimize compared greedy bottom-up counterpart used denoted ‘greedy’. examine applicability method various settings look varying numbers points layers. setting randomly generated increasing exemplar preferences every layer diﬀerent hierarchical datasets sampled top-down points top-most layer sampled standard normal distribution large standard deviation consequent layers sampled points previous layer considered exemplars. given exemplars layer’s points sampled randomly picking exemplars sampling point normal distribution centered around exemplar decreasing standard deviation total numbers points sampled every layer total overall roughly points across layers sampling bottom every layer contains twice many points previous layer. fig. report median percent improvement greedy method every conﬁguration aggregating diﬀerent datasets every conﬁguration show scatter plots experiments point represents result tofigure synthetic data comparison objective achieved greedy counterpart median percent improvement greedy given number layers used. bottom scatter plots similarity achieved v.s. greedy. experiments obtains better results greedy line. total percent settings outperforms greedy reported inset. color scatter-plot indicates number layers. generated drawing number mutations geometric distribution mean mutating parent sequence many randomly drawn positions. process repeated four-layer hierarchy obtained resulting total sequences. given generated sequences pairwise similarities obtained using data-generating mutation process logarithm truncated geometric distribution evaluated observed number mutations sequences. compare abilities greedy counterpart hierarchical version k-medians clustering hierarchical version k-means clustering task reconstructing hierarchy. hkmc works ﬁrst identifying exemplars lowest layer using k-medians clustering clustering resulting exemplars hkmeans algorithm ﬁnds lowest layer using k-means clustering post-process cluster means nearest points. applied diﬀerent algorithms four-layer hierarchy. greedy varied exemplar preferences obtain solutions diﬀering number exemplars diﬀerent layers; total settings used. hkmc hkmeans using total settings varying number clusters layer including ground-truth setting setting report result best random restarts using diﬀerent initializations median cluster center set. figure synthetic data precision-recall greedy hkmc hkmeans applied problem identifying ancestral sequences synthetic sequences. hkmc hkmeans plot best precision obtained unique recall value. evaluated reconstructed trees using methods. first plotted precision v.s. recall various clustering settings sequence-layer combination labelled positive sequence generated layer ground truth labelled negative otherwise. precision v.s. recall different methods plotted fig. results demonstrate outperforms methods experiments v.s. greedy. ﬁrst second third layers mean rand index v.s. greedy v.s. greedy v.s. greedy. since correct solution single ancestral exemplar layer calculated fraction experiments correctly identiﬁes single ancestor. successful cases whereas greedy successful cases. explore application unlabelled real sequences obtained human sequences los-alamos database. sequence pairwise similarity calculated using dnadist commonly used method aligning sequences allow gaps nonuniform nucleotide substitution. before diﬀerent clusterings obtained varying exemplar preferences. decrease layer index increases. obtained total diﬀerent settings. results experiments plotted using discs fig. demonstrate outperforms greedy method settings. ground truth known data results indicate ﬁnds better solutions terms sequence-alignment objective function. greatest improvement greedy occurs regime high exemplar preferences across layers. experimental settings correspond bottom-right area fig. possible explanation general objective function value layer decreases number datapoints decreases. grouping datapoints clusters ﬁnding fewer exemplars lower layers fewer datapoints need clustered higher layers. given high preferences might locally optimal layer many datapoints exemplars possible. however globally often better incur penalty clustering datapoints together lower layer fewer datapoints upper layers cluster. able globally better solutions. clear cases greedy method optimizes layer hierarchy independently poorly. figure synthetic data distribution rand index diﬀerent experiments using greedy. higher rand index indicates solution better resembles ground truth. experiments obtains better results greedy line. percentage solutions identiﬁed correct single ancestor sequence layer also reported. in-depth comparison greedy applied additional evaluation method using modiﬁed rand index supp. material) measures frequently pairs datapoints correctly classiﬁed siblings non-siblings hierarchical clustering algorithms given ground truth clustering. fig. compares greedy diﬀerent parameter settings plotting rand index values layer ground truth labelling. high rand index regime almost always performs better greedy. interesting considering layers able achieve signiﬁcantly higher rand indices bottom layer shown black triangles. mean rand index across layers this direct comparison per-parameter setting basis cannot replicated hkmc hkmeans since operate diﬀerent parameters namely number clusters ﬁnd. figure data comparison similarity achieved v.s. greedy v.s. hkmc variety exemplar preference settings. axes correspond similarity log-log scale. ﬁgures experiments obtains better results alternative method line. compare hkmc without availability ground truth labelling considered wide range clustering solutions found described above. solution determined number clusters layer hkmc using number clusters using random restarts. able compute similarity obtained hkmc solutions number clusters corresponding layers. shown blue boxes fig. outperforms hkmc almost experiments. investigate eﬀect varying exemplar preferences controlled manner ﬁxed preferences bottom three layers varied preferences layer. shown fig. top-layer exemplar preference decreases number clusters found layer tends decrease greedy. however connectivity layers change exemplar preference layer aﬀects number exemplars found every layer. greedy layer optimized independently top-layer effect number exemplars found lower layers. thus even many exemplars layer become undesirable greedy cannot correct adjusting lower layers. finally explored could used partition sequences interesting ways compared results information country origin sequence subtype. tuning parameter control knobs diﬀerent layers figure data distribution cluster sizes various exemplar preferences greedy exemplar preference layer adjusted others held ﬁxed. exemplar preferences decrease overall number clusters decreases. nonlinear eﬀect layers aﬀects layer greedy. used identify partitions highly stable. conﬁgurations least sensitive variations pairwise similarities computed thus likely biologically relevant. identiﬁed setting parameters corresponded exemplars sequence los-alamos database used extract country origin subtype categorization used researchers thought related evolutionary groupings fig. shows three thirdlayer clusters identiﬁed broken composition subtype country origin. ﬁrst clusters account data third cluster speciﬁc. mostly associated african strains predominantly type variants well type variant. compares well focused studies showing strains mostly restricted west-central african regions ﬁrst cluster groups many variant subtypes second cluster groups ‘b’‘c’ subtypes. color coding indicates diﬀerent clusters correspond mostly nonoverlapping sets subtypes. also note south american strains found second cluster. analysis subtypes found second layer solution available supp. materials. figure data third layer. percentage indicate number datapoints associated cluster datapoints clustered layer. subtype labels shown number datapoints associated subtype cluster larger demonstrate applicability used identify protein composition biological sample mass spectrometry data. proteins broken peptides mass spectra measured analyzed computationally infer identities proteins. however complex mixtures current state-of-the-art algorithms successfully small fraction mass spectra researchers investigating ways using prior knowledge protein groups improve detection identifying proteins viewed two-layer clustering problem mass spectra assigned proteins bottom layer proteins assigned protein clusters corresponding functional groups layer total score given mass spectra-protein scores protein cluster scores. include ‘dummy protein’ used collect false peptide predictions. proteins protein cluster scores extracted yeast gene functional network provides similarity scores pairs proteins based various sources data. scores normalized interval log-transformed. normalcompared method three state-of-the-art algorithms proteinprophet mspresso msnet used curated list proteins msnet ‘gold standard’ compared curve method curves methods. across wide range false positive rates achieves signiﬁcantly higher true positive rates proteinprophet mspresso performs similarly best method computational biology literature msnet performs slightly better reformulation problem facility location solved using message-passing. results demonstrate signiﬁcantly outperform state-of-the-art computational biology methods indicate general utility hap. formulated objective function described extended aﬃnity propagation algorithm hierarchical exemplar-based clustering. achieved generalizing factor graph corresponding algorithm used derive single-layer aﬃnity propagation algorithm. method called hierarchical aﬃnity propagation approximately maximizes natural objective function. using message passing method algorithm able consider points potential exemplars diﬀerent layers hierarchy send information hierarchy identify exemplars layer. able outperform methods message passing build hierarchy layer layer techniques make kmedians k-means clustering random restarts. small number cases fails properly converge sensible strategy apply greedy method pick best objective function. also extended facility location problem derived general framework constructing hierarchies layer encoded either facility location exemplar-based clustering problem. addition found worked well applied real-world problems ﬁeld computational biology including analysis sequences protein identiﬁcation. interesting direction future research apply problems domains hierarchical shape matching multihop wireless sensor networks (manjeshwar agrawal main goal experimental analysis show given sensible optimization function outperforms variants terms objective function value clustering metrics ground truth available across wide range parameter settings. thus makes algorithm choice problems. question come good parameters investigated part fig. well choice particular result analyze fig. based stability clustering properties number clusters layer relative number clusters layer reasonably spread modiﬁed exemplar preference parameters. investigations problem explicit parameter setting optimization ongoing research direction akin similar issues arise clustering hierarchical models hierarchical agglomerative clustering known hard unsupervised clustering setting.", "year": 2012}