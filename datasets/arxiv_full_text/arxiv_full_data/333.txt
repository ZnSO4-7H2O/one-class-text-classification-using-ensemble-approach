{"title": "The Symmetry of a Simple Optimization Problem in Lasso Screening", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds.", "text": "today’s ever increasing size data makes solving slower loading entire data memory problematic ﬁrst place. places demand improving effectiveness screening turn relies tight region bound studies suggest tighter screening algorithm reject features. simple obtain tighter imposing larger empirical studies shown increasing number hyperplane constraints improves rejection rate. instance shows moves rejection percentage increases mnist dataset yalebxf dataset target λ/λmax likely increasing screening performance boosted. finding half space constraints problem. borrowing similar ideas previous works half spaces codeword constraints dual problem greedy fashion solutions previous solved instances sequential screening scheme however problem larger potential computation cost. clean closed-form solution unlikely. even closed-form solution already complicated. larger might eventually resort numerical solutions solving optimization problem high data dimension complex region overhead might compromise beneﬁts screening. thus interest study properties hope simplifying solving optimization problem analysis paper shows solution function projection features onto subspace recently dictionary screening proposed effective improve computational efﬁciency solving lasso problem commonly used method learning sparse representations. address today’s ever increasing large dataset effective screening relies tight region bound solution dual lasso. typical region bounds form intersection sphere multiple half spaces. tighten region bound using half spaces however adds overhead solving high dimensional optimization problem lasso screening. paper reveals interesting property optimization problem depends projection features onto subspace spanned normals half spaces. property converts optimization problem high dimension much lower dimension thus sheds light reducing computation overhead lasso screening based tighter region bounds. despite efﬁcient algorithms solving exists scalability large datasets remains major problem. dictionary screening lasso proposed address computational issue given target vector dictionary screening identiﬁes subset features features removed dictionary smaller lasso problem solved obtain solution original problem. signiﬁcantly reduce size dictionary loaded memory make ﬁnding lasso solution faster. spanned normals half spaces shreds light reducing optimization problem dimension practical implications considering scale dimension reduction usually ranges scale hundreds hundred thousands current less vector speciﬁes linear objective function parameters specify spherical bound half space constraints feasible points problem respectively. using change variable problem simpliﬁed problem parameterized pair speciﬁes dome axes vector speciﬁes respective sizes. denote group real orthogonal matrices. transform parameters problem maps maps change second constraint problem becomes thus rotates problem solution problem obtained ﬁrst inverse rotating computing intuitively obvious. indicates must determined function problem parameters invariant orthogonal transformations. invariant must depend function invariant simplest nontrivial function property reasonable expect function depends entries determines inter-dome conﬁguration overall orientation speciﬁes respective dome sizes. symmetry group subgroup orthogonal group property hence lemma subset symmetry group speciﬁc cases strict subset symmetries among domes example consider case domes identical except centered along along reﬂection hyperplane formed perpendicular bisector line joining maps vice versa. thus symmetry leave invariant hence hand symmetry construct below. columns rn×m form orthonormal basis select orthogonal matrix. similarly select orthogonal matrix. since orthogonal moreover finally using fact georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee transactions pattern analysis machine intelligence vol. paper studied simple optimization problem lasso screening. n-dimension optimization problem linear objective function feasible intersection spherical region half spaces. incorporating half spaces gives hope stronger screening performance meantime increase computational cost screening. analysis paper demonstrates optimization problem function projection feature onto subspace spanned normals half spaces. result reduces dimension problem reduction several orders magnitude. simpliﬁed problem form original problem linear objective linear quadratic constraints implies dimensionality reduction lead reduction computational cost. sheds light improving effectiveness screening using tighter region bound time keeping computational cost bay.", "year": 2016}