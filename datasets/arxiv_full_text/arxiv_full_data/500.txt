{"title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware  Ones", "tag": ["cs.CL", "cs.NE", "stat.ML", "68T50", "I.2.7"], "abstract": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.", "text": "much research done subword-level subword-aware neural language modeling subwords characters morphemes however much work done syllable-level syllable-aware nlm. mikolov show subword-level language models outperform character-level ones. keep frequent words untouched split words syllable-like units. approach differs mainly following aspects make predictions word level linguistically sound syllabiﬁcation algorithm consider variety advanced neural architectures. recently come across concurrent paper authors systematically compare different subword units morphemes) different representation models languages various morphological typology. however consider syllables experiment relatively small models small data sets subword-level rely subword-level inputs make predictions level subwords; subword-aware also rely subword-level inputs make predictions level words. syllabiﬁcation seem improve word-level language modeling quality compared characterbased segmentation. however best syllable-aware language model achieving performance comparable competitive character-aware model fewer parameters trained times faster. recent advances neural language modeling connected character-aware models promising approach propose following direction related would like make sure pursuit ﬁne-grained representations missed possible intermediate ways segmentation e.g. syllables. syllables opinion better supported linguistic units language single characters. languages words naturally split syllables based observation attempted determine whether syllable-aware advantages character-aware nlm. experimented variety models could evidence support hypothesis splitting words syllables seem improve language modeling quality compared splitting characters. however positive ﬁndings best syllable-aware syllable-aware word embeddings ﬁnite vocabularies words syllables respectively. assume words syllables already converted indices. r|s|×ds embedding matrix syllables i.e. matrix corresponds embedding syllable word sequence syllables hence represented sequence corresponding syllable vectors question shall pack sequence single vector produce better embedding word case better means better character-aware embedding char-cnn model present several viable approaches. state vector assumed contain information whole sequence therefore used word embedding variety transformations choose however recent thorough evaluation shows lstm forget bias initialized outperforms popular architectures almost tasks decided experiments. refer model syl-lstm. convolutional model inspired recent work character-aware neural language models decided approach syllables. case differs mainly following aspects syllables usually bigger characters also dimensionality syllable vectors expected greater dimensionality character vectors. factors result allocating parameters syllable embeddings compared character embeddings. average word contains fewer syllables characters therefore need narrower convolutional ﬁlters syllables. results spending fewer parameters convolution. like char-cnn syllable-aware model referred syl-cnn- utilizes maxpooling highway layers model interactions syllables. dimensionality highway layer denoted dhw. approach used botha blunsom combine word morpheme embeddings single word vector. syl-avg simple average syllable vectors obtained setting /nw. also called continuous syllables analogy cbow model vectors neighboring words averaged word embedding current word. syl-avg-a weights function parameters model jointly trained together parameters. maxw{nw} maximum word length syllables. order weighted average apply softmax normalization rds×n parameters determine importance syllable type position bias conditioned relative position. approach motivated recent work using attention mechanism cbow model mentioned section huge variety architectures choose from. advanced recurrent neural architectures time writing recurrent highway networks novel model obtained neural architecture search reinforcement learning models spiced recent regularization techniques rnns reach state-of-theart. however make results directly comparable select two-layer lstm regularize zaremba search best model steps ﬁrst block word-level lstm’s architecture pre-select three best models small parameter budget tune three best models’ hyperparameters larger budget pre-selection units layer syllable-aware word embedding method section english data keeping total parameter budget architectural choices speciﬁed appendix hyperparameter tuning hyperparameters three best-performing models preselection step thoroughly tuned english data random search according marginal distributions restriction dlm. total parameter budget kept allow easy comparison results three best models trained evaluated small medium-sized data sets languages. number parameters. therefore follows make detailed comparison sylconcat char-cnn. shared errors interesting whether char-cnn syl-concat making similar errors. model gives error assigns probability less correct word test set. figure shows percentage errors shared syl-concat charcnn depending value vast majority errors shared models even small breakdown token frequency char-cnn outperforms syl-concat partition test sets token frequency computed training data. observe figure that average frequent word bigger advantage char-cnn syl-concat. char-cnn sees word different contexts learn word sylconcat hand limitations cansee syllables prevents extracting amount knowledge word. optimizaton performed almost work zaremba appendix details. syllabiﬁcation true syllabiﬁcation word requires grapheme-to-phoneme conversion splitting syllables based rules. since always available lessresourced languages decided utilize liang’s widely-used hyphenation algorithm results pre-selection reported table syllable-aware models comfortably outperform char-cnn budget limited parameters. surprisingly pure word-level model lstm-word also beats character-aware budget. three best conﬁgurations syl-concat syl-sum syl-cnn- tuning hyperparameters parameter budget gives architectures table results evaluating three models small medium-sized data sets char-cnn different languages provided table models demonstrate similar performance small data char-cnn scales signiﬁcantly better medium-sized data. three syllable-aware models syl-concat looks advantageous demonstrates stable results least when words directly embedded lagus default conﬁguration training data used instead syllabiﬁer models. interestingly unique morphemes whereas number unique syllables trained models parameter budget keeping state size word-level lstm reduction number subword types allowed give higher dimensionality additive models performed better others test ppls respectively. limited amount time perform thorough hyperparameter search budget. instead conﬁgurations morphcnn- conﬁgurations morph-sum hyperparameters close those optimal syl-cnn- syl-sum correspondingly. told best morpheme-aware model morph-sum test practically result best syllable-aware model syl-concat makes morph-sum notable alternative charcnn syl-concat defer thorough study future work. source code source code models discussed paper available https//github.com/zhnis/lstm-syl. seems syllable-aware language models fail outperform competitive character-aware ones. however usage syllabiﬁcation reduce total number parameters increase training speed albeit expense languagedependent preprocessing. morphological segmentation noteworthy alternative syllabiﬁcation simple morpheme-aware model sums morpheme embeddings looks promising study deferred future work. word embeddings intrinsic advantage char-cnn syl-concat also supported following experiment took word embeddings produced models english applied them. regardless threshold percentage variance retain embeddings char-cnn always principal components embeddings syl-concat means char-cnn embeds words higher dimensional space syl-concat thus better distinguish different contexts. lstm limitations hyperparameters tuning noticed increasing optimal values result better performance syl-concat. could limitations word-level lstm whether case replaced lstm variational resulted signiﬁcant reduction perplexities char-cnn syl-concat moreover increasing result better performance syl-concat. optimization details given appendix comparing syllable morpheme embeddings interesting compare morphemes syllables. trained morfessor convolutional ﬁlter widths corresponding convolul= experimented corresponding values chosen total parameter budget. activation tanh. linear combinations give higher dimensionality syllable vectors since resulting word vector size syllable vectors models except syl-avg-b syl-concat lstm-based models perform training truncated bptt backpropagate time steps data-s time steps data-l using stochastic gradient descent learning rate initially halved perplexity decrease validation epoch. batch sizes data-s data-l. train epochs data-s epochs data-l picking best-performing model validation set. parameters models randomly initialized uniformly except forget bias word-level lstm initialized regularization dropout probability wordlevel lstm layers hidden-to-output softmax layer. clip norm gradients choices guided previous work wordlevel language modeling lstms shows outperformed sampled softmax europarl corpus data-l derived rhn-based models optimized zilly except unrolled networks time steps truncated bptt dropout rates chosen follows embedding layer input gates hidden units output activations. gratefully acknowledge nvidia corporation donation titan pascal used research. work bagdat myrzakhmetov funded committee science ministry education science republic kazakhstan under targeted program authors would like thank anonymous reviewers aibek makazhanov valuable feedback makat tlebaliyev dmitriy polynin support yoon providing preprocessed datasets. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research wang ling chris dyer alan black isabel trancoso ramon fermandez silvio amir luis marujo tiago luis. finding function form compositional character models open vocabproceedings ulary word representation. emnlp.", "year": 2017}