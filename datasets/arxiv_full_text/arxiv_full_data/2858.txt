{"title": "Essentially No Barriers in Neural Network Energy Landscape", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.", "text": "training neural networks involves ﬁnding minima high-dimensional non-convex loss function. knowledge structure energy landscape sparse. relaxing linear interpolations construct continuous paths minima recent neural network architectures cifar cifar. surprisingly paths essentially training test landscapes. implies neural networks enough capacity structural changes changes small minima. also minimum least vanishing hessian eigenvalue addition resulting trivial invariance. work argue neural network loss minima isolated points parameter space essentially form connected manifold. precisely part parameter space loss remains certain threshold forms single connected component. support claim studying energy landscape several resnets densenets cifar cifar pairs minima construct continuous paths parameter space loss remains close value found directly minima. example path shown figure neural networks achieved remarkable success practical applications object recognition machine translation speech recognition etc. theoretical insights neural networks trained successfully despite high-dimensional non-convex loss functions based strong assumptions eigenvalues hessian critical points random linear activations wide hidden layers current literature minima loss function typically depicted points bottom valley certain width reﬂects generalisation network parameters given location minimum also picture obtained heidelberg collaboratory image processing university heidelberg heidelberg germany institut theoretische physik university heidelberg heidelberg germany. correspondence felix draxler <felix.draxleriwr.uni-heidelberg.de>. figure left slice million-dimensional training loss function densenet-- cifar minimum energy path found method. plane spanned minima mean nodes path. right loss along linear line segment minima along high-dimensional path. surprisingly energy along path essentially ﬂat. abundance paths suggests modern neural networks enough parameters achieve good predictions part network undergoes structural changes. closing offer qualitative justiﬁcation behaviour offer handle future theoretical investigation. discussions neural networks generalise despite extremely large number parameters often ﬁnds argument wide minima generalise better picture conﬁrmed visualising parameter space random plane around minimum draw completely different image loss landscape minima located ﬁnite-width valleys paths parameter space along loss remains close value minima. previously argued minima networks relu activations strictly valleys scale parameters layer constant following layer without changing output network. here provide different class valley construct paths independent minima essentially ﬂat. showed local minima connected without large barriers mnist next word prediction. cifar however found signiﬁcant barriers minima considered. extend work ways first consider resnets densenets outperform plain cnns large margin. second apply state method connecting minima molecular statistical mechanics automated nudged elastic band algorithm based nudged elastic band algorithm additionally systematically replace paths unnaturally high loss barrier. combining paths essentially energy barrier. applied multi-layer perceptron single hidden layer high energy barriers minima network found using three hidden neurons disappeared upon adding neurons hidden layer. follow-up trained multi-layer perceptron single hidden layer mnist. found l-regularisation landscape signiﬁcant energy barriers. however network report error rate higher achieved even linear classiﬁer achieved standard work apply autoneb nontrivial network ﬁrst time make surprising observation different minima state networks cifar cifar connected essentially paths. submission work international machine learning conference independently reported also constructed paths between neural network minima. study loss landscape several architectures cifar cifar report surprising observation minima connected paths constantly loss. neural network loss function depends architecture training network parameters keeping former ﬁxed simply write start parameter sets case minima loss function i.e. result training networks convergence. goal continuous path parameter space lowest maximum loss optimisation tractable loss function must sufﬁciently smooth i.e. contain jumps along path. output loss neural networks continuous functions parameters derivative discontinuous case relu activations. however cannot give bounds steep loss function address problem sampling paths densely. lowest path called minimum energy path refer parameter maximum loss path saddle point path true saddle point loss function. possible present day’s neural networks parameter spaces millions dimensions. thus must resort methods construct approximation points using local heuristics. particular resort automated nudged elastic band algorithm method based nudged elastic band algorithm figure dimensional loss surface minima connected minimum energy path nudged elastic band iteration converged. construction update pivot. tangent points neighbouring pivot higher energy. re-distribution acts parallelly loss force perpendicularly tangent. formulation high energy pivots longer slide down saddle point. spring force redistributes pivots path straighten pivots spaced unequally introducing target distances unequal spring constants equation claim wide range leads result given loss surface. however chosen large optimisation become unstable. small excessive number iterations needed pivots become equally distributed. value worked well across different loss surfaces number pivots instead re-distribute pivots iteration actual spring force zero. loss force still restricted parallelly path. literature sometimes referred string method mechanical model chain pivots connected springs stiffness initial ﬁnal pivots ﬁxed minima connect i.e. using gradient descent path minimises following energy function found problem energy formulation lies choice spring constant hand small distances pivots become larger areas high energy. however identifying highest point path energy goal algorithm sampling rate high high-energy regions. hand chosen large becomes energetically advantageous shorten hence straighten path spring energy grows quadratically total length path. cuts corners loss surface resulting path miss saddle point. nudged elastic band inspired model presented nudged elastic band brevity directly present improved version force resulting equation consists force derived loss force originating springs bends straight line segment applying gradient forces gradients perpendicular path. then highest point resulting path critical point. critical point necessarily saddle point looking gives upper bound energy saddle point. algorithm shows initial path iteratively updated using forces. companion figure visualises forces update step dimensional example. formulation gradient descent update path. gradient based optimiser used. typically introduces additional hyperparameters example learning rate number iterations chosen large enough optimisation converge. number pivots trades computational effort hand subsampling artefacts hand. neural networks known sampling density needed traversing parameter space. adaptive procedure inserts pivots needed autoneb automated nudged elastic band wraps algorithm initially runs small number iterations checked current pivots sufﬁcient accurately sample path. case pivots added locations estimated path requires accuracy. criterion pivots inserted true loss values deviate linear interpolation neighbouring pivot pair larger certain threshold visualised figure requires handling unequal spaces pivots. autoneb guaranteed true mep. instead stuck local minimum energy paths means saddle point energies reported autoneb upper bound unknown minimal saddle point losses. figure items inserted cycle autoneb true energy interpolated position points rises high compared interpolated energy. pivot inserted. difference small enough additional pivot needed. good news graph minima local meps ultrametric property suppose local meps minimum known. call pbc. respective saddle point energies give upper bound true saddle point energies figure overview algorithm examples ﬁrst nine iterations later iteration. first minima connected particular minimum. then autoneb computes local meps circumvent worst local meps minimum spanning tree. repeated local meps known pairs minima procedure stopped early. whenever algorithm stops upper bound pair minima available minimum spanning tree. soon minima computed local meps form connected graph upper bounds saddle energies available. hence quickly upper bounds pairs minima connecting minimum others. autoneb ﬁnds local addressed computing paths pairs minima. soon lower path found concatenating paths local removed. means local paths easily corrected for. evaluate saddle point energies computed local meps ignore paths higher energy concatenation paths lower maximal energy. lowest local meps form minimum spanning tree available graph minimum spanning tree found efﬁciently e.g. using kruskal’s algorithm. procedure suggests tuples minima local meps known pairs minima. since running autoneb computationally expensive stop iteration minimum spanning tree contains similar saddle point energies. connect minima different resnets densenets image classiﬁcation tasks cifar cifar. train several instances network distinct random initialisations following instructions original literature. connect pairs minima using autoneb. report average cross-entropy loss misclassiﬁcation rates full training test data minima found. ﬁnal evaluation reduce saddle points minimum spanning tree corresponding training loss weight. figure typical snapshots loss along path autoneb cycles resnet- cifar ﬁrst cycle typically corners cut. pivots inserted high loss values second third pivot. four cycles high learning rate highest loss path reduced factor ﬁve. pivots energy regions attribute high learning rate ﬁrst round learning rate reduces energy another factor two. cycles major energy bumps exist pivots procedure converged. cycle pivots inserted positions loss exceeds energy estimated linear interpolation pivots least compared total energy difference along path. comparing total loss difference prioritises errors beneﬁcial additional pivot implies loss evaluations iteration. energy evaluated points pair neighbouring pivots. procedure autoneb cycles conﬁgured exactly resnets except batch sizes before train minima apply connection procedure algorithm major improvements made. figure comparison minimum saddle point loss training test set. cifar left cifar right. pair points represents average loss minima corresponding mean saddle point loss connected straight line. lower evaluated training higher losses test set. training saddle point loss small compared test set. test points minima saddle points close. table quantitative results. min. denotes average value minima. saddle point values maximum value metric along ﬁnal path computed results averaged. epoch measured point loss falls saddle point loss ﬁrst time. noted bold belongs third part training learning rate resnets trained epochs densenets epochs. average loss untrained network. cross-entropy loss cifar cifar. saddle point energies training sets orders magnitude smaller loss initialisation network. saddle point energies cifar order magnitude smaller average minimum energy test set. cifar saddle point energies resnets smaller third value test set. densenets least order magnitude smaller. loss saddle points times large mean loss minima. ratios noisy denominator approach zero network data perfectly keep mind reading reported factor saddle point energies minima. test error rate saddle point gives intuition much information lost saddle point. resnets error rises maximally cifar cifar. densenets error rises cifar cifar. differences small compared error rate minima. also measure epoch loss crosses saddle point loss training listed table procedure visualised densenet--bc resnet cifar figure learning curve fall saddle point energy learning figure learning curve densenet---bc resnet- cifar. training loss passes mean saddle point energy epochs training respectively epochs among architectures considered average saddle point crosses training loss last densenet--bc earliest resnet- cifar. crossing densenet identiﬁed plot. pointed intriguing property loss surface current-day deep networks upper-bounding saddle points parameter sets result stochastic gradient descent a.k.a. minima. empirical upper bounds astonishingly close loss minima themselves. point cannot give formal characterization regime ﬁnding holds. consider textbook example two-layer perceptron problem. neurons traditionally used ﬁrst hidden layer let’s call alice shown figure obtain equivalent network exchanging alice network also corresponding minimum loss surface shown figure path minima entail parameter sets figure incur high loss. hand introduce auxiliary neuron charlie play small choreography enter charlie. charlie stands bob. transitions alice’s role. alice takes charlie. exit charlie. neuron second hidden layer adjusts weights disregard output neuron-in-transition entire network incurs higher loss original minima. constructed perfect minimum energy path. loss surface deep neural networks contains paths constantly loss. forth closely related arguments above. hold network extra capacity degrees freedom spare. empirically seems case modernday architectures applied standard problems. argue width layer network heavily replace parameters producing output loss. method opens door empirical research energy landscape neural networks. hyperparameters autoneb reﬁned expect even lower paths level true saddle points recovered. interesting certain minima higher barrier others. makes possible recursively form clusters minima i.e. using single-linkage clustering. analysis possible large error bars ﬁnd. traditional energy landscape literature kind clustering done disconnectivity graphs practical applications imagine using resulting paths large ensemble neural networks especially given observe practically lower test loss along path. figure network capacity dataset continuous transition minimum another minimum possible without misclassifying least instance. adding helper neuron makes transition possible always predicting right class data points i.e. turning outgoing weight bob. formal proof also complicated fact loss surface function parameters architecture also training set; distribution real-world structured data images sentences lend compact mathematical representation. said want make related arguments help explain observe substantial barrier minima. state neural networks dozens hundreds neurons channels layer skip connections non-adjacent layers. assume training parameter loss identiﬁed. perturb single parameter adding small constant leave others free adapt change still minimise loss argued adjusting somewhat myriad parameters make change imposed them. relaxation procedure argument repeated though possibly perturbation different parameter. type resilience exploited encouraged procedures dropout ensembling also reason neural networks greatly condensed substantial increase loss occurs ballard andrew stevenson jacob ritankar wales david energy landscapes machine learning application series data. chem. phys. issn http//dx.doi.org/./ ciresan meier ueli masci jonathan maria gambardella luca schmidhuber jürgen. flexible high performance convolutional neural networks image classiﬁcation. ijcai proceedings-international joint conference artiﬁcial intelligence volume barcelona spain dauphin yann pascanu razvan gülçehre çaglar kyunghyun ganguli surya bengio yoshua. identifying attacking saddle point problem high-dimensional non-convex optimization. corr abs/. http//arxiv.org/ abs/.. dinh laurent pascanu razvan bengio samy bengio yoshua. sharp minima generalize deep nets. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/dinhb.html. gower ross minimum spanning trees single linkage cluster analysis. journal royal statistical society. series issn http //www.jstor.org/stable/. graves alex mohamed abdel-rahman hinton geoffrey. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition henkelman graeme jónsson hannes. improved tangent estimate nudged elastic band method ﬁnding minimum energy paths saddle points. journal chemical physics hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine huang zhuang weinberger kilian maaten laurens. densely connected convolutional networks. proceedings ieee conference computer vision pattern recognition volume jónsson hannes mills greg jacobsen karsten nudged elastic band method ﬁnding minimum energy paths transitions. classical quantum dynamics condensed phase simulations world scientiﬁc keskar nitish shirish mudigere dheevatsa nocedal jorge smelyanskiy mikhail tang ping peter. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. zhuang jianguo shen zhiqiang huang shoumeng zhang changshui. learning efﬁcient convolutional networks network slimming. proceedings ieee conference computer vision pattern recognition montúfar guido pascanu razvan kyunghyun bengio yoshua. number linear regions deep neural networks. advances neural information processing systems sheppard daniel terrell henkelman graeme. optimization methods ﬁnding minimum energy paths. journal chemical physics issn ./.. http//dx.doi.org/./.. srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research xiong droppo huang seide seltzer stolcke zweig toward human parieee/acm conversational speech recognition. transactions audio speech language processing issn ./taslp...", "year": 2018}