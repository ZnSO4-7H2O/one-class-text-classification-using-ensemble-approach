{"title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity --- the Fisher-Rao norm --- that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.", "text": "study relationship geometry capacity measures deep neural networks invariance viewpoint. introduce notion capacity fisher-rao norm possesses desirable invariance properties motivated information geometry. discover analytical characterization capacity measure establish norm-comparison inequalities show measure serves umbrella several existing norm-based complexity measures. discuss upper bounds generalization error induced proposed measure. extensive numerical experiments cifar- support theoretical ﬁndings. theoretical analysis rests structural lemma partial derivatives multi-layer rectiﬁer networks. words phrases deep learning statistical learning theory information geometry fisher-rao metric invariance relu activation natural gradient capacity control generalization error. beyond remarkable representation memorization ability deep neural networks empirically perform well out-of-sample prediction. intriguing out-of-sample generalization property poses fundamental theoretical questions paper approach generalization question deep neural networks geometric invariance vantage point. motivation behind invariance twofold speciﬁc parametrization neural network arbitrary impact generalization power. pointed example many continuous operations parameters relu nets result exactly prediction thus generalization depend equivalence class obtained identifying parameters transformations. although ﬂatness loss function linked generalization existing deﬁnitions ﬂatness neither invariant nodewise re-scalings relu nets general coordinate transformations parameter space calls question utility describing generalization. thus natural argue purely geometric characterization generalization invariant aforementioned transformations additionally resolves conﬂict minima requirement invariance. information geometry concerned study geometric invariances arising space probability distributions leverage motivate particular geometric notion complexity fisher-rao norm. algorithmic point view steepest descent induced geometry precisely natural gradient generalization viewpoint fisher-rao norm naturally incorporates distributional aspects data harmoniously unites elements ﬂatness norm argued crucial explaining generalization statistical learning theory equips many tools analyze out-of-sample performance. vapnik-chervonenkis dimension possible complexity notion large explain generalization over-parametrized models since scales size network. contrast additional distributional assumptions margin perceptron enjoys dimension-free error guarantee norm playing role capacity. observations theory large-margin classiﬁers applied kernel methods boosting neural networks particular analysis koltchinskii panchenko combines empirical margin distribution rademacher complexity restricted subset functions. turn raises capacity control question good notion restrictive subset parameter space neural networks? norm-based capacity control provides possible answer actively studied deep networks invariances always reﬂected capacity notions. general diﬃcult answer question capacity measure superior. nevertheless show proposed fisher-rao norm serves umbrella previously considered norm-based capacity measures appears shed light possible answers question. norm proved identity partial derivatives neural network appears open door geometric analysis. particular prove stationary point empirical objective hinge loss perfectly separates data must also large margin. automatic large-margin property stationary points link algorithmic facet problem generalization property. identity gives handle fisher-rao norm allows prove number facts since expect identity useful deep network analysis start stating result implications next section. section introduce fisher-rao norm establish norm-comparison inequalities serves umbrella existing norm-based measures capacity. using norm-comparison inequalities bound generalization error various geometrically distinct subsets fisher-rao ball provide rigorous proof generalization deep linear networks. extensive numerical experiments performed section demonstrating superior properties fisher-rao norm. hidden node denoted otpxq corresponding input value tpxq otpxq σtpn tpxqq. deﬁnition opxq ﬁnal output ol`pxq fθpxq subscript denotes given loss function statistical learning problem ﬁrst establish following structural result neural networks. clear later sections lemma motivated study fisher-rao norm formally deﬁned eqn. below information geometry. moment however provide diﬀerent viewpoint. linear lemma reveals structural constraints gradients rectiﬁed networks. particular even though gradients over-parametrized highdimensional space many equality constraints induced network architecture. unveil surprising connection lemma proposed fisher-rao norm take look immediate corollaries result. ﬁrst corollary establishes large-margin property stationary points separate data. remark simple lemma quite asserting stationary points global optima since global optima satisfy xwpθq´xt proved stationary points satisfy xwpθq xwpθq section propose notion complexity neural networks motivated geometrical invariance considerations speciﬁcally fisherrao metric information geometry. postpone motivation section instead start deﬁnition properties. detailed comparison known norm-based capacity measures generalization results delayed section underlying distribution expectation deﬁnition left ambiguous useful specialize diﬀerent distributions depending context. even though call quantity fisherrao norm noted satisfy triangle inequality. following theorem unveils surprising identity fisher-rao norm. illustrating explicit formula theorem viewed uniﬁed umbrella many known norm-based capacity measures point simple invariance property fisher-rao norm follows direct consequence thm. property satisﬁed norm spectral norm path norm group norm. section employ theorem reveal relationship among diﬀerent norms corresponding geometries. norm-based capacity control active ﬁeld research understanding deep learning generalizes well including norm path norm group-norm spectral norm norms closely related fisher-rao norm despite fact capture distinct inductive biases diﬀerent geometries. argument readily adopted general setting. show fisher-rao norm serves lower bound norms considered literature pre-factor whose meaning clear section addition fisher-rao norm enjoys interesting umbrella property considering constrained geometry fisher-rao norm motivates norm-based capacity control methods. parametric family indexed parameters neural network architecture. fisher-rao metric tpθu deﬁned terms local inner product value follows. deﬁne corresponding tangent vectors dpθ`tα{dt|t dpθ`tβ{dt|t. deﬁne local inner product inner product extends riemannian metric space positive densities probpmq called fisher-rao metric. ipθq statistics literature follows identity ¯βypθ xαipθqβy notice fisher information matrix induces semi -inner product xαipθqβy unlike fisher-rao metric non-degenerate. make additional modeling assumption pθpx ppxqpθpy fisher identiﬁed geometric origin fisher-rao norm study implications generalization minima. dinh argued counter-example existing measures ﬂatness inadequate explaining generalization capability multi-layer neural networks. speciﬁcally utilizing invariance property multi-layer rectiﬁed networks nonnegative nodewise rescalings proved hessian eigenvalues loss function made arbitrarily large thereby weakening connection minima generalization. also identiﬁed general problem aﬄicts hessian-based measures generalization network architecture activation function hessian sensitive network parametrization whereas generalization invariant general coordinate transformations. proposal motivated following fact relates ﬂatness geometry words fisher-rao norm evades node-wise rescaling issue because exactly invariant linear re-parametrizations. fisher-rao norm moreover possesses inﬁnitesimal invariance property non-linear coordinate transformations seen passing inﬁnitesimal form non-linear coordinate invariance realized exactly following inﬁnitesimal line element fisher-rao norm approximate geodesic distance origin. important realize deﬁnition ﬂatness diﬀers employed hessian loss hessθ unlike fisher-rao norm norm induced hessian loss enjoy inﬁnitesimal invariance property emphasize invariance property natural gradient re-parametrization approximate invariance property over-parametrization satisﬁed classic gradient descent. formal statement proof deferred lemma section invariance property desirable pfθpxq pθpy|xq recall fact fisher information viewed multi-layer relu networks many equivalent re-parametrizations problem nodewise rescalings slow optimization process. advantage natural gradient also illustrated empirically section |vi|pq{p vector vector norm denoted }v}p matrix }m}σ maxv‰ m}{}v} denotes spectral norm; }m}pñq maxv‰ m}q{}v}p denotes matrix induced norm }m}pq remark spectral norm capacity control considered lemma shows spectral norm serves stringent constraint fisher-rao norm. provide explanation pre-factor induced fisher-rao norm geometry remark group norm capacity measure considered lemma shows group norm serves stringent constraint fisher-rao norm. again provide explanation pre-factor section investigate generalization puzzle deep learning lens fisher-rao norm. ﬁrst introduce rigorous proof case multi-layer linear networks capacity control fisher-rao norm ensures good generalization. provide heuristic argument fisher-rao norm seems right norm-based capacity control rectiﬁed neural networks norm caparison section complement heuristic argument extensive numerical investigations section remark combining theorem classic symmetrization margin bounds deduce binary classiﬁcation following generalization guarantee holds dependent factor. surprising fact despite distinct geometry subsets b}¨}σ b}¨}pq b}πp¨q} rademacher complexity sets depend enveloping fisher-rao norm explicitly without either intriguing combinatorial factor dependent factor. believe envelope property sheds light compare diﬀerent norm-based capacity measures. concluding section present contour plot fisher-rao norm path- norm simple layer relu network fig. better illustrate geometry fisher-rao norm subset induced norm. choose weights y-axis plot levelsets norms. k-dimensional output layer network focus relu activation σpxq maxt intermediate layers. loss function taken cross entropy ´xey gpyqy denotes one-hot-encoded class label gpzq softmax function deﬁned cross-entropy loss cifar- image classiﬁcation dataset explicit regularization data augmentation. cross-entropy loss optimized using epochs minibatch gradient descent utilizing minibatches size otherwise identical experimental conditions described experiment repeated using minibatch natural gradient descent employing kronecker-factored approximate curvature method learning rate momentum schedules. ﬁrst fact observe fisher-rao norm remains approximately constant network overparametrized increasing width model distribution. self-normalizing property understood consequence relationship ﬂatness discussed section holds expectation taken respect model. over-parametrized neural networks tend exhibit good generalization despite perfectly ﬁtting training order pinpoint correct notion complexity drives generalization error conducted series experiments changed network size signal-tonoise ratio datasets. particular focus neural architectures tandem generalization error moreover correlation seems persist vary label randomization. overall fisher-rao norm distinguished measures capacity fact empirical version seems track generalization moreover trend appear sensitive choice optimization. exist. finally note unlike vanilla gradient natural gradient diﬀerentiates diﬀerent architectures fisher-rao norm. although don’t completely understand phenomenon likely consequence fact natural gradient iteratively minimizing fisher-rao semi-norm. bartlett adopted margin story explain generalization. investigated spectrally-normalized margin explain cifar- random labels harder dataset uncorrupted cifar- adopt idea experiment plot margin normalized empirical fisher-rao norm comparison spectral norm based model trained either vanilla gradient natural gradient. seen fig. fisher-rao-normalized margin also accounts generalization random original cifar. addition table shows empirical fisher-rao norm improves normalized margin relative spectral norm. results obtained optimizing natural gradient sensitive choice optimizer. shown multi-layer networks struggle learn certain piecewise-linear curves problem instances poorlyconditioned. failure attributed fact simply using black-box model without deeper analytical understanding problem structure could computationally sub-optimal. results suggest problem overcome within conﬁnes black-box optimization using natural gradient. words natural gradient automatically pre-conditions problem appears achieve similar performance attained hard-coded convolutions within number iterations dependence capacity measures label randomization optimizing natural gradient descent. colors show eﬀect varying network width increments natural gradient optimization clearly distinguishes paper studied generalization puzzle deep learning invariance point view. notions invariance come several angles invariance information geometry invariance non-linear local transformation invariance function equivalence algorithmic invariance parametrization minima invariance linear transformations among many others. proposed non-convex capacity measure using fisher-rao norm. demonstrated good properties fisher-rao norm capacity measure theoretical empirical side. brieﬂy discuss aforementioned parameter identiﬁability issue deep networks. function classes considered paper admit various group actions leave function output invariant. means hypothesis class bijection equivalence class identify unlike previously considered norms capacity measure introduced paper respects symmetries σpxq following non-abelian group symmetry acting network weights glpk glpkl negative invariant respect outgoing weights permutation symmetry. addition continuous symmetries discrete group permutation symmetries. case single hidden layer units discrete symmetry gives rise equivalent weights given consider diﬀerentiable transformation parametrization another denoted ξpθq denote jacobian jξpθq bpξξ...ξqq bpθθ...θpq rqˆp. deﬁne loss function satisﬁes lemma denote diﬀerentiable transformation parametrization another ξpθq assume ipθq ˜ipξq invertible consider natural gradient ﬂows deﬁned eqn. re-parametrization assume jξpθq invertible natural proof lemma ﬁrst construct realizes λqfθ. idea simple networks side-by-side construct additional output layer weights output ﬁnal output layer passed σpxq easily sepp hochreiter j¨urgen schmidhuber. flat minima. neural computation vladimir koltchinskii dmitry panchenko. empirical margin distributions bounding james martens roger grosse. optimizing neural networks kronecker-factored approximate curvature. international conference machine learning pages behnam neyshabur ruslan salakhutdinov nati srebro. path-sgd path-normalized optimization deep neural networks. advances neural information processing systems pages", "year": 2017}