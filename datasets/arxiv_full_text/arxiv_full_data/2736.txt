{"title": "MRNet-Product2Vec: A Multi-task Recurrent Neural Network for Product  Embeddings", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell billions of products. Machine learning (ML) algorithms involving products are often used to improve the customer experience and increase revenue, e.g., product similarity, recommendation, and price estimation. The products are required to be represented as features before training an ML algorithm. In this paper, we propose an approach called MRNet-Product2Vec for creating generic embeddings of products within an e-commerce ecosystem. We learn a dense and low-dimensional embedding where a diverse set of signals related to a product are explicitly injected into its representation. We train a Discriminative Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a product title fed through a Bidirectional RNN and at the output, product labels corresponding to fifteen different tasks are predicted. The task set includes several intrinsic characteristics about a product such as price, weight, size, color, popularity, and material. We evaluate the proposed embedding quantitatively and qualitatively. We demonstrate that they are almost as good as sparse and extremely high-dimensional TF-IDF representation in spite of having less than 3% of the TF-IDF dimension. We also use a multimodal autoencoder for comparing products from different language-regions and show preliminary yet promising qualitative results.", "text": "abstract. e-commerce websites amazon alibaba flipkart walmart sell billions products. machine learning algorithms involving products often used improve customer experience increase revenue e.g. product similarity recommendation price estimation. products required represented features training algorithm. paper propose approach called mrnet-productvec creating generic embeddings products within e-commerce ecosystem. learn dense low-dimensional embedding diverse signals related product explicitly injected representation. train discriminative multi-task bidirectional recurrent neural network input product title bidirectional output product labels corresponding ﬁfteen diﬀerent tasks predicted. task includes several intrinsic characteristics product price weight size color popularity material. evaluate proposed embedding quantitatively qualitatively. demonstrate almost good sparse extremely high-dimensional tf-idf representation spite less tf-idf dimension. also multimodal autoencoder comparing products diﬀerent language-regions show preliminary promising qualitative results. large e-commerce companies amazon alibaba flipkart walmart sell billions products websites. data scientists across companies solve hundreds machine learning problems everyday involve products e.g. duplicate product detection product recommendation safety classiﬁcation price estimation. ﬁrst step towards training model usually involves creating feature representation relevant entities i.e. products scenario. however searching hundreds data resources within company identifying relevant information processing transforming product related data feature vector tedious time consuming process. furthermore teams data scientists performing tasks regular basis makes overall process ineﬃcient wasteful. represent product using order-independent bag-of-words approach leveraging textual metadata associated product. approach constructs tf-idf vector representation based title description bullet points product. although representations eﬀective features wide variety classiﬁcation tasks usually high-dimensional sparse e.g. tf-idf representation product titles minimum document frequency represents product using dimensions typically features non-zero. using high-dimensional features creates several problems practice overﬁtting i.e. generalize novel test data training algorithms using high-dimensional features usually computational storage ineﬃcient computing semantically meaningful nearest neighbors straightforward cannot directly used downstream algorithms deep neural networks increases number parameters signiﬁcantly. hand using dense low-dimensional features could alleviate issues. paper goal create generic low-dimensional dense product representation work almost eﬀectively high-dimensional tf-idf representation. propose novel discriminative multi-task learning framework inject diﬀerent kinds signals pertaining product embedding. signals capture diﬀerent static aspects color material weight size subcategory target-gender dynamic aspects price popularity views product. signal captured formulating classiﬁcation/regression decoding task depending type corresponding label. proposed architecture contains bidirectional recurrent neural network lstm cells input layer takes sequence words product title input creates hidden representation refer product embeddings. training phase embeddings multiple classiﬁcation/regression/decoding units corresponding training tasks. full multi-task network trained jointly end-to-end manner. refer proposed approach mrnet embeddings created using method referred mrnet-productvec. section elaborates this. products sold e-commerce websites usually belong multiple product groups furniture jewelry clothes books home sports items. signals inject within products pg-speciﬁc. example weights home items diﬀerent distribution weights jewelry. similarly sizes clothes could quite diﬀerent sizes furniture believe common embedding products across able capture intra-pg variations. hence initially learn embeddings pg-speciﬁc manner sparse autoencoder project pg-speciﬁc embeddings pg-agnostic space. ensures mrnet-productvec also used train encode diﬀerent signals products embeddings embeddings generic possible. however creating embeddings work well every product related task without feature processing easy perhaps impossible. create embeddings keeping particular e-commerce use-cases mind anyone building model products embeddings build good baseline model little eﬀort. someone task speciﬁc features embeddings means augment generic signals captured representations. end-goal provide generic feature representation product e-commerce system data scientists don’t spend days months build ﬁrst prototype. evaluate mrnet-productvec quantitative qualitative ways. mrnet-productvec applied diﬀerent classiﬁcation tasks plugs ship container browse category ingestible sioc unseen population compare mrnet-productvec tf-idf bag-of-words title words show spite much lower dimension tf-idf mrnet-productvec comparable tf-idf representation. performs almost good tf-idf tasks better tf-idf tasks worse tfidf remaining task. sect. provide qualitative analysis mrnet-prodvec. sect. variant multimodal autoencoder used compare products sold diﬀerent language-regions/countries. preliminary qualitative results using approach also provided. several prior works entity embeddings using deep neural networks. perhaps famous work entity embeddings wordvec method continuous distributed vector representations words learned based co-occurrences large text corpus. also prior research works creating product embeddings recommendation. prodvec uses wordvec-like approach learns vector representations products email receipt logs using notion purchase sequence sentence products within sequence words. product representations used recommendation. authors propose metaprodvec extends prodvec loss including additional interaction terms involving products’ meta-data. however embeddings speciﬁcally ﬁne-tuned predeﬁned end-task i.e. recommendation perform well wide variety product related tasks. traditionally multi-task learning used individual tasks smaller training datasets tasks somehow correlated training data correlated tasks improve learning particular task. however paucity data tasks used train mrnet-productvec largely uncorrelated. used unrelated multi-task learning learned representations generic. best knowledge ﬁrst work performs multi-task learning explicitly encode diﬀerent kinds static dynamic signals generic entity embedding. section describe proposed embedding mrnet-productvec. mrnet-productvec feed vector representation word product title bidirectional rnn. wordvec create dense compact representation words product catalog. large corpus text created comprising titles descriptions million randomly selected products catalog. gensim learn dimensional wordvec representation words corpus occur least times. proposed embeddings mrnet-productvec created explicitly introducing diﬀerent kinds static dynamic signals embeddings using discriminative multi-task bidirectional rnn. goal injecting diﬀerent signals create embeddings generic possible. believe learned embeddings eﬀective task correlated tasks train embeddings describe ﬁfteen diﬀerent tasks used learn product embeddings. tasks selected primarily thought corresponding signals intrinsic included generic product embedding. however tasks exhaustive capture possible information products. future research could incorporate tasks training also study dense product embeddings small ﬁxed dimension capture signals eﬀectively. present tasks grouped several ways. capture static information unlikely change lifetime product e.g. size weight material. also dynamic likely change every week month e.g. price number views. tasks classiﬁcation problems others regression decoding. summarize tasks table omit details lack space. color size material subcategory item type formulated multi-class classiﬁcation problems frequent ones treated individual classes remaining ones grouped class. rest classiﬁcation tasks binary. mentioned earlier list tasks exhaustive. however capture wide variety aspects regarding product eﬀective encoding signals create embeddings generic enough address wide class problems pertaining products. block diagram mrnet-productvec shown fig. wordvec representation word product title bidirectional layer containing lstm cells. hidden layer representation forward backward rnns concatenated create product embedding used predict multiple task labels described above. network trained jointly tasks. assume wordvec representation words product title words denoted x... bidirectional forward backward rnn. assume denote hidden states forward backward respectively time recursive equations forward backward given feedforward weight matrices forward backward rnns respectively. recursive weight matrices forward backward rnns respectively. usually nonlinearity tanh relu. ﬁnal hidden representation product words product title forward backward rnns. rnns trained using backpropagation time although rnns designed model sequential data found simple rnns unable model long sequences vanishing gradient problem long short term memory units designed tackle issue along standard recursive feed-forward weight matrices input forget output gates control information remember arbitrarily long sequences. practice observed rnns lstm units better traditional rnns hence lstm units forward backward rnns. skip details lstm units suggest interested readers look article intuitive explanation lstms. suppose want train network diﬀerent tasks. tasks classiﬁcation regression decoding i.e. denotes loss corresponding p-th regression task denotes loss q-th decoding task. losses corresponding tasks normalized task higher loss cannot dominate tasks. training optimize following loss losses tasks. hidden representation corresponding product projected multiple output vectors using task speciﬁc weights biases loss computed function output vector target vector according type task. example task ﬁve-way classiﬁcation projected dimensional output followed softmax layer eventually cross-entropy loss computed softmax output true target labels. regression task projected scalar squared loss computed respect true score. similarly decoding task projected dimensional vector squared loss computed projected representation target dimensional tf-idf representation. optimization deep multi-task neural network cost function equation optimized diﬀerent ways joint optimization iteration training update weights network using gradients computed respect total loss deﬁned equation however training example labels corresponding tasks training possible. alternating optimization iteration training randomly choose tasks optimize network respect loss corresponding task only. case weights correspond particular task weights task-invariant layers updated. style training useful task labels product. however training might biased towards speciﬁc task number training examples corresponding task signiﬁcantly higher tasks. training mrnet diﬃcult obtain task labels product. hence alternating optimization obvious choice sample training batches task uniformly avoid biasing towards speciﬁc task. training proposed network trained separate model label distribution could diﬀerent across pgs. example median price price range jewelry diﬀerent books. similarly materials used clothes diﬀerent kitchen items usually made aluminium metal glass. train model across embeddings unlikely capture ﬁner intra-pg variations representations. hence build model paper train diﬀerent models diﬀerent pgs. pg-speciﬁc embeddings used problem either pg-speciﬁc large number training examples separate pg-speciﬁc models trained. however many practical situations none might true. hence also important product embeddings pg-agnostic models trained products spanning multiple pgs. handle problem training sparse autoencoder projects pg-speciﬁc embeddings pg-agnostic space assume pg-speciﬁc embedding dimension total pg-speciﬁc embeddings. first represent embeddings using dimensional vector pg-speciﬁc embedding placed index range rest ﬁlled zeros. vector called xga. train sparse autoencoder reconstruct fully connected network containing hidden layer dimension hidden layer representation used pg-agnostic embedding. enforce sparsity autoencoder learn interesting structures data learning identity function. section evaluate mrnet-productvec various ways. sect. discuss quantitative results qualitative studies discussed sect. sect. architecture framework details mrnet-productvec layer bidirectional containing lstm nodes followed multiple classiﬁcation/regression/decoding units. train pg-speciﬁc model maximum million training samples corresponding training task. took around minutes train epoch using grid gpu. training pg-agnostic embeddings used randomly chosen products sparse autoencoder took around minutes epoch training. product embeddings created many possible ways. capture diﬀerent kinds signals products varying performance diﬀerent end-tasks. sense eﬃcacy mrnet-productvec consider diﬀerent classiﬁcation tasks. tasks diﬀerent tasks used train mrnet-productvec. plugs binary classiﬁcation problem goal predict product electrical plug not. dataset labeled products. perform ﬁve-fold cross validation report average auc. sioc classiﬁcation problem tries predict product ship container provided seller e-commerce company needs provide additional container product. also binary classiﬁcation problem. dataset labeled examples. five-fold cross validation performed average ﬁve-folds reported. browse categories multi-class classiﬁcation problem products classiﬁed diﬀerent website browse categories total samples dataset. perform ﬁve-fold cross validation report average accuracy. ingestible classiﬁcation apply mrnet-productvec product classiﬁcation problem predicts product ingestible not. however training samples available learning classiﬁer. perform ﬁve-fold cross validation report average ﬁve-folds. sioc believe test data distribution signiﬁcantly diﬀerent train data distribution dense embeddings mrnet-productvec perform better sparse/high-dimensional tf-idf representations. simulate modifying sioc dataset using following steps. first split full dataset ﬁxed training test parts. ﬁlter test product maximum intersection it’s title training product title larger threshold remaining products used test data set. lower threshold diﬀerence test train data distribution increases. ﬁxed training dataset examples used test examples report auc. compare mrnet-productvec sparse high-dimensional tf-idf representation product titles classiﬁcation tasks. sparse tf-idf representation classiﬁcation task created using training examples corresponding task. pg-agnostic version mrnet-productvec sioc plugs ingestible sioc data spanned multiple pgs. pgspeciﬁc version mrnet-productvec browse category classiﬁcation samples i.e. toy. mrnetproductvec tf-idf diﬀerent classiﬁers logistic regression random forest report evaluation metric table observe plugs sioc mrnet-productvec almost good sparse high-dimensional tf-idf spite much lower dimension tf-idf. however browse categories mrnet-productvec performs much worse tf-idf. happens tasks subcategory classiﬁcation task somewhat related browse categories. hence browse category related information encoded mrnetproductvec suﬃcient enough hard -class classiﬁcation task. mrnet-productvec performs better sparse high-dimensional tf-idf ingestible sioc since dense embeddings semantically meaningful i.e. knows chair sofa similar objects able learn classiﬁers even smaller training datasets generalize well unseen test population however sparse high-dimensional tf-idf eﬀective scenarios. overall observe mrnet-productvec mostly comparable tf-idf spite less tf-idf dimension. nearest neighbor analysis study characteristics mrnet-productvec analyzing nearest neighbors several products. since computing meaningful straightforward using sparse tf-idf features show using method. created universe products furniture computed several randomly chosen products based euclidean distance. fig. show ﬁrst nine four hand-picked products. mrnet-productvec ﬁnds several grey colored tables nns. several full-sized beds obtained nns. mrnet-productvec fetches four blue-colored tables drum barrel\" tables nns. mrnetproductvec produces several tools/tool-boxes nns. overall mrnet-productvec learned several intrinsic characteristics products size color type used train mrnet-productvec. mrnet-productvec trained multiple tasks incorporate diﬀerent product related signals. performed preliminary analysis pgagnostic embeddings detect subset features encode particular signal first mrnetproductvec used battery classiﬁcation task multiple random forest models randomly chosen subsets training data built. found features always appear quartile features respect feature importance. indicates features mrnet-productvec indicative product’s electrical properties play role plugs classiﬁcation indeed features important context plugs classiﬁcation features also important battery classiﬁcation task. likewise important features weight classiﬁcation training task also important sioc evaluation task. demonstrates mrnet-productvec encodes diﬀerent product characteristics play crucial role ﬁnal evaluation tasks. e-commerce companies usually sell products across multiple countries language-regions. many scenarios important compare products details title description bullet-points diﬀerent languages. seller lists product country e-commerce company would like know product already listed countries accurate stock-accounting price estimation. customer might like know product liked france website available purchase website not. often also required apply trained model particular language-region diﬀerent language-region product labeled data language available. use-cases important learn cross-language transformations products diﬀerent countries/language-regions compared seamlessly. address issue propose variant multimodal autoencoder project mrnet-productvec trained diﬀerent languages common space comparison. assume paired product titles diﬀerent countries france i.e. product title french know corresponding title vice versa. separately train mrnet-productvec uk-english french refer mrnet-productvec-uk mrnet-productvecfr respectively. mrnet-productvec-uk mrnet-productvec-fr used obtain embeddings products. corresponding embeddings p-th product deﬁned respectively. train autoencoder input output hidden layer dimension assume denotes zero vector dimension. training data network consists three parts train provide initial results using that. mrnet-productvec applied generate product embeddings around billion products made available internally within company product related model building. periodically retrain mrnet keep model updated dynamic signals also update resulting embeddings products. note mrnet-productvec suitable cold-start scenarios embeddings created using product titles available part catalog data. although mrnet-productvec trained proposed diﬀerent tasks framework provides ﬂexibility learn embeddings tasks ﬁne-tune already learnt embeddings additional tasks.", "year": 2017}