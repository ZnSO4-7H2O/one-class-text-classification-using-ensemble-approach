{"title": "A PCA-Based Convolutional Network", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-the-art deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.", "text": "paper propose novel unsupervised deep learning model called pca-based convolutional network architecture composed several feature extraction stages nonlinear output stage. particularly feature extraction stage includes layers convolutional layer feature pooling layer. convolutional layer ﬁlter banks simply learned pca. nonlinear output stage binary hashing applied. higher convolutional layers ﬁlter banks learned feature maps obtained previous stage. test conducted extensive experiments challenging tasks including handwritten digrecognition face recognition texture classiﬁcation. results show performs competitive even better state-of-theart deep learning models. importantly since back propagation supervised ﬁnetuning much efﬁcient existing deep networks. introduction traditional models classiﬁcation tasks generally composed hand-crafted feature extraction trainable classiﬁer. popular hand-crafted features include gabor features locally binary patterns sift successfully applied texture classiﬁcation face recognition object recognition tasks. however features extracted hand-crafted methods always low-level suited speciﬁc data tasks prior knowledge. recently deep learning become popular automatically learning features data disentangles underlying factor variations. proposed deep approaches always include layerwise stacking feature extractors. example deep belief networks composed stacking pre-trained restricted boltzmann machines deep auto-encoders stacked rbms auto-encoders. deep architectures lead learn hierarchical abstract features higher layers representations. powerful deep architectures biologically inspired model convolutional networks convnets trainable multi-stage architecture stage composed three layers ﬁlter banks layer nonlinearity layer feature pooling layer. weight sharing convolution layer pooling operations convnets lead features invariant small variations. deep convnets multistage architecture learn hierarchical features local low-level features global high-level ones. however training deep network typically uses gradient descent method supervised mode always need large scale labeled samples training. addition good results sometimes depend tricks trade parameter tuning e.g. using dropout regulation recent research shown using unsupervised learning stage convnets helped reducing requirement labeled data signiﬁcantly. pcanet variation deep convolutional networks convolution ﬁlter banks stage simply chosen ﬁlters surprisingly simple ﬁlters used deep network architecture demonstrated competitive performance deep networks. however pcanet dispenses pooling layer feature learning stage uses block-wise histogram together nonlinear operation output stage. results exponentially growing dimensions training time increasing number samples. paper propose convolutional architecture ﬁlters learned unsupervised mode. network composed feature extraction stage could stacked multiple stages nonlinearity stage. feature extraction stage includes convolution layer pooling layer easily cascaded deep architecture. nonlinearity stage includes binary hashing histogram statistics; output trainable classiﬁer. ﬁlter bank convolution layer learned generated feature maps aggregated pooling layers. results multiple sets feature maps corresponding different ﬁlters probably detect distinctive features input. ﬁlter banks higher convolution layer computed based combinations multiple sets feature maps. inspired intuition high level features combinations abstract level features. multiple feature maps corresponding input represent different features extracted input. experiments show comparative performance classiﬁcation tasks state-of-the-art approaches. related work past years variations convolutional network proposed respect pooling convolutional operation. recently unsupervised learning used pre-training stage would alleviate need labeled data. stages pretrained network ﬁne-tuned using stochastic gradient descent method. many methods proposed pre-train ﬁlter banks convolution layers unsupervised feature learning mode. convolutional versions sparse rbms sparse coding predictive sparse decomposition reported achieved high accuracy several benchmarks. alternatively networks similar convnets proposed used pre-ﬁxed ﬁlters convolution layer yielded good performance several benchmarks. gabor ﬁlters used ﬁrst convolution layer. meanwhile wavelet scattering networks also used pre-ﬁxed convolutional ﬁlters called scattering operators. using similar multiple levels convnets algorithm achieved impressive results applications handwritten digits texture recognition. closely related work called pcanet simply ﬁlters unsupervised learning mode convolution layer. built upon multiple convolution layers nonlinear output stage applied hashing block-wise histogram. cascaded convolution layers demonstrated able achieve records several challenging vision tasks face handwritten recognition comparative results texture classiﬁcation object recognition. pca-based convolutional network essentially multi-stage convolutional network trained layer-wise unsupervised manner. composed cascaded feature extraction stages nonlinear output stage. figure illustrates structure typical three stages including output stage. feature extraction stage consists convolutional layer pooling layer. inputs ﬁrst convoluted pcabased ﬁlters produce feature maps. pooling layer generally computes average maximum value neighborhood. purpose pooling layer build robustness small distortions reduce resolution feature maps factor horizontally vertically. propagated feature maps pooling layer next stage input. ﬁnal output stage comprises binary hashing block-wise histogram statistics. ﬁrst feature extraction stage inspired weight sharing receptive ﬁelds convnets input image sample number patches size every pixel locations i.e. sample interval pixels. patch vectorized form column elements. patches sampled input image together form matrix size denoted represents vector patch introduce competitions adjacent column vecmean value operation reminiscent local contrast normalization used imagenet matrices input images constructed assembled form large matrix subsequently subtracts mean result also denoted eigenvalue decomposition performed matrix convolutional ﬁlters selected ﬁrst principle eigenvectors thus learned ﬁlters described matkk) rk×k matkk denote mapping relationship vector matrix rk×k represent eigenvector matrix eigenvectors reshaped size obtain ﬁlters size subsequently convolute learned ﬁlters input images generate ﬁlter responses pixel location; call ﬁltering results feature maps. convolution input image produces feature maps. feature represents particular features extracted corresponding location image. divide feature maps generated convolutional layer several non-overlapping pooling regions size pooling average pooling applied pooling regions. pooling operation results feature maps reduced resolution pooling features robust small distortions. represent pooling result feature input image. given collection {ii}n input images convolution pooling operation using ﬁlter obtain feature maps denoted since ﬁlters ﬁrst extraction stage obtain feature maps total. second feature extraction stage pooled feature maps ﬁrst stage treated original input second stage. feature maps divided subsets. subset includes feature maps produced convoluting input images ﬁlter previous stage denoted feature maps subset capture certain features input images whereas different subsets capture different types features. figure shows structure proposed pcn. since high level features combinations abstract level features combine subsets {sl}l according certain rule form several groups. table demonstrates combine subsets. group feature maps corresponding input image added. combined subsets used actual input second feature extraction stage. table represents subset feature maps obtained previous stage column represents group combining subsets particular way. ﬁlters ﬁrst stage result subsets. subsets added groups formed. table indicates corresponding subsets combined form group. practice indexing matrix used deﬁne combination. indexing matrix entries zeros entries ones indicate subsets belonging group. thus different indexing matrices deﬁned. indexing matrix deﬁned identity matrix combination subsets. subset denoted feature maps. repeating procedure ﬁrst stage sample patches feature subset. also subtract patch mean values join vectors together form matrix denoted ¯yli ¯yli··· ¯yli ¯ylij represents mean removed vector patch feature subset. collect patches feature maps subset remove patch mean concatenate matrixes afterwards mean removed form since subsets obtain matrixes subset following equation separately matkk subset choose ﬁrst principle eigenvectors ﬁlters denoted input feature subset convoluted ﬁlters resulted feature maps. since subsets produce feature maps total second stage output second feature extraction stage pooling process second stage ﬁrst stage. output feature maps divided several non-overlapping patches size maximum average value calculated pooling region. feature extraction stages process output stage reconstruct feature maps form ﬁnal representations input image. binary hashing histogram statistics pcanet input feature second stage produces output maps. binarize heavoutput maps calculate iside step function whose value positive entries zero otherwise. pixel location treat vector binary bits decimal number. converts outputs generated second stage back single integer-valued image. integer-valued images partition blocks. compute histogram decimal values block concatenate histograms vector. encoding process feature input image becomes block-wise histograms. local blocks either overlapping non-overlapping depending applications. experiments three-stage applied different data sets simplicity. ﬁnal output features sent linear classiﬁcation. conﬁgurations keep ﬁxed. compared efﬁciency different recognition tasks using desktop intel memory. images mnist datasets small patching sampling interval i.e. sample patch pixel location. patch size output stage block size block overlapping ratio three parameters keep unchanged experiment. particular pooling layer disabled every feature extraction stage easily controlled parameter code. select identity matrix indexing matrix make every group second stage contain subset. basic mnist dataset smaller subset mnist. contains training images validation images testing images. ﬁrst perform experiment basic dataset. hyper-parameters selected maximize performance validation set. then system trained entire training validation set. achieve highest accuracy numbers ﬁlters ﬁrst stage second stage respectively.this higher related methods literature. standard mnist dataset consists training images testing images. adjust hyperparameters validation samples class taken training sets. hyper-parameters selected maximize performance validation set. then system trained entire training set. found best conﬁguration numbers ﬁlters ﬁrst second stage respectively accuracy reached outperformed related works shown table overall achieve competitive performance compared state-of-the-art much less computation simple network structure. texture classiﬁcation curet dataset curet texture dataset contains categories textures. category contains images material different pose illumination conditions. experiment following pcanet subset original data azimuthal viewing angles less degrees selected thereby yielding images class. central region cropped selected images. dataset randomly split training testing training images class. hyper-parameters selected according literature. ﬁlter size patch sampling interval number ﬁlters stage non-overlapping block size pooling layer disabled extraction stage. identity matrix used indexing matrix second stage. accuracy reached higher result achieved pcanet. texture classiﬁcation outex dataset outex framework empirical evaluation texture classiﬁcation segmentation algorithms. problems encapsulated welldeﬁned test suites precise speciﬁcations input output data. outex database contains surface textures natural scenes. collection surface textures expanding continuously. moment database contains surface textures macrotextures microtextures. many textures variations local color content results challenging local gray scale variations intensity images. source textures large tactile dimension induce considerable local gray scale distortions. source texture imaged according certain procedure. images used texture classiﬁcation suite extracted given source images centering sampling grid equally many pixels left side sampling grid. training testing images particular texture classiﬁcation problem extracted source images images divided randomly halves equal size purpose obtaining unbiased performance estimate. directory images test suite includes images needed test suite. directory indexed three numbers includes speciﬁed problem test suite. directories three ﬁlesclasses.txttest.txt train.txt deﬁne problem. problem indexed outext test suite selected experiment. several validating trails extended yale dataset contains frontal-face images individuals. cropped normalized face images captured various lighting conditions. subject randomly select images testing images rest training. validation images subject taken training sets. hyper-parameters selected maximize performance validation set. then system trained entire training set. patch size numbers ﬁlters ﬁrst second stage respectively. patch sampling interval pooling module used boxcar ﬁlter down-sampling step. used non-overlapping blocks output stage block size identity matrix used indexing matrix second stage. achieve average accuracy experiments shown table training time method including plus testing time sample much efﬁcient compared pcanet. ﬁlters ﬁrst stage shown ﬁgure obvious ﬁlter ﬁrst stage captures directionrelated features input face image. column figure contains ﬁlters group second stage; seen ﬁlter banks different groups similar large extend still differences can’t ﬁlters different groups. found identity matrix better matrix used indexing matrix caused blur effect subsets group connected ﬁlter bank maybe different ﬁlters future. patch size patch sampling interval numbers ﬁlters ﬁrst second stage respectively. pooling layer disabled stage. block size block overlapping ratio identity matrix used indexing matrix second stage. achieve classiﬁcation accuracy training time including svm. what’s test time sample texture classiﬁcation dataset procedural models widely used computer graphics generating realistic natural-looking textures. number procedural models proposed models produce various textures. render textures presented surface images. given surface image important know model produce kind texture. typical texture classiﬁcation problem. procedural texture dataset contains number rendered textures generated procedural texture models rendered luxrender given ﬁxed light conditions. textures generated method normally different generated methods; however textures produced different models perceived similar. forms challenging classiﬁcation task. figure shows example samples texture dataset. randomly choose images method testing rest used training. validation samples method taken training set. hyper-parameters selected maximize performance validation set. found best conﬁguration patch size patch sampling interval numbers ﬁlters extraction stages boxcar ﬁlter down-sampling step used pooling layer. particular output non-linear stage removed. feature maps feature extraction phase reshaped concatenated form vector input linear classiﬁer. trained entire training using best conﬁguration. accuracy reaches higher result achieved pcanet. results shown table importantly algorithm much efﬁcient pcanet terms computation cost. large numbers ﬁlters suggest surface images dataset contains complex structure. figure shows ﬁlters ﬁrst stage extract orientation related features input images. complex structure texture ﬁlters look complicated. fact surface images obvious edge information. ﬁgure contains ﬁlters group second stage. observed prior ﬁlters group extract large scale features posterior ﬁlters extract detailed features. comparison also traditional classiﬁcation task computation resources. running hours iterations achieve accuracy performance becomes worse number iterations increases. obvious falls overﬁtting enough training samples. propose pca-based convolutional network essentially advantage pcanet i.e. achieve competitive performance compared state-of-theart methods much efﬁcient terms computation. used experiments simply comprises feature extraction stages non-linearity output stage. however instead training network using iteration methods simply uses learn ﬁlters convolution layer. eigenvectors used ﬁlters convolute input images. similar deep networks noted proper conﬁguration important different types inputs. training images relatively simple terms structure large size relatively large interval sample patches enable pooling layer rapidly reduce feature dimension input image. hand input image small enough simply patch sampling interval disable pooling layer. grouping process subsets group connected ﬁlter bank subsets form subset. different ﬁlter banks maybe work effectively. consider different ﬁlter banks group future. references serge belongie jitendra malik puzicha. shape matching object recognition using shape contexts. pattern analysis machine intelligence ieee transactions mikael henaff kevin jarrett koray kavukcuoglu yann lecun. unsupervised learning sparse features scalable audio classiﬁcation. ismir volume page geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventarxiv preprint co-adaptation feature detectors. arxiv. laurent sifre st´ephane mallat. rotation scaling deformation invariant scattering computer vision pattern texture discrimination. recognition ieee conference pages ieee dacheng xuelong xindong stephen maybank. general tensor discriminant analysis gabor features gait recognition. pattern analysis machine intelligence ieee transactions yuanqing john lafferty. learning image representations pixel level hierarchical sparse coding. computer vision pattern recognition ieee conference pages ieee kevin jarrett koray kavukcuoglu ranzato yann lecun. best multi-stage architecture object recognition? computer vision ieee international conference pages ieee koray kavukcuoglu marcaurelio ranzato fergus yann le-cun. learning invariant features topographic ﬁlter maps. computer vision pattern recognition cvpr ieee conference pages ieee koray kavukcuoglu pierre sermanet y-lan boureau karol gregor micha¨el mathieu yann cun. learning convolutional feature hierarchies visual recognition. advances neural information processing systems pages rahul sukthankar. pca-sift distinctive representation local image descriptors. computer vision pattern recognition cvpr proceedings ieee computer society conference volume pages ii–. ieee daniel keysers thomas deselaers christian gollan hermann ney. deformation models image recognition. pattern analysis machine intelligence ieee transactions alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages honglak roger grosse rajesh ranganath andrew convolutional deep belief networks scalable unsupervised learning hierarchical proceedings annual inrepresentations. ternational conference machine learning pages honglak roger grosse rajesh ranganath andrew convolutional deep belief networks scalable unsupervised learning hierarchical proceedings annual inrepresentations. ternational conference machine learning pages mutch david lowe. multiclass object recognition sparse localized features. computer vision pattern recognition ieee computer society conference volume pages ieee katsunori onishi tetsuya takiguchi yasuo ariki. human posture estimation using features monocular image. pattern recognition icpr international conference pages ieee thomas serre lior wolf tomaso poggio. object recognition features inspired visual cortex. computer vision pattern recognition cvpr ieee computer society conference volume pages ieee", "year": 2015}