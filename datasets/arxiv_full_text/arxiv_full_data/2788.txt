{"title": "Stochastic Dual Coordinate Descent with Bandit Sampling", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Coordinate descent methods minimize a cost function by updating a single decision variable (corresponding to one coordinate) at a time. Ideally, one would update the decision variable that yields the largest marginal decrease in the cost function. However, finding this coordinate would require checking all of them, which is not computationally practical. We instead propose a new adaptive method for coordinate descent. First, we define a lower bound on the decrease of the cost function when a coordinate is updated and, instead of calculating this lower bound for all coordinates, we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease while simultaneously performing coordinate descent. We show that our approach improves the convergence of the coordinate methods (including parallel versions) both theoretically and experimentally.", "text": "stochastic gradient descent works time step selecting datapoint optimize over. hand coordinate descent algorithms work time step selecting coordinate optimize over. shown outperform various applications ﬁrst introduced algorithms often selected coordinates uniformly random time step however recent work shown sampling coordinates appropriate non-uniform distributions result better convergence guarantees theory practice work propose method non-uniform coordinate selection described section time ﬁrst give lower bound decrease cost function attained updating decision variable time maintaining decision variables ﬁxed. initial approach simply update decision variable corresponding coordinate largest approach particularly beneﬁcial distribution high variance across cases approach times better uniform sampling improving current state times better uniform however calculating computationally prohibitive. therefore works must reﬁne algorithm decrease per-iteration cost. adopt novel principled time comapproach learn best single coordinate pute marginal decrease eﬀect learn estimates simultaneously optimizing cost function prove approach learns fast performs almost well initial approach required calculations time step note approach generic fact coordinate descent methods minimize cost function updating single decision variable time. ideally would update decision variable yields largest marginal decrease cost function. however ﬁnding coordinate would require checking them computationally practical. instead propose adaptive method coordinate descent. first deﬁne lower bound decrease cost function coordinate updated instead calculating lower bound coordinates multi-armed bandit algorithm learn coordinates result largest marginal decrease simultaneously performing coordinate descent. show approach improves convergence coordinate methods theoretically experimentally. supervised learning algorithms work minimizing empirical risk cost function dataset. designing fast optimization algorithms cost functions crucial especially size datasets ever increasing. empirical risk cost functions often written algorithm works class update rules decision variable example update rule lasso update rule hinge-loss addition update rule belong class. deﬁnition /β-smooth µi-strongly convex convexity parameter assume li-bounded support. consider following function begin analysis lemma lower bounds decrease cost function updating decision variable according update rule class marginal decrease quantity lemma assumptions deﬁnition after selecting coordinate updating dual residue introduced similar notions appear deﬁnition show section framework also accelerate convergence parallel framework updates decision variables time; theorem best knowledge ﬁrst adaptive approach parallel setting need strongly convex. lastly reviewing related work show empirically approaches outperform state algorithms real datasets section main contributions section formally describe coordinate selection algorithm present analysis convergence. lemma give bound decrease cost function decision variable updated. theorem gives convergence guarantee initial approach selects iteration coordinate largest finally theorem gives convergence guarantee reﬁned bandit approach selects distribution maintained using convex conjugates respectively. argminα∈rnfp sub-optimality i.e. well known optimal primal-dual pair reached following optimality conditions satisﬁed figure approach stochastic coordinate descent. part approach handles updates decision variable bottom part approach handles selection according sampling distribution updated bandit optimization optimized corresponds update rule using bound lemma know updating coordinate cost function decrease least hence initial approach choose coordinate largest proposition assumptions lemma optimal coordinate minimizing right-hand side time assume selection rule results linear convergence matches state convergence rate. theorem µi-strongly convex assumptions deﬁnition following convergence guarantee selection rule optimal bound however make analysis simpler consider following sub-optimal selection rule. iteration choose coordinate largest updating; call method max_gap. note randomness selection rule max_gap hence following convergence result holds expectation also almost surely. theorem assumptions deﬁnition result proven induction. distinguish cases case validate induction hypothesis using lemma complete proof given appendix remark know lemma libounded support depending problem assume data normalized normalization widely used technique avoid numerical instability. thus important parameter theorem ratio g/gi parameter large diﬀerent coordinate-wise gaps equal. case non-uniform sampling bring advantage uniform sampling. contrast g/gi choosing coordinate largest results decrease cost function times larger compared uniform sampling. compare bound theorem bound ada_gap algorithm found general convex theorem best date. simplicity assume bound proved theorem split cases. best case identical convergence guarantee ada_gap algorithm whose ﬁrst term times better ada_gap algorithm times better uniform sampling. rate algorithm ada_gap better uniform sampling coordinate-wise gaps non-uniform. bandit sampling sdca consider update ideally would update decision varih. able reduces cost function most i.e. minj∈ however ﬁnding coordinate would require calculating often computationally prohibitive. could instead bandit approach aims learn best time. probrithm takes decrease cost function feedback. lastly instead computing compute lower bound feedback bandit algorithm note using bandit update general approach allows theorem hence whatever coordinate choose cost function increase. select coordinate rb/. hence number know maxj∈ times choose coordinate smaller neglect decrease cost function iterations. therefore iterations cost function decreases least maxi∈ following analysis theorem last equality fact appendix present modiﬁed version rexp achieves probability desired. similar ﬁxed non-uniform sampling algorithm iteration rexp require computations bandit algorithms also used accelerate minimizing variance stochastic gradient remark theorem holds weaker condition harder verify eﬃcient parallelization large datasets make computationally expensive optimize cost function them. parallelization often crucial. section sketch extend main results parallel setting. concrete application consider bucket sampling coordinates split buckets containing coordinates. bucket given processor time processor selects coordinate updates corresponding decision variable hold update rule well-known bandit algorithm useful trick works total variation diﬀerent rounds bounded value reset bandit algorithm’s parameters iterations. known rexp oﬀers following guarantee constant translate convergence guarantee sdca convert bound number times bandit algorithm selects best coordinate follows. theorem consider assumptions definition divide time batches size max{n n√t} iterations bandit algorithm reset. batch sume maxi∈ cbrb batch also assume modiﬁed version rexp achieves probability i|ai. upper bound selection rule optimal bound simplify proof ﬁrst sub-optimal selection rule max_gap iteration bucket choose coordinate largest implies convergence guarantee selection rule complete proof presented appendix compare bound theorem bound theorem worst case gi’s equal other non-uniform sampling bring advantage. even small processors reduces number iterations required factor k/d. sparse datasets expect small hence max_gap roughly improves rate convergence factor additional assumption improve ﬁrst term numerator convergence guarantee factor corollary section first deﬁne notion internal correlation extend lemma parallel setting notion guarantees eﬀect updating optimal update small. lower bound decrease updating coordinates next section choose coordinates largest marginal decrease. before computationally expensive reﬁne approach using deﬁnition extend lemma follows. lemma data matrix k-internal correlated. assumptions deﬁnition update coordinates iteration using update rule approach sampling however instead using bandit minimize cost function directly used minimize variance estimated gradient. results improvement opposed work. empirical results number non-zero entries data matrix compare algorithm state approaches ways. first compare adaptive approach state methods similarly computations epoch size next compare adaptive-bandit approach methods appropriate heuristic modiﬁcations also computations epoch. datasets found consider usps aloi protein using update rule corresponding lasso using update rule corresponding logistic regression comparators adaptive algorithm uniform select coordinate uniformly random. ada_gap compute sample coordinate probability gi/g. adasdca compute bility |κi|/n sample coordinate proba sdca_exp select coordinate compute compute probability sdca_rexp select coordinate compute compute probability related work various works also considered specifying probability distribution select coordinate update time static computed lipschitz constant time varying distributions gi/g also considered. cases computing distribution needs calculations step costly. address problem heuristics often used; e.g. calculated beginning epoch length left unchanged throughout remainder epoch. contrast principled approach leverages bandit algorithm learn good coordinates; allows theoretical guarantees outperforms state seen section furthermore approach require cost functions strongly convex large body work studied extension coordinate descent parallel setting. majority approaches select coordinates uniformly random static non-uniform sampling parallel considered strongly convex cost function class cost functions additionally satisfy expected separable over-approximation rather general approach requires convexity. adaptive sampling approach studied however algorithm requires calculations compute gradient cost function. comparison addition uniform sampling consider coordinate selection methods shown best performance empirically uniform select coordinate uniformly random. gap_per_epoch algorithm heuristic version ada_gap sampling probability gi/g re-computed beginning epoch length adasdca+ algorithm heuristic version adasdca sampling probability re-computed beginning epoch length computational cost algorithms consist parts sampling coordinate updating coordinate average computational cost epoch algorithms depicted table logistic regression figure again algorithms outperform state art. figure sdca_rexp outperforms sdca_exp conﬁrms fact sometimes information slow convergence. also test coordinate selection methods parallel setting. consider lasso datasets usps aloi protein. compare best algorithms observed experiments namely uniform ada_gap gap_per_epoch gap_wise. results depicted figures appendix algorithms either match outperform state approaches tested. also test ijcnn observe similar performance conclusion work propose approach select coordinates update sdca methods. show approach converges faster state approaches theoretically empirically. aspect approach bandit algorithm learn coordinates decrease cost function signiﬁcantly throughout course optimization algorithm. show extend approach parallel settings. emphasize approach quite general works large class update methods cost function bandit algorithms. cost function lasso λ|αi| number features. normalize consider update proposed /∂αi sign max{|q| update rule belongs class lasso gi’s strongly convex computing dual residue technique i.e. bounded support size sdca_exp sdca_rexp initialize similarly restart rexp reset weights usps aloi protein total number iterations cost function logistic regression λ|αi| number features. normalize consider update proposed /∂αi. update rule also belongs parameters sdca_exp sdca_rexp lasso. datasets namkoong sinha yadlowsky duchi. adaptive sampling probabilities non-smooth optimization. proceedings international conference machine learning necoara clipici. eﬃcient parallel coordinate descent algorithm convex optimization problems separable constraints application distributed mpc. journal process control osokin alayrac lukasewitz dokania lacoste-julien. minding gaps block frankwolfe optimization structured svms. proceedings international conference machine learning perekrestenko cevher jaggi. faster coordinate descent adaptive importance sampling. proceedings international conference artiﬁcial intelligence statistics richtárik takáč. eﬃcient serial parallel coordinate descent methods huge-scale truss topology design. operations research proceedings pages springer hsieh dhillon. scalable coordinate descent approaches parallel matrix factorization recommender systems. data mining ieee international conference pages ieee storkey. stochastic parallel block coordinate descent large-scale saddle point problems. association advancement artiﬁcial intelligence pages depends previous selected coordinates given marginal decrease random. hence change much batch second regret holds almost surely. result suﬃces bandit algorithm reaches high probability. example exp.p decoupled algorithm reach high-probability theresection simplex valid probability distributions. performance adversarial bandit algorithm comparable performance obtained best static sampling probability i.e. vector maxp∈∆n example updating probabilities theorem deduce following guarantee here present modiﬁed version rexp achieves probability needed theo| bandit algorithm rexp reaches expectation. show modify reach high-probability well. rexp time divided batches size inside batch used beginning batch parameters reset. using guarantees proof max_gap selection rule. time max_gap chooses coordinate largest note optimal selection rule minimizing bound lemma select coordinate largest shown proposition prove bound using induction. first induction base case trivially proven setting becomes +ψ/. suppose holds want verify argmaxi∈gi. study cases separately deﬁned section show implement rexp tree data structure similar interval tree sample coordinate update weight node store value shows depth node makes node identiﬁable uniquely node children denote right child left child weights stored leafs tree. height tree parent node’s value children’s values i.e. hence value stored root weights i.e. tasks need implement sampling coordinate updating weight sampling rexp coordinate sami/w η/n. ﬁrst biased coin probability probability coin select coordinate uniformly random. select coordinate probability using procedure total probability η/n. sample coordinates probability tree structure. generate random number uniformly. next search coordinate coordinate satisﬁes probability pseudo-code ﬁnding coordinate satisﬁes presented algorithm height tree algorithm takes iterations coordinate satisﬁes updating sample coordinate update weight update tree simply values ancestors proofs section present proofs section ﬁrst present proof theorem next proof theorem cost function strongly convex theorem provides linear convergence result. lemma /β-smooth µistrongly convex convexity parameter data matrix k-internal correlated. case require l-bounded support. update coordinates iteration arbitrary iterates sdca method satisfy using fenchel-young property recover recover then setting conclude proof. theorem data matrix k-internal correlated µi-strongly convex split coordinates buckets. assumptions deﬁnition following convergence guarantee proof similar proof theorem proof following selection rule bucket time choose coordinate largest giµi/ai/β). following steps proof theorem show similar proof theorem proof max_gap selection rule. bucket time max_gap chooses coordinate largest note optimal selection rule minimizing bound lemma select setting choose whichever coordinates update coordinates. call setting minibatch. proposition assumptions deﬁnition lemma optimal minimizing right-hand side lemma containing largest theorem assumptions deﬁnition lemma following convergence guarantee faster. reason gap_per_epoch work good iterations gets closer optimal coordinate gap_per_epoch become small slightly large makes large compared others means epoch coordinate selection section returns coordinate often suboptimal. several consecutive updates becomes small need update anymore results results shown figures adaptive-bandit setting sdca_rexp converges faster algorithms gap_per_epoch converges faster sdca_exp. adaptive setting algorithm remain always best among algorithms. hinge-loss datasets ijcnn similar experiments section compare algorithm state approaches. first compare adaptive approach methods similarly computations epoch size compare adaptive-bandit approach methods appropriate heuristic modiﬁcations also allow attaining similar computations epoch. here number datapoints. consider update rule update coordinate sdca_rexp moreover similar previous experiments initialize sdca_exp similarly restart sdca_rexp weights sdca_exp sdca_rexp reward algorithm becomes stable large theorem theorem still hold. usps ijcnn results figure results. max_gap sampling method outperforms sampling methods figure figure sampling methods perform similarly. computational version algorithms depicted figures figure adasdca+ gap_per_epoch converge faster sdca rexp however slower figure figure gap_per_epoch converges slightly faster sdca_exp sdca_rexp sdca_exp sdca_rexp converge", "year": 2017}