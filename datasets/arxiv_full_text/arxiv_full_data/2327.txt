{"title": "UPAL: Unbiased Pool Based Active Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In this paper we address the problem of pool based active learning, and provide an algorithm, called UPAL, that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space. For the space of linear classifiers and the squared loss we show that UPAL is equivalent to an exponentially weighted average forecaster. Exploiting some recent results regarding the spectra of random matrices allows us to establish consistency of UPAL when the true hypothesis is a linear hypothesis. Empirical comparison with an active learner implementation in Vowpal Wabbit, and a previously proposed pool based active learner implementation show good empirical performance and better scalability.", "text": "paper address problem pool based active learning provide algorithm called upal works minimizing unbiased estimator risk hypothesis given hypothesis space. space linear classiﬁers squared loss show upal equivalent exponentially weighted average forecaster. exploiting recent results regarding spectra random matrices allows establish consistency upal true hypothesis linear hypothesis. empirical comparison active learner implementation vowpal wabbit previously proposed pool based active learner implementation show good empirical performance better scalability. learn classiﬁer predicts well unseen points. certain problems cost obtaining labeled samples quite expensive. instance consider task speech recognition. labeling speech utterances needs trained linguists fairly tedious task. similarly information extraction natural language processing needs expert annotators obtain labeled data gathering huge amounts labeled data tedious experts also expensive. cases interest design learning algorithms need labeled examples training also guarantee good performance unseen data. label active learning algorithms query oracle times possible learn provably good hypothesis labeled samples. broadly speaking active learning algorithms classiﬁed three kinds namely membership query based algorithms stream based algorithms pool based human annotators algorithms might work poorly demonstrated lang baum case handwritten digit recognition annotators faced awkward situation labeling semantically meaningless images. stream based algorithms stream based algorithms tend computationally eﬃcient appropriate underlying distribution changes time. pool based algorithms assume access large maximum number points allowed query query informative points. pool based algorithms stream based algorithms overcome problem awkward queries based algorithms face. however experiments discovered stream based algorithms tend query points necessary poorer learning rates compared pool based algorithms. space margin based loss function minimizes provably unbiased estimator risk e)]. unbiased estimators risk used stream based algorithms estimators introduced pool based algorithms. using idea importance weights introduced beygelzimer roughly speaking upal proceeds rounds round puts probability distribution entire pool samples point pool. queries label point. probability distribution round determined current active learner obtained minimizing importance weighted risk seen pruning hypothesis space soft manner placing probability distribution determined importance weighted loss classiﬁer currently labeled part pool. section prove consistency upal squared loss true underlying hypothesis linear hypothesis. proof employs elegant results random matrix theory regarding eigenvalues sums random matrices possible improve constants exponent dimensionality involved used theorem results qualitatively provide insight label complexity squared loss depend condition number minimum eigenvalue covariance matrix kind insight knowledge provided literature active learning. section provide thorough empirical analysis upal comparing active learner implementation vowpal wabbit batch mode active learning algorithm shall call bmal experiments demonstrate positive impact importance weighting better performance upal implementation. also empirically demonstrate scalability upal bmal mnist dataset. required query large number points upal upto times faster bmal. good active learning algorithm needs take account fact points queried might reﬂect true underlying marginal distribution. problem similar problem dataset shift train test distributions potentially diﬀerent learner needs take account bias learning process. approach problem importance weights training process instead weighing points equally algorithm weighs points diﬀerently. upal proceeds rounds round probability distribution entire pool sample point distribution. sampled point queried previous rounds queried label previous round reused random variable takes else oracle queried label point. denote value point queried it’s label round otherwise. order guarantee estimate error rate hypothesis unbiased importance weighting point round gets importance weight formally prove importance weighted risk unbiased estimator true risk. denote product distribution theorem guarantees long probability querying point pool round non-zero unbiased estimator come probability distribution round solve problem resort probabilistic uncertainty sampling point whose label uncertain current hypothesis hat− gets higher probability mass. current hypothesis simply minimizer importance weighted risk i.e. hat− minh∈h ˆlt−. point calculate uncertainty label ﬁrst estimate def= using hat− entropy label distribution calculate probability querying estimate round depends current active learner hat− loss function. general possible estimate arbitrary convex loss functions. however conditional distribution steps algorithm depend loss function used. logistic loss min{max{ shown zhang squared logistic exponential losses tend estimate underlying at−x) case squared loss at−x} since loss function convex constraint convex design upal might requery points. alternate strategy allow requerying points. however importance weighted risk unbiased estimator true risk case. hence order retain unbiasedness property allow requerying upal. case squared loss interesting look behaviour upal case squared loss rest paper shall denote hypothesis returned upal rounds. show prediction simply exponentially weighted average predictions theorem instructive. tells assuming matrix invertible exponentially weighted average hypothesis hence view upal learning expert advice stochastic setting individual hypothesis expert exponential used weigh hypothesis forecasters commonly used learning expert advice. also allows interpret upal pruning hypothesis space soft exponential weighting hypothesis suﬀered cumulative loss gets lesser weight. bounding excess risk natural upal consistent? upal well optimal hypothesis answer question aﬃrmative. shall analyze excess risk hypothesis returned active learner denoted rounds loss function squared loss. prime motivation using squared loss loss functions squared losses yield closed form estimators elegantly analyzed using results random matrix theory possible extend results loss functions logistic loss exponential loss using results empirical process theory assumption necessary problem well deﬁned. used recent literature analyze linear regression random design bernstein like condition seen softer form boundedness condtion support distribution. particular data bounded d-dimensional unit cube suﬃces take possible satisfy mapping data kernel spaces. though popularly used kernels gaussian kernel data inﬁnite dimensional spaces ﬁnite dimensional approximation kernel mappings found random features matrix -norm positive semideﬁnite matrix nothing maximum eigenvalue matrix. obsercation exploiting structure matrix problem reduces giving probabilistic upper bounds maximum eigenvalue random rank- matrices. theorem provides tool prove bounds. resulting probability problem bounding maximum eigenvalue random matrices necessarily rank-. theorem provides bernstein type bounds analyze eigenvalues sums random matrices. finally steps conditioned invertibility random matrices ˆσz. provide conditions guarantee invertibility ˆσz. problems boil calculating lower bounds minimum eigenvalue random matrices question establish lower bounds theorems shall provide bound excess risk active learner hypothesis. suppose hypothesis represented active learner rounds. deﬁnition active learner deﬁnition desired result. lower bound λmin also obtained way. lemma probability atleast separately λmin λmax proof. using lemma probability atleast λmin probability atleast λmax finally since σ/jς/ following upper bound probability atleast equation follows equation deﬁnition queried i.e. follows equation weyl’s inequality. equation follows equation substituting place equation follows equation weyl’s inequality. equation follows equation using fact vector λmax ||p||. equation follows probability atleast proof. proof lemma similar proof lemma lemma hence probability atleast using assumption ||σ/ smallest eigenvalue symmetric positive deﬁnite matrix ˆσzς−/. hence cross terms disappear expand square. equation follows equation using fact etqt equation follows equation weyl’s inequality fact maximum eigenvalue rank- matrix form ||v||. equation follows equation using assumption equation follows equation choice using lemma lower bound λmin applying union bound failure probabilities max{tδ probability atleast second step used hoeﬀding’s lemma along boundedness property shown equation upper bound shown quantity information extraction usually uncertainty label calculated using certain information-theoretic criteria entropy variance label distribution. uncertainty sampling mostly used probabilistic setting algorithms learn non-probabilistic classiﬁers using uncertainty sampling also proposed. tong proposed algorithm framework query point closest current hyperplane. seung introduced query-by-committee framework committee potential models agree currently labeled data maintained point committee members disagree considered querying. order design committee framework algorithms query-by-boosting query-by-bagging discriminative setting sampling dirichlet distribution model parameters generative setting proposed. frameworks include querying point causes maximum expected reduction error variance reducing query strategies ones based optimal design thorough literature survey diﬀerent active learning algorithms done settles algorithms consistent provable label complexity proposed agnostic setting loss recent years iwal framework introduced beygelzimer ﬁrst algorithm guarantees general loss functions. however authors unable provide non-trivial label complexity guarantees hinge loss squared loss. upal least squared losses seen using based querying strategy committee entire hypothesis space disagreement among committee members calculated using exponential weighting scheme. however unlike previously proposed committees committee inﬁnite choice point queried randomized. implemented upal along standard passive learning algorithm variant upal called using logistic loss matlab. choice logistic loss motivated fact bmal designed logistic loss. matlab codes vectorized maximum possible extent eﬃcient possible. similar upal round samples point uniformly random currently unqueried pool. however importance weights calculate estimate risk classiﬁer. purpose implementing demonstrate potential eﬀect using unbiased estimators check strategy randomly querying points helps active learning. also implemented batch mode active learning algorithm introduced which shall call bmal. paper showed superior empirical performance bmal competing pool based active learning algorithms primary motivation choosing bmal competitor pool algorithm paper. bmal like upal also proceeds rounds iteration selects examples minimizing fisher information ratio current unqueried pool queried pool. however point queried bmal never requeried. order tackle high computational complexity optimally choosing points round authors suggested monotonic submodular approximation original fisher ratio objective optimized greedy algorithm. start round when bmal already queried points previous rounds order decide point query next bmal calculate potential query product remaining unqueried points. calculation done possible potential queries takes time. hence budget total computational complexity bmal note calculation take account complexity solving optimization problem round queried point. order reduce computational complexity bmal round restrict search next query small subsample current unqueried points. value pmin step algorithm order avoid numerical problems implemented regularized version upal term λ||w|| added optimization problem shown step algorithm value allowed change current importance weight pool. optimal value chosen fold cross-validation eyeballing value gave best cost-accuracy trade-oﬀ. experiments mnist dataset datasets repository namely statlog abalone whitewine. figure shows performance algorithms ﬁrst queried points. mnist dataset average performance bmal similar upal noticeable performance bmal upal ral. similar results also seen case statlog dataset though towards performance upal slightly worsens compared bmal. however upal still better ral. figure empirical performance passive active learning algorithms.the x-axis represents number points queried y-axis represents test error classiﬁer. subsample size approximate bmal implementation ﬁxed active learning always helpful success story depends match marginal distribution hypothesis class. clearly reﬂected abalone performance better upal atleast initial stages never signiﬁcantly worse. upal uniformly better bmal though diﬀerence error rates signiﬁcant. however performance signiﬁcantly worse. similar results also seen case whitewine dataset outperforms algorithms. upal better bmal times. even witness huge performance bmal upal. conclude though computationally eﬃcient higher error rate number queries. uniformly poor performance signiﬁes querying uniformly random help. whole upal bmal perform equally well show next experiments upal signiﬁcantly better scalability especially relatively large budget round upal takes plus time solve optimization problem shown step algorithm similar optimization problem also solved bmal problem. cost solving coptt). bmal topt complexity solving optimization problem bmal round approximate implementation bmal described subsample size ﬁrst experiments budget calculate test error combined training testing time bmal upal varying sizes training set. experiments performed mnist dataset. table shows increasing sample size upal tends eﬃcient bmal though gain speed observed factor second scalability experiments ﬁxed training size studied eﬀect increasing budget. found increasing budget size speedup upal bmal increases. particular budget upal arpproximately times faster bmal. experiments dual core machine memory. paper proposed ﬁrst unbiased pool based active learning algorithm showed good empirical performance ability scale higher budget constraints large dataset sizes. theoretically proved true hypothesis linear hypothesis able recover high probability. view important extension work would establish tighter bounds excess risk. possible provide upper bounds excess risk expectation much sharper current high probability bounds. another theoretically interesting question calculate many unique queries made rounds upal. problem similar calculating number non-empty bins balls-and-bins model commonly used ﬁeld randomized algorithms motwani raghavan bins balls diﬀerent points pool bins process throwing ball round equivalent querying point round.", "year": 2011}