{"title": "Reversible Jump MCMC Simulated Annealing for Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated annealing algorithm to optimize radial basis function (RBF) networks. This algorithm enables us to maximize the joint posterior distribution of the network parameters and the number of basis functions. It performs a global search in the joint space of the parameters and number of parameters, thereby surmounting the problem of local minima. We also show that by calibrating a Bayesian model, we can obtain the classical AIC, BIC and MDL model selection criteria within a penalized likelihood framework. Finally, we show theoretically and empirically that the algorithm converges to the modes of the full posterior distribution in an efficient way.", "text": "jump markov propose chain monte carlo simulated nealing radial basis function enables distribution performs number basis functions. search joint space global thereby rameters number parameters problem local minima. surmounting also show calibrating bayesian model obtain classical model selection criteria within likelihood framework. penalized finally show theoretically empirically modes full converges algorithm distribution posterior way. efficient model likelihood estimation cali­ maximizing criteria selection accomplish goal brated posterior annealing algorithm propose jump reversible makes homogeneous mcmc kernel start arbitrary model vantage dimension jumps algorithm finds \"true\" model order. resort expensive task pos­ algorithm running sible model order subsequently best selecting model. also present convergence theorem algorithm. problem complexity comprehensive discussion readers consult details doucet paper show traditional within penalized criteria information akaike's length bayesian scription choices parameter bayesian possible model selection problem likelihood context model selection previously technique variable resolving regression linear noise sequence assumed white gaussian. mention important dependency made explicit value indeed affected parameters knowledge depending priori choose differ­ smoothness mapping jones pog­ common choices thin plate spline convenience gaussian. model expressed approximation vector-matrix form xnd)a+d+kc approach drawn appropriate prior distributions. priors degree belief values prior structure hierarchical used treat pri­ ors' parameters random vari­ ables drawn suitable distributions here focus performing model selection using classical criteria mdl. show using criteria performing model selection joint computing equivalent maximum poste­ posterior distribution. riori \"calibrated\" efficient allows interpretation develop annealing solve difficult algorithms simulated problem. optimization global outputs. ylnj denotes observations j-th output simplify notation equivalent ytlc· index appear implied referring equivalent favored longer notation invoked avoid ambiguities tation although complex essential design reversible penalized traditionally based standard tion strategies require model order. number estimates prohibitively required heuristics less appropriate selected particular model minimizes log-likelihood term depends model dimension gelfand estimate case network). different criteria information based expected involves totic bayes factor minimum information required model describes nications using conventional /-keo equations agree whenever expression proportionality ensures corresponds note purposes need proportion­ criterion ality condition log/ criteria. thus shown calibrating pri­ obtain bayesian formulation expression classi­ likelihood penalized model selection consequently likelihood penalized interpreted maximizing problem distribution sufficient conditions proper distribution overly restrictive. firstly problem setting. sec­ ondly y�nip ikylni zero shown case unless ylni spans space columns case y�nipikylni event zero mea­ sure. solve stochastic optimization problem posed previous subsection using simulated annealing annealing method involves chain whose invariant iteration longer equal moves defined heuristic considerations condition fulfilled maintain correct invariant distribution. particular choice influence convergence rate algo­ rithm. birth death moves allow network grow decrease respectively split merge moves also perform dimension changes merge move serves avoid problem placing many basis functions hand split move useful regions data close components. moves proposed ones suggested found lead satisfactory results. resulting transition markov chain mixture different tran­ sition kernels associated above. means iteration candidate moves birth death merge split update randomly chosen. probabilities choosing moves respectively bk+dk+mk+sk+uk kmax· move performed algorithm accepts death split merge moves impossible merge move also permitted kmax birth split moves therefore consequently find joint estimate ilik weak regularity probability global maxima simulated annealing proposal sampling value according moves towards probability otherwise remains obtain efficient portance choose efficient choose homogeneous satisfies involves distribution value given current markov chain -lfl/t; paramount proposal distribution. transition kernel following reversibility property metropolis algorithm djuric doucet intro­ proposes candidates according distributions. candidates ran­ acceptance ratio thus invariance markov posterior distribution. here different proposal distributions proposal domly accepted ensures chain respect chain must move across subspaces mensions complex details. selected birth basis proposing input data.", "year": 2013}