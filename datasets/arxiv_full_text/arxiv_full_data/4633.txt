{"title": "Reduced-Rank Hidden Markov Models", "tag": ["cs.LG", "cs.AI"], "abstract": "We introduce the Reduced-Rank Hidden Markov Model (RR-HMM), a generalization of HMMs that can model smooth state evolution as in Linear Dynamical Systems (LDSs) as well as non-log-concave predictive distributions as in continuous-observation HMMs. RR-HMMs assume an m-dimensional latent state and n discrete observations, with a transition matrix of rank k <= m. This implies the dynamics evolve in a k-dimensional subspace, while the shape of the set of predictive distributions is determined by m. Latent state belief is represented with a k-dimensional state vector and inference is carried out entirely in R^k, making RR-HMMs as computationally efficient as k-state HMMs yet more expressive. To learn RR-HMMs, we relax the assumptions of a recently proposed spectral learning algorithm for HMMs (Hsu, Kakade and Zhang 2009) and apply it to learn k-dimensional observable representations of rank-k RR-HMMs. The algorithm is consistent and free of local optima, and we extend its performance guarantees to cover the RR-HMM case. We show how this algorithm can be used in conjunction with a kernel density estimator to efficiently model high-dimensional multivariate continuous data. We also relax the assumption that single observations are sufficient to disambiguate state, and extend the algorithm accordingly. Experiments on synthetic data and a toy video, as well as on a difficult robot vision modeling problem, yield accurate models that compare favorably with standard alternatives in simulation quality and prediction capability.", "text": "introduce reduced-rank hidden markov model generalization hmms model smooth state evolution linear dynamical systems well non-log-concave predictive distributions continuous-observation hmms. rr-hmms assume m-dimensional latent state discrete observations transition matrix rank implies dynamics evolve k-dimensional subspace shape predictive distributions determined latent state belief represented k-dimensional state vector inference carried entirely making rr-hmms computationally efﬁcient k-state hmms expressive. learn rr-hmms relax assumptions recently proposed spectral learning algorithm hmms apply learn k-dimensional observable representations rank-k rr-hmms. algorithm consistent free local optima extend performance guarantees cover rr-hmm case. show algorithm used conjunction kernel density estimator efﬁciently model high-dimensional multivariate continuous data. also relax assumption single observations sufﬁcient disambiguate state extend algorithm accordingly. experiments synthetic data video well difﬁcult robot vision modeling problem yield accurate models compare favorably standard alternatives simulation quality prediction capability. models stochastic discrete-time dynamical systems important applications wide range ﬁelds. hidden markov models gaussian linear dynamical systems examples latent variable models dynamical systems assume sequential data points noisy incomplete observations latent state evolves time. hmms model latent state discrete variable represent belief discrete distribution states. ldss hand model latent state real-valued variables restricted linear transition observation functions employ gaussian belief distribution. distributional assumptions hmms ldss also result important differences evolution belief time. discrete state hmms good modeling systems mutually exclusive states completely different observation signatures. joint predictive distribution observations allowed non-log-concave predicting simulating future leading call competitive inhibition states competitive inhibition denotes ability model’s predictive distribution place probability mass observations disallowing mixtures observations. conversely gaussian joint predictive distribution observations ldss log-concave thus exhibit competitive inhibition. however ldss naturally model smooth state evolution hmms particularly dichotomy models hinders ability compactly model systems exhibit competitive inhibition smooth state evolution. present reduced-rank hidden markov model smoothly evolving dynamical model ability represent nonconvex predictive distributions relating discrete-state continuous-state models. hmms approximate smooth state evolution tiling state space large number low-observation-variance discrete states speciﬁc transition structure. however inference learning model highly inefﬁcient large number parameters fact existing learning algorithms expectation maximization prone local minima. rr-hmms allow reap many beneﬁts large-state-space hmms without incurring associated inefﬁciency inference learning. indeed show inference operations rr-hmm figure graphical model representation rr-hmm. denotes k-dimensional state vector mdimensional discrete state discrete observation. distributions deterministic functions illustration different rr-hmm parameters spaces random variables projection sets predictive distributions rank rr-hmm states -state full-rank similar parameters. carried low-dimensional space dynamics evolve decoupling computational cost number hidden states. makes rank-k rr-hmms computationally efﬁcient kstate hmms much expressive. though rr-hmm novel low-dimensional representation related existing models predictive state representations observable operator models generalized hmms weighted automata well representation ldss learned using subspace identiﬁcation related models algorithms discussed section learn rr-hmms data adapt recently proposed spectral learning algorithm kakade zhang learns observable representations hmms using matrix decomposition regression empirically estimated observation probability matrices past future observations. observable representation allows model sequences series operators without knowing underlying stochastic transition observation matrices. algorithm free local optima asymptotically unbiased ﬁnite-sample bound error joint probability estimates resulting model. however original algorithm bounds assume transition model full-rank single observations informative entire latent state i.e. -step observability. show generalize bounds low-rank transition matrix case derive tighter bounds depend instead allowing learn rank-k rr-hmms arbitrarily large time number samples. also describe test method circumventing -step observability condition combining observations make informative. version learning algorithm learn general psrs though error bounds don’t generalize case. experiments show learning algorithm recover underlying rr-hmm variety synthetic domains. also demonstrate rr-hmms able compactly model smooth evolution competitive inhibition clock pendulum video well real-world mobile robot vision data captured ofﬁce building. robot vision data exhibits smoothly evolving dynamics requiring multimodal predictive beliefs rr-hmms particularly suited. compare performance rr-hmms ldss hmms simulation prediction tasks. proofs details regarding examples appendix. reduced-rank hidden markov models denote discrete hidden states time denote discrete observations. rm×m state transition probability matrix rn×m observation probability matrix initial state distribution denote system’s belief i.e. distribution hidden states time denote column identity matrix equivalent conditional expectation conditioning variables clear context. addition standard notation assume rank rm×k rk×m. implies dynamics system expressed rather convention think projecting m-dimensional state distribution vector k-dimensional state vector expanding low-dimensional state back m-dimensional state distribution vector propagating forward time. possible choice independent columns columns columns contain coefﬁcients required reconstruct though choices possible also assume denote k-dimensional projection hidden state vector simply vector real numbers rather stochastic vector. assume initial state distribution lies dimensional space well i.e. vector figure illustrates graphical model corresponding rr-hmm. figure illustrates different rr-hmm parameters spaces latter parametrization casts rank-k rr-hmm k-dimensional transformed inference carried time representation. however since every trivially leads question expressive rank-k rr-hmms comparison k-state full-rank hmms. following example instructive. describe rank-k rr-hmm whose possible predictive distributions easy visualize describe. consider following rank rr-hmm states observations. observation probabilities state form interpreted discrete observations factored binary components independent given state. chosen place vertices possible predictive distributions evenly spaced points along circle -space plot marginal probability component observation ranging across achievable values latent state vector case yielding -sided polygon projection possible predictive distributions. distributions columns also plot corresponding marginals fullrank case yield triangular set. generally k-state k-sided polygon possible predictive distributions. example illustrates rank-k rr-hmms states model sets predictive distributions fullrank hmms less states cannot express. however shall inference rank-k rr-hmms arbitrary efﬁcient inference k-state full-rank hmms. implies additional degrees freedom rrhmm’s low-dimensional parameters state vectors considerable expressive power. since rr-hmms also related psrs pointed previous section since learning algorithm shown consistent estimating psrs also instructive examine expressivity psrs general. refer reader jaeger james this. learning reduced-rank hmms full-rank maximum likelihood solution parameters found iterative techniques expectation maximization however prone local optima address model selection problem. model selection algorithms avoid local minima better still guaranteed return anything close optimal data increases slow beyond certain state space magnitude. moreover learning rr-hmms face additional challenge learning factors low-rank transition matrix. could estimate followed matrix factorization algorithms singular value decomposition non-negative matrix factorization approach several drawbacks. example noisy estimate low-rank transition matrix low-rank itself could cause negative numbers appear reconstructed transition matrix. also algorithms locally optimal overly restrictive alternative approach adopt learn asymptotically unbiased observable representation rr-hmm directly using probability matrix relating past future observations. idea roots subspace identiﬁcation multiplicity automata well psr/oom literature recently formulated paper kakade zhang full-rank hmms. algorithm extending theoretical guarantees low-rank case rank transition matrix computationally difference base algorithm learn rank representation instead rank allows learn much compact representations possibly large-state-space real-world hmms greatly increases applicability original algorithm. even underlying low-rank examine singular values tune complexity underlying rr-hmm thus providing natural method model selection. present main deﬁnitions algorithm performance bounds below. detailed versions supporting proofs lemmas found appendix. vector rn×n rn×n matrices. quantities closely related matrices computed algorithms learning ooms psrs ldss using subspace identiﬁcation expressed terms parameters note contain factor hence rank rank-k rr-hmm. property important recovering estimate rr-hmm parameters matrices. primary intuition that projected onto appropriate linear subspace linearly related product rr-hmm parameters. allows devise algorithm estimates data projects appropriate linear subspace computed using uses linear regression estimate rr-hmm parameters projections. speciﬁcally algorithm attempts learn observable representation rr-hmm using matrix rn×k invertible. observable representation deﬁned follows. deﬁnition observable representation deﬁned parameters b∞{bx}n rr-hmm note dimensionality parameters determined rk×k. though deﬁnitions seem arbitrary ﬁrst sight observable representation rr-hmm closely related true parameters rr-hmm following manner hence similarity transform rr-hmm parameter matrix diagr corresponding linear transformations rr-hmm initial state distribution rr-hmm normalization vector. note must invertible relationships hold. together parameters comprise observable representation rr-hmm. learning algorithm estimate parameters data. algorithm estimating rank-k rr-hmms equivalent spectral learning algorithm learning k-state hmms. relaxation conditions performance guarantees learning rank-k rr-hmms show algorithm learns much larger class k-dimensional models class k-state hmms. learn-rr-hmm learning algorithm takes input desired rank underlying rr-hmm rather number states alternatively given singular value threshold algorithm choose rank examining singular values step assumes given independently sampled observation triples hmm. practice single long sequence observations long discount bound number samples based mixing rate case must correspond stationary distribution allow estimation algorithm results examine perform inference rr-hmm using observable representation. this need deﬁne internal state parameter linear transform initial rr-hmm belief state linear transform belief state rr-hmm time predict sequence probabilitypr =bt∞bxt internal state updatebt+ bbxtbbt bt∞bbxtbbt conditional probability given xt−pr bbt∞bbxtbbt xbbt∞bbxbbt former rarely negative; using real-valued observations makes negative normalizers even less likely since case normalizer weighted several estimated probabilities. practice recommend thresholding normalizers small positive number trusting probability estimates steps normalizers fall threshold. note inference operations occur entirely mentioned earlier parameterizing rr-hmm parameters observations casts dimensions. fact learning inference algorithms rr-hmms proposed dependence number states whatsoever though learning algorithms rr-hmms depend rr-hmm formulation intuitively appealing idea large discrete state space low-rank transitions approach also provably consistent learning algorithm psrs general ﬁnite-sample performance guarantees case rr-hmm. since psrs provably expressive compact ﬁnite-state hmms indicates learn powerful class models hmms using algorithm. following ﬁnite sample bound estimated model generalizes analogous results case low-rank theorem bounds error joint probability estimates learned model. bound shows consistency algorithm learning correct observable representation underlying rr-hmm without ever needing recover high-dimensional parameters latent representation. note error bounds independent number hidden states; instead depend rank transition matrix much smaller since explicitly assumes full-rank transition matrix bounds become vacuous otherwise generalizing framework involves relaxing condition generalizing theoretical guarantees deriving proofs guarantees. deﬁne denote largest singular value matrix sample complexity bounds depend polynomially /σk. larger well-separated dynamics noise. larger informative observation regarding state. quantities larger magnitude fewer samples need learn good model. bounds also depend term minimum number observations account total probability mass i.e. number important observations. recall number independently sampled observation triples comprise training data though mentioned earlier also learn single long training sequence. theorem holds mild conditions. conditions namely prior nonzero everywhere number matrices interest rank least invertibility reasons. conditions unique low-rank setting namely diagπ)ot rank least implies column space space degree overlap. satisﬁed case hmms thinking containing linearly independent probability distributions along columns containing coefﬁcients needed obtain columns. alternatively conditions satisﬁed arbitrary scaling entries scaling entries accordingly. however increases hence price increasing number samples needed attain particular error bound. appendix formal statements conditions. probability matrix relates past timestep future timestep assumption vector observation probabilities single step sufﬁcient disambiguate state system identiﬁcation theory corresponds assuming -step observability assumption unduly restrictive many real-world dynamical systems interest. complex sufﬁcient statistics past future need modeled block hankel matrix formulations subspace methods identify linear systems -step observable. rr-hmms corresponds case and/or rank similar hankel matrix formulation stack multiple observation vectors augmented observation comprises data several possibly consecutive timesteps. observations augmented observation vectors assumed non-overlapping i.e. observations observation vector time larger time indices observations observation vector time corresponds assuming past sequences future sequences spanning multiple timesteps events characterize dynamical system causing larger. note still denotes single observation whereas indices associated events. example stack consecutive observations equals probability seeing n-length sequence followed single observation followed n-length sequence. empirically estimating matrix consists scanning appropriate subsequences separated observation symbol normalizing obtain occurrence probability. become larger matrices larger events past future. however stacking observations complicate dynamics shown rank cannot exceed since learning algorithm relies means augmenting observations increase rank trying recover. also since still observation probability matrix respect single unstacked observation middle number observable operators need remains constant. complexity bounds successfully generalize case since rely matrices probabilities summing here. extension given learning hmms ambiguous observations differs approach suggested simply substitutes observations overlapping tuples observations pr). potential problems approach. first number observable operators increases exponentially length tuple observable operator tuple instead observation. second cannot decomposed product matrices includes consequently longer rank equal rank modeled. thus learning algorithm could require much data recover correct model approach. default rr-hmm formulation assumes discrete observations. however since model formulation converts discrete observations n-dimensional probability vectors ﬁltering smoothing learning algorithms discuss same straightforward model multivariate continuous data kernel density estimation affects assume ease notation i.e. though practice could learning single long sequence also assume observation vector contains single observation though technique easily combined sophisticated sequence-based learning feature-based learning methods described above. pick kernel function kernel centers bandwidth parameter goes zero appropriate rate limit. first compute feature vectors compute ‘base’ observable operators estimated probability matrices well vectors using algorithm learn-rr-hmm given parameters ﬁltering sequence figure learning discrete rr-hmms. three ﬁgures depict actual eigenvalues three different rr-hmm transition matrices eigenvalues rr-hmm observable operators estimated training observations. -state -observation rank rr-hmm. full-rank -state -observation hmm. -state -observation rank rr-hmm. theoretical results carry case modiﬁcations described rr-hmm document. essentially bound still holds predicting functions though results connecting bound error estimating probabilities observations. designed several experiments evaluate properties rr-hmms learning algorithm synthetic real-world data. ﬁrst experiments tests ability spectral learning algorithm recover correct rr-hmm. second experiment evaluates representational capacity rrhmm learning model video requires competitive inhibition smooth state evolution. third experiments tests model’s ability learn ﬁlter predict simulate video captured robot moving indoor ofﬁce environment. first evaluate unbiasedness spectral learning algorithm rr-hmms synthetic examples. case build rr-hmm sample observations model estimate model spectral learning algorithm learned model eigenvalues transition matrix true model. similarity transform therefore non-zero eigenvalues expect estimated eigenvalues converge true eigenvalues enough data. necessary condition unbiasedness sufﬁcient one. section appendix parameters hmms used examples below. example rr-hmm examine hidden states observations full-rank observation matrix rank transition matrix. figure plots true estimated eigenvalues increasing size dataset along error bars suggesting recover true dynamic model. figure clock video texture simulated stable rr-hmm. clock modeled -state hmm. manifold consists principal components predicted observations simulation. generated frames coherent motion video jerky. clock modeled -dimensional lds. manifold indicates trajectory model state space simulation. motion video smooth frames degenerate superpositions. clock modeled rank rr-hmm. manifold consists trajectory model dimensional subspace state space simulation. motion frames correct. example -step-observable examine hidden states observations full-rank transition matrix violates condition. parameters cannot estimated original learning algorithm since single observation provide enough information disambiguate state. stacking consecutive observations however spectral learning algorithm applied successfully example -step-observable rr-hmm examine hidden states observations rank transition matrix example rank multiple observations required disambiguate state. again stacking consecutive observations conjunction spectral learning algorithm enough recover good rr-hmm parameter estimates competitive inhibition smooth state evolution video model clock pendulum video consisting frames -state dimensional rank rr-hmm stacked observations. note could easily learn models latent states/dimensions; limited dimensionality order demonstrate relative expressive power different models. convert continuous data discrete observations kernel centers sampled sequentially training data. trained resulting discrete using learned directly video using subspace stability constraints using hankel matrix stacked observations. trained rr-hmm stacking observations choosing approximate rank dimensions learning observable operators corresponding gaussian kernel centers. simulate series observations model compare manifolds underlying simulated observations frames simulated videos small number states sufﬁcient capture smooth evolution clock simulated video characterized realistic looking frames exhibits jerky irregular motion. although -dimensional subspace captures smooth evolution simulated video system quickly degenerates individual frames video modeled poorly rr-hmm simulated video beneﬁts smooth state evolution competitive inhibition. manifold -dimensional subspace smooth structured video realistic. results demonstrate rr-hmm beneﬁts smooth state evolution compact state space beneﬁt competitive inhibition hmm. compare hmms ldss rr-hmms problem modeling video data mobile robot indoor environment. video frames collected point grey bumblebee stereo camera mounted botrics obot mobile robot platform circling stationary obstacle frames used training figure sample images robot’s camera. ﬁgure depicts hallway environment central obstacle path robot took environment collecting data squared error prediction different estimated models baselines averaged different initial ﬁltering durations data model. frame training data reduced dimensions single observations. using training data trained rr-hmm using spectral learning sequences continuous observations gaussian kernels centers -dimensional using subspace hankel matrices timesteps -state discrete observations using convergence. model performed ﬁltering different extents predicted image steps future squared error prediction pixel space recorded averaged different ﬁltering extents obtain means plotted figure baselines plot error obtained using mean ﬁltered data predictor error obtained using last ﬁltered observation baselines perform worse complex algorithms indicating nontrivial prediction problem. well initially well longer rr-hmm performs well better time scales since models smooth state evolution competitive inhibition predictive distribution. particular rr-hmm yields signiﬁcantly lower prediction error consistently duration prediction horizon predictive state representations observable operator models model sequence probabilities product observable operator matrices. idea well idea learning models using linear algebra techniques originates literature multiplicity automata weighted automata despite recent improvements practical learning algorithms psrs ooms lacking. rr-hmms spectral learning algorithm also closely related methods subspace identiﬁcation control systems learning parameters determine relationship hidden states observations. pointed earlier spectral learning algorithm presented learns psrs. brieﬂy discuss algorithms learning psrs data. several learning algorithms psrs proposed easier learning algorithms return consistent parameter estimates parameters based observable quantities. develops svd-based method ﬁnding low-dimensional variant psrs called transformed psrs instead tracking probabilities small number tests tpsrs track small number linear combinations larger number tests. allows compact representations well dimensionality selection based examining singular values decomposed matrix subspace identiﬁcation methods. note nonlinearity encoded introduced concept e-tests psrs indicator functions aggregate design core tests. sets future outcomes e.g. sequence observations immediate future particular observation timesteps. general tests discrete psrs indicator functions arbitrary statistics future events thus encoding nonlinearities might essential modeling dynamical systems. recently exponential family psrs introduced attempt generalize model allow general exponential family distributions next observations. efpsr state represented modeling parameters time-varying exponential family distribution next timesteps. allows graphical structure encoded distribution choosing parameters accordingly. justiﬁcation choosing exponential family comes maximum entropy modeling. though inference parameter learning difﬁcult graphical models non-trivial structure approximate inference methods utilized make problems tractable. like plgs dynamical component efpsrs modeled extending conditioning distribution time. however method presented drawbacks e.g. extend-and-condition method inconsistent respect marginals individual timesteps extended un-extended distributions. rr-hmms algorithms also related hybrid models. note previous models name address completely different problem i.e. reducing rank gaussian observation parameters. since shortly advent ldss attempts combine discrete states hmms smooth dynamics ldss. perform brief review literature hybrid models; thorough review. formulates switching variant state observation variable noise models mixture gaussians mixture switching variable evolving according markovian dynamics derives optimal ﬁltering equations number gaussians needed represent belief increases exponentially time. also propose approximate ﬁltering algorithm model based single gaussian. proposes learning algorithms switching observation matrices. reviews models observations state variable switch according discrete variable markov transitions. hidden filter hmms combine discrete realvalued state variables outputs depend both. real-valued state deterministically dependent previous observations known manner discrete variable hidden. allows exact inference model tractable. formulates mixture kalman filter model along ﬁltering algorithm similar except ﬁltering algorithm based sequential monte-carlo sampling. commonly used hmms mixture-model observations special case rr-hmms. kstate state corresponds gaussian mixture observation models dimensions subsumed k-rank rr-hmm distinct continuous observations dimensions each since former constrained non-negative various places latter not. switching state-space models posit existence several real-valued hidden state variables evolve linearly single markovian discrete-valued switching variable selecting state explains real-valued observation every timestep. since exact inference learning intractable model authors derive structured variational approximation decouples state space switching variable chains effectively resulting kalman smoothing state space variables forward-backward switching variable. experiments authors sssms perform better regular ldss physiological data modeling task multiple distinct underlying dynamical models. hmms performed comparably well terms log-likelihood indicating ability model nonlinear dynamics though resulting model less interpretable best sssm. recently models nonlinear time series modeling gaussian process dynamical models proposed however parameter learning algorithm locally optimal exact inference simulation expensive requiring mcmc long sequence frames once. necessitates heuristics inference learning. another recent nonlinear dynamic model differs greatly methods treats component dynamic model learning problem separately using supervised learning algorithms proves consistency aggregate result certain strong assumptions. spectral learning algorithm blurs line latent variable models psrs. psrs developed focus problem agent planning actions partially observable environment. generally many scenarios sequential data modeling underlying dynamical system inputs. inference task learned model track belief state conditioning observations incorporating inputs. input-output conditional probabilistic model properties. natural generalization work task learning rr-hmms inputs controlled psrs. recently carried generalization controlled psrs; details found question proving containment equivalence rr-hmms respect psrs theoretical interest. observable representation rr-hmm transformed every rr-hmm psr; remains seen whether every corresponds rr-hmm well. idea difﬁcult psrs somehow correspond rr-hmms large inﬁnite state space intuitively appealing straightforward prove. another interesting direction would bound performance learning algorithm underlying model approximately reduced-rank much algorithm includes bounds underlying model approximately would useful since practice realistic expect underlying system comply exact model assumptions. positive realization problem i.e. obtaining stochastic transition observation matrices rr-hmm observable representation also signiﬁcant though observable representation allows carry possible operations. describes method based which however highly erratic practice. rr-hmm case additional challenge ﬁrstly computing minimal positive realization exists since algorithm learns psrs guarantee particular learned parameters conforms exactly rr-hmm. applications side would interesting compare rr-hmms dynamical models classiﬁcation tasks well learning models difﬁcult video modeling graphics problems simulation purposes. elaborate choices features useful applications would usage high-dimensional inﬁnite-dimensional features reducing kernel hilbert spaces acknowledge helpful conversations sham kakade regarding spectral learning algorithm julian ramos assisted gathering robot vision data used experiments. supported grant number usaf grant number fa--c- usda grant number project mobilefusion/ttc. supported grant number eeec-. supported darpa grant number hr-- computer science study panel program darpa/aro muri grant number wnf---. supported muri grant number n--.", "year": 2009}