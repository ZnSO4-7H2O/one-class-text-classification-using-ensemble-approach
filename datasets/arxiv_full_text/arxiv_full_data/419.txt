{"title": "Multi-Agent Diverse Generative Adversarial Networks", "tag": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "stat.ML"], "abstract": "We propose an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known mode collapse problem. Firstly, we propose a multi-agent GAN architecture incorporating multiple generators and one discriminator. Secondly, to enforce different generators to capture diverse high probability modes, we modify discriminator's objective function where along with finding the real and fake samples, the discriminator has to identify the generator that generated the fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. Our framework (MAD-GAN) is generalizable in the sense that it can be easily combined with other existing variants of GANs to produce diverse samples. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for the challenging tasks such as image-to-image translation (known to learn delta distribution) and face generation. In addition, we show that MAD-GAN is able to disentangle different modalities even when trained using highly challenging multi-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we also show its efficacy for the unsupervised feature representation task. In the appendix we introduce a similarity based competing objective which encourages the different generators to generate varied samples judged by a user defined similarity metric. We show extensive evaluations on a 1-D setting of mixture of gaussians for non parametric density estimation. The theoretical proofs back the efficacy of the framework and explains why various generators are pushed towards distinct clusters of modes.", "text": "propose intuitive generalization generative adversarial networks conditional variants address well known mode collapse problem. firstly propose multi-agent architecture incorporating multiple generators discriminator. secondly enforce different generators capture diverse high probability modes modify discriminator’s objective function along ﬁnding real fake samples discriminator identify generator generated fake sample. intuitively succeed task discriminator must learn push different generators towards different identiﬁable modes. framework generalizable sense easily combined existing variants gans produce diverse samples. perform extensive experiments synthetic real datasets compare mad-gan different variants gan. show high quality diverse sample generations challenging tasks image-to-image translation face generation. addition show mad-gan able disentangle different modalities even trained using highly challenging multi-view dataset also show efﬁcacy unsupervised feature representation task. appendix introduce similarity based competing objective encourages different generators generate varied samples judged user deﬁned similarity metric. show extensive evaluations setting mixture gaussians parametric density estimation. theoretical proofs back efﬁcacy framework explains various generators pushed towards distinct clusters modes. figure multi-view data generation using mad-gan notice three generators generate images different modalities trained highly challenging multi-view data column represents generations given random noise input generating data points given unlabeled dataset core challenging problem machine learning. paper address task using generative models. underlying idea behind models generate highdimensional data images texts using low/highdimensional interpretable latent space. though models highly useful various applications computationally challenging efﬁciently train normally requires intractable integration high-dimensional space. recently remarkable progress ﬁeld development generative models explicitly require integration trained using back-propagation algorithm. famous examples generative adversarial networks variational autoencoders work focus gans known produce sharp plausible images. brieﬂy objective function gans employ generator discriminator involved minimax game. task discriminator learn difference ‘real’ ‘fake’ samples however task generator maximize mistakes discriminator. convergence generator learns produce real looking images. successful applications gans video generation image inpainting image manipulation object generation interactive image generation using brush strokes image superresolution diagrammatic abstract reasoning conditional gans despite remarkable success gans major drawback problem ‘mode collapse’ even though theoretically convergence generator able learn true data distribution practically observed difﬁculties involved optimizing gans makes hard reach true equilibrium. broadly speaking directions issue addressed improving learning aspect gans similar enforcing gans capture diverse modes similar work focus latter. inspired multi-agent algorithm coupled propose multiple generators discriminator allow generators share information. call multi-agent architecture shown figure detail similar standard objective generator maximize mistakes common discriminator. allow different generators share information other share initial layer parameters among generators. anreason behind sharing parameters fact initial layers capture high-frequency structures alparticular type dataset therefore sharing reduces redundant computations. naively using simple approach lead trivial solution generators learn generate similar samples. resolve issue generate different visually plausible samples capturing diverse high probability modes propose simple intuitive solution. propose modify objective function discriminator which along ﬁnding real fake samples discriminator also correctly predict generator generated given fake sample. intuitively order succeed task discriminator must learn push generations corresponding different generators towards different identiﬁable modes. experimentally show approach highly effective outperforms existing approaches terms diverse generations. also provide theoretical analysis approach show proposed modiﬁcation discriminator’s objective function allows generators learn together mixture model generator represents mixture component. show convergence global optimum value achieved number generators. combining multiagent architecture diversity enforcing term allows generate diverse plausible samples thus name multi-agent diverse analyze mad-gan extensive experiments. first synthetic datasets show mad-gan able capture diverse clustered modes. move towards complicated stacked/compositional mnist dataset hand engineered modes compare mad-gan several variants gans. using divergence number modes recovered criterion show approach outperforms variants compare with able generate high quality samples capturing high number modes. evaluate approach real world much challenging datasets show high quality diverse sample generations challenging tasks image-to-image translation multiview generations face generation using svhn dataset show framework capable learning better feature representation unsupervised setting compared dcgan figure multi-agent diverse discriminator outputs softmax scores signifying probability input sample either generators real distribution. thus follows generator identiﬁcation based objective. last layer parameters generators shared. recent work called infogan proposed information-theoretic extension gans order address problem mode collapse. brieﬂy infogan disentangles latent representation assuming factored representation latent variables. order enforce generator learn factor speciﬁc generations infogan maximizes mutual information factored latents generator distribution. proposed mode regularized uses encoder-decoder paradigm. basic idea behind modegan sample true data distribution belongs particular mode sample generated generator true sample passed encoder-decoder likely belong mode. modegan assumes exists enough true samples mode generator able capture another work metz proposed surrogate objective update generator respect unrolled optimization discriminator address issue convergence training process gans. improves training process generator turn allow generators explore wide coverage true data distribution. presented coupled method training generators shared parameters learn joint distribution data. shared parameters guide generators towards similar subspaces since trained independently domains promote diverse generations. durugkar proposed model multiple discriminators whereby ensemble multiple discriminators shown stabilize training generator guiding produce better samples. w-gan recent technique employs integral probability metrics based earth mover distance rather js-divergences original uses. began builds upon w-gan using autoencoder based equilibrium enforcing technique alongside wasserstein distance. dcgan iconic technique used fully convolutional generator discriminator ﬁrst time able generate compelling generations along introduction batch normalization thus stabilizing training procedure. gogan introduced training procedure training discriminator using maximum margin formulation alongside earth mover distance based wasserstein- metric. introduced technique theoretical formulation stating importance multiple generators discriminators order completely model data distribution. present brief review gans given unlabelled samples true data distribution learning problem obtain optimal parameters generator sample approximate data distribution prior input noise order learn optimal objective employs discriminator learns differentiate ‘real’ ‘fake’ sample overall objective function folobjective optimized block-wise manner optimized time ﬁxing other. intuitively given sample parameter function produces score represents probability belonging true data distribution hence discriminator’s objective learn parameters maximizes score true samples minimizing fake ones hand generator’s objective maximize ez∼pz θd). thus generator learns maximize scores fake samples exactly opposite discriminator trying achieve. manner generator discriminator involved minimax game task generator maximize mistakes discriminator. theoretically equilibrium generator learns generate real samples means objective generator’s task much harder discriminator produce real looking images maximize mistakes discriminator. this along minimax nature objective raise several challenges gans mode collapse; difﬁcult optimization; trivial solution. work propose framework address ﬁrst challenge ‘mode collapse’ increasing capacity generator well known optimization tricks partially avoid challenges brieﬂy propose multi-agent architecture employs multiple generators discriminator order generate different samples capturing high probability regions true data distribution. order direct different generators towards diverse modes propose modify objective function discriminator along ﬁnding fake samples discriminator identify generator produced given fake sample. thus follows generator identiﬁcation based objective. theoretically show objective allows generators mixture model generator capturing component. also provide conditions global optimality. framework general sense used different variants gan. tween soft-max output discriminator dirac delta distribution sample belongs j-th generator otherwise therefore objective function optimization keeping constant becomes ex∼p i=supp∪ supp where supp intuitively negative cross entropy function. order correctly identify generator produced given fake sample discriminator must learn push different generators towards different identiﬁable modes. however objective generator remains standard gan. thus i-th generator objective minimize following ex∼pd dk++ez∼pz log; θd))). notice generators case updated parallel. discriminator given corresponding gradient j-th index therefore using approach requires minor modiﬁcations standard optimization algorithm easily used different variants gan. theorem shows objective function actually allows generators form mixture model generator represents mixture component global optimum achieved pgi. notice that case generator obtain exactly jensenshannon divergence based objective function shown optimal value theorem given optimal discriminator objective function training generators boils minimizing figure visualization different generators pushed towards different modes. note could cluster modes cluster contain different modes. arrows abstractly represent generator speciﬁc gradients purpose building intuition. describe proposed architecture involves generators discriminator. case single-view data allow generators share information tying initial layer parameters. essential avoid redundant computations initial layers generator captures high-frequency structures almost particular type dataset. also allows different generators converge faster. however case multi-view data necessary avoid sharing parameters allow generator capture view speciﬁc structures. speciﬁcally given i-th generator similar standard ﬁrst step involves generating sample ˜xi. since generator receives latent input sampled distribution naively using simple approach lead trivial solution generators learn generate similar samples. follows propose intuitive solution avoid issue capture diverse modes. inspired discriminator formulation semisupervised learning generator identiﬁcation based objective function that along minimizing score requires discriminator identify generator generated given fake sample order opposed standard objective function discriminator outputs scalar value modify output soft-max scores. detail given generators discriminator produces softmax probability distribution classes. score index represents probability sample belongs true data distribution score k}-th index represents probability generated j-th generator. setting learning optimize cross-entropy bediscussion generators’ objective minimized discriminator optimal holds true analyze carefully inﬁnitely many conﬁgurations generators assign probabilities different still holds true. different conﬁgurations cross entropy error discriminator different minimum cross entropy occurs point means given enough capacity generators discriminator good enough optimizer modes distributed among generators generators tasked mastering distinct modes data distribution. obvious question could arise possible generators learn capture mode?. short answer theoretically practice begin discussion understand this. theoretically according theorem also minimum objective value achieved. implies worst case mad-gan would perform standard gan. however discussed below possible following highly unlikely situations generators always generate exactly similar samples discriminator able differentiate them. case discriminator learn uniform distribution generator indices thus gradients passed discriminator exactly generators. however situation general possible generators initialized differently. even slight variation samples generators enough discriminator identify pass different gradient information generator. addition objective function generators generate ‘real’ samples thus nothing encourage generate exactly samples. discriminator enough capacity learn optimal parameters. contrast assumption made theorem discriminator optimal. thus enough capacity learn feature representation correctly identify samples different generators. practice easy task modify anything feature representation stage architecture discriminator. used standard architectures tasks. hence random initializations sufﬁcient capacity generator/discriminator easily avoid trivial solution generators focus exactly region true data distribution. clearly supported various experiments showing diverse generations mad-gan. show efﬁcacy framework synthetic real-world datasets. first simple mixture gaussian distribution show framework able capture diverse modes. next perform experiments stacked/compositional mnist dataset compare methods several known variants gans dcgan wgan began gogan unrolled mode-reg furthermore also created another baseline called ma-gan trivial extension multiple generators discriminator. opposed mad-gan ma-gan simple multi-agent architecture without modiﬁcation discriminator’s objective function. comparison allow better understand effect explicitly enforcing diversity mad-gan’s objective. kl-divergence number modes recovered criterion comparison show superior results compared methods. addition show diverse generations challenging tasks ‘image-to-image translation’ ‘multi-view data generation’ ‘face generation’. note image-to-image translation objective known learn delta distribution thus agnostic input noise vector. however show mad-gan able produce highly plausible diverse generations task. show mad-gan able learn better feature space unsupervised representation learning task. provide detailed overview architectures datasets parameters used experiments respective references section paper. order understand behaviour mad-gan different state-of-the-art models ﬁrst perform simple synthetic experiment much easier generating high-dimensional complex images. consider distribution mixture components modes standard deviation respectively. ﬁrst modes overlap signiﬁcantly ﬁfth mode stands isolated evident figure train different models using samples distribution generate data points model. order compare learned distribution ground truth distributions ﬁrst estimate using bins data points create histograms. histograms carefully created using different sizes best chosen. then chi-square distance kl-divergence compute distance histograms. figure table evident madfigure example understand behaviour different variants order compare mad-gan orange bars show density estimate training data blue ones generated data points. careful cross-validation chose size figure example understand behavior mad-gan different number generators orange bars show density estimate training data blue ones generated data points. careful cross-validation chose size able capture clustered modes includes signiﬁcantly overlapped modes well. mad-gan obtains minimum value terms chi-square distance kl-divergence. experiment mad-gan ma-gan used four generators. provide in-depth analysis effect varying generators supplementary material. understand effect varying number generators mad-gan synthetic experiment setup i.e. real data distribution gaussians. million sample points real distribution generate equal number points generators million. results shown figure corresponding table quite clear number generators increased sampling keeps getting realistic. real data distribution four generators enough capture modes. increase number generators further generator capture mode. hence sampling samples obtained mode. seen figure number generators generators focus overlapping modes non-overlapping ones samples region overlap. move challenging setup similar order examine compare mad-gan variants. created stacked mnist dataset samples sample three channels stacked together random mnist digit them. thus creating distinct modes data distribution. used stripped version generator discriminator pair reduce modeling capacity. follow path fair comparisons used architecture mentioned paper. similarly created compositional mnist whereby took random mnist digits place quadrants dimensional image. also resulted data distribution modes. distribution resulting generated samples estimated using pretrained mnist classiﬁer classify digits either channels quadrants decide modes particular generation referred tables compare method many variants terms divergence number modes recovered stacked compositional mnist datasets respectively. evident table mad-gan outperforms variants terms number modes recovered divergence. interestingly case compositional mnist experiments shown table madgan wgan unrolled able recover modes. however terms divergence distribution generated mad-gan closest true data distribution. table stacked-mnist experiment. modes dataset. mad-gan recovered maximum number modes also generated distribution closest true data distribution compared variants terms divergence. table compositional-mnist experiment. modes dataset. mad-gan wgan unrolled recovered modes however terms divergence distribution generated mad-gan closest true data distribution. task known learn delta distribution thus generates image irrespective variations input noise vector. generating diverse samples setting open problem. experiments show mad-gan able generate diverse samples task. claim capture possible modes knowing priori number modes data distribution possible. however show mad-gan generate many diverse samples number generators use. note that adding generators requires adding additional last layer. follow approach employ patch based conditional task. figure diverse generations ‘edges handbags’ generation task. sub-ﬁgure ﬁrst column represents input columns represents generations mad-gan columns generations infogan. evident different generators able produce diverse results capturing different colors textures design patterns among others. however infogan generations visually same clearly indicated able capture diverse modes. figure infogan ‘edges handbags’ task using three categorical code values. sub-ﬁgure ﬁrst column represents input columns represents generations input categorical code besides conditioning image columns generations noise additional input. generations architectures visually irrespective categorical code value clearly indicates able capture diverse modes. latent codes infogan enable diverse generations. however infogan used situations bias introduced categorical variables significant impact generator network. image-to-image translation high resolution generations categorical variable sufﬁcient impact generations. seen shortly validate hypothesis comparing method infogan task. infogan generator capture three kinds distinct modes categorical code take three values. hence case categorical code matrix third entries remaining category. generator input image along categorical code appended channel wise image. netfigure shows generations mad-gan infogan ‘edges handbags’ task given edges handbags objective generate real looking handbags. clearly mad-gan generator able produce meaningful diverse images terms ‘color’ ‘texture’ ‘patterns’. however infogan generations almost three categorical code values. results shown infogan obtained sharing discriminator network parameters. explore mode capturing capacity madgan experimented much challenging task multi-view data generation. detail trained madgan combined dataset consisting various highly diverse images islets icebergs broadleaf-forest bamboo-forest bedroom obtained places dataset create training data images randomly selected them creating dataset consisting images. generators architecture dcgan. case images belong diverse modalities generator parameters shared. shown figure surprise found that even highly challenging setting generator able generate samples different modality. clearly indicates mad-gan able disentangle diverse modes even training dataset highly challenging given contains images varying modalities. section show diverse face generations using mad-gan dcgan generators. again setting provided dcgan. high quality face generations shown figure similarly dcgan train framework using svhn dataset trained discriminator used extract features. using features train classiﬁcation task. mad-gan obtain misclassiﬁcation error almost better results reported dcgan clearly indicates framework able learn better feature space unsupervised setting. presented simple effective framework multi-agent diverse generating diverse meaningful samples. showed efﬁcacy approach compared various variants using extensive experiments showed captures diverse modes producing high quality samples. presented theoretical analysis mad-gan conditions global optimality. interesting future direction would estimate priori number generators needed. figure diverse generations ‘night day’ image generation task. first column sub-ﬁgure represents input. remaining three columns show diverse outputs different generators. evident different generators able produce diverse results capturing different lighting conditions different patterns different weather conditions different landscapes among many minute useful cues. figure face generations using mad-gan. generator employed dcgan. represents generator. column represents generations given random noise input ﬁrst generator generating faces pointing left. second generator generating female faces long hair third generator generates images light background. bags’ task. figure experiments sharing initial layers discriminator network. ﬁrst experiment input categorical code besides conditional image. second experiment noise also added input. architecture details given section figure show results experiments side side. still many perceivable changes vary categorical code values. generator simply learn ignore input noise also pointed addition figure show diverse generations using mad-gan ‘night day’ task given night images places objective generate equivalent images. generated images figure differ terms lighting conditions patterns weather conditions many minute useful cues. figure face generations using mad-gan. sub-ﬁgure represents generations single generator. ﬁrst generator generating faces dark background. second generator generating female faces long hair light background third generating faces colored background casual look", "year": 2017}