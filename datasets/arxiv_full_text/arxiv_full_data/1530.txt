{"title": "Embedding Lexical Features via Low-Rank Tensors", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.", "text": "avoid model over-ﬁtting often results features lexical components several smoothed lexical representations proposed shown improve performance various tasks; instance word embeddings help improve dependency parsing semantic role labeling however using word embeddings sufﬁcient represent complex lexical features features word embedding conjoined different non-lexical properties result features indicating different labels; corresponding lexical feature representations take interactions consideration. important interactions also increase risk over-ﬁtting feature space grows exponentially capture interactions representation learning remains open question. address problems propose general uniﬁed approach reduce feature space constructing low-dimensional feature representations provides combining word embeddings traditional non-lexical properties label information. model exploits inner structure features breaking feature multiple parts lexical non-lexical label. demonstrate full feature outer product among parts. thus parameter tensor scores feature produce prediction. model reduces number parammodern models rely heavily engineered features often combine word contextual information complex lexical features. combination results large numbers features lead overﬁtting. present model represents complex lexical features—comprised parts words contextual information labels—in tensor captures conjunction information among parts. apply lowrank tensor approximations corresponding parameter tensors reduce parameter space improve prediction speed. furthermore investigate methods handling features include n-grams mixed lengths. model achieves state-of-the-art results tasks relation extraction ppattachment preposition disambiguation. statistical models usually rely handdesigned features customized task. features typically combine lexical contextual information label scored. relation extraction example parameter presence speciﬁc relation occurring feature conjoining word type dependency path information measuring phrase semantic similarity word type conjoined position phrase signal role. figure shows example dependency parsing multiple types conjoined tags distance information. figure example lexical features used dependency parsing. predict pmod with rely lexical features indices word with child candidate head. figure shows ﬁfth feature like candidate see. common multi-class classiﬁcation tasks template generates different feature label thus feature conjunction four parts. figure one-hot representation equivalent outer product among four one-hot vectors. means vector single non-zero element position. eters approximating parameter tensor low-rank tensor tucker approximation applied embedding type canonical/parallel-factors decomposition models fewer parameters previous work learns separate representation feature approximation also allows much faster prediction going method cubic rank exponential number lexical parts method linear both. furthermore consider methods handling features rely n-grams mixed lengths. applied combine different views features. compared work usage cp-decomposition different application feature learning focus dimensionality reduction existing well-veriﬁed features generates features combining atom features. thus work ignore useful features; relies binary features supplementary model needs not. factorization relies views explicit meanings e.g. head/modiﬁer/arc dependency parsing making less general. therefore applications tasks like relation extraction less obvious. resulting method learns smoothed feature representations combining lexical non-lexical label information achieving state-of-the-art performance several tasks relation extraction preposition semantics pp-attachment. ×i=k matrix obtained concatenating thei=k mode-k ﬁbers along columns. given matrices rd×r rd×r write denote kronecker product deﬁne frobenius product tensor i-mode product matrix. tensor size called core tensor. tucker rank rank) rank mode-i unfolding. simplify learning deﬁne tucker rank r=rank) bounded simply dimensions i.e. allows enforce rank constraint simply restricting dimensions described decomposition decomposition represents d×d×. .×dk tensor rank-one tensors matrix vector j-th row. decomposition rank tensor deﬁned number rank-one tensors decomposition. decomposition viewed special case tucker decomposition superdiagonal tensor. suppose feature includes information label multiple lexical items non-lexical property feature factorized conjunction part .∧wn. feature ﬁres parts instance one-hot representation viewed tensor ⊗···⊗ feature part also represented one-hot vector. figure illustrates case lexical parts. given input instance associated label extract features traditional log-linear model view instance bag-of-features i.e. feature vector dimension corresponds feature value log-linear model scores instance w... note one-hot vectors words large thus formulation parameter tensor large making parameter estimation difﬁcult. instead estimating values dimensions appear training data traditional methods reduce size tensor low-rank approximation. different approximation methods different equivalent forms e.g. optimization objective loss function training log-linear model uses scores e.g. exp{s} log-loss exp{s;t learning formulated following optimization problem constraints rank depend chosen tensor approximation method framework advantages first discussed here hope representations capture rich interactions different parts lexical features; low-rank tensor approximation methods keep important interaction information original tensor signiﬁcantly reducing size. second low-rank structure encourage weight-sharing among lexical features similar decomposed parts leading better model generalization. note examples features different numbers multiple lexical parts unigram bigram features pp-attachment. different methods handle features factorization) remarks compared prior work e.g. proposed factorization following advantages parameter explosion mapping view lexical properties representation vector factorization allows model treat word embeddings inputs views lexical parts dramatically reducing parameters. prior work cansince views mixtures lexical non-lexical properties. note uses embeddings concatenating speciﬁc views increases dimensionality improvement limited. weight-sharing among conjunctions lexical property like child-word word conjunction head-postag word word figure factorization prior work treats independent features greatly increasing dimensionality. factorization builds representations features based embedding word thus utilizing connections reducing dimensionality. using one-hot encodings parts feature results large tensor. section shows compute score without constructing full feature tensor using tensor approximation methods begin intuition. score original tensor representation need parameter tensor size vocabulary size number lexical parts feature number different labels non-lexical properties respectively. methods reduce tensor size embedding part lower dimensional space represent label non-lexical property words dimensional vector respectively ∀i). embedded features scored much smaller tensors. denote transformations methods transformations embeddings parts low-rank tensors embeddings non-lexical properties labels trained simultaneously low-rank tensors. note one-hot input encodings transformation matrices essentially lookup tables making computation transformations sufﬁciently fast. amounts ﬁrst projecting lower dimensional vectors weighting hidden representations using ﬂattened core tensor low-dimensional representations corresponding weights learned jointly using discriminative criterion. call model based representation low-rank feature representation tucker form lrfrn-tucker. mensionality address scalability control complexity tensor based model approximate parameter tensor using decomposition resulting following scoring function pre-trained word embeddings computational statistical bottlenecks learning lrfrn models vocabulary size; number parameters learn matrix scales linearly would require large sets labeled training data. alleviate problem pre-trained continuous word embeddings input embeddings rather one-hot word encodings. denote m-dimensional word embeddings transformation matrices lexical parts size note sufﬁciently large labeled data available model allows ﬁne-tuning pre-trained word embeddings improve expressive strength model common deep network models. remarks lrfrs introduce embeddings non-lexical properties labels making better suit common setting rich linguistic properties; large label sets open-domain tasks lrfr-cp better suits n-gram features since increases parameters corresponding also efﬁcient prediction since cost transformations ignored help look-up tables pre-computing. propose solutions. ﬁrst straightforward solution based framework handles -way tensor. strategy commonly used e.g. taub-tabib different kernel functions different order dependency features. second approximation method aims single tensor handle multiple low-rank tensors suppose divide feature subsets correspond features lexical part lexical parts lexical parts respectively. handle types features modify training objective follows word clusters alternatively handle different numbers lexical parts replace lexical parts discrete word clusters. denote word cluster word bigram features have word introduced additional non-lexical properties conjunctions word clusters original non-lexical properties. allows reduce n-gram feature representation unigram representation. advantage method uses single low-rank tensor score features different numbers lexical parts. particularly helpful limited labeled data. denote method lrfr-brown since brown clusters practice. experiments parameter estimation goal learning tensor solves problem note non-convex objective compared convex objective traditional log-linear model trading better feature representations cost harder optimization problem. stochastic gradient descent natural choice learning representations large data settings problem involves rank constraints require expensive proximal operation enforce constraints iteration sgd. seek efﬁcient learning algorithm. note ﬁxed size transdimension matches upper bound rank. therefore rank constants always satisﬁed essence unconstrained optimization problem. note guarantee orthogonality fullrank learned transformation matrices. properties assumed general necessary according pp-attachment relation extraction fundamental tasks test models largest english data sets. preposition disambiguation task designed compositional semantics important application deep learning distributed representations. tasks compare state-of-the-art. relation extraction english portion relation extraction dataset following gold entity spans types train model news domain test broadcast conversation domain. highlight impact training data size evaluate relations reduced training ﬁrst relations. report precision recall compare baseline methods loglinear model rich binary feature zhou described embedding model gormley uses rich linguistic features relation extraction. feature templates evaluate ﬁnegrained relations evaluate lrfr utilize nonlexical linguistic features. pp-attachment consider prepositional phrase attachment task belinkov correct head must selected content words before formulate task ranking problem optimize score correct head list candidates varying sizes. pp-attachment suffers data sparsity bi-lexical features model methods belikov show rich features wordnet verbnet help task. combination features give large number non-lexical properties embeddings non-lexical properties lrfr useful. extract section following description belinkov preposition disambiguation consider preposition disambiguation task proposed ritter task determine spatial relationship preposition indicates based objects connected preposition. example apple refrigerator indicates support horizontal surface relation apple branch indicates support above relation. since meaning preposition depends table statistics task. pp-attachment preposition disambiguation unigram bigram features. therefore list numbers non-lexical properties types. table up-left unigram lexical features relation extraction denote target entities dependency path right uni/bi-gram feature pp-attachment feature deﬁned tuple preposition word child preposition candidate head word preposition collocation verb verbnet; root hypernym word wordnet. number candidate heads words. down-left uni/bi-gram feature preposition disambiguation since sentences different ignore words positions. include three baselines point-wise addition concatenation based handcrafted features table ritter show ﬁrst methods beat compositional models. hyperparameters tuned set. chosen values learning rate weight regularizer lrfr except third lrfr table select rank lrfr-tucker grid search following values lrfr-cp select pp-attachement task since uses ranking model. preposition disambiguation choose since number labels small. best reported numbers task. however lrfr-cp work well features lexical part. tucker-form better capturing interactions different views. limited training setting lrfr-cp best. additionally primary advantage approximation reduction number model parameters running time. report model’s running time single pass development set. lrfr-cp fastest. ﬁrst three lrfr-tucker models slightly slower work dense nonlexical property embeddings beneﬁts sparse vectors. pp-attachment table shows lrfr improves previous best standalone system hpcd large margin exactly resources. belinkov also reported results parsers parser re-rankers access additional resources unfair compare standalone systems like hpcd lrfr. nonetheless lrfr-tucker lrfr-cp still outperforms state-of-the-art parser reranker charniak-rs combination state-of-the-art parser compositional model hpcd thus even fewer resources lrfr becomes best system. shown table also tried lrfrtucker lrfr-cp postag features grand-head-modiﬁer conjunctions removed note compared lrfr beneﬁts binary features also exploit grand-head-modiﬁer structures. reduced models still work better without using additional resources. moreover results lrfr still potentially improved combining binary features. results show advantage factorization method allows utilizing pre-trained word embeddings thus beneﬁt semi-supervised learning. based original lexical features task lrfr-brown better represents unigram bigram lexical features compared usage low-rank tensors lrfr-brown fewer parameters better smaller training sets. also include control setting full rank parameter tensor inputs view lrfrbrown represented vectors without transforming hidden representations equivalent compound cluster features performs much worse lrfr-brown showing advantage using word embeddings low-rank tensors. still fair comparison since different training objectives. using rbg’s factorization training objective give fair comparison leave future work. features lexical parts faster predictions advantageous lrfrn-cp best fewer parameters estimate. ngrams variable length lrfr-tucker lrfrcp best. settings fewer training examples lrfr-brown best parameter tensor estimate. dimensionality reduction complex features standard technique address high-dimensional features including alternating structural optimization denoising autoencoders feature embeddings methods treat features atomic elements ignore inner structure features learn separate embedding feature without shared parameters. result still suffer large parameter spaces feature space huge. another line research studies inner structures lexical features e.g. turian nguyen grishman roth woodsend hermann used pre-trained word embeddings replace lexical parts features srikumar manning gormley propose splitting lexical features different parts employing tensors perform classiﬁcation. therefore seen special cases model embed certain part complex features. restriction also makes model parameters form full rank tensor resulting data sparsity high computational costs tensors large. composition models build representations structures based component word embeddings using word embeddings models achieved successes several tasks sometimes fail learn useful syntactic semantic patterns beyond strength combinations word example state-of-the-art dependency parser extracts million features; case learning -dimensional feature embeddings involves estimating approximately billion parameters. embeddings dependency relation figure tackle problem work designed model structures according speciﬁc kind linguistic patterns e.g. dependency paths recent trend enhances compositional models linguistic features. example belinkov concatenate embeddings linguistic features before feeding neural network; socher hermann blunsom enhanced recursive neural networks reﬁning transformation matrices linguistic features models similar sense learning representations based linguistic features embeddings. low-rank tensor models handle conjunction among different views features dredze proposed model compose phrase embeddings words equivalent form cpbased method certain restrictions. work applies similar idea exploiting inner structure complex features handle n-gram features different factorization general easy adapt tasks. importantly makes model beneﬁt pre-trained word embeddings shown pp-attachment results. presented lrfr feature representation model exploits inner structure complex lexical features applies low-rank tensor efﬁciently score features representation. lrfr attains state-of-the-art several tasks including relation extraction pp-attachment preposition disambiguation. make implementation available general use. yonatan belinkov regina barzilay amir globerson. exploring compositional architectures word vector representations prepositional phrase attachment. transactions association computational linguistics yoshua bengio holger schwenk jean-s´ebastien sen´ecal fr´ederic morin jean-luc gauvain. neural probabilistic language models. innovations machine learning. springer. matthew gormley mark dredze. improved relation extraction feature-rich compositional embedding models. proceedings conference empirical methods natural language processing. karl moritz hermann dipanjan jason weston kuzman ganchev. semantic frame identiﬁcation distributed word representations. proceedings annual meeting association computational linguistics yuan zhang regina barzilay tommi jaakkola. low-rank tenproceedsors scoring dependency structures. ings annual meeting association computational linguistics yuan zhang llu´ıs m`arquez alessandro moschitti regina barzilay. high-order low-rank tensors semantic role labelproceedings conference ing. north american chapter association computational linguistics human language technologies. yang furu sujian heng ming zhou houfeng wang. dependency-based neural network relation classiﬁcation. proceedings annual meeting association computational linguistics international joint conference natural language processing mingbo liang huang bowen zhou bing xiang. dependency-based convolutional neural networks sentence embedding. proceedings annual meeting association computational linguistics international joint conference natural language processing david mcclosky eugene charniak mark johnson. effective self-training parsing. proceedings main conference human language technology conference north american chapter association computational linguistics. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages yang jacob eisenstein. unsupervised multi-domain adaptation feature embeddings. proceedings conference north american chapter association computational linguistics human language technologies pages denver colorado may–june. association computational linguistics. matthew gormley mark dredze. combining word embeddings feature embeddings ﬁne-grained relation extracnorth american chapter association tion. computational linguistics thien nguyen ralph grishman. employing word representations regularization domain adaptation relation extraction. association computational linguistics samuel ritter cotie long denis paperno marco baroni matthew botvinick adele goldberg. leveraging preposition ambiguity assess representation semantic interaction cdsm. nips workshop learning semantics. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings emnlp. christopher manning. learning distributed representations structured output prediction. advances neural information processing systems. ralph grishman satoshi sekine. semi-supervised relation extraction large-scale word clustering. proceedings annual meeting association computational linguistics human language technologies. hillel taub-tabib yoav goldberg amir globerson. template kernels proceedings dependency parsing. conference north american chapter association computational linguistics human language technologies. pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning. christopher walker", "year": 2016}