{"title": "Cluster-based Kriging Approximation Algorithms for Complexity Reduction", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Kriging or Gaussian Process Regression is applied in many fields as a non-linear regression model as well as a surrogate model in the field of evolutionary computation. However, the computational and space complexity of Kriging, that is cubic and quadratic in the number of data points respectively, becomes a major bottleneck with more and more data available nowadays. In this paper, we propose a general methodology for the complexity reduction, called cluster Kriging, where the whole data set is partitioned into smaller clusters and multiple Kriging models are built on top of them. In addition, four Kriging approximation algorithms are proposed as candidate algorithms within the new framework. Each of these algorithms can be applied to much larger data sets while maintaining the advantages and power of Kriging. The proposed algorithms are explained in detail and compared empirically against a broad set of existing state-of-the-art Kriging approximation methods on a well-defined testing framework. According to the empirical study, the proposed algorithms consistently outperform the existing algorithms. Moreover, some practical suggestions are provided for using the proposed algorithms.", "text": "contributions. overview kriging approximation methods presented novel divide conquer based approach cluster kriging introduced. novel cluster kriging number clusters dimensionality input space respectively. addition regression function denoted complexity statements paper ignore since kriging generally used dimensional datasets. without loss generality special case adopt mathematical treatment gaussian process. assume input data points summarized corresponding output variables represented y)]. speciﬁcally mostly used variant kriging ordinary kriging models regression function random process covariance function kernel function performing so-called kernel trick computes inner product feature space function input space. paper covariance function chosen stationary meaning kernel function gaussian invariant translations input space. consequently variance following. practice common choice gaussian covariance function θi’s called hyper-parameters either predetermined estimated model ﬁtting inferred maximum likelihood method. omit likelihood function paper. infer output value unobserved data point joint distribution observed outputs derived conditioning input data unknown prior mean joint distribution multivariate gaussian expressed denotes column vector length contains homogeneous variance noise either determined user estimated maximum likelihood method. posterior distribution calculated marginalizing applied large data sets. major bottleneck high time complexity model ﬁtting process inverse covariance matrix needs computed posterior mean variance roughly time complexity. moreover optimizing hyper-parameters kernel function likelihood function parameters calculated resulting computational cost optimization iteration. thus large data high overhead model ﬁtting renders addition output values also grouped according clustering clustering done many ways simple sible approach random clustering. framework however introduce three cluster labels probabilities point belongs cluster calculated cluster points highest membership values assigned user deﬁned setting deﬁnes overlap. conditional density function whole data using posterior densities clusters. applying total probability respect cluster indicator variable density function written independence assumption gaussian process models still holds approximately amount overlap clusters small. thus density function approximately equals ﬁrst term inside predictive density density function mixture predictive distributions gaussian process models clusters. predict expectation conditional density function calculated", "year": 2017}