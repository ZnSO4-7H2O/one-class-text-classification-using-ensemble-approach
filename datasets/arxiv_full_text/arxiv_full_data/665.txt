{"title": "Correlation Alignment for Unsupervised Domain Adaptation", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-art performance on standard benchmark datasets. Our code is available at:~\\url{https://github.com/VisionLearningGroup/CORAL}", "text": "abstract chapter present correlation alignment simple effective method unsupervised domain adaptation. coral minimizes domain shift aligning second-order statistics source target distributions without requiring target labels. contrast subspace manifold methods aligns original feature distributions source target domains rather bases lower-dimensional subspaces. also much simpler distribution matching methods. coral performs remarkably well extensive evaluations standard benchmark datasets. ﬁrst describe solution applies linear transformation source features align target features classiﬁer training. linear classiﬁers propose equivalently apply coral classiﬁer weights leading added efﬁciency number classiﬁers small number dimensionality target examples high. resulting coral linear discriminant analysis outperforms large margin standard domain adaptation benchmarks. finally extend coral learn nonlinear transformation aligns correlations layer activations deep neural networks resulting deep coral approach works seamlessly dnns achieves state-of-theart performance standard benchmark datasets. code available https //github.com/visionlearninggroup/coral machine learning different human learning. humans able learn labeled examples apply learned knowledge examples novel conditions. contrast traditional machine learning algorithms assume training test data independent identically distributed supervised machine learning methods perform well given extensive labeled data distribution test distribution. however assumption rarely holds practice data likely change time space. compensate degradation performance domain shift domain adaptation tries transfer knowledge source domain target domain. approach line existing unsupervised domain adaptation approaches ﬁrst transform source data close target data possible. classiﬁer trained transformed source domain applied target data. chapter mainly focus unsupervised scenario believe leveraging unlabeled target data success domain adaptation. real world applications unlabeled target data often much abundant easier obtain. side labeled examples limited require human annotation. question utilize unlabeled target data important practical visual domain adaptation. example would convenient applicable pedestrian detector automatically adapt changing visual appearance pedestrians rather human annotator label every frame different visual appearance. goal propose simple effective approach domain adaptation researchers little knowledge domain adaptation machine learning could easily integrate application. chapter describe frustratingly easy unsupervised domain adaptation method called correlation alignment coral short. coral aligns input feature distributions source target domains minimizing difference second-order statistics. intuition want capture structure domain using feature correlations. example imagine target domain western movies people wear hats; head feature positively correlated feature. goal transfer correlations source domain transforming source features. section describe linear solution coral distributions aligned re-coloring whitened source features covariance target data distribution. solution simple efﬁcient computations needs computing covariance statistics domain applying whitening re-coloring linear transformation source features. then supervised learning proceeds usual–training classiﬁer transformed source features. linear classiﬁers equivalently apply coral transformation classiﬁer weights leading better efﬁciency number classiﬁers small number dimensionality target examples high. present resulting coral–linear discriminant analysis section show outperforms standard linear discriminant analysis large margin cross domain applications. also extend coral work seamlessly deep neural networks designing layer consists differentiable coral loss detailed section contrary linear coral deep coral learns nonlinear transformation also provides endto-end adaptation. section describes extensive quantitative experiments several benchmarks section concludes chapter. section present correlation alignment unsupervised domain adaptation derive linear solution. ﬁrst describe formulation derivation followed main linear coral algorithm relationship existing approaches. section section constrain transformation linear. section extends coral learn nonlinear transformation aligns correlations layer activations deep neural networks describe method taking multi-class classiﬁcation problem running example. suppose given source-domain training examples {xi} labels {yi} ...l} target data {ui} d-dimensional feature representations input suppose csct feature vector means covariance matrices. assuming features normalized zero mean unit variance normalization step minimize distance second-order statistics source target features apply linear transformation original source features frobenius norm matrix distance metric denotes rank rank analytical solution obtained choosing however data typically lower dimensional manifold covariance matrices likely rank derive solution general case using following lemma. lemma real matrix rank real matrix rank σyvy largest singular values corresponding left right singular vectors optimal solution problem respectively. then theorem moore-penrose pseudoinverse denote rank respectively. then optimal solution problem equation min. proof. since linear transformation acsa increase rank rcs. since symmetric matrices conducting thus gives usσsus respectively. ﬁrst optimal value considering following cases case optimal solution thus σtut fig. illustration correlation alignment domain adaptation original source target domains different distribution covariances despite features being normalized zero mean unit standard deviation. presents problem transferring classiﬁers trained source target. domains source decorrelation i.e. removing feature correlations source domain. target re-correlation adding correlation target domain source features. step source target distributions well aligned classiﬁer trained adjusted source domain expected work well target domain. might instead attempt align distributions whitening source target. however fail since source target data likely different subspaces domain shift. figures illustrate linear coral approach. figure shows example original source target data distributions. think transformation whitens source data intuitively follows ﬁrst part re-colors target covariance. second part illustrated figure figure respectively. practice sake efﬁciency stability avoid expensive steps perform traditional data whitening coloring. traditional whitening adds small regularization parameter diagonal elements covariance matrix explicitly make full rank multiplies original features inverse square root advantageous because faster stable original covariance matrices might stable might slow converge; illustrated figure performance similar analytical solution equation stable respect experiments provided chapter ﬁnal algorithm written four lines matlab code illustrated algorithm fig. sensitivity coral covariance regularization parameter plots show classiﬁcation accuracy target data domain shifts regularization analytical solution equation please refer section details experiment. might instead attempt align distributions whitening source target. shown figure fail source target data likely different subspaces domain shift. alternative approach would whitening target re-coloring source covariance. however demonstrated experiments transforming data source target space gives better performance. might fact transforming source target space classiﬁer trained using label information source unlabelled structure target. coral transforms source features target space classiﬁer parametrized trained adjusted source features directly applied target features. linear classiﬁer apply equivalent transformation parameter vector instead features results added efﬁciency number classiﬁers small number dimensionality target examples high. linear extension straightforward. section apply idea coral another commonly used linear classiﬁer– linear discriminant analysis special sense weights also covariance data. also extremely efﬁcient training large number classiﬁers long known input feature normalization improves many machine learning methods e.g. however coral simply perform feature normalization rather aligns different distributions. batch normalization tries compensate internal covariate shift normalizing mini-batch zero-mean unit-variance. however illustrated figure normalization might enough. even used full whitening batch normalization compensate external covariate shift layer activations decorrelated source point target point. what’s more mentioned section whitening domains successful strategy. recent state-of-the-art unsupervised approaches project source target distributions lower-dimensional manifold transformation brings subspaces closer together coral avoids subspace projection costly requires selecting hyper-parameter controls dimensionality subspace note subspace-mapping approaches align eigenvectors source target covariance matrices. contrary coral aligns covariance matrices re-constructed using eigenvectors eigenvalues. even though eigenvectors aligned well distributions still differ difference eigenvalues between corresponding eigenvectors source target data. coral general much simpler method takes account eigenvectors eigenvalues covariance matrix without burden subspace dimensionality selection. maximum mean discrepancy based methods domain adaptation interpreted moment matching express arbitrary statistics data. minimizing polynomial kernel similar coral objective however previous work used kernel domain adaptation proposed closed form solution best knowledge. difference based approaches usually apply transformation source target domain. demonstrated asymmetric transformations ﬂexible often yield better performance domain adaptation tasks. intuitively symmetric transformations space ignores differences source target domain asymmetric transformations bridge domains. section introduce coral applied aligning multiple linear classiﬁers. particular take example illustration considering commonly used effective linear classiﬁer. combining coral gives efﬁcient adpative learning approach coral-lda. task object detection running example explain coral-lda. begin describing decorrelation-based approach detection proposed given image follows sliding-window paradigm extracting d-dimensional feature vector window across locations multiple scales. scores windows using scoring function recent years linear scoring function usually histogram gradients features emerged predominant object detection paradigm. observed hariharan training svms expensive especially usually involves costly rounds hard negative mining. furthermore training must repeated object category makes scale poorly number categories. hariharan proposed much efﬁcient alternative learning linear discriminant analysis well-known linear classiﬁer models training examples labels generated prior class labels class-conditional densities normal distributions innovation re-use background mean categories reducing task learning category model computing average positive feature accomplished calculating largest possible window subsampling estimate smaller window sizes. also shown sparse local structure correlation falling sharply beyond nearby image locations. like classiﬁers learns suppress non-discriminative structures enhance contours object. however learning global covariance statistics natural images using inverse covariance matrix remove non-discriminative correlations negative mean refig. applying linear classiﬁer learned source data equivalent −/x. however target points applying classiﬁer hurting performance. method uses target-speciﬁc still correlated covariance obtain properly decorrelated observe estimating global statistics re-using tasks work training testing domain case source training data likely different statistics target data. figure illustrates effect centering decorrelating positive mean using global statistics wrong domain. effect clear important discriminative information removed irrelevant structures not. based observation propose adaptive decorrelation approach detection. assume given labeled training data {xy} source domain unlabeled examples target domain evaluating scoring function source domain equivalent ﬁrst de-correlating computing positive negative class means training features projecting decorrelated feature onto decorrelated difference means illustrated figure however figure assumption input properly decorrelated hold input comes target domain different co−/u variance structure. figure illustrates case showing isotropic covariance. therefore cannot used directly. able compute covariance target domain unlabeled target points positive class mean. therefore would like re-use decorrelated mean difference adapt covariance target domain. rest chapter make assumption difference positive negative means source target. hold practice discuss section practice either source target component transformation also work even statistics similar domains. however section dissimilar domain statistics signiﬁcantly hurt performance. furthermore either source target images positive category available cannot used properly compute background statistics domain still used. coral-lda works purely unsupervised way. here extend semisupervised adaptation labeled examples available target domain. following simple adaptation method used whereby template learned source positives combined template learned target positives using weighted combination. difference approach target template uses target-speciﬁc statistics. author uses background statistics estimated natural images pascal dataset. based analysis above even though background statistics estimated large amount real image data work domains. section results conﬁrm claim. section extend coral work seamlessly deep neural networks designing differentiable coral loss. deep coral enables end-to-end adaptation also learns powerful nonlinear transformation. easily fig. sample deep coral architecture based classiﬁer layer. generalization simplicity apply coral loss layer alexnet integrating layers network architectures also possible. integrated different layers network architectures. figure shows sample deep coral architecture using proposed correlation alignment layer deep domain adaptation. refer deep coral deep network incorporating coral loss domain adaptation. ﬁrst describe coral loss domains single feature layer. suppose numbers source target data respectively. d-dimensional deep layer activations input trying learn. suppose indicates j-th dimension i-th source data example denote feature covariance matrices. loss used adapt existing neural network return multi-class classiﬁcation problem. suppose start network ﬁnal classiﬁcation layer convnet shown figure mentioned before ﬁnal deep features need discriminative enough train strong classiﬁer invariant difference source target domains. minimizing classiﬁcation loss likely lead overﬁtting source domain causing reduced performance target domain. hand minimizing coral loss alone might lead degenerated features. example network could project source target data single point making coral loss trivially zero. however strong classiﬁer constructed features. joint training classiﬁcation loss coral loss likely learn features work well target domain denotes number coral loss layers deep network weight trades adaptation classiﬁcation accuracy source domain. show below losses play counterparts reach equilibrium training ﬁnal features discriminative generalize well target domain. evaluate coral deep coral object recognition using standard benchmarks protocols. experiments assume target domain unlabeled. coral follow standard procedure linear base classiﬁer. model selection approach used parameter cross-validation source domain. coral-lda efﬁciency main concern evaluate time constrained task– object detection. follow protocol features. fair comparison accuracies reported authors exactly setting conduct experiments using source code provided authors. experiments domain adaptation used improve accuracy object classiﬁer novel image domains. standard ofﬁce extended ofﬁce-caltech datasets used benchmarks chapter. ofﬁce-caltech contains object categories ofﬁce environment image domains webcam dslr amazon caltech. standard ofﬁce dataset contains object categories domains webcam dslr amazon. follow standard protocol conduct experiments ofﬁce-caltech dataset shallow features surf features encoded -bin bag-of-words histograms normalized zero mean unit standard deviation dimension. since four domains experiment settings namely mazon test altech) mazon test slr) follow standard protocol conduct experiments randomized trials domain shift average accuracy trials. trial standard setting randomly sample number labelled images source domain training unlabelled data target domain test set. table compare method recent published methods svma well adaptation baseline manifold based methods project source target distributions lower-dimensional manifold. integrates inﬁnite number subspaces along subspace manifold using kernel trick. aligns source target subspaces computing linear minimizes frobenius norm difference. performs domain adaptation parametric kernel using feature extraction methods projecting data onto learned transfer components. introduces smoothness assumption enforce target classiﬁer share similar decision values source classiﬁers. even though methods complicated require tuning hyperparameters method achieves best average performance across domain shifts. method also improves adaptation baseline cases increasing accuracy signiﬁcantly visual domain adaptation deep features follow standard protocol labeled source data target data without labels standard ofﬁce dataset since domains conduct experiments shifts taking domain source another target. experiment apply coral loss last classiﬁcation layer general case–most deep classiﬁer architectures contain fully connected layer classiﬁcation. applying coral loss layers network architectures also possible. dimension last fully connected layer number categories initialized learning rate times layers training scratch. initialized layers parameters pre-trained imagenet kept original layer-wise parameter settings. training phase batch size base learning rate weight decay momentum weight coral loss training classiﬁcation loss coral loss roughly same. seems reasonable choice want feature representation discriminative also minimizes distance source target domains. used caffe bvlc reference caffenet experiments. coral manifold based methods project source target distributions lowerdimensional manifold end-to-end deep methods. adds domain confusion loss alexnet ﬁne-tunes source target domain. similar utilizes multi-kernel selection method better mean embedding matching adapts multiple layers. direct comparison paper uses hidden layer coral feature ﬁne-tuned source domain achieves better performance generic pre-trained features train linear fair comparison accuracies reported authors exactly setting conduct experiments using source code provided authors. table deep coral achieves better average performance coral baseline methods. shifts fig. detailed analysis shift training v.s. coral loss. training test accuracies training v.s. coral loss. adding coral loss helps achieve much better performance target domain maintaining strong classiﬁcation accuracy source domain. classiﬁcation loss coral loss training coral loss. last fully connected layer randomly initialized coral loss small classiﬁcation loss large beginning. training hundred iterations losses same. coral distance training coral loss distance getting much larger times larger compared training coral loss). better understanding deep coral generate three plots domain shift a→w. figure show training testing accuracies training v.s. without coral loss. clearly adding coral loss helps achieve much better performance target domain maintaining strong classiﬁcation accuracy source domain. figure visualize classiﬁcation loss coral loss training coral loss. last fully connected layer randomly initialized beginning coral loss small classiﬁcation loss large. training hundred iterations losses reach equilibrium. figure show coral distance domains training coral loss distance getting much larger times larger compared training coral loss). comparing figure figure even though coral loss always decreasing training weight distance source target domains becomes much larger. reasonable ﬁne-tuning without domain adaptation likely overﬁt features source domain. coral loss constrains distance source target domain ﬁne-tuning process helps maintain equilibrium ﬁnal features work well target domain. following protocol conduct object detection experiment ofﬁce dataset features. setting performing detection webcam domain target domain evaluating image test categories source domains remaining real-image domains ofﬁce amazon dslr domains contain virtual images only virtual virtual-gray generated models. inclusion virtual domains reduce human effort annotation facilitate future research examples virtual virtualgray shown figure please refer detailed explanation data generation process. also compare corresponding imagenet synsets source. thus four potential source domains target domain. number positive training images category domain shown table figure shows overview evaluation. fig. sets virtual images used section virtual background texturemap random real imagenet image; virtual-gray uniform gray texturemap white background. please refer detailed explanation data generation process. first explore effect mismatched precomputed image statistics detection performance. source domain train coral-lda detectors using positive mean source pair covariance negative mean domains. virtual ofﬁce domains used sources test domain always webcam. statistics four domains calculated using training data following approach pre-computed statistics real images pascal proposed also evaluated. detection performance measured mean average precision shown table also calculate normalized euclidean distance pairs domains c)/c show average distance source target parentheses table results trend larger domain difference leads poorer performance. note larger difference target domain also leads lower performance conﬁrming hypothesis source target statistics matter. variation could also stem assumption difference means quite holding true. finally pascal statistics perform worst. thus practice statistics either source domain target domain domains close could used. however unrelated statistics work even though might estimated large amount data table coral-lda trained positive examples row’s source domain background statistics column’s domain. average distance background statistics true source target statistics shown parentheses. next report results unsupervised semi-supervised adaptation technique. setting three positive nine negative labeled images category used semi-supervised adaptation. target covariance equation estimated unlabeled training examples. also followed approach learn linear combination unsupervised supervised model cross-validation. results presented table please note target-only compared also conﬁrms conclusion statistics come related domain. clear unsupervised semi-supervised adaptation techniques outperform method furthermore virtual-gray data outperforms virtual dslr best close target domain finally compare method trained virtual-gray results adapting imagenet reported figure unsupervised models learned real imagenet images category background statistics estimated pascal images virtual images category background statistics learned images. what’s more virtual images used uniform gray texturemap white background. clearly demonstrates importance domain-speciﬁc decorrelation shows need collect large amount real images train good classiﬁer. fig. comparison unsupervised semi-supervised adaptation virtual detectors using method results training imagenet supervised adaptation imagenet reported semi-supervised adapted detectors achieve comparable performance despite using real source training data using positive images adaptation even outperform imagenet signiﬁcantly several categories table comparison source-only semi-supervised adapted model unsupervised-adapted semi-supervised adapted models. target domain webcam. mean across categories reported webcam test data using different source domains training. bottom sample detections dslr-unsupadapt-ours detectors. chapter described simple effective efﬁcient method unsupervised domain adaptation called correlation alignment coral minimizes domain shift aligning second-order statistics source target distributions without requiring target labels. also developed novel domain adaptation algorithms applying idea coral three different scenarios. ﬁrst scenario applied linear transformation minimizes coral objective source features prior training classiﬁer. case linear classiﬁers equivalently applied linear coral transform classiﬁer weights signiﬁcantly improving efﬁciency classiﬁcation accuracy standard several domain adaptation benchmarks. extended coral learn nonlinear transformation aligns correlations layer activations deep neural networks. resulting deep coral approach works seamlessly deep networks integrated arbitrary network architecture enable end-to-end unsupervised adaptation. limitation coral captures second-order statistics preserve higher-order structure data. however demonstrated chapter works fairly well practice also potentially combined domain-alignment loss functions.", "year": 2016}