{"title": "Monotone Retargeting for Unsupervised Rank Aggregation with Object  Features", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering between any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods are often ill-defined, and difficult to solve. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy.", "text": "learning true ordering objects aggregating expert opinion rank order lists important ubiquitous problem many applications ranging social choice theory natural language processing search aggregation. study problem unsupervised rank aggregation ground truth ordering information available neither true preference ordering objects quality individual rank lists. aggregating often inconsistent poor quality rank lists unsupervised manner highly challenging problem standard consensus-based methods often ill-deﬁned diﬃcult solve. manuscript propose novel framework bypass issues using object attributes augment standard rank aggregation framework. design algorithms learn joint models rank lists object features obtain aggregated rank ordering accurate robust also helps weed rank lists dubious validity. validate techniques synthetic datasets algorithm able estimate true rank ordering even rank lists corrupted. experiments three real datasets ohsumed show using object features result signiﬁcant improvement performance existing rank aggregation methods object information. furthermore least rank lists high quality methods able eﬀectively exploit high expertise output aggregated rank ordering great accuracy. learning preference orderings among objects common problem many modern applications including search document retrieval collaborative ﬁltering recommendation systems. rank aggregation version problem appears areas ranging voting social choice theory meta search search aggregation ensemble methods combining classiﬁers focus problem unsupervised rank aggregation manuscript. standard setup consists items objects ranked ranking lists experts containing rank scores relevance scores subset items objective combine lists diﬀerent experts obtain consensus rank order items. note setup diﬀerent supervised rank aggregation semi-supervised rank aggregation where addition rank lists also ground truth rank orderings least subset objects ranked contrast unsupervised setup given access rank lists without information quality list ground truth rank order objects. diﬀerence letor unsupervised rank aggregation two-fold. first unlike latter letor supervised access training data. second letor models rank scores functions object features existing rank aggregation methods completely agnostic properties objects ranked even information available example case meta-search search aggregation ensemble methods learning agents. result rank aggregation signiﬁcantly challenging tackle compared letor supervised/semi-supervised methods general. without access information true preference ordering among objects standard rank aggregation problem becomes hard combinatorial problem often rife paradoxes rule based approaches like borda winner condorcet winner etc. commonly used many rules incompatible alternative approaches learn consensus orderings minimising disagreements distance-like metrics among rank scores preference orderings speciﬁed ranked lists experts approach runs problem many commonly used distance metrics corresponding optimisation problem np-hard indeed without additional information cannot form incontrovertible deﬁnition consensus disagreement rank scores even decide metric used consensus rank ordering. moreover methods excessively dependent assumption experts reliable competent. spurious rank order lists even presence noise ranked lists signiﬁcantly deteriorate performance standard methods used rank aggregation. absence supervision reduces rank aggregation problem choosing competing heuristics. obtaining even partial ground truth expensive time consuming often requires involvement dedicated human experts annotators. manuscript introduces novel rank aggregation framework mitigates handicaps unsupervised rank aggregation using information object attributes augment standard approaches recovery true rank order. contrast existing methods blind sense completely agnostic properties objects themselves. best knowledge ﬁrst object information rank aggregation problem. introduce novel framework combines information rank scores object features learn consensus ordering items. formulate rank aggregation problem using isotonically coupled models expert lists object features model rank scores describe solution algorithm estimate true ordering involves alternate interdependent iterations letor step rank aggregation step separately simple convex optimisation problem monotonicity constraints. evaluate framework experiments synthetic data where unlike existing methods algorithm able reconstruct true rank ordering exactly given corrupted rank lists. also demonstrate methods three real datasets algorithm signiﬁcantly outperforms existing rank aggregation techniques object information. note even though framework uses letor inspired methods formulation still unsupervised learning algorithm since assume access full partial ground truth rank order pairwise preferences even information quality rank lists. light diﬃculties inherent rank aggregation problem commonly used rank aggregation applications apply heuristic based techniques ranging classical methods vote counting borda count traditional vote counting procedure uses positional information opposed rank scores. procedure orders candidates number candidates ranked lower them averaged voters combination methods work explicit relevance scores provided experts include various linear combinations like combsum combmnz combanz well non-linear methods like combmin combmax. seen three linear combination methods combmnz best performance. items retrieved expert lists three methods give ﬁnal ranking. recent work rank aggregation introduced mcmc based methods. basic idea items ranked states markov chain deﬁne transition probability switching state another based relative scores preference values corresponding items across rank lists. four diﬀerent markov chain constructions described diﬀerent heuristics construct transition probability matrix. ﬁnal ranking items deﬁned stationary distribution across items deﬁned states markov chain. suppose true rank ordering items given vector search aggregation example could true relevance scores annotated human subject matter expert. slight abuse notation shall overload denote rank score well rank ordering deﬁned rank score vector. unsupervised rank aggregation true ordering known even among partial subset objects. however assumed least rank lists generated using standard rank aggregation methods recover learning model maps vector isotonic manuscript overcome limitations unsupervised rank aggregation described section augmenting setup object features often available applications like meta-search etc. suppose items v··· item represented d-dimensional feature vector collect item feature vectors form rows study rank aggregation context applications like meta-search assumed exist motivate approach consider standard supervised letor framework access well popular letor methods proceed learning model maps rank score vector isotonic similarly many standard methods supervised rank aggregation proceed learning model maps vector agrees available information contrast completely unknown unsupervised rank aggregation. idea method explicit access implicitly tied together implicit association exploited recover jointly modelling mappings learned respectively. knowledge object attributes commonly available many rank aggregation setups none existing rank aggregation methods exploit suﬃciently. ﬁrst work explore generalised linear models glms modeling framework since large class models subsume many standard modeling frameworks widely used many applications across wide variety domains. particular glms found extensive usage letor well most rank aggregation methods glm’s model target variable linear function feature variables monotonically transformed monotonic link function. assume true rank ordering modeled monotonically transformed linear combination rank order lists rn×p experts even without monotonic transformation setup subsumes standard score fusion algorithms also borda-count weighted borda-fuse rank scores deﬁned number items ranked particular item. setup also eﬀectively handle case ranked lists dubious validity– rank lists simply assigned zero weight discarded model. objective estimate joint modelling estimate minimising appropriate distance-like cost function. clearly standard cost functions like square loss make sense every context therefore present techniques much general class cost functions called bregman divergences distance-like functions intimately associated glm’s include many standard commonly used loss functions like square loss kl-divergence generalised i-divergence etc. bregman divergences matching loss functions associated learning parameters distance-like functions called bregman divergences generalisations square loss. bregman separately convex well many standard distance-like functions like square loss kullbackleibler divergence generalised i-divergence written members family corresponding succinctness shall henceforth denote cost function following discussion using bregman divergences loss function estimating tempting target variable feature write joint optimisation framework follows alternatively order arguments reversed. however formulations deﬁcient modelling capacity since force coupling domain domain particular often real valued often integer valued binary valued even categorical partial ordering make sense both. moreover take account general assumption true ordering need isotonic better alternative scheme bypasses issues involves decoupling cost function partsone involving rank scores experts involving object features. since consider rank score vectors invoke ordering among items equivalent shall learning across monotonic transformations incorporate invariance across isotonic vectors formulation. similarly suppose rank score vector obtained linear function object features. correspondingly part cost function becomes dφzxβ) appropriate along choice divergence functions depends domain modeling assumptions used discussion learning appropriate detailed shall later optimisation framework involves steps invariant value chosen simplicity. equation optimisation problem function separately convex arguments non-convex set. joint optimisation variables simultaneously diﬃcult therefore divide variables disjoint sets perform alternating minimisation variables separately. nevertheless monotonic invariance isotonicity constraint need perform optimisation step monotonic transformations vectors diﬃcult problem general. however turns setup becomes relatively easy handle using technique called monotone retargeting used supervised learning rank problems. monotone retargeting letor technique uses object attributes known rank score vector learns model ﬁnds best mapping possible monotonic transformations rank score vector speciﬁc optimisation problem handles case bregman divergences following impose strict ordering constraints avoid degenerate solutions variation called margin equipped monotone retargeting memr introduced uses margin constraints together ordering constraints deﬁned detailed description memr given appendices respectively. steps used exact optimisation algorithm summarised algorithm versatile framework found usage outside traditional supervised ranking application designed for. particular versions framework used collaborative ﬁltering recommendation systems learning generalised linear models aggregated data manuscript apply framework problem rank aggregation. consider variables divided sets variables initialisation algorithm proceeds alternating steps. first keeping ﬁxed optimisation problem becomes initialisation however happen alternate steps deﬁne isotonicity constraint partially ordered rather totally ordered. enables algorithm freely move permutations successive steps. detailed discussion phenomenon provided section individually equations standalone instances inherit desirable properties algorithm detailed main steps algorithm nice interpretation– equation equivalent analogous letor step equation equivalent analogous rank aggregation step. steps used overall optimisation summarised algorithm convergence eﬃciency algorithm uses alternating minimisation non-negative cost function therefore algorithm always converges stationary point. iteration involves procedure detailed algorithm implemented eﬃciently practice particular step implemented step glm-solver step extensively studied fast oﬀ-the-shelf implementations readily available. avoid certain kinds degeneracies memr uses formulation enforces \u0001-margin between relevance scores items adjacent learned rank score vector formulate margin-equipped version methods well however memr scheme cannot used directly. discuss section salient desiderata formulation able move rank orderings every step algorithm achieved using partial orderings generated optimisation algorithm iteration. imposing strict ordering would make longer possible. adding convex regularisation term equation many useful eﬀects retaining convexity properties optimisation problem. particular suppose know rank order lists spurious generated sources dubious expertise. case sparsity promoting methods like lasso regularisation help weed spurious rank lists enforcing sparsity. similar argument also made adding sparsity boosting regularisers weed spurious features. desiderata algorithm intermediate steps free move permutations ﬁnal aggregated output rank order inﬂuenced heavily initialisation. accomplished algorithm following manner. update step algorithm involves pool-adjacent-violator smoothing operation property ordering constraint match ordering right hand side ﬁnal output would partially ordered suppose time start total ordering step algorithm consistent partially ordered. partially ordered used input estimating step algorithm ﬁnal output total ordering consistent partial ordering speciﬁed necessarily consistent total ordering therefore time time algorithm ends moving ordering ordering subsequently shall show experiments synthetic data indeed case. starting initialisation reﬂect true ordering obtained algorithm allowed move permutations till converge stationary point corresponding exact ordering. methods outlined manuscript used glm’s bregman divergences formulation much general. speciﬁcally techniques like used illustrate many possible ways jointly modeling rank scores object features methods supervised letor rank aggregation literature easily incorporated framework changing cost function optimisation algorithm. similarly methods extended many contexts including partial orderings either learned step iteratively using isotonic regression algorithms speciﬁed using directed acyclic graphs using additional constraints speciﬁed using implicit feedback finally adding supervision framework straightforward simply strict wide-margin constraints margin equipped formulation described section evaluate performance algorithm synthetic real datasets. metrics used evaluation kendall’s coeﬃcient spearman’s rank correlation coeﬃcient well popular ndcg metric list-wise ranking. three metrics higher value indicates better recovery rank order value indicates exact order recovery. baselines classical borda count. among linear combination methods following combmnz also show comparisons non-linear aggregation methods like combmin combmax further also compare four markov chain based methods presented ﬁrst evaluate synthetic data assumption holds rank aggregation framework evaluated items query. matrix feature vectors generated using standard multivariate normal distribution normally distributed used compute true rank scores according generalised linear models corresponding gaussian poisson distributions respectively appropriate function rank lists consist vectors generated randomly corrupting true rank score diﬀerent perturbations including translation additive multiplicative noise. multiple spurious expert rank figure synthetic data kendall-tau spearman’s ndcgk iterations algorithm gaussian respectively] poisson models respectively] results show method exactly recover true rank order even corrupted rank lists within iterations. contrast none baseline rank aggregation methods recover true ordering. figure real datasets ndcgk versus averaged across queries ohsumed datasets respectively] ndcgk averaged across queries datasets ohsumed augmented rank lists respectively] results show methods outperform standard rank aggregation techniques general. rank lists good quality experts improvement performance substantial method opposed baselines figures respectively show value kendall’s spearman’s estimated true iteration algorithm setup uses gaussian model. figures show setup generated using poisson model. plots show models algorithm able exactly recover original rank ordering within small number iterations. contrast none standard rank aggregation methods able recover true ordering. figures show ndcgk ﬁnal ordering output method compared baseline methods gaussian setup poisson setup respectively. opposed rank orderings output method always perfectly extracts items none baselines able correctly identify relevant items value evaluate techniques applied challenging case real datasets assumption always hold. three complex real world datasets widely used ranking applicationsmq microsoft’s letor repository ohsumed letor repository datasets letor repository query sets million query track trec letor repository contains query associated document object features ranking well rank lists. rank lists object features matrix respectively. pagerank relevance scores computed diﬀerent methods also added rank lists ohsumed dataset subset medline database medical publications standard application involves extraction relevant documents given medical queries. rank list matrix constructed columns dataset contain relevance scores computed using score diﬀerent methods based language models remaining columns used object features. datasets compare ground truth available query associated item relevance score goes additional details datasets including feature lists available respectively. note experiment object attributes rank lists disjoint sets columns. compare performance diﬀerent methods using ndcgk metric applied relevance score averaged across queries results shown ﬁgures ohsumed respectively. plots show three datasets even though assumption necessarily hold method nevertheless outperforms standard rank aggregation methods take account object features. interesting phenomenon observed real datasets least rank lists high quality sense learned supervision form true relevance scores performance algorithm improves substantially. eﬀect seen baselines show marginal improvement compared performance rank lists performed similar experiments above except augmented rank lists experts relevance scores output ranking function learned training data using methods ndcgk versus plots augmented rank lists shown ohsumed dataset dataset dataset side side comparison original plots show rank lists contain least list high quality algorithm returns aggregated ordering ndcg score shows marked improvement standard methods fail exploit augmented rank lists substantial degree. note information rank lists high quality available algorithms experiments. experimental results suggest rank lists generated sources whose expertise levels wide spectrum framework nevertheless able information rank lists higher credibility recover ordering matches true ranking greater degree compared rank aggregation methods depend solely rank lists blind object features. manuscript introduced novel rank aggregation scheme augments expert rank lists information object features bypass various issues plague standard rank aggregation setup process obtain accurate robust aggregated rank scores. experiments synthetic data real datasets indicate using object features result signiﬁcant improvement performance rank lists contain orderings generated sources genuine expertise. future work would cover theoretical analyses scheme including statistical guarantees extensions general models ranking rank aggregation.", "year": 2016}