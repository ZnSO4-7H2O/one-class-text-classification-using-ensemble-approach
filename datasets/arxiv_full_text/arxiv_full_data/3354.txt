{"title": "Unsupervised Learning of 3D Structure from Images", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "A key goal of computer vision is to recover the underlying 3D structure from 2D observations of the world. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 3D and 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained end-to-end from 2D images. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.", "text": "goal computer vision recover underlying structure observations world. paper learn strong deep generative models structures recover structures images probabilistic inference. demonstrate high-quality samples report log-likelihoods several datasets including shapenet establish ﬁrst benchmarks literature. also show models inference networks trained end-to-end images. demonstrates ﬁrst time feasibility learning infer representations world purely unsupervised manner. live three-dimensional world observations typically form twodimensional projections capture eyes cameras. goal computer vision recovering underlying structure gives rise observations. projection scene complex function attributes positions camera lights objects make scene. endowed understanding agents abstract away complexity form stable disentangled representations e.g. recognizing chair chair whether seen side different lighting conditions partial occlusion. moreover representations would allow agents determine downstream properties elements easily less training e.g. enabling intuitive physical reasoning stability chair planning path approach ﬁguring best pick models representations also applications scene completion denoising compression generative virtual reality. many attempts performing kind reasoning dating back earliest years ﬁeld. despite this progress slow several reasons first task inherently illposed. objects always appear self-occlusion inﬁnite number structures could give rise particular observation. natural address problem learning statistical models recognize structures likely not. second even endowed statistical model inference intractable. includes sub-tasks mapping image pixels representations detecting establishing correspondences different images structures handling multi-modality representations space. third unclear structures best represented e.g. dense volumes voxels collection vertices edges faces deﬁne polyhedral mesh kind representation. finally ground-truth data difﬁcult expensive collect therefore datasets relatively limited size scope. figure motivation representation image ambiguous multi-modal. achieve reasoning learning generative model structures recover structure images probabilistic inference. paper deep neural networks learn generative models structures recover structure images probabilistic inference. learning models structures directly pixels long-standing research problem number approaches different levels underlying assumptions feature engineering proposed. traditional approaches vision inverse graphics analysis-by-synthesis rely heavily engineered visual features inference object properties shape pose substantially simpliﬁed. recent work addresses limitations learning parts encoding-decoding pipeline depicted ﬁgure separate stages. discuss related work unlike existing approaches approach ﬁrst learning representations unsupervised end-to-end manner directly images. contributions follows. design strong generative model structures deﬁned space volumes meshes using ideas state-of-the-art generative models images show models produce high-quality samples effectively capture uncertainty amenable probabilistic inference allowing applications generation simulation. report log-likelihoods dataset shape primitives version mnist shapenet best knowledge constitutes ﬁrst quantitative benchmark density modeling. show complex inference tasks e.g. inferring plausible structures given image achieved using conditional training models. demonstrate models recover representations forward pass neural network accurately capture multi-modality posterior. explore volumetric mesh-based representations structure. latter achieved ﬂexible inclusion off-the-shelf renders opengl allows build knowledge rendering process e.g. light bounces surfaces interacts material’s attributes. show aforementioned models inference networks trained end-to-end directly images without ground-truth labels. demonstrates ﬁrst time feasibility learning infer representations world purely unsupervised manner. section develop framework learning models structure volumetric data directly images. consider conditional latent variable models structured ﬁgure given observed volume image context wish infer corresponding representation achieved modelling latent manifold object shapes poses low-dimensional codes context quantity always observed traintest-time conditions computations inference generation experiments context either nothing object class label views scene different cameras. order learn interpretable representations models made employ generative process consists ﬁrst generating representation projecting domain observed data instance model ﬁrst generate volume mesh representation scene object render using convolutional network opengl renderer form image. generative models latent variables describe probability densities datapoints immodels built using multiple layers latent variables layer speciﬁes conditional distribution parameterized deep neural network. examples models include marginal likelihood intractable must resort approximations. variational approximations bound marginal likelihood klp] true posterior distribution approximated parametric family posteriors parameters learning involves joint optimization variational parameters model parameters framework think generative model decoder latent variables inference network encoder figure proposed framework left given observed volume image contextual information wish infer corresponding representation achieved modeling latent manifold object shapes low-dimensional codes experiments consider unconditional models well models context class views scene. right train contextconditional inference network object model ground-truth volumes available trained directly. ground-truth images available renderer required measure distance inferred representation ground-truth image. capture complex distribution structures apply recent work sequential generative models extending operate different representations. family models generates observed data course computational steps. precisely models operate sequentially transforming independently generated gaussian latent variables reﬁnements hidden representation refer ‘canvas’. ﬁnal conﬁguration canvas transformed target data ﬁnal smooth transformation. framework refer hidden representation representation’ since special form amenable transformations. generative process described following equations step generates independent k-dimensional variables fully connected long short-term memory network transition function fstate. context encoder fread task dependent; provide details section using volumetric latent representation representation update function fwrite equation parameterized volumetric spatial transformer precisely fwrite mlps take state appropriate sizes. details provided appendix using mesh representation fwrite fully-connected mlp. function proj projection operator model’s latent representation training data’s domain plays role ‘renderer’. conditional density either diagonal gaussian product bernoulli distributions denote parameters generative model θp}. details inference model variational objective used optimization provided appendix figure projection operators drop-in modules relate latent representation training data. choice representation type available training data determine operator used. left volume-to-volume projection middle volumeto-image neural projection right mesh-to-image opengl projection discuss projection operators detail. drop-in modules relate latent representation training data. choice representation type available training data determine operator used. projection cases training data already form volumes directly deﬁne likelihood density projection operator simply identity function neural projection practical applications access images captured camera. moreover camera pose unknown partially known. cases construct learn -dimensional volume observed images combining convolutions. multiple views different positions simultaneously observed projection operator simply cloned many times target views. parameters projection operator trained jointly rest model. operator depicted ﬁgure details appendix opengl projection working mesh representation projection operator equation complex mesh description provided generative model rendered images experiments off-the-shelf opengl renderer treat black-box parameters. operator depicted ﬁgure challenge working sophisticated renderers back-propagating errors image mesh. requires either differentiable renderer resort gradient estimation techniques ﬁnite-differences monte carlo estimators scheme based reinforce details provided appendix demonstrate ability model learn exploit scene representations challenging tasks. tasks establish powerful robust scalable model able provide high quality generations scenes robustly used tool scene completion adapted provide class-speciﬁc view-speciﬁc generations allow variations scenes explored synthesize multiple scenes form coherent understanding scene operate complex visual systems graphics renderers. explore four data sets necker cubes necker cube classical psychological test human ability spatial reasoning. simplest dataset consists volumes wire-frame cube drawn random orientation center volume primitives volumetric primitives size volume contains simple solid geometric primitive undergoes random translations pixels) rotations radians). mnistd extended mnist dataset create volumetric dataset extruding mnist images. resulting dataset number images mnist. figure generative model volumes dataset display samples model. samples sharp capture multi-modality data. left primitives middle mnistd right shapenet videos samples seen https//goo.gl/hckxs. figure probabilistic volume completion left full ground-truth volume. middle first steps mcmc chain completing missing left half data volume. right iteration mcmc chain. best viewed screen. videos samples seen https//goo.gl/hckxs. shapenet shapenet dataset large dataset meshes objects. experiment -class subset dataset commonly referred shapenet. render mesh binary volume. experiments used lstms hidden neurons latent variables generation step. context encoder varied task. image inputs used convolutions standard spatial transformers volumes used volumetric convolutions vsts. class-conditional experiments context one-hot encoding class. meshes much lower-dimensional volumes number steps working representation. used adam optimizer experiments. ground-truth volumes available directly train model using identity projection operator explore performance model training several datasets. show ﬁgure capture rich statistics shapes translations rotations across datasets. simpler datasets primitives mnistd model learns produce sharp samples. even complex shapenet dataset samples show large diversity shapes whilst maintaining details. test ability model impute missing data volumes. capability often needed remedy sensor defects result missing corrupt regions volume completion unconditional volumetric model alternate inference generation feeding result other. procedure simulates markov chain samples correct distribution show appendix test model figure quantitative results increasing number steps number contextual views lead improved log-likelihoods. left primitives. middle mnistd. right shapenet. occluding half volume completing missing half. figure demonstrates model successfully completes large missing regions high precision. examples shown appendix models also trained context representing class object allowing class conditional generation. train class-conditional model shapenet show multiple samples classes ﬁgure model produces high-quality samples classes. note sharpness accurately capture object rotations also provide variety plausible generations. samples shapenet classes shown appendix also form conditional models using single view contexts. results shown ﬁgure indicate model generates plausible shapes match constraints provided context captures multi-modality posterior. instance consider ﬁgure model conditioned single view object triangular shape. model’s three shown samples greatly varying shape whilst maintaining triangular projection. examples inferences shown appendix quantify performance model computing likelihood scores varying number conditioning views number inference steps model. figure indicates number generation steps important factor performance. additional context views generally improve model’s performance effect relatively small. experiments establish ﬁrst benchmark likelihood-bounds primitives mnistd shapenet three datasets made available upon publication. practical applications ground-truth volumes available training. instead data captured collection images accommodate fact extend generative model projection operator maps internal volumetric representation image imitates ‘camera’ ﬁrst applies afﬁne transformation volumetric representation ﬂattens result using convolutional network. parameters projection operator trained jointly rest model. details explained appendix experiment train model learn reproduce image object given views ﬁxed camera locations. model’s responsibility infer volumetric representation well camera’s position relative volume. clear model ‘cheat’ generating volumes lead good reconstructions capture underlying structure. overcome reconstructing multiple views volumetric representation using pose information reference frame internal volume. enforces consistent hidden representation generalises views. train model conditions ﬁxed context views reproduce simultaneous random views object. training sample representation given context render arbitrary camera angles. show model’s ability learn perform kind inference figure class-conditional samples given one-hot encoding class context model produces high-quality samples. notice instance sharpness variability generations ‘chair’ accurate capture rotations ‘car’ even identiﬁable legs ‘person’ class. videos samples seen https//goo.gl/hckxs. finally consider mesh-based representation demonstrate feasibility training models fully-ﬂedged black-box renderer loop. renderers accurately capture relationship representation rendering box. image complex function objects’ colors materials textures positions lights objects. building knowledge model give hints learning constrain hidden representation. consider primitives dataset however access images objects training time. primitives textured color side rendered three lights. train unconditional model given image infers parameters mesh orientation relative camera textured rendered reconstructs image accurately. inferred mesh formed collection vertices move ﬁxed lines spread object’s center parameterized vertices’ positions lines. results experiments shown ﬁgure observe addition reconstructing images accurately model correctly infers extents object view demonstrated views inferred mesh unobserved camera angles. paper introduced powerful family generative models inspired recent advances image modeling. showed trained ground-truth volumes produce high-quality samples capture multi-modality data. showed common inference tasks inferring posterior structures given image performed efﬁciently conditional training. also demonstrated end-to-end training models directly images differentiable renderers. demonstrates ﬁrst time feasibility learning infer representations purely unsupervised manner. experimented kinds representations volumes meshes. volumes ﬂexible capture diverse range structures however introduce modeling computational challenges high dimensionality. conversely meshes much lower dimensional therefore easier work with data-type choice common rendering engines however standard paramaterizations restrictive range shapes capture. figure recovering structure images model trained volumes conditioned context. corresponds independent sample model given display viewed angle columns display inferred representation different viewpoints. model generates plausible varying interpretations capturing inherent ambiguity problem. left mnistd. right shapenet. videos samples seen https//goo.gl/hckxs. figure structure multiple images conditioned depth images object context model trained reconstruct depth images object different views. left context views. right columns display inferred abstract representation rendered different viewpoints learned projection operator. videos samples seen https//goo.gl/hckxs. figure unsupervised learning structure model observes trained reconstruct using mesh representation opengl renderer resulting rotate camera around inferred mesh visualize model’s understanding shape. observe addition reconstructing accurately model correctly infers extents object view demonstrating true understanding scene. best viewed color. videos samples seen https//goo.gl/hckxs.", "year": 2016}