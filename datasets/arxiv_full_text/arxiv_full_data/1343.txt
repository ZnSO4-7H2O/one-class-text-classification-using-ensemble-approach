{"title": "Place classification with a graph regularized deep neural network model", "tag": ["cs.RO", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. It is a nontrivial classification problem which has attracted many research. In recent years, there is a high exploitation of Artificial Intelligent algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With the deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. Firstly, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Secondly, each layer of data is fed into a deep neural network model for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all the layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effective- ness of our end-to-end place classification framework in which both the multi-layer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.", "text": "analysing forms data sensory data location information gain insights characteristics place classiﬁcation problem. sensory data encode environment information different locations provide discriminative information different classes. however requires effective feature extraction method previous works tend extract hand-engineered features data opinion hand held features fully exploit potential achieve higher generalization ability. hand locations encode spatial information environment indicate local consistency labels means positions spatial proximity higher probability class labels. noted another difﬁculty place classiﬁcation inﬂuence different ﬁeld views sensors used. example laser range ﬁnder collects data facing approximately corner corridor contain enough information classiﬁcation. laser range ﬁnder collects data door ofﬁce room robot might confused mixed information classes. construction multi-layer inputs environmental information paper represented generalized voronoi graph topological graph nodes correspond sensory data edges denote relationships. fusing information eliminating end-nodes implement recursive algorithm construct multi-layer inputs hierarchical gvgs. inputs higher layers contain information larger ﬁeld view represented increasingly succinct gvg. features extracted layer input classiﬁed independently. graph regularized deep architecture feature learning classiﬁcation adopt deep architecture learns features data automatically. graph regularizer imposed deep architecture keep local consistency adjacency graph constructed depict adjacency similarity samples. training testing maps deep architecture feature learning time forms semi-supervised learning framework. output step predicted labels different layers. abstract— place classiﬁcation fundamental ability robot possess carry effective human-robot interactions. nontrivial classiﬁcation problem attracted many research. recent years high exploitation artiﬁcial intelligent algorithms robotics applications. inspired recent successes deep learning methods propose end-to-end learning approach place classiﬁcation problem. deep architectures methodology automatically discovers features contributes general higher classiﬁcation accuracies. pipeline approach composed three parts. firstly construct multiple layers laser range data represent environment information different levels granularity. secondly layer data deep neural network model classiﬁcation graph regularization imposed deep architecture keeping local consistency adjacent samples. finally predicted labels obtained layers fused based conﬁdence trees maximize overall conﬁdence. experimental results validate effectiveness end-to-end place classiﬁcation framework multi-layer structure graph regularization promote classiﬁcation performance. furthermore results show features automatically learned input range data achieve competitive results features constructed based statistical geometrical information. place classiﬁcation important problems human-robot interactions mobile robotics aims distinguish differences environmental locations assign label location allows robots achieve spatial awareness semantic understanding rather rely precise coordinates communicating humans. furthermore semantic labels potential efﬁciently facilitate robotic functions mapping behavior-based navigation task planning active object search rescue environment sensing. laser range ﬁnders cameras rgbsensors mostly used sensing modalities. location topological information also informative place classiﬁcation. work attempted exploit sensory data location information. assume maps paper contain parts information maps labeled human knowledge. conﬁdence tree decision making receiving classiﬁcation results multi-layer inputs conﬁdence trees constructed according topological graph decision making process carried maximize overall conﬁdence. remainder paper organized follows section reviews related literature. section introduce construction multi-layer inputs conﬁdence tree decision making. semisupervised classiﬁcation graph regularization given section experimental results presented section validate effectiveness end-to-end classiﬁcation framework. paper concluded section various sensors help robots sense environments cameras laser range ﬁnders. previous works demonstrated effectiveness camera data laser range ﬁnder data classifying places. example viswanathan extracted features vision data mozos sousa classiﬁed places based laser range data. paper focus place classiﬁcation based laser range data however approach easily extended modality sensors vision data. laser range ﬁnders provide nonnegative beam sequences describing range bearing nearby objects. contain structural information including clutter environment. mozos extracted features laser range data features adaboost classiﬁer label environment. sousa reported superior results binary classiﬁcation task using subset mentioned features support vector machine classiﬁer. past work implemented logistic regression based classiﬁer binary multiclass problem contributing higher accuracies work extended address generalizability solution semi-supervised place classiﬁcation generalized voronoi graph methods features extracted laser range data based statistical geometrical information so-called hand-engineered features. instance average standard deviation beam length area perimeter polygon speciﬁed observed range data bearing included feature set. unsupervised feature learning drawn much attention deep learning methods developed deep learning methods achieved remarkable results many areas including object recognition natural language processing speech recognition demonstrated discovering extracting features automatically usually achieve better results representation learning inspired success unsupervised feature learning article present end-to-end framework deep learning method learn features automatically laser range data. also exploit local consistency classes assumption samples located small region likely labels. previous research included particular characteristic performance promotion many studies carried consideration local consistency paper consider local consistency feature learning process where features learn keep local invariance graph regularization. similar works implementing graph regularized deep learning models utilized margin-based loss function proposed hadsell works demonstrated effectiveness graph embedding dimensionality reduction image classiﬁcation. lack crucial information. however full ﬁeld view also lead misclassiﬁcations boundaries different classes places. therefore considering problems propose construct multi-layer inputs classiﬁcation followed fusion results. data representation paper multilayer inputs represented hierarchical generalized voronoi graph topological graph successfully applied navigation localization mapping. general representation composed meet-points edges adopt resolution previous work construct ﬁrst layer higher layers gvgs constructed describe environment different levels granularity. let’s denote hierarchical gvgs g··· denotes number layer denotes nodes layer denotes edges layer layer independent sensing information carried nodes local connectivity represented edges speciﬁcally corresponds sequence range data node training maps reveals connection nodes distance ﬁrst layer describes environment detailed level granularity originally adopted laser range data. laser range ﬁnder ﬁeld view angular resolution node corresponds sequence range data dimension. recursive higher layer construction algorithm construction higher layer implemented fusing information carried connected nodes eliminating marginal nodes. algorithm demonstrates process building higher layer given lower layer. make deﬁnitions better explanation algorithm. deﬁned directly connected neighbour means edge addition numel deﬁned number elements contained numel) means end-node i.e. node without children. deﬁne endnodes connected obviously seen algorithm construction process fuses information carried vi’s neighbors endnode otherwise eliminated higher layer. layer data generated recursively applying algorithm times means taking output layer input layer. process illustrated figure example end-nodes denoted red. noted moving higher layers number nodes layer decreases elimination end-nodes. details given caption figure illustration different layers constructed speciﬁc given figure ﬁrst layer nodes distributed densely map. approaching higher layers tree structure represents abstract information. noted number end-nodes decreases progression layers consideration choosing experiments. data generation section describes details construction higher-layer range data latter generated former ﬁxed length. stated algorithm given satisfying numel end-node) range data received respective nodes integrated achieve better perception. ﬁrstly local generated using occupancy grid mapping based respective range data layer including achieved transforming coordinate frame assumes knowledge global robot poses times. local virtual scan generated angular applying casting position resolution setting real laser range ﬁnder. tree structures built dependencies algorithm except minor difference construction tree structures parent node owns children range data constructed range data carried reason nodes also reserved higher layer predicted labels don’t consider inﬂuence them. noted number tree structures equal number nodes left layer fig. example multi-layer end-nodes denoted red. nodes layer fused neighbours respectively where composed nodes eliminated layer process performed recursively layer generate layer measure used decision making process discussed next section thus denote uniformity however don’t apply linear interpolation layer since initial laser range data always dimension necessary linear interpolation. applying data pre-processing approach laser range data layer layer kept ﬁxed length note always employed construct next layer rather pre-processed example figure illustrates construction sequence input layer using corresponding inputs layer followed result linear interpolation. details given caption figure construction conﬁdence tree layer inputs obtain predicted labels independent classiﬁers formed conﬁdence trees layers shown figure node denotes predicted label corresponding conﬁdence maximizing overall conﬁdence layer layer conﬁdence parent always compared average optimized conﬁdence children assume optimized conﬁdences layer known original conﬁdences. optimized predicted labels algorithm tells changed follow ancestor ancestor beats children conﬁdence. words none ancestor leaf node gain advantages conﬁdence leaf node would keep initial label optimized label note although obtain optimized labels nodes decision algorithm labels leaf nodes exported output since classiﬁcation performance evaluated based leaf nodes. example given figure better clarity. also evaluate results obtained independent classiﬁers separately help constructed trees. ensure fairness results obtained different layer classiﬁers compared accuracy bottom layer. obviously results observed input layer need modiﬁed higher layers spread predicted labels bottom layer. given speciﬁc layer nodes bottom layer assigned label ancestor layer example shown figure predicted label evaluate results layer fig. example constructing range data collected real environment. middle ﬁgure shows constructed given right magenta points correspond interpolated ones. example fig. conﬁdence trees built figure corresponding example. conﬁdence tree parent node decision example example let’s assume conﬁdence node known. applying decision method given algorithm ﬁrstly initialization average conﬁdence children bottom layer compared corresponding parents immediate upper layer. left tree therefore assigned label smaller average value hence leaf nodes remain initial label finally compared since conﬁdence layer smaller optimized average conﬁdence combined layer layer ﬁnal optimized conﬁdence optimized labels change. applying decision process right tree ﬁgure iii. section discuss classiﬁcation problem train layer input data obtain predicted labels testing maps. implemented deep learning structure capability automatically learn features input data. layer inputs trained independent deep learning modes indicated figure though models structure laser range data input predicted labels output shown figure thus discussion section conﬁned speciﬁc layer hence superscripts omitted. noted training process semi-supervised since training testing employed model training labels training available. semi-supervised learning process advantage gaining richer information data distribution keeping spatial consistency introduce chapter. classiﬁcation problem denote training pairs convention denotes input dimension denotes number training samples. particularly column sequence laser range data i.e. testing data deﬁned rm×nu denotes number testing samples. task classiﬁcation problem obtain predicted labels given addition denote rm×n combination training data testing data since model whole semi-supervised training process. illustrated figure input ﬁrstly ﬁxed parameters compute differences consecutive beams scan consecutive differences also provide rich information place classiﬁcation often employed extracting geometric features previous works pre-training classiﬁcation back propagation used ﬁne-tune whole learning process promotion means parameters preserved encoders softmax trained together. order keep local consistency graph regularization term ﬁne-tuning learned representation. cost function ﬁne-tuning given follow ﬁrst term corresponds prediction error training data second term graph regularization. outputs last hidden layer respect inputs similarity measurement between samples connected element adjacency graph n×n. figure also illustrates cost function work. costs caused prediction error imposed softmax classiﬁer graph regularization imposed last hidden layer. ﬁne-tuning jlabel inﬂuence parameters jgraph inﬂuence parameters feature learning. shown learned features large weight pushed together graph regularization term. section describe details construction adjacency graph built steps. firstly deﬁne connected relationships samples calculate weights connected edges. place classiﬁcation problem connected relationships topological graph directly employed adjacency graph. samples close coordinates forced represented features close distances. weights corresponds strength graph regularization inversely associated distances i.e. distance coordinates distance input data formulated fig. model training semi-supervised learning second layer ﬁxed parameters computes consecutive differences input input output second layer latter process. ﬁne-tuning process jlabel imposed softmax classiﬁer parameters neural network adjusted jgraph imposed last hidden layer inﬂuence feature learning process. point input output ﬁxed layer stacked auto-encoders feature learning. auto-encoder widely used structure building deep architectures composed encoder decoder. feeding representation learned previous encoder input another autoencoder obtain stacked hidden representations shown figure let’s denote sigmoid function layer encoder decoder represented follow ˆhi− denote input reconstruction denotes hidden representation denote weighted parameters respectively. paper weights pair encoder decoder tied together shown weighting scheme dose evaluate geometrical information also considers closeness inputs. example given edge connects nodes belonging corridor ofﬁce respectively although small large. therefore nodes forced close representation space however still keeps discriminative information. validate effectiveness end-to-end multi-layer learning system conduct experiments data sets collected international university indoor environments stated previously robot collected range data nodes using laser range ﬁnder maximum range horizontal ﬁeld view noted classes deﬁned humans somewhat vague plentiful according different functions places. however range data contain enough discriminative information classify human-designed classes. therefore careful thinking consider target classes class -space designed small number individuals including cubicle ofﬁce printer room kitchen bathroom stairwell elevator; class -space group activities including meeting room laboratory; class -corridor. among data sets contain classes others contain parts classes. consider leave-many-out training means data utilized training others used testing. therefore obtained groups results training intellab respectively. feature learning classiﬁcation model layer input shown figure given input rm×n dimension conﬁguration learning model means consecutive differences layer dimension input dimension hidden layers respectively. thus dimension learned features finally output model represents probabilistic measure data belonging class. thus output dimension number classes. addition since perform interpolation dimension higher layers introduced section iii-a. paper choose ﬁrst conduct experiments evaluate performance multi-layer inputs. table table shows leave-many-out classiﬁcation results training intellab respectively. noted graph regularization considered therefore also carried experiments validate effectiveness graph regularization. algorithm remains previous settings however changed value graph regularization. experiments attention geometrical neighborhood thus construction adjacency graph. classiﬁcation results shown table table trained intellab respectively. results similar trends table table higher layers give rise better accuracies. comparisons table table show feature learning graph regularization performs better without reveals graph regularization advantage improving classiﬁcation performances keeping local consistency. shown table table fusion results achieved better accuracies. results trained intellab average accuracy fusion results risen results trained also reached l.%. fused test results trained intellab diagrammatically illustrated figure noted confusions class class account major misclassiﬁcations especially test cause might class featured narrow environment including massive clutters class featured relatively larger spaces therefore corners meeting room mostly classiﬁed ofﬁce room rooms center positions ofﬁce room assigned ofﬁce room. also make comparisons results achieved previous work spcogvg spcogvg also semi-supervised approach composed support vector machine conditional random ﬁeld ensure generalization ability. dimensional hand-engineered features spcogvg extracted range data geometrical knowledge. notice learned features dimension hand-engineered features experiments. seen table table achieve slightly better average results spcogvg. paper presented end-to-end place classiﬁcation framework. implemented multi-layer learning framework including construction multi-layer inputs decision making multi-layer results. layer inputs semi-supervised model feature experimental results showed higher layer input data higher classiﬁcation accuracy validated effectiveness multi-layer structure. performing semi-supervised learning without graph regularization also showed graph regularization help promoting classiﬁcation performance keeping local consistency. furthermore fusion results based conﬁdence tree achieved comparable results stateof-art method. nutshell achieved generalization ability preserved local consistency end-to-end place classiﬁcation framework. future work apply framework type sensor data rgb-d data representative discriminative ability. aydemir sjoo folkesson pronobis jensfelt. search real world active visual object search based robotics automation ieee spatial relations. international conference pages ieee y.-l. boureau roux bach ponce lecun. locals multi-way local pooling image recognition. computer vision ieee international conference pages ieee choset burdick. sensor based planning. generalized voronoi graph. robotics automation proceedings. ieee international conference volume pages ieee dahl deng acero. context-dependent pretrained deep neural networks large-vocabulary speech recognition. audio speech language processing ieee transactions glorot bordes bengio. domain adaptation largeproscale sentiment classiﬁcation deep learning approach. ceedings international conference machine learning pages hadsell chopra lecun. dimensionality reduction learning invariant mapping. ieee computer society conference computer vision pattern recognition volume cvpr pages washington ieee computer society. hadsell chopra lecun. dimensionality reduction computer vision pattern learning invariant mapping. recognition ieee computer society conference volume pages ieee building high-level features using large scale unsupervised learning. acoustics speech signal processing ieee international conference pages ieee mozos stachniss burgard. supervised learning places range data using adaboost. robotics automation icra proceedings ieee international conference pages ieee poncela urdiales fern´andez-espejo sandoval. place characterization navigation behaviour merging electrotechnical conference autonomous mobile robot. melecon ieee mediterranean pages ieee pronobis jensfelt. large-scale semantic mapping reasoning heterogeneous modalities. robotics automation ieee international conference pages ieee rifai mesnil vincent muller bengio dauphin glorot. higher order contractive auto-encoder. proceedings european conference machine learning knowledge discovery databases volume part ecml pkdd’ pages berlin heidelberg springer-verlag. kodagoda dissanayake. laser range data based semantic labeling places. intelligent robots systems ieee/rsj international conference pages ieee kodagoda dissanayake. multi-class classiﬁcation semantic labeling places. control automation robotics vision international conference pages ieee samarabandu. investigating performance corridor door detection algorithms different environments. information automation icia international conference pages ieee sousa ara´ujo nunes. real-time labeling places using support vector machines. industrial electronics isie ieee international symposium pages ieee tully kantor choset. incremental construction saturated-gvg multi-hypothesis topological slam. robotics automation ieee international conference pages ieee viswanathan meger southey little mackworth. automated spatial-semantic modeling applications place lacomputer robot vision beling informed search. crv’. canadian conference pages ieee yuan chan lee. robust semantic place recognition proc. iros vocabulary tree landmark detection. workshop active semantic perception object search real world", "year": 2015}