{"title": "Sum-Product Networks: A New Deep Architecture", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.", "text": "limiting factor graphical model inference learning complexity partition function. thus question general conditions partition function tractable? answer leads kind deep architecture call sumproduct networks spns directed acyclic graphs variables leaves sums products internal nodes weighted edges. show complete consistent represents partition function marginals graphical model give semantics nodes. essentially tractable graphical models cast spns spns also strictly general. propose learning algorithms spns based backpropagation experiments show inference learning spns faster accurate standard deep networks. example spns perform image completion better state-of-the-art deep networks task. spns also intriguing potential connections architecture cortex. goal probabilistic modeling represent probability distributions compactly compute marginals modes efﬁciently learn accurately. graphical models represent distributions compactly normalized products factors d-dimensional vector potential function subset x{k} variables px∈x partition function. graphical models number important limitations. first many distributions admit compact representation form above. second inference still exponential worst case. third sample size required accurate learning worst-case exponential scope size. fourth learning requires inference subroutine take exponential time even ﬁxed scopes compactness graphical models often greatly increased postulating existence hidden variables φk{k}). deep architectures viewed graphical models multiple layers hidden variables potential involves variables consecutive layers variables shallowest layer many distributions represented compactly deep networks. however combination non-convex likelihood intractable inference makes learning deep networks extremely challenging. classes graphical models inference tractable exist thin junction trees quite limited distributions represent compactly. paper starts observation models multiple layers hidden variables allow efﬁcient inference much larger class distributions. surprisingly current deep architectures take advantage this typically solve harder inference problem models hidden layers. seen follows. partition function intractable exponential number terms. marginals sums subsets terms; thus computed efﬁciently they. function potentially compactly represented using deep architecture. computed using types operations sums products. computed efﬁciently px∈x reorganized using distributive computation involving polynomial number sums products. given graphical model inference problem nutshell perform reorganization. instead learn outset model already efﬁciently computable form answer question providing conditions tractability showing general previous tractable classes. introduce sum-product networks representation facilitates treatment also semantic value right. spns viewed generalized directed acyclic graphs mixture models nodes corresponding mixtures subsets variables product nodes corresponding features mixture components. spns lend themselves naturally efﬁcient learning backpropagation course many distributions cannot represented polynomial-sized spns whether sufﬁcient real-world problems need solve empirical question. experiments show quite promising. simplicity focus ﬁrst boolean variables. extension multi-valued discrete variables continuous variables discussed later section. negation boolean variable represented ¯xi. indicator function value argument true otherwise. since clear context whether referring variable indicator abbreviate ¯xi. build ideas darwiche particular notion network polynomial. unnormalized probability distribution. network polynomial product indicators value state example network polynomial bernoulli distribution variable parameter ¯xi. network polynomial bayesian network network polynomial multilinear function indicator variables. unnormalized probability evidence value network polynomial indicators compatible remainder example value network polynomial remaining indicators throughout. partition function value network polynomial indicators evidence cost computing linear size network polynomial. course network polynomial size exponential number variables able represent evaluate polynomial space time using sum-product network. rooted directed acyclic graph whose leaves indicators whose internal nodes sums products. edge emanating node non-negative weight wij. value product node product values children. value node pj∈ch wijvj children value node value value root. denote sum-product network function indicator variables indicators specify complete state abbreviate indicators specify evidence abbreviate indicators abbreviate subnetwork rooted arbitrary node denote values deﬁne unnormalized probability distribution unnormalized probability evidence distribution px∈e states consistent partition function distribution deﬁned px∈x scope variables appear variable appears negated leaf non-negated leaf induction hypothesis node state corresponds multiple states therefore monomial non-zero state breaking correspondence monomials states however complete states one-to-one correspondence. therefore induction hypothesis monomials also one-to-one correspondence wφ)π; i.e. expansion network polynomial. φπ¢. follows immediately expansion network polynomial. corresponding states. since non-zero exactly state similarly monomial product nonzero state consistent least monomial product contains positive negative indicators variable ¯xi. since monomial network polynomial contains means expansion equal network polynomial. ensure monomial non-zero least state every pair must exist state therefore indicators monomials must consistent. since induction hypothesis completely specify must monomials. therefore monomials must indicators i.e. must consistent. completeness consistency necessary validity; network incomplete inconsistent satisﬁes px∈e evidence however completeness consistency necessary stronger property every subnetwork valid. proved refutation. input nodes valid definition. node violates either completeness consistency descendants satisfy conditions. show valid since either undercounts summation overcounts words valid always correctly computes probability evidence. particular valid valid computes probability evidence time linear size. would like learn valid spns otherwise much ﬂexibility possible. thus start establishing general conditions validity spn. proof. every expressed polynomial monomial indicator variables coefﬁcient. call expansion spn; obtained applying distributive bottom-up product nodes treating leaf leaf ¯xi. valid expansion network polynomial i.e. monomials expansion states one-to-one correspondence monomial non-zero exactly state state exactly monomial non-zero condition equal coefﬁcient monomial non-zero therefore px∈e px∈e sknk number states consistent condition state consistent evidence otherwise therefore valid. prove induction leaves root that complete consistent expansion network polynomial. trivially true leaf. consider internal nodes children; extension general case immediate. arbitrary internal node children denote scope state expansion subgraph rooted unnormalized probability similarly complete monomials missing indicators relative monomials network polynomial thus invalid spns useful approximate inference. exploring direction future work. theorem partition function markov network d-dimensional vector computed time polynomial representable sumproduct network number edges polynomial decomposability restricted consistency consistent decomposable.) makes spns general representations require decomposability like arithmetic circuits probabilistic context-free grammars mixture models junction trees others. spns extended multi-valued discrete variables simply replacing boolean indicators indicators variable’s possible values short. example multinomial distribution represented complete consistent every node pj∈ch children case view node result summing implicit hidden variable whose values correspond children variable summed setting indicators children product nodes whose value omitted. thus rooted node viewed mixture model children mixture components turn products mixture models. parent children’s weights yi’s prior distribution otherwise condition that least path root yi’s ancestors values lead network also decomposable subnetwork rooted child represents distribution variables conditioned thus viewed compact specify mixture model exponentially many mixture components subcomponents composed reused larger ones. perspective naturally derive algorithm learning. spns generalized continuous variables viewing multinomial variables inﬁnite number values. multinomial’s weighted indicators becomes integral p.d.f. example univariate gaussian. thus spns continuous variables integral nodes instead nodes indicator children form sums products nodes before leading rich compact language specifying high-dimensional continuous distributions. inference evidence includes value integral node otherwise value computing probability evidence proceeds before. given valid marginals variables computed differentiation arbitrary node value input instance parents. product node parents nodes ∂s/∂si pk∈p wki∂s/∂sk. pk∈p ai/∂sk) ql∈ch−i ch−i children parent excluding thus evaluate si’s upward pass input root parents following children compute ∂s/∂wij ∂s/∂si downward pass root input children following parents. marginals nodes derived partial derivatives particular child node wki∂s/∂sk; indicator ∂s/∂si. state maxxy computed replacing sums maximizations. upward pass node outputs maximum weighted value among children instead weighted sum. downward pass starts root recursively selects highest-valued child node children product node. based results darwiche prove state decomposable. extension proof consistent spns straightforward since deﬁnition conﬂicting input indicators chosen. continuous case similar straightforward long computing argmax easy compact representation distribution moder class size size representation constant exponential function. model class general model class distributions size) size) exist distributions size) exp). sense sum-product networks general hierarchical mixture models thin junction trees clearly represented spns without loss compactness spns exponentially compact hierarchical mixture models allow mixtures subsets variables reuse. spns exponentially compact junction trees context-speciﬁc independence determinism present since exploit junction trees not. holds even junction trees formed bayesian networks contextspeciﬁc independence form decision trees nodes decision trees suffer replication problem exponentially larger representation function. figure shows implements uniform distribution states variables even number well corresponding mixture model. distribution also non-uniform weights uniform. general spns represent distributions size linear number variables reusing intermediate components. contrast mixture model requires exponential number components since component must correspond complete state else assign non-zero probability state number figure representing uniform distribution states variables containing even number bottom mixture model distribution. simplicity omit uniform weights. cannot simpliﬁed polynomial size cannot represented compactly spns. interestingly previous example shows spns compactly represent classes distributions conditional independences hold. multi-linear representations also property since mlrs essentially expanded spns exponentially compact corresponding mlr. spns closely related data structures efﬁcient inference like arithmetic circuits and/or graphs however date viewed purely compilation targets bayesian network inference related tasks semantics models right. result problem learning generally considered. exceptions aware lowd domingos gogate lowd domingos’s algorithm standard bayesian network structure learner complexity resulting circuit regularizer ﬂexibility learning. gogate al.’s algorithm learns markov networks representable compact circuits reuse subcircuits. case-factor diagrams another compact representation similar decomposable spns. algorithms learning computing probability evidence proposed date. explicitly represent features require sums inefﬁciently computed gibbs sampling otherwise approximated. convolutional networks alternate feature layers pooling layers pooling operation typically average features layer subset input variables. convolutional networks probabilistic usually viewed vision-speciﬁc architecture. spns viewed probabilistic general-purpose convolutional networks average-pooling corresponding marginal inference max-pooling corresponding inference. proposed probabilistic version max-pooling architecture correspondence pooling operations probabilistic inference result inference generally intractable. spns also viewed probabilistic version competitive learning sigma-pi networks like deep belief networks spns used nonlinear dimensionality reduction allow objects reconstructed reduced representation probabilistic context-free grammars statistical parsing straightforwardly implemented decomposable spns non-terminal nodes corresponding sums productions corresponding products learning amounts directly learning chart parser bounded size. however spns general represent unrestricted probabilistic grammars bounded recursion. spns also well suited implementing learning grammatical vision models structure parameters learned together starting densely connected architecture learning weights multilayer perceptrons. algorithm shows general learning scheme online learning; batch learning similar. first initialized generic architecture. requirement architecture valid example processed turn running inference updating weights. repeated convergence. ﬁnal obtained pruning edges zero weight recursively removing non-root parentless nodes. note weighted edge must emanate node pruning edges violate validity spn. therefore learned guaranteed valid. completeness consistency general conditions leave room ﬂexible choice architectures. here propose general scheme producing initial architecture select subsets variables. subset create nodes select ways decompose selected subsets decompositions create product node parents require polynomial number subsets selected subset polynomial number decompositions chosen. ensures initial polynomial size guarantees efﬁcient inference learning ﬁnal spn. domains inherent local structure usually intuitive choices subsets decompositions; give example section image data. alternatively subsets decompositions selected randomly random forests domain knowledge also incorporated architecture although pursue paper. spns lend naturally efﬁcient computation likelihood gradient backpropagation child node ∂s/∂wij /∂si)sj computed along ∂s/∂si using marginal inference algorithm described section weights updated gradient step. ensure throughout renormalizing weights step i.e. projecting gradient onto constraint surface. alternatively vary optimize s/s. spns also learned using viewing node result summing corresponding hidden variable described section inference algorithm step computing marginals yi’s weight update step adding yi’s marginal previous iterations renormalizing obtain weights. either case learning done placing prior weights. particular sparse prior leading smaller pruning zero weights thus faster inference well combatting overﬁtting. unfortunately gradient descent described give poor results learning deep spns. gradient descent falls prey gradient diffusion problem layers added gradient signal rapidly dwindles zero. difﬁculty deep learning. also suffers problem updates also become smaller smaller deeper. propose overcome problem using hard i.e. replacing marginal inference inference. algorithm maintains count child step simply increments count winning child; weights obtained normalizing counts. avoids gradient diffusion problem updates root inputs unit size. experiments made possible learn accurate deep spns tens layers instead typically used deep learning. evaluated spns applying problem completing images. good test deep architecture extremely difﬁcult task detecting deep structure key. image completion studied quite extensively graphics vision communities focus tends restoring small occlusions facilitate recognition tasks. recent machine learning works also showed selected image completion results limited often focused small images. contrast conducted extensive evaluations half image occluded. conducted main evaluation caltech- well-known dataset containing images categories faces helicopters dolphins. category aside last third test trained using rest. test image covered half image applied learned complete occlusion. additionally also experiments olivetti face dataset containing faces. initialize used architecture leverages local structure image data. speciﬁcally generatedensespn rectangular regions selected smallest regions corresponding pixels. rectangular region consider possible ways decompose rectangular subregions. made learning much faster little degradation accuracy. particular adopted architecture uses decompositions coarse resolution m-by-m large regions ﬁner decompositions inside m-byblock. experiments. spns learned experiments deep containing layers. general architecture layers root input images. numbers spns multiple resolution levels computed similarly. used mini-batches online hard processing instances batch trivially parallelized. running soft hard yielded improvement. best results obtained using sums upward pass maxes downward pass initialized weights zero used add-one smoothing evaluating nodes. penalized non-zero weights prior parameter handle gray-scale intensities normalized intensities input images zero mean unit variance treated pixel variable continuous sample gaussian mixture unit-variance components. pixel intensities training examples divided equal quantiles mean component corresponding quantile. used four components experiments. compared spns deep belief networks deep boltzmann machines state-of-the-art deep architectures codes publicly available. dbns dbms consist several layers restricted boltzmann machines differ probabilistic model training procedure. also compared spns principal component analysis nearest neighbor. used extensively previous image completion works used principal components experiments. despite simplicity nearest neighbor give quite good results image similar test seen past test image found training image similar right half using euclidean distance returned left half. figure scatter plots comparing spns dbms nearest neighbor mean square errors caltech-. point represents image category. axes scale axes. left completion. bottom bottom completion. efﬁcient exact inference dbns dbms require approximate inference. problem gradient diffusion limits learned dbns dbms layers whereas online hard deep accurate spns learned experiments. practice dbns dbms also tend require substantially engineering. example hyperparameters spns preliminary experiments found values worked well datasets. also used architecture throughout learning adapt details dataset. contrast dbns dbms typically require careful choice parameters architectures dataset. learning terminates average log-likelihood improve beyond threshold dbns/dbms however number iterations determined empirically using large development set. further successful dbn/dbm training often requires extensive preprocessing examples used essentially none spns. second spns least order magnitude faster learning inference. example learning caltech faces takes minutes cpus hours cpu. contrast depending number learning iterations whether much larger transformed dataset used learning time dbns/dbms ranges hours week. inference spns took less second completion image compute likelihood completion compute marginal probability variable results exact. contrast estiresults example categories olivetti. note results directly comparable others. using original images without additional preprocessing learned gave poor results despite extensive effort experiment using code hinton salakhutdinov hinton salakhutdinov reported results image reconstruction olivetti faces used reduced-scale images required training containing images derived transformations like rotation scaling etc. converting reduced scale initializing learned model results improve signiﬁcantly report results instead. note reducing scale artiﬁcially lowers mean square errors reducing overall variance. although appears lower errors nearest neighbor completions actually much worse overall outperforms methods wide margin. performs surprisingly well terms mean square errors compared methods completions often quite blurred since linear combination prototypical images. nearest neighbor give good completions similar image training general completions quite poor. figure shows scatter plots comparing spns dbms nearest neighbor conﬁrms advantage spn. differences statistically signiﬁcant binomial sign test level. potential spns object recognition. reported results convolutional dbns training cdbn three caltech- categories faces motorbikes cars computed area precision-recall curve comparing probabilities positive negative examples classiﬁcation problem followed experimental setting conducted experiments using spns. table compares results obtained using layer features convolutional dbns spns obtained almost perfect results three categories whereas cdbns’ results substantially lower particularly motorbikes cars. cortex composed main types cells pyramidal neurons stellate neurons. pyramidal neurons excite neurons connect stellate neurons inhibit them. interesting analogy types neurons nodes spns particularly inference used. case network composed nodes nodes also uses nodes probabilistic model.) nodes analogous inhibitory neurons select highest input propagation. nodes analogous excitatory neurons compute inputs. spns weights inputs nodes analogy cortex suggests inputs nodes. mapped nodes ignore children’s weights consider values. possible justiﬁcations include potentially reduces computational cost allowing nodes merged; ignoring priors improve discriminative performance priors approximately encoded number units representing pattern facilitate online hard learning. unlike spns cortex single root node straighforward extend spns multiple roots corresponding simultaneously computing multiple distributions shared structure. course spns still biologically unrealistic mating likelihood dbns dbms challenging problem estimating marginals requires many gibbs sampling steps take minutes even hours results approximate without guarantee quality. third spns appear learn much effectively. example show faces completion results figure paper. network able complete small portion faces leaving rest blank completions generated dbms look plausible isolation often odds observed portion completions often reused different images. mean square error results table conﬁrmed completions often good. among categories dbms performed relatively well caltech olivetti faces. contrast example completions figure shows results completing left halves previously unseen faces. completions often seem derive nearest neighbor according learned model suggests might learned deep regularities. comparison successfully completed faces hypothesizing correct locations types various parts like hair mouth face shape color. hand also weaknesses. example completions often look blocky. sum-product networks dags sums products efﬁciently compute partition functions marginals high-dimensional distributions learned backpropagation spns viewed deep combination mixture models feature hierarchies. inference spns faster accurate previous deep architectures. turn makes learning faster accurate. experiments indicate that robustness spns require much less manual engineering deep architectures. much remains explored including learning methods spns design principles architectures extension sequential domains applications. acknowledgements thank ruslan salakhutdinov help experiments dbns. research partly funded grant wnf--- afrl contract fa-c- grant iis- grant n--. views conclusions contained document authors interpreted necessarily representing ofﬁcial policies either expressed implied afrl united states government.", "year": 2012}