{"title": "Getting deep recommenders fit: Bloom embeddings for sparse binary  input/output networks", "tag": ["cs.LG", "cs.AI", "cs.IR", "cs.NE"], "abstract": "Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., one-hot-encoded vectors of item preferences), these algorithms tend to have large input and output dimensionalities that dominate their overall size. This makes them difficult to train, due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "text": "option reduce size sparse inputs outputs embed lower-dimensional space. embedding sparse high-dimensional inputs commonplace however embedding sparse high-dimensional outputs even inputs outputs time much less common importantly typical embeddings still require storage processing large matrices dimensionality input/output gains terms space limited. mentioned size models dominated input/output dimensionality input output layers representing around total amount models’ parameters. example found deep recommender hidasi uses gated recurrent unit perform session-based recommendations input/output layers dimensionality internal layers dimensionality general ideal embedding procedure sparse highdimensional inputs/outputs produce compact embeddings much lower dimensionality original input/output. addition consume lile space terms storage memory space. smaller sizes imply less parameters thus training model embedded vectors would also faster original instances. embedding output also lead formulation appropriate loss clear. embeddings compromise accuracy model required number training epochs obtain accuracy. addition changes original core architecture model required achieve good performance embedding also fast; done directly ‘on-the-y’ least fast enough speed improvements made training lost embedding operation. last least output embeddings easily reversible output model could mapped original items prediction time. work propose unsupervised embedding technique fullls previous requirements. applied input output layers neural network models deal binary inputs and/or outputs. produces lower-dimensionality binary embeddings easily mapped original instances. provided embedding dimension accuracy compromised. furthermore cases show training embedded vectors even increase prediction accuracy abstract recommendation algorithms incorporate techniques deep learning becoming increasingly popular. structure data coming recommendation domains algorithms tend large input output dimensionalities dominate overall size. makes dicult train limited memory graphical processing units dicult deploy mobile devices limited hardware. address diculties propose bloom embeddings compression technique applied input output neural network models dealing sparse high-dimensional binary-coded instances. bloom embeddings computationally ecient seriously compromise accuracy model compression ratios. cases even improve original accuracy relative increases evaluate bloom embeddings data sets compare alternative methods obtaining favorable results. also discuss number advantages bloom embeddings ‘on-the-y’ constant-time operation zero marginal space requirements training time speedups fact require change core model architecture training conguration. introduction size neural network models deal sparse inputs outputs dominated dimensionality inputs outputs. deep networks used recommender systems collaborative ltering paradigmatic case deal high-dimensional sparse vectors typically order tens thousands hundreds millions input output network results large models present number diculties training prediction stages. apart training prediction times case three considered recommendation tasks). embedding requires changes core network structure model conguration works somax output common output activation binary-coded instances. embedding moreover preserves ranking order items crucial recommender systems. unsupervised embedding require preliminary training. moreover constant-time operation either performed on-the-y requiring disk memory space cached memory occupying orders magnitude less space typical embedding matrix. lower dimensionality input/output vectors result faster training mapping embedded space original overwhelming amount time prediction stage. proposed embedding based idea bloom lters therefore inherits part theory developed around idea general perspective reducing space neural network models active research topic driven need deploy models systems limited hardware resources. common approach reduce size already trained models quantization and/or pruning connections dense layers less frequent approach reduce model size training methods typically focus input layers best knowledge none deals high-dimensional outputs. also worth noting number techniques proposed eciently deal high-dimensional outputs specially natural language processing domain. hierarchical somax approach recent adaptive somax examples those. mentioned focus works speed space. work vincent focuses aspects large sparse outputs best knowledge cannot directly applied common somax outputs. related work common approach embed high-dimensional inputs hashing trick however hashing trick approach deal outputs oers explicit back embedding space original space. elementary version hashing trick used outputs considering special case bloom-based methodology proposed here. framework providing encoding decoding strategies error-correcting output codes framework originally designed single-class outputs also applied class sets another example framework oering recovery capabilities kernel dependency estimation compressed sensing approach builds ecoc reduce multi-label regression binary regression problems. similarly ciss´e bloom lters reduce multi-label classication binary classication problems improve robustness individual binary classiers’ errors. data-dependent embeddings require form learning also exist. typical approach rely variants latent semantic analysis singular value decomposition exploiting similarities correlations present data. again issue mapping embedding space original space unresolved. nonetheless recently chollet successfully applied k-nearest neighbors algorithm perform mapping derive ranking items original space. decomposition pairwise mutual information matrix used perform embedding cosine similarity used loss function retrieve neighbors. using trick oers possibility exploit dierent types factorization similarity-based matrices. canonical correlation analysis example considers inputs outputs time examples considering output embeddings nuclear norm regularized learning label embedding trees wsabie algorithm presence side information like text descriptions item class taxonomies manually-collected data range approaches applicable. akata provide comprehensive list. study assume side information available focus input/output-based embeddings. bloom embeddings bloom lters bloom lters compact probabilistic data structure used represent sets items eciently check whether item member since instances deal represent sets one-hot encoded items bloom lters interesting option embed compact space good recovery guarantees. essence bloom lters project every item dierent positions binary array size projections done using independent hash functions {hi}k range ideally distributing projected items uniformly random proper independent hash functions derived using enhanced double hashing triple hashing number hash functions usually constant proportional expected number items projected. check item feeds hash functions array positions. bits positions item denitely set. item checks return false negatives meaning structure gives answer recall however bits projected positions either item bits chance insertion items. implies false positives possible collisions projections dierent items values adjusted control probability collisions. however practice usually constrained space requirements employed independently number items projected giving less false positive probability embedding recovery following describe bloom lter techniques embedding binary high-dimensional instances recovery mapping instances embeddings. denote approach bloom embedding idea pursue embed inputs outputs perform training embedding space. that probability-based output activation suitability note construction already oers number aforementioned desired qualities sparse binary high-dimensional embeddings specically designed inputs outputs oering rank-based mapping original instances embedded vectors. yields compact representation original instance requires disk memory space addition performed on-the-y without training constant time. following demonstrate remaining desirable qualities using comprehensive experimental setup show accuracy model remains stable even increases given reasonable embedding dimension changes model architecture conguration required training times faster thanks reduction number parameters model evaluation times carry much overhead performance generally beer number alternative approaches. experimental setup general considerations demonstrate works several seings applied multiple tasks. particular focus recommendation collaborative ltering also demonstrate validity approach natural language processing task. consider number data sets network architectures congurations evaluation measures. total dene dierent setups describe sec. also demonstrate competitive respect available alternatives. consider dierent state-of-the-art approaches overview sec. data sets formed inputs instances corresponding either individual instances les) sequences instances lists). outputs also instances correspond individual instances class labels. instances original dimensionality corresponding cardinality possible prole items. given nature considered problems instances sparse items dierent typically order data based literature select appropriate baseline neural network architecture. experiment feed-forward recurrent networks carefully selecting parameters conguration match state-of-the-art results. sake comparison also choose appropriate well-known evaluation measures depending data work mean average precision reciprocal ranks accuracy combination data network architecture conguration evaluation measure denes task. every task compute baseline score corresponding running plain neural network model without embedding. report performance i-th execution particular embedding respect baseline score using si/s. compare input output instance dimensionality instances assumed that conveniently represent {pi}c number non-zero elements position elements every generate embedded instance dimensionality components iteratively every position every projection assign notice that since range number original positions index bloom lters mitigate properly choosing independent hash functions notice furthermore process space requirements computed on-the-y. finally notice embedding constant time process bounded maximum number active items constant beforehand practice constant time dominated time spent generate hash. want faster that time ensure optimal distribution outputs pre-compute hash matrix storing projections hash indices potential items generating vectors uniformly randomly chosen integer pre-generating projections items matrix integers easily store random-access memory memory. explain recover probability-based ranking items output model. assuming somax activation used probability vector that training time compared binary embedding ground truth active positions unravel embedding original items understand k-way factorization every item prediction following idea bloom lters maps conrm item denitely output model otherwise relatively large want likelihood item reect that. specically given active position representing compute likelihood table data statistics data cleaning splitting. right data name number instances test split size instance dimensionality median number nonzero components median density c/d. data architecture optimizer meas. feed-forward adam lstm feed-forward rmsprop cade feed-forward adam feed-forward adam feed-forward adam adagrad performance across dierent tasks using dierent evaluation measures reporting relative improvement/loss respect baseline. similarly compare across dierent dimensionalities report ratio embedding dimensionality respect original dimensionality compare across dierent training evaluation times report time ratios respect baseline ti/t. tasks give brief summary considered tasks data sets publicly-available tasks make sure network architecture experimental setup sucient achieve state-of-the-art result. somax outputs categorical cross-entropy losses experiments. movielens movie recommendation movielens data ratings discretized threshold movies less ratings users less movies discarded. inputs outputs built spliing user proles certain timestamp uniformly random ensuring minimum movie input output. perform recommendations build employ -layer feed-forward neural network model rectied linear units hidden layers. optimize weights network using cross-entropy adam million song data song recommendation million song data assume user likes song he/she listened minimum times. remove songs appear less times build user proles minimum songs. inputs outputs split uniformly random timestamp. recommend future listens user -layer feed-forward neural network rectied linear units hidden layers. rest proceed task. evaluate accuracy model map. amazon book reviews book recommendation amazon book reviews data proceed data time seing minimum number ratings book employ -layer feed-forward neural network rectied linear units hidden layers optimize parameters adam. evaluate accuracy model map. book crossing book recommendation book crossing data perform recommendations architecture conguration task time units hidden layers. yoochoose session-based recommendation yoochoose recsys challenge data set. work training challenge keep click events. take million sessions data minimum clicks. predict next click proceed hidasi consider model inner dimensionality train network adagrad using learning rate evaluate accuracy model penn treebank next word prediction penn treebank data vocabulary limited words words mapped ‘unknown’ token consider sentence additional token form input sequences length inspired graves perform next word prediction lstm network inner dimensionality train network sgd. learning rate momentum clip gradients maximum norm evaluate accuracy model correct prediction. cade directory text categorization cade directory brazilian pages data contains around documents assigned categories services education health culture. perform classication -layer feed-forward hp//labrosa.ee.columbia.edu/millionsong/tasteprole hp//jmcauley.ucsd.edu/data/amazon/ hp//www.informatik.uni-freiburg.de/∼cziegler/bx/ hp//recsys.yoochoose.net hp//ana.cachopo.org/datasets-for-single-label-text-categorization neural network somax output. number units input output rectied linear units activations hidden layers. train network rmsprop learning rate exponential decay notice considered task output embeddings required cation text categories). evaluation measure. alternative approaches compare performance state-of-the-art consider dierent embedding alternatives. base evaluation performance measured given input/output compression ratio. important note that general besides performance alternative approaches present desired qualities oers on-the-y operation constanttime supervision network/conguration changes note also methods embedding inputs outputs allowing embedded instances original ones scarce that considered alternatives perform adaptations. hashing trick consider popular hashing trick classier recommender inputs general methodologies focus inputs designed deal type output. nonetheless case binary outputs variants like used ganchev dredze adapted original items using eqs. fact considering adaptation recovery approach seen special case error-correcting output codes originally designed single-class targets ecoc applied class sets corresponding encoding decoding strategies case training neural networks clear loss function used. obvious choice would hamming distance. however pre-analysis hamming loss turned signicantly inferior cross-entropy. erefore laer experiments. construct ecoc matrix randomized hill-climbing method pairwise mutual information recently chollet proposed approach embedding sets image labels dense space real-valued vectors. approach based matrix computed counting pairwise co-occurrences. uses cosine similarity loss function prediction time performs projection individual labels obtain ranking. canonical correlation analysis common learn joint dense real-valued embedding inputs outputs time computed using correlation matrix similarly trick rank items labels prediction time. correlation metric results compression performance start reporting performance function embedding dimension. mentioned facilitate comparisons report relative terms using score ratios si/s dimensionality ratios m/d. ploing former function laer several things worth noting firstly observe that tasks score ratios approach approaches indicates introduction degrade original score baseline embedding dimension comparable original dimension secondly observe lower dimensionality ratio lower score ratio. expected cannot embed sets items intrinsic dimensionality innitesimally small importantly reduction si/s linear maximize curves close corner fig. fullls requirement. general reduce input/output size times still maintain value original score. task exception think abnormally high density data inhibiting embedding dimensions. cade task achieves highest presumably cade task easiest consider input embeddings required. additional observation worth noting interestingly improve scores baseline number tasks. case considered tasks fact embedding performs beer original baseline also observed methods specic data sets instance chollet reported increases using approach so-called data set. here depending task embedding dimension relative increases given data sets observe increases less dense ones hypothesize that case increases come times active positions ground truth output times elements output beer estimation gradient computed focus performance function number projections reporting score ratios si/s above. repeating plots dierent values observe si/s always except approaches almost behavior general si/s jumps remains stable decrease si/s becomes apparent best operating range typically corresponds task exception best operating range around training retrieval time besides performance scores interesting assess whether reduction input output dimensions eect training evaluation times. plot time ratios ti/t function dimensionality ratio regarding training times basically observe linear decrease exception trend cade experiment almost decrease dimensionality ratios general conrm faster training times thanks reduction number parameters model dominated input/output matrices ecting time compute loss function). obtain times speedup times input/output compression roughly lile times speedup times input/output compression. regarding evaluation times also observe linear trend however time ti/t values slightly always overall indicates that compared baseline evaluation time mapping used reconstructing output introduce overwhelming amount extra computation time. exception extra computation time figure time ratios ti/t function dimensionality ratios training time evaluation time alitatively similar plots observed values denotes baseline. comparison alternatives finally compare performance considered alternative methods. establishing dimensionality ratio computing corresponding score ratio si/s given task beer alternative methods tasks beer tasks beer also tasks relevant note that wins always relatively large margin otherwise alternative approach wins generally smaller margin results become relevant realize svd-based approaches introducing separate degree supervised learning task exploiting pairwise item co-occurrences correlations respectively contrast require learning. formulate co-occurrencebased version below achieves moderate performance increments closely approaches performance tasks already performing best. interesting thing note conrm small variation score ratios obtained here score ratios comparable statistical signicance sense co-occurrence-based embedding bloom lters collisions unavoidable lower embedding dimensionality multiple projections addition seen alternative approaches produce embeddings exploiting co-occurrence information here study variant takes advantage co-occurrence information adjust collisions inevitably take place performing embedding. denote approach co-occurrence-based bloom embedding table comparison considered alternatives. score ratios si/s dierent combinations data compression ratio m/d. best results highlighted bold statistical signicance propose quite straightforward approach much extra pre-computation time. training testing times remain same uses pre-computed hashing matrix general idea proposed approach ‘re-direct’ collisions co-occurring items bits positions implementation idea detailed algorithm briey explained below. first count pairwise co-occurrences store sparse matrix next threshold average item frequency using hadamard product componentwise sign function lower triangular part return coordinates format using tuple values indices column indices order cval update hash matrix loop indices sorted values cval increasing order selecting corresponding items draw integers urnd function urnd uniform random integer generator output integer included urnd rows transformed sets union computed finally integers generated urnd pick projections assign updating projections increasing order co-occurrence give priority pairs largest co-occurrence seing collide results overall performance provides moderate increments original approach exception task performance always higher however exception task observe dramatic increases average increases possible explanation moderate performance increases cooccurrence considered data seen typically less possible pairs show co-occurrence. moreover average co-occurrence count co-occurring pairs ratios total number instances order despite moderate average observed increments provided prominent dimensionality ratios m/d. relating best approaches resulting previous comparison table generally beer sometimes statistically significant dierence furthermore based co-occurrences closely approaches tasks performing best even outperforms test point closer cooccurrence-based approaches indication leverages co-occurrence information extent. table comparison versus results table score ratios si/s dierent combinations data compression ratio m/d. best results highlighted bold statistical signicance times. approach compares favorably respect considered alternatives oers number advantages on-the-y operation zero space requirements without introducing changes core network architecture task conguration loss function. future besides continuing exploit co-occurrences could enhance proposed approach considering extensions bloom lters counting bloom lters theory extensions could provide compact representation breaking binary nature embedding. however could require modication loss function mapping process faster mapping process using sorted probabilities could also studied. detailed comparative analysis false positives false negatives also pending. finally would interesting assess utility combination classical collaborative ltering approaches factorization machines table co-occurrence statistics average score increase right data name input percent co-occurrent pairs input average cooccurrence ratio co-occurrent pairs output percent co-occurrent pairs output average co-occurrence ratio co-occurrent pairs average score increases averaging points). co-occurrence values inputs correspond considering training sequences isolated sequence items. conclusion proposed bloom embeddings represent sparse high-dimensional binary-coded inputs outputs. shown compact representation obtained without compromising performance original neural network model cases even increasing substantial factor. compact representation loss function input output layers deal less parameters results faster training kakade langford zhang. multi-label prediction compressed sensing. advances neural information processing systems bengio schuurmans laerty williams culoa vol. y.-d. park choi yang shin. compression deep convolutional neural networks fast power mobile applications. proc. int. conf. learning representations arxiv. kingma adam method stochastic optimization. proc. int. conf. learning representations arxiv. hps//arxiv.org/abs/. langford strehl. vowpal wabbit online learning project. technical report. hp//hunch.net/?p= retrieval. cambridge university press cambridge mcauley pandey leskovec. inferring networks substitutable complementary products. proc. sigkdd int. conf. knowledge discovery data mining tieleman hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning turian ratinov bengio. word representations simple general method semi-supervised learning. proc. annual meeting association computational linguistics vincent br´ebisson bouthilier. ecient exact gradient update training deep networks large sparse targets. advances neural information processing systems weinberger dasgupta aenberg langford smola. feature hashing large scale multitask learning. proc. int. conf. machine learning weston bengio usunier. large scale image annotation learning rank joint word-image embeddings. machine learning weston chapelle elissee sch¨olkopf vapnik. kernel dependency estimation. advances neural information processing systems becker obermayer vol. commun. blustein el-maazawi. bloom lters tutorial analysis survey. technical report. faculty computer science dalhousie university halifax canada. hps//www.cs.dal.ca/research/techreports/cs-- bonomi mitzenmacher panigrahy singh varghese. improved construction counting bloom lters. european symposium algorithms azar erlebach lecture notes computer science vol. springer-verlag berlin germany cardoso-cachopo. improving methods single-label text categorization. ph.d. dissertation. instituto superior tecnico universidade tecnica lisboa. chen wilson tyree weinberger chen. compressing neural networks hashing trick. proc. int. conf. machine learning h.-t. cheng harmsen shaked chandra aradhye anderson corrado chai ispir anil haque hong jain shah. wide deep learning recommender systems. proc. workshop deep learning recommender systems merri¨enboer bahdanau bengio. properties neural machine translation encoder-decoder approaches. proc. workshop syntax semantics structure statistical translation courbariaux bengio j.-p. david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems cortes lawrence sugiyama garne dillinger manolios. bloom lters probabilistic verication. proc. int. conf. formal methods computer-aided design duchi hazan singer. adaptive subgradient methods online learning stochastic optimization. journal machine learning research glorot bordes bengio. deep sparse rectier neural networks. proc. int. conf. articial intelligence statistics grave joulin ciss´e grangier j´egou. ecient somax dally. deep compression compressing deep neural networks pruning trained quantization human coding. proc. int. conf. learning representations arxiv.", "year": 2017}