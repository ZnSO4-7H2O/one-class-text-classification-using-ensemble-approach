{"title": "The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Most current sampling algorithms for high-dimensional distributions are based on MCMC techniques and are approximate in the sense that they are valid only asymptotically. Rejection sampling, on the other hand, produces valid samples, but is unrealistically slow in high-dimension spaces. The OS* algorithm that we propose is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces, and we present first experiments in two different settings: inference in high-order HMMs and in large discrete graphical models.", "text": "current sampling algorithms high-dimensional distributions based mcmc techniques approximate sense valid asymptotically. rejection sampling hand produces valid samples unrealistically slow high-dimension spaces. algorithm propose uniﬁed approach exact optimization sampling based incremental reﬁnements functional upper bound combines ideas adaptive rejection sampling optimization search. show choice reﬁnement done ensures tractability high-dimension spaces present ﬁrst experiments diﬀerent settings inference high-order hmms large discrete graphical models. common algorithms sampling high-dimensional distributions based mcmc techniques approximate sense produce valid samples asymptotically. contrast elementary technique rejection sampling directly produces exact samples applied naively high-dimensional spaces typically requires unacceptable time producing ﬁrst sample. main idea upper-bound complex target distribution simpler proposal distribution dynamic programming method applied order eﬃciently sample maximize case sampling rejection sampling applied constraint integrated tends highly relevant increase acceptance rate algorithm. contrast many constraints constitutive never activated sampling never explores regions would become visible. example anticipating experiments section little point explicitly including -gram constraint certain latent sequence sequence already unlikely bigram level bigram constraints present proposal ensure sequence never explored case optimization treated exactly sampling. formally consists moving assessing proposals terms norm assessing terms norm. typically dynamic programming procedure available sampling also available maximizing main diﬀerence cases criteria selecting among possible reﬁnements. related work heuristic optimization context interesting apparently little known papers discuss technique decoding images based high-order lanmaximizing high-order similar technique; however attempt generalize approach optimization problems amenable dynamic programming discuss connection sampling. order improve acceptance rate rejection sampling lower proposal curve much possible keeping curve. order that authors proposed adaptive rejection sampling where based tance rate. techniques predominantly applied continuous distributions one-dimensional real line convexity assumptions target distribution exploited progressively approximate tighter tighter upper bounds consisting piecewise linear envelopes. also context rejection sampling considers case probabilistic graphical model; introduces heuristically determined order variables model uses order deﬁne sequence exact samplers increasing variables exact sampler ﬁrst variables recursively obtained using preceding exact sampler ﬁrst variables accepting rejecting samples based variable. experiments graphical models section similarities approach cascade exact samplers rather partition space conﬁgurations dynamically based rejects experienced current proposal. function seen unnormalized density normalized density deﬁnes probability distribution called target distribution want sample from. able sample directly target distribution assume easily compute given rejection sampling works follows. deﬁne certain unnormalized proposal density know directly sample dominates probability accept otherwise reject repeat process shown procedure produces exact sample furthermore average rate produces samples acceptance rate equal ﬁrst attempt produces ratio close leads acceptance second attempt produces ratio much lower leading rejection. although rejection gained useful information namely much lower going evidence deﬁne proposal better generic following. suppose provided small ﬁnite one-step reﬁnement actions depending able move everywhere also select among moves norm minimal among possible minimal idea that improve acceptance rate much possible explore directly large space possible reﬁnements moving representation slightly complex representation rather much complex representation could result exploring larger space possible reﬁnements intuition behind onestep reﬁnement actions become clearer consider concrete examples later paper. particular even could reﬁnement would exactly coincide therefore would smallest possible norm might want reﬁnement involved overly complex representation simple central observation following one. suppose distance smaller distance also smaller checked immediately ﬁgure fact hand higher hand fortiori coordinate observe conclude approach sampling case diﬀerence one-step reﬁnement option selected chosen basis much decreases norm where reminder also notated using norm notation. selected maximum process repeated diﬀerence smaller certain predeﬁned threshold. sampling optimization usually seen completely distinct tasks actually viewed extremities continuous range considered context spaces measure space real-valued function standard notion sampling relative however introduce following generalization notation instead order avoid confusion denoting target distribution. performing sampling non-negative function relative distribution case sampling relative performing informally sampling relative tends sampling tending sense large sampled relative tends close maximum attempt give precise formal meaning observation here note connection idea simulated annealing view mcmc metropolis-hastings sampling technique idea sampling spaces larger larger α’s. sense natural criterion means directly lowering norm controls eﬃciency acceptance rate smaller however using criterion require computation norm possible one-step reﬁnements costly entry algorithm assume either sample mode optimization mode also starting proposal dominates sample optimize directly. terminology os-sample represent either proposal optimizing depending case. line refers history sampling namely trials done marked acceptance rejection stopping criterion stop stop sampling mode number acceptances relative number trials larger certain predeﬁned threshold case return line ﬁrst list accepted already valid sample second last reﬁnement used produce number future samples desired acceptance ratio similar observed far; optimization mode last element history accept case return line ﬁrst value mode accepted trial history second last reﬁnement line compute ratio line decide accept based ratio; optimization mode accept ratio close enough determined threshold; sampling mode accept based bernoulli trial probability rithm interesting right. ﬁrst focus sampling suppose represents initial proposal density upper-bounds target density start sampling ﬁrst reject somewhere split disjoint subsets obtaining partition using precise context provided partition able improve upper bound whole tighter upper bounds resulting upper bound whole sample using experience later time another reject point point partition subsets tighten bounds subsets obtain reﬁned proposal iterate process building hierarchical partition reach certain acceptance-rate threshold. proposed present analogy technique used illustrated fig. start constant optimistic bound corresponding objective function computed root search tree assume binary. expand daughters root re-evaluate optimistic bounds constants obtaining piecewise constant proposal move daughter highest bound. continue expanding step leaf partial search tree highest optimistic bound optimization/decoding want argmax sampling sample note state space associated huge used optimization mode fact strictly general reasons assume piecewise reﬁnement strategy namely reﬁnements follow hierarchical partition space given reﬁnement limited leaf current partition even stategy followed assume piecewise upper-bounds constant. points become clearer experiments section including higher-order n-gram impact several regions simultaneously possibly overlapping complex ways regions touched previous reﬁnements; addition impact single n-gram non-constant even regions touches depends multiplicity n-gram presence absence. need represent explicitly contexts case trigram model even contexts higher-order models. deﬁne maxxi− along maxxi− maxima taken possible context words vocabulary. quantities precomputed eﬃciently seen optimistic max-backoﬀs trigram forgotten part context. initial proposal clearly sequence words state space much less costly represent proposals incorporate n-grams variable orders represented eﬃciently wfsas fig. show wfsa representing initial proposal corresponding example four observations take acoustic realizations words ‘the dogs barked’. weights edges correspond unigram max-backoﬀs thus state corresponds nullcontext. wfsa optimization sampling done eﬃciently standard dynamic programming techniques backward ﬁltering-forward sampling forward weights associated states computed similarly either max-product sum-product semiring. consider ﬁrst sampling suppose ﬁrst sample produces barked marked bold edges drawing. computing ratio gives result much smaller part viewpoint full model trigram context two. perform sampling trial time tends avoid producing context two; experience reject later sequence reﬁne again meaning identify n-gram which extend context accounts signiﬁcant part stop reﬁnement process start observing acceptance rates certain ﬁxed threshold. case optimization similar. suppose maximum barked observe lower reject reﬁne stop process point value maximum equal value implies found maximum english side europarl corpus training test data -gram language model trained using srilm sentences. remaining randomly select sequences lengths obtain sequences remove ones containing numbers obtaining test size optimization limit average number latent tokens decoding experiments plot fig. show number iterations diﬀerent n-gram models size take exact decoding test-set. ﬁxed sentence length decoding larger n-gram models leads sub-linear increase w.r.t. number iterations taken. demonstrate reduced nature q-automaton show table distribution n-grams ﬁnal model speciﬁc input sentence length total number n-grams full model would sampling sampling experiments limit number latent tokens reﬁne automaton reach certain ﬁxed cumulative acceptance rate also compute rate based last trials tends better reﬂect current acceptance rate. plot bottom fig. show single sampling using -gram model example input cumulative accepts took iterations ﬁrst successful sample noted trade-oﬀ time needed compute forward probability weights needed sampling time takes adapt variable-order hmm. resolve this batch-updates making trials q-automaton updating model step. this noted signiﬁcant speed-ups sampling times. empirically found good value. plot show average iterations models reﬁnements ﬁnished diﬀerent orders diﬀerent lengths. note sub-linear increase number trials moving higher length= average number trials deﬁnes undirected graph nodes edges unary potential functions denoted binary potentials since integrating sampling done eﬃciently trees ﬁrst determine spanning tree graph denote φmax φmin potential edge deﬁne choice tree produces upper-bound advantageous choose close possible heuristically using prim’s algorithm selecting maximum spanning tree graph weights bound φmax tree. edge incident edge absorbed neighbors implies maximum φmax replaced exact value necessarily lower implicitly reﬁning conditioning respect possible values obtain diﬀerent graphs node removed words partitioned event space subspaces; deﬁne subspace proposal equation lower restriction xik. deﬁne reﬁnement whole taken null xik. scheme seen instance piecewise bounds. repeat process iteratively based observed rejections subspaces obtain hierarchical partition ﬁne-grained regions others. reﬁnements obtained form reduced graphs introduced evidences given conditioned variables. cardinality hierarchical partition grow exponentially monitor acceptance rate/computation time ratio below. follows consider exact samising model experiment pling problem estimation graphical models thoroughly investigated past decade interesting question understand trade-oﬀ between improving acceptance rate incurring cost performing reﬁnement. priori possible policy could ﬁrst reﬁne proposals certain point sample required number samples. experiments show however reasons interleave sampling reﬁning ﬁrst observing rejected samples helps choose reﬁnements argued previously; second criterion stopping reﬁnement process computation criterion requires estimate current acceptance rate estimated samples. consider ising model binary variables uniform grid unary potentials binary coupling strengths drawn according centered normal distribution standard deviation note that certain cases exact sampling ising models done polynomial time using elegant mcmc approach called coupling past based fact properly coupled markov chains follow exactly target distribution time coaelesce. however applications approach rely certain strong assumptions potentials typically either attractive repulsive sample models random positive negative coupling strengths making problem much harder. interesting extension work would algorithms proposal distributions general models. reﬁnements policy function takes input current proposal returns pair. ﬁrst policies addition rejected sample input last deterministic random split region rejected sample means observation rejected reﬁne subspace contains sample conditioning respect remaining variables selected random; highest bound improvement rejected sample index conditioning variable selected reﬁning leads largest decrease value similar reﬁnement strategy used experiments; probable region reﬁnes variable probable subspace piecewise bound highest acceptance rate ambitious greedy policy largest reduction total mass proposal identiﬁed among possible choices implemented eﬃciently maintaining priority queue containing possible triples acceptance rates obtained following policies compared figure running reﬁnements policies reﬁnements policy reﬁnement policy results conﬁrm best reﬁnement obtained using deterministic policy policy based rejected samples reaches acceptance rates using twice many reﬁnements. policies naive reach signiﬁcant acceptance rate even iterations. based results might conclude policy best. however case computation time taken account. note estimators based samples accepted not. hence decide stop reﬁning expected time obtain additional exact samples target distribution approximately samp samp average time obtain trial distribution total time spent computing reﬁnements current reﬁnement obtain estimate expected total time obtain samples quantity computed sample plotted acceptance rate figure policy expected computation time starts decrease acceptance rate increases; regime reﬁnement time small compared time reduction higher acceptance rate. large values acceptance rate reﬁnement time negligeable leading increase total computation time. highest acceptance rate policy despite good acceptance rate ﬁxed number reﬁnements requires time total alternative reﬁnements policies. diﬀerence striking look minimum curve least times faster policy based reﬁnement rule applied rejected sample. experiment conﬁrms beneﬁt using rejected samples adaptively choosing reﬁnements spot regions space important reﬁne much faster computing best possible alternative reﬁnement. paper proposed uniﬁed viewpoint rejection sampling heuristic optimization using functional upper-bounds both. sampling upper-bounds reﬁned decreasing integral optimization reﬁned decreasing maximum depending problem several classes upper bounds used. showed variable-order max-backoﬀ bounds n-gram probabilities gave state-of-the performances exact decoding high-order hmms. many practical problems simpler piecewise bounds derived illustrated example sampling decoding large tree-width graphical model. interesting property proposed approach adaptive nature algorithm rejected sample used quickly choose eﬀective reﬁnement approach computational attractive compared computation norm potential one-step reﬁnements. case graphical model sampling showed lead speedup factor order magnitude. results presented paper motivate research development domain-speciﬁc functional bounds. important extension derive bounds agreement-based models corresponding product simple models typical example corresponds agreement probabilistic context-free grammar order take account syntactic low-level n-gram structures language rush another research direction improve models based piecewise bounds quality limited since reﬁnement always local i.e. improve bound single element partition. example graphical model sampling condition bound could improved could reﬁne jointly analogous done context hmms incorporation higher-order n-gram proposal global impact whole event space.", "year": 2012}