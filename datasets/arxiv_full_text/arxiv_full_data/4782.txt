{"title": "A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic  Grouping", "tag": ["cs.LG", "cs.AI"], "abstract": "In this study, a spectral graph-theoretic grouping strategy for weakly supervised classification is introduced, where a limited number of labelled samples and a larger set of unlabelled samples are used to construct a larger annotated training set composed of strongly labelled and weakly labelled samples. The inherent relationship between the set of strongly labelled samples and the set of unlabelled samples is established via spectral grouping, with the unlabelled samples subsequently weakly annotated based on the strongly labelled samples within the associated spectral groups. A number of similarity graph models for spectral grouping, including two new similarity graph models introduced in this study, are explored to investigate their performance in the context of weakly supervised classification in handling different types of data. Experimental results using benchmark datasets as well as real EMG datasets demonstrate that the proposed approach to weakly supervised classification can provide noticeable improvements in classification performance, and that the proposed similarity graph models can lead to ultimate learning results that are either better than or on a par with existing similarity graph models in the context of spectral grouping for weakly supervised classification.", "text": "study spectral graph-theoretic grouping strategy weakly supervised classiﬁcation introduced limited number labelled samples larger unlabelled samples used construct larger annotated training composed strongly labelled weakly labelled samples. inherent relationship strongly labelled samples unlabelled samples established spectral grouping unlabelled samples subsequently weakly annotated based strongly labelled samples within associated spectral groups. number similarity graph models spectral grouping including similarity graph models introduced study explored investigate performance context weakly supervised classiﬁcation handling diﬀerent types data. experimental results using benchmark datasets well real datasets demonstrate proposed approach weakly supervised classiﬁcation provide noticeable improvements classiﬁcation performance proposed similarity graph models lead ultimate learning results either better existing similarity graph models context spectral grouping weakly supervised classiﬁcation. weakly supervised learning paradigms learner receives training consisting limited amount labelled data well fairly larger amount unlabelled data. without loss generality assume dataset consisting bags instances binary labels assigned based existence speciﬁc object interest. consists instances. bags well instances labelled positive negative. positive label means corresponding contains object whereas negative label signiﬁes contain object. labels provided bags only. labelled negative fully labelled instances deﬁnitely represent object interest therefore instances parts labelled. positive-labelled considered fully labelled instances representing object interest indicated bag. positivelabelled information provided instances represent object words instance labels treated latent variables. datasets containing latter kind represent example datasets handled weakly supervised learning paradigms. instances positive-labelled bags unlabelled performance classiﬁer undoubtedly optimal. weakly supervised learning approach introduced study weakly annotate unlabelled instances performance classiﬁer applied subsequently data improved. weakly supervised learning paradigms recently gaining signiﬁcance machine learning increasing importance learning unlabelled data. obtaining fully labelled data apparently useful error-prone importantly expensive obtain weakly supervised learning paradigms crucial. real-world challenge lends well application weakly supervised learning paradigms datasets bags-of-instances form problem muscle classiﬁcation based electromyographic signals. here datasets consist sets electromyographic signals represent particular muscle signal comprising signal contributions diﬀerent components make muscle. component muscle referred motor unit contribution component referred motor unit potential train label three; normal myopathic neurogenic. three labels used label muscle. training dataset feature values component muscle i.e. labels available muscle whole. three labels muscle myopathic neurogenic contains normal myopathic neurogenic components respectively. hand normal muscle components normal. seen multiclass version bags-of-instances example described earlier. rather labels; containing fully labelled instances another containing unlabelled instances three labels containing fully labelled instances containing unlabelled instances. instances normal muscles fully labelled labelled normal instances myopathic neurogenic muscles unlabelled known disordered normal. therefore normal label muscles equivalent negative label bags myopathic neurogenic labels muscles equivalent positive label bags. numerous examples learning algorithms unlabelled weakly labelled data utilised also bergamo torresani provide another example exploit weakly annotated images build weakly supervised object classiﬁer. addition object recognition images weakly supervised learning applications computer vision. example prest perform learning based weakly labelled videos fully labelled images order detect objects videos. examples weakly supervised learning algorithms applied videos include leistner multiple-instance learning algorithms closely related weakly supervised data. training instances grouped together bags. label associated instance belongs bag. instance label associated diﬀerent label belongs instance labels observed. galleguillos train discriminative classiﬁer weakly annotated image data. andrews represents another example algorithm applied weakly annotated data blaschko vezhnevets buhmann vijayanarasimhan grauman examples mil-based approaches applied weakly labelled images purpose object recognition. main objective study turn training dataset consisting limited number labelled instances larger number unlabelled instances larger annotated training dataset consisting weakly labelled instances unlabelled strongly labelled instances sake improving classiﬁcation performance. achieved proposed weakly supervised learning approach. unlabelled data weakly annotated applying spectral graph-theoretic grouping strategy makes strongly labelled instances well similarity among instances order assign weak labels unlabelled instances. spectral graph-theoretic grouping based similarity graph models. addition similarity graph models literature similarity graph models introduced study. weakly labelled strongly labelled instances form larger annotated training dataset. utilizing proposed spectral grouping strategy facilitate weakly supervised learning obtain larger annotated training dataset composed strongly weakly labelled data consequently lead improved classiﬁcation performance compared case strongly labelled data strongly labelled data unlabelled data used. study serves methodological guide weakly supervised learning paradigms limited aforementioned muscle classiﬁcation example. addition part proposed weakly supervised learning paradigm performance similarity graph models tested separately three benchmark datasets abalone swiss banknotes segmentation. abalone dataset consists abalone’s physical measurements required predict abalone’s measurements. features swiss banknotes dataset explanatory variables describing characteristics swiss banknotes goal decide whether banknote genuine segmentation dataset contains images representing seven outdoor objects i.e. grass path window cement foliage brickface main contribution study introduction weakly supervised learning approach based spectral graph-theoretic grouping. approach mainly targets datasets bags-of-instances form like shown figure instances belonging green labelled bags labelled instances belonging blue bags unlabelled. instances latter labels instances grouped together spectral graph. speciﬁcally instances blue labelled bags grouped together similarity graph graph figure whereas graph figure contains instances labelled bags. spectral graph-theoretic grouping performed ﬁrst constructing similarity graph models performing spectral grouping step similarity graph models proposed. using groups resulting spectral grouping along strongly labelled instances associated within spectral groups unlabelled instances weakly annotated result figure assumption across blue bags total number green instances less total number blue instances. thus group greater cardinality assigned blue label. premise applying eﬃcient grouping strategy nodes similarity graph weakly annotate involved instances. construct larger annotated training set. finally weakly supervised classiﬁer exploits whole dataset consisting strongly labelled data weakly labelled data study introduce weakly supervised learning approach also investigate devise number similarity graph models study eﬀect ability obtain reliable weak annotations unlabelled instances. graph-theoretic grouping studied literature number diﬀerent applications. fowlkes used spectral graph-theoretic grouping image segmentation application. developed spectral groups based using small number samples extrapolating computational requirements reduced. aksoy haralick developed graph-theoretic grouping algorithm used image grouping. grouped images based observation visually similar images also similar feature space similar feature vectors. leahy represent example graph-theoretic grouping algorithm develop algorithm image segmentation. performed grouping building undirected graph using data instances forming mutually exclusive subgraphs gradually removing arcs according certain criterion. here make graph-theoretic grouping completely diﬀerent purpose improved weakly annotation unlabelled data weakly supervised learning. strategy weakly annotating unlabelled data apply grouping technique part unlabelled data related group turn assigned certain label depending inherent relationships strongly labelled part data unlabelled part. fact spectral grouping acts unlabelled part data introduced spectral graph-theoretic grouping strategies applicable fully unlabelled datasets practically unlabelled part data long grouping issues handled problem assumptions. datasets instances belonging disordered type known either normal disorder. therefore groups. benchmark datasets used number groups known. preliminary phase study spectral grouping well grouping algorithms applied data. normalised spectral graph-theoretic grouping according malik performed slightly better spectral graph-theoretic grouping algorithms turn performed better grouping algorithms. however improvement provided malik’s normalised spectral graph-theoretic grouping considerable. careful inspection reason malik’s normalised spectral graph-theoretic grouping perform better does turned fact similarity graph models used spectral clustering literature capture well pairwise similarities instances. worth noting main normalised spectral graph-theoretic grouping algorithms; according malik according sake simplicity former shortly referred study normalised spectral graph-theoretic grouping unless stated otherwise. ﬁrst step proposed weakly supervised learning approach perform spectral graph-theoretic grouping unlabelled part dataset. spectral graph-theoretic grouping turn begins forming similarity graph model unlabelled data. literature several popular similarity graph models transform given data instances pairwise similarities pairwise distances graph. constructing similarity graph model goal model local neighbourhood relationships data instances. following list main similarity graph models literature. \u0001-neighbourhood graph instances pairwise distances among less connected instances pairwise distances greater equal not. weights considered scale distances; therefore \u0001-neighbourhood graph unweighted. k-nearest neighbour graph instance connected instance among k-nearest neighbours neighbourhood relationship symmetric resulting graph directed graph. therefore graph transformed undirected graph. transforming undirected graph connecting instances among neighbours among neighbours resulting undirected graph referred k-nearest neighbour graph another connect instances among neighbours among neighbours resulting undirected graph case referred mutual k-nearest neighbour graph. building similarity graph model cases edges graph weighted measuring similarity respective vertices fully connected graph instances connected another; words instances considered similar another. edges weighted sij. graph useful local neighbourhoods modelled similarity function fully connected graph represent local neighbourhood function. parameter gaussian similarity function controls width neighbourhoods. parameter acts like parameter construction \u0001-neighbourhood graph datasets provide examples datasets neither \u0001-neighbourhood graph k-nearest neighbour graph fully connected graph capture properly similarities data instances especially diﬀerent densities within dataset. address issue propose similarity graph models aimed providing greater robustness handling diﬀerent data densities within dataset. referred probabilistic threshold graph probabilistic criterion graph. main advantages proposed probabilistic threshold probabilistic criterion graphs problem dealing instances diﬀerent scales. means unlike \u0001-neighbourhood graph connect instances belonging scale dataset diﬀerent scales unlike k-nearest neighbour graph which case would connect instances diﬀerent scales proposed similarity graph models connect instances within regions constant density data diﬀerent scales. mutual k-nearest neighbour graph times diﬀerent scales setting parameter case usually problem because ﬁrst ﬁnding optimal value certain dataset tricky second importantly dataset optimal value data scales another part dataset diﬀerent optimal value another part dataset. proposed probabilistic threshold graph parameter used threshold similarity values. similarity values greater equal kept similarity values smaller assigned using truncated gaussian distribution mean standard deviation another parameter used decide ﬁnal similarity values illustrated probabilistic thresholding similarity graph model section. shown equations initial values similarity compared normalised based summation distances certain instance. leads fact thresholding applied relative data depend absolute values case \u0001-neighbourhood graph k-nearest neighbour graph. nonetheless still provide hard thresholding similarity values therefore parameters aﬀect similarity values great deal. order alleviate eﬀect hard thresholding similarity graph model based probabilistic acceptance criterion proposed. illustrated similarity values greater equal kept rest similarity values assigned using truncated gaussian distribution mean standard deviation smoothing parameter controls normalised similarity values tuning ratio given distance. limit distances assigned equal weight acts k-nearest neighbour similarity graph model. hand larger absolute value using −−−− highest classiﬁcation accuracy always obtained used represent respective edge weight similarity graph model. interval values used decide ﬁnal value follows. weight value generated smaller certain small threshold value respective similarity value otherwise similarity value generated weight. summary deﬁne deﬁned equation then similarity values greater equal taken similarity values smaller interval similarity values assigned shown equation similarity graph models constructed probabilistic based comprehensive cross-validation optimal values obtained. parameter controls width neighbourhoods instances farther limit neighbourhoods assigned weight value smaller value sparse similarity graph model. distances corresponding initial similarity values calculated thresholding. similarity values greater equal kept truncated gaussian distribution mean standard deviation utilised follows order calculate similarity values smaller weight values resulting accepted neighbourhood probability based generated weight therefore stochastic acceptance criterion require threshold provided. similarity values greater probability based weight generated otherwise displayed equation similarity graph models constructed probabilistic acceptance criterion referred probabilistic criterion graphs. neighbourhood relationships proposed similarity graph models turned symmetric neighbourhoods fashion similar k-nearest neighbour graph; either assigning maximum value similarity similarity taking minimum value values updated symmetric similarity value. examples advantages proposed probabilistic threshold probabilistic criterion graphs clear usually relate groups irregular shapes. example figure shows dataset representing pattern takes place quite often datasets well datasets irregular groups data. matlab developed hein luxburg tailored order show ﬁgures used throughout illustrative example. always values greater latter. figure value resulted leave-one-out cross-validation small dataset leads groups loosely never connects instances belonging correct group bigger values like figure overconnects instances belonging diﬀerent correct groups. figure shows similarity graph models symmetric k-nearest neighbour graph used values equal number groups always values greater value made symmetric k-nearest neighbour graph correct groups. value resulting leave-one-out cross-validation connects instances belonging diﬀerent correct groups. figure shows similarity graph models mutual k-nearest neighbour graph used values equal number groups always values greater value made mutual k-nearest neighbour graph correct groups. value resulting leave-one-out cross-validation even better \u0001-neighbourhood graph symmetric k-nearest neighbour graph disconnected components right side figure connected group because figure belong correct group. goes group right side along middle figure figure shows similarity graph model resulting probabilistic threshold graph used value weight threshold equal value resulting applying leave-one-out cross-validation illustrative dataset. probabilistic threshold graph similarity graph model leads correct groups values compared normalised values representing distance certain instance another divided summation distances former instances dataset. normalization leads similarity graph model depends absolute values parameters also heavily impacted relative weights distance certain instance another taken consideration relative distances former instance others. formal notations general form grouping graph grouping presented followed three equations presenting main graph laplacian matrices literature. grouping input learner receives i.i.d. instances instance features. even always case let’s assume another number given representing number groups. line study. grouping output learner required return partition instances disjoint subsets good partitioning minimise pairwise distances among instances subset maximise graph-theoretic grouping learner required return partition graph disjoint subsets groups vertices edges vertices diﬀerent groups weights possible edges vertices within group weights high possible calculate normalised laplacian lnor d−l. calculate ﬁrst eigenvectors lnor. number groups. study rn×k matrix whose columns group subsets using k-means. proposed weakly supervised learning approach mainly consists spectral graph-theoretic grouping strategy turn based similarity graph models subsequent weak classiﬁer. therefore investigating performance similarity graph models crucial performance evaluation weakly supervised learning approach. datasets used evaluation proposed weakly supervised learning approach whole real-world datasets whilst datasets used separate evaluation introduced similarity graph models three benchmark datasets abalone swiss banknotes segmentation. ground truth labelling available unlabelled instances datasets. main purpose weakly annotating unlabelled instances improve performance subsequent weak classiﬁer. means accuracy weak classiﬁer main metric measuring quality weak annotation. still present results internal evaluation metric grouping order demonstrate quality grouping strategy generic sense. davies-bouldin index used internal evaluation measure datasets. centroid group average distance instances group centroid distance centroids numerator expresses compactness groups grouping result denominator expresses separation among groups smaller value davies-bouldin index better corresponding grouping. values davies-bouldin index depend number groupings grouping algorithm another advantage davies-bouldin index better time complexity internal evaluation measures grouping benchmark grouping datasets used experiments study ground truth labels available. ground truth labels never used learning process means. score used external evaluation measure datasets ground truth labels available. score grouping external evaluation measure weights recall parameter precision results divided spectral graph-theoretic results based similarity graph models weakly supervised classiﬁcation results. former represent content analysis similarity graph models section provides analysis similarity graph models proposed study. second part results compares weakly supervised classiﬁers corresponding fully supervised classiﬁer regarding datasets parts used every dataset similarity graph models represent unlabelled parts. outcome spectral graph-theoretic grouping consists groups. labelled part every dataset used annotate groups group smaller number elements assigned label labelled instances group elements assigned disordered label. assumption based structure disordered muscle typically contains disordered normal mus. elements annotated spectral grouping represent weakly labelled data later used annotated training data classiﬁcation. three datasets ordinary spectral grouping problem. grouping applied data labelled part data. spectral grouping evaluated section impact classiﬁcation performance evaluated weakly supervised classiﬁer fully supervised classiﬁer section. part spectral graph-theoretic grouping evaluation proposed probabilistic threshold probabilistic criterion similarity graph models evaluated based grouping evaluation measures comparing similarity graph models existent literature. abalone dataset instances features groups. swiss banknotes dataset instances features groups. segmentation dataset instances features groups. myopathic upper dataset instances features groups. neurogenic upper dataset instances features groups. myopathic lower dataset instances features groups. neurogenic lower dataset instances features groups. \u0001-neighbourhood optimal value obtained cross-validation. k-nearest neighbour optimal value obtained cross-validation. mutual k-nearest neighbour optimal value obtained cross-validation. fully connected graph optimal value obtained cross-validation. ﬁrst datasets publicly available datasets used grouping before. latter datasets represent real datasets. datasets acquired upper lower recordings contains types instances; normal well disordered fact rather datasets myopathic neurogenic upper datasets represent bags dataset shown diﬀerent datasets treated separately spectral graph-theoretic grouping evaluation concerned. weakly supervised classiﬁer fully supervised classiﬁer section weakly supervised classiﬁcation applied weakly annotated data upper datasets processed together along strongly labelled instances upper dataset. data collected approval de-identiﬁed. mentioned earlier score greater value accurate similarity graph model daviesbouldin index smaller value better similarity graph model. seen table figure figure proposed probabilistic similarity graph models grouping results better least good similarity graph models. figure shows improvement achieved using four proposed similarity graph models similarity graph models comparison davies-bouldin index values clearly better former. conclusion shown case segmentation dataset figure abalone dataset figure probabilistic threshold maximum graph leads best result score value slightly better achieved constructing probabilistic criterion maximum graph well k-nearest neighbour graph. graphs nearly equally good banknotes dataset displayed figure advantage proposed similarity graph models lies fact depend distance among instances location instances number neighbours speciﬁed priori perform well part dataset another part dataset example dataset containing diﬀerent densities within results show probabilistic threshold probabilistic criterion graphs lead similar grouping results among shown values validity indices. former leads better results case abalone dataset minimum graph segmentation dataset latter leads slightly better results case maximum graph segmentation dataset. rest datasets results quite similar among proposed similarity graph models. hand datasets minimum similarity graph models lead better results maximum similarity graph models vice versa datasets. suggests choice minimum maximum similarity graph models model depend value group validity index resulting cross-validation. annotated training data larger addition weakly labelled instances annotated spectral grouping want evaluate signiﬁcance weak labelling procedure respect classiﬁcation performance datasets. results classiﬁcation evaluated weak labelling i.e. fully supervised classiﬁer weak classiﬁer uses weakly labelled data resulting spectral grouping along strongly labelled data. k-nearest neighbour k-nearest neighbour classiﬁers usually perform well cases decision boundary complex enough data train. here value obtained cross-validation. quadratic discriminant analysis assumes gaussian distribution label assigns instance label greater posterior probability. parameters gaussian distribution class estimated training data maximum likelihood estimate posterior probability refers assigned label obtained argmaxi fully supervised classiﬁer unlabelled instances dealt follows. instance belonging myopathic neurogenic assumed myopathic neurogenic respectively. assumption accurate myopathic neurogenic bags contain normal instances. therefore easy performance fully supervised classiﬁer would severely suﬀer assumption demonstrated results. hand weak classiﬁer exploits weak labels assigned previously unlabelled instances spectral grouping procedure. upper dataset labelled instances unlabelled instances. fully supervised classiﬁer assigns respective label unlabelled instance. hand weakly supervised classiﬁer processes weakly labelled instances well strongly labelled instances. lower dataset labelled instances unlabelled instances. therefore weakly supervised classiﬁer processes weakly labelled instances well strongly labelled instances. leave-one-out cross-validation implemented setting instances belonging single muscle test data training instances rest muscles repeating process every muscle. precisely referred leave-one-muscle-out cross-validation study concerned. overall muscle classiﬁcation accuracy main metric used evaluate classiﬁcation performance. table shows results fully supervised weakly supervised classiﬁers. every weak classiﬁer named spectral graph-theoretic grouping strategy pursued three diﬀerent classiﬁcation algorithms utilised each. results show that using weak classiﬁer muscle classiﬁcation accuracy signiﬁcantly improves compared fully supervised classiﬁer. logistic regression performs slightly better compared k-nearest neighbours diﬀerence huge. shows training instance annotation process properly performed classiﬁcation results stable algorithm-dependent. weakly supervised learning paradigm introduced. goal improve classiﬁcation performance ﬁrst weakly annotating unlabelled samples training dataset using spectral graph-theoretic grouping strategy using weakly annotated data along strongly labelled data construct larger annotated training used classiﬁcation. spectral graph-theoretic grouping exploits similarity among data instances well relationship unlabelled strongly labelled data instances constructing similarity graph models weakly annotate unlabelled data instances. similarity graph models provide greater robustness handling diﬀerent data densities within dataset introduced. afterwards classiﬁer learns weakly well strongly labelled data. results show performance resulting weakly supervised classiﬁer whole better counterpart fully supervised classiﬁer datasets. also results experiments performed benchmark datasets show spectral graph-theoretic grouping strategy based introduced similarity graph models leads grouping results better similarity graph models literature. proposed spectral graph-theoretic grouping strategy weakly supervised learning provided improved performance compared fully supervised learning counterpart primarily fact approach obtain reliable weakly labelled data that augmented strongly labeled data form larger annotated training dataset used train classiﬁer stronger classiﬁcation performance achieved using strongly labelled data strongly labelled data unlabelled data. furthermore proposed similarity graph models improved results primarily ﬂexibility models adapt underlying data compared existing graph models require strict require rigid parameter optimization.", "year": 2015}