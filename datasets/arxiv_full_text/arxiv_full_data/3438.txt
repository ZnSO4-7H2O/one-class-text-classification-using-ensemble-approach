{"title": "Close Yet Distinctive Domain Adaptation", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Domain adaptation is transfer learning which aims to generalize a learning model across training and testing data with different distributions. Most previous research tackle this problem in seeking a shared feature representation between source and target domains while reducing the mismatch of their data distributions. In this paper, we propose a close yet discriminative domain adaptation method, namely CDDA, which generates a latent feature representation with two interesting properties. First, the discrepancy between the source and target domain, measured in terms of both marginal and conditional probability distribution via Maximum Mean Discrepancy is minimized so as to attract two domains close to each other. More importantly, we also design a repulsive force term, which maximizes the distances between each label dependent sub-domain to all others so as to drag different class dependent sub-domains far away from each other and thereby increase the discriminative power of the adapted domain. Moreover, given the fact that the underlying data manifold could have complex geometric structure, we further propose the constraints of label smoothness and geometric structure consistency for label propagation. Extensive experiments are conducted on 36 cross-domain image classification tasks over four public datasets. The comprehensive results show that the proposed method consistently outperforms the state-of-the-art methods with significant margins.", "text": "domain adaptation transfer learning aims generalize learning model across training testing data different distributions. previous research tackle problem seeking shared feature representation source target domains reducing mismatch data distributions. paper propose close discriminative domain adaptation method namely cdda generates latent feature representation interesting properties. first discrepancy source target domain measured terms marginal conditional probability distribution maximum mean discrepancy minimized attract domains close other. importantly also design repulsive force term maximizes distances label dependent sub-domain others drag different class dependent sub-domains away thereby increase discriminative power adapted domain. moreover given fact underlying data manifold could complex geometric structure further propose constraints label smoothness geometric structure consistency label propagation. extensive experiments conducted cross-domain image classiﬁcation tasks four public datasets. comprehensive results show proposed method consistently outperforms state-of-the-art methods signiﬁcant margins. thanks deep networks recent years witnessed impressive progress increasing number machine learning computer vision tasks e.g. image classiﬁcation object detection semantic segmentation however impressive progress made possible massive amount labeled training data available requirement hampers adoption number real-life applications labeled training data don’t exist enough quantity. hand manual annotation large training data could extremely tedious prohibitive given application. interesting solution problem transfer learning domain adaptation aims leverage abundant existing labeled data different related domain generalize predictive model learned source domain unlabeled target data despite discrepancy source target data distributions. core idea proposed methods domain adaptation reduce discrepancy domains learn domain-invariant predictive model data. state featured mainstream algorithms reducing data distribution discrepancy feature representation transfer aims good feature representations minimize domain differences error classiﬁcation regression models; instance transfer attempts re-weight good data source domain useful figure illustration major difference proposed method previous state-of-the-art geometrical shape round triangle square represents samples different class labels. cloud colored blue represents source target domain respectively. latent shared feature space represented ellipse. green ellipse illustrates latent feature space obtained previous approaches whereas purple illustrates novel latent shared feature space proposed method. upper part ellipses represents marginal distribution lower part denotes conditional distribution. seen marginal distribution lower part fig. samples label clustered together samples different labels thus different sub-domains separated. contrast conditional distribution lower part fig. samples different labels completely mixed thus making harder discrimination samples different labels. paper interested feature representation transfer seeks domain invariant latent space preserving time important structure original data e.g. data variance geometry. early methods e.g. propose structural correspondence learning ﬁrst deﬁnes pivot features identiﬁes correspondences among features different domains modeling correlations pivot features. later transfer learning problems learns proached dimensionality reduction. novel feature representation across domains reproducing kernel hilbert space maximum mean discrepancy measure so-called transfer component analysis extension purpose reduce computational burden. goes step remarks marginal conditional distribution could different source target domains. result joint distribution adaptation proposed jointly minimize mismatches marginal conditional probability distributions. previous research thus focused matching marginal and/or conditional distributions transfer learning ignoring discriminative properties reinforced different classes adapted domain. paper propose extract latent shared feature space underlying domains discrepancy domains reduced importantly original discriminative information classes simultaneously reinforced. speciﬁcally seek shared feature space minimizing discrepancy marginal conditional probability distributions also introduce discriminative model called subsequently repulsive force light fisher’s linear discriminant analysis repulsive force drags sub-domains different labels away maximizing distances measured terms maximum mean discrepancy thereby making discriminative data different sub-domains. clear contrast previous approaches illustrated fig.. previous works e.g.jda seek align marginal conditional distributions source target domain resultant latent subspace therefore falls short terms discrimination power illustrated lower part green ellipse fig. samples different labels mixed contrast seen lower part purple ellipse fig. proposed method uniﬁes decrease data distribution discrepancy increase discriminative property classes framework ﬁnds novel latent subspace samples label close samples different labels well separated. moreover given fact manifold source target data shared latent feature space could complex geometric structure propose label propagation based respect constraints namely label smoothness consistency geometric structure consistency prediction target data labels. good label propagation well preserve label information change much shared data manifold unlike number domain adaptation methods e.g. nearest neighbor euclidean distance predict labels target domain prediction proposed model deduced label propagation respect underlying data manifold geometric structure. extensive experiments conducted comprehensive datasets verify effectiveness proposed method outperforms state-of-the-art domain adaptation algorithms signiﬁcant margin. rest paper organized follows. section discuss previous works related highlight differences. section ﬁrst describe problem preliminaries domain adaptation present proposed method. experiment results discussions presented section ﬁnally draw conclusion section machine learning domain adaptation transfer learning aims learn effective predictive model target domain without labeled data leveraging abundant existing labeled data different related source domain. collection large labeled data needed traditional machine learning often prohibitive many real-life applications increasing interest young topic according taxonomy made recent surveys proposed method falls feature representation category. recent popular methods embrace dimensionality reduction seek latent shared feature space source target domain. core idea project original data low-dimensional latent space preserving important structure original data. however points direct application principal component analysis guarantee preservation discriminative data structures. proposed remedy maximize variance embedded data. another interesting idea nonparametric criterion namely maximum mean discrepancy based reproducing hilbert space estimate distance distributions. later further improves terms computational efﬁciency. goes step propose minimize mismatch cross-domains marginal probability distributions also conditional probability distributions based framework proposed framework paper considered extension major differences. first seek latent subspace minimizes mismatch marginal conditional probability distributions across domains also reinforces discriminative structure sub-domains original data. achieve goal introducing novel term acts repulsive force drag away different sub-domains source target domain respectively. note discuss line work literature transfer learning embedded deep convolutional neural network features used work deep features; nevertheless noticed impressive performance thanks combination latest advances transfer learning discussed cutting-edge understanding transferability state-of-the-art deep neural networks e.g. deep adaptation network etc. mixing seamlessly proposed transfer knowledge model state-of-theart deep networks subject upcoming investigation. unsupervised domain adaptation given source domain labeled samples unlabeled target domain j}nt unlabeled samples assumption source domain target domain different i.e. also deﬁne notion sub-domain denoted representing samples label similarly sub-domain deﬁned target domain samples label however target domain unlabeled samples basic classiﬁer e.g. needed attribute pseudo labels samples close discriminative domain adaptation learn latent feature space following properties distances marginal conditional probability source target domains reduced; distances sub-domain others increased order push away other; deduction label prediction imposed constraints i.e. label consistency geometric structure label space. ﬁnding latent feature space dimensionality reduction demonstrated useful several previous works e.g. domain adaptation. important properties original data projected lower dimensional space considered principal structure data. proposed method also apply principal component analysis mathematically given input data matrix centering matrix deﬁned trix ones. optimization projection space maximizes embedded data varic number classes represents sub-domain source domain number samples source sub-domain. deﬁned similarly target domain. finally represents conditional distribution sub-domains deﬁned latent feature subspace obtained joint marginal conditional domain adaptation reduce differences source target domain. such spaces data attracted close other. however model ignored important property elaboration effective predictor i.e. preservation reinforcement discriminative information related sub-domains. paper introduce novel repulsive force domain adaption aims increase distances sub-domains different labels improve discriminative power latent shared features thereby making possible better predictive model target domain. denotes trace matrix data covariance matrix rm×k feature dimension dimension projected subspace. optimal solution calculated solving eigendecomposition problem diag largest eigenvalues. finally original data projected optimal k-dimensional subspace using however feature space calculated sufﬁciently good enough problem domain adaptation problem seeks maximize variance projected data domains explicitly reduce distribution mismatch since distance data distributions across domain also empirically measured explicitly leverage nonparametric distance measurement rkhs compute distance expectations source domain target domain original data projected low-dimensional feature space via. formally empirical distance domains deﬁned distmarginal deﬁne ms→t mt→s repulsive force constraint matrix.while minimization makes closer marginal conditional distributions source target maximization increases distances source target sub-domains different labels thereby improve discriminative power underlying latent feature space. label deduction number domain adaptation methods e.g. simple nearest neighbor classiﬁer applied label deduction. nn-based label deduction applied twice iteration. ﬁrst applied target domain order generate pseudo labels target data enable computation conditional probability distance deﬁned section optimized latent subspace identiﬁed applied iteration label prediction target domain. however could good classiﬁer given fact usually based distance. could fall short measure similarity source target domain data embedded manifold complex data structure. furthermore cross-domain discrepancy still exists even within reduced latent feature space. respect underlying data manifold structure better bridge mismatch source target domain distributions propose paper consistency constraints namely label smoothness consistency geometric structure consistency pseudo ﬁnal label prediction. generate latent feature space discrepancy domains reduced simultaneously distances sub-domains different labels increased reinforced discriminative power underlying latent feature space. deﬁned distrepulsive distrepulsive index disdistrepulsive tances computed respectively. distrepulsive represents distances source sub-domain target sub-domains r∈{{...c}−{c}} except label distances explicitly ﬁned symmetrically distrepulsive represents distances target sub-domain source sub-domains r∈{{...c}−{c}} except source sub-domain label similarly distances explicitly deﬁned ﬁrst sub-problem explained amounts solving generalized eigendecomposition problemi.e. xhxt then obtain adaptation matrix underlying embedding space domain adaptation usps+minist coil ofﬁce+caltech standard benchmarks purpose evaluation comparison state art. paper follow data preparation previous works. construct datasets different image classiﬁcation tasks. usps minist datasets digits different distribution probabilities. built cross-domains usps mnist mnist usps; coil dataset classes split coil coil coil coil; face database different face poses subsets selected denoted etc. resulting domain adaptation tasks i.e. afﬁnity matrix giving afﬁnity samples deﬁned expxi−xj otherwise diag{d...d} wij. minimized geometric structure consistency ensures label space change much nearby data. proposed domain adaptation integrates marginal conditional distribution repulsive force well ﬁnal label prediction using label smoothness geometric structure consistencies. model deﬁned step construct step projection space calculation calculate mcyd mˆc; solve generalized eigendecomposition problem obtain adaptation matrix embed data transformation atx; step labels deduction isempty ofﬁce caltech. ofﬁce contains three real-world datasets amazon webcam dslr. caltech- standard dataset object recognition contains images categories. denote dataset amazonwebcamdslrand caltech- awdand respectively. domain adaptation tasks constructed namely respectively. -nearest neighbor classiﬁer; prinare cipal component analysis +nn; geodesic flow kernel transfer component analysis +nn; transfer subspace learning +nn; joint domain adaptation +nn. note viewed special case special case proposed cdda method repulsive force domain adaptation ignored label generation simply based instead label propagation label smoothness geometric structure consistency constraints. problem domain adaptation possible tune optimal hyper-parameters given fact target domain labeled data. following setting also evaluate proposed cdda empirically searching parameter space optimal settings. speciﬁcally proposed cdda method three hyper-parameters i.e. subspace dimension regularization parameters experiments usps mnist coil ofﬁce caltech-. table. highest accuracy cross-domain adaptation task highlighted bold. better understanding proposed cdda evaluate proposed cdda method using settings cdda simple used label predictor instead proposed label propagation; cdda proposed label propagation activated prediction target data labels. cdda reduced repulsive force domain adaptation label propagation integrated setting cdda enables quantify contribution adding repulsive force domain adaptation w.r.t. whereas setting cdda makes possible evidence contribution proposed label propagation comparison cdda highlight overall behavior proposed method. seen table. proposed cdda depicts overall average accuracy respectively respect settings. outperform baseline algorithms large margin. repulsive force integrated label predictor cdda outperforms crossdomain tasks improves jda’s overall average accuracy roughly points thereby demonstrating effectiveness proposed repulsive force domain adaptation. adopting proposed label propagation constraint label smoothness geometric structure consistency cdda further improves cdda roughly points terms overall average accuracy outperforms points. compared baseline methods proposed cdda method consistently shows superiority depicts best average accuracy four datasets seen fig. cdda represented curve curves along axis cross-domain image classiﬁcation tasks. worth noting proposed cdda depicts accuracy coil; rather unexpected impressive score given unsupervised nature domain adaptation target domain. using coil coil datasets also empirically check convergence sensitivity proposed cdda respect hyper-parameters. similar trends observed datasets. experiment cdda settings parameters cdda three cdda. accuracy variation w.r.t regularization parameter shown fig. indicates cdda achieves best performance close coil performance less stable less given novel dataset tune parameter range instance paper proposed close discriminative domain adaptation method based feature representation. comprehensive experiments cross-domain datasets highlight interest reinforcing data discriminative properties within model label propagation respect geometric structure underlying data manifold verify effectiveness proposed method compared baseline methods literature.", "year": 2017}