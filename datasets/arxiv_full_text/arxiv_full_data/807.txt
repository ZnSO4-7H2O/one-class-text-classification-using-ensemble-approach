{"title": "Survey of reasoning using Neural networks", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it poses many challenges like, how to train neural networks for discrete memory representation, how to describe long term dependencies in sequential data etc. Most prominent neural architectures for such tasks are Memory networks: inference components combined with long term memory and Neural Turing Machines: neural networks using external memory resources. Also, additional techniques like attention mechanism, end to end gradient descent on discrete memory representation are needed to support these solutions. Preliminary results of above neural architectures on simple algorithms (sorting, copying) and Question Answering (based on story, dialogs) application are comparable with the state of the art. In this paper, I explain these architectures (in general), the additional techniques used and the results of their application.", "text": "abstract. reason inference require process well memory skills humans. neural networks able process tasks like image recognition memory aspects still limited recurrent neural network it’s modiﬁed version lstm able solve small memory contexts context becomes larger threshold diﬃcult them. solution large external memory. still poses many challenges like train neural networks discrete memory representation describe long term dependencies sequential data etc. prominent neural architectures tasks memory networks inference components combined long term memory neural turing machines neural networks using external memory resources. also additional techniques like attention mechanism gradient descent discrete memory representation needed support solutions. preliminary results neural architectures simple algorithms question answering application comparable state art. paper explain architectures additional techniques used results application. artiﬁcial intelligence grand challenges build models make multiple computational steps answering question models describe long term dependence sequential data. machine learning models lack ability easily read write memory component infer using small part large memory. example tasks answer questions facts story watch movie answer questions conduct dialogues cannot solved models. principle solved language modeler recurrent neural network memory small compartmentalized enough remember required facts accurately. even simple memorization task like copying seen sequence rnns known problems rnns turing complete therefore capacity simulate arbitrary procedures practice able analogy turing’s enrichment ﬁnite-state machine inﬁnite memory tape. work like working memory solving tasks require application approximate rules rapidly-created variables using attentional process read write memory selectively. memory networks idea combine successful machine learning strategies memory component read written end-to-end memory networks extends memory networks removing problem backpropagation requirement strong supervision layer. continuous model require inputoutput pairs comparison memory network require supporting facts memory well. paper structure organization survey paper begins brief background basic neural architectures kind memory reasoning inference follows explanation prominent approaches using large memory additional techniques like memory focus continuous representation discussed along approaches. approaches experiments done using them results conclusions discussed finally conclusion comparison drawn experiments approaches recurrent neural networks neural networks loop hidden node i.e. output hidden node back hidden node alongwith input next timestamp. thus output hidden node acts like dynamic state whose evolution depends input current state unfolding time perceive context earlier timestamp could aﬀect behaviour network later timestamps. give vanishing exploding gradient problem. gradient moves across timestamps backpropagation it’s multiplied depending it’s value vanishes explodes thus unfold limited number timetamps increasing won’t eﬀect problem. solve problem vanishing exploding gradient another architecture called long short-term memory developed. solves problem embedding perfect integrators memory storage network. implemented complex structure gates shown fig. understanding purpose take simple perfect integrator input memory storage. weight identity gradient vanish explode. attach mechanism choose integrator takes input i.e. programmable gate depending context selectively store information indeﬁnite length time. gates similar sense used lstm make possible. architecture neural turing machine contains mainly neural network controller memory bank. controller interacts outside environment using input vector output vector. unlike neural networks also interact memory matrix using selective read write operations every component architecture diﬀerentiable gradient descent applied train network. attention mechanism uses ‘degree blurriness’ deﬁnes degree head reads writes memory location. words head read write completely location distributed many locations. components deﬁned follows. writing write operation parts erase followed operation. erase operation model deﬁnes erase vector whose elements memory erased using following equation row-vector multiplication memory locations acts point-wise. memory erased weighting erase element location operation length vector deﬁned. performed erase follows since multiplication erase addition operations commutative order multiple heads write irrelevant. ﬁnal memory content obtained heads done write operation. addressing mechanism weightings deﬁned reading writing operations produced using addressing mechanism. types addressing mechanism complement used focusing content model uses length vector produced head positive strength amplify attenuate precision focus similarity measure memory vector combined according following equation give normalized content based weighting weighting chosen content system wighting chosen content system shifted location system. allows head contiguous block data then access particular element within block. recurrent controller lstm internal memory network considered hidden activations registers controller taken cpu. allows controller information across multiple time steps operations. feedforward controller mimic recurrent network reading writing location memory step. additionally ’read write operations’ memory matrix easier interpret internal state ’read write operations’ however number concurrent read write heads feedforward controller imposes limitations type computation single read head unary operations performed memory timestep binary operations follows. it’s taken care storing read vectors internally previous time steps. memory network consists memory four components input feature converts input internal features generalization updates memories according input output feature produces output feature representation space based input current memory state response converts output desired format memnn implementation text neural networks used components memory network called memory neural network basic model four components memnn deﬁned follows sentences transformed embedding vectors memories stored number memories. output features produced ﬁnding supporting memories given first memory retrieved using following equation scored respect original input square brackets denote list. produces textual response limiting textual response single word response produced ranking them training done fully supervised setting desired inputs responses supporting sentences labeled training data thus known training time. training performed margin ranking loss stochastic gradient descent model takes discrete input representations store memory query outputs answer contains symbols dictionary vocabulary model converts continuous representation. representation processed multiple hops output representations continuous backpropagation training. deﬁned probability vector inputs. output memory representation using emedding matrix input transformed output vector output response vector computed following equation experiments done simple algorithms tasks like copying sorting data sequences. goal experiments observe problem solving learning compact internal programs architecture. solution programs could generalize well beyond training data. example network trained copy sequences length tested sequences length three architectures compared experiments tasks episodic thus dynamic state reset start input sequence. tasks supervised learning problems; network logistic sigmoid output layers trained cross-entropy objective function. sequence prediction errors reported bits-per-sequence. copy task copy sequence random binary vectors followed delimiter thus input sequence delimiter output sequence without delimiter. done compare eﬀect longer time delays lstm. seen fig. learned much faster lstm alone converged lower cost. continues copy length increases lstm rapidly degrades beyond length disparities suggest qualitative rather quantitative diﬀerence problem solving architectures. fig. memory priority sort task. left write locations returned ﬁtting linear function priorities observed write locations. middle observed write locations. right read locations. priority sort sorting capacity tested task. input collection random binary vectors priority range hypothesis uses priorities determine relative location write. test hypothesis linear function priority ﬁtted write locations. fig. shows results locations returned linear function closely match write locations reads memory locations increasing order thereby sequences traversed sorted manner. learning curves fig. show outperforms lstm. large-scale experiments performed dataset introduced consists statements stored triples. combination pseudo-labeled pairs made question associated triple pairs paraphrased questions. experiment framework returned candidate answers re-ranked results measured using score test set. systems developed following architecture given tested compared shown table results show viability memnns large scale answering lookup slow method extension like word hashing cluster hashing used. fig. example story statements questions answers generated simple simulation. answering location milk requires comprehension picked left actions. also requires comprehension time elements story e.g. answer where oﬃce?. simulated world simple simulation characters objects rooms characters move around pick drop objects based approach built. simulation converted text form statements sample shown fig. statements questions generated training testing. memnns compared rnns lstms task. diﬃculty task based limit number time steps entity last mentioned. three kinds questions presented system separately actor actor object actors without i.e. previous location actor lstm solved diﬃculty task performed worse questions even worse diﬃculty poor performance attributed failure encoding long term memory failure remember sentences lstm. synthetic question answering experiments experiments performed synthetic tasks deﬁned task consists statements question corresponding answer. answers available training time predicted testing time. diﬀerent types tasks require diﬀerent forms reasoning deduction. subset provided statements task relevant answering. information provided model training testing time. following three models compared approach memnn strongly supervised am+ng+nl memory network approach proposed uses supporting facts n-gram modelling nonlinear layers adaptive number hops query. task supporting fact supporting facts supporting facts argument relations argument relations yes/no questions counting lists/sets simple negation indefinite knowledge basic coreference conjunction compound coreference time reasoning basic deduction basic induction positional reasoning size reasoning path finding agent’s motivation mean error failed tasks table test error rates tasks models using training examples bag-of-words representation; position encoding representation; linear start training; random injection time index noise; rnn-style layer-wise weight tying joint joint training tasks ntms enrich capabilities recurrent networks profoundly using attention mechanism memory write large addressable memory. however results ntms shown simple tasks copying sorting discussed section results memnn memnn compared table suggest strong supervision memnn work best least error percentage. case weak supervision memnn better. consistently observed experiments architectures better performance lstm tasks require large memory lookup inference. memnn applied many situations like dialogs restaurant setting", "year": 2017}