{"title": "Soft-Deep Boltzmann Machines", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We present a layered Boltzmann machine (BM) that can better exploit the advantages of a distributed representation. It is widely believed that deep BMs (DBMs) have far greater representational power than its shallow counterpart, restricted Boltzmann machines (RBMs). However, this expectation on the supremacy of DBMs over RBMs has not ever been validated in a theoretical fashion. In this paper, we provide both theoretical and empirical evidences that the representational power of DBMs can be actually rather limited in taking advantages of distributed representations. We propose an approximate measure for the representational power of a BM regarding to the efficiency of a distributed representation. With this measure, we show a surprising fact that DBMs can make inefficient use of distributed representations. Based on these observations, we propose an alternative BM architecture, which we dub soft-deep BMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed representations in terms of the measure. Experiments demonstrate that sDBMs outperform several state-of-the-art models, including DBMs, in generative tasks on binarized MNIST and Caltech-101 silhouettes.", "text": "present layered boltzmann machine better exploit advantages distributed representation. widely believed deep greater representational power shallow counterpart restricted boltzmann machines however expectation supremacy dbms rbms ever validated theoretical fashion. paper provide theoretical empirical evidences representational power dbms actually rather limited taking advantages distributed representations. propose approximate measure representational power regarding efﬁciency distributed representation. measure show surprising fact dbms make inefﬁcient distributed representations. based observations propose alternative architecture soft-deep show sdbms efﬁciently exploit distributed representations terms measure. experiments demonstrate sdbms outperform several stateof-the-art models including dbms generative tasks binarized mnist caltech- silhouettes. aspect behind superior performance deep architectures effective distributed representations representation said distributed consists mutually non-exclusive features distributed representations efﬁciently model complex functions enormous number variations dividing input space huge number sub-regions combination features recent analyses proven efﬁcient distributed representations deep feed forward networks rectiﬁed linear activations deep networks model complex input-output relationships dividing input space enormous number sub-regions grow exponentially number parameters. multiple levels feature representations deep feed forward networks successfully facilitate efﬁcient reuse low-level representations deep feed forward networks thus manage exponentially greater number sub-regions shallow architectures. interesting whether deep generative models could attain property deep discriminative models. answer question would useful compare restricted boltzmann machines deep boltzmann machine rbms shallow generative model distributed representations deep boltzmann machines deep extension rbms dbms commonly expected greater representational power rbms relatively easy trained compared rbms. however expectation supremacy dbms rbms ever validated theoretical fashion. paper provide theoretical empirical evidences representational power dbms actually rather limited exploiting advantages distributed representations. figure free energy hard-min free energy two-layered sdbm free energy bounds indicated shaded regions. layers unit. sdbm parameters generated algorithm rescaling. contributions follows. first propose approximate measure efﬁciency distributed representations inspired recent analysis deep feedforward networks measure number linear regions piecewise linear function approximates free energy function measure approximates number sub-regions manages visible space. show depth largely improve representational power terms measure. indicates surprising fact dbms make inefﬁcient distributed representations despite common expectations. second propose superset dbms soft-deep sdbm layered layer pairs connected topologically deﬁned regularization. relaxed connections realize soft hierarchy opposed hard hierarchy conventional deep networks neighboring layers connected. show number linear regions approximate free energy sdbm scales exponential number layers thus large general exponentially greater dbm. finally experimentally demonstrate high generative performance sdbms. sdbms trained without pretraining outperform state-ofthe-art generative models including dbms benchmark datasets mnist caltech- silhouettes. propose soft-deep boltzmann machine consists multiple layers layer pairs connected connections within layers restricted. figure illustrates sdbm. energy sdbm deﬁned θsdbm state layer {vh} units hidden layers visible layer. number layers s.t. visible layer hidden layer. number layers total number units number units layer nvis number visible units nhid number hidden units. sdbm assigns probability exp) conﬁguration rbms dbms subsets sdbms; rbms sdbms dbms sdbms section deﬁne approximate measure representational power based free energy function. compare various terms measure show sdbms could attain richer representations dbms rbms. would able measure representational power complexity free energy function free energy function contains information probability distribution models. deﬁne piecewise linear approximation free energy function rbms widely known free energy function well approximated piecewise linear function idea extended general connections garded relaxed operation; virtually dominated smallest energy i.e. hidden conﬁgurations. negative logarithm thus nearly minh based observation deﬁne following approximation free energy deﬁnition hard-min free energy parameters deﬁned tightness bound determined dominance minimum energy free energy. difference upper lower bounds becomes fairly tight smaller eres contribution non-minimum energies free energy. theorem shows rough approximation free energy; less accurate mean-ﬁeld approximation fmf. nevertheless bound tight except points several energy terms nearly achieve minimum e.g. boundaries linear regions figure demonstrates idea. therefore able roughly measure complexity free energy quantifying complexity natural quantify complexity piecewise linear function count number linear regions. quantify representational power deep feedforward network activation strategy recently applied piecewise linear input-output function inspired analyses propose number linear regions measure bm’s representational power. intuitively measure roughly indicates number effective bernoulli mixing components linear regions well approximated negative probability function mixture bernoulli components assigning component region. therefore shall call measure number effective mixtures obviously deﬁnitions maximal number effective mixtures bounded number hidden conﬁgurations proposition number effective mixtures upper bounded nhid. number effective mixtures approximately measures efﬁciency distributed representation. conﬁguration distributed representation give rise linear region therefore efﬁcient distributed representation potentially manages nhid subregions visible space. efﬁciency however substantially damaged restricted connections. deep feedforward networks mont´ufar showed deeper network model piecewise linear function much linear regions shallow network number parameters. number linear regions grows exponentially number layers. question also true dbms terms approximate free energy surprisingly answer shall provide proofs following sections. ﬁrst analyze rbms. free energy function approximated -layered feedforward network number linear regions input-output function shallow network studied pascanu mont´ufar slight modiﬁcation results compute maximal number effective mixtures next analyze dbms. provide lower upper bounds maximal number effective mixtures dbms. lower bound dbms superset rbms proposition maximal number effective mixtures lower bounded idea proof upper bound show appendix energies associated conﬁguration ﬁrst hidden layer identical gradient space example gradient i.e. slope fig. affect statistics given number linear regions therefore bounded energy terms slope become globally smaller energy term e.g. generalize dbms leading natural somewhat shocking result bound depends number units ﬁrst hidden layer theorem number effective mixtures number hidden layers upper bounded depth largely help number effective mixtures dbms. suggests distributed representation inefﬁciently used least scope approximate free energy proposition theorem readily show serious limitation number effective mixture dbms proposition number effective mixture never achieves bound nhid. limited number effective mixtures dbms independency given conversely exists dependency visible upper hidden layers even given limitation number effective mixtures hold. bypassing connections sdbms therefore might improve number effective mixtures. figure demonstrate idea showing sdbm attains nhid linear regions properly chosen parameters. section reﬁne idea general sdbms. ﬁrst analyze number effective mixtures general visible unit regarded elemental sdbm. general hidden units visible unit whose energy function deﬁned deﬁned regard elemental sdbm layers unit index units parameters superscripts. call unit layer reason. dimensional linear functions deﬁned visible unit θgbm)|x }l}. ﬁrst analyze arrangement elements network construction procedure analyze number linear regions arrangement. procedure listed algorithm network constructed appending unit recursive manner starting unit unit example). construction show elements arranged tangent quadratic curve different points lemma assume {w}{b} computed softdeep large integer elements tangents quadratic function equally spaced points tangency. figures demonstrate statements lemma different hidden units control slope energy different levels magnitude allows mutually different slopes thus leads effective mixtures. call connections determined algorithm soft-deep connections connection strengths regarded decay exponentially distance layers. units aligned sequential order fig. strength connection unit upper unit units away unit spatial conﬁguration proportional observe connection pattern soft counterpart conventional deep connection pattern adjacent layers connected. figure heatmap two-layered sdbm. sdbm constructed bundle independent gbms. lines indicate boundary linear regions parameters computed algorithm rescaling. applying lemma sdbm constructed bundle independent gbms show maximal number effective mixtures l-layered sdbm scales exponentially theorem suppose sdbm hidden layers contains units. number effective mixtures sdbm reaches nhid bound proposition certain parameter conﬁguration. figure demonstrates claim theorem free energy function sdbm four hidden units well approximated linear regions. along analysis dbms rbms theorem indicates soft-deep connections bypass remote layers vital deeply layered superior representational power shallow terms efﬁciency distributed representations. clearly contrasts feedforward networks bypassing connections critically affect representational power appealing properties sdbms huge number effective mixtures. first fast block gibbs sampling performed. although sampling efﬁciency degrades compared dbms dependency hidden layers introduced soft-deep connections believe beneﬁts huge representational power offset negative effect. second soft-deep connections ease difﬁculties learning deeply layered bms. dbms connections remote layers effect visible layer exerts remote hidden layers decays exponentially depth. phenomenon hinder learning signals correctly propagating deep layers. believe beneﬁts pretraining help stochastic vanishing gradient effect. soft-deep connections ease problem bypassing visible layer remote hidden layers. believe high generative performance sdbms without pretraining shown section achieved huge representational power proven theorem also less severe vanishing gradient effect. number effective mixtures sdbm scale exponentially depth theorem essential connection weights multiple levels magnitude. without regularization uniform regularization networks attain property learning. address point introduce soft-deep regularization strength regularization connections layers inversely proportional |w|η computed algorithm hyper parameter. although technique strictly guarantee discussed representational power based approximated free energy validate approximation experimentally demonstrated advantages sdbms. performed experiments datasets mnist digits caltech- silhouettes used theano pylearn implement sdbms. used stochastic maximum likelihood jointly train networks centering method soft-deep regularization. perform pretraining. scheduled learning rates linearly decay initial value zero. evaluate networks used estimate variational lower bound average log-likelihood test data. evaluated reliability estimates computing conﬁdence intervals show supplementary material. datasets trained -layered sdbms various hyper parameters. number units hidden layer ﬁxed hyper parameters tuned random sampling supplementary material detail. figure random samples -layered sdbm displayed nearest training test samples binarized mnist. generated images probabilities pixels sampled from. nearest neighbors computed terms pixelwise distance. sdbm simply memorize training examples generalize unseen test examples. mnist collection gray scaled digit images consists training samples test samples binarized images following procedure salakhutdinov murray generate training test data. table compares sdbms various models literature terms generative performance. sdbms greatly performed compared models. even -layered sdbm outperformed previous state-of-the-art test log-likelihood recent report best-performing -layered sdbm achieved test log-likelihood conﬁdence interval nat. note sdbms -hidden layers outperformed dbms number layers result would seen reﬂect exponentially greater number effective mixtures sdbms dbms number parameters. parameter tuning; -layered model performed worse -layered model training data shown supplementary material. performance models would uniformly improve depth networks precise parameter tuning. table comparison generative performance various generative models binarized mnist. report average test log-likelihood measured nat. test model table compares sdbms several models generation caltech- silhouettes. sdbms outperformed previous state-of-the-art nade trained reweighted wake-sleep algorithm -layered sdbm achieved test log-likelihood conﬁdence interval nat. result best average test log-likelihood achieved caltech- silhouettes best knowledge. figure shows samples generated best performing -layered model. samples proves nice generalization network though samples resemble training examples. paper proposed architecture better exploit distributed representation. proposed measure efﬁciency distributed representation number effective mixtures number linear regions piecewise linear function approximates free energy function showed inefﬁciency dbms respect maximal number effective mixtures. proposed sdbms extension dbms. showed maximal number effective mixtures sdbm exponentially larger dbm. finally experimentally demonstrated high generative performance sdbms. guido mont´ufar razvan pascanu kyunghyun yoshua bengio. number linear regions deep neural networks. advances neural information processing systems paul smolensky. information processing dynamical systems foundations harmony theory. david rumelhart james mcclelland editors parallel distributed processingexplorations microstructure cognition foundations pages press geoffrey hinton terrence sejnowski. learning relearning boltzmann machines. david rumelhart james mcclelland editors parallel distributed processingexplorations microstructure cognition foundations pages press james martens arkadev chattopadhyay toniann pitassi richard zemel. representational efﬁciency restricted boltzmann machines. advances neural information processing systems pages fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard david warde-farley yoshua bengio. theano features speed improvements. arxiv.org november goodfellow david warde-farley pascal lamblin vincent dumoulin mehdi mirza razvan pascanu james bergstra fr´ed´eric bastien yoshua bengio. pylearn machine learning research library. arxiv.org august tijmen tieleman. training restricted boltzmann machines using approximations likelihood gradient. proceedings international conference machine learning pages ruslan salakhutdinov iain murray. quantitative analysis deep belief networks. proceedings international conference machine learning. july james bergstra yoshua bengio. random search hyper-parameter optimization. journal karol gregor danihelka alex graves daan wierstra. draw recurrent neural network image generation. proceedings international conference machine learning ruslan salakhutdinov geoffrey hinton. better pretrain deep boltzmann machines. advances neural information processing systems pages salimans diederik kingma welling. markov chain monte carlo variational inference bridging proceedings international conference machine learning iain murray ruslan salakhutdinov. evaluating probabilities high-dimensional latent variable models. advances neural information processing systems pages danilo jimenez rezende shakir mohamed daan wierstra. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning simon osindero geoffrey hinton. modeling image patches directed hierarchy markov random ﬁelds. advances neural information processing systems pages geoffrey hinton terrence sejnowski. optimal perceptual inference. proceedings ieee conference computer vision pattern recognition pages ieee york symmetric connection weights units biases parameters. visible hidden units rich representations; visible units correspond data variables hidden units correspond latent features data. units either visible units hidden unit numbers visible hidden units denoted nvis nhid. various network topologies restrict connections attracted great research interests albeit general superset restricted connections general rarely used practice. main problem general difﬁculty intractability expectations respect data-dependent model distributions. approach approximate expectations expensive mcmc relaxation time markov chain quite long general enormous number well-separated modes explored. moreover dense connections require generic gibbs sampling updates unit time. restriction connections alleviate issues. particularly layered connection patterns widely studied appealing properties efﬁciency sampling less-complex energy landscapes simplicity learning algorithms. review representative layered restricted deep model denotes states hidden layer θrbm parameters. redundant notation layer indices associated superscript avoid confusion notations models shall describe later sections. rbms exhibit nice property conditional distributions p|v) tractable factorized. allows perform fast block gibbs sampling makes data-dependent expectation tractable. dbms extension rbms multiple hidden layers form deep hierarchy. connections within layer restricted units layer connected units neighboring layers energy function layers dbms several appealing properties. fast block gibbs sampling also applicable dbms rbms. particularly block sampling highly efﬁcient conditional distributions even layers given layers layers given even layers tractable factorized. moreover dbms possess greater representation power rbms multiple hidden layers. however improved representation power causes serious difﬁculty. data-dependent expectation needs approximated learning conditional distribution longer tractable; stochastic approximation procedure variational inference used approximation. appearance dbms salakhutdinov hinton introduced pre-training algorithm ease problem. recently centering method proposed joint training dbms without pre-training upon introduction dbms dbms would expected scalable i.e. great performance improvements achieved dbms stacking layer deep neural models. however experiments suggest seems true; improvements hard gained deep hidden layers even elaborated learning algorithms widely conceived poor scalability dbms attributed cannot exploit huge representation capacity dbms inefﬁcient optimization methods. true extent. however shall provide empirical theoretical evidences poor scalability dbms optimization issues also rather limited representation capacity dbms. provide lower upper bounds maximal number effective mixtures dbms respect parameters. begin lower bound. dbms superset rbms number effective mixtures large maximal number effective mixtures rbms. observation leads lower bound proposition maximal number effective mixtures lower bounded outline idea proof theorem marginal distribution visible units written summation θdbm)p; θdbm) θdbm) marginal distribution indicates number mixing components bounds maximal number effective mixtures observations lead natural somewhat shocking result bound depends number units ﬁrst hidden layer theorem number effective mixtures number hidden layers upper bounded proof. suppose linear functions v})|h }nkfor linear functions within identical αj)vj constant depends therefore linear αj)vj cmin cmin minh...h hard-min free energy minh minh fmin) maximal number linear regions bounded number conﬁgurations i.e. results depict serious limitation representation power dbms. ways increase number effective mixtures dbm. ﬁrst stack layers. however number effective mixtures never become greater solely determined therefore depth largely help capacity dbms measured number effective mixtures. second increase strategy however least necessitates presence second layer units improve bound otherwise equivalent therefore number maximal number effective mixtures merely θ}{b} computed softdeep large integer elements tangents quadratic function equally spaced points tangency. main text linear functions x)|x }l}. assume elements tangent point tangency xk−. divide sets lines correspond either readily show elements tangent sx=. show tangency elements follows. gx=η; element sx=η hidden conﬁguration i.e. proof. assume whose parameters generated softdeep. lemma element tangent different points. strictly concave equality holds point tangency. therefore point tangency thus neighbor elements tangents different points number effective mixtures proves claim. result directly indicates general visible units also achieve maximal number effective mixtures nhid connection weights determined construction procedure visible-hidden connections replicated visible units. theorem suppose sdbm hidden layers contains units. number effective mixtures sdbm reaches nhid bound proposition certain parameter conﬁguration. increasingly intense research interests connection deep neural networks biological neural networks prevalent aspect layers deep neural networks correspond cortical regions form hierarchy however unlike conventional deep networks widely known biological neural networks many connections bypass functionally remote cortical regions bypassing connections largely contribute representation power feedforward neural networks recent great success deep feedforward networks explain functional role bypassing connections brain. results sdbms help understand mystery. parameters tuned hyper parameters random sampling; initial learning rates sampled mnist caltech- silhouettes strengths regularization sampled sampled update constants centering parameters sampled generated conﬁgurations hyper parameters experiment setting. number parameter updates networks trained stochastic maximum likelihood perform variational inference number positive phase markov chain updates parameter update mnsit caltech- silhouettes. number negative phase markov chain updates parameter update batch size throughout training monitored training test log-likelihood models occasionally performing ais. monitoring executed rather cheap settings runs intermediate distributions. training performed expensive several best performing models evaluated cheap gain thorough estimates. expensive table details estimates sdbms trained mnist. estimated variational lower bounds training test data reported. conﬁdence intervals also reported parentheses.", "year": 2015}