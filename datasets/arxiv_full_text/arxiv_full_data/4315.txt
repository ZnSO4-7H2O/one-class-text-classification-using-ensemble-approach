{"title": "Sparsity Based Methods for Overparameterized Variational Problems", "tag": ["cs.CV", "stat.ML", "47N10, 35A15, 49N45, 65M20, 65J22, 68U10, 94A12, 65D18"], "abstract": "Two complementary approaches have been extensively used in signal and image processing leading to novel results, the sparse representation methodology and the variational strategy. Recently, a new sparsity based model has been proposed, the cosparse analysis framework, which may potentially help in bridging sparse approximation based methods to the traditional total-variation minimization. Based on this, we introduce a sparsity based framework for solving overparameterized variational problems. The latter has been used to improve the estimation of optical flow and also for general denoising of signals and images. However, the recovery of the space varying parameters involved was not adequately addressed by traditional variational methods. We first demonstrate the efficiency of the new framework for one dimensional signals in recovering a piecewise linear and polynomial function. Then, we illustrate how the new technique can be used for denoising and segmentation of images.", "text": "images returns horizontal vertical derivatives using ﬁlters respectively. note dimensional signals difference gradient equals derivative. however case ﬁrst considers gradients second considers absolute directional derivatives approximated ﬁnite differences. recently interesting connection drawn total-variation minimization problem sparsity model. shown viewed ℓ-relaxation technique approximating signals sparse derivatives domain i.e. applying operator ωdif signals said cosparse operator ωdif analysis sparsity model notice regularization example variational framework. another recent technique focus paper overparameterization idea represents signal combination known functions weighted space-variant parameters model introduce overparameterized model example. signal known piecewise linear i-th element written local coefﬁcients describing local line-curve. such vectors piecewise constant discontinuities locations. constant interval corresponds linear segment abstract—two complementary approaches extensively used signal image processing leading novel results sparse representation methodology variational strategy. recently sparsity based model proposed cosparse analysis framework potentially help bridging sparse approximation based methods traditional total-variation minimization. based this introduce sparsity based framework solving overparameterized variational problems. latter used improve estimation optical also general denoising signals images. however recovery space varying parameters involved adequately addressed traditional variational methods. ﬁrst demonstrate efﬁciency framework dimensional signals recovering piecewise linear polynomial function. then illustrate technique used denoising segmentation images. many successful signal image processing techniques rely fact given signals images interest belong class described certain priori known model. given model signal processed estimating correct parameters model. example sparsity framework assumption signals belong union dimensional subspaces variational strategy model imposed variations signal e.g. derivatives required smooth though sparsity-based variational-based approaches widely used signal processing computer vision often viewed different methods little common them. well known variatonal tools total-variation regularization used mainly denoising inverse problems. formulated using sparsity. section proposes recovery strategy polynomial case based sscosamp technique optimal projections provide stable recovery guarantees algorithm case additive adversarial noise denoising guarantees case zero-mean white gaussian noise. section extend scheme beyond case higher dimensional polynomial functions images. employ extension gapn algorithm block sparsity task. section present experiments linear overparameterization images dimensional signals. demonstrate proposed method used image denoising segmentation. section concludes work proposes future directions research. note without prior knowledge cannot recover variational framework regularization imposed variations signal popular strategy recovering signal framework solving following minimization problem regularization weight type norm used regularization operator typically local operator. example minimization another example regularization operator laplace operator types regularization operators variational formulations found overameterized variational framework introduced extension traditional variational methodology instead applying regularization signal itself applied coefﬁcients signal global parameterization space. element signal modeled coefﬁcients vectors contain parameterization basis functions rd×d diagonal matrix values main diagonal. images parameterization would similarly a+bi+bj. strategy referred overparameterization because number representation parameters larger signal size. example original signal contains unknown values recovery problem seeks twice many variables. clearly many parameterization options signals beyond linear one. parameterizations shown improve denoising performance solution problem posed cases provide high quality results optical estimation true force behind overparameterization uses variables needed representing signals often naturally suited describe structure. example signal piecewise linear impose constraint overparameterization coefﬁcients piecewise constant. note piecewise constant signals sparse ωdif operator. therefore coefﬁcients tools developed analysis sparsity model however case jointly sparse i.e. change points collocated therefore extension necessary. constraints structure sparsity pattern representation already analyzed literature. commonly referred joint sparsity models found literature context handling groups signals considering blocks non-zeros single representation vector tools extend existing analysis techniques handle block sparsity overparameterized scheme. paper introduce general sparsity based framework solving overparameterized variational problems. structure problems enables segmentation recovering signal provide elegant recovering signal deteriorated measurements using approach accompanied theoretical guarantees. demonstrate efﬁciency framework dimensional functions recovering piecewise polynomial signals. shift view images demonstrate approach used denoising segmentation. paper organized follows section present overparameterized variational model details. section describe brieﬂy synthesis analysis sparsity models. sections introduce framework solving overparameterized variational problems simply minimization problems nphard many approximation techniques proposed approximate solution accompanied recovery guarantees depend properties matrices include ℓ-relaxation known also lasso matching pursuit orthogonal matching pursuit compressive sampling matching pursuit subspace pursuit iterative hard thresholding hard thresholding pursuit another framework modeling union dimensional subspaces analysis model considers behavior signal applying given operator assumes vector sparse. note zeros characterize subspace resides zero corresponds orthogonal therefore resides subspace orthogonal spanned rows. cosparse cosupport sub-matrix rows corresponding synthesis case minimization problems also np-hard approximation techniques proposed including greedy analysis pursuit noise analysis cosamp analysis analysis analysis overparameterization variational problem. know signal piecewise linear coefﬁcients parameters piecewise constant discontinuity locations linear overparameterization used. denote number discontinuity locations. returning example linear overparameterization case diag diagonal matrix diagonal. piecewise linear function coefﬁcients vectors piecewise constant therefore would natural regularize coefﬁcients gradient operator. leads following minimization problem special case main advantages using overparameterized formulation these unknowns simpler form thus easier recover; formulation leads recovering parameters signal along signal itself. overparametrization idea introduced builds upon vast work signal processing refers variational methods. such known guarantees quality recovery signal using formulation posed variants. moreover shown even case poor recovery achieved recovering parameterization coefﬁcients. note happens even sophisticated regularizations combined applied eventually leads look another strategy approach problem recovering piecewise linear function deteriorated measurement describing scheme introduce next section sparsity model developing alternative strategy. popular prior recovering signal distorted measurements sparsity model idea behind know priori resides union dimensional subspaces intersect trivially null space estimate stably selecting signal belongs union subspaces closest classical sparsity model signal assumed sparse representation given dictionary i.e. dαkαk pseudo-norm counts number non-zero entries vector sparsity signal. note dimensional subspace standard sparsity model known also synthesis model spanned collection columns model recover solving whose atoms step functions different length. known observation every dimensional signal change points sparsely represented using atoms observe fact ωdif ˜dhs ˜dhs submatrix obtained removing last column therefore recover coefﬁcient parameters sparse representations solving minimization problem approximated using block-sparsity techniques group-lasso estimator mixed-ℓ/ℓ relaxation block algorithm extensions cosamp structured sparsity joint sparsity framework also used problem synthesis techniques twofold recovery guarantees exist formulation dictionary dhs; hard generalize model higher order signals e.g. images. reason theoretical guarantees provided dictionary high correlation columns. create high ambiguity causing classical synthesis techniques fail recovering representations problem addressed several contributions treated signal directly representation introduce algorithm approximates solutions theoretical reconstruction performance guarantees dimensional functions matrices near isometric piecewise polynomial functions. next section shall present another algorithm guarantees generalizable higher order functions. though till restricted discussion piecewise linear functions turn look general case piecewise polynomial functions degree note method approximates following minimization problem generalization |ωdifbi| element-wise operation calculates absolute value entry ωdifbi. employ signal space cosamp strategy approximate solution algorithm assumes existence projection given signal ﬁnds closest signal belongs model case model piecewise polynomial functions jump points. algorithm along projection required presented appendix recovery sscosamp employ theorems lead reconstruction error bounds sscosamp guarantee stable recovery noise adversarial effective denoising effect zero-mean white gaussian. note theorem implies compressively sense piecewise polynomial functions achieve perfect recovery noiseless case note also subgaussian random matrix sufﬁcient measurements upper-bound noise energy given sparsity common many applications. notice synthesis model generalization trivial easy extend ωdif operator high dimensions clear heaviside dictionary. therefore consider overparameterized version noise energy known analysis model used. matrices space variables coefﬁcients parameters. example case piecewise linear constant identity matrix diagonal matrix values main diagonal similarly diagonal matrix main diagonal. assuming coefﬁcient parameters jointly sparse general operator recover coefﬁcients solving case reason noise adversarial leading worst-case bound. introducing random distribution noise better reconstruction guarantees. following theorem assumes noise randomly gaussian distributed enabling provide effective denoising guarantees. easier higher dimensional functions. move next section note advantages formulation bgapn algorithm relative ease adding constraints. example encounter piecewise polynomial functions also continuous. however continuity constraint current formulation. shall next section absence constraint allows jumps discontinuity points polynomial segments therefore important algorithm better reconstruction. possibility solve problem continuity constraint jump points signal. appendix present also modiﬁed version bgapn algorithm imposes continuity constraint next section shall handles problem. note example constraint bgapn technique. example images smoothness constraint edges’ directions. demonstrating efﬁciency proposed method perform several tests. start dimensional case testing polynomial ﬁtting approach continuity constraint without continuous piecewise polynomials ﬁrst second degrees. compare results optimal polynomial approximation scheme presented section variational approach continue compressed sensing experiment discontinuous therefore bgapn approximates ﬁnding ﬁrst. starts includes rows gradually removes elements solving problem posed iteration ﬁnding largest correlation current temporal known recovery guarantees bgapn form sscosamp before. therefore present efﬁciency several experiments next section. explained appendix advantages bgapn sscosamp despite lack theoretical guarantees need foreknown fig. recovered piecewise linear functions piecewise second-order polynomial functions function noise variance methods bgapn without continuity constraint optimal approximation without continuity post-processing. reference compare local overparameterized approach introduced piecewise polynomials compare bgapn sscosamp. perform tests images using bgapn. start denoising cartoon images using piecewise linear model. compare outcome denoising show result suffer staircasing effect compare also denoising version overparameterization show framework used image segmentation drawing connection mumford-shah functional compare results ones obtained popular graph-cuts based segmentation order check performance polynomial ﬁtting generate random continuous piecewise-linear secondorder polynomial functions samples jumps dynamic range contaminate signal white gaussian noise standard deviation compare recovery result bgapn without continuity constraint optimal approximation. figs. present bgapn reconstruction results linear second order polynomial cases respectively different noise levels. observed addition continuity constraint essential correctness recovery. indeed without jumps segments. note also number jumps recovery different original signal bgapn preliminary information however still manages recover parameterization good especially lower noise case. terms mean squared error using methods free-knot spline however approximated function guaranteed piecewise linear therefore learning change points sub-optimal. references therein details. evaluate method respect compare optimal approximation piecewise polynomial function presented appendix a-a. note target signals continuous algorithm assumption. therefore continuity constraint method post processing take changing points recovered project noisy measurement closest continuous piecewise polynomial function discontinuities. figure presents recovery performance bgapn projection algorithm without continuous constraint. without constraint observed bgapn achieves better recovery performance. fact restricted number change points initial signal therefore points thus adapt better signal achieving lower mse. however adding constraint piecewise linear case optimal projection achieves better recovery error. reason that optimal projection uses exact number points ﬁnds changing locations accurately. note though case second order polynomial functions bgapn gets better recovery. happens program uses continuity constraint also within iterations ﬁnal step case projection algorithm. second order polynomial case complex piecewise linear impact usage continuity prior higher signiﬁcant information number change points. compare also non-local opverapameterized algorithm shown better task line segmentation compared several alternatives including ones reported clearly proposed scheme achieves better recovery performance tvopnl demonstrating supremacy line segmentation strategy. perform also compressed sensing experiment compare performance sscosamp optimal projection bgapn recovering second order polynomial function jumps small linear measurements. entry measurement matrix selected i.i.d normal distribution columns normalized unit norm. polynomial functions selected previous experiment differences omit continuity constraint; normalize signals unit norm. fig. presents recovery rate program function number measurements note small large number samples bgapn behaves better. however middle range sscosamp turn evaluate performance approach images. piecewise smooth model considered good model images especially ones texture i.e. cartoon images therefore linear overparameterization dimensional plane employ dimensional difference operator ωdif calculates horizontal vertical discrete derivatives image applying ﬁlters case fig. denoising house using bgapn algorithm. notice staircasing effect appears reconstruction. model linear recover texture thus slightly inferior results compared respect psnr. note cubic overparameterization bgapn instead linear psnr better swoosh sign. compare results ones denoising figs. present recovery swoosh sign noisy version contaminated additive white gaussian noise note achieve better recovery results suffer staircasing effect. tuned parameters separately image optimize output quality used setup method denoising experiments. good quality bgapn algorithm several times different parameters provide output average image runs. notice using technique degrades results. since scheme divides image piecewise linear regions view strategy approach minimizes mumford-shah functional hand image regions segmentation result viewed solution chan-vese functional difference model region polynomial function instead approximating constant present segmentation results three images display piecewise constant version image together boundary map. segmentation results appear figs. compare results popular graph-cuts based segmentation notice achieve comparable performance places method behaves better others strategy provides better result. though good segmentation clear still large room improvement compared current state-of-the-art. direction improvement ﬁlters within another calculate gradients coefﬁcients parameters recovered image supposed truly piecewise constant. leave ideas future work. work presented novel framework solving overparameterized variational problem using sparse representations. demonstrated framework used dimensional dimensional functions generalization higher dimensions straightforward. solved problem line ﬁtting piecewise polynomial signals shown technique used compressed sensing denoising segmentation. though work focused mainly linear overparameterizations extension forms straightforward. however keep discussion simple possible chosen simple forms overparameterizations experiments section. future research believe learning process added scheme. adapt functions space variables ﬁlters signal hand. believe potential lead state-of-the-art results segmentation denoising signal processing tasks. combining scheme standard sparse representation approach provide possibility support images texture. lead scheme works globally image cartoon part locally texture part. another route future work integrate scheme stateof-the-art overparameterized based algorithm optical overparameterization outcome framework compare also linear overparameterization notice plugging overparameterization directly improves results cases case images here. therefore framework links sparsity overparameterization advantage approach still acts within variational scheme. could forms overparameterizations cubical instead planar directions derivatives addition horizontal vertical ones. example apply scheme also using operator calculates also diagonal derivatives using ﬁlters improvement different scenarios. future work focus learning overparameterizations type derivatives used denoising tasks. believe learning potential lead state-of-the-art results. motivation task segmentation present denoising image texture. continue using model consider house image example. fig. demonstrates denoising result image. note well suffer staircasing effect appears recovery. however nature model loose texture therefore achieve inferior psnr compared denoising. though removal texture favorable task denoising makes recovery salient edges original image easier. fig. present gradient recovered image original image. seen gradients original image capture also code provided authors. lower psnr method model linear therefore less capable adapt texture. using cubic overparameterization psnr equal note also larger noise magnitudes recovery performance algorithm terms psnr becomes better also linear model conditions tend loose texture anyway. projection technique uses fact jump points optimal parameters polynomial segment calculated optimally solving least squares minimization problem leading results received funding european research council european unions seventh framework program grant agreement research partially supported afosr nga. authors would like thank anonymous reviewers helpful constructive comments greatly contributed improving paper. method uses projection given signal ﬁnds closest piecewise polynomial functions jump points. calculate projection using dynamic programming. strategy generalization appears presented next subsection. denoting worst case complexity calculating pair complexity step step computation projection error complexity step summing together total complexity algorithm polynomial complexity since polynomial. approximating extend gapn technique block sparsity adapt model. presented algorithm notice program unlike sscosamp assume knowledge existence optimal projection onto signals’ dimensional union subspaces. note also suits general form overparameterization piecewise polynomial functions. possible accelerate bgapn highly scaled problems removing cosupport several elements time instead update cosupport stage. case size segment smaller number parameters e.g. segment size linear function minimization problem inﬁnitely many options setting parameters. however lead result keeping values points segment i.e. ideally would expect several iterations updating cosupport bgapn would however many signals nearly cosparse i.e. signiﬁcantly large values rest smaller small constant therefore natural stopping criterion case would stop maximal value smaller stopping criterion throughout paper bgapn. course option stopping criterion e.g. look relative solution change iteration constant number iterations foreknown. present also modiﬁed version bgapn algorithm imposes continuity constraint change points. done creating binary diagonal matrix diag iteration program i-th element corresponds change point zero otherwise. matrix serves weights matrix penalize discontinuity change point. done adding regularizing term cotter engan kreutz-delgado sparse solutions linear inverse problems multiple measurement vectors ieee trans. signal process. vol. july tropp gilbert strauss algorithms simultaneous sparse approximation. part greedy pursuit signal processing vol. sparse approximations signal image processing. tropp algorithms simultaneous sparse approximation. part convex relaxation signal processing vol. sparse approximations signal image processing. piecewise |ωdifbi|k number jumps representation coefﬁcients additive noise parameter algorithm. procedure approximates given signal piecewise polynomial function order jumps. giryes elad gribonval davies greedylike algorithms cosparse analysis model linear algebra applications vol. jan. special issue sparse approximate solution linear systems. rosman shem-tov bitton adiv kimmel feuer bruckstein over-parameterized optical using stereoscopic constraint scale space variational methods computer vision ser. lecture notes computer science bruckstein haar romeny bronstein bronstein eds. springer berlin heidelberg vol. giryes gribonval davies iterative cosparse projection algorithms recovery cosparse vectors european signal processing conference barcelona spain shem-tov rosman adiv kimmel bruckstein globally optimal local modeling moving least squares overparametrization innovations shape analysis. springer elad milanfar rubinstein analysis versus synthesis signal priors inverse problems vol. june chen donoho saunders atomic decomposition basis pursuit siam journal scientiﬁc computing vol. savage chen multigrids solving class improved total variation based staircasing reduction models image processing based partial differential equations ser. mathematics visualization x.-c. k.-a. chan osher eds. springer berlin heidelberg ambrosio tortorelli approximation functional depending jumps elliptic functional t-convergence communications pure applied mathematics vol. felzenszwalb huttenlocher efﬁcient graph-based image segmentation international journal computer vision vol. cand`es donoho curvelets? surprisingly effective nonadaptive representation objects edges curves surface fitting saint-malo cohen schumaker eds. vanderbilt university press nashville davies elad gribonval recovery cosparse signals greedy analysis pursuit presence noise ieee international workshop computational advances multisensor adaptive processing weickert bruhn brox papenberg survey variational optic methods small displacements mathematical models registration applications medical imaging ser. mathematics industry. springer berlin heidelberg vol.", "year": 2014}