{"title": "Fast Sampling for Bayesian Max-Margin Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach.", "text": "bayesian max-margin models shown superiority various practical applications text categorization collaborative prediction social network link prediction crowdsourcing conjoin ﬂexibility bayesian modeling predictive strengths max-margin learning. however monte carlo sampling models still remains challenging especially applications involve large-scale datasets. paper present stochastic subgradient hamiltonian monte carlo methods easy implement computationally efﬁcient. show approximate detailed balance property subgradient reveals natural validated generalization ordinary hmc. furthermore investigate variants stochastic subsampling thermostats better scalability mixing. using stochastic subgradient markov chain monte carlo efﬁciently solve posterior inference task various bayesian max-margin models extensive experimental results demonstrate effectiveness approach. bayesian max-margin models shown effective many real-world applications text analysis collaborative prediction social network link prediction crowdsourcing models conjoin advantages discriminative max-margin learning ﬂexible bayesian models achieve best worlds obtaining ﬂexibility bayesian model meanwhile discriminative max-margin learning newly-developed uniﬁed bayesian inference framework regularized bayesian inference order deal large-scale datasets developing effective scalable inference methods crucial problem bayesian max-margin models becoming norm many application areas. previous variational-approximation-based inference methods raised solve models meanﬁeld assumptions posterior distributions models nonparametric bayesian priors variational methods need adopt model truncation ﬁnish variational approximation moreover inference scheme solving support vector machine subproblems time-consuming motivated developments gibbs classiﬁer formulation data augmentation-based gibbs sampler bayesian inference conjugate prior easily derive closeform posterior however models usually non-conjugate non-smoothness hinge loss often involved unnormalized pseudo-likelihood. straightforward gibbs sampler applicable non-conjugacy. newly discovered data augmentation technique augmented gibbs sampler achieves accurate posterior sampling truncation-free nonparametric models however gibbs samplers data augmentation efﬁcient either high-dimensional spaces often involve inverting large matrices moreover beneﬁt introducing extra variables would counteracted view extra computation dealing extra sampling variables paper present subgradient-based hamiltonian monte carlo methods models directly draw samples original posterior instead augmented one. adopting mild conditions posterior functions show approximate detailed balance property subgradient methods. using stochastic subgradient estimation develop stochastic subgradient mcmc fast computation. annealing discretization stepsizes properly stochastic subgradient mcmc methods approximately converge target posteriors basic bayesian fairly efﬁciently. apply stochastic subgradient mcmc different types models latent variables design different inference algorithms latent structure discovery including nonparametric bayesian model. stochastic subgradient mcmc achieve dramatically fast sampling meanwhile draw accurate posterior samples. carry extensive empirical studies large-scale applications show effectiveness scalability presented stochastic subgradient mcmc methods models. note several previous attempts using subgradient information langevin monte carlo work stands ﬁrst close investigation give theoretical guarantee carry systematic studies stochastic subgradient mcmc bayesian max-margin learning. ﬁrst brieﬂy review bayesian max-margin models gibbs classiﬁers. then introduce background knowledge inference methods including hamiltonian monte carlo extension well stochastic gradient hamiltonian monte carlo. generic framework regbayes design ﬂexible bayesian models adding proper regularization target posterior. namely adding posterior regularization functionaloptimization-reformulated bayesian model regbayes model generally solves following problem denotes model feasible space probability distributions ||π) divergence target posterior prior observation dataset; nonnegative regularization parameter well-designed regularization term hard show equals solution problem bayes posterior zero extra dimension freedom introduce side information inference procedure posterior regularization term example regularization deﬁned hinge loss supervised learning tasks regbayes models turn bayesian max-margin models successfully incorporate ﬂexibility bayesian models max-margin classiﬁers. strategy demonstrated promising performance various tasks including text classiﬁcation topic extraction social network analysis matrix factorization paper consider examples bayesian max-margin models latent variables including max-margin topic model inﬁnite methods applied models. speciﬁcally medlda uses topic model latent topic representations documents uses max-margin classiﬁer document classiﬁcation. inﬁnite generally uses bayesian nonparametric dirichlet process prior describe data multi-modality meanwhile uses max-margin classiﬁers discriminative tasks. details examples provided along development proposed fast samplers them. supervised learning setting generally types classiﬁers used bayesian model deﬁne model namely expected classiﬁers gibbs classiﬁers. part give introduction formulations analyze merits choosing gibbs classiﬁers. given training set. data point denotes input features corresponding label binary multi-valued. build classiﬁer bayesian max-margin model either input features learn latent features. denote features classiﬁer. consider linear classiﬁer parameterized labels binary prediction rule deﬁned setting expected classiﬁer learns posterior distribution hypothesis space smallest possible risk indicator d)]) d)]) hinge loss function regard data point cost making ydeq wrong prediction. then regbayes formulation deﬁne model d)]). known hinge loss upper bounds training error alternatively gibbs classiﬁer draws classiﬁer according uses classiﬁcation proven nice generalization performance gibbs classiﬁer corresponding loss expected hinge loss then expected hinge loss also upper bound expected training error gibbs classiﬁer ˆyd)]. therefore gibbs classiﬁer formulation gives relaxed model time obtain uncertainty draw single model time. addition gibbs classiﬁers truncation-free sampling performed models bayesian nonparametric priors accurate variational approximation. models gibbs classiﬁers already shown better performance classiﬁcation results efﬁciency inference algorithms popular mcmc inference method hamiltonian monte carlo also known hybrid monte carlo hamiltonian monte carlo built molecular dynamics advantage random walk metropolis gibbs sampling proposing distant move high acceptance probability. recently stochastic extensions developed fast sampling. assuming differentiable potential energy sampler infer posterior distribution simulating dynamics discretization integrators euler leapfrog. speciﬁcally using conventional leapfrog integrator stepsize method performs following steps challenge gradient-based methods dealing massive data expensive evaluation posterior gradient save time unbiased noisy gradient estimate constructed subsampling whole dataset stochastic optimization idea ﬁrst proposed develop stochastic gradient langevin dynamics later extended stochastic gradient friction stochastic gradient thermostats. stochastic mcmc methods gradient log-posterior estimated brieﬂy review stochastic gradient thermostats stochastic gradient nos´e-hoover thermostat sgnht uses simple euler integrator introduces thermostat variable control momentum ﬂuctuations well injected noise. dynamics simulated stochastic gradient mcmc methods shown weak posterior-mean convergence instead strong sample-wise convergence weak convergence sufﬁcient many real-world applications. central part methods gradient log-posterior. however gradient might always available. section investigate general subgradientbased method analyze theoretical properties fast inference bayesian linear svms. log-posterior non-differentiable gradient-based applicable. using general subgradients could potentially address problem analogy subgradient descent methods deterministic optimization plugging posterior subgradient ordinary come subgradient theoretical perspective able readily analyze volume preservation property hamiltonian dynamics non-differentiable potential energy detailed balance general subgradient sampler. instead give approximated theoretical analysis based several practical assumptions potential energy. practical bayesian models non-smoothness posterior often lies hinge loss induced likelihoods mainly considered paper. posteriors continuous everywhere piecewise smooth ﬁnite number non-smooth points. sampler non-differentiable states probability zero. practical assumptions show following approximate detailed balance property claims subgradient satisﬁes detailed balance property polynomial smooth potential energy ﬁrst give polynomial smooth potential energy continuous piece-wise differentiable posterior non-smooth ﬁnite {si}m \u0001-neighborhoods around deﬁned {θ|θ setting small enough \u0001-neighborhoods mutually disjoint using mutually disjoint neighborhoods constructed neighborhood bounded subgradient equivalent drawing samples smooth posterior instead. approximation subgradient satisﬁes detailed balance thus valid generating approximate samples true posterior give intuitive illustration theoretical analysis. fig. construct several polynomial smooth functions continuous non-smooth function seen small close it’s unlikely sampler ﬁnite samples neighborhoods obtain version stochastic subgradient langevin dynamics replacing gradient logposterior subgradient. formally ssgld generates samples simulating following dynamics existing sgld methods recommended polynomial decaying stepsize save correction step langevin proposals. stepsize properly decays markov chain would gradually converge target posterior. subtle part method thus tuning discretization stepsize. pre-speciﬁed annealing scheme would make chain either miss oscillate around target. recent work recommends relatively optimal scheme sgld. inspired adaptive stepsizes gradient descent methods paper adopt adaptive stepsize setting ssgld methods shall experiments scheme beneﬁcial yield faster mixing speeds. derive stochastic subgradient hamiltonian monte carlo likewise. adopt improved version stochastic gradient derive stochastic subgradient nose-hoover thermostat generates samples following iterations stochastic subgradient mcmc bayesian linear svms stochastic subgradient mcmc used fast sampling bayesian linear svm. given training dataset n-dimensional feature vector d-th instance binary label. linear classiﬁers weight vector decision rule naturally sgnx). bayesian linear model interested learning prior commonly standard normal distribution per-datum unnormalized likelihood expxd)). then subgradient log-posterior involves evaluating subgradient non-differentiable log-likelihood show leverage stochastic subgradient mcmc methods derive fast sampling algorithms bayesian max-margin models latent variables. develop algorithms different models latent variables. parametric models whose model parameter number ﬁxed calculate log-posterior subgradient stochastic subgradient mcmc method. part gibbs medlda example show fast sampling parametric models. illustrated fig. max-margin topic model parts latent dirichlet allocation model modeling underlying topic structures given documents max-margin classiﬁer predicting document labels. part hierarchical bayesian model uses admixture topics {φk}k latent document representation. topic multinomial distribution -word vocabulary symmetric dirichlet prior dir. single document words generated detailed process gibbs classiﬁer formulation build gibbs medlda model. drawn sample topic assignments classiﬁer weights posterior distribution prediction document label collapsed posterior medlda sample classiﬁers using stochastic subgradient mcmc sample topic model parameters using sgrld method randomly-drawn document minibatch stochastic subgradient posterior respect stochastic posterior gradients respect calculated given expectation calculate expectation gibbs sampling iterations topic assignments document follows draw stochastic subset draw topic assignments documents using eqn. compute stochastic posterior gradient respect subgradient sampler stochastic posterior subgradient another important type bayesian max-margin models latent variables uses bayesian nonparametric priors. models deﬁned inﬁnite-dimensional spaces size models learned data. typical example type inﬁnite hmc-within-gibbs strategy build fast sampling methods type models. real world data often latent clustering structures mixture-of-experts models generally capable capturing local structures. expert linear resultant mixture svms learns non-linear model instead simply linear recent work presents nonparametric extension inﬁnite automatically infers number experts. below apply subgradient-based fast sampling method inﬁnite svm. though alternative approaches exist deﬁne expert classiﬁer gibbs classiﬁer uncertainty assignments classiﬁer weights namely given posterior distribution gibbs classiﬁer draws component assignment classiﬁer data point makes prediction develop fast sampling method gibbs isvm incorporating stochastic subgradient mcmc method within loop gibbs sampler. hmc-within-gibbs strategy isvm detailed below. prior. hyper-parameter prior n−dk number data points belong component except given classiﬁers assignments data points sample component assignments normalizing following probabilities n−dkψ standard normal prior. proposed stochastic subgradient mcmc classiﬁers directly sampled using minibatch whole dataset. here give stochastic subgradients conditional distribution subgradients multi-class hinge loss similarly deﬁned eqn. using subgradient ssgld ssgnht derive stochastic subgradient inner sampler classiﬁers implement stochastic subgradient mcmc various bayesian max-margin models including basic bayesian linear sophisticated bayesian max-margin models latent variables results demonstrate stochastic subgradient mcmc achieve great improvement time efﬁciency meanwhile still generating accurate posterior samples. experiments done desktop computer single-core rate .ghz. stepsize parameter iteration decays normally classiﬁer topic-word parameter choose grid search. furthermore adagrad stepsizes considered stochastic subgradient langevin dynamics method. ﬁrst consider basic bayesian linear model compare stochastic subgradient sampling methods gibbs sampler data augmentation random walk metropolis stochastic test ﬁrst test methods synthetic dataset show methods give correct samples posterior distribution. note view results experiment simple proof idea hence choose direct visual comparison. follow bayesian linear model deﬁned section generate observations synthetic dataset. speciﬁcally generate features uniform i.i.d∼ coefﬁcient vector normal distribution given distribution features coefﬁcients labels generated bernoulli distribution parameter where compare samples obtained ssgld ssgnht data augmentation method accurate sampler bayesian svms. take samples method sufﬁciently long burn-in stage give comparison fig. densities obtained samples shown grayscales grids. results suggest stochastic subgradient mcmc methods accurate although stochastic subsampling neglect test bring noise. result compatible previous weak convergence analysis ordinary methods test stochastic subgradient mcmc methods ssgld ssgnht realsim dataset larger higgs dataset higgs dataset contains samples -dimensional feature space. randomly choose samples training rest testing set. realsim dataset stochastic batchsize stochastic inference methods. higgs dataset ssgld srwm ssghnt. tuned polynomial decaying stepsizes stochastic subgradient mcmc methods speciﬁcally ssgld prefer adaptive stepsize adagrad successfully applied stochastic gradient descent srwm variance parameter turn good setting analyzed following sensitivity analysis section convergence curves various methods respect running time datasets shown fig. stochastic subgradient mcmc methods several magnitudes faster baseline methods. compared gibbs sampling data augmentation method stochastic subgradient mcmc methods much cheaper updates hence scalable. specially larger higgs dataset single update gibbs sampling ﬁnished stochastic subgradient mcmc converged. furthermore although srwm stochastic subgradient mcmc stochastic minibatches sensitivity analysis tuning batchsize reﬂects accuracy-efﬁciency trade-off analogous bias-variance tradeoff stochastic monte carlo sampling general using smaller batchsize often leads larger injected noise computation cost iteration reduced linear batchsize cross validation select parameters accuracy time efﬁciency factors taken consideration. fig. presents sensitivity analysis batchsize stochastic subgradient mcmc methods higgs realsim datasets. performance stochastic subgradient mcmc appears fairly promising except extremely tiny batchsizes. experiments adaptive stepsizes bring better mixing rate polynomial decaying stepsizes. result ﬂexible stepsize decaying different dimensions. also give empirical analysis fig. seen higgs dataset adaptive stepsizes bring better classiﬁcation results pre-deﬁned polynomial-decaying stepsizes. implement fast sampling gibbs medlda. show efﬁciency accuracy stochastic subgradient riemannian langevin dynamics using news dataset larger wikipedia dataset. following dataset setting stop words removed according standard list. compare ssgrld data augmentation newly developed extension online bayesian passive-aggressive learning framework smaller news dataset involved three methods binary version adopt one-vs-all strategy multi-class classiﬁcation. larger wikipedia dataset ssgrld method uses multi-class setting multi-task formulation described ﬁrst test news dataset consists training documents categories. hyper-parameters suggested fig. shows number documents processed order reach speciﬁc accuracy score topic number stochastic samplers much fewer documents efﬁciently explore data redundancy using minibatch iteration. test larger wikipedia dataset consists million training documents categories. hyper-parameter setting news dataset except settings ssgrld gibbs medlda pamedlda-gibbs. topic number fig. shows f-scores function time. seen ssgrld produces comparable classiﬁcation results. efﬁciency ssgrld pamedlda-gibbs order magnitude efﬁcient previous gibbs medlda. minibatch training. meanwhile although magnitude ssgrld still faster pamedlda-gibbs. argue ssgrld augmented variables directly draws samples classiﬁer. moreover matrix inversion involved data augmentation technique costly whole procedure. finally visualize discovered topic representations ssgrld news dataset. categories show average topic representations documents form category. fig. average topic distribution corresponding classiﬁer sparse also give representative words salient topic category table. words salient topic highly related category information. example salient topic learned classiﬁer sci.space words nasa launch moon satellite etc. patterns similar choose datasets protein ijcnn test methods. protein dataset created protein fold classiﬁcations consists samples classes features. ijcnn dataset originated engine system binary classiﬁcation problem consists training implement inference methods isvm including ssgnht within gibbs gibbs sampling data augmentation models also implemented comparison multinomial logit model linear rbf-svm mixture generalized linear models cross-validations choose hyper-parameters results table. nonlinear models using mixture-of-experts gisvm dpmnl superior classiﬁcation. stochastic subgradient mcmc sampling step dramatically accelerated comparable even better prediction performance. superiority results stochastic subsampling avoiding matrix inversion data augmentation technique. systematically investigate fast sampling methods bayesian max-margin models. ﬁrst study general subgradient sampling method several stochastic variants including ssgld ssgnht. theoretical analysis shows approximated detailed balance proposed stochastic subgradient mcmc methods. apply stochastic subgradient samplers bayesian linear svms sophisticated bayesian max-margin models latent variables extensive empirical studies demonstrate effectiveness stochastic subgradient mcmc methods improving time efﬁciency maintaining high accuracy samples. sampling method data augmentation; accurate sampling good gibbs sampling data augmentation applications non-conjugate posterior sampling cannot simply accomplished. however data sizes applications large processed single machine still difﬁcult stochastic subgradient mcmc solve problem. consider future work three categories algorithm-level model-level application-level. proposed algorithm itself future work includes scaling using parallel computation model setting future work includes applying method models continuous non-smooth posteriors sparse models laplacian priors. application level consider using method scale several bayesian max-margin models used intelligent systems nonparametric max-margin matrix factorization collaborative ﬁltering data identiﬁed important building block intelligent systems fast inference becoming central element therein related bayesian models learning bayesian models recent research focuses particularly bayesian max-margin models well studied various machine learning applications still lack fast inference methods. method accomplishes fast sampling models used future large scale intelligent systems.", "year": 2015}