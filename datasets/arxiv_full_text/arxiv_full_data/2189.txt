{"title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement  Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We present a new deep meta reinforcement learner, which we call Deep Episodic Value Iteration (DEVI). DEVI uses a deep neural network to learn a similarity metric for a non-parametric model-based reinforcement learning algorithm. Our model is trained end-to-end via back-propagation. Despite being trained using the model-free Q-learning objective, we show that DEVI's model-based internal structure provides `one-shot' transfer to changes in reward and transition structure, even for tasks with very high-dimensional state spaces.", "text": "present deep meta reinforcement learner call deep episodic value iteration devi uses deep neural network learn similarity metric non-parametric model-based reinforcement learning algorithm. model trained end-to-end back-propagation. despite trained using model-free q-learning objective show devi’s model-based internal structure provides ‘one-shot’ transfer changes reward transition structure even tasks high-dimensional state spaces. deep reinforcement learning paradigm popularized uniquely capable obtaining good asymptotic performance complex high-dimensional domains. using highly non-linear function-approximators traditional model-free reinforcement learning algorithms solve highly non-linear control problems. work matured interest shifted asymptotic performance sample complexity task transfer. well known model-free methods provide poor transfer knowledge transition reward functions \"baked representation value function. meta-learning begun appear possible solution issues sample complexity deep learning models suffer from. treating ‘solve task examples’ instance metatask slow gradient based learning applied across tasks within-task learning delegated forward pass network. traditional networks architectures could principle much current success comes networks designed parametrize speciﬁc nonparametric algorithm however previous attempts focused supervised learning especially few-shot classiﬁcation. recently attempts meta reinforcement learning none utilize non-parametric internal structures proven vital classiﬁcation context reinforcement learning ﬁeld around decades rich variety algorithmic approaches work utilizing non-linear function approximation focused model-free algorithm q-learning. section review algorithm alongside value iteration kernel-based reinforcement learning model-based reinforcement learning algorithms forms foundation approach. q-learning consider discrete action markov decision process possible states ﬁnite discrete actions reward function mapping state-action-next-state tuples scalar immediate reward state transition function mapping state-action-next-state tuples probability transitions occurrence discount factor weighing immediate versus future rewards. wish policy maximizes expected cumulative discounted reward eliminated explicit usage second equation evaluated without dynamics model simply taking actions observing consequences bellman equation holds optimal value function. thus view difference error relationship value estimates time since sample directly true reward function makes sense treat target value lhs. making value symmetric loss function training parametric function approximator deep neural network optimizing temporal difference loss using stochastic gradient descent non-trivial sampling non-trivial dependency current estimated target values. problems addressed using random samples ‘replay buffer’ past transition samples address concerns slow changing ‘target network’ break dependence estimated target values. despite focus classiﬁcation matching networks vinyals closest prior work approach. shown ﬁgure matching networks kernel-based averaging estimate label probabilities learn latent-space kernel applied end-to-end. relating approach reinforcement learning problem could imagine using kernel relate novel state states known values blundell nearest neighbor kernel empirical returns value estimates previously encountered states embedding space used kernel based either random projections learnt unsupervised fashion kernel could theoretically learned end-to-end manner based loss predicted actual returns. episodic neural control recently made something akin extension explored greater detail discussion section. however works constrained deterministic mdps frequently revisited states limiting overall applicability. kernel-based reinforcement learning non-parametric reinforcement learning algorithm several advantageous properties model-based reinforcement learning algorithm figure matching network architecture neuralnetwork used embed query image latent space alongside several images known labels similarity vector produced passing query latent vector cosine kernel known latent vector. vector normalized multiplied vector known labels produce estimate query image’s label. operations differentiable supervised loss back-propagated parameters embedding network. relies stored empirical transitions rather learning parametric approximation. reliance empirical state reward transitions results sample complexity excellent convergence results. insight algorithm stored transitions paired similarity kernel represent resultant state distribution origin states approximate real arbitrarily closely advantage closed states whose transitions sampled environment. case note that empirically know state transitions stored origin states since dealing continuous space state resulting transitions unlikely stored transition using similarity kernel replace unknown resultant state distribution origin states. makes state transitions instead meaning transitions involve states experienced empirically. since ﬁnite number states states known transitions perform value iteration obtain value estimates resultant convergence global optima assuming underlying dynamics lipschitz continuous kernel appropriately shrunk function data. result doesn’t hold non-linear function approximator used adapt kernel seems converge empirically proper hyper-parameter tuning. kbrl algorithm great theory suffers so-called ‘curse dimensionality’ dimensionality state space grows number exemplars needed cover space grows exponentially. taking kbrl algorithm making similarity kernel output deep neural network force data live lower dimensional space. random projections could accomplish thing embedding function optimized overall performance able much better focusing performance relevant aspects observed states. probability select random action otherwise select maxaq execute action observe reward observation store transition sample random minibatch transitions loss resulting model deep episodic value iteration kbrl applied state representation learned q-learning. representation output deep neural network weights tied query states encoded using function states episodic store. ensure that learning parametric encoder doesn’t rely upon particular states episodic store random subsample episodic store used given training step. order keep memory usage relatively constant training size subsampled experiences held constant though fundamental objection using growing kbrl. size computation graph grows linearly planning horizon devi uses ﬁxed number value iteration steps rather iterating convergence simple tasks considered here trivial determine maximum number value iteration sweeps required compute value function adaptive computation techniques designed recurrent neural networks able automate process demanding environments. devi trained using q-learning improvements made since inﬂuential paper could also incorporated. however unique problem devi faces long gradient path corresponding value iteration steps. structure computational graph means paths corresponding shorter term predictions last receive gradient information. possible eliminate problem using values step value iteration calculate predicted value current state-action pair. thus steps value iteration performed model different predictions value corresponding different planning horizons. amenable q-learning losses simple averaged calculate loss. primary objective work demonstrate ‘one-shot learning’ possible reinforcement learning problems using deep neural networks. prior work demonstrating phenomenon we’ve designed task domain proof concept minimalistic possible still allowing variety tasks requiring non-linear state representations. omniglot dataset consists pixel grey-scale images depicting characters real ﬁctional languages. often referred ‘transpose mnist’ different classes character examples class. limited sample size quickly become standard bearer ‘one-shot’ learning however format dataset conducive ‘one-shot’ learning classiﬁcation. extend dataset proof-of-concept ‘one-shot’ reinforcement learning. omniglot graph world task domain consisting mdps small ﬁnite number states randomly assigned character class omniglot dataset. rather observe states directly agent access sample corresponding omniglot class. trivially easy solve isolation performance criterion interest number samples required learn task domain given prior exposure tasks. domain interesting tasks share similarity structure differing greatly state reward transition structure. number tasks contained within omniglot graph world huge vary widely difﬁculty possibility degenerate transition structures deﬁnition one-shot learning ability solve novel task without parameter updates episodic store must still populated transitions task requires orders magnitude fewer samples parametric adaptation. figure three sub-domains within omniglot graph world. nodes represent states edges representing connectivity resulting possible actions. agent observes images characters omniglot dataset rather states themselves mapping randomly drawn task. additionally relative reward locations action semantics permuted. ring task distribution consists states single positive terminal reward. hard ring task distribution consists states large terminal negative reward small non-terminal reward. tree task distribution states form full binary tree leaf nodes except giving negative terminal rewards. analogously work ‘one-shot’ classiﬁcation trained devi interleaved manner across tasks randomly sampled task domain. rather interleave within mini-batches tasks switched every step ‘burn steps taken small replay buffer sample minibatch tuples q-learning. trained minibatches initial transfer tasks. replay buffer used capacity ﬁlled prior start learning avoid tainting sample complexity analysis. focus evaluation task transfer rather within task exploration decided train models uniform random behavior policy. ensures data provided different algorithms equivalent performance differences aren’t structured exploration. also modiﬁed devi’s episodic store always contains samples state. randomly sampling replay buffer works similarly method allows useful comparison work ‘one-shot’ learning classiﬁcation. basic deep neural network architecture used algorithms taken directly matching networks paper consists four convolutional layers ﬁlters interleaved max-pooling. followed densely connected layer relu activations. used encode latent space devi represent value function dqn. however since output dimensionality tied number actions ﬁnal linear layer added. adam optimizer used algorithms batch-size default hyperparameters used though preliminary grid search showed didn’t signiﬁcantly impact performance across wide range values. likewise extensions considered none yielded qualitatively different results. while intelligent exploration isn’t free byproduct approach easily obtained exploiting pseudo-tabular nature learned model. extension r-max whereby neighbors within ﬁxed distance substitute visitation counts shown quite effective. figure results omniglot graph task domain. instances devi initially trained using random seeds. average performance time initial phase shown black dqn. devi trained distribution tasks performance curve shown. models used initial weights transfer learning. transfer attempts performed model averaged together average across models bold. case devi transferred weights frozen emphasize non-parametric nature generalization. figure shows devi able instantly generalize tasks even though none characters representing underlying states ever seen addition test task distributions qualitatively different state reward transition structures training task distribution. expected exhibited little transfer needing samples order magnitude networks initialized random weights. differences models give evidence claim trained using q-learning model-free reinforcement learning algorithm devi’s non-parametric internal structure allows learning representations work across wide array tasks. precisely structure paired interleaved learning forces devi learn appropriate latent space kernel-based planning. initial experiments demonstrate possibility meta model-based reinforcement learning remains unclear well approach scale realistic domains. recently several papers proposing model-free meta reinforcment learning methods interesting open question devi’s model-based generalization compares fast model-free adaptation. similar among recent work model-free meta reinforcement learning neural episodic control interestingly isn’t presented meta learning algorithm rather sample-efﬁcient learner. despite this seen offspring matching networks uses kernel-based averaging weight values instead labels. shares devi idea training encoding network using q-learning differs storing latent space episodic store meaning encoder updated queried state states compared instead latent vectors treated like parameters stored modiﬁed back-propagation. additionally similarity approximated comparing query approximate nearest neighbors. tricks allow scale complex tasks atari environment. despite similar titles ‘value-iteration networks’ work related quite distinct. work authors assumed underlying state-space local transitions. allowed perform efﬁcient value iteration noticing speciﬁc topology value iteration algorithm performed convolution. additionally approach made reward value function internal foregoing attempt learn veridical reward function. work presented shares neither qualities though interesting open-question approach likely scale complex domains. pragmatic concern architecture scales poorly size episodic store grows. cognitive science literature well known ﬁxed episodic stores augmented traditional neural networks incorporate knowledge distant past though empirically validated large scale model. immediate solution would sparsify similarity matrices zeroing nearest neighbors row. could theoretically make gradient propagation problematic recent related work neural episodic control sparsiﬁed neural turing machines suggests approach quite effective empirically devi similarity-based model-based worth emphasizing even without interleaved learning multiple tasks signiﬁcant generalization possible. long optimal q-values initial task don’t alias state-action pairs important transfer task ‘one shot’ transfer possible extent tasks share similarity structure. example changing reward structures ﬁxed environment would guarantee value aliasing would prevent transfer. work so-called successor representation ﬂeshes similar argument though architecture imposes harsher limitation immediate reward aliasing additional constraints owing on-policy nature recently researchers noted humans’ ability react changes task structure surpasses deep reinforcement learning approaches speciﬁcally authors note even extreme alterations reward structure readily solvable humans deep learning systems. previously mentioned model unique able rapidly adjust changes reward structure. several technical hurdles overcome believe devi able tackle challenge within immediate future.", "year": 2017}