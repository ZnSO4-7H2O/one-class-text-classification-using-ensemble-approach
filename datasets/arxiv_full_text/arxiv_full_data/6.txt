{"title": "Generating Sentences by Editing Prototypes", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.", "text": "indeed difﬁcult even humans write complex text scratch single pass; often create initial draft incrementally revise inspired process propose generative model text call prototype-then-edit model illustrated figure ﬁrst samples random prototype sentence training corpus invokes neural editor draws random edit vector generates sentence attending prototype conditioning edit vector. motivation sentences corpus provide high quality starting point grammatical naturally diverse exhibit bias towards shortness vagueness. attention mechanism extracts rich information prototype generalizes novel sentences. propose generative model sentences ﬁrst samples prototype sentence training corpus edits sentence. compared traditional models generate scratch either left-toright ﬁrst sampling latent sentence vector prototype-then-edit model improves perplexity language modeling generates higher quality outputs according human evaluation. furthermore model gives rise latent edit vector captures interpretable semantics sentence similarity sentence-level analogies. ability generate sentences core component many systems including machine translation summarization speech recognition dialogue state-of-the-art models largely based recurrent neural language models generate sentences scratch often left-to-right manner. often observed nlms suffer problem favoring generic utterances don’t know time naive strategies increase diversity shown compromise grammaticality suggesting current nlms lack inductive bias faithfully represent full diversity complex utterances. formulation stems observation many sentences large corpus represented minor transformations sentences. example yelp restaurant review corpus test within wordtoken jaccard distance training sentence even though almost sentences repeated verbatim. implies neural editor models lexically similar sentences effective generative model large parts test set. edit vector model/control variation type edit performed. apply edit vector different sentences neural editor perform semantically analogous edits across sentences. would like train neural editor maximizing marginal likelihood exact likelihood difﬁcult maximize involves latent prototypes integration latent edit vector approximation generative model’s loglikelihood. objective lexicallysimilar sentence pairs training scalably approximate using locality sensitive hashing. also show empirically lexically similar sentences also semantically similar thereby endowing neural editor additional semantic structure. example neural editor perform random walk seed sentence traverse semantic space. compare prototype-then-edit model approaches generate scratch fronts language generation quality semantic properties. former model generates higher quality generations according human evaluations improves perplexity points yelp corpus points billion word benchmark. latter show latent edit vectors outperform standard sentence variational autoencoders semantic similarity locally-controlled text generation sentence analogy task. many applications machine translation dialogue generation context supplied prototype selector neural editor. paper focuses unconditional case. sentences corpus grounded real world events lexically similar sentences also semantically similar. example given enjoyed delicious pizza likely enjoyed delicious macaroni versus hated delicious pizza. human evaluations edit pairs sampled lexical similarity neighborhoods yelp corpus support conclusion. sentence pairs judged exact paraphrases pairs judged least roughly equivalent. sentence pairs negated change topic time. thus neural editor trained distribution preferentially generate semantically similar edits. lelbo maximized parameters pedit note used training discarded test time. introduction training conditional variational autoencoder maximize objective approximating gradient using usual monte carlo estimate vaes neural editor pedit implement neural editor left-to-right sequence-to-sequence model attention prototype input sequence revised sentence output sequence. employ encoder-decoder architecture similar extending condition describe motivate approximations subsections below. approximate prototypes equation deﬁnes probability generating sentence total probability reaching edits every prototype however prototypes unrelated small probability transforming therefore approximate marginal distribution prototypes considering prototypes high lexical overlap measured word token jaccard distance formally deﬁne lexical similarity neighborhood neighborhoods constructed efﬁciently locality sensitive hashing minhashing. full procedure described appendix inequalities follow summing fewer terms multiplying number jensen’s inequality. treating constant summing training arrive following objective interlude lexical similarity semantics. motivated lexical similarity neighborhoods computational considerations lexical similarity training also captures semantic similarity. certainly construct sentences small lexical distance differ semantically however since mine resulting zdirznorm. resulting distribution three parameters hyperparameters. distribution straightforward part variational autoencoder sampling easily performed using reparameterization trick rejection sampler wood furthermore term closed form expression independent first construction edit vectors sums word vectors since cosine distances traditionally used measure distances word vectors would natural encode distances between edit vectors cosine distance. vonmises fisher distribution captures idea likelihood transforming fdir zdir decays linearly cosine similarity. second parameterization avoids collapsing latent code serious problem variational autoencoders practice gaussian latent noise variable kl-divergence term instance-dependent thus encoder must decide much information pass decoder. even training techniques annealing model often learns ignore encoder entirely. case occur since divergence von-mises fisher parameter uniform distribution independent mean fdir allowing optimize separately using binary search. practice never observe issues encoder collapse using standard gradient training. edit prior sample prior drawing random magnitude znorm unif drawing random direction zdir uniform distribution unit sphere resulting znormzdir. approximate edit posterior named approximate edit posterior elbo tight matches true posterior best possible estimate edit given prototype revision design treats edit vector generalization word vectors. case single word insertion good edit vector would word vector inserted word. extending multi-word edits would like multi-word insertions represented individual word vectors. cannot deterministically output without entropy term equation would inﬁnity training would infeasible. hence design output noisy perturbed version perturb norm adding uniform noise perturb direction adding von-mises fisher noise. vonmises fisher distribution distribution vectors unit norm mean precision log-likelihood drawing vector decays linearly cosine similarity divide experimental results parts. section evaluate merits prototypethen-edit model generative modeling strategy measuring improvements language modeling generation quality section focus semantics learned model latent edit vector space. demonstrate possesses interpretable semantics enabling smoothly control magnitude edits incrementally optimize sentences target properties perform analogy-style sentence transformations. evaluate perplexity yelp review corpus billion word language model benchmark qualitative evaluations generation quality semantics focus yelp primary test case found human judgments semantic similarity much better calibrated focused setting. corpora used named-entity recognizer spacy replace named entities categories. replaced tokens outside frequent tokens out-of-vocabulary token. svae sentence variational autoencoder bowman sharing decoder architecture neuraleditor. compare edit vector differences sentence vectors svae. generative modeling perplexity. start evaluating neuraleditor’s value language model measured terms perplexity. neuraleditor likelihood lower bound equation training instances within jaccard distance term neuraleditor one-sample approximation lower bound used kingma bowman compared initial result neuraleditor able drastically improve loglikelihood signiﬁcant number sentences test considering test sentences least similar sentence training set. however places lower log-likelihood sentences prototypes trained make extremely large edits. proximity prototype seems chief determiner neuraleditor’s performance. evaluate neuraleditor’s perplexity smoothing account rare sentences within jaccard distance threshold. neuraleditor improves perplexity table shows case yelp general billionword contains substantially fewer test-set sentences close training set. yelp surpass even best ensemble billionword nearly match performance. since neuraleditor draws strength sentences training also compared simpler alternative ensemble memorization neuraleditor performs dramatically better alternative. table also qualitatively demonstrates sentences generated human evaluation. turn human evaluation generation quality focusing grammaticality plausibility. evaluate generations neuraleditor temperature parameter per-token softmax popular technique suppressing incoherent ungrammatical sentences. many systems noted undesireable tradeoff grammaticality diversity temperature enough enforce grammaticality results short generic utterances figure illustrates grammaticality plausibility neuraleditor temperature best tuned temperature higher diversity measured unigram entropy. also decreasing temperature neuraleditor used slightly improve grammaticality without subprototype fried whiteﬁsh taco decent i’ve much better. \"hash browns\" unseasoned frozen potato shreds burnt crisp outside mushy inside. sure preventing giving <cardinal> stars probably should. quick place grab light tasty teriyaki. advantage edit-based models thus emerges prototypes sampled training organically inject diversity generation process even temperature decoder neuraleditor zero. hence keep decoder temperature maximize grammaticality plausibility without sacriﬁcing sample diversity. contrast zero temperature would collapse outputting generic sentence. also suggests temperature parameter neuraleditor captures natural notion diversity higher temperature encourages aggressive extrapolation training lower temperatures favor conservative mimicking. likely useful tradeoff generation-from-scratch temperature also affects quality generations. figure neuraleditor provides plausibility grammaticality best temperature-tuned language model without loss diversity function temperature. results based human evaluations. semantics neural editor section investigate learned semantics neuraleditor focusing desiderata discussed section semantic smoothness consistent edit behavior. order establish baseline properties consider existing sentence generation techniques sample semantically similar sentences. aware approaches attempt learn vector space edits many approaches learn vector space sentences. particular relevance sentence variational autoencoder also imposes semantic structure onto latent vector space uses latent vector represent entire sentence rather edit. svae edit target sentence semantically similar sentence perturb underlying latent sentence vector decode result back sentence method used bowman semantic smoothness. good editing system ﬁne-grained control semantics sentence i.e. edit alter semantics sentence small well-controlled amount. call property semantic smoothness. study smoothness ﬁrst generate edit sequence randomly selecting prototype sentence repeatedly editing neural editor produce sequence revisions. human annotators rate size semantic changes revisions. example given table svae baseline generate similar sequence sentences ﬁrst encoding prototype sentence decoding addition random gaussian variance process figure neural editor frequently generates paraphrases similar sentences avoiding unrelated degenerate ones. contrast svae frequently generates identical unrelated sentences rarely generates paraphrases. figure shows neural editor frequently generates paraphrases despite trained lexical similarity edits unrelated prototype. contrast svae often repeats sentences exactly makes edit equally likely generate unrelated sentences. suggests neural editor produces substantially smoother sentence sequences surprisingly high frequency paraphrases. qualitatively neuraleditor seems generate long diverse sentences smoothly change time svae biases towards short sentences several semantic jumps presumably difﬁculty training sufﬁciently informative svae encoder. raleditor average human similarity judgement successive sentences. avoids situations svae produces completely unrelated sentence perturbation size. similarity assessments pairs collected amazon mechanical turk following agirre scale prompt. similarity judgements converted descriptions deﬁning paraphrase roughly equivalent topic unrelated neuraleditor food amazing best i’ve tried service fast great. best food best service i’ve tried <gpe>. best <norp> food i’ve <date> i’ve lived <gpe>. best <norp> food i’ve <gpe>. best <norp> food i’ve since moving <gpe> <date>. best <norp> food i’ve <gpe>. i’ve lived <gpe> <date> every time come great i’ve lived <gpe> <date> enjoyed haircut <gpe> since <date>. svae food amazing best i’ve tried service fast great. place great place want quick bite. food good service terrible. best <norp> food <gpe>. place great place want eat. best <norp> food <gpe>. food good service great. food good service terrible. table examples word inclusion trajectories ‘pizza’. neuraleditor produces smooth chains lead word inclusion svae gets stuck generic sentences. smoothly controlling sentences. show selectively choose edits sampled neural editor incrementally optimize sentence towards desired attributes. task serves useful measure semantic coverage edit model high coverage sentences semantically similar prototype able satisfy target attribute deviating minimally prototype’s original meaning. given prototype sentence discover semantically similar sentence satisfying target attribute using following procedure first generate edit sequences using procedure described earlier. then select sequence highest likelihood whose endpoint possesses tarfigure aggregate sentences collected edit sequences plot semantic similarity prototype success satisfying target attribute. surprisingly target attribute satisfaction rises semantic similarity drops. however also neural editor sacriﬁces less semantic similarity achieve level attribute satisfaction svae. svae reasonable tasks involving common words fails model asked generate rarer words pizza. examples word inclusion problems show consistent edit behavior sentence analogies. previous results showed edit models learn generate semantically similar sentences. assess whether edit vector possesses globally consistent semantics. speciﬁcally applying edit vector different sentences result semantically analogous edits. evaluation task difﬁcult arbitrary sentences often related well-deﬁned relationship. however prior work already provide well-established sets word analogies leverage generate dataset sentence analogies using simple strategy given analogous word pair mine yelp corpus sentence pairs transformed inserting removing task initially compared svae top-k accuracy close zero. hence instead compare baseline randomly sampling edit vector instead using derived also compare numbers original word analogy task restricted relationships yelp cortop- performance model table nearly good performance glove vectors simpler lexical analogy task despite fact sentence prediction task harder. categories neural editor top- actually performs better word vectors since neural editor understanding words likely appear context yelp review. examples table show model accurate captures lexical analogies requiring word reorderings. work connects broad literature neural retrieval attention-based generation methods semantically meaningful representations sentences nonparametric statistics. neural language models based upon recurrent neural networks sequenceto-sequence architectures widely used ﬂexibility performance across wide-range tasks work motivated emerging consensus attention-based mechanisms substantially improve performance various sequence sequence tasks capturing information input sequence work extends applicability attention mechanisms beyond sequence sequence tasks deriving training method models attend randomly sampled sentences. growing literature applying retrieval mechanisms augment neural sequence-tosequence models. example song ensembled retrieval system dialogue using transform retrieved utterance used off-the-shelf search engine system retrieve condition training examples. approaches rely deterministic retrieval mechanism selects prototype using input contrast work treats prototype latent varitable examples lexical analogies correctly answered neuraleditor. sentence pairs generating analogy relationship shortened lexical differences. able marginalizes possible prototypes challenge motivates lexical similarity training method section practically marginalization makes model attend training examples based similarity output sequences retrieval models attend examples based similarity input sequences. terms generation techniques capture semantics sentence variational autoencoder closest work attempts impose semantic structure latent vector space. however svae’s latent vector meant represent entire sentence whereas neural editor’s latent vector represents edit. results section suggest local variation edits easier model global variation sentences. lexical similarity neighborhoods comparable context windows used word vector training proximity words within text lexical similarity serve ﬁlter reveals semantics distributional statistics corpus. generally results manifold learning demonstrate weak metric lexical similarity used extract semantic similarity distributional statistics semi-parametric approach remembers entire training uses neural editor generalize meaningfully beyond training set. training provides strong inductive bias corpus characterized prototypes surrounded semantically similar sentences reachable edits. beyond improvements generation quality measured perplexity approach also reveals semantic structures edit vector. natural next step apply ideas conditional setting tasks dialogue generation. reproducibility. code made available https//github.com/kelvinguu/neural-editor. reproducible available https//worksheets.codalab.org/worksheets/ xabafbddfcbdeccc. construction maps sentence lexically similar sentences corpus representing graph sentences. speed corpus construction apply breadth-ﬁrst search sentence graph started randomly selected seed sentences. store edges encountered uniformly sample form training set. ensures every query index returns valid edit pair cost adding bias training set. neural editor architecture encoder. prototype encoder -layer bilstm. inputs layer concatenation forward backward hidden states previous layer exception ﬁrst decoder. decoder -layer lstm attention. time step hidden state layer used compute attention toplayer hidden states prototype encoder. resulting attention context vector concatenated decoder’s top-layer hidden state used compute softmax distribution output tokens.", "year": 2017}