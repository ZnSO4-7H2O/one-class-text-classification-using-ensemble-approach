{"title": "An expressive dissimilarity measure for relational clustering using  neighbourhood trees", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones.", "text": "abstract clustering underspeciﬁed task universal criteria makes good clustering. especially true relational data similarity based features individuals relationships them both. existing methods relational clustering strong often implicit biases respect. paper introduce novel dissimilarity measure relational data. ﬁrst approach incorporate wide variety types similarity including similarity attributes similarity relational context proximity hypergraph. experimentally evaluate proposed dissimilarity measure clustering classiﬁcation tasks using data different types. considering quality obtained clustering experiments demonstrate using dissimilarity standard clustering methods consistently gives good results whereas measures work well data sets match bias; data sets novel dissimilarity outperforms even best among existing ones. classiﬁcation tasks proposed method outperforms competitors majority data sets often large margin. moreover show learning appropriate bias unsupervised challenging task existing methods offer marginal gain compared proposed similarity method even hurt performance. finally show asymptotic complexity proposed dissimilarity measure similar existing state-of-the-art approaches. results conﬁrm proposed dissimilarity measure indeed versatile enough capture relevant information regardless whether comes attributes vertices proximity connectedness vertices even without parameter tuning. keywords relational learning clustering similarity structured objects relational learning data contains instances relationships them. standard learning methods typically assume data i.i.d. ignore information relationships. relational learning methods exploit information often results better performance. complex data relational data ubiquitous modern world. among notable examples social networks typically consist network people interacting other. another example includes rich biological chemical data often contains many interaction atoms molecules proteins. finally data stored form relational databases essentially relational data. much research relational learning focuses supervised learning probabilistic graphical models clustering however received less attention relational context. clustering underspeciﬁed learning task universal criterion makes good clustering inherently subjective. known i.i.d. data even true relational data. different methods relational clustering different biases often left implicit; instance methods represent relational information graph assume similarity refers proximity graph whereas methods take relational database stance assume similarity comes relationships objects participate strong implicit biases make clustering algorithm difﬁcult problem hand without deep understanding clustering method problem hand. paper propose versatile framework clustering relational data makes underlying biases transparent user. views relational data graph typed vertices typed edges attributes associated vertices. view similar viewpoint relational databases predicate logic. task consider clustering vertices particular type. distinguishes approach approaches concept similarity used broad. take account attribute dissimilarity dissimilarity relations object participates dissimilarity neighbourhoods interconnectivity graph proximity objects compared. consider example figure relational dataset describes people organizations relationships persons organizations vertices graph shown relationships shown edges attributes shown dashed boxes. vertices clustered different ways google microsoft similar attributes could clustered john rose form densely interconnected cluster rose share property fulﬁll role supervisor non-relational clustering systems yield clusters ﬁrst one; look attributes individuals. graph partitioning systems yield clusters second type. relational clustering systems yield clusters third type deﬁned local structural properties. existing clustering systems strong bias towards their type clusters; graph partitioning system instance cannot possibly come {google microsoft} cluster since connected component graph. clustering approach propose able types clusters even clusters found mixing biases. fig. illustration relational data containing people organizations different clusters might instances people organizations represented vertices relationships among represented edges. rectangles list associated attributes corresponding vertex. within relational learning least three different paradigms exist inductive logic programming uses ﬁrst-order logic representations; relational data mining context relational databases; graph mining relational data represented graphs. illustrate different types representation figure example represents people organizations relationships them. relational database format perhaps familiar people. table entity type relationship type entities table contains multiple attributes fig. representation paradigms relational data. section represents relational data logical facts; upper part represents deﬁnition predicate bottom part lists facts. section illustrates database view relational data logical predicate associated single database table. section illustrates graph view relational data set. circle represents instance rectangle represents attributes associated corresponding instance relations represented edges. identiﬁer particular entity property entity logic-based format similar; consists logical facts predicate name corresponds tables name arguments attribute values. one-to-one mapping rows table logical facts. logic based view allows easy integration background knowledge data. finally attributed graph representation entities nodes graph binary relationships edges nods edges attributes. representation advantage makes entities connectivity explicit naturally separates identiﬁers real attributes disadvantage edges graph represent binary relationships. though different representations largely equivalent provide different views data affects clustering methods used. instance notion shortest path distance much natural graph view logic based view fact different types entities explicit database view distinction entities attribute values explicit graph implicit database view absent logic view. paper hypergraph view combines elements above. oriented hypergraph structure vertices hyperedges; hyperedge ordered multiset whose elements directed graphs special case oriented hypergraphs hyperedges cardinality two. relational data represented typed labeled oriented hypergraph vertices hyperedges type function assigns type vertex hyperedge type attributes associated maps vertex vector values value attribute write value relational database converted hypergraph representation follows. table attribute vertex type introduced whose attributes non-key attributes table. becomes vertex whose identiﬁer value whose attribute values non-key attribute values row. table attribute hyperedge introduced contains vertices corresponding entities order occur table. hypergraph representation associate attributes hyperedges vertices; hence non-unary relationships contain non-key attributes vertex type corresponding hyperedge type introduced. clustering task consider following given vertex type partition vertices type clusters vertices cluster tend similar vertices different clusters dissimilar subjective notion similarity. practice course possible subjective notion; uses well-deﬁned similarity function hopefully average approximates well subjective notion user mind. following section introduces neighbourhood trees structure compactly represent describe neighbourhood vertex. neighbourhood tree directed graph rooted vertex interest i.e. vertex whose neighbourhood wants describe. constructed simply following hyperedges root vertex outlined algorithm construction neighbourhood tree parametrized pre-speciﬁed depth vertex interest original hypergraph. consider vertex every hyperedge participates directed edge vertex label vertex type attach corresponding attribute vector label edge hyperedge type position hyperedge vertices thus added said depth multiple hyperedges connecting vertices added time encountered. repeat procedure depth vertices thus added depth continue procedure predeﬁned depth root element never added subsequent levels. example neighbourhood tree given figure main idea behind proposed dissimilarity measure express wide range similarity biases emerge relational data discussed exempliﬁed section proposed dissimilarity measure compares vertices comparing neighbourhood trees. comparing level tree distribution vertices attribute values outgoing edge labels observed level. earlier work relational learning shown distributions good summarizing neighbourhoods method comparing distributions distinguishes discrete continuous domains. discrete domains distribution simply maps value relative frequency observed multiset values χ-measure comparing distributions used. given multisets dissimilarity deﬁned fig. illustration neighbourhood tree. domain contains types vertices objects elements ﬁctitious relations vertices type object associated attributes. section contains database view domain. section contains corresponding hypergraph view. here edges represented full lines hyperegdges represented dashed lines. finally section contains corresponding neighbourhood tree vertex continuous case compare distributions applying aggregate functions multiset values comparing aggregates. given aggregate functions dissimilarity deﬁned normalization constant minm ranging multisets attribute observed entire neighbourhood trees). implementation mean standard deviation aggregate functions. methods comparing distributions chosen simplicity ease implementation. sophisticated methods could used. main point section however distributions compared compared. intuitively proposed method starts comparing vertices according attributes. proceeds comparing properties neighbourhoods vertices there attributes interacting. finally looks proximity vertices given hypergraph. formally dissimilarity vertices deﬁned dissimilarity neighbourhood trees component normalized scale highest value obtained amongst pair vertices ensuring inﬂuence factor proportional weight. weights equation allow formulate bias similarity measure. remainder text term approach recent beneﬁts downsides formulation discussed contrasted existing approaches sections formulation somewhat similar multi-view clustering components forming different view data. however important fundamental difference multi-view clustering methods want clusters good view separately whereas components represent different views data different potential biases jointly contribute similarity measure. interpretations hypergraph view relational data exist literature. incorporate here domain objects form vertices hypergraph associated attributes relationships form hyperedges ﬁrst introduced richards mooney alternative view logical facts form vertices presented representations later utilized learn formulas relational models relational path-ﬁnding neighbourhood tree introduced section seen summary paths hypergraph originating certain vertex. though neighbourhood trees relational path-ﬁnding rely hypergraph view tasks solve conceptually different. whereas goal neighbourhood tree compactly represent neighbourhood vertex summarizing paths originating vertex goal relational path-ﬁnding identify small important paths appear often hypergraph. additionally practical difference distinction hyperedges attributes neighbourhood tree constructed following hyperedges mentioned work either treats attributes unary hyperedges requires declarative bias user. problems related consider graph tree partitioning graph partitioning focuses partitioning original graph smaller graphs certain properties satisﬁed. though partitions seen clusters vertices clusters limited vertices connected other. thus problem consider strictly general restriction kind cluster memberships; similarity vertices originate similarity sources consider cannot expressed within graph partitioning problem. number tree comparison techniques exists literature. approaches consider identity vertices source similarity ignoring attributes types vertices hyperedges. thus well suited comparison neighbourhood trees. relational learning community well graph kernel community previously shown interest clustering relational data. existing similarity measures within relational learning community coarsely divided groups. ﬁrst group consists similarity measures deﬁned attributed graph model examples hybrid similarity hybrid similarity annotated graphs approaches focus attribute-based similarity vertices compares attributes connected vertices hsag’s similariy measure compares attributes vertices attributes neighbouring vertices. main limitations approaches ignore existence vertex edge types impose strict bias towards attributes vertices. comparison presented approach deﬁnes dissimilarity component edge vertices otherwise. hsag deﬁnes dissimilarity linear combination components pair vertices. contrast ﬁrst group employs graph view second group methods employs predicate logic view prominent approaches conceptual clustering multi-relational data relational instance-based learning ﬁrstly describes example logical clauses generated bottom clause saturation obtained clauses considered features similarity measured tanimoto similarity measure overlap sets. sense similar using components generating clauses. note approach differentiate relations attributes consider distributions kind sense depth neighbourhood. finally ribl follows intuition similarity objects depends similarity attributes’ values similarity objects related them. extent ﬁrst constructs context descriptor objects related object interest similarly neighbourhood trees. comparing object involves comparing features computing similarity objects linked requires matching object similar object expensive operation contrast distance linear size multiset. further distance takes multiplicity elements account ribl approach not. within graph kernel community prominent groups exist weisfeiler-lehman graph kernels random walk based kernels common feature approaches measure similarity graph comparing structural properties. weisfeiler-lehman graph kernels family graph kernels developed upon weisfeiler-lehman isomorphism test. idea isomorphism test extend vertex attributes attributes neighbouring vertices compress augmented attribute attributes. attribute vertex corresponds subtree rooted vertex similarly neighbourhood trees. shervashidze borgwardt introduced fast subtree kernel undirected graphs performing isomorphism test update vertex labels followed counting number matched vertex labels. difference approach kernel family subtle important graph kernels extend attributes identifying isomorphic subtrees present graphs. reﬂected bias impose similarity comes structure graph rooted kernel ordered hypergraph instance random walk kernels successfully applied relational learning tasks. approaches estimate similarity graphs comparing walks obtain traversing hypergraph. rkoh deﬁnes similarity measure compares hypergraphs comparing paths originating every edge hypergraphs instead paths originating root hypergraph. rkoh differentiate attributes hyperedges treats everything hyperedge instead table summarizes different aspects similarity considered mentioned approaches. interpretations similarity divided sources similarity. ﬁrst categories concern attributes attributes vertices neighbouring vertices. following categories concern identities vertices neighbourhood vertex interest. concern subgraphs centered vertex proximity vertices. ﬁnal category concerns though scalability focus work show proposed approach scalable state-of-the-art kernel approaches substantially less complex majority above-mentioned approaches attribute link structure. sake clarity comparison assume homogeneous graph vertex type edge type. number vertices hyper-graph total number hyperedges depth neighbourhood representation structure applicable. well number attributes data set. additionally assume vertices participate number hyperedges refer refer length clause path rkoh multisets representing vertices proportional relational learning community expose different biases. characterization data sets summarized table include total number vertices hypergraph number vertices interest total number attributes number attributes associated vertices interest number hyperedges well number different hyperedge types. data sets range small number vertices attributes hyperedges considerably large number vertices attributes hyperedges chosen data sets originally classiﬁcation data sets allows evaluate approach respect well extracts classes present data set. imdb data small snapshot internet movie database. describes movies people acting directing them. goal differentiate people groups actors directors. uw-cse data describes interactions employees university washington roles publications courses teach. task identify clusters people students professors. mutagenesis data described section describes chemical compounds atoms consist compounds atoms described attributes describing chemical properties. task identify clusters compounds mutagenic mutagenic. webkb data consists pages links collected cornell university’s webpage. pages links associated words appearing page anchor text link. pages classiﬁed seven groups according role personal departmental project page. ﬁnal data termed terrorists describes terrorist attacks assigned labels indicating type attack. attack described total distinct features relations available http//alchemy.cs.washington.edu/data/imdb available http//alchemy.cs.washington.edu/data/uw-cse/ available http//www.cs.ox.ac.uk/activities/machlearn/mutagenesis.html available http//alchemy.cs.washington.edu/data/webkb/ available http//linqs.umiacs.umd.edu/projects//projects/lbc/ experiment used aforementioned similarity measures conjunction spectral hierarchical clustering algorithms. intentionally chosen clustering approaches assume different biases able similarity measure affected assumptions clustering algorithms make. altered depth neighbourhood trees wherever possible report results. evaluate approach using following validation method number clusters equal true number clusters data evaluate obtained clustering regards well matches known clustering given labels. obtained clustering evaluated using adjusted rand index measures similarity clusterings case obtained clustering provided labels. score ranges score closer corresponds higher similarity clusterings hence better performance chance level. data similarity measure report score achieve. additionally timeout hours report results approach takes time compute. achieve fair time comparison implemented similarity measures scala optimized caching intermediate results re-used. used clustering algorithms implemented python’s scikit-learn package hierarchy obtained hierarchical clustering reached pre-speciﬁed number clusters. ﬁrst experiment weights tuned used mean standard deviation aggregates continuous attributes. table performance approaches three data sets. similarity measure achieved true number clusters used. results shown hierarchical spectral clusteringwhile depth approaches indicated subscript. last column counts number wins algorithm means achieving highest data set. well weisfeiler-lehman subtree kernel linear kernel vertex histograms linear kernel vertex-edge histograms provided rkoh subscript recent hsag ribl kernel approaches denotes depth neighbourhood tree subscript denotes length clauses. second subscript wlst rkoh indicates parameters wlst parameter indicating number iterations whereas rkoh indicates length walk. results ﬁrst experiment summarized table table contains values obtained similarity measures data clustering algorithm used. last column table states number wins approach. number wins calculated simply counting number cases approach obtained highest value case combination data clustering algorithm. recent wins times thus outperforms methods. best results achieved combination spectral clustering exception terroristattack data wlst∗ wlst combined hierarchical clustering achieved highest contrast obtained recent. cases mutagenesis uwcse data sets recent wins larger margin. however important note remaining cases closest competitor always same. case imdb data combination spectral clustering closest competitor well case webkb combination spectral clustering. cases terroristattack data combined spectral clustering closest competitors hsag hsag case hierarchical clustering approach outperformed wlst∗ wlst. results show proposed similarity measure performs better wide range different tasks biases compared remaining approaches. moreover combined spectral clustering recent consistently performs well data sets achieving second-best result terroristattack data set. data sets exposes different bias inﬂuences performance methods. order successfully identify mutagenic compounds consider attribute link information including attributes neighbours. chemical compounds similar structure tend similar properties. data suitable ribl recent kernel approaches. recent ribl achieve best results here kernels approaches surprisingly perform better chance level. uw-cse social-network-like data task interacting communities different attribute-values students professors. distinction classes made single attribute professors positions students relation stating professors advide students. task suitable hsag. however approaches substantially outperformed recent cc∗. similarly imdb data consists network people roles movies seen social network. here directors differentiated actors single edge type actors work directors explicitly encoded data set. type interactions entities matters most attribute-rich data thus suitable methods account structural measures. accordingly recent ribl wlst∗ kernels achieve best results. remaining data sets webkb terroristattack entirely different nature aforementioned ones. data substantially larger number attributes sufﬁcient identify relevant clusters supported labels interactions contain important information. bias implicitly present partially assumed kernel approaches. results show recent wslt∗ kernels achieve almost identical performance webkb data remaining approaches outperformed even baseline approach. terroristattack data wlst∗ kernel achieves best performance outperforming recent hsag. similarly webkb approaches outperformed baseline approach. results summarized table point several conclusions. firstly given proposed approach achieves best results test cases results suggest indeed versatile enough capture relevant information regardless whether comes attributes vertices proximity connectedness vertices even without parameter tuning. moreover combined spectral clustering approach consistently obtains good results data sets competitor approaches achieve good results problem bias. secondly results show consider bias similarity measure bias clustering algorithm well evident data sets spectral clustering achieves substantially better performance hierarchical clustering. finally recent approaches tend sensitive depth parameter evident drastic difference performance table performance recent different parameter settings. upper part table presents results neighbourhood trees depth whereas bottom part contains results depth parameters italic indicate best performance achieved. different depths used. suggests increasing depth neighbourhood tree consequently introduces noise. interestingly results suggest recent depth performs best performance kernel methods tend increase depth parameter. results justify basic assumption approach important information contained small local neighbourhoods. second experiment evaluate relevant components equation table summarizes results. cases using single component sufﬁces results comparable using components conﬁrms clustering relational data difﬁcult needs choose right source similarity also similarity relational objects come multiple sources take account order discover interesting clusters. results explain recent almost consistently outperforms methods ﬁrst experiment. first recent considers different sources relational similarity; second ensures source comparable impact guarantees component contains useful information taken account. component useful information adds noise similarity measure clustering process seems quite resilient this. components irrelevant noise dominate pattern. likely happens experiment depth neighbourhood trees used much irrelevant information introduced level dominating signal level one. ﬁrst experiment shows recent outperforms competitor methods even without parameters tuned. second experiment shows typically consider multiple interpretations similarity order obtain useful clustering. natural question whether parameters could learned data unsupervised way. possibility tuning offers additional ﬂexibility user. knowledge right bias available advance specify adjusting parameters similarity measure potentially achieving even better results presented table however tuning weights automated systematic difﬁcult task clear objective function optimize purely unsupervised settings. many clustering evaluation criteria require reference clustering available clustering itself. clustering quality measures require reference clustering bias approach might help direction afﬁnity aggregation spectral clustering work extends spectral clustering multiple afﬁnity case. authors start position similarity objects often measured multiple ways often difﬁcult know advance different similarities combined order achieve best results. thus authors introduce approach learns weights would clustered desired number clusters yield highest intra-cluster similarity. achieved iteratively optimizing cluster assignment given ﬁxed weights weights given ﬁxed cluster assignment. thus treating component equation separate afﬁnity matrix approach tries learn optimal combination. tried aasc recent results summarized table results lead several conclusions. firstly cases aasc yields substantial beneﬁt even hurts performance. conﬁrms learning appropriate bias entirely unsupervised difﬁcult problem. main exceptions found depth here substantial improvement found uwcse terroristattack. seems indicate performance depth indeed overload irrelevant information aasc able weed that. still obtained results depth comparable ones obtained depth conclude tuning weights unsupervised manner require sophisticated methods current state art. previous experiments point proposed dissimilarity measure performs well compared existing approaches ﬁnding appropriate weights difﬁcult. though focus clustering tasks dissimilarity measure classiﬁcation tasks well. availability labels offers clear objective optimize learning weights thus allows evaluate appropriateness recent classiﬁcation. experiment nearest neighbours classiﬁer similarity measures. consists -fold cross-validation within training fold internal -fold cross-validation used tune parameters similarity measure tuned similarity measure next used classify examples corresponding test fold. results experiment summarized table recent achieves best performance data sets. imdb data recent achieves perfect performance ribl uwcse recent accurate; closest competitor achieves classiﬁcation viewpoint data sets easy classes differentiable particular attribute relation. mutagenesis terrorists difference outspoken recent achieves around accuracy closest competitor achieving webkb ﬁnally recent ribl substantially outperform approaches recent achieving ribl remarkable performance recent webkb explained inspecting tuned weights. reveal recent’s ability jointly consider vertex identity edge type distribution vertex attributes reason performs well. none approaches take three components account achieve substantially worse results. results clearly show accounting several views similarity beneﬁcial relational learning. moreover availability labelled information clearly helpful recent capable successfully adapting bias towards needs data set. table presents comparison runtimes approach. experiments computer power ram. runtimes include construction supporting structures calculation similarity pairs vertices clustering. measured runtimes consistent previously discussed complexities approaches. hsag table runtime comparison minutes runtimes include construction supporting structures time needed calculate similarity pair vertices given hypergraph. note graph kernel measures obtained using external software provided sugiyama borgwardt indicates calculation took hours. recent kernel approaches substantially efﬁcient remaining approaches. surprising hsag limited information. however interesting recent wlst substantially information take slightly time compute achieving substantially better performance data sets. approaches also orders magnitude efﬁcient ribl rkoh complete data sets depth particularly case rkoh complete hours even depth walk length work propose novel dissimilarity measure clustering relational objects based hypergraph interpretation relational data set. contrast previous approaches approach takes multiple aspects relational similarity account allows focus speciﬁc vertex type interest time leveraging information contained vertices. develop dissimilarity measure versatile enough capture relevant information regardless whether comes attributes proximity connectedness hyper-graph. make approach efﬁcient introduce neighbourhood trees structure compactly represent distribution attributes hyperedges neighbourhood vertex. finally experimentally evaluate approach several data sets clustering classiﬁcation tasks. experiments show proposed method often achieves better results competitor methods regards quality clustering classiﬁcation showing indeed versatile enough adapt data individually. moreover show proposed approach though expressive efﬁcient state-of-the-art approaches. open challenge extent parameters proposed similarity measure learnt data unsupervised way. conducted experiments afﬁnity aggregation approaches demonstrated difﬁculty problem. proposed similarity measure sensitive depth neighbourhood tree poses problem large neighbourhoods compared. however experiments demonstrated depth often sufﬁces. future work. work extended several directions. first number options concerning choice weights proposed similarity measure. learning weights works well class labels available difﬁcult unsupervised setting. semi-supervised classiﬁcation constraint-based clustering limited information available help tune weights. small number labels pairwise constraints sufﬁce tune weights recent. second direction comes ﬁeld multiple kernel learning ﬁeld multiple kernel learning concerned ﬁnding optimal combination ﬁxed kernel sets might inspirational learning weights directly data. contrast many relational clustering techniques approach neighbourhood trees allows construct prototype representative example cluster many clustering algorithms require. moreover constructing prototype cluster might great help analysing properties objects clustered together. integrating measure scalable clustering methods birch would allow cluster large hypergraphs. interesting extension would modify summations levels neighbourhood trees weighted sums levels following intuition vertices vertex interest less relevant time giving chance make difference.", "year": 2016}