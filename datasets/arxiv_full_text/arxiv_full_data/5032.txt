{"title": "Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir  Computing Framework", "tag": ["cs.LG", "cs.AI"], "abstract": "As an efficient recurrent neural network (RNN) model, reservoir computing (RC) models, such as Echo State Networks, have attracted widespread attention in the last decade. However, while they have had great success with time series data [1], [2], many time series have a multiscale structure, which a single-hidden-layer RC model may have difficulty capturing. In this paper, we propose a novel hierarchical reservoir computing framework we call Deep Echo State Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its ability to deal with time series through hierarchical projections. Specifically, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., a PCA, autoencoder, or a random projection) can project the echo-state representations into a lower-dimensional space. These low-dimensional representations can then be processed by another ESN. By using projection layers and encoding layers alternately in the hierarchical framework, a Deep-ESN can not only attenuate the effects of the collinearity problem in ESNs, but also fully take advantage of the temporal kernel property of ESNs to explore multiscale dynamics of time series. To fuse the multiscale representations obtained by each reservoir, we add connections from each encoding layer to the last output layer. Theoretical analyses prove that stability of a Deep-ESN is guaranteed by the echo state property (ESP), and the time complexity is equivalent to a conventional ESN. Experimental results on some artificial and real world time series demonstrate that Deep-ESNs can capture multiscale dynamics, and outperform both standard ESNs and previous hierarchical ESN-based models.", "text": "state machines continuous dynamics spiking neurons echo state networks discrete dynamics rate-coded neurons. relative simplicity esns widely used. usually consists three components input layer large layer linear output layer. weights input reservoir layers randomly initialized ﬁxed learning stage. reservoir initialized sparse connections constraint spectral radius weight matrix guarantees rich long-term dynamics reservoir viewed nonlinear temporal kernel sequences inputs highdimensional space learning reduced linear regression reservoir output. hence esns powerful tool analysis dynamic data simplifying training process. however hierarchical multiscale structures naturally exist temporal data single difﬁculty dealing input signals require explicit support multiple time scales therefore quote herbert jaeger natural strategy deal multiscale input design hierarchical information processing systems modules various levels hierarchy specialize features various scales. recent years several hierarchical architectures proposed however lengthy process training deep rnns still practical issue hence constructing hierarchical attractive approach problem training trivial. nevertheless constructing hierarchical esn-based model maintaining stability echo state property hierarchical system still challenging issue. develop hierarchical model proposed dynamical jaeger main idea stacking multiple reservoirs outputs higher level hierarchy serve coefﬁcients mixing outputs lower learns outputs reservoir simultaneously gradient-based algorithm increases computational complexity compared linear regression training original esn. subsequently triefenbach explored cascaded esns obtain multi-level phonetic states used hidden markov models phoneme language model speech recognition. cascaded feeds outputs previous reservoir next supervised trains output weights layer layer. abstract—as efﬁcient recurrent neural network model reservoir computing models echo state networks attracted widespread attention last decade. however great success time series data many time series multiscale structure single-hidden-layer model difﬁculty capturing. paper propose novel hierarchical reservoir computing framework call deep echo state networks distinctive feature deep-esn ability deal time series hierarchical projections. speciﬁcally input time series projected high-dimensional echo-state space reservoir subsequent encoding layer project echo-state representations lower-dimensional space. low-dimensional representations processed another esn. using projection layers encoding layers alternately hierarchical framework deep-esn attenuate effects collinearity problem esns also fully take advantage temporal kernel property esns explore multiscale dynamics time series. fuse multiscale representations obtained reservoir connections encoding layer last output layer. theoretical analyses prove stability deep-esn guaranteed echo state property time complexity equivalent conventional esn. experimental results artiﬁcial real world time series demonstrate deep-esns capture multiscale dynamics outperform standard esns previous hierarchical esn-based models. ating recurrent neural networks efﬁciently reservoir dynamics easily tapped process complex temporal data. using ﬁxed input hiddento-hidden weights reservoir computing avoids laborious process gradient-descent training achieve excellent performance nonlinear system identiﬁcation signal processing time series prediction deep esns leaky integrator units. constructed artiﬁcial random time series -out-of-n inputs constructed second time series adding typo partway series. goal measure long representation differed original series typo. found varying integration parameter slowing integration stacks lead long memory. unclear well generalize realistic time series interesting observation. recently multilayered echo-state machine proposed pipeline multiple same-size reservoirs. reservoir uses internal weight matrix connected subsequent layers coupling weight matrix. standard esns output weights last reservoir trainable parameters. proved architecture satisﬁes echo-state property rigorous comparison performance compared standard esns multiple datasets. achieved better results compared standard esns. however setting reservoir size layer size hierarchical models take advantage high-dimensional projection capacity since representations generated ﬁrst reservoir projected state space dimension ﬁrst one. apart above-mentioned esn-based hierarchy work tries take advantage random-static-projection technique extreme learning machine augment nonlinearity reservoirs. model ϕ-esn two-layer model adding static feed-forward layer reservoir. main idea increase dimensionality reservoir order increase separability. similar ϕ-esn adding elms encode inputs reservoir states respectively. results showed employing static feed-forward networks intermediate layers obtain nonlinear computational power reservoir. mentioned before directly stacking several reservoirs sufﬁcient built efﬁcient hierarchical esn-based model. hand well known important property high-dimensional projection capacity reservoir. connecting several size reservoirs together usually small advantage obtained layer mesm model appear gain anything beyond layers. hand reservoir usually randomly sparsely connected input data expressed echo-state representations reservoir representations tend redundant. called collinearity problem esns therefore hierarchical encoding high-dimensional echo-state representations previous reservoir proper low-dimensional data vital playing role projection next reservoir. furthermore although explored time-scale hierarchical differentiation among layers providing input data every intermediate layer existing hierarchical esn-based models fail fuse multiple time-scale features last layer. multiscale deep reservoir computing framework call deep echo state networks distinctive feature deep-esn uses projection layer encoding layer alternately hierarchical framework. specifically input time series projected echostate space reservoir subsequent encoding layer receives echo states previous reservoir input encodes highdimensional echo-state representations lower dimension. low-dimensional representations projected high-dimensional space following reservoir. multiple projection-encoding method deep-esn attenuate effects collinearity problem esns also fully take advantage temporal kernel property reservoir represent multiscale dynamics time series. moreover integrate multiscale representations connections encoding layer last output layer. stability deep-esn guaranteed run-time complexity equivalent standard esn. main contributions paper summarized follows. unsupervised encoding echo states adding direct information encoder layer last output layer proposed deep-esn obtain multiscale dynamics also dramatically attenuate effects collinearity problem esns. theoretical analysis analyze stability computational complexity deep-esn. also verify collinearity problem alleviated small condition numbers layer deep-esn capture various dynamics time series. compared several hierarchies deep-esn achieves better performances well-known chaotic dynamical system tasks real world time series. rest paper organized follows. section introduce typical architecture properties related hyper-parameters. section describe details proposed deep-esn conduct stability computational complexity analysis. that report experimental results section also give structure analysis collinearity analysis dynamic analysis deep-esn. finally give discussion conclusions section echo state network recurrent neural network consisting three basic components input layer large recurrent layer ﬁxed sparse hidden-to-hidden connections output layer. general architecture illustrated fig.. λmax largest eigenvalue matrix elements generated randomly satisfy echo state property smaller necessary condition stability. discussed detail later. short esns simple training procedure high-dimensional projection highly sparse connectivity neurons reservoir abundant nonlinear echo states short-term memory useful modeling dynamical systems. however single deal input signals require complex hierarchical processing cannot explicitly support multiple time scales. next section propose novel deep reservoir computing framework resolve issue. although main idea hierarchical esn-based models capture multiscale dynamics time series constructing deep architecture three main features deepesn contrast previous approaches multiple projection-encoding instead directly stacking multiple esns deep-esn uses encoder layer between reservoirs. deepesn obtain abundant multiscale dynamical representations inputs fully taking advantage high-dimensional projections also solves collinearity problem esns. multiscale feature fusion order better fuse multiscale dynamical representations captured reservoir connections encoding layer last output layer. simplicity training unlike previous hierarchical esn-based models training whole model layer layer trainable layer deep-esn last output layer retains efﬁcient computation without relying gradient-propagation algorithms. fig. architecture basic consists three components input layers neurons large resrevoir neurons last output neurons blueberry links denote ﬁxed sparse self-connections reservoir. to-reservoir reservoir-to-reservoir collected nby-d matrix n-by-n matrix wres. weights input-to-output reservoir-to-output cascaded single l-by- matrix wout. among these randomly initialized uniform distribution wres deﬁned ﬁxed training stage wout needs adapted. trained supervised learning. steps involved. ﬁrst d-dimensional inputs high-dimensional reservoir state space driving reservoir obtain echo states step learn output matrix wout simple regression techniques. introduce leaky-integrator neurons proposed jaeger also adopted mathematical formulations entire system follows denote inputs reservoir states outputs respectively. non-linear activation function reservoir activation function output denotes time step. denotes leak rate used integrating states previous time step current time step. output signals linear combinations echo states reservoir simple linear regression algorithms compute linear readout layer weights. therefore compared rnns training simple fast. moreover stuck local fig. architecture proposed deep-esn reservoirs encoders. direct connections inputs output layer feature links encoder output layer cascaded connected output neurons. architecture deep-esn illustrated fig.. hidden layers consist reservoirs encoders. avoid confusion deﬁne deep-esn k-layer network. denote number neurons i-th reservoir layer j-th encoder layer -length training series inputs denoted teacher signals denoted time step activations i-th layer reservoir j-th layer encoder denoted respectively further collect input weights i-th reservoir wres collect recurrent weights wenc collect input weights j-th encoder. matrix wout weights direct connections output weights last reservoir weights feature links. formulation details proposed deep-esn presented follows. denotes inputs i-th reservoir. equals greater means inputs i-th reservoir output encoder. simplicity operator denote high-dimensional projection update step given states previous reservoir unsupervised dimension reduction technique encode produce encoded features. thus encoding procedure j-th encoder formulated fenc activation function encoder. fenc identity function linear dimensionality reduction technique. choices introduced later. according description obtain state still simple regression problem parameters wout. since time series present high-dimensional form problem always over-determined adopt ridge-regression tikhonov regularization solve principal component analysis popular statistical method. adopts orthogonal base transformation project observations linearly uncorrelated low-dimensional representation selected orthogonal bases called principal components. mathematical terms attempts linear mapping rd×m maximizes following optimization problem covariance matrix zero-mean inputs rd×n identity matrix. original reduced dimension respectively. optimal provided eigenvectors corresponding largest eigenvalues covariance matrix standard dominated eigenvalue decomposition fast iterative methods number data points number leading eigenvectors required number iterations converge usually quite small therefore complexity rewritten elm-based auto-encoder recent tool based extreme learning machine used simplifying training traditional autoencoders. main idea obtain hidden random features rm×n using random weights rm×d bias rm×d formulated regularization coefﬁcient. problem simple regression problem solved pseudoinverse technique finally reduced data henc represented henc computational complexity random projection kind monte carlo method constructs lipschitz mappings preserve local separation high-dimensional data high probability different methods realized various random matrices deep-esn select random matrix rd×m designed achlioptas elements given distribution reasonable setting hyperparameters vital building high performance network. three commonly used strategies direct method grid search heuristic optimization. former strategies used general single-reservoir esns unsuitable deep-esns larger parameter space. thus adopt heuristic optimization hyperparameters. genetic algorithm commonlyused heuristic technique generate high-quality solutions optimization search problems works population candidate solutions. first ﬁtness every individual population evaluated iteration ﬁtness value objective function. individuals stochastically selected current population used form generation three biologically-inspired operators mutation crossover selection. finally algorithm terminates maximum number generations satisfactory ﬁtness level reached. deep-esn view cascaded hyper-parameter vector reservoir individual search space constrained interval additionally prediction error system ﬁtness value individual population size individuals evaluate generations. experiments follow training optimize hyperparameters ﬁtness measured validation set. deﬁnition assuming standard compactness conditions states come compact respectively) satisﬁed. moreover assume given output feedback connections. produce echo states every echo-state uniquely determined every left-inﬁnite input according deﬁnition. nearby echo states similar input histories means past information gradually washed recent inputs states remembered. so-called fading memory short-term memory. capacity esns accurately model underlying dynamical characteristics time series. unlike aforementioned necessary condition stability sufﬁcient condition provides restrictive theoretical support global asymptotic stability. following formally introduce analysis global asymptotic stability deep-esn. convenience consider leaky unit reservoir neurons consistent jaeger’s work theorem assume deep-esn reservoirs ﬁxed high-dimensional projection matrices internal matrix wres reservoir learned encoding matrix wenc winwenc bounded activation function reservoir tanh satisﬁes lipschitz condition encoders linear convenience. moreover denote distinct echo states i-th reservoir time then largest singular value internal weight matrix reservoir satisfy condition deep-esn echo-state property e.g. limt→∞ right-inﬁnite inputs proof first consider asymptotic condition ﬁrst reservoir. time input projected win. distinct echo states res) previous time difference current time follows although deep-esn deep neural model reservoir computing large additional costs whole learning process. section analyze computational complexity deep-esn. assuming deep-esn reservoirs pcabased encoders sizes reservoirs ﬁxed reduced dimensionality given length d-dimensional input sequences analyze computational complexity deep-esn follows. cenc updating echo states time stamps layers collect last reservoir states inputs middle-layer-encoded features size full rank. complexity solving regression problem computed performance methods evaluated three widely used metrics root mean squared error normalized root mean squared error mean absolute percentage error used esn-based methods formulated follows. denotes t-th observation -length target signals denotes mean observation points denotes t-th observation output forecasting process. note metric mape used evaluating time series involving zero values modify adding small number e.g. adding sunspot time series predicted results computing mape. three metrics smaller values mean better performance. following simulations adopt metric rmse direct ga-based hyperparameter optimization although baseline systems choose hyperparameters original work optimize baselines ensure fairness comparison. mesm deepesns report best result among variants different number layers different size encoders intervals cross-validation determine optimal layer number encoder size. experiments deepesns larger sizes reservoirs better performances. simplicity sizes reservoirs ﬁxed discuss effects reservoir size section iv-e. details parameters settings given table section provide comprehensive experimental analysis proposed deep-esn chaotic systems real world time series. speciﬁcally time series mackey-glass system narma system; monthly sunspot series daily minimumtemperature series. fig. shows examples time series. qualitatively narma dataset daily minimum-temperature series present strong nonlinearity monthly sunspot series presents nonlinearity peaks chaotic series relatively smooth. evaluate effectiveness proposed deep-esn including singlecompare four baseline models reservoir leaky neurons aforementioned two-layer variants ϕ-esn recent hierarchical esn-based model called multilayered echo-state machine fact mesm viewed simpliﬁed variant deep-esn without encoders. since core work exploring analyzing effectiveness hierarchical schema ignore variants esns example simple cycle reservoir circular topology reservoir support vector echo state machine optimizes output weights svm. worth noting single-reservoir variants viewed module integrated deep-esn framework. also compare performance among deep-esns various encoders furthermore comparisons conduct experiments deep-esns without feature links evaluate impact fusing multiscale dynamics outputs. therefore deep-esn realize equivalent computational performance single-reservoir means deep-esn remains high computational efﬁciency traditional reservoir computing networks. mackey-glass system classical time series evaluating performance dynamical system identiﬁcation methods discrete time mackey-glass delay differential equation formulated parameters usually system becomes chaotic previous work thus also task. detail generate -length time series equation fourth-order runge-kutta method. split points three parts length ttrain tvalidate ttest avoid inﬂuence initial states discard certain number initial steps twashout reservoir. input time steps ahead. thus methods required learn mapping current input target output model conduct simulations independently record average result standard deviation. detailed results presented table seen table deep-esn models outperform baselines order magnitude. among baseline systems ϕ-esn best. mesm reaches best result reservoirs. case deep-esn different encoders variant outperforms elm-ae encoders task result reprised every experiment. also found depth network signiﬁcant effect deep-esn performance. tried layers performance improved added layers. analysis effects depth network performance section iv-e. models removing feature links reduces performance. again theme repeated remaining experiments. demonstrates incorporating feature links crucial part design deepesn provide multiscale information output layer compared networks output depends dynamics last reservoir. fig. prediction curves absolute error. results deepesn using encoder. -step-ahead prediction mackeyglass time series; one-step-ahead prediction narma time series; one-step prediction monthly sunspot number time series; one-step prediction daily minimum temperature time series. fig. presents prediction curve absolute error curve time series simulated deep-esn testing phase. noted target curve obscured system output deep-esn chaotic time series well small errors. narma short nonlinear autoregressive moving average system highly nonlinear system incorporating memory. tenth-order narma depends outputs inputs time steps back considered difﬁcult identify. tenth-order narma system described random input time step drawn uniform distribution output signal initialized zeros ﬁrst steps task generate narma time series total length ttotal split three parts length ttrain tvalidate ttest respectively. washout length also reservoir algorithms. conduct one-step-ahead prediction tenth-order narma time series. averaged results independent simulations presented table iii. again deep-esn models outperform baselines encoder version best. again results deep-esns feature links outperform ones without links. mesm best among baselines. note that compared results fig. illustrates prediction curve absolute error curve -layer deep-esn th-order narma time series. case errors visible plot strong nonlinear changes mirrored error plot. sunspot number dynamic manifestation strong magnetic ﬁeld sun’s outer regions. found sunspot number close statistical relationship solar activity complexity underlying solar activity high nonlinearity sunspot series forecasting analyzing series challenging task commonly used evaluate capability timeseries prediction models open source -month smoothed monthly sunspot series provided world data center silso data july november forecasting experiment. total sample points. since last points still provisional subject possible revision remove points remaining observations. split data three parts length ttrain tvalidate ttest respectively. washout length twashout reservoir real world time series proposed deep-esns also outperform baselines. metrics rmse nrmse -layer deep-esn achieves best performance. exception previous results deep-esn elm-ae presents best result mape. feature links improve performance feature links. daily minimum temperatures melbourne australia second real world time series dataset recorded january december total sample points ttrain tvalidate ttest respectively. washout length twashout reservoir since real world time series shows strong nonlinearity smooth -step sliding window. smoothed data seen fig.. perform one-step-ahead prediction daily minimum temperatures. results listed table previous observations deep-esn gives best results mesm also performs well better ϕ-esn two-layer reservoirs. here feature links make much difference suggesting time series multiscale. fig. shows predicted results absolute errors plots test data. plots error signals show drastic oscillations strong nonlinearity target time series reﬂecting high complexity dynamical system. four experiments mackey-glass -stepahead prediction task needs deepest structure one-step-ahead prediction daily minimum temperature series required shallower networks believe reﬂects four time series different multiscale structures. furthermore interesting note feature links much larger effects deeper models shallower ones suggests time series shallower models multiscale. based informal observations provide detailed analysis effects network structure next section. first examine effects size reservoir -step-ahead prediction time series. experiment used -layer deep-esn pca. illustrated fig. size encoder increase size reservoirs enlarging reservoirs generally improves overall performance. observation shows size reservoir plays important role deep-esn. deep-esn second look effects encoder dimensionality figure size reservoirs increase encoder dimension intervals seen figure dimensionality encoders also affects performance deep-esns extent. encoder dimension small lose much information previous reservoir. generally deep-esn larger encoders performs better smaller ones diminishing returns dimensions problem. effect persists slope suspect effect orthogonalization variables. increase dimensionality encoder match reservoir interest deep-esns still good performances performances deep-esns elm-ae continue rapidly worse encoder size larger therefore better smaller size elm-ae. order evaluate effect depth networks varied number layers figure compared deep-esn mesm -stepahead prediction mackey-glass system one-stepahead prediction tenth-order narma. sizes reservoirs models ﬁxed size encoder deep-esn mackey-glass narma respectively. fig. investigation effects size reservoirs encoders -layer deep-esn. fixing size encoder increasing size reservoirs fixing size reservoirs increasing size encoder narma task networks layers perform well. observation consistent results previous nonlinear real world experiments hand comparisons deep-esn mesm figure also demonstrate effectiveness proposed model. fourth investigate effects feature links vary depth deep-esns. contrast trends mackey-glass narma former task networks with/without feature links much better performance layers latter task networks without feature links perform well layers. hand networks shallow much performance differences deepesn feature links ones without feature links. words deeper network useful feature links are. last explore general schema deepen deep-esn since number hyper-parameters become unwieldy many layers ﬁrst optimize -layer network directly copy hyperparameters reservoir reservoirs deepen deep-esn without fig. comparisons deep-esns with/without feature links. rmse results -step-ahead prediction mgs; rmse results one-step-ahead prediction narma. seen figure appears directly adding layers copying hyperparameters feasible deep-esn improves performance. deepesn obtains best performance layers. moreover also consider shallower model deep-esn -layers layers instead enlarge last reservoir number learned output weights deep-esn corresponding depth. compared model deep-esn outperforms layers added. also veriﬁes deep-esn better shallow models scale learned weights. high-dimensional projection afforded echostate reservoir important feature reservoir computing. process reservoir network produce abundant echo states high-dimensional space enhancing separability samples allowing outputs obtained simple linear combination variables. however solving regression problem collinearity issue redundant echo states variables echo states. redundant states lead inability make different predictions states similar redundant components waste predictor variables. better echo states many independent contributions possible. deep-esn added unsupervised encoders reservoirs. encoders model extract uncorrelated subspace representations echo states reduce redundancy reservoir. section visualize effects encoders collinearity problem. here condition number describe redundancy system. condition number standard measure ill-conditioning matrix given regression problem rm×n condition number matrix deﬁned σmax σmin maximal minimal singular values respectively. deep-esn compute condition number echo-state collected matrix i-th reservoir encodedx j-th encoder. state collected matrix mesm compute condition number res. results collinearity analysis three -layer deep-esns different encoders -layer mesm shown fig.. fig. results collinearity analysis three -layer deep-esns -layer mesm time series denotes i-th reservoir denotes j-th encoder order facilitate visualization logarithmic form condition numbers given plot hierarchical direction mesm reduce redundancy layers. high condition number mesm lowers accuracy linear regression. compared mesm deep-esn works well encoders. higher layer redundancy reservoirs less especially although ﬂattens fact condition number appears increase suggests encoder effective. note condition number oscillates encoder reservoir layers automatically reduces condition number orthogonalization data. hand also high-dimensional projection results abundant redundant features demonstrates tradeoff high-dimensional projection hierarchical construction reservoir computing. high-dimensional projection major feature reservoir computing. hope retain projection capacity hierarchical framework reservoir computing size higher reservoir increase redundancy serious. layer holds check preventing network becoming overly redundant. believe method alternating projection encoding scheme deep-esn effective strategy construct hierarchical reservoir computing framework. visualizing multiscale dynamics intuitive effective ways understand internal mechanism given hierarchical system section construct experiment similar work visualize multiscale dynamics proposed deep-esn various encoders mesm. generate mackey-glass time series length perturb adding noise value time series time step denoted drive hierarchical esnbased model measure euclidean distance echo states generated formally measure difference original echo states i-th reservoir layer time fig. visualization multiscale dynamics mesm deep-esns four datasets mackey-glass narma monthly sunspot series daily minimum-temperature series. consider -layer deep-esn -layer mesm hyperparameters used section iv-a. reference model also plot dynamics single-layer esn. visualization results multiscale dynamics plotted figure plots line denotes singlelayer reference blue lines denote perturbation effects layer. darker colors correspond deeper layers. figure multiscale dynamics -layer mesm observed. plot timescales differences among layers quite small almost dominated ﬁrst reservoir although small persistent memory. therefore directly stacking reservoirs equal size dynamics similar shallow unlikely produce signiﬁcant multiscale behaviors hierarchy. figure shows multiscale dynamics deepesn pca. compared mesm figure deep-esn produces greater persistent multiscale dynamics. deep-esn short time-scales shallow layers deeper layers show longer memory. particular last reservoir longest time-scale. results verify proposed hierarchical framework generates rich multiscale dynamic behaviors suitable modeling time series. similar deep-esn figure elm-ae based deep-esn also performs multiscale dynamics figure. interestingly case random projection figure distances subsequent layers even larger ﬁrst layer. random projections enlarging added noise corresponding encoder layers. furthermore explore difference multiscale structures four time series data mackey-glass narma sunspots temperature figure deepesns optimal depth according results previous sections. similar figure noise value time series long perturbation affects layer. figure mackey-glass multiscale structure three time series. although time-scales differences among layers deep-esn three time series shallower deep-esn capture dynamics datasets mackey-glass. especially sunspots temperature time series deep-esns layers layers respectively good performances shown previous sections. therefore important choose proper hierarchy deep-esn time series various multiscale structures. again mesm show signiﬁcant time-scales differences among layers matter time series multiscale not. hierarchical multiscale structures naturally exist many temporal data phenomenon capture conventional esn. overcome limitation propose novel hierarchical reservoir computing framework called deep-esns. instead directly stacking reservoirs combine randomly-generated reservoirs unsupervised encoders retaining high-dimensional projection capacity well efﬁcient learning reservoir computing. multiple projection-encoding system alleviate collinearity problem esns also capture multiscale dynamics layer. feature links deep-esn provides multiscale information fusion improves ability network time series. also presented derivation stability condition computational complexity deep-esn. results show deep-esn efﬁcient unsupervised encoders efﬁciently learned shallow retaining major computational advantages traditional reservoir-computing networks. experiments demonstrated empirically deep-esns outperform baselines including approaches multiscale esns four time series furthermore found increasing size reservoirs generally improved performance increasing size encoder layer showed smaller improvements. also showed increasing depth network could either help hurt performance depending problem. demonstrates important network structure parameters using cross-validation. also evaluated model overcomes collinearity problem measuring condition numbers generated representations different layers network. found using encoders controlled redundancy especially case pca. hand simply stacking reservoirs mesm leads higher condition numbers overall. suggests encoders vital part design system main effects control collinearity deeper reservoirs. finally investigated multiscale dynamics models using perturbation analysis. found models demonstrated long-term memory perturbation evident ﬁnal layer. mesm seemed never quite recover perturbation small persistent effects. also found four time series used different multiscale structures. thus different hierarchies deep-esns deal various multiscale dynamics. extensive training. former pursues conciseness effectiveness latter focuses capacity learn abstract complex features service task. thus merits weaknesses approaches potentially fruitful future direction discover bridge models achieve balance efﬁciency feature learning other. deep-esn ﬁrst step towards bridging reservoir computing deep learning. jelfs hulle pr´ıncipe mandic augmented echo state network nonlinear adaptive ﬁltering complex noncircular signals ieee transactions neural networks vol. soriano ort´ın keuninckx appeltant danckaert pesquera sande delay-based reservoir computing noise effects combined analog digital implementation ieee transactions neural networks learning systems vol. hermans schrauwen training analysing deep recurrent neural networks advances neural information processing systems burges bottou welling ghahramani weinberger eds. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference ieee pascanu gulcehre bengio construct deep recurrent neural networks arxiv preprint arxiv. triefenbach jalalvand schrauwen j.-p. martens phoneme recognition large hierarchical reservoirs advances neural information processing systems triefenbach jalalvand demuynck j.-p. martens acoustic modeling hierarchical reservoirs ieee transactions audio speech language processing vol. fildes conditioning diagnostics collinearity weak data regression journal operational research society vol. available http//dx.doi.org/./ jors.. triefenbach demuynck martens large vocabulary continuous speech recognition reservoir-based acoustic models ieee signal processing letters vol. march gallicchio micheli deep reservoir computing critical analysis european symposium artiﬁcial neural networks computational intelligence machine learning butcher verstraeten schrauwen haycock reservoir computing extreme learning machines non-linear time-series data analysis neural networks vol. extending reservoir computing random static projections hybrid extreme learning european symposium artiﬁcial neural networks d-side cambria huang kasun zhou vong leung feng akusok lendasse corona nian miche gastaldo zunino decherchi yang jeon teoh chen extreme learning machines ieee intelligent systems vol. bingham mannila random projection dimensionality reduction applications image text data proceedings seventh sigkdd international conference knowledge discovery data mining ser. york achlioptas database-friendly random projections proceedings twentieth sigmod-sigact-sigart symposium principles database systems ser. pods york rodan tino minimum complexity echo state network ieee transactions neural networks vol. support vector echo-state machine chaotic timeseries prediction trans. neur. netw. vol. mar. available http//dx.doi.org/./tnn.. bray loughhead sunspots. dover publications suyal prasad singh nonlinear time series analysis sunspot data solar physics vol. available http//dx.doi.org/./s---x qiao growing echo-state network multiple subreservoirs ieee transactions neural networks learning systems vol. february available http//dx.doi.org/./tnnls.. silso world data center international sunspot number international sunspot number monthly bulletin online catalogue available http//www.sidc.be/silso/", "year": 2017}