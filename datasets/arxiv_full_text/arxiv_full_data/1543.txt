{"title": "End-to-end LSTM-based dialog control optimized with supervised and  reinforcement learning", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.", "text": "paper presents model end-toend learning task-oriented dialog systems. main component model recurrent neural network maps dialog history directly distribution system actions. lstm automatically infers representation dialog history relieves system developer much manual feature engineering dialog state. addition developer provide software expresses business rules provides access programmatic apis enabling lstm take actions real world behalf user. lstm optimized using supervised learning domain expert provides example dialogs lstm imitate; using reinforcement learning system improves interacting directly users. experiments show complementary alone derive reasonable initial policy small number training dialogs; starting optimization policy trained substantially accelerates learning rate consider person would teach another person conduct dialog particular domain. example experienced call center agent would help agent started. first teacher would provide orientation agent controls available look customer’s information well business rules conﬁrm customer’s identity conﬁrmation message must read performing ﬁnancial transaction. second student would listen good dialogs teacher goal imitating them. third student would begin taking real calls teacher would listen providing corrections student made mistakes. finally teacher would disengage student would continue improve experience. paper provide framework building maintaining automated dialog systems bots domain mirrors progression. first developer provides actions text actions calls invoke action masking code indicates action possible given dialog far. second domain expert need developer machine learning expert provides example dialogs recurrent neural network learns imitate. third conducts conversations domain expert makes corrections. finally interacts users scale improving automatically based weak signal indicates whether dialogs successful. concretely paper presents model taskoriented dialog control combines trainable recurrent neural network domain-speciﬁc software encodes business rules logic provides access arbitrary apis actions domain ordering taxi reserving table restaurant. recurrent neural network maps directly sequence user turns actions infers representation state. result minimal hand-crafting state required design dialog taxonomy necessary. neural network trained using supervised learning good dialogs provided neural network imitate using reinforcement learning tries sequences actions improves based weak signal whole-dialog success. neural network re-trained second means corrections made on-line conversation real time. paper organized follows. first section describes model section compares model related work. section presents example application optimized using supervised learning section reinforcement learning section finally section concludes. three components high level model recurrent neural network; targeted well-encapsulated software implementing domain-speciﬁc functions; language understanding module. software enables developer express business logic gating actions available; presents coherent surface apis available neural network placing phone call; tracks entities mentioned dialog; provides features neural network developer feels useful choosing actions. recurrent neural network responsible choosing action take. neural network chooses among action templates abstract entities text action want call <name>? action placephonecall. recurrent neural network internal state accumulate history sufﬁcient choosing among action templates. loop shown figure cycle begins user provides input input could text typed text recognized user speech. text passed entity extraction module identiﬁes mentions entities user text example identifying jason williams <name> entity. entity input code provided developer resolves entity mentions grounded entities example maps text jason williams speciﬁc database developer-provided code stateful allows retain entities processed step step feature vector formed takes input sources. first entity extraction module indicates entity types recognized. example vector could indicate name recognized type phone not. second entity input module return arbitrary features speciﬁed developer. example code returns features indicating jason williams matched person jason williams types phones available. sources described below. step recurrent neural network work chose softmax output layer. long short-term memory neural network ability remember past observations arbitrarily long shown yield superior performance many domains. lstm takes feature vector step input updates internal state outputs distribution template actions i.e. actions entity values replaced entity names want call <name>?. step code developer outputs action mask indicating actions permitted current timestep. example target phone number identiﬁed action place phone call masked. step mask applied clamping masked actions zero probability re-normalizing resulting vector probability distribution step action chosen probability distribution. action chosen depends whether reinforcement learning currently active. active exploration required case action sampled distribution. active best action chosen action highest probability always selected. identity template action selected used ways ﬁrst passed lstm next timestep; second passed entity output developer code substitutes template entities. step control branches depending type action figure operational loop. green trapezoids refer programmatic code provided software developer. blue boxes indicate recurrent neural network trainable parameters. orange performs entity extraction. vertical bars steps feature vector distribution template actions respectively. text complete description. comparing past work helpful consider main problems dialog systems solve state tracking refers information past represented action selection refers mapping state action constructed. consider turn. state tracking task-oriented dialog systems state tracking typically consists tracking user’s goal cuisine type price range search criteria restaurant dialog history whether slot already asked conﬁrmed whether restaurant offered already whether user favorite cuisine listed proﬁle past work building task-oriented dialog systems used hand-crafted state representation quantities i.e. possible values user’s goal dialog history manually designed. example dialog state tracking challenge state consisted pre-speciﬁed frame name/value pairs form user’s goal many dstc entries learned data update state using methods recurrent neural networks schema state being tracked hand-crafted. manually designed frames also used tracking user’s goal dialog history methods based partially observable markov decision processes methods learn example dialogs supervised learning/reinforcement learning hybrid methods also commercial open source frameworks voicexml aiml. contrast method automatically infers representation dialog history recurrent neural network optimal predicting actions take future timesteps. important contribution designing effective state space quite labor intensive omissions cause aliasing spurious features slow learning. worse learning progresses optimal history features change. thus ability automatically infer dialog state representation tandem dialog policy optimization simpliﬁes developer work. hand like past work possible user goals method hand-crafted many taskoriented systems seems desirable order support integration back-end databases large table restaurant names price ranges etc. therefore method delegates tracking user goals developer-provided code. www.w.org/tr/voicexml www.alicebot.org/aiml.html when entity extraction reliable textbased interfaces speech recognition errors simple name/value store track user goals approach taken example application below. another line research sought predict words next utterance directly history dialog using recurrent neural network trained large corpus dialogs work infer representation state; however approach differs several respects ﬁrst work entities tracked separately allows generalization entities appeared training data; second approach includes ﬁrst-class support action masking calls allows agent encode business rules take real-world actions behalf system; ﬁnally addition supervised learning show method also trained using reinforcement learning. action selection broadly speaking three classes methods action selection explored literature hand-crafting supervised learning reinforcement learning. first action selection hand-crafted voicexml aiml number long-standing research frameworks beneﬁt hand-crafted action selection business rules easily encoded; however hand-crafting action selection often requires specialized rule engine skills rules difﬁcult debug hand-crafted system don’t learn directly data. second action selection learned example dialogs using supervised learning example user input received corpus example dialogs searched similar user input dialog state following system action output user beneﬁt approach policy improved time adding example dialogs respect rather easy make corrections sl-based systems. however system doesn’t learn directly interaction users. taken. action selection originally framed markov decision process later partially observable markov decision process reward signal naturally occurs whether user successfully completed task beneﬁt learn directly interaction users without additional labeling. business rules incorporated similar manner approach however debugging system difﬁcult corrections made reward signal many designers unfamiliar with non-obvious effects resulting policy. addition early stages learning performance tends quite poor requiring practice users like crowd-workers simulated users. contrast existing work neural network method optimized using supervised learning reinforcement learning neural network trained using gradient descent optimizing simply requires different gradient computation. started designer provides training dialogs recurrent neural network trained reconstruct using supervised learning avoids poor out-of-the-box performance. neural network optimized using reward signal policy gradient sl-based approaches found training dialogs added training system remains easy debug. addition implementation ensures policy always reconstructs provided training optimization contradict training dialogs provided designer. finally action mask provided developer code allows business rules encoded. past work explored alternate combining supervised learning reinforcement learning learning dialog control work goal learn ﬁxed corpus heterogeneous control policies i.e. corpus dialogs many different experts. reward function augmented penalize policies deviated policies found corpus. action selection differs view training corpus authoritative goal avoid deviations training corpus on-line improve persummary knowledge ﬁrst end-to-end method dialog control trained supervised learning reinforcement learning automatically infers representation dialog history also explicitly tracking entities. test approach created dialog system initiating phone calls contact address book taken microsoft internal employee directory. system contact’s name synonyms contact phone number work mobile etc. phone types synonyms like cell mobile. started deﬁning entities. user entities <name> <phonetype> <yesno>. system also entities plus three more <canonicalname> <canonicalphonetype> allow user name call hillary system respond canonical name calling hillary clinton; <phonetypesavail> allows system which type phone mobile work?. entity extraction trained model using language understanding intelligent service next wrote programmatic portion system. first tracking entities used simple approach entity retained indeﬁnitely recognized replaced value observed. deﬁned actions places call commits phone type contact phone type address book. deﬁned features back-end return lstm including many people match recently recognized name many phone types person database. altogether dimension lstm input finally action mask allow action system entities help you? always available language action calling <name> <phonetype> available back-end able populate entities. altogether code comprised lines python. figure example dialogs used supervised learning training. space entity tags appear user system sides dialogs removed example call <name>jason</name> shown call jason. appendix additional examples. wrote example dialogs covering scenarios spoken name single multiple address book matches; phone types available; user speciﬁes phone type not; user’s speciﬁed phone type available; etc. example given figure several given appendix example dialogs average turns; longest turns shortest turns. action templates experiments below make hand-designed stochastic simulated user. start dialog simulated user randomly selected name phone type including names phone types covered dialog system. speaking simulated user canonical name nickname; usually answers questions ignore system; provide additional information requested; give simulated user parameterized around probabilities consisted lines python. ﬁrst sought measure whether lstm trained small number dialogs would successfully generalize using -fold leave-one-out cross validation experiment. folds diset reconstructed loss plateaued epochs. results shown table shows unable reconstruct training dialogs. upon investigation found turns different actions identical local features different histories. since unable store history differences indistinguishable dnn. also reconstructed training set; suggests line future work investigate relative beneﬁts different recurrent neural network architectures task. active learning next examined whether model would suitable active learning goal active learning reduce number labels required reach given level performance. active learning current model unlabeled instances unlabeled instances model uncertain labeled next. model re-built cycle repeats. active learning effective scores output model must good indicator correctness. assess this plotted receiver operating characteristic curve figure ﬁgure dialogs randomly assigned training dialogs test dialogs. lstm estimated training applied test logging highest scoring action action’s correctness. whole process repeated times resulting correctly predicted actions incorrectly predicted actions. ﬁgure shows model scores looking strong predictors correctness. lowest scored actions although incorrectly predicted actions make turns actions figure average accuracy leave-one-out cross-fold validation. axis shows number training dialogs used train lstm. axis shows average accuracy held-out dialog green bars show average accuracy measured turn blue bars show average accuracy dialog. dialog considered accurate contains zero prediction errors. figure shows per-turn accuracy wholedialog accuracy averaged across folds. after single dialog dialog turns correctly predicted. dialogs rises nearly dialogs predicted completely correctly. sufﬁcient deploying ﬁnal system shows lstm generalizing well enough preliminary testing small number dialogs. next investigated whether recurrency lstm beneﬁcial whether non-stateful deep neural network would perform well. substituted lstm non-stateful number hidden units lstm loss function gradient accumulator. also experiment standard recurrent neural network training either training approach conceptually policy gradient-based model outputs distribution actions sampled timestep. dialog return dialog computed gradients probabilities actions taken respect model weights computed. weights adjusted taking gradient step weighted difference return dialog long-run average return. intuitively better dialogs receive positive gradient step making actions selected likely; worse dialogs receive negative gradient step making actions selected less likely. policy gradient methods successfully applied dialog systems robotics board game learning rate; action taken timestep dialog history time return dialog; denotes jacobian respect baseline described below; lstm i.e. stochastic policy outputs distribution given dialog history parameterized weights baseline estimate average return current policy estimated last dialogs using weighted importance sampling. past work applied so-called natural gradient estimate dialog systems natural gradient second-order gradient estimate often shown converge faster standard gradient. however computing natural gradient requires inverting matrix model weights found intractable large numbers weights found neural networks. standard policy gradient update make three modiﬁcations. first effect action mask clamp action probabilities zero causes logarithm term policy gradient update undeﬁned. solve this small constant action probabilities figure plot scores actions selected lstm. false positive rate number incorrectly predicted actions threshold divided total number incorrectly predicted actions; true positive rate number correctly predicted actions threshold divided total number correctly predicted actions. finally note re-training lstm requires less second standard means lstm could retrained frequently. taken together model building speed combined ability reliably identify actions errors suggests approach readily support active learning. previous sections supervised learning applied train lstm mimic dialogs provided system developer. system operates scale interacting large number users desirable system continue learn autonomously using reinforcement learning turn receives measurement goodness called reward; agent explores different sequences actions different situations makes adjustments maximize expected discounted rewards called return. deﬁned reward successfully completing task otherwise. discount used incentivize system complete dialogs faster rather slower. figure task completion rate mean standard deviation policy initially trained dialogs using supervised learning optimized dialogs using reinforcement learning training evaluation done stochastic simulated user. line shows average runs dialogs used training randomly sampled example dialogs. applying update. second well-known neural network convergence improved using form momentum i.e. accumulation gradient steps multiple turns. problem found using adadelta sped convergence substantially finally setting want ensure policy continues reconstruct example dialogs provided developer. therefore gradient step check whether updated policy reconstructs training set. supervised learning training training reconstructed. note approach allows training dialogs added time whether optimization underway not. evaluate optimization ways. first randomly initialize lstm begin optimization. second initialize lstm ﬁrst applying supervised learning training consisting dialogs formed randomly sampling example dialogs. policy updates made dialog. updates freeze policy dialogs user simulation measure task completion. repeat runs report average performance. results shown figure alone sometimes fails discover complete policy ﬁrst dialogs runs fewer pre-training dialogs failed discover certain action sequences resulting lower average task completion black line note average figure high variance figure difﬁculty discovering long action sequences delayed rewards observed applications dialog systems contrast addition dialogs pre-training accelerates learning average reduces variability performance resulting policy. paper taken ﬁrst step toward end-toend learning task-oriented dialog systems. approach based recurrent neural network maps dialog history distributions actions. lstm automatically infers representation dialog state alleviating much work hand-crafting representation dialog state. code provided developer tracks entities wraps calls external actuators enforce business rules policy. experimental results shown training supervised learning yields reasonable policy small number training dialogs initial policy accelerates optimization reinforcement learning substantially. knowledge ﬁrst demonstration end-to-end learning dialog control task-oriented domains. oliver lemon kalliroi georgila. hybrid reinforcement/supervised learning dialogue policies proc workshop communicator data. knowledge reasoning practical dialogue systems intl joint conf artiﬁcial intelligence edinburgh pages matthew henderson blaise thomson steve young. word-based dialog state tracking recurrent neural networks. proc sigdial workshop discourse dialogue philadelphia usa. takuya hiraoka graham neubig koichiro yoshino tomoki toda satoshi nakamura. active learning example-based dialog systems. proc intl workshop spoken dialog systems saariselka finland. chiori hori kiyonori ohtake teruhisa misu hideki kashioka satoshi nakamura. statistical dialog management applied wfst-based dialog systems. acoustics speech signal processing icassp ieee international conference pages april. lluis hurtado david griol emilio sanchis encarna segarra. stochastic approach dialog management. proc ieee workshop automatic speech recognition understanding juan puerto rico usa. jozefowicz wojciech zaremba ilya sutskever. empirical exploration recurrent network architectures. proceedings international conference machine learning pages filip jurˇcíˇcek blaise thomson steve young. natural actor belief critic reinforcement algorithm learning parameters dialogue systems modelled pomdps. transactions speech language processing nate kohl peter stone. policy gradient reinforcement learning fast quadrupedal locomotion. robotics automation proceedings. icra’. ieee international conference volume pages ieee. esther levin roberto pieraccini wieland eckert. stochastic model human-machine interaction learning dialogue ieee trans speech audio prostrategies. cessing ryan lowe nissan iulian serban joelle pineau. ubuntu dialogue corpus large dataset research unstructured multi-turn dialogue systems. proc sigdial workshop discourse dialogue prague czech republic. dialogue management joseph polifroni. mercury ﬂight proceedings anlp/naacl workshop conversational systems volume pages anlp/naacl-convsyst association computational linguistics. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature jason williams eslam kamal mokhtar ashour hani jessica miller gezweig. fast easy language understanding dialog systems microsoft language understanding intelligent service proc sigdial workshop discourse dialogue prague czech republic. help you? call michael theres person named michael. full name? call michael seltzer savephonetypeavail calling michael seltzer work placecall help you? call michel sorry don’t know names called michel. again? call michael seltzer savephonetypeavail calling michael seltzer work placecall jason williams. applying pomdps dialog systems troubleshooting domain. naacl-hlt workshop bridging academic industrial research dialog technologies rochester york pages subset example dialogs created paper given below. space entity tags appear user system sides dialogs removed example call <name>jason</name> shown call jason. help you? call frank cellphone sorry don’t cellphone number frank seide. work phone. want call number? savephonetypeavail calling frank seide work placecall", "year": 2016}