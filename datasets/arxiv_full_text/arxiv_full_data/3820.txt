{"title": "Recurrent Neural Network Grammars", "tag": ["cs.CL", "cs.NE"], "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "text": "introduce recurrent neural network grammars probabilistic models sentences explicit phrase structure. explain efﬁcient inference procedures allow application parsing language modeling. experiments show provide better parsing english single previously published supervised generative model better language modeling state-of-the-art sequential rnns english chinese. sequential recurrent neural networks remarkably effective models natural language. last years language model results substantially improve long-established state-ofthe-art baselines obtained using rnns well various conditional language modeling tasks machine translation image caption generation dialogue generation despite impressive results sequential models priori inappropriate models natural language since relationships among words largely organized terms latent nested structures rather sequential surface order probabilistic model sentences explicitly models nested hierarchical relationships among words phrases. rnngs operate recursive syntactic process reminiscent probabilistic context-free grammar generation decisions parameterized using rnns condition entire syntactic derivation history greatly relaxing context-free independence assumptions. foundation work top-down variant transition-based parsing give variants algorithm parsing generation. several transition-based neural models syntactic generation exist relied structure building operations based parsing actions shift-reduce leftcorner parsers operate largely bottomfashion. construction appealing because inference relatively straightforward limuse top-down grammar information helpful generation rnngs maintain algorithmic convenience transitionbased parsing incorporate top-down syntactic information rnngs based lends discriminative modeling well sequences transitions modeled conditional full input sentence along incrementally constructed syntactic structures. similar previously published discriminative bottomtransition-based parsers greedy prediction model yields linearthe left-corner parsers used henderson incorporate limited top-down information complete path root tree terminal generally present terminal generated. refer henderson example. time deterministic parser however algorithm generates arbitrary tree structures directly without binarization required shift-reduce parsers. discriminative model also lets ancestor sampling obtain samples parse trees sentences used solve second practical challenge rnngs approximating marginal likelihood tree sentence generative model. present simple importance sampling algorithm uses samples discriminative parser solve inference problems generative model experiments show rnngs effective language modeling parsing generative model obtains best-known parsing results using single supervised generative model better perplexities language modeling state-of-the-art sequential lstm language models. surprisingly—although line previous parsing results showing effectiveness generative models parsing generative model obtains signiﬁcantly better results parsing discriminative model. formally rnng triple consisting ﬁnite nonterminal symbols ﬁnite terminal symbols collection neural network parameters explicitly deﬁne rules since implicitly characterized algorithm grammar uses generate trees strings language characterized terms transition-based algorithm outlined next section. section that semantics parameters used turn stochastic algorithm generates pairs trees strings discussed. rnngs based top-down generation algorithm relies stack data structure partially completed syntactic constituents. emphasize similarity algorithm familiar bottom-up shift-reduce recognition algorithms ﬁrst present parsing version algorithm present modiﬁcations turn generator parser transitions parsing algorithm transforms sequence words parse tree using data structures bottomalgorithm sagae lavie algorithm begins stack empty complete sequence words input buffer buffer contains unprocessed terminal symbols stack contains terminal symbols open nonterminal symbols completed constituents. timestep following three classes operations selected classiﬁer based current contents stack buffer introduces open nonterminal onto stack. open nonterminals written nonterminal symbol preceded open parenthesis e.g. examples therefore include them. transition closely related operations used earley’s algorithm likewise introduces nonterminals symbols predict operation later completes consuming terminal symbols time using scan likewise closely related linearized parse trees proposed vinyals top-down left-to-right decompositions trees used previous generative parsing language modeling work connection parsing uses unbounded lookahead distinguish parse alternatives top-down parser however parser uses encoding lookahead rather dfa. constraints parser transitions. guarantee well-formed phrase-structure trees produced parser impose following constraints transitions applied step function parser state number open nonterminals stack operation applied quires special handling. however leaving reduces number transitions also reduces number action types reduce runtime. furthermore standard parsing evaluation scores depend preterminal prediction accuracy. since parser allows unary nonterminal productions inﬁnite number valid trees ﬁnite-length sentences. constraint prevents classiﬁer misbehaving generating excessively large numbers nonterminals. similar constraints proposed deal analogous problem bottom-up shift-reduce parsers generator transitions parsing algorithm maps sequences words parse trees adapted michanges produce algorithm stochastically generates trees terminal symbols. changes required input buffer unprocessed words rather output buffer instead shift operation operations generate terminal symbol stack output buffer. timestep action stochastically selected according conditional distribution depends current contents algorithm terminates single completed constituent remains stack. fig. shows example generation sequence. transition generates trees using nearly structure building actions stack conﬁgurations top-down construction proposed abney albeit without restriction trees chomsky normal form. transition sequences trees parse tree converted sequence transitions depth-ﬁrst left-to-right traversal parse tree. since unique depth-ﬁrst leftro-right traversal tree exactly transition sequence tree. tree sequence symbols write indicate corresponding sequence generation transitions indicate parser transitions. figure parser transitions showing stack buffer open nonterminal count action type. represents stack contains open nonterminals completed subtrees; represents buffer unprocessed terminal symbols; terminal symbol nonterminal symbol completed subtree. stack right buffer consumed left right. elements stack buffer delimited vertical action buffer the| hungry| cat| meows| the| hungry| cat| meows| the| hungry| cat| meows| hungry| cat| meows| cat| meows| meows| meows| meows| output buffer stack history sequences grow unboundedly obtain representations recurrent neural networks encode contents since output buffer history actions appended contain symbols ﬁnite alphabet straightforward apply standard encoding architecture. stack complicated reasons. first elements stack complicated objects symbols discrete alphabet open nonterminals terminals full trees present stack. second manipulated using push operations. efﬁciently obtain representations push operations stack lstms represent complex parse trees deﬁne syntactic composition function recursively deﬁnes representations trees. reduce operation executed parser pops sequence completed subtrees and/or tokens stack makes children recent open nonterminal stack completing constituent. compute embedding subtree composition function based bidirectional lstms illustrated fig. suming availability constant time push operations runtime linear number nodes parse tree generated parser/generator since bound number output nodes parse tree function number input words stating runtime complexity parsing algorithm function input size requires assumptions. assuming ﬁxed constraint maximum depth linear. comparison models generation algorithm algorithm differs previous stack-based parsing/generation algorithms ways. first constructs rooted tree structures second transition operators capable directly generating arbitrary tree structures rather than e.g. assuming binarized trees case much prior work used transition-based algorithms produce phrase-structure trees rnngs generator transition presented deﬁne joint distribution syntax trees words distribution deﬁned sequence model generator transitions parameterized using continuous space embedding algorithm state time step i.e. representation algorithm state time computed combining representation generator’s three data structures output buffer represented embedding stack represented embedding figure neural architecture deﬁning distribution given representations stack output buffer history actions details composition architecture action history lstm elements stack shown. architecture corresponds generator state line figure ﬁrst vector read lstm forward reverse directions embedding label constituent constructed followed embeddings child subtrees forward reverse order. intuitively order serves notify lstm sort head looking processes child node embeddings. ﬁnal state forward reverse lstms concatenated passed afﬁne transformation tanh nonlinearity become subtree embedding. because child node embeddings computed similarly composition function kind recursive neural network. word generation reduce size word generation broken parts. first decision generate made choosing word conditional current parser state. reduce computational complexity modeling generation word class-factored softmax second could deal n-ary nodes made poor nonterminal information crucial task. discriminative parsing model discriminative parsing model obtained replacing embedding time step embedding input buffer train model conditional likelihood sequence actions given input string maximized. parser need able parse tree i.e. tree maximizes however unbounded dependencies across sequence parsing actions model exactly solving either inference problems intractable. obtain estimates these importance sampling algorithm uses conditional proposal distribution following properties samples obtained efﬁciently; conditional probabilities samples known. many distributions available discriminatively trained variant parser fulﬁlls requirements sequences actions sampled using simple ancestral sampling approach since parse trees action sequences exist one-to-one relationship product action probabilities conditional probability parse tree therefore discriminative parser proposal distribution. present results models parsing language model english chinese. data. english penn treebank used training corpus both held validation used evaluation. singleton words training corpus unknown word classes using berkeley parser’s mapping rules. orthographic case distinctions preserved numbers normalized. chinese penn chinese treebank version chinese experiments single unknown word class. corpus statistics given table model training parameters. discriminative model used hidden dimensions -layer lstms generative model used dimensions layer lstms. models tuned dropout rate maximize validation likelihood obtaining optimal rates sequential lstm baseline language model also found optimal dropout rate training used stochastic gradient descent learning rate parameters initialized according recommendations given glorot bengio english parsing results. table gives performance parser section well performance several representative models. discriminative model used greedy decoding rule opposed beam search shift-reduce baselines. generative model obtained independent samples ﬂattened distribution discriminative parser reranked according this preprocessing scheme similar standard parsing standard language modeling. however since model parser language model opted parser normalization. table parsing results indicates result trained corpus without ensembling. model vinyals henderson socher petrov klein shindo single shindo ensemble mcclosky vinyals single discriminative generative clear experiments proposed generative model quite effective parser language model. result relaxing conventional independence assumptions inferring continuous representations symbols alongside non-linear models syntactic relationships. signiﬁcant question remains discriminative model—which information available generative model—performs worse generative model. pattern observed neural parsing henderson hypothesized larger unstructured conditioning contexts harder learn from provide opportunities overﬁt. discriminative model conditions entire history stack buffer generative model accesses history stack. fully discriminative model vinyals able obtain results similar generative model similar results discriminative parser using data. light results believe henderson’s hypothesis correct generative models considered statistically efﬁcient method learning neural networks small data. language model results. report held-out perword perplexities three language models sequential syntactic. probabilities normalized number words recurrent neural network language models syntactic language modeling. recurrent neural network language models rnns compute representations unbounded history words left-to-right language model syntactic language models jointly generate syntactic structure sequence words extensive literature here strand work emphasized bottom-up generation tree using variants shift-reduce parser actions deﬁne probability space neural-network–based model henderson particularly similar using unbounded history neural network architecture parameterize generative parsing based left-corner model. dependency-only language models also explored modeling generation top-down rooted branching process recursively rewrites nonterminals explored charniak roark particular note work charniak uses random forests hand-engineered features entire syntactic derivation history make decisions next action take. neural networks model sentences structured according syntax sentence generated. syntactically structured neural architectures explored number applications including discriminative parsing sentiment analysis sentence representation however models been without exception discriminative; ﬁrst work syntactically structured neural models generate language. earlier work demonstrated sequential rnns capacity recognize contextfree languages contrast work understood incorporating context-free inductive bias model structure. second possibility replace sequential generation architectures found many neural network transduction problems produce sentences conditioned input. previous work machine translation showed conditional syntactic models function quite well without computationally expensive marginalization process decoding time third consideration regarding rnngs human sentence processing takes place left-toright incremental order. rnng processing model fact left-to-right opens several possibilities developing sentence processing models based explicit grammars similar processing model charniak finally although considered supervised learning scenario rnngs joint models could trained without trees example using expectation maximization. introduced recurrent neural network grammars probabilistic model phrase-structure trees trained generatively used language model parser corresponding discriminative model used parser. apart out-of-vocabulary preprocessing approach requires feature design transformations treebank data. generative model outperforms every previously published parser built single supervised generative model english behind best-reported generative model chinese. language models rnngs outperform best single-sentence language models. swabha swayamdipta feedback drafts paper buys phil blunsom zhang help data preparation. work sponsored part defense advanced research projects agency information innovation ofﬁce resource languages emergent incidents program issued darpa/io contract hr--c-; also supported part contract wnf--- darpa army research ofﬁce approved public release distribution unlimited. views expressed authors reﬂect ofﬁcial policy position department defense u.s. government. miguel ballesteros supported european commission contract numbers fp-ict- h-ria-", "year": 2016}