{"title": "Deep Matching Autoencoders", "tag": ["cs.CV", "stat.ML"], "abstract": "Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.", "text": "tion deep multi-modal models deepcca branch deep networks tasks imagecaption matching zero-shot learning nevertheless pervasive limitation methods fully supervised methods sense require paired training data learn cross-modal mapping embedding space. however many applications paired data relatively sparse compared unpaired data case semi-supervised cross-modal learning methods would beneﬁcial exploit abundant unpaired data. moreover cases desirable learn pools data modality completely unpaired necessitating unsupervised cross-modal learning. paper address task cross-modal learning partially completely unpaired data. prior attempts address inferring pairings partially completely unmatched data. include kernelized sorting least-square object matching matching however existing algorithms shallow thus perform well challenging complex data representation learning important images text. introduce deep matching autoencoders knowledge provide ﬁrst deep representation learning approach unpaired crossmodal learning. dmae method employs auto-encoder model data view learned minimizing reconstruction error usual. introduce latent alignment matrix model unknown pairing views optimize using cross-modal dependency measures kernel target alignment squaredloss mutual information framework simultaneously learn autoencoding representation cross-view pairing. representation trained support cross-view matching. training learned representation improves cross-view matching progressively disambiguated cross-modal items paired accurately learned representation progressively improves. increasingly many real world tasks involve data multiple modalities views. motivated development many effective algorithms learning common latent space relate multiple domains. however existing cross-view learning algorithms assume access paired data training. applicability thus limited paired data assumption often violated practice many tasks small subset data available pairing annotation even paired data all. paper introduce deep matching autoencoders learn common latent space pairing unpaired multi-modal data. speciﬁcally formulate cross-domain representation learning object matching problem. simultaneously optimise parameters representation learning auto-encoders pairing unpaired multi-modal data. framework elegantly spans full regime fully supervised semisupervised unsupervised multi-modal learning. show promising results image captioning task uniquely enabled methodology unsupervised classiﬁer learning. learning representations multi-modal data widely relevant problem setting many applications machine learning pattern recognition. computer vision arises tagging cross-view crossmodal learning. particularly relevant border vision modalities example audiovisual speech classiﬁcation generating descriptions images videos case audio text respectively. wide applicability multi-modal representation learning motivated study numerous cross-modal learning methods including canonical correlation analysis kernel progress further accelerated recently contribution large parallel datasets permitted applicafully supervised fully unsupervised cross-modal learning. fully supervised case corresponds conventional cross-modal learning approach alternative deepcca branch matching nets except statistical dependency-based rather correlation ranking-based loss. show approach performs comparably state alternatives supervised tasks. interestingly approach effective semi-supervised learning show able better exploit unlabeled multi-modal data improve performance compared alternatives matching interestingly approach even effective unsupervised crossmodal learning pairings given. demonstrate capability introducing solving novel task termed unsupervised classiﬁer learning task assume pool unlabelled images given along pool category embeddings describe images pool. however unsupervised pairings images categories given. task corresponds application pool images idea classes likely represented images; speciﬁc class-image pairings. based inputs alone train classiﬁers recognise categories represented category embedding pool. like classic clustering problem task unsupervised supervision/pairing given. however like conventional supervised learning setting produces classiﬁers speciﬁc nameable image categories output. task seen extreme version zero-shot learning auxiliary image class embedding pairs available learn image-category embedding mapping. image-category mapping must learned entirely unsupervised way. contributions summarized follows propose dmae cross-view learning matching framework elegantly spans supervised semisupervised unsupervised cross-modal learning. figure multimodal learning unpaired data problem setting. dmae inputs unpaired instances view learns permutation matrix associating objects across views. representation maximum statistical dependency. caption comment. supervised multimodal learning setups privilege access i=). example paired data vector image vector text. unsupervised multi-modal learning setup access unpaired semi-supervised setup data {xi}n mixture supervised unsupervised setup. supervised multi-modal learning established supervised multi-modal learning algorithm canonical correlation analysis learns linear projection features views maximally correlated common latent space. studied extensively number useful properties particular optimal linear projection mapping obtained solving eigenvalue decomposition. also extended non-linear case kernelization huge success deep neural network computer vision inspired many deep multi-modal learning algorithms including deepcca multi-modal deep autoencoders branch matching ranking networks deepcca shares correlation maximizing objective classic learns non-linear projection deep neural networks. shown outperform linear non-linear kcca extension. multi-modal daes multi-modal autoencoders trained shared hidden layer. generally paired data used train branch dnns learn view-invariant embeddings example learning rank objective. contrast euclidean-based metrics statistical dependency-based measures like hilbert-schmidt independence criterion much less studied objectives multi-modal learning. examhowever supervised algorithms particularly deep learning ones require large number paired samples learn effective cross-modal embedding. unsupervised multi-modal learning desirability learning widely available unpaired data motivated research harder problem unsupervised cross-modal learning introducing latent variables cross-view pairing. early approach matching alternates learning joint embedding space solving bipartite matching problem associate unpaired data. unlike statistical dependency measures cca’s correlation-based objective requires comparable embeddings estimate match. matching never bootstrap initialised completely random embeddings pairing information all. indeed shown work used seed paired samples bootstrapping i.e. semisupervised setting. probabilistic latent variable approaches also proposed match across-views however demonstrated work problems. limited linear projections. handle non-linearity unsupervised multi-modal learning kernel based approaches proposed including kernelized sorting least-squared object matching unpaired data matched maximizing hsic outperformed mcca tasks lsom squared-loss mutual information used dependence measure experimentally shown outperform hsic-based however lsom shallow methods perform well image text data representation learning beneﬁcial. paper leverage hsic smi-based objectives learning representations matching deeper context. visual description natural language generating matching natural language descriptions images videos recently become popular topic cross-modal learning last years common approach learn image representation text representation common latent space two-branch deep networks based latent space images videos associated text descriptions matched supporting annotation retrieval applications. proposed dmae solves supervised image captioning comparably well state methods. unlike prior approaches generalized semi-supervised unsupervised case exploiting unpaired data. zero-shot learning zero-shot learning aims improve scalability visual classiﬁer learning terms required image annotation. speciﬁcally assumes subset image categories provided example images aims generate classiﬁers disjoint image categories example images. achieved assuming image categories associated category embedding vector example word-vectors attributes given assumption cross-modal mapping learned visual feature space category embedding space using data seen classes. cross-modal mapping used classiﬁer order recognise unseen categories annotated examples provided dmae approach related methods applied learn cross-modal embeddings images category vectors hence also used classiﬁer novel classes. however crucial beneﬁts learned semi-supervised encompasses transductive semisupervised variants zsl. interestingly also learned entirely un-supervised requiring paired samples all; unlike existing methods. term speciﬁc problem setting unsupervised classiﬁer learning recent method revise related also beneﬁt semi-supervised learning setting hsic-based domain adaptation loss. however revise engineered speciﬁcally zsl. contrast dmae general cross-modal learner address completely unsupervised setting unlike revise. introduce cross-domain object matching methodology deep matching autoencoders unsupervised learning perspective paired training data assumed. semi-supervised supervised variants straightforward special case. exposition simplicity also assume equal number samples view relaxed practice. multi-view autoencoders trace operator gram matrix gram matrix similarity function takes large value gram matrices similar small value similar. note that original normalization term. however makes optimization hard thus employ unnormalized variant kta. moreover ukta regarded non-centered variant hsic goal learn comparable representation embeddings given paired training data. signiﬁcantly harder problem multi-modal autoencoder approaches rely paired data learn unpaired data introduce permutation matrix represent unknown correspondence data items views permutation function corresponding permutation indicator matrix simultaneously optimising autoencoders well cross-domain match tradeoff parameter component function non-negative statistical dependence measure views. needs measure require comparable representations priori order enable learning started. statistical dependence measure crucial component achieving goal. paper explore alternatives unnormalized kernel target alignment squared-loss mutual information note independence measure. however since want make generate similar representations dependence measure. initializing ﬁrst independently estimate using autoencoders. employ alternative optimization learning together. optimize ﬁxed optimize ﬁxed alternation continued convergence. optimization ﬁxed permutation matrix objective function written deep-ukta dmae-ukta without reconstruction loss. deep-smi dmae-smi without reconstruction loss. alternatives supervised supervised learning evaluate following state alternatives deepcca deep architecture hsic-deepcca baseline created. extending hsiccca include deep architecture updating deepcca hsic rather correlation-based loss. two-way nets nets pre-trained networks followed fully connected layers relu nonlinearities captioning only. revise uses autoencoders modality minimizing reconstruction loss modality also maximum mean discrepancy only. settings implement dmae theano. number encoding decoding layers regularization parameter kernel parameters experiments. experiments nvidia processors. image-sentence/sentence-image retrieval benchmark details evaluate image→sentence sentence→image retrieval using widely studied flickrk ms-coco datasets. flickrk consists images accompanied descriptions. larger ms-coco dataset consists images along accompanied descriptions. dataset testing images. flickrk test sentences coco compare methods evaluation metrics proposed image-text text-image matching performance quantiﬁed recallk encode image vgg- deep feature word-vector average sentence. supervised learning ﬁrst experiment evaluate methods prior state imagesentence matching standard supervised learning setting. results table make following observations provides better objective method ukta expected since ukta special case smi. autoencoding approach helpful cross-view representation learning dmae-smi dmae-ukta outperform deep-smi deep-ukta respectively. overall approach performs comparably slightly better alternatives fully supervised image-caption matching setting. despite competitive alternative uses advanced fisher-vector representation text domain. semi-supervised unsupervised learning second experiment investigate whether possible learn captioning partially paired unpaired data. results table left block uses fully supervised case fully supervised case case trivial extension above. given need optimize matching criterion many-one-pairing introduced methodology pairing square matrix. relax assumption obtain many-one pairing considering rectangular removing column-sum constraint. evaluate contributions sets experiments including image-caption matching classiﬁer learning alternatives semi-supervised evaluate proposed dmae-ukta dmae-smi methods following alternatives unpaired data learning mcca matching learning paired unpaired data across multiple views. deep-mcca original matching shallow method. extend multi-layer deep architecture. table fully supervised image-sentence matching results flickrk ms-coco. improved fusion hsic-cca deep-cca implementation reviseb variant block prior methods. middle block ablations method. bottom block methods. speciﬁed labeled data right block uses labeled available unlabeled data. make following observations expected task clearly signiﬁcantly harder percentage labeled data decreases towards zero dmae particularly dmae-smi perform effective semi-supervised learning exploiting unlabelled data outperform supervised case performance decreases less rapidly labeled data decreases. dmae particularly dmae-smi uses unlabeled data much effectively prior mcca deep-mcca extension outperforming semi-supervised learning evaluation point. dmae-smi performs signiﬁcantly chance completely unsupervised image-caption matching. unsupervised classiﬁer learning consider training classiﬁer given stack images stack category embeddings describe categories covered images stack. ‘unsupervised classiﬁer learning’ problem annotated images pairings given. category labels images unknown categories least annotated image semisupervised learning problem. case category labels images unknown categories annotated images zero-shot learning problem. category labels images known standard supervised learning problem. framework apply settings fully supervised zero-shot learning well studied focus unsupervised semi-supervised variants. benchmark details evaluate approach cifar- datasets. category embeddings word-vectors image features vgg- features feature cifar-. thus image data stack images category domain data stack word vectors. unsupervised dmae learns joint embedding association matrix }n×m pairs images categories. learned ideally match -hot label matrix would normally given target supervised learning. settings consider three settings unsupervised learning paired data given. semi-supervised learning subset paired data given remaining unpaired data exploited. transductive semisupervised semi-supervised model also testing data training. metrics fully diagnose performance evaluate metrics matching accuracy. accuracy presiﬁer evaluate accuracy image recognition testing split using trained svm. results unsupervised matching unsupervised classiﬁer learning setting non-trivial achievement correctly estimate associations images categories better chance since annotated pairings domains priori comparable. quantrue compute compute precision recall class. learning dmae-smi obtain impressive precision recall averaged classes given prior pairings start with. accuracy estimation changes learning visualise mean precision recall learning iterations figure that precision recall rise monotonically time eventually asymptoting. dmae-smi performance grows faster converges higher point alternatives. results testing accuracy complete evaluation actual learned classiﬁer next assume estrain classiﬁer evaluated testing split dataset. results cifar- shown tables respectively. shown l/u/t proportions listed supervised semi-supervised semi-supervised transductive unsupervised settings evaluated. results that unsupervised learned classiﬁers work well method exploit semisupervised learning also exploit seamlessly semi-supervised transductive learning unpaired testing data available method outperforms alternatives experiment proposed deep matching autoencoders capable learning pairing common latent space unpaired multi-modal. dmae approach elegantly spans unsupervised semi-supervised fully supervised zero-shot settings. supervised setting competitive state alternatives captioning. less studied semi-supervised unsupervised settings outperforms existing alternatives. prior studies unsupervised cross-domain matching generally used data. showed dmae approach scale real vision language data solve novel unsupervised classiﬁer learning problem. although evaluated approach captioning classiﬁer learning widely applicable cross-modal method potential applications person re-identiﬁcation wordalignment explore future work.", "year": 2017}