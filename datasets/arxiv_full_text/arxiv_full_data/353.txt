{"title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using  Block-CirculantWeight Matrices", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregular network structure after pruning; 2) the increased training complexity; and 3) the lack of rigorous guarantee of compression ratio and inference accuracy. To overcome these limitations, this paper proposes CirCNN, a principled approach to represent weights and process neural networks using block-circulant matrices. CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and training) from O(n2) to O(nlogn) and the storage complexity from O(n2) to O(n), with negligible accuracy loss. Compared to other approaches, CirCNN is distinct due to its mathematical rigor: it can converge to the same effectiveness as DNNs without compression. The CirCNN architecture, a universal DNN inference engine that can be implemented on various hardware/software platforms with configurable network architecture. To demonstrate the performance and energy efficiency, we test CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN architecture achieves very high energy efficiency and performance with a small hardware footprint. Based on the FPGA implementation and ASIC synthesis results, CirCNN achieves 6-102X energy efficiency improvements compared with the best state-of-the-art results.", "text": "abstract large-scale deep neural networks compute memory intensive. size dnns continues grow critical improve energy efficiency performance maintaining accuracy. dnns model size important factor affecting performance scalability energy efficiency. weight pruning achieves good compression ratios suffers three drawbacks irregular network structure pruning affects performance throughput; increased training complexity; lack rigorous guarantee compression ratio inference accuracy. overcome limitations paper proposes circnn principled approach represent weights process neural networks using block-circulant matrices. circnn utilizes fast fourier transform -based fast multiplication simultaneously reducing computational complexity storage complexity training) negligible accuracy loss. compared circnn architecture recursive property used computing kernel ensures universal small-footprint implementations. compressed regular network structure avoids pitfalls network pruning facilitates high performance throughput highly pipelined parallel design. demonstrate performance energy efficiency test circnn fpga asic embedded processors. results show circnn architecture achieves high energy efficiency permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior specific permission and/or fee. request permissions permissionsacm.org. micro- october cambridge association computing machinery. isbn ----//.... https//doi.org/./. performance small hardware footprint. based fpga implementation asic synthesis results circnn achieves energy efficiency improvements compared best state-of-the-art results. keywords deep learning block-circulant matrix compression acceleration fpga reference format caiwen ding+ siyu liao+ yanzhi wang+ ning youwei zhuo chao wang xuehai qian geng yuan xiaolong yipeng zhang jian tang qinru yuan. circnn accelerating compressing deep neural networks using blockcirculant weight matrices. proceedings micro- cambridge october pages. https//doi.org/./. introduction first decade century neural networks experiencing phenomenal resurgence thanks data significant advances processing speeds. large-scale deep neural networks able deliver impressive results many challenging problems. instance dnns breakthroughs object recognition accuracy imagenet dataset even achieving human-level performance face recognition promising results triggered revolution several traditional emerging real-world applications self-driving systems automatic machine translations drug discovery toxicology result academia industry show rising interests significant resources devoted investigation improvement promotion deep learning methods systems. enablers unprecedented success deep learning availability large models. modern dnns typically consist multiple cascaded layers least millions hundreds millions parameters entire model larger-scale neural networks tend enable extraction complex high-level features therefore lead significant improvement overall accuracy side layered deep structure large model sizes also demand increasing computational capability memory requirements. order achieve higher scalability performance energy efficiency deep learning systems orthogonal research development trends attracted enormous interests. first trend hardware acceleration dnns extensively investigated industry academia. representative technique fpga-based accelerators offer good programmability high degree parallelism short development cycle. fpga used accelerate original dnns binary neural networks recently dnns model compression alternatively asic-based implementations recently explored overcome limitations general-purpose computing approaches. number major hightech companies announced asic chip designs inference framework intel google etc. academia three representative works architectural level eyeriss diannao family focus specifically convolutional layers fully-connected layers memory design/organization respectively. number recent tapeouts hardware deep learning systems prior works mainly focus inference phase dnns usually suffer frequent accesses off-chip dram systems limited on-chip sram memory hardly accommodate large model sizes. unfortunately off-chip dram accesses consume significant energy. recent studies show per-bit access energy off-chip dram memory compared on-chip sram. therefore easily dominate whole system power consumption. energy efficiency challenge large models motivates second trend model compression. several algorithm-level techniques proposed compress models accelerate dnns including weight quantization connection pruning rank approximation approaches offer reasonable parameter reduction minor accuracy degradation. however suffer three drawbacks sparsity regularization pruning typically result irregular network structure thereby undermining compression ratio limiting performance throughput training complexity increased additional pruning process rank approximation step etc.; compression ratios depending network heuristic cannot precisely controlled. believe ideal model compression technique should maintain regular network structure; reduce complexity inference training importantly iii) retain rigorous mathematical fundation compression ratio accuracy. effort achieve three goals propose circnn principled approach represent weights process neural networks using block-circulant matrices concept block-circulant matrix compared ordinary unstructured matrix shown fig. square circulant matrix vector circulant reformat vectors. non-squared matrix could represented square circulant submatrices therefore representing matrix vector first benefit circnn storage size reduction. fig. unstructured weight matrix holds parameters. suppose represent weights using circulant matrices need store parameters easily leading model size reduction. intuitively reduction ratio determined block size circulant submatrices larger block size leads high compression ratio. general storage complexity reduced vectors represent outputs neurons previous layer current layer respectively; m-by-n weight matrix; activation function. block-circulant matrix fast fourier transform -based fast multiplication method utilized computational complexity reduced unstructured weight matrices retrain network; circnn directly trains network assuming block-circulant structure. leads advantages. first prior work reduce model size heuristic factor depending network circnn provides adjustable fixed reduction ratio. second fft-based fast multiplication computational complexity training also unfortunately prior work reduced training complexity. storage computational complexity reduction circnn clearly attractive. question network really represented block-circulant matrices accuracy loss? question natural much less weights vectors network able approximate function network unstructured weight matrices. fortunately answer question yes. circnn mathematically rigorous developed theoretical foundation formal proof showing dnns represented block-circulant matrices converge effectiveness\" dnns without compression fundamentally distinguishing method prior convolutional layer name implies performs two-dimensional convolution extract features inputs subsequent layers extracting higherlevel features. conv layer associated learnable filters activated specific types features found spatial positions inputs. filter-sized moving window applied inputs obtain feature maps calculating convolution filter inputs moving window. convolutional neuron representing pixel feature takes inputs corresponding filter weights calculate inner-product. given input feature r-sized filter output feature calculated xa+i−b+j− elements respectively. multiple convolutional kernels adopted extract different features input feature map. multiple input feature maps convolved filter results summed derive single feature map. pooling layer performs subsampling operation extracted features reduce data dimensions mitigate overfitting issues. here subsampling operation inputs pooling layer realized various non-linear operations average l-norm calculation. among them pooling dominant type pooling strategy state-of-the-art dcnns higher overall accuracy convergence speed among three types layers majority computation occurs conv layers pool layer relatively lower computational complexity storage requirement dnns weight matrices layers convolutional kernels conv layers. result conv layers become major research focuses energy-efficient implementation weight reduction dnns. based block-circulant matrix-based algorithms propose circnn architecture universal inference engine implemented various hardware/software platforms configurable network architecture applying circnn neural network accelerators enables notable architectural innovations. recursive property intrinsic role circnn implemented basic computing block. ensures universal small-footprint implementations. pipelining parallelism optimizations. taking advantage compressed regular network structures aggressively apply inter-level intra-level pipelining basic computing block. moreover conduct joint-optimizations considering parallelization degree performance power consumption. platform-specific optimizations focusing weight storage memory management. demonstrate performance energy efficiency test circnn architecture three platforms fpga asic embedded processors. results show circnn architecture achieves high energy efficiency performance small hardware footprint. based fpga implementation asic synthesis results circnn achieves energy efficiency improvements compared best state-of-the-art results. background motivation deep neural networks deep learning systems constructed using different types architectures including deep convolutional neural networks deep belief networks recurrent neural networks despite differences network structures target applications share construction principle multiple functional layers cascaded together extract features multiple levels abstraction fig. illustrates multi-layer structure example dcnn consists stack fullyconnected layers convolutional layers pooling layers. three types layers fundamental deep learning systems. fully-connected layer storage-intensive layer architectures since neurons fully connected neurons previous layer. computation procedure layer consists matrix-vector arithmetics transformation activation function described follows mathematical investigations demonstrated significant sparsity margin weight reduction dnns number prior works leverage property reduce weight storage. techniques classified categories. systematic methods singular value decomposition despite systematic methods typically exhibit relatively high degradation overall accuracy heuristic pruning methods heuristic weight together weight quantization. method could achieve better parameter reductions i.e. small accuracy degradation. however network structure weight storage besides pros cons approaches prior works share following common limitations mainly focusing weight reduction rather computational complexity reduction; reducing model size heuristic factor instead reducing big-o complexity; performing weight pruning applying matrix transformations based trained model thereby adding complexity training process. third item crucial limit scalability future larger-scale deep learning systems. fft-based methods lecun proposed using ffts accelerate computations conv layers applies single filter conv layer uses calculate traditional inner products filters input feature maps achieve speedup large filter sizes underlying neural network structure parameters remain unchanged. speedup filter reuse cannot achieve either asymptotic speedup big-o notation weight compressions work closely related circnn proposed circulant matrix inference training algorithms. however number limitations. first applied layers conv layer. limits potential gain weight reduction performance. second uses single circulant matrix represent weights whole layer. since number input output neurons usually same method leads storage waste padded zeros novelty circnn compared lecun circnn fundamentally different achieves asymptotic speedup big-o notation weight compression simultaneously. compared circnn generalizes three significant novel aspects. supporting conv layers. unlike layers matrices conv layers small filters instead representing filter circulant matrix circnn exploits inter-filter sparsity among different filters. another word circnn represents matrix filters input output channels figure baseline circnn. baseline method formulates large square circulant matrix layer weight representation numbers inputs outputs equal whereas proposed method uses block-circulant matrix achieve fine-grained tradeoff accuracy compression/acceleration. block-circulant matrices. mitigate inefficiency single large circulant matrix used circnn uses blockcirculant matrices weight representation. benefits twofold. first avoids wasted storage/computation zero padding numbers inputs outputs equal. second allows derive fine-grained tradeoff accuracy compression/acceleration. specifically achieve better compression ratio larger block size used however lead accuracy degradation. smaller block sizes provide better accuracy less compression. compression block size mathematical rigorousness. importantly perform theoretical analysis prove effectiveness\" block-circulant matrix-based dnns approach original networks without compression. theoretical proof also distinguishes proposed method prior work. outline proof discussed section details provided reports fig. illustrates difference baseline circnn. baseline method formulates large square circulant matrix zero padding layer weight representation numbers inputs outputs equal. contrast circnn uses block-circulant matrix avoid storage waste achieve fine-grained tradeoff accuracy compression/acceleration. overall novel techniques circnn algorithm level possible achieve simultaneous significant reduction computational storage complexity inference training. circnn algorithms foundation layer algorithm idea block-circulant matrix-based layers partition original arbitrary-size weight matrix rm×n blocks square sub-matrices sub-matrix circulant matrix. proved block-circulant matrices. calculated fft→elementtherefore wise multiplication→ifft procedure equivalent computational complexity layer. algorithm illustrates backward propagation process layer circnn. circnn inference training constitute integrated framework reduction computational complexity gained both. directly train vectors wij’s corresponding circulant sub-matrices wij’s layer using algorithm clearly network training procedure naturally follows block-circulant matrix structure. advantage circnn compared prior works require additional steps trained neural network. conv layer algorithm practical models conv layers often associated multiple input multiple output feature maps. result computation conv layer expressed format tensor computations below insights shown fig. denote block size assume blocks partitioning correspondingly input also partitioned then forward propagation process inference phase given column vector. assume circulant matrix defined vector i.e. first vector wij. according circulant convolution theorem denotes element-wise multiplications. operation procedure shown right fig. inference phase computational complexity layer equivalent small values. similarly storage complexity need store sub-matrix equivalent small values. therefore simultaneous acceleration model compression compared original achieved inference process. algorithm illustrates calculation inference process layer circnn. next consider backward propagation process training phase. l-th output element denote loss function. using chain rule derive backward propagation process follows input maps number input maps size convolutional kernel number output maps. generalize concept block-circulant structure\" rank- tensor conv layer i.e. slices form circulant matrices. next reformulate inference training algorithms conv layer matrix operations. inference process example training process formulated similar way. software tools caffe provide efficient methodology transforming tensor-based operations conv layer matrixbased operations order enhance implementation efficiency fig. illustrates application method reformulate eqn. matrix multiplication r×cr recall slice circulant matrix. according reshaping principle have means actually block-circulant matrix. hence fast multiplication approach block circulant matrix fft→ component-wise multiplication →ifft\" procedure applied accelerate thereby resulting acceleration proposed approach computational complexity reduced max. proof universal approximation property block circulant matrix-based neural networks briefly outlined follows objective prove continuous measurable function approximated arbitrary accuracy using blockcirculant matrix-based network. equivalently prove function space achieved block-circulant matrix-based neural networks dense space continuous measurable functions inputs. important property activation function i.e. component-wise discriminatory property proved. based property objective proved using proof contradiction hahn-banach theorem derived approximation error bound number neurons layer limited details shown implies approximation error reduce increasing i.e. increasing number neurons/inputs network. result guarantee universal effectiveness\" proposed framework different types sizes application domains hardware/software platforms. compression ratio test accuracy section apply circnn different models software investigate weight compression ratio accuracy. fig. show weight storage reduction layer test accuracy various image recognition datasets dcnn models mnist cifar- svhn stl- imagenet here -bit weight quantization adopted model size reduction. baselines original dcnn models unstructured weight matrices using -bit floating point representations. block-circulant weight matrices enable ×-+× reduction weight storage corresponding layers. parameter reduction layers also observed entire dcnn model size reduced applying block-circulant matrices layer regarding accuracy loss negligible sometimes compressed models even outperform baseline models. fig. illustrates application block-circulant weight matrices conv layers mnist svhn cifar- imagenet datasets accuracy degradation constrained optimizing block size. -bit weight quantization adopted softmax layer excluded. -bit quantization also contributes reduction model size. comparison reductions number parameters lenet- alexnet. moreover another crucial property circnn parameter storage compression regular whereas result irregular weight storage patterns. irregularity requires additional index weight significantly impacts available parallelism degree. results clearly significant benefit potential circnn could produce highly compressed models regular structure. circnn yields reductions parameters compared state-of-the-art results lenet- alexnet. fact actual gain could even higher indexing requirements also performed testing models found circnn achieve similar even higher compression ratio demonstrating wide application blockcirculant matrices. moreover acceleration training observed dbns less phenomenal model reduction ratio. gpus less optimized operation matrix-vector multiplications. circnn architecture based block-circulant matrix-based algorithms propose circnn architecture universal inference engine implemented various hardware/software platforms configurable network architecture applying circnn neural network accelerators enables notable architectural innovations. recursive property intrinsic role circnn implemented basic computing block ensures universal small-footprint implementations. pipelining parallelism optimizations taking advantage compressed regular network structures aggressively apply inter-level intra-level pipelining basic computing block. moreover conduct jointoptimizations considering parallelization degree performance power consumption. platform-specific optimizations focusing weight storage memory management.. circnn fft→component-wise multiplication →ifft\" fig. universal procedure used conv layers inference training processes different models. consider computing kernel circnn architecture recursive property. known highly efficient computational complexity hardware implementation investigated recursive property states calculation size-n implemented using ffts size plus additional level butterfly calculation shown fig. decomposed four ffts size additional levels. recursive property ensure universal reconfigurable design could handle different types sizes scales etc. because large-scale calculated recursively executing computing block additional calculations; ifft implemented using structure different preprocessing procedure parameters also ensures design small footprint because multiple small-scale blocks multiplexed calculate large-scale certain parallelism degree; actual hardware systems fpga asic designs pose constraints parallel implementation hardware footprint logic block/interconnect resource limitations. result define basic computing block parallelization degree depth shown fig. butterfly computation comprises cascade connection complex number-based multiplications additions basic computing block responsible implementing major computational tasks operation done decomposition iterative execution basic computing blocks. compared conventional calculation simplify computing based following observation inputs deep learning system actual applications real values without imaginary parts. therefore result level symmetric sequence except base component example shown basic computing block shown fig. partial outcomes layer butterfly computations symmetric therefore outcomes circles need calculated stored partial outcomes. observation significantly reduce amount computations storage partial results memory traffic. overall architecture overall circnn architecture shown fig. includes basic computing block peripheral computing block control subsystem memory subsystem subsystem basic computing block responsible major ifft computations. peripheral computing block responsible performing component-wise multiplication relu activation pooling etc. require lower computational complexity hardware footprint. implementations relu activation pooling comparators inherent difference compared prior work control subsystem orchestrates actual fft/ifft calculations basic computing block peripheral computing block. different sizes conv layer layer different types deep learning applications different setting fft/ifft calculations configured control subsystem. memory subsystem composed utilized store coefficients fft/ifft calculations used store weights e.g. results fft. asic design memory hierarchy utilized carefully designed ensure good performance. -bit fixed point numbers input weight representations common widely accepted enough accurate dnns furthermore pointed inaccuracy caused quantization largely independent inaccuracy caused compression quantization inaccuracy accumulate significantly deep layers. figure storage saving test accuracy using block-circulant layer dcnn models different datasets. storage saving using block-circulant layer block-circulant conv layer dcnns mnist svhn cifar- imagenet datasets. based definition larger values indicate higher level parallelism therefore lead higher performance throughput also higher hardware cost/footprint. larger value would also result less memory accesses cost higher control complexity. derive optimal values optienergy efficiency performance circnn architecture considers pipelining techniques shown fig. inter-level pipelining pipeline stage corresponds level basic computing block. intra-level pipelining additional pipeline stage added within butterfly computation unit i.e. deriving optimal stage division cascade connection complex number-based multiplication additions. proper selection pipelining scheme highly depends target operating frequency memory subsystem organization. experimental prototype target clock frequency around therefore inter-level pipelining simpler structure sufficient efficient implementations. hardware platforms. focus weight storage memory management order simplify design achieve higher energy efficiency performance. fpga platform. observation weight storage requirement representative applications on-chip block memory state-of-the-art fpgas. representative large-scale dcnn model imagenet application whole alexnet results around storage requirement applying block-circulant matrices layers using -bit fixed point numbers results negligible accuracy loss. storage requirement fulfilled on-chip block memory state-of-the-art fpgas intel stratix xilinx virtex- etc. consist tens on-chip memory moreover applying block-circulant matrices also conv layers storage requirement reduced even less then storage requirement becomes comparable input size potentially supported low-power high energy-efficiency fpgas intel cyclone xilinx kintex- fpgas. similar observation holds applications different network models like rnn. even future larger-scale applications full model compression fpga leveraging storage space integrated core memory observation would make fpga-based design significantly efficient. state-of-the-art fpgas on-chip memory organized form memory blocks certain capacity bandwidth limit. number on-chip memory blocks represents proper tradeoff lower control complexity higher energy efficiency thus become additional knob design optimizations. state-of-the-art fpgas equipped comprehensive resources variable-size multipliers effectively exploited performance energy efficiency improvements. asic platform. mainly investigate aspects memory subsystem potential memory hierarchy memory bandwidth aspect ratio. representative deep learning applications require hundreds multiple memory storage depending different compression levels assume conservative value multiple universal reconfigurable property circnn architecture. potential memory hierarchy structure depends strongly target clock frequency proposed system. specifically target clock frequency around memory hierarchy necessary single-level memory system support operating frequency. rather memory/cache reconfiguration techniques employed executing different types sizes applications performance enhancement static power reduction. target higher clock frequency effective memory hierarchy least levels becomes necessary single-level memory cannot accommodate high operating frequency case. please note cache-based memory hierarchy highly efficient results cache miss rate because prefetching also depends platform specifications target type size models power consumption power close-to-linear function accounting static dynamic components power dissipation. optimization constrained compute memory hardware resource limits well memory bandwidth constraints. proposed algorithm design optimization illustrated algorithm sets optimization priority order increase control complexity. algorithm depends accurate estimation performance power consumption design configuration. provide example design optimization effects assuming block size fpga-based implementation operating frequency increasing value maintaining increases power consumption less however performance increased simple pipelining control. increasing results even less increase power performance increase results seem show increasing slightly beneficial reduced memory access overheads. however value higher result high control difficulty pipelining bubbles whereas increased control complexity thanks high bandwidth block memory fpgas. result optimization priority algorithm technique improve performance highly effective regular weight access patterns. effectiveness prefetching regularity proposed block-circulant matrix-based neural networks showing another advantage prior compression schemes. experimental results next section target lower clock frequency therefore memory hierarchy structure needed. besides memory hierarchy structure memory bandwidth determined parallelization degree basic computing block. based configuration aspect ratio memory subsystem determined. relatively high memory bandwidth requirement compared total memory capacity column decoders eliminated general thereby resulting simpler layout lower routing requirements. evaluation section provide detailed experimental setups results proposed universal inference framework different platforms including fpgas asic designs embedded processors. experimental results representative benchmarks mnist cifar- svhn imagenet provided conducted comprehensive comparison state-of-the-art works hardware deep learning systems. order magnitude energy efficiency performance improvements observed using proposed universal inference framework. fpga-based testing first illustrate fpga-based testing results using lowpower low-cost intel cyclone fpga. cyclone fpga exhibits static power consumption less highest operating frequency making good choice energy efficiency optimization fpga-based deep learning systems. fig. illustrates comparison performance energy efficiency proposed reference fpga-based implementations. fpga implementation uses alexnet structure representative dcnn model five conv layers three layers imagenet applications reference fpga-based implementations state-of-the-arts represented reference works implement largescale alexnet vgg- medium-scale cifar- custom-designed recurrent neural network note equivalent gops gops/w methods weight storage compression including ours. although references focus different models structures gops gops/w general metrics independent model differences. widely accepted hardware deep learning research compare gops gops/w metrics proposed designs reported reference work shown please note entirely fair comparison implementation layerwise implementation extracts on-chip fpga power consumptions. fig. observe significant improvement achieved proposed fpga-based implementations compared prior arts terms energy efficiency even achieving improvement comparing prior work heuristic model size reduction techniques uses heuristic weight pruning method uses binary-weighted neural network xor-net). comparing prior arts uncompressed deep learning system energy efficiency improvement reach results demonstrate clear advantage circnn using block-circulant matrices energy efficiency. performance energy efficiency improvements algorithm complexity reduction efficient hardware design weight reduction elimination weight accessing off-chip storage. first source results improvement second results ×-×. please note circnn architecture yield highest throughput single low-power fpga reference uses high-performance fpga together large off-chip dram custom-designed recurrent neural network. needed increase number fpgas process multiple neural networks parallel thereby improving throughput without incurring degradation energy efficiency. besides comparison large-scale dcnns circnn architecture state-of-the-art fpga implementations also compare truenorth neurosynaptic processor benchmark data sets including mnist cifar- svhn supported truenorth. fair comparison because proposed system truenorth end-to-end implementations. truenorth neuromorphic cmos chip fabricated technology cores simulating programmable silicon neurons time-multiplexed manner. implements spiking neural networks bioinspired type neural networks benefits ability globally asynchronous implementations widely perceived achieve lower accuracy compared state-of-the-art models. truenorth exhibits advantages reconfigurability fig. compares throughput energy efficiency fpgabased implementations circnn architecture truenorth different benchmark data sets. throughput energy efficiency truenorth results mnist cifar- svhn) choose results low-power mapping mode using single truenorth chip high energy efficiency. observe improvement throughput mnist svhn data sets energy efficiency level magnitude. throughput cifar- using fpga implementation circnn lower truenorth requires specific preprocessing cifar- performing inference model chose uses small-scale ffts limits degree improvements. besides circnn architecture achieves higher test accuracy general results minor accuracy degradation compared software dcnns whereas low-power mode truenorth incurs higher accuracy degradation results demonstrate high effectiveness circnn architecture widely perceived fpga-based implementations result lower performance energy efficiency compared asic implementations benefits short development round higher flexibility. asic designs synthesis results derive asic synthesis results circnn architecture largescale dcnn implementations compare state-ofthe-art asic developments synthesis results. delay power energy asic designs obtained synthesized nangate process using synopsys design compiler. memories sram based estimated using cacti fig. illustrates comparison results asicbased implementations/synthesis state-of-the-arts also target large-scale deep learning systems. reference asic implementations synthesis results represented fig. observe synthesis results achieve highest throughput energy efficiency times compared highest energy efficiency best state-ofthe-art implementations. also striking even fpga implementation could achieve order energy efficiency higher throughput compared best state-of-the-art asics. worth noting best state-of-the-art asic implementations report highest energy efficiency near-threshold regime aggressively reduced bit-length using -bit input weight representations near-threshold computing voltage level another improvement energy efficiency achieved synthesis results compared super-threshold implementation shown fig. makes total improvement compared best state-of-the-art. moreover systems memory fact consumes slightly less power consumption compared computing blocks demonstrates weight storage longer system bottleneck. please note overall accuracy using -bit representation circnn architecture hence -bit representation utilized provide fair comparison baseline methods using number bits representations. also perform comparison performance energy efficiency energy-efficient nvidia jetson embedded toolkit optimized deep learning applications. observed energy efficiency improvement achieved using implementation improvement reaches incorporating near-threshold computing -bit weight input representations. embedded arm-based processors arm-based embedded processors widely used embedded processors smartphones embedded devices implement proposed block-circulant matrix-based inference framework smartphone using cortex processor cores provide sample results. demonstrate potential real-time implementation deep learning systems embedded processors thereby significantly enhancing wide adoption deep learning systems personal embedded devices. implementation lenet- dcnn model mnist data proposed embedded processor-based implementation achieves performance .ms/image accuracy slightly faster compared truenorth high-accuracy mode energy efficiency slightly lower level peripheral devices smartphone. comparing gpu-based implementation using nvidia tesla images/s energy efficiency significantly higher consumes power consumption embedded processor consumes around interesting comparing fully-connected layer alexnet smartphone-based implementation circnn even achieves higher throughput compared nvidia tesla gpu. benefits computational complexity reduction become significant model size becomes larger. summary discussions energy efficiency performance overall circnn architecture achieves significant gain energy efficiency performance compared best state-of-the-arts different platforms including fpgas asic designs embedded processors. reasons improvements include fundamental algorithmic improvements weight storage reduction significant reduction off-chip dram accessing highly efficient implementation basic computing block fft/ifft calculations. fundamental algorithmic improvements accounts significant portion energy efficiency performance improvements around rest accounts ×-×. particular emphasize that hardware resources power/energy consumptions associated memory storage order computing blocks absolute dominating factor overall hardware deep learning system; medium large-scale models implemented small footprint thanks recursive property fft/ifft calculations. characteristics enable highly efficient implementations circnn architecture low-power fpgas/asics elimination complex control logics high-power-consumption clock networks. reconfigurability property circnn architecture allowing applied wide deep learning systems. resembles truenorth could significantly reduce development round promote wide application deep learning systems. unlike truenorth circnn) need specialized offline training framework specific preprocessing procedures certain data sets like cifar result hardware resource waste small-scale neural networks additional chips large-scale ones. former property proposed training algorithms general latter different scales models conducted basic computing block using different control signals thanks recursive property fft/ifft. software interface reconfigurability development released public testing. online learning capability circnn architecture described mainly focuses inference process deep learning systems although algorithmic framework applies inference training. focus inference difficult perform online training hardware embedded deep learning systems limited computing power data encounter. conclusion paper proposes circnn principled approach represent weights process neural networks using block-circulant matrices. circnn utilizes fast fourier transform -based fast multiplication simultaneously reducing computational complexity negligible accuracy storage complexity demonstrate performance energy efficiency test circnn architecture fpga asic embedded processors. results show circnn architecture achieves high energy efficiency performance small hardware footprint. based fpga implementation asic synthesis results circnn achieves energy efficiency improvements compared best state-of-the-art results. acknowledgement work funded national science foundation awards cns- cns- cns- crii- ccf- cns- algorithm-in-the-field program darpa saga program case center syracuse university. references deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database computer vision pattern recognition cvpr ieee conference ieee taigman yang ranzato wolf deepface closing human-level performance face verification proceedings ieee conference computer vision pattern recognition huval wang tandon kiske song pazhayampallil andriluka rajpurkar migimatsu cheng-yue empirical evaluation deep learning highway driving arxiv preprint arxiv. collobert weston unified architecture natural language processing deep neural networks multitask learning proceedings international conference machine learning karpathy fei-fei deep visual-semantic alignments generating image descriptions proceedings ieee conference computer vision pattern recognition suda chandra dasika mohanty vrudhula j.-s. throughput-optimized opencl-based fpga accelerator large-scale convolutional neural networks proceedings acm/sigda international symposium field-programmable gate arrays wang zhou tang song going deeper embedded fpga platform convolutional neural network proceedings acm/sigda international symposium field-programmable gate arrays zhang fang zhou cong caffeine towards uniformed representation acceleration deep convolutional neural networks proceedings international conference computer-aided design mahajan park amaro sharma yazdanbakhsh esmaeilzadeh tabla unified template-based framework accelerating statistical machine learning high performance computer architecture ieee international symposium ieee zhao song zhang xing j.-h. srivastava gupta zhang accelerating binarized convolutional neural networks softwareprogrammable fpgas proceedings acm/sigda international symposium field-programmable gate arrays umuroglu fraser gambardella blott leong jahre vissers finn framework fast scalable binarized neural network inference proceedings acm/sigda international symposium field-programmable gate arrays y.-h. chen krishna emer eyeriss energy-efficient reconfigurable accelerator deep convolutional neural networks ieee journal solid-state circuits vol. pedram horowitz dally efficient inference engine compressed deep neural network proceedings international symposium computer architecture ieee press chen zhang wang chen dadiannao machine-learning supercomputer proceedings annual ieee/acm international symposium microarchitecture ieee computer society reagen whatmough adolf rama hernándezlobato g.-y. brooks minerva enabling low-power highly-accurate deep neural network accelerators proceedings international symposium computer architecture ieee press desoli chawla boesch s.-p. singh guidetti ambroggi majo zambotti ayodhyawasi singh tops/w deep convolutional neural network fd-soi intelligent embedded systems solid-state circuits conference ieee international ieee moons uytterhoeven dehaene verhelst envision .-to-tops/w subword-parallel dynamic-voltage-accuracy-frequency-scalable convolutional neural network processor fdsoi solid-state circuits conference ieee international ieee whatmough rama brooks g.-y. nj/prediction sparse deep-neural-network engine with> timing error rate tolerance applications solid-state circuits conference ieee international ieee bang wang dong y.-p. chen fick dreslinski programmable deep-learning processor on-chip weight storage using non-uniform memory hierarchy mobile intelligence solid-state circuits conference ieee international lukefahr palframan dasika mahlke scalpel customizing pruning underlying hardware parallelism proceedings annual international symposium computer architecture https//drive.google.com/open?id=bxkzgxlwayjvjwckcxsrm. zhao liao wang tang yuan theoretical properties neural networks weight matrices displacement rank arxiv preprint arxiv. grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations proceedings annual international conference machine learning karpathy toderici shetty leung sukthankar fei-fei largescale video classification convolutional neural networks proceedings ieee conference computer vision pattern recognition cheng feris kumar choudhary s.-f. chang exploration parameter redundancy deep networks circulant projections proceedings ieee international conference computer vision shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proceedings international conference multimedia netzer wang coates bissacco reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning vol. salehi amirfattahi parhi pipelined architectures realvalued hermitian-symmetric ifft real datapaths ieee transactions circuits systems express briefs vol. ayinala parhi architectures real-valued signals based radix-ˆ{} radix-ˆ{} algorithms ieee transactions circuits systems regular papers vol. oppenheim discrete-time signal processing. pearson education india altera mega-core function user guide altera jose calif judd albericio hetherington aamodt moshovos stripes bit-serial deep neural network computing microarchitecture annual ieee/acm international symposium ieee https//www.altera.com/products/fpga/stratix-series/stratix-/overview.html. https//www.xilinx.com/products/silicon-devices/fpga/virtex-.html. wang mishra ranka dynamic cache reconfiguration partitioning energy optimization real-time multi-core systems proceedings design automation conference jouppi improving direct-mapped cache performance addition small fully-associative cache prefetch buffers years international symposia computer architecture esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch convolutional networks fast energy-efficient neuromorphic computing proceedings national academy sciences", "year": 2017}