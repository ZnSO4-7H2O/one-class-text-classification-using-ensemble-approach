{"title": "Deep Fried Convnets", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The fully connected layers of a deep convolutional neural network typically contain over 90% of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices.  In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all fully connected layers in a deep convolutional neural network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance.", "text": "fullyconnected layers deep convolutional neural networks typically contain network parameters. reducing number parameters preserving predictive performance critically important training models distributed systems deployment embedded devices. paper introduce novel adaptive fastfood transform reparameterize matrix-vector multiplication fully connected layers. reparameterizing fully connected layer inputs outputs adaptive fastfood transform reduces storage computational costs costs respectively. using adaptive fastfood transform convolutional networks results call deep fried convnet. convnets end-to-end trainable enable attain substantial reductions number parameters without affecting prediction accuracy mnist imagenet datasets. recent years witnessed explosion applications convolutional neural networks millions billions parameters. reducing vast number parameters would improve efﬁciency training distributed architectures. would also allow deployment state-of-the-art convolutional neural networks embedded mobile applications. train test time considerations great importance. standard convolutional network composed types layers different properties. convolutional layers contain small fraction network parameters represent computational effort. contrast fully connected layers contain vast imbalance memory computation suggests efﬁciency types layers addressed different ways. describe methods minimizing computational cost evaluating network test time approximating learned convolutional ﬁlters separable approximations. approaches realize speed gains test time address issue training since approximations made after network fully trained. additionally neither approach achieves substantial reduction number parameters since work approximations convolutional layers represent small portion total number parameters. many works addressed computational efﬁciency convolutional networks specialized settings contrast approaches demonstrates signiﬁcant redundancy parameterization several deep learning models exploits reduce number parameters. speciﬁcally method represents parameter matrix product rank factors training algorithm ﬁxes factor updates factor uses low-rank matrix factorization reduce size fully connected layers train time. demonstrate large improvements reducing number parameters output softmax layer modest improvements hidden fully connected layers. implements low-rank factorizations using training full model. contrast methods advanced paper apply train test time. performance. approach works replacing fully connected layers network adaptive fastfood transform generalization fastfood transform approximating kernels convolutional neural networks adaptive fastfood transforms refer deep fried convnets end-to-end trainable achieve predictive performance standard convolutional networks imagenet using approximately half number parameters. several works considered kernel methods deep learning doubly stochastic gradients method showed effective randomization allow kernel methods scale extremely large data sets. however approach used ﬁxed convolutional features cannot jointly learn kernel classiﬁer convolutional ﬁlters. showed learn kernel function unsupervised manner. attempts replace fully connected layers. network network architecture achieves state results several deep learning benchmarks replacing fully connected layers global average pooling. similar approach used ilsvrc object detection competition although global average pooling approach achieves impressive results signiﬁcant drawbacks. first feature transfer difﬁcult approach. common practice take convolutional network trained imagenet re-train layer different data re-using features learned imagenet task difﬁcult global average pooling. deﬁciency noted motivates extra linear layer network enable easily adapt tune network label sets. second drawback global average pooling computation. convolutional layers much expensive evaluate fully connected layers replacing fully connected layers convolutions decrease model size comes cost increased evaluation time. parallel ﬁrst version work several researchers attempted create sparse networks applying pruning sparsity regularizers approaches however require training original full model consequently enjoy efﬁcient training time beneﬁts techniques proposed paper. since then hashing methods also advanced reduce number parameters hashes irregular memory access patterns consequently good performance large gpu-based platforms demonstrated. finally distillation also offers compressing neural networks postprocessing step. large dense matrices main building block fully connected neural network layers. propagating signal l-th layer activations layer activations compute storage computational costs matrix multiplication step storage cost particular prohibitive many applications. proposed solution reparameterize matrix parameters rn×d adaptive fastfood transform follows section provide background intuitions behind design. sufﬁces state storage requirements reparameterization computational cost also show experimental section theoretical savings mirrored practice signiﬁcant reductions number parameters without increased prediction errors. understand claims need describe component modules adaptive fastfood transform. simplicity presentation ﬁrst assume rd×d. adaptive fastfood three types module diagonal matrices parameters. original non-adaptive fastfood formulation random matrices described section computational storage costs trivially }d×d random permutation matrix. implemented lookup table storage computational costs also summary overall storage cost adaptive fastfood transform computational cost substantial theoretical improvements costs ordinary fully connected layers. parameters adaptive fastfood transform learned standard error derivative backpropagation. moreover backward pass also computed efﬁciently using fast hadamard transform. note operations simply ap∂hh plications hadamard transform since consequently computed time. operation application permutation computed time. operations diagonal matrix multiplications. learned kernel. views shed light adaptive fastfood competing techniques also open room innovate techniques reduce computation memory neural networks. adaptive fastfood based fastfood transform diagonal matrices random entries. experiments compare performance existing random proposed adaptive versions fastfood used replace fully connected layers convolutional neural networks. intriguing idea constructing neural networks random weights reasonably explored neural networks ﬁeld idea related random projections deeply studied theoretical computer science random projection basic operation form random matrix either gaussian binary importantly embeddings generated random projections approximately preserve metric information formalized many variants celebrated johnson-lindenstrauss lemma. shortcoming random projections cost storing matrix using sparse random matrix reduce cost often viable option variance estimates high inputs example also sparse. this consider extreme case sparse input many products zero hence help improve estimates metric properties embedding space. popular option reducing storage computational costs random projections adopt random hash functions replace random matrix multiplication. example count-sketch algorithm uses pairwise independent hash functions carry effectively many applications technique often referred hashing trick machine learning literature. hashes irregular memory access patterns clear good performance gpus following approach pointed ailon chazelle introduced alternative approach efﬁcient also preserves desirable theoretical properties random projections. idea replace random matrix transform mimics properties random matrices stored efﬁciently. particular proposed following transform apply monte carlo methods approximate expectation hence approximate kernel inner product stacked cosine sine features. speciﬁcally suppose sample vectors i.i.d. collect matrix kernel approximated inner-product following random features approximating given kernel function random features requires speciﬁcation sampling distribution distributions derived many popular kernels. example want implicit kernel squared exponential kernel know distribution must gaussian )−). words draw rows gaussian distribution equation implement neural module implicitly approximating squared exponential kernel. sparse random matrix gaussian entries hadamard matrix diagonal matrix {+−} entries drawn independently probability inclusion hadamard transform avoids problems using sparse random matrix itself still efﬁcient compute. alternative this. fastfood reduces computation storage random projections original formulation respectively. diagonal random matrices computed stored. contrast proposed adaptive fastfood transform diagonal matrices learned backpropagation. adapting effectively implementing automatic relevance determination features. matrix controls bandwidth kernel spectral incoherence. finally represents different kernel types. example kernel follows chi-squared distribution. adapting learn correct kernel type. computational reasons often want determine features associated kernel. working features preferable kernel matrix dense large. space computing takes operations number data points dimension.) might also want design statistical methods using kernels designs features used modules neural networks. unfortunately difﬁculties line attack deriving features kernels trivial general. important fact noted inﬁnite kernel expansions approximated unbiased manner using randomly drawn features. shift-invariant kernels relies classical result harmonic analysis known bochner’s lemma states continuous shiftinvariant kernel positive deﬁnite fourier transform nonnegative measure measure known spectral density turn implies existence probability table mnist jointly trained layers comparison reference convolutional network fully connected layer deep fried networks mnist dataset. numbers indicate number features used fastfood transform. results tagged obtained wtihout dropout. results mnist experiment shown table width deep fried network substantially larger reference model also experimented adding dropout model increased performance deep fried case. deep fried networks able obtain high accuracy using small fraction parameters original network interestingly beneﬁt adaptation experiment powerful adaptive models performing equivalently worse nonadaptive counterparts; however contrasted imagenet results reported following sections. figure structure deep fried convolutional network. convolution pooling layers identical standard convnet. however fully connected layers replaced adaptive fastfood transform. propose greatly reduce number parameters fully connected layers replacing adaptive fastfood transform followed nonlinearity. call architecture deep fried convolutional network. illustration architecture shown figure principle could also apply adaptive fastfood transform softmax classiﬁer. however reducing memory cost layer already well studied; example show low-rank matrix factorization applied training reduce size softmax layer substantially. importantly also show training rank factorization internal layers performs poorly agrees results reason focus attention reducing size internal layers. ﬁrst problem study classical mnist optical character recognition task. simple task serves easy proof concept method contrasting results section later experiments gives insights behavior adaptive fastfood transform different scales. jointly train layers deep fried network scratch. compare adaptive non-adaptive fastfood transforms using features. non-adaptive transforms report best performance achieved varying standard deviation random gaussian matrix examine deep fried networks behave realistic setting much larger dataset many classes. speciﬁcally imagenet ilsvrc dataset training examples validation examples distributed across classes. caffe imagenet model reference model experiments model modiﬁed version alexnet achieves top- error ilsvrc- validation set. initial layers model cascade convolution pooling layers interspersed normalization. last several layers network take form follow architecture. ﬁnal layer logistic regression layer output classes. layers network relu nonlinearity dropout used total parameters reference model fully connected layers convolutional layers. parameters fully connected layer take total number parameters. show adaptive fastfood transform used substantially reduce number parameters model. table imagenet ﬁxed convolutional layers indicates re-train convolutional weights pretrained ﬁxed. method fastfood fastfood using fastfood features respectively. report results max-voting transformations test set. previous work applying kernel methods imagenet focused building models features extracted convolutional layers pre-trained network setting less general training network scratch mirror common case convolutional network ﬁrst trained imagenet used feature extractor different task. order compare adaptive fastfood transform directly previous work extract features ﬁnal convolutional layer pre-trained reference model train adaptive fastfood transform classiﬁer using features. although reference model uses fully connected layers investigate replacing single fastfood transform. experiment sizes transform fastfood fastfood using fastfood features respectively. since fastfood transform composite module apply dropout layers. experiments reported here applied dropout matrix matrix. also applied dropout last convolutional layer also train structure layers reference model comparison. setting important compare re-trained rather jointly trained reference model training features extracted ﬁxed convolutional layers typically leads lower performance joint training imagenet activations produces signiﬁcantly lower performance original jointly trained network. nonetheless deep fried networks able outperform re-trained model well results using fewer parameters. contrast mnist experiment adaptive fastfood transform provides signiﬁcant performance boost non-adaptive version improving top- performance .-.%. finally train deep fried network scratch imagenet. features fastfood layer lose less top- validation performance number parameters network reduced corresponds factor increasing number features able perform better reference model using approximately half many parameters. results experiment shown table table imagenet jointly trained layers. method fastfood fastfood using fastfood features respectively. reference model shows accuracy jointly trained caffe reference model. nearly parameters deep fried network reside ﬁnal softmax regression layer still uses dense linear transformation accounts parameters network. side effect large number classes imagenet. data fewer classes advantage deep fried convolutional networks would even greater. moreover shown last layer often contains considerable redundancy. also note techniques could applied ﬁnal layer deep fried network reduce memory consumption test time. illustrate low-rank matrix factorization following section. section provide comparison existing works reducing number parameters convolutional neural network. techniques compare post-processing techniques start full trained model attempt compress whereas method trains compressed network scratch. rd×d rn×n diagonal matrix. order reduce parameters truncate largest singular values leading approximation rd×k rn×k absorbed factors. sufﬁciently small storing less expensive storing directly parameterization still learnable. shown training factorized representation directly leads poor performance however ﬁrst training full model preforming weight matrices followed tuning phase preserves much performance original model compare deep fried approach followed tuning show approach achieves better performance parameter spite training compressed parameterization scratch. also compare post-processed version model train deep fried convnet apply plus ﬁne-tuning ﬁnal softmax layer reduces number parameters. results post-processing experiments shown table decomposition three fully connected layers reference model min/ svd-half min/ svd-quarter. svd-half-f svd-quarter-f mean model tuned decomposition. drop accuracy svd-half drop svd-quarter. even though increase error mitigated ﬁnetuning deep fried convnets still perform better terms accuracy number parameters. reference also include results collins kohli pre-train full network sparsity regularizer ﬁne-tuning encourage connections fully connected layers zero. able achieve signiﬁcant reduction number parameters however performance compressed network suffers compared reference model. andrawback method using sparse weight matrices requires additional overhead store indexes table comparison methods. result based caffe alexnet model achieves reduction memory usage svd-half ------ structure. svd-quarter ------ structure. means tuning. many methods advanced reduce size convolutional networks test time. contrast trend adaptive fastfood transform introduced paper end-to-end differentiable hence enables attain reductions number parameters even train time. deep fried convnets capitalize proposed adaptive fastfood transform achieve substantial reduction number parameters without sacriﬁcing predictive performance mnist imagenet. also compare favorably simple test-time low-rank matrix factorization schemes. experiments also cast light issue random versus adaptive weights. structured random transformations developed kernel literature perform well mnist without learning; however moving imagenet beneﬁt adaptation becomes clear allows achieve substantially better performance. important point illustrates importance learning would visible experiments small data sets. fastfood transform allows theoretical reduction computation however computation convolutional neural networks dominated convolutions hence deep fried convnets necessarily faster practice. clear looking results imagenet table remaining parameters mostly output softmax layer. comparative experiment section showed matrix parameters softmax easily compressed using many methods could used achieve this. avenue future research involves replacing softmax matrix train test times using abundant techniques proposed solve problem including low-rank decomposition adaptive fastfood pruning. development optimized fastfood transforms used replace linear layers arbitrary neural models would also great value entire research community given ubiquity fully connected layers layers.", "year": 2014}