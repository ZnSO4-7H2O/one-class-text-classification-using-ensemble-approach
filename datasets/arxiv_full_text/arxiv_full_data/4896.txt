{"title": "Analysis of a Design Pattern for Teaching with Features and Labels", "tag": ["cs.AI", "cs.LG"], "abstract": "We study the task of teaching a machine to classify objects using features and labels. We introduce the Error-Driven-Featuring design pattern for teaching using features and labels in which a teacher prefers to introduce features only if they are needed. We analyze the potential risks and benefits of this teaching pattern through the use of teaching protocols, illustrative examples, and by providing bounds on the effort required for an optimal machine teacher using a linear learning algorithm, the most commonly used type of learners in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs of using different learning algorithms and between the effort required for featuring (creating new features) and labeling (providing labels for objects).", "text": "study task teaching machine classify objects using features labels. introduce error-driven-featuring design pattern teaching using features labels teacher prefers introduce features needed. analyze potential risks beneﬁts teaching pattern teaching protocols illustrative examples providing bounds effort required optimal machine teacher using linear learning algorithm commonly used type learners interactive machine learning systems. analysis provides deeper understanding potential trade-offs using different learning algorithms effort required featuring labeling. featuring labeling critical parts interactive machine learning process person machine learning algorithm coordinate build predictive system unlike case using labels alone little known quantify effort required teach machine using features labels. paper consider problem teaching machine classify objects teacher provide labels objects provide features—functions objects values. understand effort required teacher suitable representation objects teach target classiﬁcation function provide guidance teachers provide features labels teaching. similar previous work active learning teaching dimension take idealized view cost labeling featuring. particular ignore variability effort required respective actions. addition similar work teaching dimension assume idealized teacher complete knowledge learner target classiﬁcation function range possible objects want classify. analyze effort required teach classiﬁcation function relative given feature functions. features functions thought teachable functions. several observations motivate quantify teaching effort relative feature functions. natural expect available teachable functions depends speciﬁc learner teaching types objects want classify addition teaching effort required teach learner heavily dependent available functions. instance teacher could directly teach learner target classiﬁcation function function would required wide variety learning algorithms teacher would require provide labeled examples case binary classiﬁcation. course unreasonable expect target classiﬁcation function directly encoded feature function fact possible need machine learning algorithm build predictor. reasons assume features teachable deﬁne effort relative features. order capture dependencies among features consider lattice sets features rather features. lattice enforce assumption features taught time capture dependencies allowing features taught constituent features taught thus lattice feature sets captures potential alternative sequences features teacher teach learner. introduce error-driven-featuring design pattern teaching teacher prefers features needed prediction error training set. order analyze risks beneﬁts teaching pattern consider teaching protocols forces teacher teaching pattern not. quantifying featuring labeling effort required protocols provide deeper understanding risks beneﬁts pattern potential trade-offs featuring labeling generally. analysis consider speciﬁc learning algorithms; one-nearestneighbor classiﬁer linear classiﬁer. using measures teaching cost demonstrate signiﬁcant risks adding features high-capacity learning algorithms controlled using low-capacity learning algorithm also demonstrate additional labeling costs associated using teaching pattern high capacity learning algorithms bounded. combination results suggest would valuable empirically evaluate design pattern teaching. analyzing costs error-driven-featuring protocol provide results hypothesis speciﬁc pool-based teaching dimension linear classiﬁers pool-based exclusion dimension linear classiﬁers. variety work aimed understanding labeling effort required build classiﬁers. section brieﬂy review related work. first note work shares common roots work meek focus prediction errors rather teaching effort. closely related concept teaching dimension. primary work quantify worst case minimal effort teach learner classiﬁcation function among alternative classiﬁcation functions. large body work aimed understanding teaching dimension reﬁning teaching dimension relationship concepts learning theory vc-dimension work rather attempting quantify difﬁculty learning among classiﬁcations aimed quantifying effort required teach particular classiﬁcation function understand relationship adding features adding labels. work teaching dimension abstracts role learner rather deals directly hypothesis classes classiﬁcation functions. furthermore work teaching dimension abstracts away concept features making useless understanding interplay learner featuring labeling. said several concepts treated previously related literature. instance idea concept teaching closely related teaching sequence optimal concept speciﬁcation cost essentially speciﬁcation number hypothesis concept distinguish representation speciﬁcation cost. existing concepts include exclusion dimension unique speciﬁcation dimension certiﬁcate size similar invalidation cost. addition deﬁne teaching dimension hypothesis equivalent speciﬁcation number concept speciﬁcation cost. also provide bounds concept speciﬁcation cost linear classiﬁers. results related proposition unlike result assume space objects dense. terms provide hypothesis speciﬁc teaching dimension pool-based teaching. many domains image classiﬁcation document classiﬁcation entity extraction associated feature sets assumption dense representation unnatural like work classical teaching dimension work consider teaching labels features. body related work active learning. body work develop algorithms choose items label quality algorithm measured number labels required obtain desirable classiﬁcation function. thus given interest labeling featuring body work perhaps better named active labeling. contrast work teaching dimension teacher access target classiﬁcation function active learning teacher must choose item label without knowledge target classiﬁcation function. makes active learning critical many practical systems. excellent survey research area given settles surprisingly work active learning related work teaching dimension section deﬁne features labels learning algorithms. three concepts core concepts needed discuss cost teaching machine classify objects. thus deﬁnitions foundation remainder paper. addition providing deﬁnitions also describe properties learning algorithms related machine teaching describe speciﬁc learning algorithms used remainder paper. interested building classiﬁer objects. denote particular objects denote objects interest. particular labels denote space possible labels. binary classiﬁcation classiﬁcation function function classiﬁcation functions denoted denote target classiﬁcation function. central paper features functions objects scalar values. feature function objects real numbers feature features denote generic feature sets. feature fip} p-dimensional. p-dimensional feature object point denote mapped object using feature fip) result vector length entry result applying feature function object. deﬁne potential sequences teachable features lattice feature sets. deﬁnition feature lattice enforces restriction features taught sequentially. denote teachable feature functions objects feature lattice feature ﬁnite subsets either |fj| |fi|. restrict attention ﬁnite sets capture fact teachers teach ﬁnite number features. note feature lattice also allows represent constraints order features taught. constraints arise naturally. instance teaching concept area rectangle needs ﬁrst teach concepts length width added added features). deﬁnitions illustrated figure order deﬁne learning algorithm ﬁrst deﬁne training sets considering learning alternative feature sets featurized training sets. training labeled examples. consider honest training sets case training examples denote training examples training unfeaturized. feature sets create featurized training sets. p-dimensional feature example training denote featurized training yn)} call resulting training featurized training featurization training prepared deﬁne learning algorithm. first d-dimensional learning algorithm function takes p-dimensional feature training outputs function thus output learning algorithm using training composed functions feature yield classiﬁcation function objects hypothesis space d-dimensional learning algorithm image function denoted classiﬁcation function consistent training case d-dimensional figure example feature lattices four feature sets nine objects. shape color objects denote target binary classiﬁcation. rectangular region associated d-dimensional feature contains plot objects lowest rectangular region panel associated empty feature maps objects point. graphically depict mapping overlaying circles rectangles. learning algorithm consistent learning algorithm outputs hypothesis consistent training whenever hypothesis consistent training set. vector learning algorithm d-dimensional learning algorithms dimensionality. consistent vector learning algorithm d-dimensional learning algorithms consistent. finally learning algorithm takes feature training vector learning algorithm returns classiﬁcation function particular |f|◦ vector learning algorithm clear context discussing generic vector learning algorithm drop write important property feature whether sufﬁcient teach target classiﬁcation function feature feature sufﬁcient learner target classiﬁcation function exists training natural desiderata learning algorithm adding feature sufﬁcient feature make impossible teach target classiﬁcation function. capture following property learning algorithm. learning algorithm monotonically sufﬁcient case sufﬁcient sufﬁcient. many learning algorithms fact property. distinguish type training sets central teaching. first training concept teaching feature learning algorithm second training invalidation example correctly classiﬁed following proposition demonstrates that consistent learning algorithms ﬁnding invalidation demonstrates feature sufﬁcient target classiﬁcation function. proposition learning algorithm consistent invalidation feature target concept sufﬁcient meek suggests identifying minimal invalidation sets might helpful teachers wanting identify mislabeling errors representation errors. paper invalidation indication representation errors assume labels training correct implying mislabeling errors. remainder paper consider binary classiﬁcation algorithms ﬁrst learning algorithm consistent one-nearest-neighbor learning algorithm onenearest-neighbor algorithm d-dimensional one-nearest-neighbor learning algorithms d-dimensional feature project training algorithm identiﬁes closest points outputs minimal label value points set. thus closest point labels disagree learned classiﬁcation output construction consistent learning algorithm. second learning algorithm linear learning algorithm llin. consistent linear learning algorithm d-dimensional linear learning algorithms decision surface deﬁned hyperplane formally sign hyperplane deﬁned terms weights consider linear learner llin produces maximum margin separating hyperplane training exists outputs constant zero function otherwise. note maximum margin separating hyperplane training separating hyperplane maximizes minimum distance points training hyperplane. again construction consistent learning algorithm. note feature linearly sufﬁcient target classiﬁcation function sufﬁcient target classiﬁcation function using consistent linear learning algorithm. ﬁnish section following proposition demonstrates learning algorithms monotonically sufﬁcient. proposition learning algorithms llin monotonically sufﬁcient. section introduce error-drive-featuring design pattern teaching teaching protocols. introduce teaching protocols means study risks beneﬁts teaching pattern. teaching patterns related design patterns whereas design patterns programming formalized best practices programmer design software solutions common problems design pattern teaching formalized best practice teacher teach computer. pair teaching protocols study risks beneﬁts teaching pattern. teaching protocol algorithmic description method teacher teaches learner. order study teaching pattern protocol force teacher follow teaching pattern other allow teacher full control actions. contrast teaching protocols comparing optimal teaching costs subsequent section bounds optimal teaching costs. facilitate discussion optimal teaching costs next deﬁne several teaching costs associated feature set. next deﬁne costs feature set. ﬁrst measure measure cost specifying feature set. measure representation speciﬁcation cost feature cardinality feature |fi|. idealized measure differentiate effort required specify features. practice different features might require different effort specify cost specify different features depend upon interface features communicated learner. second measure feature measure cost specifying target classiﬁcation function using feature given learning algorithm. measure optimal concept speciﬁcation cost size minimal concept teaching using learner sufﬁcient inﬁnite otherwise. third measure feature measure cost demonstrating feature sufﬁcient given learning algorithm. measure optimal invalidation cost feature using learner size minimal invalidation sufﬁcient inﬁnite otherwise. deﬁne optimal feature cost vector scost feature learning algorithm feature cost vector length three ﬁrst component feature speciﬁcation cost second component optimal concept speciﬁcation cost third component optimal invalidation cost. consider feature figure training three objects minimal concept teaching minimal invalidation thus specify optimal feature costs representation speciﬁcation cost optimal concept speciﬁcation cost optimal invalidation cost optimal feature cost vectors feature sets shown table figure describes teaching protocols. algorithm teacher able choose whether feature labeled example. teacher choose feature labeled example call teaching protocol open-featuring protocol. adding feature teacher selects features taught given feature lattice teaching protocol adds feature current feature retrain current classiﬁer. adding label teacher chooses labeled example current training teaching protocol adds example training retrains current classiﬁer. algorithm teacher feature prediction error training set. proposition using consistent learner know implies feature sufﬁcient indicates need additional features. note assumes teacher provides correct labels. related alternative teaching protocol allows mislabeling errors meek protocol current feature sufﬁcient teacher adds labeled examples invalidation enables feature improve feature representation. process creating invalidation sets continues sufﬁcient feature identiﬁed. ideal teacher protocol would want minimize effort invalidate feature sets sufﬁcient. cost particular feature measured invalidation cost. possibility reuse examples invalidation sets previously visited smaller feature sets invalidation costs along paths feature lattice provides upper bound cost discovering sufﬁcient feature sets. given protocols natural compare costs number features added number labeled examples added deﬁning classiﬁer. associate teaching cost feature feature lattice teaching cost also function learning algorithm featuring protocol optimal teaching costs llin different feature sets given table inﬁnite label cost indicates feature cannot used teach target classiﬁcation function using protocol learning algorithm. teaching cost components would need choose method combine quantities order discuss optimal teaching policies. teacher provided learner feature sufﬁcient teacher needs teach concept represented classiﬁcation function. labeling cost required captured concept speciﬁcation cost. open-featuring protocol affords teacher ﬂexibility error-driven-featuring protocol. particular assuming teacher ideal teacher would reason prefer error-driven-featuring protocol. however teacher ideal teacher always able identify features improve representations beneﬁts inspecting invalidation identify features might prefer error-driven-featuring protocol. particular possibility adding poor feature increase labeling cost. instance using poor teacher taught learner feature might feature rather feature signiﬁcantly increasing concept speciﬁcation cost. next section demonstrate fact unbounded risk short-comings error-drive-featuring protocol that feature sufﬁcient teacher cannot another feature. instance example figure inaccessible. might mean representations lower concept speciﬁcation costs cannot used teach instance concept speciﬁcation cost whereas concept speciﬁcation cost difference large easy create example costs differ signiﬁcantly. contrast using open-featuring protocol teacher choose teach either trading costs adding features concept speciﬁcation error-driven-featuring protocol mitigate risk poor featuring discussed above come potential costs. alternative approach mitigating risks featuring different learning algorithm. llin potential increasing cost concept speciﬁcation adding feature signiﬁcantly limited. discussed detail next section. section provide bounds optimal feature teaching costs optimal teaching costs llin teaching protocols deﬁned section section assume ﬁnite realizable objects provide propositions provides tight bounds optimal concept speciﬁcation costs optimal invalidation costs llin propositions presented table full statements proofs presented full paper. fact optimal concept speciﬁcation cost unbounded function size feature fact classiﬁer high capacity. proposition however bounds potential increase effort required deﬁne concept adding feature llin. important note optimal concept speciﬁcation cost llin labeled objects general. fact construct objects feature size requires objects specify linear hyperplane generalizes objects. similar bound optimal concept speciﬁcation cost bound optimal invalidation cost llin tight. demonstrated constructing labeled objects subset labeled objects linearly separable. proposition provide bound invalidation cost bound llin larger provided proposition suspect however practice invalidation cost linear classiﬁer would typically less non-trivial bounding teaching costs section consider bounding cost teaching target classiﬁcation function using learning algorithms llin. first consider proposition cannot bound risk adding feature thus cannot bound teaching costs teaching protocols. however provide bounds teaching protocols using llin. following proposition provides upper bound teaching cost feature set. error-driven-featuring protocol computation cost difﬁcult need account cost invalidating feature sets. proposition demonstrates useful connection between invalidation sets nested feature sets using linear classiﬁer. proposition invalidation target classiﬁcation function consistent linear learner invalidation section provide proofs propositions. several proofs rely convex geometry assume reader familiar basic concepts elementary results convex geometry. denote convex closure points conv. proposition learning algorithm consistent invalidation feature target concept sufﬁcient proof invalidation target concept consistent learning algorithm aiming contradiction assume sufﬁcient fact sufﬁcient target concept learning algorithm exists training implies classiﬁcation function hypothesis class learning algorithm consistent training including fact fact invalidation implies consistent contradiction. follows sufﬁcient. proposition learning algorithms llin monotonically sufﬁcient. proof simply node adding features makes distinctions objects thus sufﬁcient superset remain sufﬁcient. llin d-dimensional feature sufﬁcient target classiﬁcation function. means sign offset weight vector agrees feature zero otherwise equivalent classiﬁer deﬁned sign proves claim. proof deﬁne points closest points convex closure conv conv∀s conv∀t convdist dist t)}). maximum margin hyperplane deﬁned points sufﬁce deﬁne hyperplane separate consider pair construction must case belongs face conv similarly belongs face conv. fact points subset cartesian product face conv face conv share points equidistant. next choose subset basis faces pair points belongs. euclidean dimension minimal face conv containing face conv. deﬁne minimal closest pairs pairs whose summed face euclidean dimension minimal conv∀t conv implies incp next establish suppose case case consider dimensional ball variation around dimensional ball variation around must parallel direction variation. rays direction starting deﬁne pairs points following common direction variation must either lower dimensional face conv conv implies incp contradiction thus finally applying carath´eodory’s theorem twice represent point thus points sufﬁce deﬁne separating hyperplane using maximum margin hyperplane. proposition invalidation target classiﬁcation function consistent linear learner invalidation proof invalidation consistent linear learner suppose invalidation case parameters sign consistent means parameters sign consistent thus invalidation contradiction. thus must invalidation proving proposition. proposition labeling cost minimal sufﬁcient feature using optimal teacher error-driven-featuring protocol learning algorithm llin upper-bounded proof consider ideal teacher ﬁrst provides labels invalidate subsets along path feature lattice provides labels teach classiﬁcation function. minimally sufﬁcient consider subset |f|. sufﬁcient proposition invalidation size proposition invalidation invalidation feature sets along paths thus examples sufﬁcient allow teacher features second phase teacher proposition need provide additional labels create concept speciﬁcation set. thus phases optimal teacher need provide labeled examples. proof objects target target classiﬁcation function. deﬁne rd|x rd|x linearly sufﬁcient exists hyperplane separating positive negative examples apply lemma using obtain desired result. proposition consistent linear learner d-dimensional feature linearly sufﬁcient target classiﬁcation function representation invalidation cost", "year": 2016}