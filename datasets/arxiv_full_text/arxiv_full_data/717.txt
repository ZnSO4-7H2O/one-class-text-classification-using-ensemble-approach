{"title": "Applying Policy Iteration for Training Recurrent Neural Networks", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally into the reinforcement learning framework.", "text": "abstract. recurrent neural networks often used learning time-series data. based assumptions model learning task minimization problem nonlinear least-squares cost function. special structure cost function allows build connection reinforcement learning. exploit connection derive convergent policy iteration-based algorithm. furthermore argue training naturally reinforcement learning framework. recurrent neural networks attractive tools learning time-series. however traditional long-term prediction methods either work iterated -step methods direct learning k-step-ahead value vast amount literature rnns reviewed accompanying paper former scheme learning phase prediction errors computed relative previous sequence values errors accumulate. however capture well behavior testing phase errors cannot corrected step-by-step accumulate rather quickly. latter scheme escapes trap needs diﬀerent estimator lookahead time economic references therein. resulting objective function least-squares function highly nonlinear terms intractable traditional minimization techniques. proceed showing task interpreted reinforcement learning problem hypothetical agent abstract environment. connection enables minimization objective function using version policy iteration. outline paper follows. theoretical part made three sections. first deﬁne learning task derive learning rules algorithm section section optimization problem rewritten policy iteration algorithm recurrent artiﬁcial networks hidden activities using novel form give convergence proof piranha section section argue considering learning process consistent well classical framework ongoing recent eﬀorts last section summarizes results. network architecture. consider fully connected recurrent neural networks input neurons hidden neurons. separate output layer; states ﬁrst hidden neurons considered outputs. state neuron piranha viewed gradient based approach. purpose technical report extend gradient based description accompanying paper framework provide insight piranha works. real number interval neurons admit activation squashing function maps onto interval state time denoted ˆxt+ respectively. explicit bias term applied instead constant might make component input. recurrent input output weight matrices denoted input generating ˆxt′ least-square errors possible values characterizes replay capability network network weights given. however cost function pitfalls ordinary gradientbased methods little changes weights considerably inﬂuence output many steps ahead. sensitivity become crucial condition. observation solution problem emerges reinforcement learning shall reformulate learning task highlight connection shall apply algorithms optimize problem. subject constraints well known recurrent network optimization tasks hard objective function many local minima often ill-conditioned. investigate possible reasons consider case predicted output values correct except single time step hidden state time step also modify modifying weights however modiﬁcation likely compromise errors resulting total increase objective value. reason phenomenon hidden states diﬀerent time steps strongly coupled underlying idea approach coupling relaxed resulting error surface become smoother. naturally enforce ﬁrst consider diﬀerent constraints. suppose sequence arbitrary state sequence necessarily belonging rnn. matrices state sequence good predictor? one-step prediction overall cost state k-step prediction error norms time instants ensure convergence geometrically decaying weight sequence decay rate total cost state sequence converse also holds. note also cost function large number partial sequences enormous number adjustable quantities. gives great freedom accomplishing minimization. however price minimization ill-posed number parameters much larger number data points. restrict choices parameters original problem diﬀerent reﬂects structure sequence replay problem better. using notation statement says good predictor also good predictor. argument applied iteratively yielding good predictor however easy equal state sequence generated denoted first note number adjustable parameters original method parameters play diﬀerent role chosen minimize multi-step prediction errors relation part full optimization problem. several numerical simulations showed step could substituted conditions constraint appear all. however theoretical analysis easier deﬁnition includes deﬁnition used here. using formulae obtain weight matrices better taking gradient descent step. description algorithm therefore complete. summarized fig. next section show algorithm seen policy iteration problem justiﬁes name policy iteration recurrent artiﬁcial networks hidden activities piranha short. notice cost function formally similar cost function reinforcement learning problem. furthermore proposed algorithmic solution much like policy iteration widely used algorithm solving problems. although similarity formal case uncertain transitions considered policy iteration formalism matched perfectly gives valuable insight piranha works. furthermore similarity enables prove appropriate conditions algorithm convergent. firstly give brief overview reinforcement learning framework policy iteration. next reformulate sequence learning problem special case point important differences concerning traditional problems. finally using policy reinforcement learning framework policy iteration. deals agents make decisions i.e. selects actions stochastic environment. state environment inﬂuenced decisions agent. point view agent actual state observed actions selected state-dependent immediate costs received. aims minimize total cumulated cost ﬁnding optimal decisions agent. cases problems treated markov decision processes i.e. states fully observable rewards successor states depend current state action history. sake simplicity also assume costs bounded deterministic depend current state. denote state space policy iteration approaches optimal policy two-phase iteration procedure iteration current policy ﬁrst evaluated i.e. computed usually letting agent take many steps using processing experienced costs. phase called policy improvement. phase policy modiﬁed using inequality transition probabilities policies cost function values stored lookup-table separate entries diﬀerent argument policy iteration converges optimal policy appropriate conditions. true even approximations used either step known advance estimated agent-environment interaction. details conditions various convergence results cases construction lookup-table feasible e.g. state and/or action spaces continuous. needs revert function approximation methods. shown however policy iteration function approximators divergent even simplest cases turn proof convergence becomes central issue. however problem time independent weight matrices searched therefore restrict mapping constant policies. policies execute ﬁxed action regardless current state. policy also notation instead policy evaluation. iteration policy evaluated. evaluation starts taking many steps using guarantees agent state therefore evaluate cost function single state only contrast traditional policy iteration done directly. policy improvement. policy improvement accomplished single state state full cost known. deviation basic policy improvement strategy take small step towards diﬀerences standard policy iteration. several important diﬀerences standard policy iteration algorithm piranha. firstly policy improvement step modiﬁed policy cost single state considered. however changing policy changes costs every state well guarantee change really improvement states thus cannot apply convergence results policy iteration algorithms require improvement whole state space. fortunately finite-k approximation. gradients inﬁnite sums approximated ﬁnite sums term. ﬁrst question answer whether approximation feasible. following lemma shows suﬃciently small discount factor gradients convergent therefore sufﬁciently large gradients approximated well convergence proof. proving convergence algorithm table easy gradient descent method therefore well known converges minimum step sizes suﬃciently small. however algorithm oﬀers possibilities within framework reinforcement learning shall discuss later. give alternative proof somewhat complicated exploits policy iteration reformulation. derivation reﬂects mechanism potentials algorithm better. open continuous function furthermore also contains ball positive radius centered uπi. note sπiuπi operator sπi+β∆π continuous exists kuπi sπi+β∆πuπik means sπi+β∆πuπi general exists πi+βk∆πuπi proof. lemma states monotonously decreasing thus necessarily convergent also means policy series converges limi→∞ taking limit αi∆π applying lemma ¯α∆¯π means either however lemmas instead ﬁxed lookahead parameter following lemma rephrasing lemma shows arbitrarily close minimum suﬃciently large norm recurrent weights grow much beyond concatenation real-valued matrix. scalar product policies normal scalar products euclidean space norm policy πi/. norm matrices vectors deﬁned usual; k.k∞ denotes proof. proof exploit fact gradient descent convergent approximating direction steepest descent long actual direction descent direction steepest descent enclose acute angle i.e. long scalar product positive. case means prove iteration remark. assumption requires recurrent weights remain relatively small thus avoiding chaotic behavior. although constraint cannot veriﬁed priori enforced either choosing small values applying weight regularization technique. hierarchy reinforcement learners? many reinforcement learning problems state space prohibitively large continuous storing state values infeasible. cases problem resolved applying kind function approximation. well known neural networks excellent function approximation capabilities wonder widely applied various problems. neural networks feedforward ones completely satisfactory estimating value function markovian problems need keep past memories. identifying abstract states problem able identify spatio-temporal regions state space spatial ones. means give tool handling non-markovian partially observable problems. furthermore prediction capability provides model environment well. consequence rnns piranha naturally hierarchical scheme states lower level input lower-level agent uses reconstruction/prediction error reward signal able learn identifying region input providing high-level state description. state description used traditional agent upper level. working details hierarchical structure topic ongoing research e.g. references therein. summary. article solution sequence replay problem proposed. question represent time series data reproduced sequentially seeing small portion formalized task minimization cost function recurrent neural network. analogy reinforcement learning enabled adapt policy iteration method problem yielding novel algorithm called piranha. showed piranha convergent naturally framework yielding hierarchical architecture.", "year": 2004}