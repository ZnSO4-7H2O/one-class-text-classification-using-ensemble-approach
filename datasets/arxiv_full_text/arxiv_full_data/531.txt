{"title": "Bidirectional Recurrent Neural Networks for Medical Event Detection in  Electronic Health Records", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored various recurrent neural network frameworks and show that they significantly outperformed the CRF models.", "text": "sequence labeling extraction medical events attributes unstructured text electronic health record notes step towards semantic understanding ehrs. important applications health informatics including pharmacovigilance drug surveillance. state supervised machine learning models domain based conditional random fields features calculated ﬁxed context windows. application explored recurrent neural network frameworks show signiﬁcantly outperformed models. ehrs report patient’s health medical history treatments compiled medical staff hospitals. well known notes contain information medical events including medication diagnosis adverse drug events etc. medical event context described change patient’s medical status. identifying events structured manner many important clinical applications discovery abnormally high rate adverse reaction events particular drug surveillance drug efﬁcacy etc. paper treat clinical event detection task sequence labeling. learning framework wide applications many disciplines genomics intrusion detection natural language processing speech recognition etc. however sequence labeling ehrs challenging task. unlike text open domain notes frequently noisy containing incomplete sentences phrases irregular language. addition notes incorporate abundant abbreviations rich medical jargons variations make recognizing semantically similar patterns notes difﬁcult. additionally different events exhibit different patterns possess different prevalences. example medication comprises words noun vary comprise signiﬁcant part sentence. medication information frequently described ehrs ades typically rare events. rule-based learning-based approaches developed identify extract information notes learningbased approaches sequence labeling algorithms hidden markov models max-entropy markov models major drawback graphical models label prediction time point depends data instance immediate neighboring labels. signed learn dependencies surrounding quite immediate neighborhood. therefore feature vectors explicitly modeled include surrounding contextual information. traditionally words representation surrounding context shown reasonably good performance. however information contained words vector sensitive context window size. context window short include information. hand context window large compress vital information irrelevant words. usually tackle problem different context window sizes gives highest validation performance. however method cannot easily applied task different medical events like medication diagnosis adverse drug reaction require different context window sizes. example medication determined context three words containing drug name adverse drug reaction would require context entire sentence. example sentence ehrs follow-up needle biopsy results consistent bronchiolitis obliterans likely bleomycin component abvd chemo. sentence true labels adverse drug event bronchiolitis obliterans drugname abvd chemo. however bronchiolitis obliterans could misslabeled another disease symptom entire sentence taken context. recent advancements recurrent neural networks opened avenues research sequence labeling. traditionally recurrent neural networks hard train back-propagation learning long term dependencies using simple recurrent neurons lead problems like exploding vanishing gradients recent approaches modiﬁed simple neuron structure order learn dependencies longer intervals efﬁciently. study evaluate performance neural networks namely long short term memory gated recurrent units people’s lives billions dollars. study empirically evaluated lstm notes focusing clinically important task detecting medication diagnosis adverse drug event. knowledge ﬁrst group reporting uses frameworks information extraction notes. medication detection important task biomedicine. related existing approaches grouped knowledge rulebased supervised machine learning hybrid approaches. example hazlehurst developed mediclass knowledge-based system deploys domain-speciﬁc logical rules medical concept extraction. wang humphreys et.al. others notes medical concepts external knowledge resource using hybrid rule-based syntactic parsing approaches. gurulingappa detect medical entities corpus annotated medline abstracts. contrast work uses corpus actual medical notes detects additional events attributes. rochefort developed document classiﬁers classify whether clinical note contains deep venous thromboembolisms pulmonary embolism. haerian applied distance supervision identify terms associated suicide events. zuofeng extracted medication information using crfs. many named entity recognition systems biomedical domain driven shared tasks bionlp biocreative shared tasks share/clef evaluation tasks best performing clinical systems named entity recognition includes tang applied structured svm. neural network models like convolutional neural networks recurrent neural networks recently successfully used tackle various sequence labeling problems nlp. collobert used convolutional neural network sequence labeling problems like tagging etc. later huang achieved comparable better scores using bi-directional lstm based models. annotated corpus contains english notes word tokens cancer patients diagnosed hematological malignancy. note annotated least annotators inter-annotator agreement kappa. annotated events attributes instances annotated corpus shown table annotated events broadly divided groups medication disease. medication group contains drugname dosage frequency duration route. corresponds information medication events attributes. attributes medication occur less frequently drugname itself ehrs report complete attributes event. disease group contains events related diseases attributes injury disease labeled indication depending semantic context. marked side effect drug. marked indication diagnosed currently doctor medication prescribed sign symptom disease fall aforementioned categories labeled ssd. common label corpus methods long short term memory long short term memory networks type recurrent neural networks rnns modiﬁcations feed-forward neural networks recurrent connections. typical neuron output time given since rnns previous outputs recurrent connections current output depends previous states. property remembers previous information sequence making useful sequence labeling tasks. rnns trained back-propagation time. bengio showed learning long term dependencies recurrent neural networks gradient decent difﬁcult. mainly back-propagating error frequently blow-up explode makes convergence infeasible vanish renders network incapable learning long term dependencies contrast lstm networks proposed solutions vanishing gradient problem designed efﬁciently learn long term dependencies. lstms accomplish keeping internal state represents memory cell lstm neuron. internal state read written gates control information ﬂowing cell state. updates various gates computed calculated larger data corpus described section ensures words seen frequently labeled data corpus still reasonable vector representation. step necessary unlabeled corpus much larger labeled one. words mapped corresponding vector representations lstm layer. lstm layer consists lstm chains propagating forward direction backward direction. concatenate output chains form combined representation word context. concatenated vector feed-forward neuron softmax activation function. softmax activation function normalizes outputs produce probability like outputs label type follows label concatenated vector time step likely label word position selected. entire network trained back-propagation. embedding vectors also updated based backpropagated errors. gated recurrent unit another type recurrent neural network recently proposed purposes machine translation similar lstms gated recurrent units also additive mechanism update cell state current update. however grus different mechanism create update. reset gate controls previous cell state calculating input activation. reset gate also computed based previous cell activation current candidate activation denote input forget output gate respectively. forget input gate determine contributions previous output current input cell state output gate controls much exposed output. cell state output calculated follows tanh cell state stores relevant information previous time-steps. modiﬁed additive fashion input forget gates. simplistically viewed allowing error back cell state unchecked till back propagates time-step added relevant information. nature allows lstm learn long term dependencies. lstm cells neural network setup shown ﬁgure xkyk input word predicted label word sentence. embedding layer contains word vector mapping words dense n-dimensional vector representations. initialize embedding layer start training word vectors ticular factorization variables provides speciﬁc independence relations enforced data. unlike hidden markov models model joint crfs model posterior probability directly. conditional written product factors follows crfs word inputs corresponding skip-gram word embedding compare crfs extra context feature word. done show rnns perform better crfs using context windows. extra feature consists vectors words representation sentence sections word respectively. feature explicitly provide mechanism somewhat similar surrounding context generated bi-directional shown figure model referred crf-context paper. also evaluate crf-nocontext model trains without context features. skip-gram word embeddings skip-gram word embeddings trained shallow neural network shown mikolov initialize embedding layer rnns. embedding also used baseline model feature. embeddings trained large unlabeled biomedical dataset compiled three sources english wikipedia unlabeled corpus pubmed open access articles. english wikipedia consists text extracted articles english wikipedia unlabeled corpus contains electronic health record notes. pubmed open access articles obtained extracting text openly available pubmed articles. here update gate decides much contribution candidate activation previous cell state cell activation. update gate computed using following equation gated recurrent units fundamental differences lstm. example mechanism like output gate controls exposure cell activation instead entire current cell activation used output. mechanisms using previous output calculation current activation also different. recent experiments comparing architectures shown grus comparable sometimes better performance lstm several tasks long term dependencies. baseline system crfs widely used sequence labeling tasks nlp. crfs model complex dependence outputs sequence using probabilistic graphical models. probabilistic graphical models represent relationships variables product factors factor inﬂuenced smaller subset variables. partence. document level neural networks document time learn context cues reside outside sentence boundary. dimensional hidden layer directional chain. since bi-directional lstms grus essentially amounts dimensional recurrent hidden layer. hidden layer activation functions models tanh. output hidden layer softmax output layer emits probabilities nine medical labels outside label. categorical cross entropy objective function. similar implementation neural cost function also contains l-regularization component. also dropout additional measure avoid over-ﬁtting. fifty percent dropout used manipulate inputs softmax layer. adagrad optimize network cost. ten-fold cross validation calculate performance metric model. dataset divided note level. separate training form validation set. validation used evaluate different parameter combinations models. employ early stopping terminate training validation error increases consistently. maximum epochs train network. batch sizes used kept constant sentence level rnns document level rnns. report micro-averaged recall precision fscore. exact phrase matching calculate evaluation score experiments. phrase labeled learned models considered true positive matches exact true boundary phrase correctly labels words phrase. word models trained predict either nine medically relevant tags described section outside label. tagger modes. ﬁrst mode used current word corresponding skip-gram representation. second mode used extra context feature described section extra features basically words representation preceding following sections sentence. ﬁrst mode used compare performance models using input data. also serves method contrasting crf’s performance context features explicitly added. tagger uses l-bfgs optimizer lregularization. frameworks trained sentence level document level. sentence level neural networks sentence time. means lstm states preserved propagated within sentence. networks cell states re-initialized senfigure heat-maps confusion matrices method different class labels. rows reference columns predictions. value cell denotes percentage words label predicted label table shows micro averaged scores method. models signiﬁcantly outperform baseline compared baseline system best system improved recall precision fscore respectively. clearly improvement recall contributes overall increase system performance. performance different models almost similar except model exhibits fscore improvement least percentage point rest. changes label wise f-score model relative baseline crfcontext method plotted figure grudocument exhibits highest gain overall indication diagnosis route dunine tags ration severity drug name ssd. indication gain near increase baseline. overall system performance gru-sentence lstm-sentence lstm-document similar exhibit somewhat varied performance different labels. sentence level models clearly outperform document level rnns dosage. additionally sentence model shows highest gain f-score. figure shows word level confusion matrix different models label. cell shows percentage word tokens label classiﬁed column label consistent increase diagonal entries models labels indicates increase overall system accuracy compared baseline. densely populated column ﬁgure outside column denotes percentage words erroneously labeled outside. figure shows change average f-scores method changing percentage training data used. setup training development test data kept ten-fold cross validation setup mentioned section training data randomly down-sampled achieve reduced training data size. ﬁgure shows recurrent neural network models perform better traditional models even smaller training data sizes. already discussed previous section improved recall seems major reason behind improvements f-score. trend observed figure models lead signiﬁcant decreases confusion values present outside column. amples labels include seven days week duration some small signiﬁcant severity needed twice daily frequency. therefore likely confused outside label. indeed case highest confusion values outside column crf-nocontext. including context improves performance much models decrease confusion alhalf cases. example grudocument confuses frequency unlabeled word time opposed crf-nocontext crf-context respectively. document level models beneﬁt using context outside sentence. since label indication requires surrounding context clear performance would improve using information several sentences. indications diseases diagnosed medical staff entire picture diagnosis usually distributed across multiple sentences. analysis complicated. several instances sentence also contain explicit cues similar secondary caused coupled drugnames enough classify ade. sentence level models might depend local cues leads improved performance. document models hand recognize patterns larger context using small dataset quite difﬁcult. lstm-document model show improvement sentence models gru-document. possible reason might simpler recurrence structure neuron compared lstm. since document sequences dataset model smaller number trainable parameters might learn faster lstm. possible larger dataset lstm might perform comparable better gru. however experiments reducing hidden layer size lstm-document model control number trainable parameters produce signiﬁcant improvements. moreover ﬁgure seems indicate much difference performances lstm different data sizes. however examination figure shows major sources error systems. largest source error caused confusing relevant medical words outside vice versa extent false positives clear figure estimated takes account even confusion outside represents words. second largest source error confusion among indication labels. discuss following paragraphs rnns manage signiﬁcantly reduce type errors. large improvement recall labels models seems suggest rnns able recognize larger relevant patterns baselines. supports hypothesis learning dependencies variable context ranges crucial task medical information extraction notes. also evident reduced confusion among indication ssd. since tags share common vocabulary sign symptom disease names identifying underlying word phrase enough distinguish three. relevant patterns surrounding context often needed discriminative cue. consequently indication confusion values column rnns exhibit signiﬁcant decreases compared crf-nocontext crf-context. also large improvements detecting duration frequency severity. vocabulary labels often lack speciﬁc medical jargon terms. exclearly surprising models larger number parameters still perform better models smaller dataset sizes. might embedding layer contributes large section trainable parameters initialized suitably good estimate using skipgram word embeddings described section shown rnns models like lstm valuable tools extracting medical events attributes noisy natural language text notes. believe signiﬁcant improvement provided gated models ability remember information across different range dependencies required. mentioned previously introduction important task different labels different contextual dependencies. models hand crafted features like words representation ﬁxed context windows lose information process. rnns excellent extracting relevant patterns sequence data. however explicitly enforce constraints dependencies output labels. believe adding probabilistic graphical model framework structured output prediction would improve performance system. experiment remains future work. thank umassmed annotation team including elaine freund wiesong steve belknap creating gold standard evaluation used work. also thank anonymous reviewers comments suggestions. work supported part grant national institutes health also acknowledge support united states department veterans affairs award ihx. contents paper represent views united states government.", "year": 2016}