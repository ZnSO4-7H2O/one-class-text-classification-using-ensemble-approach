{"title": "Generalized End-to-End Loss for Speaker Verification", "tag": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as well as multiple dialects.", "text": "previous work proposed tuple-based end-toend model simulates two-stage process runtime enrollment veriﬁcation training. experiments model combined lstm achieved best performance time. training step tuple evaluation utterance enrollment utterances lstm network {xj∼ represents features ﬁxed-length segment represent speakers utterances equal tuple includes single utterance speaker different utterance speaker call tuple positive enrollment utterances speaker i.e. negative otherwise. generate positive negative tuples alternatively. input tuple compute normalized response lstm {ej∼ embedding vector ﬁxed dimension results sequenceto-vector mapping deﬁned lstm. centroid tuple represents voiceprint built utterances deﬁned follows standard sigmoid function equals otherwise equals loss function encourages larger value smaller value consider update positive negative tuples loss function similar triplet loss facenet paper introduce generalization architecture. architecture constructs tuples input sequences various lengths efﬁcient leading signiﬁcant boost performance training speed td-sv ti-sv. paper organized follows sec. give deﬁnition loss; sec. theoretical justiﬁcation updates model parameters effectively; sec. introduces paper propose loss function called generalized end-to-end loss makes training speaker veriﬁcation models efﬁcient previous tuple-based endto-end loss function. unlike loss function updates network emphasizes examples difﬁcult verify step training process. additionally loss require initial stage example selection. properties model loss function decreases speaker veriﬁcation reducing training time time. also introduce multireader technique allows domain adaptation training accurate model supports multiple keywords well multiple dialects. depending restrictions utterances used enrollment veriﬁcation speaker veriﬁcation models usually fall categories text-dependent speaker veriﬁcation text-independent speaker veriﬁcation td-sv transcript enrollment veriﬁcation utterances phonetially constrained ti-sv lexicon constraints transcript enrollment veriﬁcation utterances exposing larger variability phonemes utterance durations work focus ti-sv particular subtask td-sv known global password td-sv veriﬁcation based detected keyword e.g. google previous studies i-vector based systems dominating approach td-sv ti-sv applications recent years efforts focusing using neural networks speaker veriﬁcation successful systems end-to-end training systems neural network output vectors usually referred embedding vectors similarly case i-vectors embedding used represent utterances dimensional space other typically simpler methods used disambiguate among speakers. sigmoid function. every utterance exactly components added loss positive component associated positive match embedding vector true speaker’s voiceprint hard negative component associated negative match embedding vector voiceprint highest similarity among false speakers. figure positive term corresponds pushing towards negative term corresponds pulling away similar compared thus contrast loss allows focus difﬁcult pairs embedding vector negative centroid. experiments implementations loss useful contrast loss performs better td-sv softmax loss performs slightly better ti-sv. addition observed removing computing centroid true speaker makes training stable helps avoid trivial solutions. still equation calculating negative similarity instead equation generalized end-to-end training based processing large number utterances once form batch contains speakers utterances speaker average depicted figure training method fetch utterances build batch. utterances different speakers speaker utterances. feature vector represents features extracted speaker utterance similar previous work feed features extracted utterance lstm network. linear layer connected last lstm layer additional transformation last frame response network. denote output entire neural network represents parameters neural network embedding vector deﬁned normalization network output represents embedding vector speaker’s utterance. centroid embedding vectors speaker deﬁned equation similarity matrix sjik deﬁned scaled cosine similarities embedding vector centroids training want embedding utterance similar centroid speaker’s embeddings time speakers’ centroids. shown similarity matrix figure want similarity values colored areas large values gray areas small. figure illustrates concept different want blue embedding vector close speaker’s centroid others centroids especially closest given embedding vector centroids corresponding similarity matrix sjik ways implement concept generalized combine different possibly extremely unbalanced data sources assign weight data source indicating importance data source. training step fetch batch/tuple utterances data source compute combined loss αkexk∈dk experiments feature extraction process audio signals ﬁrst transformed frames width step extract -dimension log-mel-ﬁlterbank energies features frame. td-sv applications features used keyword detection speaker veriﬁcation. keyword detection system pass frames containing keyword speaker veriﬁcation system. frames form ﬁxed-length segment. ti-sv applications usually extract random ﬁxed-length segments voice activity detection sliding window approach inference production system uses -layer lstm projection embedding vector size lstm projection size. td-sv hidden nodes projection size ti-sv hidden nodes projection size training model batch contains speakers utterances speaker. train network using initial learning rate decrease half every steps. l-norm gradient clipped gradient scale projection node lstm regarding scaling factor loss function also observed good initial value smaller gradient scale helped smooth convergence. though existing voice assistants usually support single keyword studies show users prefer multiple keywords supported time. multi-user google home keywords supported simultaneously google google. lower bound equation occurs thus update loss identical least steps loss. analysis shows updates models efﬁciently consistent empirical observations converges better model shorter time consider following case care model application domain small dataset time larger dataset similar identical domain. want train single model performs well dataset help single phrase completely unconstrained. solve problem using multireader technique multireader great advantage compared simpler approaches e.g. directly mixing multiple data sources together handles case different data sources unbalanced size. case data sources training google training anonymized user queries utterances speakers; mixed ok/hey google training manually collected utterances speakers. ﬁrst dataset larger second factor number utterances number speakers. evaluation report equal error rate four cases enroll either keyword verify either keyword. evaluation datasets manually collected speakers average enrollment utterances evaluation utterances speaker. results shown table multireader brings around relative improvement four cases. also performed comprehensive evaluations larger dataset collected different speakers environmental conditions anonymized logs manual collections. average enrollment utterances evaluation utterances speaker. table summarizes average different loss functions trained without multireader setup. baseline model single layer lstm nodes embedding vector size second third rows’ model architecture -layer lstm. comparing rows better tee. similar table also model performs signiﬁcantly better multireader. shown table also worth noting model took less training time tee. ti-sv training divide training utterances smaller segments refer partial utterances. don’t require partial utterances length partial utterances batch must length. thus batch data randomly choose time length within frames enforce partial utterances batch length inference time every utterance apply sliding window ﬁxed size frames overlap. compute d-vector window. ﬁnal utterance-wise d-vector generated normalizing window-wise d-vectors taking element-wise averge ti-sv models trained around utterances speakers extracted anonymized logs. evaluation additional speakers average enrollment utterances evaluation utterances speaker. table shows performance comparison different training loss functions. ﬁrst column softmax predicts speaker label speakers training data. second column model trained loss. third column model trained loss. shown table performs better softmax tee. performance improvement larger addition also observed training faster loss functions. paper proposed generalized end-to-end loss function train speaker veriﬁcation models efﬁciently. theoretical experimental results veriﬁed advantage novel loss function. also introduced multireader technique combine different data sources enabling models support multiple keywords multiple languages. combining techniques produced accurate speaker veriﬁcation models. florian schroff dmitry kalenichenko james philbin facenet uniﬁed embedding face recognition clustering proceedings ieee conference computer vision pattern recognition has¸im andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association razvan pascanu tomas mikolov yoshua bengio understanding exploding gradient problem corr vol. abs/. https//www.androidheadlines.com///voice-matchwill-allow-google-home-to-recognize-your-voice.html overview textindependent speaker recognition features supervectors speech communication vol. jean-franc¸ois bonastre corinne fredouille guillaume gravier ivan magrin-chagnolleau sylvain meignier teva merlin javier ortega-garc´ıa dijana petrovska-delacr´etaz douglas reynolds tutorial text-independent speaker veriﬁcation eurasip journal applied signal processing vol. guoguo chen carolina parada georg heigold smallfootprint keyword spotting using deep neural networks acoustics speech signal processing ieee international conference ieee rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran tara sainath automatic gain control multi-style training robust small-footprint keyword spotting deep neural networks acoustics speech signal processing ieee international conference ieee najim dehak patrick kenny r´eda dehak pierre dumouchel pierre ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. ehsan variani erik mcdermott ignacio lopez moreno javier gonzalez-dominguez deep neural networks small footprint text-dependent speaker veriﬁcation acoustics speech signal processing ieee international conference ieee yu-hsin chen ignacio lopez-moreno tara sainath mirk´o visontai raziel alvarez carolina parada locallyconnected convolutional neural networks small footprint speaker recognition sixteenth annual conference international speech communication association chao xiaokong bing jiang xiangang xuewei zhang xiao ying ajay kannan zhenyao deep speaker end-to-end neural speaker embedding system corr vol. abs/.", "year": 2017}