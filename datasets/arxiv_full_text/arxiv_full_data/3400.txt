{"title": "Grad-CAM: Why did you say that?", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models.  We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.", "text": "tems highly interpretable accurate decomposable pipelines stage handdesigned thought interpretable individual component assumes natural intuitive explanation. tradeoff realized recent work like class activation mapping allows explanations speciﬁc class image classiﬁcation cnns. using deep models sacriﬁce degree interpretability pipeline modules order achieve greater performance greater abstraction tighter integration makes good visual explanation? consider image classiﬁcation ‘good’ visual explanation model justifying predicted class classdiscriminative high-resolution concrete introduce grad-cam using notion ‘class’ image classiﬁcation visual explanations considered differentiable node computational graph including words caption answer question. illustrated fig. visualize ‘tiger cat’ class. pixel-space gradient visualizations guided backpropagation seen fig. highresolution highlight ﬁne-grained details image class-discriminative. highlighted despite ‘tiger cat’ class interest however grad-cam visualization low-resolution contain details. high-res visualization like guided grad-cam helps show details highlighting stripes addition localizing cat. combine best worlds fusing existing pixel-space gradient visualizations novel localization method called grad-cam create guided grad-cam visualizations high-resolution class-discriminative. abstract propose gradient-weighted class activation mapping generate visual explanations cnn-based network without requiring arpropose technique making convolutional neural network -based models transparent visualizing input regions ‘important’ predictions producing visual explanations. approach called gradient-weighted class activation mapping uses class-speciﬁc gradient information localize important regions. localizations combined existing pixel-space visualizations create novel high-resolution class-discriminative visualization called guided gradcam. methods help better understand cnn-based models including image captioning visual question answering models. evaluate visual explanations measuring ability discriminate classes inspire trust humans correlation occlusion maps. grad-cam provides understand cnn-based models. released code online demo hosted cloudcv full paper introduction convolutional neural networks deep networks enabled unprecedented breakthroughs variety computer vision tasks image classiﬁcation image captioning visual question answering. deep neural networks enable superior performance lack decomposability intuitive understandable components makes hard interpret. consequently today’s intelligent systems fail fail spectacularly disgracefully without warning explanation leaving user staring incoherent output wondering system did. order build trust intellegent systems move towards meaningful integration everyday lives clear must build ‘transparent’ models explain predict obtain class-discriminative localization gradgrad-cam ru×v generic cnn-based architeccam tures ﬁrst compute gradient respect feature maps convolutional layer i.e. gradients global-average-pooled obtain weights represents partial linearization deep weight network downstream captures ‘importance’ feature target class general need class score could differentiable activation. grad-cam heat-map weighted combination feature maps follow relu results coarse heat-map normalized visualization. relu grad-cam generalization cnn-based architecture etc.). guided grad-cam. order combine classdiscriminative nature grad-cam high-resolution nature guided backpropagation fuse pointwise multiplication create guided grad-cam shown left fig. expect last convolutional layers best compromise high-level semantics detailed spatial information feature maps compute grad-cam guided grad-cam. experiments evaluate visualization show image captioning visual question answering examples. evaluating visualizations evaluating class discrimination. intuitively good prediction explanation produces discriminative visualizations class interest. select images pascal contain exactly annotated categories create visualizations classes. shown workers amazon mechanical turk asked which object categories depicted image? presented categories present original image options. shown table. column human subjects correctly identify category visualized substantially often using grad-cam. makes guided backpropagation class-discriminative. evaluating trust. given explanations different models want evaluate seems trustworthy. alexnet vgg- compare figure grad-cam overview given image category input foward propagate image model obtain class scores softmax. gradients zero classes except desired class signal backpropagated rectiﬁed convolutional feature interest compute coarse grad-cam localization finally pointwise multiply heatmap guided backpropagation guided grad-cam visualizations high-resolution class-discriminative. chitectural changes. illustrate broad applicability technique across tasks apply grad-cam state-of-the-art image captioning visual question answering models. design conduct human studies show guided grad-cam explanations classdiscriminative help humans establish trust also help untrained users successfully discern ‘stronger’ deep network ‘weaker’ even networks make identical predictions simply basis visual explanations. code demos grad-cam released help others apply grad-cam interpret models. approach review class activation mapping propose grad-cam combine grad-cam high-resolution visualizations form guided grad-cam. summarized fig. class activation mapping produces localization image classiﬁcation cnns global-average-pooled convolutional feature maps directly softmax. speciﬁcally penultimate layer produce feature maps ru×v width height feature maps spatially pooled using global average pooling linearly transformed produce score class ru×v class produce localization computes linear combination ﬁnal feature maps using learned weights ﬁnal layer kak. normalized visualization purposes. applied networks multiple fully-connected layers output layer fully-connected layers replaced convolutional ones network re-trained. comparison approach applied directly cnn-based differentiable architecture without re-training. gradient-weighted class activation mapping. order guided backpropagation guided grad-cam visualizations noting vgg- known reliable alexnet. order tease apart efﬁcacy visualization accuracy model visualized consider instances models made prediction ground truth. given visualization alexnet vgg- name object predicted networks workers instructed rate model reliable. results shown relative reliability column table. scores range positive scores indicate judged reliable alexnet. guided backpropagation humans score slightly reliable alexnet guided grad-cam score clearly reliable alexnet. thus guided gradcam visualization help users place trust model generalize better based individual prediction explanations. faithfulness interpretability. faithfulness visualization model ability accurately explain function learned model. naturally exists tradeoff interpretability faithfulness complex models operating highly compositional inputs. faithful visualization might describe model precise detail completely opaque human inspection. interested local ﬁdelity; visualization need explain parts model relevant image. vicinity input data point explanation faithful model comparison need reference explanation high local-faithfulness. obvious choice visualization image occlusion measure difference scores patches input image masked out. interestingly patches change score also patches guided grad-cam assigns high intensity shown measuring rank correlation patch intensities table. shows guided grad-cam faithful original model guided backpropagation. table quantitative visualization evaluation. guided grad-cam enables humans differentiate visualizations different classes pick reliable models also accurately reﬂects behavior model analyzing failure modes vgg- guided grad-cam analyze failure modes vgg- imagenet classiﬁcation figure cases model failed predict correct class prediction even failed predict correct class ﬁgure errors part class ambiguity. example network predicts ‘sandbar’ based foreground also knows correct label ‘volcano’ located. cases errors still reasonable immediately apparent. example humans would hard explain predicted ‘syringes’ without looking visualization predicted class. rect class predicted class. major advantage guided grad-cam methods ability usefully investigate explain classiﬁcation mistakes since visualizations high-resolution class-discriminative. seen fig. failures ambiguities inherent imagenet classiﬁcation. also seemingly unreasonable predictions reasonable explanations similar observation hoggles image captioning figure interpreting image captioning models class-discriminative localization technique grad-cam spatial support regions captions images. fig. visual explanations image captioning model highlighting image regions considered important producing captions. fig. gradcam localizations global holistic captioning model captions generated dense captioning model three bounding proposals marked left. back grad-cam localizations agree bounding boxes even though captioning model grad-cam bounding annotations. image captioning. visualize spatial support simple image captioning model using grad-cam visualizations. given caption compute gradient probability w.r.t. units last convolutional layer generate grad-cam visualizations described section results shown fig. ﬁrst example grad-cam maps generated caption localize every occurrence kites people spite relatively small size. right example grad-cam correctly highlights pizza ignores woman nearby since ‘woman’ mentioned caption. described fig. generate captions speciﬁc bounding boxes image grad-cam highlights regions within bounding boxes. visual question answering. typical pipelines consist model images language model questions. image question representations fused predict answer typically -way classiﬁcation. since classiﬁcation problem pick conclusion work proposed novel class-discriminative localization technique gradient-weighted class activation mapping combined existing high-resolution visualizations produce visual explanations cnn-based models. human studies reveal localization-augmented visualizations discriminate classes accurately better reveal trustworthiness classiﬁer. addition image captioning examples shown here full version paper evaluates grad-cam imagenet localization challenge analyzes failure modes vgg- imagenet classiﬁcation using grad-cam measures correlation grad-cam human attention maps describes ablation studies provides many examples. grad-cam provides understand cnnbased model. references agrawal mathialagan goyal chavali banik mohapatra osman batra. cloudcv large scale distributed computer vision cloud service. mobile cloud visual media computing pages springer figure visualizations given image left question what color ﬁrehydrant? visualize grad-cams guided grad-cams answers red\" yellow\" yellow red\". visualizations highly interpretable help explain model’s predictions model focuses bottom part ﬁrehydrant; forced answer yellow model concentrates it‘s yellow forced answer yellow red\" looks whole ﬁrehydrant grad-cam resnet-based model. answer score compute grad-cam. despite complexity task involving visual language components explanations resnet based hierarchical co-attention model described fig. suprisingly intuitive informative.", "year": 2016}