{"title": "A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix  Factorization Can Cluster and Discover Sparse Features", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Despite our extensive knowledge of biophysical properties of neurons, there is no commonly accepted algorithmic theory of neuronal function. Here we explore the hypothesis that single-layer neuronal networks perform online symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of the streamed data. By starting with the SNMF cost function we derive an online algorithm, which can be implemented by a biologically plausible network with local learning rules. We demonstrate that such network performs soft clustering of the data as well as sparse feature discovery. The derived algorithm replicates many known aspects of sensory anatomy and biophysical properties of neurons including unipolar nature of neuronal activity and synaptic weights, local synaptic plasticity rules and the dependence of learning rate on cumulative neuronal activity. Thus, we make a step towards an algorithmic theory of neuronal function, which should facilitate large-scale neural circuit simulations and biologically inspired artificial intelligence.", "text": "abstract despite extensive knowledge biophysical properties neurons commonly accepted algorithmic theory neuronal function. explore hypothesis single-layer neuronal networks perform online symmetric nonnegative matrix factorization similarity matrix streamed data. starting snmf cost function derive online algorithm implemented biologically plausible network local learning rules. demonstrate network performs soft clustering data well sparse feature discovery. derived algorithm replicates many known aspects sensory anatomy biophysical properties neurons including unipolar nature neuronal activity synaptic weights local synaptic plasticity rules dependence learning rate cumulative neuronal activity. thus make step towards algorithmic theory neuronal function facilitate large-scale neural circuit simulations biologically inspired artificial intelligence. brains capable performing effectively efficiently variety different computations. focusing unsupervised tasks alone neuronal circuits must perform example clustering feature discovery. tasks reduce expand dimensionality input. time elementary building blocks brain hardware neurons synapses physiologically stereotypical. generic algorithm implementable biologically plausible neuronal circuits capable clustering feature discovery? previously machine learning literature discussed connection clustering feature discovery. specifically k-means commonly used clustering algorithm discover sparse features perform independent component algorithm implementation biological plausible neural network addressed. clustering neural networks considered previously biologically relevant online setting samples presented sequentially time assigned cluster immediately upon presentation. examples competitive hebbian learning algorithms include incremental k-means algorithm selforganizing maps neural adaptive resonance theory networks typically implement competitive winner-take-all type dynamics among neurons winner neuron signals assignment datum neuron’s associated cluster followed hebbian update synaptic weights winner neuron encode cluster centroid. hebbian update derived clustering cost function certain cases case connectivity dynamics implementing competition. instead connectivity dynamics prescribed assumed exist neural incremental k-means algorithms. time neural networks discover features principal components independent components sparse overcomplete representations single-layer implementations networks vary details derived principled cost function rely non-local hence biologically implausible learning rules others local learning rules cannot derived principled cost function. therefore except contributions able combine derivation principled cost function biologically plausible local learning rules. paper starting principled cost function snmf similarity input data derive algorithm perform online clustering feature discovery. algorithm maps onto network using biologically plausible local learning rules hebbian anti-hebbian. fig. clustering snmf snmf clustering tool. matrix inner products input data represents similarity input samples. snmf similarity matrix yields indicator matrix attributes samples clusters. note dimension input data vectors less greater number clusters schematic illustration clustering space colors indicate cluster assignments empty circles cluster centroids. note k-means clustering data point belongs cluster meaning rows orthonormaly indicator matrices type given then objective function k-means clustering written follows finally relaxing constraint binary form keeping non-negative arrive snmf cost function relaxation permits fractional values indicator matrix allowing possibility given data point belong cluster so-called soft clustering. rest paper organized follows. next section review formal connection k-means clustering snmf. section results derive online algorithm solve snmf hence clustering. section illustrate algorithm’s clustering performance numerical example. section demonstrate network operation also viewed context sparse feature discovery. discussion propose online symmetric matrix factorization serve powerful algorithmic theory neural computation compare snmf network known biological facts. review connection k-means clustering objective function snmf objective function k-means algorithm fig. clusters data solving following optimization problem function minimize snmf cost function online setting input data streamed sequentially vector time corresponding must computed arrival xt+. thus time point solve following cost function synaptic weight updates feedforward lateral connections written recursive form lead hebbian anti-hebbian updates feedforward lateral connections respectively here demonstrate clustering performance algorithm artificial datasets. generated test dataset sampling three gaussians centered randomly chosen locations identical covariance matrices fig. then applied algorithm regularization coefficient found correctly clustered data fig. compare performance online algorithm offline newton-like snmf algorithm proposed performance offline algorithm depends initialization attempted improve initializing three rows rectified projections data point onto first principal component data second principal component; iii) rectified negative first principal component. offline algorithm whole dataset. ratio costs offline online algorithm shown fig. interestingly find performance online algorithm close offline algorithm relative terms improves time. fig. neuronal network implementing online snmf. neuron rectifies inputs weighted feedforward connections neurons’ activity weighted lateral connections feedforward lateral connection weights updated according local hebbian anti-hebbian rules correspondingly. interestingly update rules functional form oja’s rule proposed previously single neurons note weight update depends activity prepost-synaptic neurons hence biologically plausible. unlike oja’s rule learning rate arbitrary learning rate specified activity dependent. best knowledge single-neuron updates previously derived multi-neuron case. fig. clustering artificial datasets using online snmf algorithm. example online algorithm. colors indicate cluster assignments. mean ratio costs online algorithm offline algorithm datasets. shown since online algorithm always achieves zero cost initial data point. shadows show standard deviation. however nervous systems encounter stimuli obvious cluster structure. section discuss performance online snmf algorithm presented natural stimuli. present algorithm natural image ensemble common computational neuroscience computer vision. specifically present sequentially -pixel pixel natural image patches using data provided patches pre-processed centering contrastnormalization whitening computations performed mammalian retina thalamus upstream primary visual cortex image patch presented network times random order resulting total presentations. number output units regularizer speed convergence used alternative step size schedule initialization neuron subsequent steps resulting feedforward weight matrix transformed neural filters acting natural images shown fig. transformation involves right-multiplying whitening matrix plotting rows product. recovered neural filters resemble gabor-filter receptive fields neurons previously gabor-filter receptive fields obtained independent components image patch well sparse dictionary learning model image patch represented small active neurons similarly find activity network sparse fig. high probability neural activity zero. therefore algorithm capable recovering sparse features. understand snmf discovers sparse features? hint comes recently reported success k-means clustering algorithm discovering gabor filters whitened natural image patches since discussed section snmf related k-means possible snmf also discovers sparse features. random source vector statistically independent elements. source assumed sparse e.g. laplace distributed. generative model describe natural images rows corresponding gabor filter. applied data filters would recover original sparse sources axt aast assume number clusters sparse selectivity mixing matrix identity cluster centroids would unit vectors along coordinate axis negatives. directions sparse directions. riss property allows prove algorithm discovers sparse features following logic. first riss clustering algorithm applied whitened input generated model finds cluster centroids aligned sparse directions. second known linear transformation centroids recover rows gabor filters natural image input. below show formally. fig. snmf network discovers sparse features natural images. neural filters trained natural images resemble gabor filters. probability density output activity pooled neurons stimuli presentations. arrow denotes probability zero output. plot obtained presenting network image patches used training frozen synaptic weights. orthogonal rotation riss property guarantees cluster centroids columns negatives. then filters i.e. rows recovered reversing whitening operation. argument summarized explain sparse feature discovery online snmf relation k-means algorithm section although able prove riss property online snmf algorithm satisfies riss property numerically fig. riss property online snmf algorithm proven argument could applied snmf directly. fig. numerical verification riss property online algorithm. two-dimensional input data generated sampling coordinate independent laplace distribution unit variance. unit vectors along positive negative axes rows generated online snmf algorithm steady state. input data generated orthonormal mixing matrix rows rotate correspondingly point along sparse directions. maximum number clusters although initially motivated snmf connection k-means believe computational role maybe broader. shown previous section snmf discover sparse features independent components data even obviously clusterable. therefore possible online snmf generic signal processing tool. generally online symmetric matrix factorization powerful tool capable performing multiple computations. addition clustering feature discovery online snmf perform unconstrained computes principal subspace streamed data addition sparsity-inducing regularizer learns sparse dictionaries given versatility online powerful neural biologically computation. therefore consider biological plausibility neural network implementation online snmf next. weighted summation input non-negativity neural output online algorithm implemented units perform weighted summation inputs followed rectification step. non-negativity algorithm’s output correctly reflects monopolar nature neuronal activity. indeed neuronal communication implemented action potentials spikes rectify membrane potential. local synaptic learning rules synaptic update rules network depend prepostsynaptic activity hence biologically plausible. feedforward synapses updated according hebbian rule lateral synapses updated according anti-hebbian rule. output neurons obey dale’s inspection reveals feedforward synaptic weights network either positive negative depending sign input lateral synaptic weights always non-positive non-negativity neural ouput. therefore neurons network obey dale’s law. dependence learning rate cumulative activity synaptic weight update predicts synaptic plasticity decays cumulative post-synaptic activity. variation plasticity time corresponds reports long-term potentiation decaying activity dependent manner sanger optimal unsupervised learning single-layer linear feedforward neural network neural networks vol. jan. foldiak adaptive network optimal linear feature extraction international joint conference neural networks ijcnn vol. kung diamantaras neural network learning algorithm adaptive principal component extraction international conference acoustics speech signal processing principal components minor components linear neural networks neural networks vol. pehlevan chklovskii hebbian anti-hebbian neural network linear subspace learning derivation multidimensional scaling streaming data nonlinear learning rule independent component analysis neurocomputing vol. sep. hyvärinen independent component analysis algorithms applications neural networks vol. jun. bell sejnowski ‘independent components’ natural scenes edge filters. vision res. vol. dec. olshausen field emergence simple-cell receptive field properties learning sparse code natural images nature vol. olshausen field sparse coding overcomplete basis strategy employed vision res. vol. ding simon equivalence nonnegative matrix factorization spectral clustering. kuang park ding symmetric nonnegative matrix factorization graph clustering. towfic pehlevan genkin chklovskii neuron signal processing device asilomar conf. signals syst. comput. nov. carandini heeger normalization canonical neural computation. nat. rev. neurosci. vol. jan. atick redlich what retina know natural scenes? neural comput. vol. hubel wiesel receptive fields binocular interaction functional architecture cat’s visual cortex physiol vol. olshausen field sparse coding sensory inputs. curr opin neurobiol vol. aug. pehlevan chklovskii hebbian/anti-hebbian network online sparse dictionary learning derived symmetric matrix factorization asilomar conference signals systems computers crair malenka critical period long-term potentiation thalamocortical synapses. nature vol. kirkwood h.-k. bear co-regulation long-term potentiation experience-dependent synaptic plasticity visual cortex experience nature isaacson early critical period long-term plasticity structural modification sensory synapses olfactory cortex. neurosci vol. jul. interesting distribution achieved without including sparsity-inducing regularizer snmf cost funciton. intuitively clustering operation snmf results sparse firing allowing output neurons active given input. paper derive online snmf algorithm demonstrate performance clustering sparse feature discovery. propose neural network implementation online snmf similarity matrix serve algorithmic theory neural computation. coates analysis single-layer networks unsupervised feature learning prodeedings international conference artificial intelligence statistics coates learning feature representations kmeans neural networks tricks trade vinnikov shalev-shwartz k-means recovers filters independent components sparse proceedings international conference machine learning hertz introduction theory neural computation westview press k.-l. clustering neural network approach. neural networks vol. jan. macqueen some methods classification analysis multivariate observations proc berkeley symp math statistics probability vol. kohonen self-organized formation topologically correct feature maps. biol. cybern. vol. martinetz schulten neural-gas’ network learns topologies artificial neural networks elsevier carpenter grossberg massively parallel architecture self-organizing neural pattern recognition machine comput. vision graph. image process. vol. carpenter grossberg self-organization stable category recognition codes analog input patterns appl. opt. vol. ritter schulten kohonen’s self-organizing maps exploring computational capabilities ieee international conference neural networks simplified neuron model principal component analyzer. math. biol. vol.", "year": 2015}