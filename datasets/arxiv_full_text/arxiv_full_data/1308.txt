{"title": "Deep Belief Networks for Image Denoising", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs' ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.", "text": "deep belief networks hierarchical generative models effective tools feature representation extraction. furthermore dbns used numerous aspects machine learning image denoising. paper propose novel method image denoising relies dbns’ ability feature representation. work based upon learning noise behavior. generally features extracted using dbns presented values last layer nodes. train network totally distinguishes nodes presenting noise nodes presenting image content last later i.e. nodes last layer trained divided distinct groups nodes. detecting nodes presenting noise able make noise nodes inactive reconstruct noiseless image. section explore results applying method mnist dataset handwritten digits corrupted additive white gaussian noise reduction average mean square error achieved proposed method used reconstruction noisy images. image signals often corrupted noise. removing noise image important issue computer vision step could preprocessing step many applications. various methods proposed remove noise visual data. focus many methods fourier analysis spatial filtering wavelet transform also methods based spare coding dictionary learning hand machine learning tools convolutional neural networks deep neural networks used several papers tackle issue. biggest difﬁculties training deep networks cost function deep architectures gets stuck poor local optima random initialization weights. hinton proposed greedy layer-wise algorithm tackle issue introduced deep belief networks dbns able present good feature representation data. features deﬁned properties input data presented nodes last layer dbn. result paper train learns extract image features. trained distinguishes noise features clean image features last layer presents distinct groups nodes. furthermore dbns capable reconstructing input data based values last layer nodes. subsequently eliminate effects nodes presenting noise reconstructed image noiseless. onwe called nodes presenting noise nodes presenting image content noise nodes image nodes respectively. rest paper organized follows section brieﬂy describe deep learning. section describe learning process. section experimental results ﬁnally conclude paper section boltzmann machines restricted boltzmann machines introduced attracted attention since hinton paper showed powerful neural network made stacking rbms. rbms kind markov random fields two-layer structure. layer called visible another called hidden layer. restricted visible-visible hidden-hidden connection connections inter layer. graphical depiction shown figure binary states visible unit hidden unit biases weight them. network assigns probability every possible pair visible hidden vector energy function angle brackets used denote expectations distribution species subscript follows. leads simple learning rule performing stochastic steepest ascent probability training data learning rate. exact maximum likelihood learning model intractable exact computation expectation model expensive. hence practice learning done following approximation gradient different objective function called contrastive divergence main problems training deep networks initialize weights. difﬁcult optimize weights nonlinear deep networks multiple hidden layers. good initial weights gradient descent works well ﬁnding initial weights requires different type algorithm learns layer features time. hinton introduced algorithm solve problem based training sequence rbms. construct train sequentially many rbms number hidden layers i.e. hidden layers train rbms. rbms placed other. figure gives overview basic concept. ﬁrst consists dbns input layer ﬁrst hidden layer input training set. second consists dbns ﬁrst second hidden layers input output previous rbms. train image denoising normalized values image pixels used. using min-max normalization grayscale value pixel transformed ﬂoating point number unlike ﬁrst last layer layers binary nodes. main idea train learns noisy images images lower noise even without noise. idea implemented learning behavior noise image contents presenting behaviors nodes last layer network. network trained collection consisted noisy noiseless images. used criterion called relative activity detect noise nodes. relative activity node deﬁned difference values particular node resulted feeding network using noiseless image corresponding noisy image result particular node noise node higher relative activity. hand image node lower relative activity. theory justiﬁed fact activation image nodes noiseless corresponding noisy images. process illustrated figure performing operation images averaging values node last layer average relative activity last layer nodes computed. nodes still high average relative activity considered noise nodes. noise nodes discovered next step lower activity. since noise nodes change much clean images network choose average value clean images neutral values finally noise nodes inactive consequently noiseless image reconstructed shown figure figure nodes last layer presenting noise contents input image. reducing noisy nodes activity noiseless image created. right side image noisy image left side image noiseless reconstruction. mnist dataset dataset handwritten digits consisted images training images test. model natural noise added additive white gaussian noise images variance therefore dataset consisted noisy clean images along noisy images test used subset training elements training phase whole test test phase. according empirical results created hidden layers ----. trained network batches data including images. according previous discussions used relative activity noise nodes last layer images dataset clean image corresponding noisy image input network. afterward computed difference last nodes’ values. average difference node considering images showed average relative activity nodes based experimental results nodes average relative activity higher considered noise nodes. clean images training compute values nodes last layer trained dbn. average values node considered neutral value node. finally reconstruct noiseless image noisy image change values noise nodes neutral values. result noise nodes inactive reconstruction would noiseless. figure shows reconstructed results lower noise. also table shows reduction average mean square error achieved proposed method used reconstruction noisy images. figure left right noiseless images noisy images reconstruction without eliminating noisy node reconstruction eliminating noise nodes. clear reconstructed results much lower noise eliminating noise nodes. paper novel method image denoising proposed. proposed method makes model learn noise behavior using deep belief network tries present noise image contents behavior distinct groups nodes. omitting noisy nodes network able produce noiseless image reconstruction input image. work thresholds detection noise nodes determined manually. future works include using automatic technique determine thresholds detecting noisy nodes denoising technique presented paper. employing denoising approach tackle issues computer vision speech recognition etc. areas addressed future phases project. vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2013}