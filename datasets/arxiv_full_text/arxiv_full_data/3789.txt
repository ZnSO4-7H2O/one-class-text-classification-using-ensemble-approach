{"title": "Subspace Clustering via Optimal Direction Search", "tag": ["cs.CV", "cs.IR", "cs.LG", "stat.AP", "stat.ML"], "abstract": "This letter presents a new spectral-clustering-based approach to the subspace clustering problem. Underpinning the proposed method is a convex program for optimal direction search, which for each data point d finds an optimal direction in the span of the data that has minimum projection on the other data points and non-vanishing projection on d. The obtained directions are subsequently leveraged to identify a neighborhood set for each data point. An alternating direction method of multipliers framework is provided to efficiently solve for the optimal directions. The proposed method is shown to notably outperform the existing subspace clustering methods, particularly for unwieldy scenarios involving high levels of noise and close subspaces, and yields the state-of-the-art results for the problem of face clustering using subspace segmentation.", "text": "exists several recent spectral-clustering-based methods superior empirical performance. popular spectral-clustering-based algorithm ﬁnds sparse representation data point respect rest data construct similarity matrix shown yield exact clustering even subspaces intersections certain conditions. different algorithm called low-rank representation uses nuclear norm minimization build similarity matrix. inner product data points used measure similarity neighborhood data point. paper presents spectral-clustering-based subspace segmentation method dubbed direction search based subspace clustering underlying approach direction search program associates optimal direction data point. data point algorithm ﬁnds optimal direction column space data matrix minimum projection rest data non-vanishing projection data point. optimization framework presented directions solving convex program. subsequently similarity matrix formed using obtained directions. presented numerical experiments demonstrate often outperforms existing spectral-clustering-based methods remarkably improves state-of-the-art result problem face clustering using subspace segmentation. addition iterative method efﬁciently solve proposed direction search optimization provided. bold-face upper-case letters used denote matrices bold-face lower-case letters used denote vectors. vector denotes p-norm. given matrices equal number rows matrix matrix formed concatenation abstract—this paper presents spectral-clustering-based approach subspace clustering problem. underpinning proposed method convex program optimal direction search data point ﬁnds optimal direction span data minimum projection data points non-vanishing projection obtained directions subsequently leveraged identify neighborhood data point. alternating direction method multipliers framework provided efﬁciently solve optimal directions. proposed method shown often outperform existing subspace clustering methods particularly unwieldy scenarios involving high levels noise close subspaces yields state-of-the-art results problem face clustering using subspace segmentation. many applications signal processing machine learning data well-approximated lowdimensional subspaces subspace recovery methods instrumental reducing dimensionality recognizing intrinsic patterns data. principal component analysis standard tool approximates data single low-dimensional subspace minimum distance data points however many applications data admits clustering structures wherefore union subspaces better model data subspace clustering problem data points union unknown number unknown linear subspaces whose dimensions also generally unknown. role subspace segmentation algorithm learn lowdimensional subspaces cluster data points respective subspaces. data model widely applied many modern signal processing machine learning applications including computer vision gene expression analysis image processing many different approaches subspace clustering devised related work including statistical-based approaches spectral clustering algebraic-geometric approach innovation pursuit approach iterative methods refer reader overview topic. much recent work focused spectral-clustering based methods share common structure. speciﬁcally neighborhood data point ﬁrst identiﬁed construct similarity matrix. subsequently spectral clustering algorithm direction search based subspace clustering initialization equal cardinality neighborhood set. rm×m equal zero matrix equal normalize -norm columns di). form matrix deﬁne optimal points corresponding searches direction column space projected data non-zero projection minimum projection rest data. paper p-norm. linear constraint enforces optimal point strong coherence practice data points within subspace mutually coherent wherefore optimal point large projection data points subspace containing accordingly sample columns corresponding elements largest values subspace containing thus exploit obtained directions construct neighborhood data point order construct similarity matrix hence name direction search subspace clustering algorithm describes proposed method. ﬁrst step ﬁnds directions shot solving convex optimization problem. similarity matrix formed second step ﬁnal step spectral clustering algorithm applied similarity matrix. information steps reader referred rm×m. sparse representation enhance robustness proposed approach noise. singular vectors corresponding noise component admit sparse representations data normally obtained linear combinations large number data points. thus enforcing sparse representation optimal direction averts solution point similarities fundamental differences related approaches. bear resemblance structural standpoint conceptually different concerning data similarity viewed measured thus neighborhoods constructed. speciﬁcally underlying convex program whereby optimal directions obtained step used step algorithm construct similarity matrix. fundamentally different thresholdingbased subspace clustering algorithm uses data points directions. thus equivalent formed indices largest elements hence performance greatly declines subspaces close proximity. example suppose columns union -dimensional subspaces {si} data points random y-dimensional subspace {ri} random -dimensional subspaces. thus dimension intersections subspaces equal high probability. solve ﬁrst columns fig. illustrate values adopted measure similarity build neighborhood ﬁrst data point last column displays values adopted tsc. ﬁrst corresponding independent subspaces intersect. second last desired largest values used form step consistently correspond ﬁrst subspace. subspaces close build correct neighborhood since data columns corresponding largest elements subspace however second third cannot form proper neighborhood data points corresponding largest elements cluster. despite close proximity subspaces ﬁnds direction data span strongly coherent ﬁrst subspace small projection subspaces. feature notably empowers distinguish data clusters. developed iterative subspace clustering approach termed ipursuit akin ipursuit leverages direction search module subspace identiﬁcation albeit approach different. describe connection difference from need following deﬁnition. regularization parameter. admm approach iterative procedure. deﬁne lagrange multipliers iteration. deﬁne deﬁne elementwise function max. deﬁne column-wise operator follows equal zero otherwise ci/ci. lies innov. therefore ipursuit exploits result i=si combined fact innov orthogonal deﬁnition directly separate different subspaces successively. contrast spectral-clustering-based approach uses outcome direction search build similarity matrix. main restriction ipursuit requires every subspace carry innovation relative subspaces. words ipursuit requires subspace lies direct subspaces. restrictions. illustration ﬁrst fig. indeed shows orthogonality optimal direction i=si however last fig. requirement ipursuit violated ipursuit cannot yield correct clustering. hand samples columns corresponding largest here investigate performance algorithms large number clusters. data follows data model dimension subspace equal data points cluster. fig. shows clustering error versus number subspaces. left plot algorithms expect yield accurate clustering. right plot case clustering error algorithms except notably increases number subspaces. face clustering challenging practical application subspace clustering extended yale dataset contains images individuals frontal view different illumination conditions faces corresponding subject approximated low-dimensional subspace. thus data containing face images multiple subjects modeled union subspaces. apply face clustering present results different number clusters table performance also compared tsc. heretofore yielded best known result problem. number clusters shown algorithms different random combinations subjects clusters. expedite runtime project data span ﬁrst left singular vectors affect performance algorithms report results without projection projection shown yields accurate clustering notably outperforms performance achieved ssc. steps repeated algorithm converges number iterations exceeds predeﬁned threshold. complexity initialization step roughly order complexity iteration also thus overall complexity number iterations second term corresponds complexity calculating matrix section study performance synthetic real data. experiments synthetic data data lies union subspaces {si}n subspace random ydimensional subspace {si}n random d-dimensional subspaces. hence dimension intersection subspaces equal data points distributed uniformly random within subspaces i.e. data point lying ri-dimensional subspace generated elements sampled independently standard normal distribution orthonormal basis number misclassiﬁed data points clustering error deﬁned compared ssc-omp simulations synthetic data performance similar. however face clustering example yields better perfromance thus report results experiments section study performance noisy data. data points union -dimensional linear subspaces data points cluster. noisy data matrix obtained follows data model elements sampled determines relative power noise component. fig. shows performance different algorithms versus dimension intersection equal worth noting experiment subspaces relative innovations excludes ipursuit feasible choice. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends® machine learning vol. rahmani atia randomized robust subspace recovery outlier detection high dimensional data matrices ieee transactions signal processing vol. march m.-h. yang k.-c. kriegman clustering appearances objects varying illumination conditions ieee computer society conference computer vision pattern recognition vol. h.-p. kriegel kr¨oger zimek clustering high-dimensional data survey subspace clustering pattern-based clustering correlation clustering transactions knowledge discovery data vol. zhang szlam lerman median k-ﬂats hybrid linear modeling many outliers ieee international conference computer vision workshops luxburg tutorial spectral clustering statistics", "year": 2017}