{"title": "Learning Python Code Suggestion with a Sparse Pointer Network", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.SE"], "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.", "text": "avishkar bhoopchand rockt¨aschel earl barr sebastian riedel department computer science university college london avishkar.bhoopchand.ucl.ac.uk {t.rocktaschele.barrs.riedel}cs.ucl.ac.uk enhance developer productivity modern integrated development environments include code suggestion functionality proposes likely next tokens cursor. current ides work well statically-typed languages reliance type annotations means provide level support dynamic programming languages statically-typed languages. moreover suggestion engines modern ides propose expressions multi-statement idiomatic code. recent work shown language models improve code suggestion systems learning software repositories. paper introduces neural language model sparse pointer network aimed capturing longrange dependencies. release large-scale code suggestion corpus lines python code crawled github. corpus found standard neural language models perform well suggesting local phenomena struggle refer identiﬁers introduced many tokens past. augmenting neural language model pointer network specialized referring predeﬁned classes identiﬁers obtain much lower perplexity percentage points increase accuracy code suggestion compared lstm baseline. fact increase code suggestion accuracy times accurate prediction identiﬁers. furthermore qualitative analysis shows model indeed captures interesting long-range dependencies like referring class member deﬁned tokens past. integrated development environments essential tools programmers. especially developer codebase useful features code suggestion given piece code context suggest likely sequence next tokens. typically suggests identiﬁer function call including calls. extensive support exists statically-typed languages java code suggestion dynamic languages like python harder less well supported lack type annotations. moreover suggestion engines modern ides propose expressions multi-statement idiomatic code. recently methods statistical natural language processing used train code suggestion systems code usage large code repositories usually n-gram language model trained score possible completions. neural language models code suggestion extended line work capture long-range dependencies. standard neural language models limited so-called hidden state bottleneck i.e. context information stored ﬁxed-dimensional internal vector representation. limitation restricts models local phenomena capture long-range semantic relationships like suggesting calling function deﬁned many tokens before. address issues create large corpus lines python code using heuristic crawling high-quality code repositories github. investigate ﬁrst time attention code suggestion that despite substantial improvement accuracy still makes avoidable mistakes. hence introduce model leverages long-range python dependencies selectively attending introduction identiﬁers determined examining abstract syntax tree. model form pointer network learns dynamically choose syntax-aware pointing modeling long-range dependencies free form generation deal local phenomena based current context. contributions threefold release code suggestion corpus lines python code crawled github introduce sparse attention mechanism captures long-range dependencies code suggestion dynamic programming language efﬁciently provide qualitative analysis demonstrating model indeed able learn long-range dependencies. ﬁrst revisit neural language models brieﬂy describing extend language model attention mechanism. introduce sparse attention mechanism pointer network exploit python abstract syntax tree current context code suggestion. code suggestion approached language model measures probability observing sequence tokens python program. example sequence joint probability factorizes according work build upon neural language models using recurrent neural networks long short-term memory neural language model estimates probabilities equation using output vector lstm time step according parameter vector associated token vocabulary. neural language models theory capture long-term dependencies token sequences internal memory. however internal memory ﬁxed dimension updated every time step models often capture local phenomena. contrast interested long-range dependencies like referring function identiﬁer introduced many tokens past. example function identiﬁer introduced used near bottom. following investigate various external memory architectures neural code suggestion. straight-forward approach capturing long-range dependencies neural attention mechanism previous output vectors language model. attention mechanisms successfully applied sequence-to-sequence tasks machine translation question-answering syntactic parsing well dual-sequence modeling like recognizing textual entailment idea overcome hidden-state bottleneck allowing referral back previous output vectors. recently mechanisms applied language modelling cheng tran formally attention mechanism ﬁxed memory rk×k vectors produces attention distribution context vector time step according equations furthermore rk×k trainable parameters. finally note represents k-dimensional vector ones. language modeling populate ﬁxed window previous lstm output vectors. obtain distribution next token combine context vector attention mechanism output vector lstm using trainable projection matrix rk×k. resulting ﬁnal output vector encodes next-word distribution projected size vocabulary subsequently apply softmax arrive probability distribution next token. process presented equation trainable parameters. problem attention mechanism quickly becomes computationally expensive large moreover attending many memories make training hard noise introduced early stages optimization lstm outputs less random. alleviate problems turn pointer networks simple heuristic populating permits efﬁcient retrieval identiﬁers large history python code. develop attention mechanism provides ﬁltered view large history python tokens. given time step memory consists context representations previous identiﬁers introduced history. allows model long-range dependencies found identiﬁer usage. instance class identiﬁer declared hundreds lines code used. given history python tokens obtain next-word distribution weighed average sparse pointer network identiﬁer reference standard neural language model. weighting determined controller. formally time-step sparse pointer network operates memory rk×k previous identiﬁer representations addition maintain vector symbol identiﬁer representations before calculate context vector using attention mechanism memory containing representations identiﬁers declared history. next obtain pseudo-sparse distribution global vocabulary figure sparse pointer network code suggestion python code snippet showing nextword distributions language model identiﬁer attention weighted combination here representation input token trainable weight matrix bias respectively. controller conditioned input output context representations. means deciding whether refer identiﬁer generate global vocabulary controller access information encoded next-word distribution standard neural language model well attention-weighted identiﬁer representations current history. figure overviews process. identiﬁer base_path appears twice argument function member class appearance different vocabulary obtains different probability model. example model correctly chooses refer member class instead out-of-scope function argument although user point-of-view suggestion would cases. previous work code suggestion either focused statically-typed languages trained small corpora. thus decided collect large-scale corpus dynamic programming language python. according programming language popularity website pypl python second popular language java. also common language terms number repositories open-source code repository github javascript java collected corpus lines python code github projects. ideally would like corpus contain high-quality python code language model learns suggest code users write code. however difﬁcult automatically assess constitutes high-quality code. thus resort heuristic popular code projects tend good quality metrics github purpose namely stars forks similar allamanis sutton allamanis select python projects stars sort number forks descending take projects. removed projects compile python leaving projects. split corpus project level train test. table presents corpus statistics. unsurprisingly long tail words vocabulary consists rare identiﬁers. improve generalization normalize identiﬁers feeding resulting token stream models. replace every identiﬁer name anonymous identiﬁer indicating identiﬁer group concatenated random number makes identiﬁer unique scope. note replace novel identiﬁers deﬁned within ﬁle. identiﬁer references external apis libraries left untouched. consistent previous corpus creation code suggestion replace numerical constant tokens remove comments reformat code replace tokens appearing less times token. although previous work white already established simple neural language model outperforms n-gram model code suggestion include number n-gram baselines conﬁrm observation. speciﬁcally n-gram models modiﬁed kneser-ney smoothing kyoto language modelling toolkit initial learning rate decay every epoch. additional baselines test neural language model lstm units without attention. attention language models experiment ﬁxed-window attention memory previous tokens respectively batch size neural language models developed tensorflow trained using cross-entropy loss. processing python source code last recurrent state initial state subsequent sequence reset ﬁles. models input hidden size lstm forget gate bias gradient norm clipping randomly initialized parameters interval regularizer dropout input representations. furthermore sampled softmax log-uniform sampling distribution sample size conﬁrm code suggestion neural models outperform n-gram language models large margin. furthermore adding attention improves results substantially interestingly increase attributed superior prediction identiﬁers increased accuracy lstm attention window gives best accuracy prediction. achieve improvements perplexity accuracy predictions using sparse pointer network uses smaller memory past identiﬁer representations. figures show code suggestion example involving identiﬁer usage. lstm baseline uncertain next token sensible prediction using attention sparse pointer network. sparse pointer network provides reasonable alternative suggestions beyond correct suggestion. figures show use-case referring class attribute declared tokens past. sparse pointer network makes good suggestion. furthermore attention weights demonstrate model distinguished attributes groups identiﬁers. give full example token-by-token suggestion sparse pointer network figure appendix. much smaller space ﬂexibility programming languages allows. able capture repetitiveness predictable statistical properties real programs using language models. subsequently improved upon hindle al.’s work adding cache mechanism allowed exploit locality stemming specialisation decoupling program modules. al.’s idea adding cache mechanism language model speciﬁcally designed exploit properties source code thus follows sparse attention mechanism introduced paper. majority preceding work trained small corpora allamanis sutton created corpus lines java code analysed n-gram language models. size corpus allowed train single language model effective across multiple different project domains. white later demonstrated neural language models outperform n-gram models code suggestion. compared various n-gram models including al.’s cache model basic neural language model. khanh compared white al.’s basic lstms found latter better code suggestion improved ability learn long-range dependencies found source code. paper extends line work introducing sparse attention model captures even longer dependencies. combination lagged attention mechanisms language modelling inspired cheng equipped lstm cells ﬁxed-length memory tape rather single memory cell. achieved promising results standard penn treebank benchmark corpus similarly tran added memory block lstms language modelling english german italian outperformed n-gram neural language models. memory encompasses representations possible words vocabulary rather providing sparse view alternative purely lexical approach code suggestion involves probabilistic context-free grammars exploit formal grammar speciﬁcations well-deﬁned deterministic parsers available source code. used allamanis sutton extract idiomatic patterns source code. weakness pcfgs inability model context-dependent rules programming languages variables need declared ling recently used pointer network generate code natural language descriptions. controller deciding whether generate language model copy identiﬁer using sparse pointer network inspired latent code predictor. however inputs short whereas code suggestion requires capturing long-range dependencies addressed ﬁltered view memory previous identiﬁer representations. paper investigated neural language models code suggestion dynamically-typed programming language python. released corpus lines python crawled github compared n-gram standard neural language models attention. using attention observed order magnitude accurate prediction identiﬁers. furthermore proposed sparse pointer network efﬁciently capture long-range dependencies operating ﬁltered view memory previous identiﬁer representations. model achieves lowest perplexity best accuracy among predictions. python corpus code replicating experiment released https//github.com/uclmr/pycodesuggest. presented methods tested code suggestion within python ﬁle. interested scaling approach level entire code projects collections thereof well integrating trained code suggestion model existing ide. furthermore plan work code completion i.e. models provide likely continuation partial token using character language models mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard manjunath kudlur josh levenberg rajat monga sherry moore derek gordon murray benoit steiner paul tucker vijay vasudevan pete warden martin wicke yuan xiaoqiang zhang. tensorﬂow system large-scale machine learning. corr abs/. http//arxiv.org/abs/. miltiadis allamanis charles sutton. mining source code repositories massive scale using language modeling. thomas zimmermann massimiliano penta sunghun ieee computer society isbn ----. http//dblp.uni-trier.de/db/conf/msr/msr.htmlallamanissa. miltiadis allamanis earl barr christian bird charles sutton. learning natural coding conventions. proceedings sigsoft international symposium foundations software engineering york acm. isbn ---. ./.. http//doi.acm.org/./ jianpeng cheng dong mirella lapata. long short-term memory-networks machine reading. proceedings conference empirical methods natural language processing association computational linguistics http// aclweb.org/anthology/d-. karl moritz hermann tom´as kocisk´y edward grefenstette lasse espeholt teaching machines read compremustafa suleyman advances neural information processing systems annual conferhend. ence neural information processing systems december montreal quebec canada http//papers.nips.cc/paper/ -teaching-machines-to-read-and-comprehend. abram hindle earl barr zhendong mark gabel premkumar devanbu. naturalness software. proceedings international conference software engineering icse piscataway ieee press. isbn ----. http//dl.acm.org/citation.cfm?id=.. s´ebastien jean kyunghyun roland memisevic yoshua bengio. using large target vocabulary neural machine translation. proceedings annual meeting association computational linguistics international joint conference natural language processing beijing china july association computational linguistics. http//www.aclweb.org/anthology/p-. rafal jozefowicz wojciech zaremba ilya sutskever. empirical exploration recurrent network architectures. david blei francis bach proceedings international conference machine learning jmlr workshop conference proceedings http//jmlr.org/proceedings/papers/v/ jozefowicz.pdf. kneser ney. improved backing-off m-gram language modeling. acoustics speech signal processing icassp-. international conference volume vol. ./icassp... wang ling edward grefenstette karl moritz hermann tomas kocisky andrew senior fumin wang phil blunsom. latent predictor networks code generation. arxiv preprint arxiv. razvan pascanu tomas mikolov yoshua bengio. difﬁculty training recurrent neural networks. proceedings international conference machine learning icml atlanta june http//jmlr.org/ proceedings/papers/v/pascanu.html. tran arianna bisazza christof monz. recurrent memory networks language modeling. naacl conference north american chapter association computational linguistics human language technologies diego california june http//aclweb.org/anthology/n/ n/n-.pdf. zhaopeng zhendong premkumar devanbu. localness software. proceedings sigsoft international symposium foundations software engineering york acm. isbn ----. ./.. http//doi.acm.org/./.. oriol vinyals lukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada http//papers.nips.cc/paper/ -grammar-as-a-foreign-language. martin white christopher vendome mario linares-v´asquez denys poshyvanyk. toward deep learning software repositories. proceedings working conference mining software repositories piscataway ieee press. http//dl.acm.org/citation.cfm?id=.. figure full example code suggestion sparse pointer network. boldface tokens left show ﬁrst declaration identiﬁer. middle part visualizes memory representations identiﬁers. right part visualizes output controller used interpolating language model attention pointer network", "year": 2016}