{"title": "Oiling the Wheels of Change: The Role of Adaptive Automatic Problem  Decomposition in Non--Stationary Environments", "tag": ["cs.NE", "cs.AI"], "abstract": "Genetic algorithms (GAs) that solve hard problems quickly, reliably and accurately are called competent GAs. When the fitness landscape of a problem changes overtime, the problem is called non--stationary, dynamic or time--variant problem. This paper investigates the use of competent GAs for optimizing non--stationary optimization problems. More specifically, we use an information theoretic approach based on the minimum description length principle to adaptively identify regularities and substructures that can be exploited to respond quickly to changes in the environment. We also develop a special type of problems with bounded difficulties to test non--stationary optimization problems. The results provide new insights into non-stationary optimization problems and show that a search algorithm which automatically identifies and exploits possible decompositions is more robust and responds quickly to changes than a simple genetic algorithm.", "text": "genetic algorithms solve hard problems quickly reliably accurately called competent gas. ﬁtness landscape problem changes overtime problem called non–stationary dynamic time–variant problem. paper investigates competent optimizing non–stationary optimization problems. speciﬁcally information theoretic approach based minimum description length principle adaptively identify regularities substructures exploited respond quickly changes environment. also develop special type problems bounded difﬁculties test non–stationary optimization problems. results provide insights non-stationary optimization problems show search algorithm automatically identiﬁes exploits possible decompositions robust responds quickly changes simple genetic algorithm. real–world problems rarely static. problems change overtime factor compounded fact environments function also constant state ﬂux. although signiﬁcant advances made development design genetic evolutionary algorithms accounted changing nature problems resolving ∗artiﬁcial life adaptive robotics laboratory school information technology electrical engineering university south wales australian defence force academy canberra australia. h.abbassadfa.edu.au problems scratch every time change occurs neither practical feasible tantamount re-inventing wheel every time problem wheel occurs. aspect problem-solving especially pertinent change frequent re-solving original problem never appropriate. hypothesize solve non-stationary problems efﬁciently previously encountered solutions used extract structural knowledge problem hand. identifying important regularities sub–structures problem help responding quickly tracking optima environment changes. class evolutionary algorithms automatically discover problem decomposition known competent genetic algorithms essence competent genetic algorithms automatically adaptively identify important sub–structures underlying search problem efﬁciently explore search space. paper explore advantages using candidate methods examine hypothesis. speciﬁcally extended compact genetic algorithm candidate probabilistic model–building gas. types variation operators replaced building sampling probabilistic model promising solutions. ecga probabilistic model based information theoretic measure known minimum description length principle structure probabilities decomposition model manipulated environment changes speed-up response solver changes. similar studies using genetic evolutionary algorithms non-stationary problems assume solutions related ones changes bounded. speciﬁcally incorporate bounded changes problem structure ﬁtness landscape. noted environment changes either unboundedly randomly average method outperform restarting solver scratch every time change occurs. structure paper follows next section present brief review background materials relevant paper. review ecga followed different methods dynamic optimization paper. present experimental setup results discussions. section present brief overview previous work evolutionary computation methods dynamic environments adaptive automatic decomposition approaches. date three main evolutionary approaches solve optimization problems changing environments. approaches diversity control memory-based multi-population methods. present brief overview literature refer reader detailed review large growing ﬁeld. diversity focal point many recent work enhancing adaptiveness evolutionary methods dynamic optimization problems. diversity controlled ways; either increasing diversity whenever change detected maintaining high diversity evolutionary run. examples former include hyper–mutation method variable local search technique methods main methods latter group include redundancy random immigrants aging thermodynamical genetic algorithms memory-based approaches attract much attention literature. main types exist implicit explicit memories. implicit memory redundant representation used means memory. explicit memories speciﬁc information include solutions stored retrieved needed evolutionary mechanism. third class approaches depends speciation multi-populations. sub–populations maintained becomes specialized part search space. facilitates process tracking optima move. example group self-organizing-scouts method previous work diversity control memory-based multi-population methods performance different techniques vary manner environment changes branke attempted classify different types dynamics gain insight level difﬁculties dynamic optimization problems. major research question make dynamic optimization problem hard solve evolutionary methods? another equally important question whether learning decomposition problem help responding quickly change environment assuming decomposition affected change? challenges area genetic evolutionary algorithms systematic design genetic operators demonstrated scalability. based holland’s notion building blocks goldberg proposed design– decomposition theory designing effective gas. theory establishes identiﬁcation suitable substructures decompositions ensuring efﬁcient exchange substructures challenging task designing competent gas. design–decomposition theory provides insight makes problem hard also resulted many competent designs. essence competent successfully solve problems bounded difﬁculties polynomial number function evaluations element competent mechanism automatically identify important substructures underlying search problem. depending mechanism used discover problem decomposition competent genetic algorithms classiﬁed three broad categories perturbation techniques include messy genetic algorithm fast messy genetic algorithm gene expression messy genetic algorithm linkage identiﬁcation nonlinearity check genetic algorithm linkage identiﬁcation monotonicity detection genetic algorithm dependency structure matrix driven genetic algorithm linkage identiﬁcation limited probing probabilistic model building techniques population-based incremental learning bivariate marginal distribution algorithm extended compact iterated distribution estimation algorithm bayesian optimization algorithm despite success competent solving stationary search problems used solve non-stationary problems apart preliminary study paper twofold ﬁrst examine performance ecga terms response rate example competent automatically decomposes identiﬁes substructures non–stationary problems; second test method problems bounded difﬁculties. conjecture mechanism focuses identifying important substructures beneﬁcial dynamic optimization problems well. furthermore problem-decomposition information serves store past information could used manipulated respond faster changes environment. extended compact genetic algorithm probabilistic model building genetic algorithm replaces traditional variation operators genetic evolutionary algorithms building probabilistic model promising solutions sampling model generate candidate solutions. harik studied problem linkage learning proposed conjecture linkage learning equivalent good model learns structure underlying genotypes. focused probabilistic models harik focused probabilistic models learn linkage. ecga method proposed minimum description length principle compress good genotypes partitions include shortest possible representations. measure tradeoff complexity measures. ﬁrst measure information content population harik calls compressed population complexity second measure size model harik calls model complexity. compressed population complexity measure statistical complexity measure based well–known information-theoretic approach shannon’s entropy shannon’s entropy population assumes partition variables random variable probability measure given constant related base chosen express logarithm number possible sequences variables belonging partition cardinality measures amount disorder associated within population decomposition scheme. equivalently seen amount information content presents population speciﬁc partition scheme. compressed population complexity scaled version entropy follows section present variations ecga algorithm dynamic environments. assume paper mechanism detect change environment. detecting change environment done several ways including re–evaluating number previous solutions; monitoring statistical measures average ﬁtness population focus paper however detect change environment; therefore assume simply detect modiﬁed ecga algorithm dynamic environments works follows version change detected population generated random followed selection crossover using last generated model. method continues population. second version dcga last learnt model used bias re–start mechanism steps selection crossover carried randomly generated population ignored. versions seen re–start approach ﬁrst instance uses last learnt model re–start second not. ecga model re-built scratch every generation. advantage recovering possible problems exist hill–climber learning model. correctly detect good relations. m¨uhlenbein showed order–k functions length solvable using goldberg achieved complexity using fast messy genetic algorithms. pelikan provided complexity using boa. sastry goldberg shown convergence time ecga follows relation derived m¨uhlenbein voosen breeder convergence time equal changing environment assume chromosome building blocks order bits ecga behave according previous complexity equation build correct decomposition model. environment affect decomposition affects peaks within building blocks complete enumeration possible solutions within building block would time complexity optima. notation represents lower upper bound complexity. expensive. assume building block replicated times cost tracking optima decomposition change compare results similar genetic algorithm except linkage learning based crossover operator ecga replaced uniform crossover operator. call algorithm emphasize uniform crossover genetic algorithms. following section present experiments test functions used test proposed method. special class problems represent challenge methods known problems bounded difﬁculty. problems characterized main features additively decomposable separable functions uniformly scaled. function said additively decomposable separable exists partition function said uniformly scaled derived class functions. assumptions multimodal function take function form. problems bounded difﬁculty studied widely provide easy analyze test functions challenge dynamics simple genetic algorithms. deﬁne order difﬁculty problem maxj |χj| represents cardinality set. solving problem bounded difﬁculty becomes easy variables correctly separated right partitions; point complete enumeration possible solutions partition sufﬁcient global optimal solution. assume cardinality partition small much smaller length solution vector. however absence value knowledge variable belongs partition problem tough. examples problems bounded difﬁculties include ising problem trap functions functions incorporate notion multimodality hierarchy crosstalk deception test problems despite easy understand incorporates many essential difﬁculties linkage identiﬁcation. repeated experiment times different seeds. results presented average performance runs. population size ﬁxed experiments. population size chosen large enough provide enough samples probabilistic model learn structure provide enough diversity uga. termination occurs algorithm reaches maximum number generations assume environment changes generations changes environment assumed cyclic tested cycles length generations respectively. crossover probability tournament size experiments based harik’s default values. method tested using dynamic versions three trap functions. trap functions introduced ackley subsequently analyzed details others trap function deﬁned follows tested methods building blocks. denote number building blocks optimal solution problem would example building blocks trap– optimal solution objective value regardless change environment. environment ﬁrst experiment actually change value optimal solution severely changes value decision variables. change severe optimal solutions isolates points separated maximum possible hamming distance hamming subspace deﬁned trap. figures present performance dcga dcga respectively. starting performance dcga depicted figure algorithm consistently responds quickly changes environment trap– regardless number building blocks cycle length. however response rate trap– less indicated drop performance cycle length good performance longer cycle length ﬁgure seen higher order trap slower method able respond change environment. also seen larger number copies building blocks chromosome slower response environmental changes. slowest response rate encountered trap– building blocks. ﬁnding logical level hardness problem increases linkage problem size increases. harder separate variables difﬁcult learn decomposition. order trap problem size quantify hard dynamic optimization problem similar patterns exist dcga depicted figure notice drop performance less dcga case dcga. also looking trap– cycle length notice performance dcga worse corresponding case using dcga. expected response rate would higher using dcga compared dcga; thanks bias initial population last linkage model found. however comparing trap– cycle length using dcga corresponding performance using dcga performance dcga consistently better corresponding performance dcga. explanation result presented following subsection. model. careful examination performance identiﬁed reason behind success simple luck. attractors problem exist solutions converges wrong attractor cycle wrong attractor becomes right attractor following cycle converges back wrong attractor environment changes switches wrong attractor become preferred attractor. words environment changes manner beneﬁcial performance uga. verify analysis conducted second experiment explained following subsection. second type experiments modiﬁed trap function order break symmetry attractors. function visualized figure time even cycles optimal solution variables second attractor equal environment changes cycles solution optimal variables deceptive attractor alternatively number setup guarantees trap symmetric regards attractors. researchers suggested simple operator trap functions would solve problem easily method converges wrong attractor simple operator would take right attractor. design figure breaking symmetry trap would also counterpart possible trick using operator. figure depicts behavior three methods using modiﬁed trap– function. expected method clearly shows worst behavior among three methods. clear unable respond changes neither able even deceptive attractor cases. behavior conﬁrms analysis previous section. looking dcga dcga however dcga better dcga. dcga method able respond changes environment quickly accurately reliably time. result somehow different compared results obtained previous section. linkage changed setups change took place attractors. suggests cause somehow inferior performance dcga compared dcga attributed crossover operator mixing strategy slow reaching attractors maximum hamming distance previous experiments. experiment subjected environment severe change linkage point view. here linkage boundary changes well attractors. depicted figure environment switching trap– optima trap– optima moreover trap– deceptive attractor exists number trap– deceptive attractor exists number setup tricky sense that hill climber gets trapped deceptive attractor trap– behavior good trap–. however hill–climber won’t escape attractor environment switches back trap- since solution surrounded solutions lower qualities. setup tests also whether methods behaving similar hill–climber. figure shows performance dcga dcga uga. varied string length step string length dividable following table lists value optimal solution string length trap- trap-. scrutinizing figure dcga faster response changes environment dcga. recognized cycle length dcga fails recover string length performance clearly inferior stuck wrong attractor ﬁrst cycle seems remained attractor struggling jump even longer cycle length. section test method using moving parabola standard functions testing optimization dynamic environments. contrast previous experiments function minimization problem. function presented figure depicts performance three methods moving parabola function cycle length clear ﬁgure performing worst actually diverging sometime. behavior surprising direction dynamics function seems come close optimum dynamics changed hard track optima iterations. look carefully figure trajectories movement somehow creating multimodal landscape seems cause problems uga. think behavior possibly attributed loss diversity. found case evidenced behavior cycle length diversity lost would continue unable respond changes forever. however figure managed recover point continued optimize function another generations. dcga dcga consistently better. closer look notice cycle length dcga better dcga gets closer minimum. cycle methods track movements well exact solution. results paper shed lights utility learning possible structural decompositions changing environment. shown learning robust simple environment changes. shifts optima radical test method sever changes. words changes environment worse changes adopted paper conclude proposed approach respond quickly accurately. however previous results left puzzled main questions. first problems bounded complexity real life problems? linkage learning shows build reliable models solving problems lessons real life applications enhance problem solving. recently work done show lessons learnt competent problems bounded complexity useful solving real life problems. believe work appear near future substantiate phenomena researchers follow lessons. second question whether type methods used handling problems changing environment superior linkage learning changes environment changes bounded complexity examples used paper. said introduction three main directions handling problems changing environment memory diversity speciation niching. method adopted paper uses large population selection pressure maintain diversity population. respect methods based memory speciation shed lights problems advantages learning problem structure. learning models depend genes locations chromosome. contrary models learn relationship genes. assume chromobuilding blocks bits building block. assume building block switches different attractors. moreover also assume building blocks affected time; environment changes subset building blocks switch peaks. therefore building blocks optima. problem proposed method obviously major problem memory niching. first look memory. number possible optima algorithm alternate would effect size memory needed able respond correctly changes environment matter changes occur previous setup. indicates exponential memory needed wish respond effectively changes. multi–population speciation niching would suffer drawbacks memory approach. number peaks grow exponentially hard respond quickly change. wonder still need store optima memory respond changes. leave future work still open research question area memory–based approaches dynamic optimization problems problem determine optimal memory size needed effectively respond changes environment. addition possible combine linkage learning memory based methods. overall seen previous discussion linkage learning offers many opportunities give insights dynamic optimization problems.", "year": 2005}