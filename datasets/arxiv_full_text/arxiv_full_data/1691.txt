{"title": "Learning Multilingual Word Representations using a Bag-of-Words  Autoencoder", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g. infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages. In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements. The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation. We evaluate our approach on a multilingual document classification task, where labeled data is available only for one language (e.g. English) while classification must be performed in a different language (e.g. French). In our experiments, we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations.", "text": "recent work learning multilingual word representations usually relies word-level alignements translated sentences order align word embeddings different languages. workshop paper investigate autoencoder model learning multilingual word representations without word-level alignements. autoencoder trained reconstruct bag-of-word representation given sentence encoded representation extracted translation. evaluate approach multilingual document classiﬁcation task labeled data available language classiﬁcation must performed different language experiments observe method compares favorably previously proposed method exploits word-level alignments learn word representations. vectorial word representations proven useful multiple tasks it’s shown meaningful representations capturing syntactic semantic similarity learned unlabled data. along labeled data representations allows exploit unlabeled data improve generalization performance given task even allowing generalize vocabulary observed labeled data only. majority previous work concentrated monolingual case recent work started looking learning word representations aligned across languages representations applied variety problems including cross-lingual document classiﬁcation phrase-based machine translation common property approaches word-level alignment translated sentences leveraged either derive regularization term relating word embeddings across languages workshop paper experiment method learn multilingual word representations without word-to-word alignment bilingual corpora training. require aligned sentences exploit word-level alignments propose multilingual autoencoder model learns relate hidden representation paired bag-of-words sentences. representations context cross-lingual document classiﬁcation labeled dataset available language another one. multilingual word representations want learn classiﬁer documents language documents another language. preliminary experiments suggest method competitive representations learned rely word-level alignments. section describe initial autoencoder model learn representation input bag-of-words reconstructed. then section extend autoencoder multilingual setting. related work discussed section experiments presented section bag-of-words representation sentence. speciﬁcally word index ﬁxed vocabulary words. bag-of-words order words within correspond word order original sentence. wish learn d-dimensional vectorial representation words training sentence bag-of-words {x}t propose achieve using autoencoder model encodes input bag-of-words word representations using non-linear decoder trained reproduce original bag-of-words. speciﬁcally matrix matrix whose columns vector representations word. aggregated representation given bag-of-words learn meaningful word representations wish encourage contain information allows reconstruction original bag-of-words done choosing reconstruction loss designing parametrized decoder trained jointly word representations minimize loss. words implicitly high-dimensional objects care must taken choice reconstruction loss decoder stochastic gradient descent efﬁcient. instance dauphin recently designed efﬁcient algorithm reconstructing binary bag-of-words representations documents input ﬁxed size vector element associated word word appears least document. importance sampling avoid reconstructing whole -dimensional input vector would expensive. work propose different approach. assume that decoder obtain then treat input bag-of-words |x|-trials multinomial sample distribution reconstruction loss negative log-likelihood must ensure decoder compute xi|φ) efﬁciently speciﬁlarge practice. precludes procedure would compute numerator w|φ) speciﬁcally probabilistic tree decomposition xi|φ). let’s assume word note sequence internal nodes path root given word always corresponding root. vector associated left/right branching choices path means path branches left internal node figure illustration bilingual autoencoder learns construct bag-of-word english sentence barked french translation chien japp´e. horizontal blue line across input-to-hidden connections highlights fact connections share parameters since words bag-of-words outputs thus required decoder. course worse case scenario since words share internal nodes paths decoder output computed once. organizing words tree larochelle lauly used random assignment words leaves full binary tree found work well practice. element wise non-linearity d-dimensional bias vector dimensional bias vector matrix sigm sigmoid non-linearity. left/right branching probability thus modeled logistic regression model applied non-linearly transformed representation input bag-of-words multilingual bag-of-words let’s assume sentence bag-of-words source language associated bag-of-words sentence translated target language human expert. assuming training pairs we’d like learn representations languages aligned pairs translated words similar representations. while literature autoencoders usually refers post-nonlinearity activation vector hidden layer different description simply consistent representation documents experiments non-linearity used achieve this propose augment regular autoencoder proposed section that sentence representation given language reconstruction attempted original sentence language. speciﬁcally deﬁne language speciﬁc word representation matrices corresponding languages words respectively. also number words vocabulary languages different. word representations however size languages. sentence-level representation extracted encoder becomes sentence either languages want able perform reconstruction original sentence languages. particular given representation language we’d like decoder perfrom reconstruction language another decoder reconstruct language again decoders form proposed section decoders language parameters either notice share bias nonlinearity across decoders. encoder/decoder structure allows learn mapping within language across languages. speciﬁcally given pair train model construct construct reconstruct reconstruct itself. experiments performed tasks simultaneously combining equally weighting learning gradient each. experiments various weighting schemes investigated left future work. another promising direction investigation exploit fact tasks could performed extra monolingual corpora plentiful. mentioned recent work considered problem learning multilingual representations words usually relies word-level alignments. klementiev propose train simultaneously neural network languages models along regularization term encourages pairs frequently aligned words similar word embeddings. similar approach different form regularizor neural network language models work speciﬁcally investigate whether method rely word-level alignments learn comparably useful multilingual embeddings context document classiﬁcation. looking generally neural networks learn multilingual representations words phrases mention work showed useful linear mapping separately training monolingual skip-gram language models could learned. however rely speciﬁcation pairs words languages align. mikolov also propose method training neural network learn useful representations phrases context phrase-based translation model. case phrase-level alignments required. language available train classiﬁer however interested classifying documents different language test time. achieve this leverage bilingual corpora importantly labeled document-level categories. bilingual corpora used instead learn document representations languages enroucaged invariant translations language another. hope thus successfully apply classiﬁer trained document representations language directly document representations language trained multilingual autoencoder learn bilingual word representation english french english german. train autoencoder used english/french english/german section pairs europarl-v dataset. data composed million sentences sentence translated relevant languages. crosslingual classiﬁcation problem used english french german sections reuters rcv/rcv corpus provided amini documents english french german respectively. document categories organized hierarchy dataset. -category classiﬁcation problem thus created using top-level categories hierarchy documents language split training validation testing sets size respectively. documents represented form bag-of-words using tfidfbased weighting scheme. generally setup follows used klementiev uses preprocessing pipeline amini described earlier crosslingual document classiﬁcation performed training document classiﬁer documents language applying classiﬁer documents different language test time. documents language representated linear combination word embeddings learned language. thus classiﬁcation performance relies heavily quality multlingual word embeddigns languages speciﬁcally whether similar words across languages similar embeddings. train bilingual word representations sentence pairs extracted europarl-v languages train document classiﬁer reuters training language documents represented using word representations used linear classiﬁer. compare representations learned klementiev achieved simply skipping ﬁrst step training bilingual word representations directly using klementiev step provided word embeddings size english french language pair size english german pair. vocabulary used klementiev consisted words english words french words german. vocabulary used model represent reuters documents. cases document representations obtained multiplying word embeddings matrix either tfidf-based bag-of-words feature vector binary version english/german pair right french word nearest neighbor english word embedding space. test classiﬁcation error results reported table observe word representations learned autoencoder competitive provided klementiev notice results klementiev worse reported original reference. difference might fact preprocessing reuters data comes amini different klementiev particular note klementiev ignored documents belonged multiple categories amini included assigning category least training examples. table also shows french words nearest neighbor words english embedding space. complete picture presented t-sne visualization figure shows visualization french/english word embeddings frequent words languages. illustrations conﬁrm multilingual autoencoder able learn similar embeddings similar words across languages. presented evidence meaningful multilingual word representations could learned without relying word-level alignments. proposed multilingual autoencoder able perform competitively crosslingual document classiﬁcation task compared word representation learning method exploits word-level alignments. encouraged preliminary results future work investigate extensions bagof-words multilingual autoencoder bags-of-ngrams model would also learn representations short phrases. model particularly useful context machine translation system. thanks probabilistic tree output layer model could efﬁciently assign scores pairs sentences. thus think could useful complementary metric standard phrase-based translation system. acknowledgements thanks alexandre allauzen help subject. thanks also cyril goutte providing classiﬁcation dataset. finally thanks alexandre klementiev ivan titov help.", "year": 2014}