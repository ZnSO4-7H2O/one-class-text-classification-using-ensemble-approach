{"title": "Stacked Structure Learning for Lifted Relational Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Lifted Relational Neural Networks (LRNNs) describe relational domains using weighted first-order rules which act as templates for constructing feed-forward neural networks. While previous work has shown that using LRNNs can lead to state-of-the-art results in various ILP tasks, these results depended on hand-crafted rules. In this paper, we extend the framework of LRNNs with structure learning, thus enabling a fully automated learning process. Similarly to many ILP methods, our structure learning algorithm proceeds in an iterative fashion by top-down searching through the hypothesis space of all possible Horn clauses, considering the predicates that occur in the training examples as well as invented soft concepts entailed by the best weighted rules found so far. In the experiments, we demonstrate the ability to automatically induce useful hierarchical soft concepts leading to deep LRNNs with a competitive predictive power.", "text": "lifted relational neural networks describe relational domains using weighted ﬁrstorder rules templates constructing feed-forward neural networks. previous work shown using lrnns lead state-of-the-art results various tasks results depended hand-crafted rules. paper extend framework lrnns structure learning thus enabling fully automated learning process. similarly many methods structure learning algorithm proceeds iterative fashion top-down searching hypothesis space possible horn clauses considering predicates occur training examples well invented soft concepts entailed best weighted rules found far. experiments demonstrate ability automatically induce useful hierarchical soft concepts leading deep lrnns competitive predictive power. lifted relational neural networks weighted sets ﬁrst-order rules used construct feed-forward neural networks relational structures. central characteristic lrnns diﬀerent neural network constructed learning example crucially weights diﬀerent neural networks shared. allows lrnns neural networks learning relational domains despite fact training examples vary considerably size structure. previous work lrnns learned hand-crafted rules. cases weights ﬁrst-order rules learned training data accomplished using variant back-propagation. hand-crafted rules oﬀers natural incorporate domain knowledge learning process. applications however domain knowledge lacking rules weights learned data. paper introduce structure learning method lrnns. ∗faculty electrical engineering prague czech republic email souregusfel.cvut.cz †faculty electrical engineering prague czech republic email svatomafel.cvut.cz ‡faculty electrical engineering prague czech republic email zeleznyfel.cvut.cz §school computer science cardiﬀ university email schockaertscardiﬀ.ac.uk ¶school computer science cardiﬀ university email kuzelkaocardiﬀ.ac.uk template learn rules intuitively correspond creating connections among existing layers strategy refer stacked structure learning. rules added given iteration either deﬁne target predicates deﬁne predicate depend predicates ‘invented’ earlier layers well predicates considered domain. since actual meaning predicates depends learned rules associated weights structure learning alternated weight learning. intuitively means deﬁnitions predicates deﬁned earlier layers ﬁne-tuned based rules added later layers. present experimental result show resulting lrnns perform comparably lrnns learned hand-crafted rules. believe makes lrnns particularly convenient framework learning relational domains without need prior knowledge extensive hypertuning. somewhat surprisingly lrnns learned rules often compact hand-crafted rules. lrnn structure. lifted relational neural network weighted deﬁnite clauses i.e. pairs deﬁnite clause lrnn write denote corresponding deﬁnite clauses i.e. grounding lrnn deﬁned restriction grounding clauses correspond active rules i.e. rules whose antecedent satisﬁed least herbrand model neural network corresponding contains following types neurons intuitively neural network computes ground atom truth forward propagation. value given output atom neuron obtain truth values network propagates values closely mimics immediate consequence operator logic progamming. particular using immediate consequence operator ways become true corresponds fact head rule whose body already satisﬁed. similarly inputs atom neuron consist fact neurons form aggregation neurons form aggh output atom neuron inputs given activation function maps intuitively expresses strongly derived using rule neuron aggh ···∧bk. inputs aggregation neuron aggh rule neurons output aggregation neuron given inputs activation function weight corresponding rule. rule neuron intuitively needs atoms true. accordingly inputs given atom neurons abkθ output third type activation function. paper activation function applications usually consider lrnns form weighted ﬁrst-order rules weighted ground facts. particular represents example acts template constructing feed-forward neural networks network corresponding example weights given weights typically need learned training data follows. given list examples lrnn list training queries loss training query atoms minimized. loss function optimized using standard section describe structure learning algorithm lrnns. algorithm receives list training examples list training queries produces lrnn. simplicity assume constants used identiﬁers objects. particular assume attribute values represented using unary literals e.g. would instead color. besides restrictions structure training examples. structure learning algorithm create lrnns generic stacked structure describe. first rules deﬁne predicates representing soft clusters unary predicates dataset. thought ﬁrst layer lrnn weighted facts dataset comprise zeroth layer. instance unary predicates dataset lrnn contain following rules general second layer consist types rules. first rules introducing latent predicates. contrast unary predicates introduced ﬁrst layer latent predicates could also higher arity although practice upper bound imposed eﬃciency reasons. body rules predicates dataset itself latent predicates introduced ﬁrst layer. latent predicates introduced rules used bodies rules subsequent layers. second also rules predicate dataset head. typically rules learned predict target predicates want learn. actual intuitive meaning predicate instance large enough predicate high output whenever arguments correspond atoms either steps apart molecule suﬃciently high membership soft cluster structure learning algorithm iteratively constructs lrnns structure described previous section. alternates weight learning steps rule learning steps. weight learning steps algorithm uses stochastic gradient descent minimise squared loss lrnn optimising weights rules described section rule learning steps algorithm ﬁxes weights rules deﬁne latent predicates searches good rule rule squared loss lrnn decreases retrain weights rules non-latent head predicates. next describe algorithm detail. ﬁrst step structure learning algorithm construction ﬁrst level lrnn deﬁnes unary predicates representing soft clusters object properties described section predicates runs beam search algorithm searching space possible rules scoring function used beam search algorithm computed follows. given rule algorithm creates copy current lrnn given candidate rule added. optimises log-loss lrnn training non-ﬁxed weights i.e. weights rules non-latent predicates heads. score rule deﬁned log-loss training non-ﬁxed weights. reason retrain weights lrnn checking score rule eﬃciency considerations training weights whole lrnn corresponds training deep neural network. beam search algorithm ﬁnishes rule returned added original lrnn. note contains target predicates head. however addition adding also related rules latent predicates head follows. here assume simplicity latent predicates arity method still used latent predicates allowed diﬀerent arities. highest index contains latent predicate form body assume contain latent predicates latent predicate layer algorithm adds lrnn rules head obtained unifying variables process illustrated following example. example revisiting example molecular datasets bond∧α algorithm following latent-predicate rules lrnn extended rules obtained weights rules including corresponding latent predicates retrained using stochastic gradient descent note typically latent predicates used rules; weights considered training. subsequently algorithm ﬁxes weights rules corresponding latent predicates repeats process additional rule. repeated given stopping condition met. contains several thousands molecules labeled ability inhibit growth diﬀerent types tumors. compare performance proposed lrnn structure learning method best previously published lrnns contain large generic manually constructed weighted rule sets comparison include relational learners kfoil nfoil respectively combine relational rule learning support vector machines naive bayes learning. results shown figure figure automatically learned lrnns outperform kfoil nfoil terms predictive accuracy learned lrnns also competitive manually constructed lrnns although outperform them. slightly worse largest manually constructed lrnns based graph patterns vertices enumerating possible combinations soft cluster types three atoms soft cluster types bonds connecting them. figure displays statistics learned lrnn rule sets. statistics show structure learner turned produce quite complex lrnns multiple layers invented latent predicates. figure statistics learned lrnn rule sets experiments datasets. display number rules number conjunctive rules learned average length rules overall number layers figure projection evolution atom embeddings ﬁrst iterations structure learning lrnn initialization based unsupervised pre-training completely random initialization interpreted coordinates vector-space embedding properties figure plot evolution embeddings rules added structure learning algorithm. left panel figure displays evolution embeddings atom types pre-trained using unsupervised method originally used statistical predicate invention right panel ﬁgure displays evolution embeddings starting random initialization without unsupervised pre-training. seen ﬁgures model becomes complex atom types start make visible clusters. interestingly perhaps somewhat intuition unsupervised pre-training seemed consistently decrease predictive performance methods presented paper many respects similar structure learning methods statistical relational learning however clearly distinguishes previous approaches ability automatically induce hierarchies latent concepts. respect also related meta-interpretive learning however meta-interpretive learning applicable learning crisp logic programs. structure learning approach also related works reﬁning architectures neural networks however diﬀers ability handle relational data. paper introduced method learning structure lrnns capable learning deep weighted rule sets invented latent predicates. predictive accuracies obtained learned lrnns competitive results obtained previous work using manually constructed lrnns. method presented paper therefore potential make lrnns useful domains would otherwise diﬃcult come rule manually. also makes adoption lrnns non-expert users straightforward proposed method learn competitive lrnns without requiring user input acknowledgements acknowledge support project granted czech science foundation. supported grant leverhulme trust supported starting grant computational resources provided cesnet cerit scientiﬁc cloud provided programme projects large research development innovations infrastructures.", "year": 2017}