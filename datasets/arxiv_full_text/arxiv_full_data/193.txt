{"title": "Parsimonious Inference on Convolutional Neural Networks: Learning and  applying on-line kernel activation rules", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40", "I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8"], "abstract": "A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.", "text": "radical design approach presented paper considering reduction total computational load inference. achieved holistic intervention architecture training procedure targets parsimonious inference learning exploit remove redundant capacity architecture. accomplished introduction structural element inserted add-on contemporary architecture whilst preserving even improving recognition accuracy. approach formulates systematic data-driven method developing cnns trained eventually change size form on-the-fly inference targeting smaller possible computational footprint. results provided optimal implementation modern high-end mobile computing platforms indicating significant speed-up times. deep learning emerged dominant approach performing various classification tasks ranging computer vision speech processing relying deep convolutional neural networks architectures success ability learn abstract high-level feature representations using large amounts training data end-to-end fashion. however top-performing systems usually involve deep wide architectures therefore come cost increased storage computational requirements trend continuously increase depth networks. besides drawback deep neural networks intriguing property provides opportunities optimizations regarding storage space computations-the redundancy parameters computational units redundancy often leads misfit machine-learning task indented carried structure selected task. time increasing need deep cnns applications running embedded devices. devices-especially internet things erausually equipped small storage computational capabilities. cannot neither store large models cope associated computational complexity deep wide cnn. therefore becomes apparent that order make cnns appealing mobile devices parameter size number kernels need reduced. then fundamental dilemma becomes apparent dilemma design computationally efficient cnns time maintain recognition accuracy generalization keep overall structure efficient operating mobile embedded devices? work tries answer above-mentioned dilemma providing systematic implementing variants parsimonious computations. proposed approach allows training iii) optimize objectives regular back-propagation simultaneously primary task objective model. avoid prune-fine-tune iterative procedure usually followed order reduce size model. proposed framework incorporates learning module learning kernel activation module serving dual purpose enforcing utilization less convolutional kernels learning kernel activation rules de-activate sub-set filtering kernels inference phase depending input image content learned activation rules. using module essentially learns reduce initial size on-the-fly exploiting presence multiple information flow paths thus maximizing computing economy inference. since reduction number applied kernels layer leads reduction channels passed next layer reduction overall computational load even important. following section present related works section present proposed method. section present experimental results based simulations section speed results obtained porting modern high-end mobile platforms. section basic ideas discussed vlsi implementation case. deep learning primarily developed tool find meaningful representations large collections data. order achieve this complex function data learnt using large sequence simple functions turn results large number parameters. simple functions however computational memory intensive. therefore initial approach contradicts modern applications power consumption inference time play major role. particular case applications overall computational load well total number memory transactions might become prohibitive. reduction computational load associated specific deep-learning structure enabling factor towards broadening application field structures general applications featuring system computational capabilities. direction researchers attempt exploit data sparsity redundancy parameters inherent cnns order prune parts convolutional network thus ease computational load overall structure off-line post-training approach. methods coefficients analyzed training zeroed according magnitudeleading sparse matrices exploitable sparse arithmetic software. others trained result coefficients containing many insignificant coefficients possible. feng proposed method estimating structure model utilizing unlabelled data. method called indian buffet process captures distribution data accordingly balances model complexity fidelity. similarly incorporated structured sparsity learning order regularize number filters number channels depth network. implementation perspective also targets formulation dense weight matrix order completely remove channels filters even whole layers. yang proposed energy-aware pruning algorithm cnns directly uses energy consumption estimation guide pruning process. layer weights first pruned locally fine-tuned closed-form least-square solution quickly restore accuracy. authors proposed three-step method allowed prune redundant connections without affecting accuracy. first step train network learn connections important. second stage connections characterized unimportant pruned last stage network re-trained order fine-tune weights. similarly authors targeting implementations power devices taking advantage sparsity immanent intermediate filter responses order reduce spatial convolution every layer. specifically inspired loop perforation technique order skip convolution operation several locations. target framework proposed paper implement structure able learn primary task economical size complexity. since main source computational load number convolutional kernels employed convolutional layer idea exploited work enforce channel-wise sparsity outputs convolutional layer. kernel either learns capture useful information vanishes. contrast regularization approaches enforce global sparsity pattern order prune kernels channels zero output values propose technique enables kernels learn information useful subset observed cases. step forward enforcing sparsity simultaneously learned data-driven kernel activation rules rules used inference order avoid computing kernels useful particular datum. relevant kernels computed resulting significant economy figure learning kernel-activation module introduced linking consecutive convolutional layers. learning switch module capable switch individual kernels layer depending input output previous convolutional layer. module learns kernel disable training process reason specifically devised facilitate operation exploiting data sparsity usually employed images. modules initially induce desired channel-wise sparsity feature maps. simultaneously lkam learns activation rule kernel later exploited inference phase. many types activation rules formulated using regular differentiable functions available deep-learning frameworks. work study simplest lightweight rules constituting bank convolutional kernels followed average pooling sigmoid function offers smooth differentiable transition active inactive state. choice rule made order keep computational overhead lkam modules possible. internal structure lkam module shown figure vector ùë†ùë§ùë†ùë§<‚Ä¶ùë†ùë§>?ab ‚àà‚Ñùf? values formed. values corresponding feature convolutional layer thus imposing combination kernels activation rules produce sparsest vectors possible. desired sparsity. phase switches figure figure activated switches figure figure deactivated. information flow tweaked enforcing certain feature maps gradually smaller influence overall network corresponding rules turn co-adapting. goal training process obtain order impose desirable channel-wise sparsity primary loss function used backpropagation augmented term penalizing convolutional kernels. easiest achieve term proportional norm vectors denoted ùêøhij given following equation gain factor length vector. overall loss becomes ùêøùë°ùë§ùëè main loss dictated primary task model tuning gain factors control sparsity level separately layer. past training phase simple statistical analysis kernel activations throughout dataset reveal kernels contribution characterized zero near-zero utilization. kernels safely considered redundant pruned model along corresponding filters modules. inference elements vector used switches control corresponding kernels convolutional layer depending input i-th layer since value real number simple thresholding take values using threshold value thres follows ùë†ùë§m= ùë†ùë§m<ùë°‚Ñéùëüùëíùë† ùë†ùë§m‚â•ùë°‚Ñéùëüùëíùë† resulting binary activation values indicators whether apply corresponding filtering kernels input data skip particular computations. note inference switches figure figure considered active switches figure figure considered inactive. computational gain achieved kernels deactivation extended layer-level using residual architecture. residual cnns convolutional layer responsible effect fine-tuning output previous layer adding learned \"residual\" input. illustrated figure residual architectures utilize layer bypass connections offer alternative path information flow consecutive convolution layers. connections enable complete deactivation convolutional layer without intercepting subsequent processing stages. analysis conducted different classification problems food recognition problem utilizing food database comprised images food organized categories general image recognition problem utilizing imagenet ilsvrc dataset comprised images organized categories. datasets chosen classification cases featuring different qualities. particular food problem considered less complex classification case smaller number classes compared ilsvrc dataset featuring abstract visual attributes since food styling diverge. hand ilsvrc dataset contains images mainly depicting single structured object similar classes often discriminated fine details. spirit popular architectures evaluated caffenet squeezenet caffenet almost identical alexnet architecture vanilla architecture medium-sized parameter space relatively shallow. hand squeezenet deeper architecture designed storage efficiency consisting times less parameters compared alexnet exhibiting similar classification performance. study architectures exhibit vastly different characteristics terms redundancy used highlight ability prosed framework achieve required computational parsimony circumstances. case caffenet architecture lkams used control activity kernels layers lkams shares input layer controls configuration equivalent illustrated figure implementation details structure given table squeezenet architecture lkams controlling activity larger kernels constitute expandx sub-modules inside fire modules lkams share input corresponding fire modules control according configuration shown figure modification fire module addition lkam module target able deactivate computationally-intensive kernels aiming highest possible computational gain obvious choice nexne filter bank however filter bank also affected specific occasions. configuration ensures maximum possible gain potential deactivation kernels since much significant load corresponds larger kernels kernels also present within fire module. detailed configuration squeezenet architecture summarized table experiments conducted using caffe framework server dual xeon nvidia gpus. order compute recognition accuracy inference kernel deactivation emulated within caffe framework performing multiplication elements prior application sigmoid function order ensure final values vectors either zeros ones subsequently values used multiply corresponding channels feature maps using configuration training phase. recognition accuracy obtained training architectures proposed framework summarized tables threshold activation kernels accuracy measured validation ilsvrc test food-. accuracy ilsvrc compared reference models caffenet squeezenet. available on-line. evident introduction objective towards computational economy degraded obtained accuracy neither tester architectures. contrary observe notable improvement classification accuracy compared reference models datasets architectures. reveals dynamic approach regarding overall control functionality cnns. important aspect presented framework though improvement required computational load inference. detailed analysis configurations presented below. evaluation caffenet contacted ilsvrc food datasets. engagement lkam modules results reduction kernel filtering operations. shown graphically figure vertical axis corresponds activation frequency particular kernel throughout validation horizontal axis corresponds kernels layer sorted ascending utilization visualization purposes horizontal range normalized equal layers even accommodate different population kernels. illustration step-like plot implies kernels mostly permanently switched either smooth curve indicates kernels whose operation data-dependent. specifically number kernels calculated test image significantly lower nominal number kernels original networks. dramatically reduces related number mathematical operations. noted reduction active kernels single layer besides obvious benefit avoiding corresponding filtering computations results respective reduction number input channels next layer. turn offers additional computational gain directly proportional number switched-off kernels. figure percentage distribution number inactive kernels throughout validation every layer caffenet layer layer ilsvrc dataset food dataset. probability much higher higher population numbers food dataset strong indication excess redundancy suppression. statistical analysis switching activity caffenet ilsvrc dataset reveals network‚Äôs kernels active throughout validation specifically layer kernels layer kernels layer kernels layer kernels activated reduction computational load terms total operations computed compared reference caffenet taking also account computational overhead introduced switching modules. analysis food dataset indicates throughout validation kernels activated layers network. specifically layer kernels layer kernels layer kernels layer kernels activated average. reduction required macs relation original caffenet model evaluation squeezenet also conducted ilsvrc food datasets. ilsvrc dataset engagement lkam modules results reduction kernel filtering operations. shown graphically figure figure kernel activity profile every layer squeezenet ilsvrc dataset food dataset. easier food problem kernels mostly either permanently active inactive. statistical analysis switching activity squeezenet ilsvrc dataset reveals average throughout validation kernels active layers network reduction required macs relation original squeezenet model similar analysis switching activity squeezenet food dataset reveals average throughout validation kernels active layers network reduction required macs relation original squeezenet model becomes evident results caffenet squeezenet networks proposed architectural modification results significant reduction active kernels inference time. equal importance fact networks adapt form size depending data complexity classification problem caffenet demonstrates decrease average number active kernels necessary carry recognition food dataset compared ilsvrc dataset regardless trained regularization gains means corresponding network features excess learning capacity evaluation squeezenet complex bypass. order push margins redundancy reduction testing ability proposed method disable whole convolutional layers used paradigm squeezenet complex bypass featuring feedforward paths. structure lkam module controls filter make feasible potential deactivation every unit within fire module. test configuration incorporated simpler classification problem. formulate day/night detection problem utilizing flickrk dataset follows class night used special category night dataset. class images belong night class also classify water structures indoor transport. dataset contains roughly images balanced contain equal number night examples. figure discernible structure responds well simplification classification problem. statistical analysis throughout validation reveals average kernels structure active recognition rate high addition fire modules never used permanently pruned. additional indication power proposed approach simplification. another interesting fact statistical analysis kernel activations caffenet throughout ilsvrc dataset reveals images similar categories tend similar kernels. trend becomes evident towards deeper layers kernels capture higher-level specialized information. facts imply data featuring similar visual attributes subnetworks explicitly formed inside overall trained architecture learned activation rules. enables forward flow information tailor-made sub-class visual attributes enabling parsimonious accurate inference process. furthermore findings indicate lkams layer could benefit receiving signals previous layer inputs acting priors kernel activations current layer. path currently open future investigation. validity proposed approach verified caffenet architecture three different platforms intrinsyc xiaomi redmi note samsung edge. implementation inference time measured results shown table platforms computations offloaded mostly used housekeeping functions. programmed opencl using hand-optimizations aiming avoid prepostprocessing operations convolutions layers minimize memory usage avoiding temporary memories reduce much possible data transfers from/to efficiently exploit de-activation kernels. becomes apparent table always important speedup economy computations filtering kernels. however speed-up although proportional reduction total macs equal that. computational overhead related real-time restructuring data kernel tensors within prior computations necessary accommodating kernel switching capabilities. overhead implemented approaches analyzed results depicted figure .this figure illustrates total inference processing time percentage total active kernels case manually activated required percentage. processing time linearly proportional number active kernels. proportionality depends target platform. regardless imposed overhead clear advantage following dynamic inference batch batch batch nominal processing time batch nominal corresponding percentage batch nominal processing time batch nominal corresponding percentage batch batch batch nominal processing time batch nominal corresponding percentage batch nominal processing time batch nominal corresponding percentage proposed method brings significant advantages case vlsi hardware implementation filter kernels implemented separated hardware blocks parallel architecture filter kernels operating feature map. case lkam implemented array switches virtue voltage controllable switches shown figure case circuitry implementing filter kernel switched-off cutting feed power clock inputs saving energy corresponding dynamic static bias consumption. presented systematic implementing variants parsimonious computations. designing approach proposed allows learn possible computing resources change size form inference real-time depending input data. proposed framework incorporates learning module learning kernel activation module able occasionally permanently de-activate sub-set filtering kernels depending input image content inference phase. using module learns training phase reduce size real time thus result significant computational economy. simulation results real-time measurements optimal embedded implementations verify efficacy proposed method demonstrate ability resulting networks adapt size complexity classification task sparsity input data. jimmy rich caruana. deep nets need deep? nips pages cheng* felix rogerio feris sanjiv kumar alok choudhary shih-fu chang exploration parameter redundancy deep networks circulant projections iccv jiashi feng trevor darrell; learning structure deep convolutional networks ieee international conference computer vision huiskes flickr retrieval evaluation. international conference multimedia information retrieval vancouver canada bossard guillaumin gool food-‚Äìmining discriminative components random forests european conference computer vision alex graves jurgen schmidhuber. framewise phoneme classification bidirectional lstm neural network architectures. neural networks kaiming xiangyu zhang shaoqing jian deep residual learning image recognition peng y.-w. c.-k. tang network trimming data-driven neuron pruning approach towards efficient deep architectures arxiv preprint arxiv. olga russakovsky* deng* jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. ijcv alex krizhevsky ilya sutskever geoffrey hinton. imagenet classification deep convolutional neural networks. advances neural information processing systems pages tien-ju yang yu-hsin chen vivienne designing energy-efficient convolutional neural networks using energy-aware pruning. corr abs/. chunpeng wang yandan chen yiran learning structured sparsity deep neural networks advances neural information processing systems", "year": 2017}