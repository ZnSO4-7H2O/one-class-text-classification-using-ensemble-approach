{"title": "Eclectic Extraction of Propositional Rules from Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a \"Black Box\" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.", "text": "therefore obtaining meaningful interpretation trained neural networks significant improvement; would allow strengths neural networks noise robustness accuracy available high fidelity learning tasks. significant research done years compensate inadequacy neural networks. different algorithms proposed since algorithms shown different performance characteristics respective strengths weaknesses. paper proposes novel rule extraction algorithm called heretic heretic uses decision tree induction novel induce symbolic rules trained feed forward neural networks multilayer perceptrons heretic computationally fast also accurate existent rule extraction methods. paper organized follows; section gives overview previous rule extraction literature; section gives background information neural networks also motivation behind heretic; section describes heretic algorithm; section describes experimentation algorithm real world dataset performance analysis. describe notation used paper. number instances number features. output neural network given instance consequently also represents actual output node whenever mentioned subscript. components neural networks represented layered fashion. superscript always represents source layer subscripts represent weight nodes successive layers. therefore node layer node layer fashion output node layer bias node represents error unit learning rate. layerk units layer first hidden layer; inputs units input represents mean. network units. bstract artificial neural network among popular algorithm supervised learning. however neural networks well-known drawback black learner comprehensible users. lack transparency makes unsuitable many high risk tasks medical diagnosis requires rational justification making decision. rule extraction methods attempt curb limitation extracting comprehensible rules trained network. many extraction algorithms developed years respective strengths weaknesses. broadly categorized three types based approach internal model network. eclectic methods hybrid algorithms combine approaches attain performance. paper present eclectic method called heretic. algorithm uses inductive decision tree learning combined information neural network structure logical rules. experiments theoretical analysis show heretic better terms speed performance. artificial neural networks popular successful supervised learning methods. widely used successfully many practical domains. reason popularity ability recognize function complex non-linear relationships also robust noise data capable online learning. moreover neural networks also suitable handle real valued data. neural networks shown accurate classifier many domains compared symbolic methods nevertheless despite predictive ability related discriminative models support vector machines well-known drawbacks; blackbox approach lack transparency. systemic underlying knowledge representation little weights activation functions optimal parameters discovered neural network training. easily comprehensible human users knowledge weights actually represent. makes difficult gain credibility learners. despite attaining higher accuracy rule extraction neural networks seen extensive research past decades. surveys extensively explore research area andrews diederich tickle developed overall taxonomy categorizing techniques extract rules trained neural networks taxonomy recommends five primary criteria. representation language extracted rules; underlying network translucency architecture; computational complexity rule extraction technique; portability across multiple architectures. criterion translucency criteria important defines approach taken algorithm. criterion defines whether algorithm knowledge internal structure network. classification criterion spectrum rule extraction techniques information underlying maximum level granularity i.e. discrete hidden output units. craven shavlik categorized techniques \"decompositional.\" basic motif decompositional rule extraction techniques extract rules level individual hidden output unit. also heuristic analyze weights connections confer rules. contrast decompositional approaches theme pedagogical approaches view trained \"black box\". used oracle internal structure exposed. focus finding rules inputs outputs using form symbolic training procedure. pedagogical methods perform symbolic supervised learning find rules. eclectic methods combine previous approaches. analyze individual unit level also extract rules training instead analyzing weights. figure shows time line various rule extraction algorithms. decompositional methods search combination weights would cause activation particular unit. subset method towell shavlik example methods. mofn algorithm extension subset. clusters weights trained network equivalence classes extracts m-of-n style rules. algorithm later extended decompositional algorithms mofn setino method extracting m-of-n rules neural networks. several problems subset style algorithms. methods require hard limiting threshold function approximated logistic activation function hidden units extract rules. search space exponential thus limiting effectiveness large networks. many later researchers tried limit search space using heuristics rather searching subsets. rulex technique exclusively developed extract rules constrained error backpropagation cebpl network performs function approximation classification similar radial basis function networks recent years setiono leow jack zurada described method called rule extraction function approximating neural networks functional extraction neural networks extracting rules trained neural networks nonlinear regression. supports networks hidden layer. decompositional algorithms shown good accuracy. algorithms either rely analyzing weights searching subsets weights suffer time. moreover decompositional methods portable across different network types. earliest pedagogical methods developed saito nakano search combinations input values activate output unit. problem method size search space grow exponentially number input values. authors used heuristics limited search space. gallant developed method similar saito nakano’s method. gallant’s method uses procedure test combinations input values network. thrun developed method called validity internal analysis method linear programming used determine constraints placed network activation values consistent. trepan algorithm craven extracts decision tree trained network. trained network used oracle able answer queries learning process. oracle determines class instance presented query. osre recent pedagogical methods osre extends algorithm proposed ruleneg ordinal continuous variables using trained data perform -from-n coding searching orthogonal directions decision surface crosses decision boundary. main advantage pedagogical methods dependent internal structure neural networks. highly portable many different types network architectures even arbitrary learner svm. however main issue pedagogical methods performance. highly portable cannot take full advantage learned neural network structure. training required also quite extensive. training essentially means relearn already learned model using symbolic learning algorithms. advantage using neural networks intermediate stage minimal. eclectic methods combine previous approaches. analyze neural network architecture individual unit level also extract rules using training. example approach method proposed tickle called dedec applicable broad class multilayer feed forward anns trained backpropagation algorithm. method works steps. identifies functional dependencies inputs outputs analyzing architecture weight vectors trained network. next phase essentially pedagogical symbolic learning performed based weight analysis. eclectic methods well explored mainstream approaches. several gains hybrid approach. using knowledge architecture would enable much information possible network. moreover symbolic learning used pedagogical methods robust learn patterns simple search subset. neural network composed several neurons. neuron processing element linear model. feed forward network network architecture composes hierarchy nodes output lower layer becomes input higher linear combination weighted input goes activation function generate output here linear combination input vectors. passed onto activation function generate final output. activation function generally takes form sigmoid function. logistic function popular choice. basic operation heretic construct decision tree every node neural network. advantages heretic supports different types network architectures. neural network number layers network also partially connected. fact adopted recurrent networks extent. strength comes fact decision tree universal approximator learn function. unit neural network learns function also approximated decision tree. however understandability rules generated restricted activation function sigmoid type. first step rule extraction train neural network training set. network input outputs discretized. discretization algorithm used purpose. assume unit possible output values restricting units binary units. assumption cause loss precision output neural network unit ranges logistic activation used. however easily resolve using steep sigmoid function approximates step function. multiplying large constant input would make function steeper. neural network takes normalized input constant large value recommended value greater enables logistic function behave step function differentiable. therefore single neuron capable solving problems linearly separable based formulation perceptron neural network architectures formed. shown figure feed forward neural network multilayer perceptron composed hierarchical layers neurons. neuron capable learning simple concept. successive layer composition simpler concept forms complex pattern neuron represents simple pattern neural network. problem extracting rules simplified extraction done neuron level instead whole network. decompositional methods moreover pedagogical methods symbolic learning methods. example trepan mentioned earlier uses neural network oracle generate training build decision tree. however argued training neural network training beneficial case ultimately another training phase needs performed generate tree done training alone training decision tree training data hard problem itself training decision tree neural network even harder. poses question train network fully accurate. idea behind heretic advantage methods. decomposition individual units would simplify problem extraction; symbolic training better searching subset weights. would perform symbolic training neuron. node would generate decision tree trained input output generated neuron. output trees previous layer would become input trees next layer. hierarchical structure trees approximates neural network. terms symbolic training method learns rules training would suffice. inductive rule learning methods foil ripper well. however decision tree algorithms shown fast accurate rule learner widely used. many modification additions basic decision tree induction method performance widely studied. that’s chosen value. finally generate rule class whole neural network substitution. unit higher layer lower layer units input therefore replace lower layer input symbol respective rule. recursively substitute original inputs remain. certainly rule would incomprehensible itself. final step algorithm logical simplification. many algorithms available purpose algorithms based heuristics quadratic time number variables. minimizer used. used popular espresso logic minimization algorithm final output provided simplified rules disjunctive normal form. training discretization next step actually generate tree using unit generates tree unit would need training learn. training given learn used purpose. samples training trained network. sample generates output neurons. output previous layer becomes input next layer. find input output pair neurons. resultant training sets would used train decision trees. decision tree learning independent others also possible learn parallel. decision tree learning algorithm used discretion users. popular older learning methods like used. recent enhancements basic algorithm also used long output univariate tree. tree generation tree converted rule disjunctive normal form. easily done unique path tree represents conjunction features classifies instances. unique path tree decision rule. time complexity heretic dependent upon steps performed algorithm. training neural network part complexity heretic extracts rules possible provide trained neural network input. given training size number features number monks problems separate training test provided tested split. fold cross validation performed datasets. training continued epochs. learning rate chosen small compensate large constant multiplied modified training rule. chose learning rate value brings overall learning rate reasonable learning rate. used weka implementation ann. standard entropy based split function used along pruning. pruning dataset kept training set. whole experiment conducted times mean value taken. following table shows performance algorithms compared heretic. experimental results show heretic performs well converting trained rules. performance heretic close source ann. shown fidelity various datasets. fidelity datasets. fact paired-t statistical test shows significant difference datasets apart heart disease monks dataset. moreover heretic accuracy similar better algorithms cases. however performs best terms fidelity. better rule extraction algorithms terms fidelity. bested trepan datasets fernn datasets. trepan performs least datasets proving analysis made earlier learning decision tree neural network hard problem real performance gains. performance quite good compared still slightly better cases. shows instead extracting rules often easier simply decision tree quickly generate rules. along rule extraction performed best datasets; significant difference performance datasets based algorithms. neurons number connections number epochs assume network fully connected. time complexity training neural network would worst case. time training neural network generating input output neuron would require time. generating tree using requires number features particular case number features would dependent upon number neurons previous layer. assume number units layer with. units layer generating tree would require tested algorithm several benchmark datasets university california irvine dataset repository. compare accuracy extracted rules terms test data. also study fidelity rules. fidelity refers closely rule classification matches original neural network. performance algorithm compared rule extraction techniques trepan fernn. chosen representative respective categories. chose pedagogical method trepan decompositional method fernn. initially neural network hidden layer. several iterations different configurations best performing network architecture chosen. datasets trained hidden layers. nominal features converted several binary variables. real features normalized. proposed fast highly accurate rule extraction algorithm heretic. theoretical experimental results show using eclectic approach combines neural network architecture symbolic training considerably fast accurate extract meaningful rules neural network. algorithm useful many different network architecture many decompositional methods support accurate. logical minimization another novel approach applied heretic ensures compact comprehensible users. experiments also show combined approach huge performance gain simply using symbolic learning algorithm. many learning problems meaningful understandable learned model needed approach would bring better performance along good understandability. research enhanced extending heretic generating rules first order logic description logics. moreover assumption neuron output would binary resolved special training rule. another good research problem heretic could extended network units support smoother activation functions also functions sigmoid rbf. etchells t.a. lisboa p.j.g. orthogonal search-based rule extraction trained neural networks practical efficient approach. ieee transactions neural networks", "year": 2011}