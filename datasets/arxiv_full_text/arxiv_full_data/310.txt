{"title": "Improvements to deep convolutional neural networks for LVCSR", "tag": ["cs.LG", "cs.CL", "cs.NE", "math.OC", "stat.ML", "65K05, 90C15, 90C90"], "abstract": "Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12% relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep analysis comparing limited weight sharing and full weight sharing with state-of-the-art features. Second, we apply various pooling strategies that have shown improvements in computer vision to an LVCSR speech task. Third, we introduce a method to effectively incorporate speaker adaptation, namely fMLLR, into log-mel features. Fourth, we introduce an effective strategy to use dropout during Hessian-free sequence training. We find that with these improvements, particularly with fMLLR and dropout, we are able to achieve an additional 2-3% relative improvement in WER on a 50-hour Broadcast News task over our previous best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5% relative improvement over our previous best CNN baseline.", "text": "deep convolutional neural networks powerful deep neural networks able better reduce spectral variation input signal. also conﬁrmed experimentally cnns showing improvements word error rate relative compared dnns across variety lvcsr tasks. paper describe different methods improve performance. first conduct deep analysis comparing limited weight sharing full weight sharing state-of-the-art features. second apply various pooling strategies shown improvements computer vision lvcsr speech task. third introduce method effectively incorporate speaker adaptation namely fmllr log-mel features. fourth introduce effective strategy dropout hessian-free sequence training. improvements particularly fmllr dropout able achieve additional relative improvement -hour broadcast news task previous best baseline. larger -hour task additional relative improvement previous best baseline. deep neural networks state-of-the-art acoustic modeling speech recognition showing tremendous improvements order relative across variety small large vocabulary tasks recently deep convolutional neural networks explored alternative type neural network reduce translational variance input signal. example deep cnns shown offer relative improvement dnns across different lvcsr tasks. architecture proposed somewhat vanilla architecture used computer vision many years. goal paper analyze justify appropriate architecture speech investigate various strategies improve results further. first architecture proposed used multiple convolutional layers full weight sharing found beneﬁcial compared single convolutional layer. locality speech known ahead time proposed limited weight sharing cnns speech. beneﬁt allows local weight focus parts signal confusable previous work focused single layer work detailed analysis compare multiple layers lws. second numerous improvements cnns computer vision particularly small tasks. example using stochastic pooling provides better generalization pooling used second using overlapping pooling pooling time also improves generalization test data. furthermore multi-scale cnns combining outputs different layers neural network also successful computer vision. explore effectiveness strategies larger scale speech tasks. third investigate using better features cnns. features cnns must exhibit locality time frequency. found vtln-warped log-mel features best cnns. however speaker adapted features feature space maximum likelihood linear regression features typically give best performance dnns. fmllr transformation applied directly correlated vtln-warped log-mel space. however improvement observed fmllr transformations typically assume uncorrelated features. paper propose methodology effectively fmllr log-mel features. involves transforming log-mel uncorrelated space applying fmllr space transforming features back correlated space. finally investigate role rectiﬁed linear units dropout hessian-free sequence training cnns. relu+dropout shown give good performance cross-entropy trained dnns employed sequence-training. however sequence-training critical speech recognition performance providing additional relative gain ce-trained training dropout mask changes utterance. however training guaranteed conjugate directions dropout mask changes utterance. therefore order make dropout usable keep dropout mask ﬁxed utterance iterations conjugate gradient within single iteration. results proposed strategies ﬁrst explored english broadcast news task. difference multiple layers lvcsr task. second various pooling strategies gave improvements computer vision tasks help much speech. third observe improving input features including fmllr gives improvements wer. finally ﬁxing dropout mask iterations lets dropout sequence training avoids destroying gains dropout accrued training. putting together improvements fmllr dropout able obtain relative rest paper organized follows. section describes basic architecture serves starting point proposed modiﬁcations. section discuss experiments lws/fws pooling fmllr relu+dropout section presents results proposed improvements task. finally section concludes paper discusses future work. section describe basic architecture introduced serve baseline system improve upon. found convolutional layers four fully connected layers optimal lvcsr tasks. found pooling size appropriate ﬁrst convolutional layer pooling used second layer. furthermore convolutional layers feature maps respectively fully connected layers hidden units. optimal feature used vtln-warped log-mel ﬁlterbank coefﬁcients including delta double delta. using architecture cnns able achieve relative improvement dnns across many different lvcsr tasks. paper explore feature architecture optimization strategies improve results further. preliminary experiments performed english broadcast news task acoustic models trained hours english broadcast news speech corpora. results reported ears devf set. unless otherwise noted cnns trained cross-entropy results reported hybrid setup. convolutional neural networks require features locally correlated time frequency. implies linear discriminant analysis features commonly used speech cannot used cnns remove locality frequency ﬁlter-bank features type speech feature exhibit locality property explore additional transformations applied features improve wer. table shows function input feature cnns. following observed using fmllr speaker-adapt input help. reason could fmllr assumes data well modeled diagonal model would work best decorrelated features. however features highly correlated. work image recognition makes convolutional layers fully connected layers. convolutional layers meant reduce spectral variation model spectral correlation fully connected layers aggregate local information learned convolutional layers class discrimination. however work done thus speech introduced novel framework modeling spectral correlations framework allowed single convolutional layer. adopt spatial modeling approach similar image recognition work explore beneﬁt including multiple convolutional layers. table shows function number convolutional fully connected layers network. note experiment number parameters network kept same. table shows increasing number convolutional layers helps performance starts deteriorate. furthermore table cnns offer improvements dnns input feature set. cnns explored image recognition tasks perform weight sharing across pixels. unlike images local behavior speech features frequency different features high frequency regions. addresses issue limiting weight sharing frequency components close other. words high frequency components different weights however type approach limits adding additional convolutional layers ﬁlter outputs different pooling bands related. argue apply weight sharing across time frequency components using large number hidden units compared vision tasks convolutional layers capture differences high frequency components. type approach allows multiple convolutional layers something thus explored speech. table shows function number hidden units convolutional layers. total number parameters network kept constant experiments. observe increase number hidden units steadily decreases. increase number hidden units past would require reduce number hidden units fully connected layers less order keep total number network parameters constant. observed reducing number hidden units results pooling frequency time shown optimal speech. pooling dependent input sampling rate speaking style compare best pooling size different tasks different characteristics namely speech switchboard telephone conversations speech english broadcast news table indicates pooling essential cnns tasks pooling= optimal pooling size. note experiment pooling already shown help swb. pooling important concept cnns helps reduce spectral variance input features. work explored using pooling pooling strategy. given pooling region activations a|rj operation max-pooling shown equation problems max-pooling overﬁt training data necessarily generalize test data. pooling alternatives proposed address problems max-pooling pooling stochastic pooling seen simple form averaging corresponds max-pooling. problems average pooling elements pooling region considered areas low-activations downweight areas high activation. pooling seen tradeoff average max-pooling. pooling shown give large improvements error rate computer vision tasks compared pooling stochastic pooling another pooling strategy addresses issues average pooling. stochastic pooling ﬁrst probabilities region formed normalizing activations across region shown equation increase wer. able obtain slight improvement using hidden units ﬁrst convolutional layer second layer. hidden units convolutional layers typically used vision tasks many hidden units needed capture locality differences different frequency regions speech. speech recognition tasks characteristics signal lowfrequency regions different high frequency regions. allows limited weight sharing approach used convolutional layers weights span small local region frequency. beneﬁt allows local weight focus parts signal confusable perform discrimination within small local region. however drawbacks requires setting hand frequency region ﬁlter spans. furthermore many layers used limits adding additional full-weight sharing convolutional layers ﬁlter outputs different bands related thus locality constraint required convolutional layers preserved. thus work point looked layer alternatively full weight sharing idea convolutional layers explored similar done image recognition community. approach multiple convolutional layers allowed shown adding additional convolutional layers beneﬁcial. addition using large number hidden units convolutional layers better captures differences high frequency components. since multiple convolutional layers critical good performance paper explore multiple layers. speciﬁcally activations layer locality preserving information another layer. results comparing shown table note results stronger vtln-warped log-mel+d+dd features opposed previous work used simpler log-mel+d+dd. used convolutional layers found optimal. first notice increase number hidden units improvement conﬁrming belief hidden units important help explain variations frequency input signal. second match number parameters slight improvements seems offer similar performance. simpler implement choose ﬁlter locations limited weight ahead time prefer fws. parameters gives best tradeoff number parameters setting subsequent experiments. multinomial distribution created probabilities distribution sampled based pick location corresponding pooled activation shown equation stochastic pooling advantages max-pooling prevents overﬁtting stochastic component. stochastic pooling also shown huge improvements error rate computer vision given success stochastic pooling compare strategies max-pooling lvcsr task. results three pooling strategies shown table stochastic pooling seems provide improvements pooling though gains slight. unlike vision tasks appears tasks speech recognition data thus better model estimates generalization methods stochastic pooling offer great improvements pooling. work presented explore overlapping pooling frequency. however work computer vision shown overlapping pooling improve error rate .-.% compared non-overlapping pooling motivations overlapping pooling prevent overﬁtting. table compares overlapping non-overlapping pooling lvcsr speech task. thing point overlapping pooling many activations order keep experiment fair number parameters non-overlapping overlapping pooling matched. table shows difference overlapping non-overlapping pooling. again tasks data speech regularization mechanisms overlapping pooling seem help compared smaller computer vision tasks. previous work speech explored pooling frequency though investigate cnns pooling time frequency. however work vision performs pooling space time paper deeper analysis pooling time speech. thing must ensure pooling time speech overlap pooling windows. otherwise pooling time without overlap seen subsampling signal time degrades performance. pooling time overlap thought smooth signal time another form regularization. diminished sequence training. appears large tasks data regularizations pooling time helpful similar regularization schemes lp/stochastic pooling pooling overlap frequency. since cnns model correlation time frequency require input feature space property. implies commonly used feature spaces linear discriminant analysis cannot used cnns. shown good feature cnns vtln-warped log-mel ﬁlter bank coefﬁcients. feature-space maximum likelihood linear regression popular speaker-adaptation technique used reduce variability speech different speakers. fmllr transformation applied features assumes either features uncorrelated modeled diagonal covariance gaussians features correlated modeled full covariance gaussians. correlated features better modeled full-covariance gaussians full-covariance matrices dramatically increase number parameters gaussian component oftentimes leading parameter estimates robust. thus fmllr commonly applied decorrelated space. fmllr applied correlated log-mel feature space diagonal covariance assumption little improvement observed semi-tied covariance matrices used decorrelate feature space modeled diagonal gaussians. offers added beneﬁt allows full covariance matrices shared many distributions distribution diagonal covariance matrix. paper explore applying fmllr correlated features ﬁrst decorrelating appropriately diagonal gaussian approximation fmllr. transform fmllr features back correlated space used cnns. algorithm described follows. first starting correlated feature space estimate matrix features uncorrelated space. mapping given transformation order hessian-free optimization method critical performance gains sequence training compared sgd-style optimization though important ce-training rectiﬁed linear units dropout recently proposed regularize large neural networks. fact relu+dropout shown provide relative reduction cross-entropy-trained dnns english broadcast news lvcsr task however subsequent sequence training used dropout erased gains performance similar trained sigmoid non-linearity dropout. given importance sequence-training neural networks paper propose strategy make dropout effective sequence training. results presented context cnns though algorithm also used dnns. popular order technique dnns hessian-free optimization denote network parameters denote loss function denote gradient loss respect parameters denote search direction denote hessian approximation matrix characterizing curvature loss around central idea optimization iteratively form quadratic approximation loss minimize approximation using conjugate gradient iteration algorithm ﬁrst gradient computed using training examples. second since hessian cannot computed exactly curvature matrix approximated damped version gauss-netwon matrix levenberg-marquardt. then conjugate gradient multiple-iterations relative per-iteration progress made minimizing objective function falls certain tolerance. iteration gauss-newton matrix-vector products computed sample training data. dropout popular technique prevent over-ﬁtting neural network training speciﬁcally feed-forward operation neural network training dropout omits hidden unit randomly probability prevents complex co-adaptations hidden units forcing hidden units depend units. speciﬁcally using dropout activation layer given equation input layer weight layer bias non-linear activation function binary mask entry drawn bernoulli distribution probability since dropout used decoding factor used training ensures test time units dropped correct total input reach layer. conjugate gradient tries minimize quadratic objective function given equation iteration damped gaussnetwon matrix estimated using subset training however cnns using correlated features critical. multiplying fmllr transformed features inverse matrix decorrelated fmllr features back correlated space used cnn. transformation propose given transformation information captured layer neural network varies general speciﬁc concepts. example speech lower layers focus speaker adaptation higher layers focus discrimination. section look combine inputs different layers neural network explore complementarity different layers could potentially improve results further. idea known multi-scale neural networks explored computer vision. speciﬁcally look combining output fullyconnected convolutional layers. output fully-connected layers entire network trained jointly. thought combining features generated dnn-style cnn-style network. note experiment input feature used streams. results shown table small gain observed combining features much smaller gains observed computer vision. however given small improvement comes cost large parameter increase gains achieved increasing feature maps alone huge value idea. possible however combining cnns dnns different types input features complimentary could potentially show improvements. results proposed fmllr idea shown table notice applying fmllr decorrelated space achieve improvement baseline vtln-warped log-mel system. gain possible fmllr applied directly correlated log-mel features. stages neural network training performed. first dnns trained frame-discriminative stochastic gradient descent cross-entropy criterion. second ce-trained weights re-adjusted using sequence-level objective function since speech sequence-level task objective appropriate speech recognition problem. numerous studies shown sequence training provides additional relative improvement trained using training closely linked speech recognition objective function compared cross-entropy. using fact explore many iterations actually necessary moving training. table shows different iterations corresponding training. note training started lattices dumped using weight stopped notice annealing times achieve training compared weights converge. points fact spending much time unnecessary. weights relatively decent space better jump sequence training closely matched speech objective function. following setup hybrid trained using speakeradapted vtln+fmllr features input context frames. -layer hidden units layer sixth softmax layer output targets used. dnns pre-trained followed training sequence-training dnn-based feature system also trained architecture uses output targets. applied softmax reduce dimensionality using dnn-based features apply maximum-likelihood training followed feature model-space discriminative training using bmmi criterion. order fairly compare results hybrid system mllr applied featurebased system. systems trained vtln-warped log-mel+d+dd features sigmoid non-linearity. proposed cnn-based systems trained fmllr features described section relu+dropout discussed section table shows performance proposed cnn-based feature hybrid systems compares systems. proposed hybrid system offers relative improvement hybrid relative improvement hybrid system. proposed cnnbased feature system offers modest improvement cnn-based feature system slight improvements featurebased system surprising all. observed huge relative improvements hybrid sequence trained output targets compared hybrid ce-trained dnn. however features extracted systems gains diminish relative feature-based systems recall dropout produces random binary mask presentation training instance. however order guarantee good conjugate search directions given utterance dropout mask layer cannot change appropriate incorporate dropout allow dropout mask change different layers different utterances iterations working speciﬁc layer speciﬁc utterance number network parameters large saving dropout mask utterance layer infeasible. therefore randomly choose seed utterance layer save out. using randomize function seed guarantees dropout mask used layer/per utterance. experimentally conﬁrm using dropout probability layers reasonable dropout layers zero. experiments hidden units fully connected layers found beneﬁcial dropout compared hidden units results different dropout techniques shown table notice dropout used sigmoid result also found dnns using dropout ﬁxing dropout mask utterance across iterations achieve improvement wer. finally compare varying dropout mask training iteration increases. investigation figure shows vary dropout mask slow convergence loss training particularly number iterations increases later part training. shows experimental evidence dropout mask ﬁxed cannot guarantee iterations produce conjugate search directions loss function. neural network learn feature transformation seem saturate performance even hybrid system used extract features improves. thus table shows potential improve hybrid system opposed feature-based system. explore scalability proposed techniques hours english broadcast news development done darpa ears devf set. testing done darpa ears evaluation set. hybrid system uses fmllr features -frame context hidden layers containing sigmoidal units. dnn-based feature system trained output targets hybrid system output targets. results reported sequence training. again proposed cnn-based systems trained fmllr features described section relu+dropout discussed section table shows performance proposed system compared dnns system. proposed hybrid cnn-based feature system improve performance slightly deteriorates cnnbased features extracted network. however hybrid offers relative improvement hybrid system relative improvement cnn-based features systems. helps strengthen hypothesis hybrid cnns potential improvement proposed fmllr relu+dropout techniques provide substantial improvements dnns cnns sigmoid non-linearity vtln-warped log-mel features. paper explored various strategies improve performance. incorporated fmllr features also made dropout effective sequence training. also explored various pooling weight sharing techniques popular computer vision found offer improvements lvcsr tasks. overall proposed fmllr+dropout ideas able improve previous best results relative. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition ieee signal processing magazine vol. kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization proc. interspeech waibel hanazawa hinton shikano lang phoneme recognition using time-delay neural networks ieee transactions acoustics speech signal processing vol.", "year": 2013}