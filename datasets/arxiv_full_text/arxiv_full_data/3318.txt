{"title": "Robust PCA via Nonconvex Rank Approximation", "tag": ["cs.CV", "cs.LG", "cs.NA", "stat.ML"], "abstract": "Numerous applications in data mining and machine learning require recovering a matrix of minimal rank. Robust principal component analysis (RPCA) is a general framework for handling this kind of problems. Nuclear norm based convex surrogate of the rank function in RPCA is widely investigated. Under certain assumptions, it can recover the underlying true low rank matrix with high probability. However, those assumptions may not hold in real-world applications. Since the nuclear norm approximates the rank by adding all singular values together, which is essentially a $\\ell_1$-norm of the singular values, the resulting approximation error is not trivial and thus the resulting matrix estimator can be significantly biased. To seek a closer approximation and to alleviate the above-mentioned limitations of the nuclear norm, we propose a nonconvex rank approximation. This approximation to the matrix rank is tighter than the nuclear norm. To solve the associated nonconvex minimization problem, we develop an efficient augmented Lagrange multiplier based optimization algorithm. Experimental results demonstrate that our method outperforms current state-of-the-art algorithms in both accuracy and efficiency.", "text": "videos face images varying illumination background underlying clean face image regarded low-rank component moving objects shadows represent sparse part common words collection text documents captured low-rank matrix words distinguish document others represented sparse matrix weight parameter. unfortunately generally np-hard problem. relaxing nonconvex rank function -norm nuclear norm -norm respectively convex formulation yielded despite convex formulation ease optimization rpca major limitations. first underlying matrix incoherence guarantee practical scenarios data grossly corrupted. circumstances resulting global optimal solution deviate signiﬁcantly truth. second rpca shrinks singular values equally. nuclear norm essentially norm singular values well known norm shrinkage effect leads biased estimator implies nuclear norm over-penalizes large singular values consequently much biased solution. nonconvex penalties norm smoothly clipped absolute deviation penalty minimax concave penalty capped- regularization truncated function shown provide better estimation accuracy variable selection consistency recently nonconvex relaxations nuclear norm received increasing attention variations nuclear norm e.g. weighted nuclear norm singular value thresholding truncated nuclear norm proposed outperform standard nuclear norm. however applications abstract—numerous applications data mining machine learning require recovering matrix minimal rank. robust principal component analysis general framework handling kind problems. nuclear norm based convex surrogate rank function rpca widely investigated. certain assumptions recover underlying true rank matrix high probability. however assumptions hold real-world applications. since nuclear norm approximates rank adding singular values together essentially norm singular values resulting approximation error trivial thus resulting matrix estimator signiﬁcantly biased. seek closer approximation alleviate above-mentioned limitations nuclear norm propose nonconvex rank approximation. approximation matrix rank tighter nuclear norm. solve associated nonconvex minimization problem develop efﬁcient augmented lagrange multiplier based optimization algorithm. experimental results demonstrate method outperforms current state-of-the-art algorithms accuracy efﬁciency. many machine learning data mining applications dimensionality data high digital text documents genomic data images video sequences social networks ﬁnancial time series. data mining data sets challenging curse dimensionality. dimensionality reduction techniques project original high-dimensional feature space lowdimensional space extensively explored. among them principal component analysis ﬁnds small number orthogonal basis vectors characterize variability data well established commonly used. however fail spectacularly even single grossly corrupted entry exists data. enhance robustness outliers corrupted observations early attempts robust made nevertheless none algorithms yields solution polynomial-time strong performance guarantees broad conditions. seminal work recent version rpca becomes popular days. idea recover low-rank matrix highly corrupted observations rm×n. entries sparse component arbitrarily large magnitude. numerous applications ranging recommender system design anomaly detection dynamic networks. example paper propose novel nonconvex function directly approximate rank provides tighter approximation nuclear norm does. crucial reveal rank low-rank matrix estimation. solve nonconvex model devise augmented lagrange multiplier based optimization algorithm. theoretical convergence analysis shows iterative optimization least converges stationary point. extensive experiments three representative applications conﬁrm advantages approach. convex approach rpca studied thoroughly. proved locations nonzero entries uniformly distributed rank sparsity satisfy mild conditions exactly recovered high probability literature numerous algorithms developed solve e.g. apgl fista among them based approach popular. although theory elegant convex technique still computationally quite expensive poor convergence rate furthermore breaks large errors concentrate number columns incorporate spatial connection information sparse elements -norm introduced outlier pursuit combining simplicity elegant theory convex rpca recent paper proposed nonconvex rpca idea project residuals onto low-rank sparse matrices alternatively. speciﬁcally proceeds stages compute rank-k projection stage process sparse errors suppressed discarding matrix elements large approximation errors. method enjoys several nice properties including complexity global convergence guarantee fast convergence rate theoretical guarantee exact recovery low-rank matrix. however needs knowledge three parameters sparsity incoherence rank knowledge always readily available. here detect outliers column-wise sparsity treats entry independently. theoretical analysis model difﬁcult. model column space column support exactly recovered rank intrinsic matrix comparable number samples working range outlier pursuit limited. alleviate deﬁciency convex relaxations capped norm based nonconvex rpca proposed solves following problem small parameters denotes level gaussian noise. singular values greater absolute values elements greater objective function falls back however hard provide convergence guarantee nonconvex method. furthermore unitarily invariant orthonormal rm×m rn×n. certainly real norm. figure plots several rank relaxations literature. among them log-determinant function logdet small constant well studied formulation closely matches true rank nuclear norm deviates considerably singular values depart result proposed γ-norm overcomes imbalanced penalization different singular values convex nuclear norm. hand nonconvex formulation usually difﬁcult optimize. next section design effective algorithm solve solve ﬁrst develop following theorem provide proof appendix theorem rm×n diag. unitarily invariant function optimal solution following problem objective function combination concave convex functions. intrinsic structure motivates difference convex programing algorithm decomposes nonconvex function difference convex functions since term right-hand side bounded bounded. last term right-hand bounded. therefore {lt} {st} bounded. theorem sequence generated algorithm accumulation point. stationary point optimization problem proof sequence generated algorithm bounded shown lemmas bolzano-weierstrass theorem sequence must least accumulation point e.g. without loss generality assume converges thus primal feasibility condition satisﬁed. section evaluate algorithm deploying three important practical applications foregroundbackground separation surveillance video shadow removal face images anomaly detection. applications involve recovery intrinsically low-dimensional convergence analysis nonconvex optimization problem usually difﬁcult. section show algorithm least convergent subsequence tends stationary point. ﬁnal solution might globally optimal experiments show algorithm converges solution produces promising results. case since ||s|| nonsmooth redeﬁne subgradient s]ij hence ∂ss)|st+ bounded. similarly shown ∂ss)|st+ also bounded. thus bounded. lemma {lt} {st} bounded data gross corruption. compare algorithm state-of-the-art methods including convex rpca cnorm altproj three methods call propack package addition convex rpca state-of-the-art solver viz. inexact augmented lagrange multiplier method cnorm fast alternating algorithm convex relaxation solutions initial conditions. perform experiments matlab windows based intel xeon .ghz ram. results insensitive pretty broad range experiments. large value lead fast convergence small value result accurate solution. literature often used value discussed also affect ialm. large rank larger desired rank. provides manipulate desired rank. principle tune value finally following four experiments respectively. practice parameters chosen cross validation. fair comparison follow experimental setting altproj stop program relative error reached. methods follow parameter settings used corresponding papers. background subtraction video sequences popular approach detecting interesting activities scene. surveillance videos ﬁxed camera naturally modeled model relatively static background sparse foreground. first experiment scenario experiment benchmark data escalator contains frames size data matrix formed vectorizing frame concatenating vectors column-wisely. result size data background appears completely static thus ideal rank background matrix one. escalator unfortunately cnorm cannot successfully separate foreground background current stopping criterion. thus relax terminating relative error altproj desired rank one. rank ground truth available videos present visual comparison extraction results using different algorithms figure cnorm suffers noticeable artifacts overﬁtting rank component. missing since sparse component absorbed relative error. blur exists ialm recovery image also observed many work contrast altproj method obtain clean background. moreover steps escalator also removed methods since moving supposed part dynamic foreground. table gives quantitative comparison results. terms computing time method twice faster altproj times faster ialm. intuitive interpretation observe fewer iterations required algorithm converge. therefore method efﬁcient even matrix size large. furthermore altproj algorithm obtain desired rankone matrix nuclear norm based ialm results rank implies contains many blurred images. increase size data matrix ialm performs even worse since incur errors rank approximation. results emphasize signiﬁcance good rank approximation. second experiment scenario purpose experiment demonstrate effectiveness algorithm dynamic backgrounds. cases background changes time e.g. illumination variation weather change. background higher rank. sequences captured lobby. size occasion background changes mainly caused switching on/off lights. therefore expect rank again desired rank altproj examples shown figure foreground-background separation escalator video. three rows bottom original image frame static background dynamic foreground respectively. method figure ﬁrst example denotes scene three lights turned example except shadows pants image recovered ialm recovered background images appear satisfactory. list numerical results table experiment algorithm almost times faster altproj times faster ialm. also noted that rank obtained ialm still high. removing shadows specularities saturations face images another important application rpca face images taken different lighting conditions often introduce errors face recognition errors might large magnitude supposed sparse spatial domain. given enough face images person possible reconstruct true face image. images extended yale database subjects subject images size taken varying different illuminations. images subject heavily corrupted different illumination conditions. images converted dimensional column vectors hence subject. since images well aligned rank figure illustrates results different algorithms images. proposed algorithm removes specularities shadows well artifacts left using ialm cnorm. although visual qualities similar altproj method numerical measurements table demonstrate algorithm times faster altproj. similar results ialm cnorm result high ranks. widely known images subject reside low-dimensional subspace. inject images different subjects dominant number images subject stand outliers anomalies. test this images usps data choose digits since share similarities. treat image -dimensional column vector. data matrix constructed contain ﬁrst samples digit last samples goal identify anomalies including unsupervised way. apply model estimate expected capture respectively. norm column used identify anomalies. ideally give larger values ’’s. figure shows norm columns visual quality apply thresholding threshold small values. found. besides four columns also appear. shown figure four written different rest ’’s. paper investigates nonconvex approach robust principal component analysis problem. particular provide novel matrix rank approximation robust less biased nuclear norm. devise augmented lagrange multiplier framework solve nonconvex optimization problem. extensive experimental results demonstrate proposed approach outperforms previous algorithms. algorithm used powerful tool efﬁciently separate low-dimensional sparse structure high-dimensional data. would interesting establish theoretical properties proposed nonconvex approach example theoretical guarantees estimator consistent. cnorm figure shadow removal face images. column displays sample images subject yale database. columns show rank approximation obtained different algorithms. rows corresponding sparse components. croux haesbroeck principal component analysis based robust estimators covariance correlation matrix inﬂuence functions efﬁciencies biometrika vol. wright ganesh peng robust principal component analysis exact recovery corrupted low-rank matrices convex optimization advances neural information processing systems zhang wright decomposing background topics keywords principal component pursuit proceedings international conference information knowledge management. note hold since frobenius norm unitarily invariant; hoffman-wielandt inequality; holds thus lower bound minimizing hence diagv optimal solution problem kang peng cheng robust subspace clustering tighter rank approximation proceedings international conference conference information knowledge management. fazel hindi boyd log-det heuristic matrix rank minimization applications hankel euclidean distance matrices american control conference proceedings vol. ieee peng kang cheng subspace clustering using log-determinant rank approximation proceedings sigkdd international conference knowledge discovery data mining.", "year": 2015}