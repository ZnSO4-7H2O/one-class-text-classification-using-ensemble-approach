{"title": "Identifiability of Nonparametric Mixture Models and Bayes Optimal  Clustering", "tag": ["math.ST", "cs.AI", "cs.LG", "stat.ML", "stat.TH"], "abstract": "Motivated by problems in data clustering, we establish general conditions under which families of nonparametric mixture models are identifiable by introducing a novel framework for clustering overfitted \\emph{parametric} (i.e. misspecified) mixture models. These conditions generalize existing conditions in the literature, and are flexible enough to include for example mixtures of Gaussian mixtures. In contrast to the recent literature on estimating nonparametric mixtures, we allow for general nonparametric mixture components, and instead impose regularity assumptions on the underlying mixing measure. As our primary application, we apply these results to partition-based clustering, generalizing the well-known notion of a Bayes optimal partition from classical model-based clustering to nonparametric settings. Furthermore, this framework is constructive in that it yields a practical algorithm for learning identified mixtures, which is illustrated through several examples. The key conceptual device in the analysis is the convex, metric geometry of probability distributions on metric spaces and its connection to optimal transport and the Wasserstein convergence of mixing measures. The result is a flexible framework for nonparametric clustering with formal consistency guarantees.", "text": "motivated problems data clustering establish general conditions families nonparametric mixture models identiﬁable introducing novel framework clustering overﬁtted parametric mixture models. conditions generalize existing conditions literature ﬂexible enough include example mixtures gaussian mixtures. contrast recent literature estimating nonparametric mixtures allow general nonparametric mixture components instead impose regularity assumptions underlying mixing measure. primary application apply results partition-based clustering generalizing well-known notion bayes optimal partition classical model-based clustering nonparametric settings. furthermore framework constructive yields practical algorithm learning identiﬁed mixtures illustrated several examples. conceptual device analysis convex metric geometry probability distributions metric spaces connection optimal transport wasserstein convergence mixing measures. result ﬂexible framework nonparametric clustering formal consistency guarantees. data clustering central problem deﬁne proper notion cluster equivalently partition input space given target partition becomes possible evaluate clustering algorithms consistent manner. modern approaches include mode clustering density clustering stochastic blockmodels hierarchical clustering classical approach problem however arguably gaussian model-based clustering points partitioned according generative gaussian mixture model model appropriate provides simple well-deﬁned partition clustering algorithms evaluated compared. model extended various parametric semiparametric models however extension methodology general nonparametric settings remained elusive. largely extreme nonidentiﬁability nonparametric mixture models problem well-studied existing results require strong structural assumptions thus signiﬁcant open problem generalize assumptions ﬂexible class nonparametric mixture models. also probability measures represent distinct subpopulations belonging overall heterogeneous population given observations interested classifying observation subpopulations without labels. mixture components weights identiﬁed expect learn model unlabeled data bayes’ rule classify observation deﬁnes target partition call bayes optimal partition thus studying partitions question mixture model identiﬁable? motivated aforementioned applications clustering question focus paper. example parametric assumptions gaussianity possible guarantee representation unique hence identiﬁable results mostly follow early line work general identiﬁcation problem parametric assumptions rarely hold practice however thus interest study nonparametric mixture models form i.e. comes ﬂexible nonparametric family probability measures. common assumption component measures multivariate independent marginals particularly useful statistical problems involving repeated measurements model also deep connections algebraic properties latent structure models various structural assumptions considered including symmetry tail conditions translation invariance identiﬁcation problem discrete mixture models also central problem topic models popular machine learning notably existing literature imposes structural assumptions components diﬃcult satisfy clustering problems. reasonable constraints ensure uniqueness avoiding restrictive modeling assumptions paper establish series positive results direction providing practical algorithm nonparametric clustering. contrast existing literature allow arbitrary probability measure instead imposing assumptions mixing measure propose novel framework reconstructing nonparametric mixing measures leveraging overﬁtted parametric mixtures mixture density estimators using clustering algorithms partition resulting estimators. construction implies regularity conditions suﬃce ensure mixture model identiﬁable. main application interest apply construction problems nonparametric clustering. note also work falls category node-model based clusterings assumed data points drawn i.i.d. distribution. contrast considerable recent work edge-model based clusterings based stochastic block model arguably received much greater attention node-model based clusterings. remainder section outline major contributions. section cover preliminary material background order groundwork abstract framework. present high-level geometric overview method section proceeding main results paper sections section present detailed construction takes mixture distribution outputs mixing measure culminating main theorem identiﬁability. section discuss leverage construction deﬁne consistent estimator parameter section provide explicit examples mixture models satisfy assumptions. section apply results problem clustering prove consistency theorem problem. section contains empirical evaluation method section concludes paper discussion extensions. identiﬁcation criterion nonparametric mixture models based notion clusterability; extending partition-based clustering nonparametric settings; practical algorithm nonparametric clustering density estimation. contributions builds previous creates compelling narrative strengthens well-known connections identiﬁability mixture models cluster analysis nonparametric density estimation. nonparametric identiﬁability formulate general assumptions guarantee family nonparametric mixtures identiﬁable informally show long exists overﬁtted mixture model simultaneously approximates globally locally simple clustering procedure correctly identify mixing measure generates long suﬃciently well-separated hellinger distance. estimation show procedure consistently recover nonparametric clusters given i.i.d. observations also discuss conditions hellinger uniform convergence mixture densities. clustering make connections so-called bayes optimal partition extend notion general nonparametric settings leveraging results nonparametric mixtures approach general built theory abstract measures metric spaces section introduce abstract setting outline notation discuss general problem identiﬁability mixture models. deliberately include plenty examples order help acquaint reader particular notation problem setting. thorough introduction general topic mixture models statistics lindsay ritter titterington nonparametric mixture models metric space denote space regular borel probability measures ﬁnite moments denote space mixing measures given deﬁne probability measure formally borel function deﬁned uniquely deﬁnes measure called mixture distribution slight abuse notation write shorthand confusion arguments. given borel deﬁne remark abstract presentation mixture models needed reasons emphasize statistical parameter interest contrast usual parametrization terms atoms weights; emphasize approach works general measures metric spaces. beneﬁts sequel albeit cost extra abstraction onset. inﬁmum taken couplings i.e. probability measures marginals wasserstein distances importance mixture models nguyen remark hellinger distance replaced metric particular remark convention upper case letters mixture distributions mixing measures lower case letters mixture components weights example consider family gaussian mixtures case obviously examples extended arbitrary parametric nonparametric families. deﬁnition mixtures subsets mixing measures—as opposed families component distributions— makes easy encode additional constraints following example. example continuing example suppose wish impose additional constraints family mixture distributions. example might interested gaussian mixtures figure mixture three gaussians. diﬀerent representations mixture gaussians mixture subgaussians. diﬀerent colours represent diﬀerent assignments mixture components. injective. good overview problem classical perspective hunter allman main purpose section highlight known subtleties identifying nonparametric mixture models. thus cannot unique decomposition measure although example allows arbitrary pathological decompositions conditional measures following concrete example shows solving nonidentiﬁability issue complicated simply avoiding certain pathological partitions input space. figure write mixture four ways panel represented uniquely mixture three gaussians. allow subgaussian components however bottom panel shows three equally valid representations mixture subgaussians. recalling examples follows identiﬁed respect respect valid objection previous example number components seems misspeciﬁed. following example shows salient issue moreover separation component means still enough ensure identiﬁability. consider moving arbitrarily right. three examples bottom panel figure mixture model components means component made arbitrarily apart mixture model still nonidentiﬁable. much existing literature makes assumptions structure allowed evidently equivalent restricting supports measures focus contrast allow components take essentially shape imposing regularity assumptions procedure makes intuitive sense main thrusts paper outlining make procedure well-deﬁned sense always return mixing measure. surprisingly subtle problem requires careful consideration various spaces involved formal details analysis postponed section furthermore evident construction guaranteed succeed arbitrary mixing measures illustrated examples section thus aspect analysis provide assumptions ensure success construction. intuitively clear long well-separated corresponding mixture approximation consist gaussian components also well-separated. unfortunately quite enough illustrated example highlights subtleties inherent construction. next section formalize ideas introduce concepts regularity clusterability axiomatize conditions needed order reconstructed—and hence identiﬁed—from sections discuss existence nontrivial mixture distributions satisfy conditions well learn mixtures data. nonparametric identiﬁability integer family mixing measures. section study conditions guarantee injectivity canonical embedding using procedure described order ensure overﬁtted mixture approximation unique interested hellinger projection onto well-behaved families mixture distributions. speciﬁcally assume indexed collection families mixing measures satisﬁes following sequel {ql}∞ {ql} monotonic i.e. ql+; collection mixture distributions identiﬁable purpose {ql} approximate sequence mixing measures increasing complexity conditions allow substantial generality; example family identiﬁable satisﬁes practice however often enough take i.e. gaussian mixing measures atoms note special case exist mixtures unique well-deﬁned every projection well-deﬁned. furthermore condition implies exists well-deﬁned sends mixture distribution mixing measure. thus unambiguously write deﬁne example measure mixing measure depicted figure remark number overﬁtted mixture components play important largely unheralded role sequel. part suppress dependence various quantities notational simplicity. always assume statement theorems typically assume suﬃciently large sense ﬁxed example obvious choice gaussian mixtures atoms. appealing property universal approximation fact limit family much still retaining universal approximation using known results approximating densities radial basis functions best approximation however contains many components true number nonparametric components next step cluster components subgroups subgroup approximates formalize this introduce notion assignment functions. denote maps al→k—a function al→k represents particular given al→k deﬁne normalizing pause review developed far. mixing measure ql-regular ρ-projections always grouped group approximates nonparametric component mixing weight note said anything might assignment exists. problem identifying discussed section need regular sense globally approximated assignment function liml→∞ suﬃces approximate independently mixtures gaussians deﬁnition regularity requires however approximation exists local approximation achieved speciﬁcally ρ-projection although always guaranteed regularity simply asks q∗—the closest mixture γ—is worse suggests condition fairly weak. remark ql-regular family deﬁnition implies thus expressivity collection {ql} constrains large regular family fortunately many families including gaussian mixtures possible approximate arbitrary measures; i.e. thus practice much constraint. concept regularity weak condition summarizes basic behaviour seek mixture distribution exploit behaviour order identify need impose slightly stronger assumption. regular assignment sense satisﬁes deﬁnition contrast regularity—which merely asserts existence regular assignment λ—clusterability takes requirement step requiring regular assignment fact determined projection alone. terminology clusterable intended provoke reader imagining cluster function clusters components weights together approximates problem constructing cluster function fascinating taken section there show separation condition regular assignments recovered single-linkage clustering assumption vacuous. remainder section however take assumption faith order complete journey identify alone. condition guarantees mixture model identiﬁable theorem ql-clusterable family exists function canonical embedding. particular bijection mixture model identiﬁable. illustrated cautionary tales examples identiﬁcation nonparametric mixtures subtle problem theorem thus provides powerful general condition identiﬁability nonparametric problems. fact show section distribution approximated arbitrarily well regular mixture. idea behind proof invoke clusterability obtain cluster function which—when combined machinery previously introduced—yields complete roadmap takes mixture distribution mixing measure atoms. following diagram summarizes roadmap remark pause unpack sequence maps given functions needed technical reasons properly identify mixing measure interest. well-known projection operator needed ensure well-deﬁned. what’s novel cluster function interpreted cluster function takes points returns assignment points identiﬁed rich family identiﬁable mixture models turn attention question learning mixing measure data. broken steps ﬁrst show natural separation condition regular assignments recovered population-level extend results case i.i.d. samples importantly results section indicate exist nontrivial families clusterable mixtures moreover families fact learned data. theorem fairly abstract relies existence cluster function reconstruct regular assignments overﬁtted mixing measure section make concepts much concrete constructing explicit cluster function simple distance-based thresholding rule. rule turn equivalent performing single-linkage clustering hence recovered practice without knowing optimal threshold advance. moreover recovered single-linkage clustering thus abstract regular assignment recovered single-linkage clustering without knowing optimal threshold crucially implies depends hence deﬁnes cluster function subset result following important corollary corollary suppose nonparametric components must separated proportional hellinger diameter approximating mixture highlights issue example —although means arbitrarily separated increase separation diameter components continues increase well. thus cannot chosen haphazard separation condition quite weak attempt made optimize lower bound. example minor tweak proof reduce constant constant although expect careful analysis weaken condition main focus present main idea behind identiﬁability connection clusterability separation save optimizations future work. however easily imagine complicated clustering schemes. indeed since distance matrix always well-deﬁned could applied clustering algorithms k-means spectral clustering produce assignment whether resulting clustering produces regular assignment. condition ensures clustering algorithm correctly reconstruct regular assignment yields identiﬁcation condition spirit proposition highlights advantage abstract viewpoint speciﬁc form assignment cluster function left unspeciﬁed. results provide framework learning nonparametric mixture measures principle however discussion restricted population-level properties measures. complete circle ideas remains discuss estimate data. assume throughout section ﬁxed. suppose iid∼ \u0001-consistent estimator l—that exampleω could minimum hellinger distance estimator beran brevity suppress =αln al→k denote assignment induced proposition notation another phrase result suﬃciently large words single-linkage clustering yields clusters regular assignment thus wasserstein consistent estimate hellinger consistent estimates component measures applications often useful strengthen latter uniform convergence densities. families equicontinuous guaranteed theorem sweeting store corollary away future sections outlined assumptions necessary identify mixture model theorem regularity separation. example indicates conditions nontrivial fail hold practice. fortunately easy construct rich collection mixture models regular well-separated present here. example choosing obtain family mixtures gaussian mixtures i.e. mixture model whose atoms gaussian mixtures. particular atoms approximate distribution follows approximated mixture distribution long suﬃciently large. goal estimate data-generating distribution merely useful partition space meaningful clusters. evaluate quality clustering however assume generative model deﬁne optimal partition space using bayes’ rule. partition known bayes optimal partition. interest establish conditions bayes optimal partition learned data. application theory developed sections consider problem nonparametric setting. first recall classical framework model-based clustering discriminant analysis order motivate concepts detail. arguably classical model gaussian gmm. intuitively density distinct clusters given gaussian densities model course equivalent integral obviously gaussian densities replaced family parametric densities given samples iid∼ we’d like learn bayes optimal partition ﬁrst estimating mixture model identiﬁable bayes optimal partition well-deﬁned unique interpretation terms distinct clusters input space thus example deﬁnes unique partition euclidean space many classes parametric mixture models. since clustering literature full examples datasets well-approximated parametric mixtures however signiﬁcant interest extending concept bayes optimal partition nonparametric setting identiﬁability issue. theorem course oﬀers solution identiﬁability problem. remainder section apply abstract framework problem. first discuss identiﬁability issues concept bayes optimal partition then provide conditions bayes optimal partition learned data easy check simply uses bayes’ rule assign point cluster maximizes posterior probability. given samples mixture distribution wish learn bayes optimal partition unfortunately is—yet again—an identiﬁability issue mixture measure represents bayes optimal partition well-deﬁned. example example figure four valid representations mixture subgaussians. four cases representation leads diﬀerent bayes optimal partition even though represent mixture distribution. setting. discussed previously k-clustering equivalent function assigns integer number clusters. clearly exceptional function. thus bayes optimal partition interpreted k-clustering. classical model-based clustering notion always implicit however extended nonparametric setting. consistent estimator learn theorem guarantees learn cluster assignment ωαln) consistently πωαln)) approximate hope course πωαln)) although proposition implies well-deﬁned additional complications. mean convergence partitions? πωαln)) even converge? instead working directly partitions πωαln)) work partition writeg densities respectively vergence function gives convenient notion consistency problem. thus also compare sets alnk optimal sets particular we’d like show akalnk small uniform convergence assumption theorem seem strong however recall corollary guarantees uniform convergence whenever equicontinuous. thus example family gaussians example —which equicontinuous—it straightforward show following figure examples order left right. original mixture density depicted solid black line overﬁtted gaussian mixture components dotted blue line. individual component figure moons dataset. contour plot overﬁtted gaussian mixture approximation centers marked ◦’s. original data colour coded approximate bayes optimal partition. interpret theorem follows long take large enough diﬀerence true bayes optimal partition estimated partition becomes negligible. fact strengthen estimate overﬁtted components; deﬁne estimated assignment function using single-linkage clustering group components together; clustering deﬁne mixture components qkα); assess eﬀectiveness meta-algorithm practice evaluate performance simulated data. implemention used algorithm regularization weight clipping learn step moons version classical moons dataset two-dimensions unbalanced clusters. chosen classical failure case spectral clustering known diﬃculties unbalanced data. results shown figures cases choosing cluster tree produce components induced partitions provided sensible meaningful clusterings original data close true bayes optimal partition. proposed novel identiﬁability criterion nonparametric mixture models based notions regularity clusterability. instead making assumptions mixture components assumptions enforce separation criterion regarding components mixed. discussed estimate identiﬁable mixtures applied framework model-based clustering generalizing notion bayes optimal partition nonparametric settings.", "year": 2018}