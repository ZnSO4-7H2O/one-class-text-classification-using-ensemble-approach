{"title": "Learning Semantic Script Knowledge with Event Embeddings", "tag": ["cs.LG", "cs.AI", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "text": "induction common sense knowledge prototypical sequences events recently received much attention instead inducing knowledge form graphs much previous work method distributed representations event realizations computed based distributed representations predicates arguments representations used predict prototypical event orderings. parameters compositional process computing event representations ranking component model jointly estimated texts. show approach results substantial boost ordering performance respect previous methods. generally believed natural language understanding systems would beneﬁt incorporating common-sense knowledge prototypical sequences events participants. early work focused structured representations knowledge manual construction script knowledge bases. however approaches scale complex domains recently automatic induction script knowledge text started attract attention methods exploit either natural texts crowdsourced data consequently require expensive expert annotation. given text corpus extract structured representations example chains general directed acyclic graphs graphs scenario-speciﬁc nodes correspond events arcs encode temporal precedence relation. graphs used inform applications providing information whether event likely precede succeed another. work advocate constructing statistical model capable answering least questions graphs used answer without explicitly representing knowledge graph. method distributed representations event realizations computed based distributed representations predicates arguments event representations used ranker predict expected ordering events. parameters compositional process computing event representation ranking component model estimated data. order intuition embedding approach attractive consider situation prototypical ordering events disembarked passengers drove away needs predicted. approach based frequency predicate pairs unlikely make right prediction driving usually precedes disembarking. similarly approach treats whole predicate-argument structure atomic unit probably fail well sparse model unlikely effectively learnable even large amounts data. however embedding method would expected capture relevant features verb frames namely transitive predicate disembark effect particle away features used ranking component make correct prediction. previous work learning inference rules shown enforcing transitivity constraints inference rules results signiﬁcantly improved performance. true event ordering task scripts largely linear structure observing likely imply interestingly approach implicitly learn model satisﬁes transitivity constraints without need explicit global optimization graph. approach evaluated crowdsourced dataset regneri demonstrate using model results absolute improvement event ordering respect graph induction method learning exploiting distributed word representations shown beneﬁcial many applications representations encode semantic syntactic properties word normally learned language modeling setting though also specialized learning context applications tagging semantic role labeling recently area distributional compositional semantics started emerge focus inducing representations phrases learning compositional model. model would compute representation phrase starting embeddings individual words phrase often composition process recursive guided form syntactic structure. work simple compositional model representing semantics verb frame model shown figure word vocabulary mapped real vector based corresponding lemma hidden layer computed summing linearly transformed predicate argument embeddings passing logistic sigmoid function. different transformation matrices arguments predicates respectively. event representation obtained applying another linear transform followed another application sigmoid function. event ranking transformation parameters well representations words forced predictive temporal order events. however important characteristic neural network embeddings induced multitasking scenario consequently learned predictive different types contexts providing general framework inducing different aspects properties events well exploiting representations different applications. task learning stereotyped order events naturally corresponds standard ranking setting. here assume provided sequences events goal capture order. discuss obtain learning material next section. learn linear ranker takes event representation returns ranking score. events ordered according score yield model prediction. note learning stage estimate also event representation parameters i.e. matrices word embedding note casting event ordering task global ranking problem ensure model implicitly exploits transitivity temporal relation property crucial successful learning ﬁnite amount data argued introduction conﬁrm experiments. online ranking algorithm based perceptron rank accurately large-margin extension. crucial difference though error computed respect also propagated back structure neural network. learning procedure sketched algorithm additionally gaussian prior weights regularizing embedding parameters vector initialize word representations using evaluate approach crowdsourced data collected script induction regneri though principle method applicable arguably general setting chambers jurafsky regneri collected short textual descriptions various types human activities using crowdsourcing dataset also complemented descriptions provided omics corpus datasets fairly small containing esds activity type average collection easily extended given cost crowdsourcing. esds written bullet-point style annotators asked follow temporal order writing. consider example scenario prepare coffee coffee maker} {ﬁll water coffee maker} {place ﬁlter holder} {place coffee ﬁlter} {place holder coffee maker} {turn coffee maker} though individual esds seem simple learning task challenging limited amount training data variability used vocabulary optionality events different granularity events variability ordering unlike work regneri relies wordnet provide extra signal using multiple sequence alignment algorithm. work description preprocessed extract predicate heads argument noun phrases used model. model estimated explained section order events esds treated gold standard. used held-out scenarios choose model parameters scenario-speciﬁc tuning performed test scripts used perform model selection. references baroni marco zamparelli robert. nouns vectors adjectives matrices representing adjectiveproceedings noun constructions semantic space. emnlp experiments compared event embedding model three baseline systems bsmsa system regneri hierarchical bayesian system frermann chooses order events based preferred order corresponding verbs training predicted stereotypical order number times corresponding verbs appear order training esds exceeds number times appear opposite order coin tossed break ties also compare version model uses verbs note eeverbs conceptually similar essentially induces ordering verbs. however ordering beneﬁt implicit transitivity assumption used eeverbs discussed introduction. results presented table ﬁrst observation full model improves substantially baseline previous methods improvement largely increase recall precision negatively affected. also observe substantial improvement metrics using transitivity seen comparing results eeverb simple approach already outperforms pipelined system. results seem support hypothesis", "year": 2013}