{"title": "An Alternative Softmax Operator for Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.", "text": "ﬁrst operator non-expansion however ignores non-maximizing selections next operator mean computes average inputs. differentiable like operator takes ﬁxed convex combination inputs non-expansion. however allow maximization third operator eps\u0001 commonly referred epsilon greedy interpolates mean. operator non-expansion convex combination non-expansion operators. non-differentiable boltzmann operator boltzβ differentiable. also approximates mean however non-expansion therefore prone misbehavior shown next section. softmax operator applied values acts somewhat like maximization function somewhat like average. sequential decision making softmax often used settings necessary maximize utility also hedge problems arise putting one’s weight behind single maximum utility decision. boltzmann softmax operator commonly used softmax operator setting show operator prone misbehavior. work study differentiable softmax operator non-expansion ensuring convergent behavior learning planning. introduce variant sarsa algorithm that utilizing operator computes boltzmann policy state-dependent temperature parameter. show algorithm convergent performs favorably practice. fundamental tension decision making choosing action highest expected utility avoiding starving actions. issue exploration–exploitation arises context dilemma decision problems interpreting observed decisions reinforcement learning approach addressing tension softmax operators value-function optimization softmax policies action selection. examples include value-based methods sarsa expected sarsa policy-search methods reinforce following section provide simple example illustrating non-expansion property important especially context planning on-policy learning. present softmax operator non-expansion. prove several critical properties operator introduce softmax policy present empirical results. ﬁrst show boltzβ lead problematic behavior. sarsa boltzmann softmax policy shown figure edges labeled transition probability reward number also state terminal state consider action values namely recall boltzmann softmax policy assigns following probability action sarsa known converge tabular setting using \u0001-greedy exploration decreasing exploration region function-approximation setting also variants sarsa update rule converge generally however example ﬁrst knowledge show sarsa fails converge tabular setting boltzmann policy. next section provides background analysis example. markov decision process speciﬁed tuple sarp states actions. functions denote reward transition dynamics mdp. finally discount rate determines relative importance immediate reward opposed rewards received future. typical approach ﬁnding good policy estimate good particular state—the state value function. value particular state given policy initial action written deﬁne optimal value state–action pair maxπ possible deﬁne recursively function optimal value state–action pairs ﬁxed point. however boltzmann softmax operator boltzβ non-expansion note relate sarsa observing sarsa’s update stochastic implementation gvi’s update. boltzmann softmax policy target sarsa update following although known long time boltzmann operator non-expansion aware published example distinct ﬁxed points exist. presented figure ﬁrst example where shown figure boltzβ distinct ﬁxed points. also show figure vector ﬁeld visualizing updates boltzβ=.. updates move current estimates farther ﬁxed points. behavior sarsa results algorithm stochastically bouncing back forth ﬁxed points. learning algorithm performs sequence noisy updates moves ﬁxed point other. show later planning also progress extremely slowly near ﬁxed points. lack non-expansion property leads multiple ﬁxed points ultimately misbehavior learning planning. mellowmax includes parameter settings allow maximization well minimization. particular goes inﬁnity acts like max. |{xi n}}|. note number maximum values then figure vector ﬁeld showing updates boltzβ=.. fixed points marked black. points large blue point updates move current estimates farther ﬁxed points. also points ﬁxed-points progress extremely slow. derived information theoretical principles regularizing policies cost function deﬁned divergence note operator previously utilized areas power engineering prove non-expansion therefore sarsa guaranteed converge unique ﬁxed point. vectors values. difference components vectors. also index maximum component-wise difference argmaxi simplicity assume unique also without loss generality assume follows that equations together linear constraints form equations constrain variables lagrangian multipliers solving system equations probability taking action maximum entropy mellowmax policy form argument existence unique root simple. term corresponding best action dominates function positive. conversely term corresponding action lowest utility dominates function negative. finally taking derivative clear function monotonically increasing allowing conclude exists single root. therefore easily using root-ﬁnding algorithm. particular brent’s method available numpy library python. policy form boltzmann softmax parameter whose value depends indirectly mathematical form arose structure maximizing entropy. view mellowmax operator then form boltzmann policy temperature parameter chosen adaptively state ensure non-expansion property holds. described computes value list numbers somewhere minimum maximum. however often useful actually provide probability distribution actions non-zero probability mass assigned action resulting expected value equals computed value. probability distribution used action selection algorithms sarsa. section address problem identifying probability distribution maximum entropy problem—over distributions satisfy properties above information entropy formally deﬁne maximum entropy mellowmax policy state ﬁrst constraint convex optimization problem mellowmax non-expansion sarsa maximum entropy mellowmax policy guaranteed converge unique ﬁxed point. note also that similar variants sarsa algorithm simply bootstraps using value next state implementing policy. observed practice computing mellowmax yield overﬂow exponentiated values large. case safely shift values constant exponentiating following equality value maxi usually avoids overﬂow. repeat experiment figure mellowmax vector ﬁeld. result presented figure show rapid steady convergence towards unique ﬁxed point. result terminate signiﬁcantly faster boltzβ illustrated figure ﬁrst present investigates behavior experiment softmax operators randomly generated mdps. second experiment evaluates softmax policies used sarsa tabular representation. last example figure created carefully hand. interesting know whether examples likely encountered naturally. constructed mdps follows sampled uniformly random. initialized transition probabilities sampling uniformly added entry probability gaussian noise mean variance next added probability gaussian noise mean variance finally normalized values ensure transition matrix. similar process rewards difference divided entry maximum entry multiplied ensure rmax measured failure rate boltzβ stopping terminate iterations. also computed average number iterations needed termination. summary results presented table below. mellowmax outperforms boltzmann based three measures provided below. challenging aspect domain admits many locally optimal policies. exploration needs carefully avoid either over-exploring under-exploring state space. note also boltzmann softmax performs remarkably well outperforming sophisticated bayesian domain figure multi-passenger taxi domain. discount rate reward delivering passenger passengers three passengers. reward zero transitions. denote passengers start state destination respectively. reinforcement-learning algorithms shown figure sarsa epsilon-greedy policy performs poorly. fact experiment algorithm rarely able deliver passengers. however sarsa boltzmann softmax sarsa maximum entropy mellowmax policy achieved signiﬁcantly higher average reward. maximum entropy mellowmax policy worse boltzmann softmax here suggesting greater stability come expense less effective exploration. section evaluate maximum entropy mellowmax policy context policy-gradient algorithm. speciﬁcally represent policy neural network maps states probabilities actions. common choice activation function last layer boltzmann softmax policy. contrast maximum entropy mellowmax policy presented section treating inputs activation function values. used lunar lander domain openai benchmark. screenshot domain presented figure domain continuous state space dimensions namely coordinates velocities angle angular velocities leg-touchdown sensors. discrete actions control engines. reward safe landing designated area crash. small shaping reward approaching landing area. using engines results negative reward. episode ﬁnishes spacecraft crashes lands. solving domain deﬁned maintaining mean episode return higher consecutive episodes. policy experiment represented neural network hidden layer comprised units relu activation functions followed second layer units softmax activation functions. used reinforce train network. batch episode size figure comparison multi-passenger taxi domain. results shown different values setting learning rate optimized. results averaged independent runs consisting time steps. used stability issues smaller episode batch sizes. used adam algorithm parameters suggested paper. used keras theano implement neural network architecture. softmax policy present figure learning curves different values free parameter. plot average return episodes. mellowmax outperforms boltzmann peak. algorithms inverse reinforcement learning problem inferring reward functions observed behavior frequently boltzmann operator avoid assigning zero probability non-optimal actions hence assessing observed sequence impossible. methods include bayesian natural gradient maximum likelihood given recursive nature value deﬁned problems mellowmax could stable efﬁcient choice. linearly solvable mdps operator similar mellowmax emerges using alternative characterization cost action selection mdps. inspired work introduced off-policy g-learning algorithm uses operator perform value-function updates. instead performing off-policy updates introduced convergent variant sarsa boltzmann policy state-dependent temperature parameter. contrast epsilon greedy behavior policy used. proposed mellowmax operator alternative boltzmann softmax operator. showed mellowmax several desirable properties works favorably practice. arguably mellowmax could used place boltzmann throughout reinforcement-learning research. analyze ﬁxed point future direction reinforcement-learning game-playing planning algorithms using mellowmax operators. particular interesting analysis could bounds sub-optimality ﬁxed points found gvi. important future work expand scope theoretical understanding general function approximation setting state space action space large abstraction techniques used. note importance non-expansion function approximation case well-established. balance exploration exploitation decision rules based epsilon-greedy boltzmann softmax simple often perform surprisingly well practice even outperforming advanced exploration techniques require signiﬁcant approximation complex domains. learning policy exploration steps perhaps become part value-estimation process itself. on-policy algorithms like sarsa made converge optimal behavior limit exploration rate update operator gradually moved toward softmax learning updates reﬂects point view shows value-sensitive behavior boltzmann exploration maintained even updates made stable. analyses behavior human subjects choice experiments frequently softmax. sometimes referred literature logit choice forms important part accurate predictor human decisions normal-form games quantal level-k reasoning softmax-based ﬁxed points play crucial role work. such mellowmax could potentially make good replacement. references babes monica marivate vukosi littman michael subramanian kaushik. apprenticeship learning international conference multiple intentions. machine learning pakman tishby naftali. taming noise reinforcement learning soft updates. proceedings thirty-second conference uncertainty artiﬁcial intelligence auai press generalized reinforcement-learning model convergence applications. saitta lorenza proceedings thirteenth international conference machine learning littman michael lederman. algorithms sequential decision making. thesis department computer science brown university february also technical report cs--. singh satinder jaakkola tommi littman michael szepesv´ari csaba. convergence results single-step on-policy reinforcement-learning algorithms. machine learning integrated architectures learning planning reacting based approximating dynamic seventh programming. international conference machine learning austin morgan kaufmann. team theano development al-rfou rami alain guillaume almahairi amjad angermueller christof bahdanau dzmitry ballas nicolas bastien fr´ed´eric bayer justin belikov anatoly theano python framework fast computation mathematical expressions. arxiv preprint arxiv. thrun sebastian role exploration learning white david sofge donald control. handbook intelligent control neural fuzzy adaptive approaches nostrand reinhold york seijen harm hasselt hado whiteson shimon theoretical empirical wiering marco. ieee symposium analysis expected sarsa. adaptive dynamic programming reinforcement learning ieee", "year": 2016}