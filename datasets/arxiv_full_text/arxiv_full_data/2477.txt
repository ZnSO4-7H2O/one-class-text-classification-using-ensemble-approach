{"title": "Algorithm Runtime Prediction: Methods & Evaluation", "tag": ["cs.AI", "cs.LG", "cs.PF", "stat.ML", "68T20", "I.2.8; I.2.6"], "abstract": "Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.", "text": "perhaps surprisingly possible predict long algorithm take previously unseen input using machine learning techniques build model algorithm’s runtime function problem-speciﬁc instance features. models important applications algorithm analysis portfolio-based algorithm selection automatic conﬁguration parameterized algorithms. past decade wide variety techniques studied building models. here describe extensions improvements existing models families models and— perhaps importantly—a much thorough treatment algorithm parameters model inputs. also comprehensively describe existing features predicting algorithm runtime propositional satisﬁability travelling salesperson mixed integer programming problems. evaluate innovations largest empirical analysis kind comparing wide range runtime modelling techniques literature. experiments consider algorithms instance distributions; also span wide range instances least structured generated uniformly random structured emerged real industrial applications. overall demonstrate models yield substantially better runtime predictions previous approaches terms generalization problem instances algorithms parameterized space simultaneously. keywords supervised machine learning performance prediction empirical performance models response surface models highly parameterized algorithms propositional satisﬁability mixed integer programming travelling salesperson problem np-complete problems ubiquitous luckily problems hard solve worst-case inputs often feasible solve even large problem instances arise practice. less luckily state-of-the-art algorithms often exhibit extreme runtime variation across instances realistic distributions even problem size held constant conversely instance take dramatically different amounts time solve depending algorithm used little theoretical understanding causes variation. past decade considerable body work shown supervised machine learning methods build regression models provide approximate answers question based given algorithm performance data; survey work section article refer models empirical performance models models useful variety practical contexts algorithm selection. classic problem selecting best given algorithms per-instance basis successfully addressed using epms predict performance candidate algorithms selecting predicted perform best parameter tuning algorithm conﬁguration. epms useful problems least ways. first model performance parameterized algorithm dependent settings parameters; sequential model-based optimization process alternates learning using identify promising settings evaluate next second epms model algorithm performance dependent problem instance features algorithm parameter settings; models used select parameter settings good predicted performance per-instance basis generating hard benchmarks. algorithms used parameters existing benchmark generators order create instances hard algorithms question gaining insights instance hardness algorithm performance. epms used assess instance features algorithm parameter values impact empirical performance. models support assessments directly models generic feature selection methods forward selection used identify small number model inputs explain algorithm performance almost well whole inputs applications motivate work following discuss detail; instead focus models themselves. idea modelling algorithm runtime longer new; however made substantial recent progress making runtime prediction methods general scalable accurate. review past work runtime prediction methods used work describe four contributions. work aiming gain insights instance hardness beyond worst case used term empirical hardness model similar regression models also used predict objectives runtime; examples include algorithm’s success probability solution quality optimization algorithm achieves ﬁxed time approximation ratio greedy local search competition scoring function reﬂect broadened scope using term epms understand umbrella includes ehms. introduce instance features propositional satisﬁability travelling salesperson mixed integer programming problems—in particular novel probing features timing features—yielding comprehensive sets features respectively assess impact advances determine current state performed believe comprehensive evaluation runtime prediction methods date. speciﬁcally evaluated methods aware performance data algorithms instance distributions spanning considering three different problems predicting runtime novel instances novel parameter conﬁgurations novel instances conﬁgurations techniques statistical literature survival analysis offer ways better handle data runs terminated prematurely. techniques used previous work—leading omit comparison above—we show leverage achieve improvements best-performing model random forests problems considered substantially different communities separately consider related work predicting runtime parameterless parameterized algorithms applications predictions gain insights instance hardness algorithm parameters. statistical regression methods runtime prediction roots range different communities dates back least mid-s. parallel computing literature brewer used linear regression models predict runtime different implementations portable high-level libraries multiprocessors aiming automatically select best implementation novel architecture planning literature fink used linear regression predict performance three planning algorithms depends problem size used predictions deciding algorithm long. community howe co-authors used linear regression predict planner’s runtime probability success depend various features planning problem; also applied predictions decide per-instance basis ﬁnite algorithms order optimize performance objective used early versions modeling techniques described section well extensions censored data described section recent conference workshop publications algorithm conﬁguration article ﬁrst comprehensively evaluate quality models. expected runtime. speciﬁcally constructed portfolio planners ordered algorithms expected success probability divided expected runtime. constraint programming literature leyton-brown studied winner determination problem combinatorial auctions showed accurate runtime predictions could made several different solvers wide variety instance distributions. work considered variety different regression methods settled relatively simpler method ridge regression preprocessing select appropriate feature subset quadratic basis function expansion log-transformation response variable. problem-independent runtime modelling techniques work subsequently applied problem leading successful portfolio-based algorithm selection method satzilla recently machine learning community huang applied linear regression techniques modeling algorithms low-order polynomial runtimes. extreme runtime variation often exhibited algorithms solving combinatorial problems common practice terminate unsuccessful runs exceed so-called captime. capped runs yield lower bound algorithm runtime typically treated succeeded captime. fink ﬁrst handle right-censored data points soundly runtime predictions planning methods used resulting predictions compute captimes maximize given utility function. gagliolo made connection statistical literature survival analysis handle right-censored data work dynamic algorithm portfolios. subsequently similar techniques used satzilla’s runtime predictions model-based algorithm conﬁguration recently smith-miles published series papers learning-based approaches characterizing instance hardness wide variety hard combinatorial problems work considered range tasks including performance prediction also clustering classiﬁcation easy hard instances well visualization. context performance prediction focus article work known neural network models. also recently kotthoff compared regression classiﬁcation ranking algorithms algorithm selection showed choice matters poor regression classiﬁcation methods yielded worse performance single best solver good methods yielded better performance. several veins performance prediction research deserve mention. haim walsh extended linear methods problem making online estimates solver runtimes. several researchers applied supervised classiﬁcation select fastest algorithm problem instance judge whether particular randomized algorithm would good machine learning community meta-learning aims predict accuracy learning algorithms meta-level control anytime algorithms computes estimates algorithm’s performance order decide stop solution found algorithm scheduling parallel distributed systems long relied low-level performance predictions example based source code analysis research aimed identify single quantities correlate algorithm’s runtime. famous early example clauses-to-variables ratio uniformrandom -sat earlier still knuth showed random probes search tree estimate size subsequent work reﬁned approach incorporated predictors features work therefore evaluate separately. literature search space analysis proposed variety quantities correlated runtimes local search algorithms. prominent examples include ﬁtness distance correlation autocorrelation length exception included measures feature sets computing quite expensive. principle particularly harder predict runtimes parameterized algorithms runtimes parameterless cousins parameters treated additional inputs model model learned standard way. past work pursued precisely approach using linear regression models exact gaussian processes model dependency runtime instance features algorithm parameter values however direct application methods designed parameterless algorithms effective small numbers continuous-valued parameters considered parameters). different methods appropriate algorithm’s parameter space becomes large. particular careful sampling strategy must used making necessary consider issues raised statistics literature experimental design. separately models must adjusted deal categorical parameters parameters ﬁnite unordered domains experimental design literature uses term response surface model refer predictor output process controllable input parameters generalize observed data unobserved parameter settings rsms core sequential model-based optimization methods blackbox functions recently adapted applications automated parameter tuning algorithm conﬁguration literature rsms algorithm performance limited consideration algorithms running single problem instances algorithms continuous input parameters. aware papers beyond relax assumptions. bartz-beielstein markon support categorical algorithm parameters existing methods consider predictions across different instances parameter settings. first ridge kudenko applied analysis variance approach detect important parameters using linear quadratic models. second chiarandini goegebeur noted constrast algorithm parameters instance characteristics cannot controlled treated so-called random effects. resulting mixed-effects models linear like ridge kudenko’s anova model assume gaussian performance distributions. note normality assumption much realistic context predicting solution quality local search algorithms context algorithm runtime prediction problem tackle here. hardness algorithm parameters leyton-brown co-authors employed forward selection linear regression models determine small sets instance features sufﬁce yield highquality predictions ﬁnding often little features yielded predictions good full feature set. hutter extended work predictions joint space instance features algorithm parameters using arbitrary models. model-speciﬁc approaches joint identiﬁcation instance features algorithm parameters anova approach ridge kudenko mixed-effects model chiarandini goegebeur mentioned previously. approaches quantifying parameter importance include entropy-based measure visualization methods interactive parameter exploration deﬁne different machine learning methods used predict algorithm runtimes ridge regression neural networks gaussian process regression regression trees section provides basis experimental evaluation different methods sections thus also discuss implementation details. describe problem instance list features drawn given feature space features must computable piece problem-speciﬁc code efﬁciently extracts characteristics given problem instance deﬁne conﬁguration space parameterized algorithm parameters respective domains subset cross-product parameter domains ×···× elements complete instantiations algorithm’s parameters refer conﬁgurations. taken together conﬁguration feature spaces deﬁne input space denote space probability distributions real numbers; real numbers represent algorithm performance measure runtime seconds reference machine. given algorithm conﬁguration space distribution instances feature space stochastic process deﬁnes probability distribution performance measures combination parameter conﬁguration problem instance features prediction entire distribution allows assess model’s conﬁdence particular input essential e.g. model-based algorithm conﬁguration nevertheless since many methods review yield point-valued runtime predictions experimental analysis focuses accuracy mean predicted runtimes. models deﬁne predictive distribution study accuracy conﬁdence values separately online appendix qualitatively similar results mean predictions. construct algorithm conﬁguration space instance various combinations conﬁgurations instances record resulting performance values record k-dimensional parameter conﬁguration m-dimensional feature vector instance used combine form m-dimensional vector predictor variables training data regression models simply denote matrix containing vector performance values various transformations make data easier model. article focus runtime performance measure log-transformation thus effectively predicting runtime. experience found transformation important large variation runtimes hard combinatorial problems. also transformed predictor variables discarding input dimensions constant across training data points normalizing remaining ones mean standard deviation instances certain feature values missing timeouts crashes undeﬁned missing values occur relatively rarely simple mechanism handling them. disregard missing values purposes normalization zero training models. means missing feature values effectively assumed equal mean respective distribution thus minimally informative. models mechanism leads ignore missing features since weight multiplied zero. modeling methods discussed paper free hyperparameters minimizing loss function cross-validation error. point hyper-parameters well default setting discussing methods. while best knowledge previous work runtime prediction used ﬁxed default hyperparameters also experimented optimizing every method experiments. purpose used gradient-free optimizer direct minimize -fold cross-validated root mean squared error ridge regression simple regression method linear function inputs simplicity interpretability combined competitive predictive performance scenarios studied method used frequently past building epms ridge regression works follows. deﬁned above identity matrix small constant. then compute weight vector given feature vector ridge regression predicts wtxn+. observe recover standard linear regression. effect regularize model penalizing large coefﬁcients equivalent gaussian prior favouring small coefﬁcients bayesian model beneﬁcial side effect regularization numerical stability improves common case rank deﬁcient nearly computational bottleneck ridge regression input dimensions inversion matrix requires time cubic algorithm runtime often better approximated polynomial function linear holds runtimes. reason make sense perform basis function expansion create features products original features. light resulting increase number features quadratic expansion particularly appealing. formally augment model input pairwise product inputs even ridge regularization generalization performance linear regression deteriorate inputs uninformative highly correlated others; experience difﬁcult construct sets instance features suffer problems. instead reduce input features performing feature selection. many different methods exist feature expansion selection; review different ridge regression variants recent literature differ design decisions. also considered third ridge regression variant originally proposed leyton-brown unfortunately running method computationally infeasible considering large number features consider paper forcing approximate method nevertheless preventing performing -fold cross-validation. hurdles made impossible fairly compare rr-el methods discuss rr-el here. however completeness online appendix includes deﬁnition approximation rr-el experimental results showing perform worse ridge regression variant cases. half decade used simple scalable feature selection method based forward selection build regression models used satzilla iterative method starts empty input greedily adds linear input time minimize cross-validation error step stops linear inputs selected. performs full quadratic expansion linear features finally carries another forward selection expanded feature starting empty input stopping features selected. reason two-phase approach scalability method prevents ever perform full quadratic expansion features. implementation reduces computational complexity forward selection exploiting fact inverse matrix resulting including additional feature computed incrementally rank-one updates previous inverse matrix requiring quadratic time rather cubic time recently huang described method predicting algorithm runtime called sparse polynomial regression based ridge regression forward-backward feature selection. huang concluded spore-foba outperforms lasso regression consistent comparison lasso leyton-brown contrast variants above spore-foba employs cubic feature expansion essentially performs single pass forward selection step adding small terms determined forward-backward phase feature’s candidate set. speciﬁcally already selected terms based features spore-foba loops features constructing candidate consists polynomial expansions include non-zero degree whose total degree bounded candidate forward-backward phase iteratively adds best term reduction root mean squared error exceeds threshold removes worst term reduction rmse phase terminates single term added reduce rmse finally spore-foba’s outer forward selection loop chooses terms resulting best forward-backward phases iterates number terms reach prespeciﬁed maximum tmax terms. experiments used original spore-foba code; free parameters ridge penalizer tmax defaults tmax neural networks well-known regression method inspired information processing human brain. multilayer perceptron particularly popular type neural network organizes single computational units layers using outputs units layer inputs units next layer. neuron hidden output layers inputs associated weight term vector bias term computes function neurons hidden layer result function propagated nonlinear activation function given input network single hidden layer neurons single output neuron computes output smith-miles hemert used hidden layer neurons predict runtime local search algorithms solving timetabling instances. used proprietary neural network software neuroshell advised compare off-the-shelf matlab implementation instead. thus employed popular matlab neural network package netlab netlab uses activation function tanh supports regularizing prior keep weights small minimizing error metric −yi) +αwtw parameter determining strength prior. experiments used netlab’s default optimizer minimize error metric stopping optimization default steps. free parameters regularization factor number hidden neurons used netlab’s default like smith-miles hemert stochastic gaussian processes popular class regression models roots geostatistics also called kriging models dominant modern approach building response surface models ﬁrst applied runtime prediction hutter found yield better results ridge regression albeit greater computational expense. construct regression model ﬁrst need select kernel function characterizing degree similarity pairs elements input space variety kernel functions possible common choice continuous inputs squared exponential kernel kernel parameters. based idea correlations decrease weighted euclidean distance input space general kernel deﬁnes prior distribution type functions expect. distribution takes form gaussian stochastic process collection random variables ﬁnite subset joint gaussian distribution. remains speciﬁed tradeoff strength prior ﬁtting observed data specifying observation noise. standard assume normally distributed observation noise mean zero variance like kernel parameters optimized improve combining prior speciﬁed training data yields posterior distribution input point derivation) equations assume ﬁxed kernel parameters ﬁxed observation noise variance constitute gp’s hyperparameters. contrast hyperparameters models number hyperparameters grows input dimensionality optimization integral part ﬁtting typically maximizing marginal likelihood data gradient-based optimizer details). choice optimizer make difference practice; used minfunc implementation limited-memory version bfgs learning model data computationally expensive. inverting matrix takes time done every hyperparameter optimization steps yielding total complexity subsequent predictions input require time mean variance respectively. regression trees simple tree-based regression models. known handle discrete inputs well; ﬁrst application prediction algorithm performance bartz-beielstein markon leaf nodes regression trees partition input space disjoint regions simple model prediction region common choice predict constant leads following prediction input point indicator function takes value proposition true otherwise. note since regions partition input space always involve exactly non-zero term. denote subset training data points region error-minimizing choice constant region sample mean data points construct regression tree following standard recursive procedure starts root tree available training data points consider binary partitionings given node’s data along split variables split points real-valued split variable scalar data point assigned region region otherwise. categorical split variable data point assigned region region otherwise. node select split variable split point minimize squared differences regions’ means chosen according equation sample means regions respectively. continue procedure recursively ﬁnding best split variable split point partitioning data child nodes recursing child nodes. process terminates training data points node share values meaning splits possible. procedure tends overﬁt data mitigated recursively pruning away branches contribute little model’s predictive accuracy. cost-complexity pruning -fold cross-validation identify best tradeoff complexity predictive quality; book hastie details. order predict response value input propagate tree node split variable split point continue left child node right child node otherwise. predictive mean constant leaf process selects; variance predictor. implemented efﬁciently computational cost ﬁtting regression tree small. single node data points dimensionality takes time identify best combination split variable point continuous split variable sort values consider possible split points different values. procedure categorical split variables complexity consider variable’s categorical values compute score mean across node’s data points sort scores consider binary partitions consecutive scores set. squared error loss function computation performed amortized time split points total time required determining best split point single variable complexity building regression tree depends balanced worst case data point split time leading best case—a balanced tree—we recurrence relation leading complexity experience trees perfectly balanced much closer best case worst case. example data points typically tree depths prediction regression trees cheap; merely need propagate query points tree. node continuous split variable split point need compare xn+j operation. categorical split variables store mask values enable member queries. worst case prediction thus takes time best case takes time. section extend existing modeling techniques epms primary goal improving runtime predictions highly parameterized algorithms. methods described draw advanced machine learning techniques best knowledge work ﬁrst applied algorithm performance prediction. speciﬁcally show extend models handle categorical inputs describe model families well-suited modeling performance highly parameterized algorithms based potentially large amounts data projected process approximation gaussian processes random forests regression trees. empirical performance models historically limited continuous-valued inputs; approach used performance predictions based discrete-valued inputs regression trees section ﬁrst present standard method encoding categorical parameters real-valued parameters present kernel handling categorical inputs directly gaussian processes. standard solution extending arbitrary modeling techniques handle categorical inputs so-called -in-k encoding scheme encodes categorical inputs ﬁnite domain size binary inputs. speciﬁcally column design matrix categorical domain replace |di| binary indicator columns column corresponding contains values data point exactly columns rest transformation columns treated exactly like original real-valued columns arbitrary modeling techniques numerical inputs become applicable. problem -in-k encoding using increases size input space considerably causing regression methods perform poorly. deﬁne kernel handling categorical inputs directly. kernel similar standard squared exponential kernel equation instead measuring squared distance computes hamming distance although kmixed straightforward adaptation standard kernel equation aware prior kernel regression show positive deﬁnite. deﬁnition function positive deﬁnite kernel symmetric pair inputs satisﬁes positive deﬁnite inputs proposition combination continuous categorical input dimensions pcont pcat kmixed positive deﬁnite kernel function. appendix online appendix provides proof shows kmixed constructed simpler positive deﬁnite functions uses facts space positive deﬁnite kernel functions closed addition multiplication. kernel understood implicitly performing -in-k encoding. note kernel kmixed hyperparameter input dimension. using -ink encoding kernel kcont instead hyperparameter encoded dimension; reparameterize kcont share single hyperparameter since kmixed rather expressive worry overﬁtting. thus also experimented variations sharing hyperparameter across input dimensions; sharing across algorithm parameters across instance features. found neither variation outperformed kmixed. time complexity ﬁtting gaussian processes cubic number data points limits amount data used practice models. deal obstacle machine learning literature proposed various approximations gaussian processes best knowledge approximate previously applied runtime prediction work parameter optimization experimented bayesian committee machine informative vector machine projected process approximation methods performed similarly approximation slight edge. below give equations pp’s predictive mean variance; derivation rasmussen williams approximation uses subset training data points so-called active set. vector consisting indices data points. extend notation exact follows denote matrix denote matrix xj). predictive distribution approximation normal distribution mean variance perform steps hyperparameter optimization based standard trained using data points sampled uniformly random without replacement input data points. resulting hyperparameters another independently sampled data points subsequent approximation. cases data points. complexity approximation superlinear therefore approach much faster choose hyperparameter optimization based data points takes time addition one-time cost evaluating equations. thus time complexity ﬁtting approximate model compared exact model. time complexity predictions approximation mean variance predictive distribution compared respectively exact version. experiments achieve good compromise speed predictive accuracy. sensitive small changes data thus prone overﬁtting. random forests overcome problem combining multiple regression trees ensemble. known strong predictions high-dimensional discrete input data random forests obvious choice runtime predictions highly parameterized algorithms. nevertheless best knowledge used algorithm runtime prediction except recent work algorithm conﬁguration used prototype implementation models describe here. following describe standard framework nonstandard implementation choices made. random forest consists regression trees. grown sufﬁcient depths regression trees extraordinarily ﬂexible predictors able capture complex interactions thus bias. however means also high variance small changes data lead dramatically different tree. random forests reduce variance aggregating predictions across multiple different trees. trees made different training different subsamples training data and/or permitting random subset variables split variables node. chose latter option using full training tree. mean predictions input trivial predict response tree average predictions. predictive quality improves number trees grows computational cost also grows linearly used throughout experiments keep computational costs low. random forests additional hyperparameters percentage variables consider split point perc minimal number data points required node make eligible split further nmin. perc nmin default. introduce simple effective method quantifying predictive uncertainty random forests. recently introduced quantile regression trees allow predictions quantiles predictive distribution; contrast predict mean variance.) leaf regression tree addition empirical mean training data associated leaf store empirical variance data. avoid making deterministic predictions leaves data points round stored variance least throughout. input regression constant tree thus yields predictive mean predictive variance combine estimates single estimate treat forest mixture model different thus predicted mean simply mean across means predicted individual trees random forest. compute variance prediction used total variance allows write total variance variance across means predicted individual trees plus average variance tree second non-standard ingredient models concerns choice split points. consider splits real-valued variable note loss equation minimized choosing split point values still free choose exact location anywhere interval traditionally chosen midpoint xlj. instead draw uniformly random limit inﬁnite number trees leads linear interpolation training data instead partition regions constant prediction. furthermore causes variance estimates vary smoothly grow distance observed data points. computational cost ﬁtting random forest relatively low. need regression trees somewhat easier normal regression tree since node consider maxperc possible split variables. building trees simply takes times long building single tree. thus—by argument regression trees—the complexity learning random forest worst case best case random forest implementation based port matlab’s regression tree code yielded speedups orders magnitude. methods discussed could used model performance algorithm solving problem experiments investigated speciﬁc np-complete problems. particular considered propositional satisﬁability problem mixed integer programming problems travelling salesperson problem reasons choosing three problems follows. prototypical np-hard decision problem thus interesting theory perspective; modern solvers also prominent approaches hardware software veriﬁcation canonical representation constrained optimization problems integer-valued continuous variables serves unifying framework np-complete problems combines expressive power integrality constraints efﬁciency continuous optimization. consequence widely used academia industry finally widely studied np-hard optimization problems also considerable interest industry tailor epms particular problem choice instance features. describe comprehensive sets features tsp. problems summarize sets features found literature introduce many novel features. features polynomial-time computable note computationally expensive large instances applications expensive features reasonable—in particular note applications take features one-time input build models repeatedly even make sense features whose cost exceeds solving instance; examples applications include model-based algorithm conﬁguration complex empirical analyses based performance predictions runtime-sensitive applications hand make sense subset features described here. facilitate this categorize features four cost classes trivial cheap moderate expensive. experimental evaluation report empirical cost feature classes predictive performance achieved using also identify features introduced work quantify contributions model performance. probing features generic family features deserves special mention. computed brieﬂy running existing algorithm given problem given instance extracting characteristics algorithm’s trajectory—an idea closely related landmarking meta-learning probing features deﬁned little effort wide variety problems; indeed earlier work features unavailable np-complete problem interest alternative reduce problem tsp—a polynomial-time operation—and compute features describe here. expect approach computationally efﬁcient observe extends reach existing construction techniques np-complete problem. introduced ﬁrst probing features showed probing features based type algorithm often useful predicting performance another type algorithm introduce ﬁrst probing features tsp. another generic family features timing features measure time groups features take compute. code binaries computing features along documentation providing additional details available online http//www.cs.ubc.ca/labs/beta/projects/epms/. figure summarizes features sat. since various preprocessing techniques routinely used applying general-purpose solver typically lead substantial reductions instance size difﬁculty apply preprocessing procedure satelite instances ﬁrst compute instance features preprocessed instances. ﬁrst features exception features introduced previously published work satzilla categorized problem size features graph-based features balance features proximity horn formula features dpll probing features lp-based features local search probing features features fall four categories. first added additional subgroups graph-based features. diameter features based variable graph node graph compute longest shortest path node. features follow compute various statistics vector state exact statistics vector list figure clustering coefﬁcient features measure local cliqueness clause graph. node clause graph denote number edges present node neighbours denote maximum possible number edges; compute node. second clause learning features based statistics gathered -second runs zchaff rand measure number learned clauses length learned clauses every search steps. third survey propagation features based estimates variable bias formula obtained using probabilistic inference used varsat’s implementation estimate probabilities variable true every satisfying assignment false every satisfying assignment unconstrained. features measure conﬁdence survey propagation /pfalse pfalse/ptrue) variable features based punconstrained vector. finally timing features measure time taken different blocks feature computation code instance preprocessing satelite problem size variable-clause graph balance features variable-clause graph variable graph proximity horn formula features diameter-based features clause graph features unit propagation features search space size estimation lp-based features local search probing features saps gsat; clause learning features survey propagation features figure summarizes features mixed integer programs include features based existing work probing features timing features. features primarily based features combinatorial winner determination problem past work generalized previously described ph.d. thesis features categorized problem type size features variable-constraint graph features linear constraint matrix features objective function features lp-based features also integrated ideas feature used kadioglu computation separate statistics continuous variables non-continuous variables union). extended existing features adding richer statistics applicable medians variation coefﬁcients percentile ratios vector-based features. introduce sets features. firstly probing features based -second runs cplex default settings. obtained cplex include presolving features based output cplex’s presolving phase probing usage features describing different cuts cplex used probing probing result features summarizing probing runs secondly timing features capture time required computing different groups features variable-constraint graph linear constraint matrix objective features three subsets variables lp-based features cplex probing features cost computing remaining features small figure summarizes features travelling salesperson problem features features introduced smith-miles features capture spatial distribution nodes clustering nodes used authors’ code compute features. features follows. problem size feature number nodes given tsp. cost matrix features statistics cost nodes. minimum spanning tree features based constructing minimum spanning tree nodes features statistics edge costs tree features based node degrees. cluster distance features based cluster distance every pair nodes minimum bottleneck cost path them; here size mean median vector composed following values bounded variables domain size binary/integer semi-continuous +domain size semi-integer variables. figure instance features; variable-constraint graph linear constraint matrix objective function features feature computed respect three subsets variables continuous non-continuous features introduced ﬁrst time marked solution probing percentage integer values non-integer values ﬁnal solution probing. non-integer values compute statics across nodes minmax quantiles bottleneck cost path deﬁned largest cost along path. local search probing features based short runs using implementation available speciﬁcally features based tour length obtained features based tour length local minima tour quality improvement search step number search steps reach local minimum respectively; features measure hamming distance local minima; features describe probability edges appearing local minimum encountered probing. branch probing features based -second runs concorde. speciﬁcally features measure improvement lower bound cut; feature ratio upper lower bound probing run; features analyze ﬁnal solution. feature autocorrelation coefﬁcient measure ruggedness search landscape based uninformed random walk finally timing features measure time required computing feature groups feed-forward neural network hidden layer projected process regression tree cost-complexity pruning random forest study performance models described sections using features described section section consider problem considered past work predicting performance achieved default conﬁguration given algorithm instances. brevity present representative empirical results. full results experiments available online appendix http//www.cs.ubc.ca/labs/beta/projects/epms. data features source code replicating experiments available site. used wide range instance distributions indu hand rand collections industrial handmade random instances international competitions races competition union; sets software hardware veriﬁcation instances swv-ibm union; randsat subset rand containing satisﬁable instances. give details distributions appendix distributions except randsat popular tree search solver minisat indu also additional solvers cryptominisat spear finally evaluate predictions local search algorithms used randsat instances considered solvers dynamic local search algorithm saps used instance distributions computational sustainability winner determination combinatorial auctions unions large diverse publicly available instances details distributions given appendix used state-of-the-art commercial solvers cplex gurobi strongest non-commercial solvers scip solve tsplib heterogeneous prominent instances. instance sets state-of-the-art systematic local search algorithms concorde lk-h latter computed runtimes time required optimal solution. collect algorithm runtime data algorithm–distribution pair executed algorithm using default parameters instances distribution measured runtimes collected results database. algorithm runs executed cluster dual .ghz intel xeon cache running opensuse linux runtimes measured time reference machines. terminated algorithm hour; gave rise capped runtime observations terminated fashion observed lower bound runtime. like past work runtime modeling simply counted capped runs taken hour. basic statistics resulting runtime distributions given table table online appendix lists details. evaluated different model families building models subset data assessing performance data used train models. done visually quantitatively. considered three complementary quantitative metrics evaluate mean predictions predictive variances given true performance vali=; ¯y)/ denote sample mean standard deviation denotes probability density function standard normal distribution. intuitively probability observing true values predicted distributions higher values better rmse lower values better. used -fold cross-validation report means measures across folds. assessed statistical signiﬁcance ﬁndings using wilcoxon signed-rank test table provides quantitative results benchmarks figure visualizes results. broadest level conclude methods able capture enough algorithm performance training data make meaningful predictions test data time easy instances tended predicted easy hard ones hard. take example case predicting runtime minisat heterogeneous competition instances minisat runtimes varied almost orders magnitude predictions better models rarely order magnitude note large rmse values ridge regression data sets large errors extremely small/large predictions data points. boldface indicates performance statistically signiﬁcantly different best method row. quantitatively rmse predicting runtime e.g. random forests means average misprediction factor models certainly perfect note even relatively poor predictions ridge regression variant tended accurate within order magnitude enough enable portfolio-based algorithm selector satzilla medals competitions. figure visual comparison models runtime predictions previously unseen test instances. data sets used column shown top. x-axis scatter plot denotes true runtime y-axis -fold cross-validated runtime predicted respective model; represents instance. predictions denoted blue cross rather black dot. figures d.–d. show equivalent plots benchmarks also include regression trees methods yielding best predictions terms quantitative measures. random forests always best method yielded best performance heterogeneous instance bigmix attribute strong performance random forests highly heterogeneous data sets fact that tree-based approach model different parts data separately; contrast methods allow given part space inﬂuenced data distant parts space. indeed ridge regression variants made extremely predictions outlying points bigmix. homogeneous data sets either random forests projected processes performed best often followed closely ridge regression variant performance cplex special case could predicted extremely well models finally projected processes ridge regression slight edge homogeneous benchmarks whereas tree-based methods performed best heterogeneous benchmark tsplib. last column figure shows that case random forests performed worst qualitative differences predictions small. terms computational requirements random forests among cheapest methods taking seconds learn model. previous experiments focussed performance various models based entire feature study performance different subsets features using overall best-performing model random forests. table presents results also lists cost various feature subsets broadest level note predictive performance improved used computationally expensive features e.g. trivial features basically free yielded rather poor performance whereas using entire feature almost always best performance. interestingly however benchmarks using moderately expensive features yielded results statistically insigniﬁcantly different best substantial reductions feature computation time. even true several benchmarks considering cheap features. features clearly showed value example cheap feature yielded similar predictive performance previous features much lower cost; moderate feature tended yield better performance previous comparable cost. features especially clear improvements yielding signiﬁcantly better predictive performance previous features cases. similarly features improved performance signiﬁcantly cases table shows representative results optimization hyperparameters improved robustness somewhat ridge regression methods improved models slightly across board. however improvements came expense dramatically slower training. although ﬁxed number hyperparameter optimization steps variation model parameters affected learning time model families others; slowdowns reached factor table quantitative comparison random forests based different feature subsets ‘prev’ previous features only; ‘mod’ moderate; ‘exp’ expensive. feature sets ‘cheap’ ‘mod’ ‘exp’ include cheaper features; e.g. ‘exp’ uses entire feature set. report -fold cross-validation performance. lower rmse values better boldface denotes results statistically signiﬁcantly different best. practice small improvements predictive performance obtained hyperparameter optimization appear likely justify drastic increase computational cost thus evaluate model performance based ﬁxed default hyperparameters rest article. table quantitative evaluation impact hyperparameter optimization predictive accuracy. model family hyperparameters report performance achieved without hyperparameter optimization show -fold cross-validation performance default hyperparameters optimized using direct -fold cross-validation. dataset model class boldface denotes λdef λopt statistically signiﬁcant better tables provide results benchmarks. study performance techniques changes based quantity training data available. figure visualizes relationship representative benchmarks; data benchmarks appears online appendix. following rather rmse scaling plots reasons. first rmse plots often cluttered outlier instances prediction accuracy poor second plotting facilitates performance comparisons across benchmarks since overall random forests performed best across training sizes. versions ridge regression performed poorly small training sets. observation signiﬁcant since past work employed ridge regression large amounts data measuring performance turns favourable condition move predicting single algorithm’s runtime across distribution instances predicting runtime across family algorithms parameterized algorithms four ways assess prediction quality achieved model predictions training conﬁgurations training instances. predictions basic case useful succinctly modeling known algorithm performance data. interestingly several methods already perform poorly here. predictions training conﬁgurations test instances. predictions used make per-instance decision given parameter conﬁgurations perform best previously unseen test instance example algorithm selection figure prediction quality varying numbers training instances. model number training instances plot mean correlation coefﬁcient true predicted runtimes test instances; larger better perfect. figures d.–d. show equivalent plots benchmarks. predictions test conﬁgurations training instances. case important algorithm conﬁguration goal high-quality parameter conﬁgurations given training instances predictions test conﬁgurations test instances. general case natural pure prediction problem also important per-instance algorithm conﬁguration could model search conﬁguration promising previously-unseen test instance understand evaluation previous section special case consider algorithm’s default conﬁguration vary instances. consider converse case instances vary conﬁgurations consider case generalize across parameter conﬁgurations instances section industrial solver spear used parameter conﬁguration space previous work includes parameters categorical four integral twelve continuous. categorical parameters mainly control heuristics variable value selection clause sorting resolution ordering also enable disable optimizations pure literal rule. continuous integer parameters mainly deal activity decay elimination variables clauses well randomized restart interval percentage random choices; discretized three eight values. total based discretization continuous parameters spear different conﬁgurations. commercial solver ilog cplex used conﬁguration space parameters previous work parameters exclude cplex settings change problem formulation include preprocessing parameters strategy parameters categorical parameters deciding aggressively types cuts; real-valued limit parameters; simplex parameters barrier optimization parameters parameters. total based discretization continuous parameters parameters gave rise unique conﬁgurations. experiments next section gathered runtime data spear cplex executing randomly sampled parameter conﬁgurations. solver instances distributions expected yield state-of-the-art performance spear ibm; cplex instance distributions discussed previous section. runtime data next section gathered -node westgrid cluster glacier large number algorithm runs required experiments described section restricted cutoff time single algorithm seconds following consider performance epms parameters vary instance features not; thus used instance distribution instance features. dataset selected easiest benchmark instance amongst ones default parameter conﬁguration required seconds reference machines. before used -fold cross validation assess accuracy model predictions previously unseen parameter conﬁgurations. table quantitative comparison models runtime predictions previously unseen parameter conﬁgurations. report -fold cross-validation performance. lower rmse better boldface indicates performance statistically signiﬁcantly different best method row. table provides additional results figure visual comparison models runtime predictions previously unseen parameter conﬁgurations. scatter plot x-axis denotes true runtime y-axis cross-validated runtime predicted respective model. represents parameter conﬁguration. figures provide results domains also show performance regression trees ridge regression variant table quantiﬁes performance models benchmark problems figure visualizes predictions. again qualitatively solver runtime function parameter settings could predicted quite well methods even runtimes varied factors observe projected processes random forests ridge regression variant consistently outperformed regression trees; signiﬁcant regression trees model previously used predictions conﬁguration spaces categorical parameters hand poor performance neural networks spore-foba underlines selecting right features straightforward. overall best performance achieved projected processes previous section however random forests also either best close best every data set. figure quality predictions conﬁguration space dependent number training conﬁgurations. model number training instances plot mean standard deviation correlation coefﬁcient true predicted runtimes test conﬁgurations. figure shows equivalent results benchmarks. results remained similar varying number training conﬁgurations. figure shows projected processes performed best overall closely followed random forests. ridge regression variant often produced poor predictions trained using relatively small number training data points performed well given sufﬁcient data. finally spore-foba neural networks performed relatively poorly regardless amount data given. consider challenging prediction problems parameterized algorithms. ﬁrst experiments discussed tested predictions challenging case conﬁgurations instances previously unseen. later section evaluate predictions made four combinations training/test instances training/test conﬁgurations. experiments section used spear cplex conﬁguration spaces section randomly sampled conﬁgurations. conﬁgurations problem instances instance sets generating runtime data thought matrix. split conﬁgurations instances training test sets equal size trained epms ﬁxed number randomly selected combinations training instances training conﬁgurations. note sound empirical evaluation methods required gathering large amount data extensive experimentation required practice. execution runs used section took years time requirements individual table root mean squared error obtained various models runtime predictions unseen instances conﬁgurations. boldface indicates best average performance row. cplexbigmix extremely poorly predicted outliers maximal prediction runtime exceeding thus bound rmse below. models based data points. table provides additional results data sets ranging years years however demonstrate section methods often yield surprisingly accurate predictions based data gathered overnight single machine. examine interesting case test instances conﬁgurations previously unseen. table provides quantitative results model performance based training data points figure visualizes performance. overall note best models generalized conﬁgurations instances almost well either alone heterogeneous data cplex-bigmix witnessed extremely poorly predicted outliers ridge regression variants cases models captured large spread runtimes quite well. experiments section tree-based approaches able model different regions input space independently performed best heterogeneous data sets. figure also shows qualitative differences predictions example ridge regression neural networks projected processes sometimes overpredicted runtime shortest runs tree-based methods problem. random forests performed best cases consistent robust predictions instance conﬁguration space observed earlier. next studied amount data actually needed obtain good predictions varying number randomly selected combinations training instances conﬁgurations. figure shows correlation coefﬁcients achieved various methods function amount training data available. overall note models already performed remarkably well include domains also show performance spore-foba regression trees higher) based hundred training data points. conﬁrmed practicality methods single machine takes hours execute algorithm runs cutoff time seconds. thus even users without access cluster expect able execute sufﬁciently many algorithm runs overnight build decent empirical performance model algorithm instance distribution interest. examining results detail ridge regression variants trouble heterogeneous benchmark cplex-bigmix otherwise performed quite well. overall random forests performed best across different training sizes. naturally methods required data make good predictions heterogeneous benchmarks relatively homogeneous ones study four combinations predictions training/test instances training/test conﬁgurations. results summarized table figures ﬁgures sorted instances average hardness parameter conﬁgurations average performance generating heatmap instances x-axis conﬁgurations y-axis greyscale values representing algorithm runtime given conﬁguration/instance combinations. compare heatmaps representing true runtimes based predictions obtained models. here show results scenarios performance advantage random forests methods highest figure quality predictions joint instance/conﬁguration space function number training data points. model number training data points plot mean correlation coefﬁcients true predicted runtimes test instances conﬁgurations. omit standard deviations avoid clutter high ridge regression variants. figure shows corresponding qualitatively similar results benchmarks. figure true predicted runtime matrices dataset spear-swv-ibm combinations training/test instances training test conﬁgurations example left heatmap shows true runtimes cross product training conﬁgurations spear training instances swv-ibm benchmark set. darker greyscale values represent faster runs i.e. instances right side heatmap hard conﬁgurations heapmap good predicted matrix regression trees visually indistinguishable random forests methods closely resemble ridge regression. table root mean squared error obtained various empirical performance models predicting runtime based combinations paramater conﬁgurations intance features. trained randomly-sampled combinations training conﬁgurations instances report performance four combinations training/test instances training/test conﬁgurations. boldface indicates model best performance. figure shows results benchmark spear-swv-ibm. features column four combinations training/test instances training/test conﬁgurations allowing visually assess well respective generalization works models. note case true heatmaps almost indistinguishable predicted random forests even challenging case unseen problem instances parameter conﬁgurations tree-based methods captured non-trivial interaction pattern instances parameter conﬁgurations. hand non-tree-based methods captured instance hardness failing distinguish good conﬁgurations even simplest case predictions training instances training conﬁgurations. figure shows results benchmark cplex-corlat. simplest case predictions training instances conﬁgurations tree-based methods yielded predictions close true runtimes capturing instance hardness performance parameter conﬁgurations. contrast even simple case methods captured instance hardness predicting conﬁgurations roughly equal performance. random forests generalized better test instances test conﬁgurations trend also evident quantitatively table cplex benchmarks. regression tree predictions visually indistinguishable random forests; strong qualitative performance figure predicted runtime matrices different number training data points compared true runtime matrix. points means entire crossproduct training instances training conﬁgurations finally investigated predictive quality depends quantity training data focusing random forests spear-swv-ibm training data points sufﬁced obtain random forest models captured salient features training data points gradually improved qualitative predictions especially distinguishing good conﬁgurations. likewise cplex-corlat salient features could detected based training data points training data improved qualitative predictions capture differences good conﬁgurations. overall increases training size yielded diminishing returns even predictions based entire cross-product training instances parameter conﬁgurations much different based subset samples past work predicting algorithm runtime treated algorithm runs terminated prematurely so-called captime ﬁnished time thus adopted practice model comparisons described revisit issue random forests. formally terminating algorithm captime yields right-censored data point learn lower bound actual time algorithm required. denote actual runtime algorithm partial right censoring training data usual input vector runtime observation censoring indicator observe typical simplistic strategy dealing censored data produces biased models; intuitively treating slow runs though faster really biases training data downwards hence likewise biases predictions. statisticians mostly literature so-called survival analysis actuarial science developed strategies building unbiased regression models based censored data gagliolo ﬁrst techniques literature runtime prediction. speciﬁcally used method handling censored data parameterized probabilistic models employed resulting models construct dynamic algorithm portfolios. survival analysis literature schmee hahn described iterative procedure handling censored data points linear regression models. employed technique improve runtime predictions made portfolio-based algorithm selection method satzilla best knowledge methods literature applied algorithm runtime prediction exist several candidates consideration future work. gaussian processes could approximations handle non-gaussian observation likelihoods resulting censorship; example ertin described laplace approximation handling right-censored data. random forests previously adapted handle censored data classical methods yield non-parametric kaplan–meier estimators undeﬁned beyond largest uncensored data point. here describe simple improvement method schmee hahn random forests developed context handling censored data model-based algorithm conﬁguration method schmee hahn expectation maximization algorithm. applied model base model algorithm would ﬁrst initial using uncensored data iterate following steps mean denotes predictive distribution current data point truncated conditioned fact least large mean best single value impute data point context models approach yields overly conﬁdent predictions trees would perfectly agree predictions censored data points. preserve uncertainty true runtime censored runs change step tree s.t. sample precise details. resulting modiﬁed variant schmee hahn’s algorithm takes prior uncertainty account computing posterior predictive distribution thereby avoiding overly conﬁdent predictions. implementation detail avoid potentially large outlying predictions known maximal runtime κmax seconds ensure mean imputed value exceed κmax. experimentally compare schmee hahn’s procedure modiﬁed version baselines ignoring censored data points altogether treating data points censored captime uncensored data points runtime report results interesting case predictions previously unseen parameter conﬁgurations instances. used benchmark distributions section artiﬁcially censoring training data different thresholds actual threshold. experimented different types capped data data figure true predicted runtime various ways handling censored data random forests scenario cplex-bigmix ﬁxed censoring threshold second training varying threshold scatterplot x-axis indicates true runtime y-axis cross-validated runtime predicted respective model. represents instance. analogous ﬁgures benchmarks given figures d.–d. ﬁxed censoring threshold across data points data thresholds instance-speciﬁc ﬁxed threshold represents sort data generated experimental studies like previous sections paper instance-speciﬁc threshold models practical applications epms model-based algorithm conﬁguration procedures types capped data prediction strategies measured predictive error quality uncertainty estimates uncensored part test data. figure illustrates predictions benchmark demonstrating qualitative differences four methods treating capped data. case ﬁxed censoring threshold simply dropping censored data yielded consistent underestimates treating censored data uncensored yielded good predictions strategy thus reasonable predictions beyond required schmee hahn variants performed similarly yielded unbiased predictions times note factor much compared orders magnitude variation observe data. much better predictions larger runtimes achieved using instance-speciﬁc captimes discussed thus advocate varying captimes order enable better scaling larger captimes. quantitative analysis showed ﬁxed-threshold case dropping censored data worst prediction errors; treating censored data uncensored improved results; using schmee hahn variants reduced prediction errors. however ﬁxed thresholds schmee hahn variants often yielded poor uncertainty estimates imputed similar values censored data points yielding little variation across trees thus also yielded overconﬁdent predictions. contrast data varying captimes treating censored data uncensored often performed worse simply dropping schmee hahn variants yielded competitive uncertainty estimates lowest prediction error. finally found qualitative ﬁndings robust respect aggressively captimes chosen. overall random forests handled censored data reasonably well. note models might better suited extrapolating training data short captimes obtain accurate runtime predictions long algorithm runs. article assessed advanced state predicting performance algorithms hard combinatorial problems. proposed techniques building predictive models particular focus improving prediction accuracy parameterized algorithms also introduced wealth features three widely studied np-hard problems beneﬁt models. conducted largest experimental study aware—predicting performance algorithms instance distributions tsp— comparing modeling approaches comprehensive methods literature. showed approaches—chieﬂy based random forests also approximate gaussian processes—offer best performance whether consider predictions previously unseen problem instances parameterless algorithms parameter settings parameterized algorithm running single problem instance parameterized algorithms parameter values previously unseen problem instances. also demonstrated settings accurate predictions possible based small amounts training data finally demonstrated best-performing model random forests could improved better handling data prematurely terminated runs. overall showed methods fast general achieve good robust performance. hope useful wide variety researchers seek model algorithm performance algorithm analysis scheduling algorithm portfolio construction automated algorithm conﬁguration applications. matlab source code models data source code reproduce experiments online appendix containing additional experimental results available online http//www.cs.ubc.ca/labs/beta/projects/epms. thank kevin murphy many valuable discussions regarding gaussian processes random forests jonathan shen proofreading early version paper. also thank anonymous reviewers comments helped signiﬁcantly improve paper. appendix gives information instance benchmarks. benchmarks number variables clauses given original instance reported numbers preprocessing explaining differences reported values swv.) appendix benchmarks indu. benchmark data comprises instances industrial categories competitions well races. instances contain average variables clauses respective standard deviations respective maxima variables clauses. hand. benchmark data comprises instances handmade categories competitions. instances contain average variables clauses respective standard deviations respective maxima variables clauses. rand. benchmark data comprises instances random categories competitions. instances contain average variables clauses respective standard deviations respective maxima variables clauses. ibm. sat-encoded bounded model checking instances comprises instances generated zarpas instances selected instances randomly-selected folders formal veriﬁcation benchmarks library. instances contained average variables clauses respective standard deviations respective maxima variables clauses. swv. sat-encoded software veriﬁcation instances comprises instances generated calysto static checker used veriﬁcation programs spam ﬁlter dspam solver hypersat wine windows emulator gzip archiver component xinetd instances contain average variables clauses respective standard deviations respective maxima variables clauses. randsat. contains satisﬁable instances data rand. instances contain average variables clauses respective standard deviations respective maxima variables clauses. appendix benchmarks bigmix. highly heterogenous publicly available mixed integer linear programming benchmarks comprises milp instances. instances average variables constraints respective standard deviations respective maxima variables constraints. corlat. comprises milp instances based real data used construction wildlife corridor grizzly bears northern rockies region made available bistra dilkina). instances variables; average constraints rcw. comprises milp instances computational sustainability project. instances model spread endangered red-cockaded woodpecker conditional decisions certain parcels land protected. generated instances using generator parameter setting used paper except smaller sample size instances variables; average constraints reg. comprises milp-encoded instances winner determination problem combinatorial auctions. generated instances using regions generator combinatorial auction test suite number bids selected uniformly random ﬁxed bids/goods ratio average variables constraints respective standard deviations respective maxima variables constraints. appendix benchmarks rue. comprises uniform random euclidean -dimensional instances generated random generator portgen number nodes randomly selected generated instances contain average nodes standard deviation maximum nodes. rce. comprises random clustered euclidean -dimensional instances generated random generator portcgen number nodes randomly selected number clusters number nodes. generated instances contain average nodes standard deviation maximum nodes. tsplib. contains subset prominent tsplib repository. included instances feature computation code code smith-miles hemert completed successfully instances nodes range", "year": 2012}