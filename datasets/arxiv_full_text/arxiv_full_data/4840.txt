{"title": "Learning Purposeful Behaviour in the Absence of Rewards", "tag": ["cs.LG", "cs.AI"], "abstract": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \"just out of reach\" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.", "text": "artiﬁcial intelligence commonly deﬁned ability achieve goals world. reinforcement learning framework goals encoded reward functions guide agent behaviour observed rewards provide notion progress. however domains reward signal reward signal sparse appear absent. withreward feedback agent behaviour typically random often dithering aimlessly lacking intentionality. paper present algorithm capable learning purposeful behaviour absence rewards. algorithm proceeds constructing temporally extended actions identiﬁcation purposes just reach agents current behaviour. purposes establish intrinsic goals agent learn ultimately resulting suite behaviours encourage agent visit different parts state space. moreover approach particularly suited settings rewards sparse behaviours help exploration environment reward observed. reinforcement learning successful generating agents capable intelligent behaviour initially unknown environments; accomplishments surpassing human-level performance backgammon helicopter ﬂight general competency dozens atari games successes achieved algorithms maximize expected cumulative rewards seen measure progress towards extended version paper presented workshop entitled abstraction reinforcement learning international conference machine learning york copyright author. interested setting reward signal uninformative even absent. uninformative setting paucity reward usually leads dithering typical \u0001-greedy exploration approaches. effect particularly pronounced agent operates time scale common video game platforms complete absence reward unclear intelligent behaviour even constitute. intrinsic motivation-based approaches offer solution form intrinsic reward signal authors proposed agents undergoing developmental periods concerned maximizing extrinsic reward acquiring reusable options autonomously learned intrinsic rewards however instead consider notion purposeful agent commit behaviour extended period time. construct purposeful agents appeal options framework options extend mathematical framework reinforcement learning markov decision processes allow agents take temporally extended actions accomplish subgoals. extension extremely powerful idea allowing reasoning different levels abstraction automatically discovering options openproblem literature. generally options designed practitioners identify meaningful subgoals used stepping-stones solve complex task. besides using options divide task subgoals advocate also options decisiveness agents environment rewards available better choice aimless exploration. paper introduce algorithm capable learning purposeful behaviour absence rewards. approach discovers options identifying purposes achievable agent. purposes turned intrinsic subgoals intrinsic reward function however large problems infeasible learn exactly state-action pair. tackle issue agents often learn approximate value function common approach approximate values using linear function approximation denotes vector weights denotes static feature representation state taking action also done non-linear function approximation methods neural networks note generally much smaller number parameters number states standard framework focused mdps actions last single time step. nevertheless convenient agents encoding higher levels abstraction also facilitate learning process properly deﬁned sutton extended framework introducing temporally extended actions called options. intuitively options higher-level actions extended several time steps. formally option deﬁned -tuple denotes initiation denotes option’s policy denotes termination set. initiated actions selected according agent reaches state originally sutton deﬁned function encode probability option terminating given state. paper deﬁne characteristic function hence overload notation. options generalize mdps semimarkov decision processes actions take variable amounts time. options also useful addressing problem exploration move agents farther state-space. potential options dramatically affect learning improving exploration well known nevertheless works beneﬁt options discover them assume provided hardwired notion interestingness used discover options e.g. salient events works investigate discover options clustered three different categories. common approach identify subgoal states heuristic visitation frequency graph-related metrics betweenness graph partitioning metrics authors also tackled problem option disresulting suite behaviours encourage agent visit different parts state space. options particularly useful absence rewards. example agent observes change environment feature representation tries learn policy capable reproducing change. also option added agent’s action agent move farther state-space events rare becoming frequent events impossible becoming just infrequent. early paper introduce main ideas underly algorithm including concepts purpose. also provide algorithm option discovery linear function approximation contrast approaches option discovery rely tabular representations. show ﬁnite learned options guaranteed least state cause option termination. finally apply approach simple domain showing reach states starting state thus exhibiting intentionality behaviour. section introduce reinforcement learning problem setting options framework. also discuss problem option discovery approaches address convention indicate random variables capital letters vectors bold letters functions lowercase letters sets calligraphic font framework agent aims maximizing notion cumulative reward taking actions environment; actions affect next state agent well subsequent rewards experience. generally assumed tasks interest satisfy markov property called markov decision processes formally deﬁned -tuple time agent state takes action leads next state according transition probability kernel encodes r|st agent also observes reward agent’s goal learn policy maximizes expected discounted return known discount factor. interested settings reward signal uniform across environment. works option discovery generally operate tabular setting states uniquely deﬁned. consequently metrics frequency visitation transition graphs used option discovery. automatically discovering options large state-spaces function approximation required still challenge. work presents approach option discovery settings linear function approximation much larger applicability. works tackled option discovery function approximation. generally simpliﬁed problem additional assumptions knowledge subgoal states control interface regions proposals depending reward signal discover meaningful behaviour looking different rates changes agent’s feature representation related work. relationship clearer next section. approaches based intrinsic motivation novelty ways circumvent absence rewards environment. schmidhuber summarizes several works based intrinsic motivation deﬁnes algorithms maximize reward signal derived agent’s learning progress. lehman stanley advocated agents drop feedback receive environment even traditional settings search maximizing novelty instead. ideas related work. discover options based novelty assuming extrinsic rewards available options based loose notion model environment aiming learning change principal components compressed environment representation algorithm based four different concepts namely storing changes seen different time steps clustering correlated changes order extract purpose learning policies capable reproducing desired purposes transforming policies options used move agent farther state space. steps options giving different purposes agent discovered. options guide agent different parts state space lead identifying purposes. steps used iteratively create self-reinforcing loop. agent initially follows default policy given number time steps using actions available action set. following policy every time step agent stores dataset difference feature representation current observation representation previous obimportant stress storing changes easily stand features rarely change. storing features less informative current transition harder agent identify feature really changed. moreover storing differences allow clearly identify different directions change something useful next steps. pre-deﬁned number time steps agent stops storing transitions identify purposes observed behaviour. clusters together features change together avoiding correlated changes generate purpose. formally step consists reducing dimensionality singular value decomposition generates lower rank representation transitions stored rank representation consists eigenvalues eigenvectors. eigenvectors encode principal components eigenvalues weight according much eigenvector important reconstruct eigenvector seen encoding different purpose agent features somehow correlated collapsed single eigenvector explaining direction variation observed transitions. call eigenvectors obtained dataset transitions eigenpurposes. deﬁnition given matrix transitions encodes difference consecutive observations i.e. having denoting i-th matrix eigenvector obtained singular value decomposition traditionally deﬁned called eigenpurpose. following example provides intuition importance eigenpurposes. imagine agent chance leaves building. value several features change collapse changes instead feature encoding temperature increase encoding change lighting eigenpurpose encoding outside building. eigenpurposes identiﬁed agent learns policies capable maximizing occurrence. order learn policy maximizes eigenpurpose need deﬁne proper reward function. reward maximize eigenpurposes possible directions i.e. agent also learns second policy maximizes −rit. policies called eigenbehaviours. deﬁnition policy called eigenbehaviour optimal policy maximizes occurrence eigenpurpose original augumented action terminates option; i.e. maxa maxπ naturally construct option learned eigenbehaviour. need deﬁne states eigenbehaviour effective termination condition. deﬁne initiation option states which learning least action intuitively corresponds every state policy still make progress towards eigenpurpose. terminal states option complement initiation i.e. options discovered agents action allows agent repeat described process policy uses discovered options collect data. notice eigenvalues loosely encode frequent eigenpurpose observed. eigenbehaviours corresponding lower eigenvalues encode purposes observed less frequently. that option rare purpose discovered purpose longer rare since single decision capable reproducing rare event. also increase likelihood observing unlikely purposes since single action moves agent much farther state-space. help agents explore environments rewards sparse guiding agent reward signal observed. described algorithm formally presented algorithm additional detail discussed decide learn maximize subset discovered eigenpurposes. work evaluate impact different approaches. propose simple eigenvalue threshold determines eigenpurposes interesting. select eigenpurposes correspondent eigenvalue greater interpret discarding noise. finally important guarantee least state satisﬁes discovered purposes words termination option empty. theorem consider option πoto discovered algorithm nonempty. proof intuition. consider state largest potential value. state agent must terminate either discount factor termination action state lesser equal potential value. cumulative reward received difference potential expected value state must non-positive. complete proof appendix. evaluated algorithm random walk domain consisting moving around ring length deterministic transitions. agent starts coordinate every time step chooses going right going left. linear function approximation two’s complement binary encoding representation agent goes left state goes state similarly going right state transitions rewards environment. evaluations made setting. selected noise parameter environment learning eigenbehaviours discount factor policies obtained value iteration iterations. round algorithm consisted time steps agent collected transitions discover eigenpurposes. rounds round zero primitive actions available. agent’s default policy select uniformly random among currently available actions options. ﬁrst evaluated results agents full observability agent perceives states described looking average length discovered options options become increasingly complex iteration. added complexity allows agent move farther states evidenced increasing distance farthest point agent’s starting state iteration improvement particularly clear comparing sample random walk primitive actions note that despite options constructed later iterations still primitive actions present complex behaviours. different typical option discovery methods construct hierarchies options. also evaluated setting agent partial observability. setting agent observe three least signiﬁcant bits encoding state. collapses several states together makes much harder figure sample random walk using primitive actions random walk using discovered options. dashed vertical lines represent iteration boundaries. agent observe progress. interestingly behavioural pattern full observability experiment emerges. agents still come options type i-th discover purposes. unique difference fewer options discovered iteration fewer observable eigenpurposes. paper introduced algorithm capable discovering options without feedback form rewards environment. algorithm discovers options reproduce purposes extracted states seen agent acting randomly. presented experimental results showing discovered options greatly improve exploration introducing decisiveness agents avoiding traditional aimless dithering. also showed evidences approach work well partial observability. future work plan investigate algorithm behaves complex environments arcade learning environment evaluate algorithm large domains amenable function approximation differently approaches option discovery. naturally able learn policy using discovered options maximize discounted rewards. preliminary results using interrupting options seem promising. however applying algorithm larger domains face challenge discussed here exploding number eigenpurposes indicating proper sampling techniques must evaluated. finally important coevolving action representation abstractions higher levels action abstraction drive agent improve representation world agent better representation world better action abstractions become available. topic commonly investigated needs addressed future maybe algorithm ﬁrst step towards direction. table characteristics discovered options iteration compared random walk ring. number average runs standard deviations reported parentheses. details selected parameters c.f. text. authors thank richard sutton marc bellemare insightful discussions helped improve work csaba szepesv´ari point neumann series used proof. work supported grants alberta innovates technology futures alberta innovates centre machine learning ¸ims¸ek ¨ozg¨ur barto andrew using relative novelty identify useful temporal abstractions reinforcement learning. proceedings international conference machine learning ¸ims¸ek ¨ozg¨ur barto andrew skill characterization based betweenness. proceedings advances neural information processing systems ¸ims¸ek ¨ozg¨ur wolfe alicia barto andrew identifying useful subgoals reinforcement learning local graph partitioning. proceedings international conference machine learning kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum joshua hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv e-prints mannor shie menache ishai hoze amit klein uri. dynamic abstraction reinforcement learning clustering. proceedings international conference machine learning mcgovern barto andrew automatic discovery subgoals reinforcement learning using diverse density. proceedings international conference machine learning menache ishai mannor shie shimkin nahum. qcut dynamic discovery sub-goals reinforcement learning. proceedings european conference machine learning mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature andrew coates adam diel mark ganapathi varun schulte jamie berger eric liang eric. autonomous inverted helicopter flight reinforcement learning. proceedings international symposium experimental robotics pickett marc barto andrew policyblocks algorithm creating useful macro-actions reinforcement learning. proceedings international conference machine learning singh satinder barto andrew chentanez nuttapong. intrinsically motivated reinforcement learning. proceedings advances neural information processing systems sutton richard precup doina singh satinder. between mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence proof. write bellman equation matrix form ﬁnite column vector entry state encoding value function. algorithm denotes eigenpurpose interest. therefore shift ﬁnite constant without changing reward i.e. therefore assume maxs ||w||∞. clearly otherwise w||∞ |vs∗ ws∗| ||w||∞ arriving contradiction.", "year": 2016}