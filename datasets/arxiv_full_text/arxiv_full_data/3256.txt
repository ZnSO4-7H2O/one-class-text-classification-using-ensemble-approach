{"title": "Generalized Nonconvex Nonsmooth Low-Rank Minimization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "As surrogate functions of $L_0$-norm, many nonconvex penalty functions have been proposed to enhance the sparse vector recovery. It is easy to extend these nonconvex penalty functions on singular values of a matrix to enhance low-rank matrix recovery. However, different from convex optimization, solving the nonconvex low-rank minimization problem is much more challenging than the nonconvex sparse minimization problem. We observe that all the existing nonconvex penalty functions are concave and monotonically increasing on $[0,\\infty)$. Thus their gradients are decreasing functions. Based on this property, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to solve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem. By setting the weight vector as the gradient of the concave penalty function, the WSVT problem has a closed form solution. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthetic data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.", "text": "figure illustration popular nonconvex surrogate functions ||θ|| supergradients penalty functions share common properties concave monotonically increasing thus supergradients nonnegative monotonically decreasing. proposed general solver based observation. rm×n smooth function type surrogate functions l-norm many nonconvex penalty functions proposed enhance sparse vector recovery. easy extend nonconvex penalty functions singular values matrix enhance lowrank matrix recovery. however different convex optimization solving nonconvex low-rank minimization problem much challenging nonconvex sparse minimization problem. observe existing nonconvex penalty functions concave monotonically increasing thus gradients decreasing functions. based property propose iteratively reweighted nuclear norm algorithm solve nonconvex nonsmooth low-rank minimization problem. irnn iteratively solves weighted singular value thresholding problem. setting weight vector gradient concave penalty function wsvt problem closed form solution. theory prove irnn decreases objective function value monotonically limit point stationary point. extensive experiments synthetic data real images demonstrate irnn enhances low-rank matrix recovery compared state-of-the-art convex algorithms. denotes i-th singular value rm×n penalty function loss function satisfy following assumptions continuous concave monotonically increasing possibly nonsmooth. exponential-type penalty geman laplace table tabulates penalty functions figure visualizes them. refer properties penalty functions. nonconvex penalties extended approximate rank function e.g. schatten-p norm another nonconvex surrogate rank function truncated nuclear norm nonconvex sparse minimization several algorithms proposed solve problem nonconvex regularizer. common method programming minimizes nonconvex function based assumption convex. iteration programming linearizes minimizes relaxed function follows subgradient programming efﬁcient since requires iterative algorithm solve note updating rule programming cannot extended solve low-rank problem reason concave guarantee convex w.r.t. programming also fails nonconvex problem many optimization problems machine learning computer vision areas fall formulation choice squared loss linear mapping widely used. case lipschitz constant spectral radius i.e. adjoint operator exactly x||∗. problem resorts well known nuclear norm regularized problem low-rank minimization problem arises many machine learning tasks multiple category classiﬁcation matrix completion multi-task learning low-rank representation squared loss subspace segmentation however solving problem usually difﬁcult even np-hard. previous works solve convex problem instead. proved under certain incoherence assumptions singular values matrix solving convex nuclear norm regularized problem leads near optimal low-rank solution however assumptions violated real applications. obtained solution using nuclear norm suboptimal since perfect approximation rank function. similar phenomenon observed convex l-norm nonconvex l-norm sparse vector recovery order achieve better approximation lnorm many nonconvex surrogate functions l-norm proposed including lp-norm smoothly clipped absolute deviation logarithm minimax concave penalty capped another related work iteratively reweighted least squares algorihtm. recently extended handle nonconvex schatten-p norm penalty actually solves relaxed smooth problem require many iterations achieve low-rank solution. cannot solve general nonsmooth problem alternative updating algorithm minimizes truncated nuclear norm using special property penalty. contains loops require computing svd. thus efﬁcient. cannot extended solve general problem either. work existing nonconvex surrogate functions l-norm extended singular values matrix enhance low-rank recovery. problem existing nonconvex penalty function shown table function satisﬁes assumption observe existing nonconvex surrogate functions concave monotonically increasing thus gradients nonnegative monotonically decreasing. based fact propose iteratively reweighted nuclear norm algorithm solve problem irnn computes proximal operator weighted nuclear norm closed form solution nonnegative monotonically decreasing supergradients. theory prove irnn monotonically decreases objective function value limit point stationary point. best knowledge irnn ﬁrst work able solve general problem convergence guarantee. note nonconvex optmization usually difﬁcult prove algorithm converges stationary points. last test algorithm several nonconvex penalty functions synthetic data real image data show effectiveness proposed algorithm. section present general algorithm solve problem handle case nonsmooth e.g. capped penalty need concept supergradient deﬁned concave function. subgradient convex function extension gradient nonsmooth point. similarly supergradient extension gradient concave function nonsmooth point. concave differentiable known supergradients called superdifferential denoted differentiable also supergradient i.e. {∇g}. figure illustrates supergradients concave function differentiable nondifferentiable points. concave convex vice versa. fact following relationship supergradient subgradient lemma concave vice versa. relationship supergradient subgradient shown lemma useful exploring properties supergradient. known subdifﬁerential convex function monotone operator i.e. supergradient monotonically decreasing table shows usual concave functions supergradients. also visualize figure seen satisfy assumption note penalty deﬁne affect algorithm convergence analysis shown latter. capped penalty nonsmooth superdifferential iteratively reweighted nuclear norm seems updating solving weighted nuclear norm problem extension weighted l-norm problem algorithm however weighted nuclear norm nonconvex weighted l-norm convex. solving nonconvex problem much challenging convex weighted l-norm problem. fact easier solving original problem particular implies boundedness {xk} obtained based assumption theorem {xk} sequence generated algorithm accumulation point {xk} stationary point proof. sequence {xk} generated algorithm bounded shown theorem thus exists matrix subsequence {xkj} theorem fact thus choice ∂gλ) lemma −wkj upper semi-continuous property subdifferential proposition exists −wkj ∂gλ) leads proposed iteratively reweighted nuclear norm algorithm. whole procedure irnn shown algorithm lipschitz constant known computable backtracking rule used estimate iteration section give convergence analysis irnn algorithm. show irnn decreases objective function value monotonically limit point stationary point problem ﬁrst recall following well-known fundamental property smooth function class lemma rm×n continuously differentiable function lipschitz continuous gradient lipschitz constant then rm×n indices samples rm×n rm×n linear operator keeps entries unchanged outside zeros. gradient squared loss function lipschitz continuous lipschitz constant algorithm choice test penalty functions listed table except capped geman since recovery performances sensitive choices different cases. choice irnn continuation technique enhance low-rank matrix recovery. initial value larger value dynamically decreased stopped till reaching predeﬁned target initialized zero matrix. choice parameters nonconvex penalty functions search candidate obtains good performance cases ﬁrst compare nonconvex irnn algorithm state-of-the-art convex algorithms synthetic data. conduct experiments. observed matrix without noise noise. noise free case generate rank matrix generated matlab command randn. elements missing uniformly random. compare algorithm augmented lagrange multiplier solves noise free problem convergence results theorem also hold since holds compared alternating updating algorithms require double loops irnn algorithm efﬁcient stronger convergence guarantee. cheaply solved. interesting application extend group sparsity singular values. dividing singular values groups i.e. {··· deﬁne group sparsity singular values x||g exactly ﬁrst term letting l-norm vector. nonconvex functions satisfying assumption specially convex absolute function. section present several experiments synthetic data real images validate effectiveness irnn algorithm. test algorithm matrix completion problem figure comparison image recovery using different matrix completion algorithms. original image. image gaussian noise text. recovered images apgl lmafit tnnr-admm irnn-lp irnn-scad respectively. best viewed sized color ﬁle. m||f m||f recovered solution certain algorithm. relative error smaller regarded successful recovery repeat experiments times underlying rank varying algorithm. frequency success plotted figure legend irnn-lp figure denotes penalty function used problem solved proposed irnn algorithm. seen irnn nonconvex penalty functions achieves much better recovery performance convex algorithm. nonconvex penalty functions approximate rank function better convex nuclear norm. noisy case data generated pω+.×randn. compare algorithm convex accelerated proximal gradient line search solves noisy problem unrelated text. usually real images lowrank singular values dominate main information thus corrupted image recovered low-rank approximation. color images three channels simply apply matrix completion channel independently. well known peak signal-to-noise ratio employed evaluate recovery performance. compare irnn matrix completion algorithms applied task including apgl low-rank matrix fitting truncated nuclear norm regularization solver based admm solve subproblem tnnr released codes tune parameters optimal chosen algorithms report best result. test consider types noises real images. ﬁrst replaces pixels random values figure adds unrelated texts image figure figure show recovered images different methods. observed irnn method different penalty functions achieves much better recovery performance apgl lmafit. results irnn-lp irnn-scad plotted limit space. test images plot results figure figure shows psnr values different methods test images. seen irnn evaluated nonconvex functions achieves higher psnr values veriﬁes nonconvex penalty functions effective situation. nonconvex truncated nuclear norm close methods running time times ours. conclusions future work task ||pω||∞ irnn. chosen algorithms times underlying rank lying relative errors ranging test mean errors different methods plotted figure seen irnn nonconvex penalty outperforms convex apgl noisy case. note cannot conclude figure irnn logarithm penalty functions always perform better scad since obtained solutions globally optimal. zhang. solving low-rank factorization model matrix completion nonlinear successive over-relaxation algorithm. mathematical programming computation rank function. observed existing nonconvex surrogate functions concave monotonically increasing general solver irnn proposed solve problem penalties. irnn ﬁrst algorithm able solve general nonconvex low-rank minimization problem convergence guarantee. nonconvex penalty nonsmooth using supergradient nonsmooth point. theory proved limit point local minimum. experiments synthetic data real images demonstrated irnn usually outperforms state-of-the-art convex algorithms. interesting future work solve nonconvex low-rank minimization problem afﬁne constraint. possible combine irnn alternating direction method multiplier research supported singapore national research foundation international research centre singapore funding initiative administered programme ofﬁce. supported china msra.", "year": 2014}