{"title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "tag": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.", "text": "fig. examples visual questions corresponding answers different people asked answer question image observed crowd sometimes agree single answer times offer different answers given visual question build prediction system automatically decides whether multiple people would give answer. work partially inspired goal improve employ crowds computing power run-time. towards satisfying existing users gaining users supporting wide range applications crowd-powered system cost fast response times yield high quality answers. today’s status assume ﬁxed number human responses visual question ﬁxed cost delay potential diversity answers every visual question instead propose dynamically solicit number human responses based visual question. particular accrue additional costs delays collecting extra answers extra responses needed discover plausible answers. show experiments system saves -hour work weeks answer visual questions compared today’s status approach visual question answering systems emerging desire empower users natural language question visual content receive valid answer response. however close examination problem reveals unavoidable entangled problem multiple humans always agree single answer visual question. train model automatically predict visual question whether crowd would agree single answer. propose exploit system novel application efﬁciently allocate human effort collect answers visual questions. speciﬁcally propose crowdsourcing system automatically solicits fewer human responses answer agreement expected human responses answer disagreement expected. system improves upon existing crowdsourcing systems typically eliminating least human effort loss information collected crowd. would possible person oracle could immediately provide answer question visual world? sight-impaired users could quickly reliably ﬁgure denomination currency whether spent appropriate amount product hikers could immediately learn bites whether seek emergency medical care. pilots could learn many birds path decide whether change course avoid costly life-threatening collisions. examples illustrate several interests visual question answering system including tackling problems involve classiﬁcation detection counting. generally goal single system accurately answer natural language question image video entangled dream system unavoidable issue that asking multiple people visual question sometimes agree single answer times offer different answers fact show paper outcomes arise approximately equal proportions today’s largest publicly-shared benchmark contains visual questions. figure illustrates human disagreements arise variety reasons including different descriptions concept different concepts irrelevant responses goal account whether different people would agree single answer visual question improve upon today’s systems. propose multiple prediction systems automatically decide whether visual question lead human agreement demonstrate value evaluate automated methods. speciﬁcally researchers ﬁelds diverse computer vision computational linguistics machine learning rely large datasets improve algorithms. datasets include visual questions human-supplied answers. data critical teaching machine learning algorithms answer questions example. data also critical evaluating well algorithms perform. general bigger data better. current methods create datasets assume ﬁxed number human answers visual question thereby either compromising quality collecting plausible answers cost collecting additional answers redundant. offer economical spend human budget collect answers crowd workers. particular actively allocate additional answers visual questions likely multiple answers. contributions work follows analysis demonstrating prevalence reasons human answer disagreements today’s largest freelyavailable benchmark. problem system predicting whether crowd agree answering visual question. novel application efﬁcient answer collection solicits additional answers additional members crowd disagreement anticipated. visual question answering services researchers spanning communities diverse human computer interaction machine learning computational linguistics computer vision proposed variety ways answer questions images commonality across communities adopt one-size-ﬁts-all approach deciding number answers visual question. example crowd-powered systems supply prespeciﬁed ﬁxed number answers visual question automated systems return single answer every visual question inspired observation multiple plausible answers visual question propose richer representation visual question answering accounts whether different people would agree single answer. propose system automatically predicts whether humans disagree. demonstrate predictive advantage system relying uncertainty algorithm predicted answer answer collection crowd work relates methods propose employ crowd workers answer questions images. approaches collect pre-speciﬁed ﬁxed number answers visual question. systems treat response time ﬁrst priority variable number answers arise varying crowdsourcing conditions available supply workers systems ensure ﬁxed number answers collected visual question unlike prior work goal collect answers economical complete capturing diversity plausible answers visual questions. knowledge work ﬁrst predict number answers collect visual question. experiments demonstrate disagreement predictions useful signiﬁcantly reduce human effort capturing diversity valid answers visual questions. analyses crowd disagreement broadly work relates efforts account crowd disagreement. example researchers suggested ways resolve crowd disagreement task difﬁculty ambiguity/speciﬁcity methods demonstrate workers trust aggregating multiple responses ﬁnal single response methods leverage context automatically disambiguate multiple outcomes desired outcome unlike prior work focus task visual question answering. moreover prior work focuses resolving speciﬁc sources crowd disagreement instead propose single integrated system jointly detects various sources crowd disagreement arise visual question answering. advantage approach separate easy answer instances instances would require additional effort resolve disagreement; e.g. collect multiple answers ambiguous subjective tasks apply aggregation scheme produce single answer multiple answers crowd workers unreliable. high quality work fixed human budget work aligns methods actively allocate limited human budget best contribute improving quality results. example method distributes budget three different levels human effort deciding segment images another method spends budget less costly crowd workers costly expert efforts improve outcomes biomedical citation screening another method predicts employ algorithms versus crowd workers segment images knowledge work ﬁrst towards deciding spend budget task visual question answering distinct prior work focused spending budget image analysis language analysis alone. furthermore spend budget capture diversity valid results every task rather collect single result every task. minimizing human labeling actively decide allocate human effort improve results also somerelated active learning speciﬁcally active learners little human effort possible train accurate prediction models. methods iteratively supplement training dataset informative images training classiﬁer methods solicit redundant labels prevent incorrect/noisy labels teaching prediction models make mistakes active learners minimize human input improve accuracy prediction model method aims minimize human input still exhaustively capturing plausible answers visual questions. visual questions. expedite arriving desired answers example clarify ambiguous questions. work offers alternative demonstrating crowdsourcing service might instead solicit multiple answers time back-and-forth rather enacting costly continuous communication channel single voice whether single person consensus crowd remainder paper organized four sections. ﬁrst describe study investigate much answer diversity arises visual questions? people disagree next explore following questions given novel visual question machine correctly predict whether multiple independent members crowd would supply answer? insights machine-learned system reveal regarding humans likely agree following section propose novel resource allocation system efﬁciently capturing diversity answers visual questions finally concluding remarks datasets conduct analysis total visual questions answers coming today’s largest freely-available benchmark chose benchmark represents diversity visual questions includes many crowdsourced answers every visual question. benchmark consists datasets reﬂect vqas real images abstract scenes. speciﬁcally vqas real images show types objects would easily recognizable year natural context remaining vqas abstract scenes created clipart show types everyday objects often observed real images benchmark includes diversity visual questions intentionally collected grounded images taskindependent. towards visual questions collected asking three amazon mechanical turk crowd workers look given image generate text-based question would stump smart robot three open-ended questions collected images resulting total visual questions. benchmark also includes open-ended natural language answers crowd workers visual question. answer collected showing worker image associated question asking him/her respond brief phrase complete sentence deﬁning answer diversity compute answer diversity visual question counting many unique valid answers observed answers. derive results using crowdsourced answers visual question. establish unique answers pre-processing answer eliminate cosmetic differences applying exact string matching identify number different answers. pre-process answer converting letters lower case converting numbers digits removing punctuation articles done prior work approach fully resolve conceptually equivalent responses reveal upper bound expected answer diversity. words lenient agreement schemes would lead either less answer diversity. establish valid answers tallying number times unique answer given accepting answers observed least people application-speciﬁc parameter set. practice prior work deems answers valid using blind trust well conservative answer validation schemes measuring answer diversity turn question much answer diversity observed practice visual questions. across visual questions tally many visual questions yield unique valid answers enrich analysis measured answer diversity examining inﬂuence different levels trust crowd well inﬂuence different datasets. speciﬁcally tally number unique valid answers observed requiring minimum members crowd offer answer answer valid. conduct analysis datasets visual questions real images abstract scenes independently. three unique answers across crowd trust levels datasets gives upper bound expected answer diversity. anticipate measured answer diversity drop less stringent answer agreement schemes exact string matching inferring agreement answer synonym plurality another answer. results show trend amount answer diversity three agreement thresholds datasets commonly unique answer followed three answers respectively. addition expected moving requiring answer agreement conservative agreement three people shifts overall distribution sharply peak less overall diversity results also show statistics transfer source images another revealing possible beneﬁt using artiﬁciallytable correlation answer agreement visual questions elicit three different types answers vqas real images abstract scenes. shown answer type percentage visual questions lead disagreement unanimous agreement exactly disagreement crowdsourced answers. average across answer types datasets crowd agrees answer nearly half vqas. moreover observe crowd disagreement arises often three answer types highlighting signiﬁcance crowd disagreement applicable across various types visual questions. figure highlights various reasons crowd workers disagree answer. disagreements arise crowd worker skill difﬁcult task necessitates domain expertise crowd worker inadequately answer seemingly simple question crowds disagree also ambiguity question visual content reasons disagreement include insufﬁcient visual evidence answer question subjective questions synonymous answers varying levels answer granularity capitalize observations next section design prediction systems automatically separate visual questions lead agreement. enrich understanding crowds disagree examining frequently crowds agree respect visual questions elicit different types answers. tally number visual questions lead yes/no number other answers. report results crowds unanimously agree well nine people agree datasets results capture untrusted result permitted crowd inferring whether crowd agrees. overall observe disagreement real images abstract scenes. disagreement arises often types answers highlighting interest crowd disagreement widespread interest many types visual questions different datasets. observe similar crowd agreement trends across datasets three answer types high agreement yes/no images datasets. hypothesize remaining quarter asked yes/no questions lead greater amounts disagreement subjective questions lead split opinions among crowd observe moderate agreement levels other visual questions possibly greater diversity opinions regarding true answer well ways express concept. greatest difference results datasets number visual questions. hypothesize counting problems easier less complex images show objects consistently case abstract scenes real images. fig. summary answer diversity outcomes showing frequently different numbers unique answers arise asking crowd workers answer visual question visual questions real images visual questions abstract scenes. results shown based different degrees answer agreement required make answer valid person offer answer least people must agree answer least three people must agree answer. visual questions often elicit exactly answer visual question also regularly elicit three different answers visual question. visual questions real images found visual questions valid answer limiting valid answers least three people agreeing respectively. suggests crowd able reach level consensus acceptable answers vast majority visual questions. fig. illustration visual questions lead humans agree single answer. observed unanimous answer agreement arises images simple questions precise questions visually grounded. answer disagreement arises variety reasons expert skill needed human mistakes ambiguous question ambiguous visual content insufﬁcient visual evidence subjective question answer synonyms varying answer granularity. explore following questions given novel visual question machine correctly predict whether multiple independent members crowd would supply answer? insights machinelearned system reveal regarding humans likely agree about? prediction systems pose prediction task binary classiﬁcation problem. speciﬁcally given image associated question system outputs binary label indicating whether crowd agree answer. goal design system detect visual questions assign disagreement label regardless disagreement cause implement random forest deep learning classiﬁers. answer agreement labels visual question assigned either answer agreement disagreement label. assign labels employ crowdsourced answers visual question. visual question assigned answer agreement label exact string match crowdsourced answers answer disagreement label otherwise. rationale permit possibility careless/spam answer visual question. outcome labeling scheme disagreement label agnostic speciﬁc cause disagreement rather represents many causes random forest system ﬁrst system domain knowledge guide learning process. compile features hypothesize inform whether crowd arrive undisputed single answer. apply machine learning tool reveal signiﬁcance feature. propose features based observation answer agreement often arises person’s attention easily concentrated single undisputed region image person would requested task easy address. employ image-based features coming salient object subitizing method produces probabilities indicate whether image contains salient objects. intuitively number salient objects shows many regions image competing observer’s attention correlate ease identifying region interest. moreover hypothesize feature capture observation previous study counting problems typically leads disagreement images showing many objects agreement otherwise. employ -dimensional feature vector represent question-based features. feature number words question. intuitively longer question offers information hypothesize additional information makes question precise. remaining features come one-hot vectors describing ﬁrst words question. one-hot vector created using learned vocabularies deﬁne possible words ﬁrst second word location question respectively intuitively early words question inform type answers fig. precision-recall curves average precision scores benchmarked systems. random forest deep learning classiﬁers outperform related automated baseline showing importance modeling human disagreement opposed system uncertainty. examples prediction results top-performing classiﬁer. shown three visual questions conﬁdently predicted instances lead answer disagreement agreement along observed crowd answers. examples illustrate strong language prior making predictions. might possible turn possible reasons/frequency answer disagreement. example expect regularly elicit many opinions disagreement. intuition beginning words question also supported analysis previous section shows different answer types yield different biases eliciting answer agreement versus disagreement. leverage random forest classiﬁcation model predict answer agreement label given visual question. model consists ensemble decision tree classiﬁers. train system learn unique weighted combinations aforementioned features decision tree applies make prediction. test time given novel visual question trained system converts feature descriptor visual question ﬁnal prediction reﬂects majority vote prediction ensemble decision trees. system returns ﬁnal prediction along probability indicating system’s conﬁdence prediction. employ matlab implementation random forests using trees default parameters. deep learning system next adapt deep learning architecture learn predictive combination visual textual features. question encoded -dimensional lstm model takes onehot descriptor word question. image described -dimensional output last fully connected layer convolutional neural network system performs element-wise multiplication image question features linearly transforming image descriptor dimensions. ﬁnal layer architecture softmax layer. train system predict agreement labels training examples example includes image question. test time given novel visual question system outputs unnormalized probability indicating belief agreement disagreement label. system’s prediction convert belief disagreement label normalized probability. consequently predicted analysis prediction system describe studies assess predictive power classiﬁcation systems decide whether visual questions lead answer agreement. capitalize today’s largest visual question answering dataset evaluate prediction system includes visual questions real images. these visual questions kept training remaining visual questions employed testing classiﬁcation system. separation training testing samples enables estimate well classiﬁer generalize applied unseen independent visual questions. knowledge prior work directly addressed predicting answer agreement visual questions. therefore employ baseline related algorithm produces given visual question answer conﬁdence score. system parallels deep learning architecture adapt. however predicts system’s uncertainty answer whereas interested humans’ collective disagreement answer. still useful baseline existing algorithm could serve purpose. classiﬁcation performance evaluate predictive power classiﬁcation systems based classiﬁer’s predictions visual questions test dataset. ﬁrst show performance baseline prediction systems using precision-recall curves. goals achieve high precision minimize wasting crowd effort efforts redundant high recall avoid missing collecting diversity accepted answers crowd. also report average precision indicates area precision-recall curve. values range better-performing prediction systems larger values. fig. precision-recall curves average precision scores random forest deep learning classiﬁers different features visual questions lead three answer types. figure shows precision-recall curves prediction systems. proposed classiﬁcation systems outperform algorithm baseline; e.g. yields percentage point improvement respect interesting shows value learning disagreement task speciﬁcally rather employing algorithm’s conﬁdence answers. generally results demonstrate possible predict whether crowd agree single answer given image associated question. despite signiﬁcant variety questions image content despite variety reasons crowd disagree learned model able produce quite accurate results. observe random forest classiﬁer outperforms deep learning classiﬁer; e.g. ours yields three percentage point improvement respect consistently yielding improved precision-recall values ours lstm-cnn general deep learning systems hold promise replace handcrafted features pick discriminative features. baselines highlight possible value developing different deep learning architecture problem learning answer disagreement applied predicting answers visual questions. show examples prediction results topperforming classiﬁer makes conﬁdent predictions examples predictor expects human agreement what room... visual questions disagreement why... visual questions. examples highlight classiﬁer strong language prior towards making predictions discuss next section. predictive cues explore makes visual question lead crowd answer agreement versus disagreement. examine inﬂuence whether visual questions lead three types answers random forest deep learning classiﬁcation systems. enrich analysis examining predictive performance classiﬁers trained tested exclusively image question features respectively. figure shows precision-recall curves classiﬁcation systems question features alone image observe change predictive performance random forest classiﬁer instead training model agreement label assigned visual question answers match. words difference predictive power labels examples nine people agree person disagrees. comparing scores observe predictors yield greatest predictive performance visual questions lead other answers followed number answers ﬁnally yes/no answers. possible reason ﬁnding question wording strongly drives whether crowd disagree other visual questions whereas notion common sense required learn whether crowd agree yes/no visual questions observe question-based features yield greater predictive performance image-based features visual questions comparing scores classiﬁcation results fact image features contribute performance improvements random forest classiﬁer visual questions lead number answers illustrated comparing scores overall ﬁnding predictive power stems language-based features parallels feature analysis ﬁndings automated literature mean however image content predictive. work improving visual content cues agreement warranted. ﬁndings suggest random forest classiﬁer’s overall advantage deep learning system arises counting questions indicated higher scores example advantage initial higher precision also observed counting questions hypothesize advantage arises strength random forest classiﬁer pairing question prior imagebased features indicates number objects image. speciﬁcally expect many lead agreement small counting problems. capturing answer diversity less effort next present novel resource allocation system efﬁciently capturing diversity true answers batch visual questions. today’s status either uniformly collect answers every visual question collect multiple answers number determined external crowdsourcing conditions system instead spends human budget predicting number answers collect fig. propose novel application predicting number redundant answers collect crowd visual question efﬁciently capture diversity answers visual questions. batch visual questions system ﬁrst produces relative ordering using predicted conﬁdence whether crowd would agree answer then system allocates minimum number annotations visual questions extra available human budget visual questions conﬁdently predicted lead crowd disagreement visual questions show results system related algorithm today’s status random predictions. boundary conditions answer answers visual questions. approach typically accelerates capture answer diversity today’s status selection; e.g. answer diversity answer diversity. translates saving -hour work weeks assuming seconds answer. suppose budget allocate collect extra answers subset visual questions. system automatically decides visual questions allocate extra answers order maximize captured answer diversity visual questions. system accrue additional costs delays collecting extra answers extra responses provide information. towards system involves three steps collect answers visual questions first system applies topperforming random forest classiﬁer every visual question batch. then system ranks visual questions based predicted scores classiﬁer visual questions conﬁdently predicted lead answer agreement crowd conﬁdently predicted lead answer disagreement crowd. finally system solicits human answers visual questions predicted reﬂect likelihood crowd disagreement fewer human answers remaining visual questions. details below. experimental design evaluate impact actively allocating extra human effort answer visual questions function available budget human effort. speciﬁcally range budget levels compute total measured answer diversity resulting batch visual questions. goal capture large amount answer diversity little human effort. conduct studies test visual questions real images visual question establish true answers unique answers observed twice crowdsourced answers visual question. require agreement workers avoid possibility careless/spam answers treated ground truth. system implementation collect either minimum answer visual question maximum answers visual question. number answers roughly aligns existing crowd-powered systems example vizwiz average participants received answers question maximum number answers also supports possibility capturing maximum three unique valid answers typically observed practice elaborate schemes distributing responses possible show approach already proves quite effective experiments. simulate answer collection randomly selecting answers crowd answers visual question. system randomly prioritizes images receive redundancy. predictor illustrates best user achieve today crowd-powered systems current dataset collection methods represents true answers i-th visual question represents unique answers captured answers collected i-th visual question represents unique answers captured answers collected j-th visual question. given extra human budget total diversity comes second term indicates diversity captured answers collected every visual question. given maximum available extra human budget total diversity comes ﬁrst term indicates diversity captured answers collected every visual question. given partial extra human budget perfect predictions minimum number answers allocated visual questions true answer diverse answers safely captured. measure diversity visual question number true answers collected visual question larger values reﬂect greater captured diversity. motivation measure give total credit visual questions valid unique human answers collected. results system consistently offers signiﬁcant gains today’s status approach example system accelerates collection diversity status baseline. addition system accelerates collection diversity would observe vizwiz absolute terms means eliminating collection answers loss captured answer diversity. translates eliminating -hour work weeks saving assuming workers paid answer take seconds answer visual question. approach ﬁlls important crowdsourcing answer collection literature targeting allocation extra answers visual questions diversity answers expected. figure also illustrates advantage system related algorithm novel application costsensitive answer collection crowd. observed relying algorithm’s conﬁdence answer offers valuable indicator today’s status passively budgeting. acknowledge method intended task speciﬁcally still serves important baseline attribute performance gains prediction system directly predicting whether humans disagree rather predicting property speciﬁc algorithm proposed problem predicting whether different people would answer response visual question. towards motivating practical implications problem analyzed nearly half million visual questions demonstrated nearly split visual questions lead answer agreement versus disagreement. observed crowd disagreement arose various types answers many different reasons. next proposed system automatically predicts whether visual question lead single versus multiple answers crowd. method outperforms strong existing system limited estimating system uncertainty rather crowd disagreement. finally demonstrated employ prediction system accelerate collection diverse answers crowd typically least today’s status ﬁxed redundancy allocation. authors gratefully acknowledge funding ofﬁce naval research national science foundation thank dinesh jayaraman yu-chuan suyog jain chao-yeh chen assistance experiments. bigham jayant little miller miller miller tatarowicz white white vizwiz nearly realtime answers visual questions symposium user interface software technology andreas rohrbach darrell klein learning compose neural networks question answering conference north american chapter association computational linguistics human language technologies antol agrawal mitchell batra zitnick parikh visual question answering ieee international conference computer vision malinowski fritz multi-world approach question answering real-world scenes based uncertain input advances neural information processing systems park berg berg visual madlibs fill blank image generation question answering ieee international conference computer vision burton brady brewer neylan bigham hurst crowdsourcing subjective fashion advice using vizwiz challenges opportunities sigaccess conference computers accessibility sheng provost ipeirotis another label? improving data quality data mining using multiple noisy labelers international conference knowledge discovery data mining http//www.bemyeyes.org/ eyes. lasecki thiha zhong brady bigham answering visual questions conversational crowd assistants sigaccess conference computers accessibility", "year": 2016}