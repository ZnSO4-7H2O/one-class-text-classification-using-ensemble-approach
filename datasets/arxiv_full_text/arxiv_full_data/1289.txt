{"title": "Self-taught learning of a deep invariant representation for visual  tracking via temporal slowness principle", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Visual representation is crucial for a visual tracking method's performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target's motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.", "text": "visual representation crucial visual tracking method’s performances. conventionally visual representations adopted visual tracking rely hand-crafted computer vision descriptors. descriptors developed generically without considering tracking-speciﬁc information. paper propose learn complex-valued invariant representations tracked sequential image patches strong temporal slowness constraint stacked convolutional autoencoders. deep slow local representations learned oﬄine unlabeled data transferred observational model proposed tracker. proposed observational model retains training samples alleviate drift collect negative samples coherent target’s motion pattern better discriminative tracking. learned representation online training samples logistic regression classiﬁer adopted distinguish target background retrained online adapt appearance changes. subsequently observational model integrated particle ﬁlter framework peform visual tracking. experimental results various challenging benchmark sequences demonstrate proposed tracker performs favourably several state-of-the-art trackers. visual tracking important research topics computer vision core many real-world applications. applications include human-computer interactions video surveillance robotics. need generality recent years seen rise online model-free visual tracking methods attempt learn appearance target object time without prior knowledge object. despite much research eﬀorts made visual tracking still regarded challenging task various appearance changes target object background distractions. illumination variations occlusion fast motion background clutters challenges visual tracking. typical visual tracking method dependent major components namely dynamic model observational model. dynamic model used model states state transition target object whereas observational model describes target object observations based certain visual representations. deal abovementioned visual tracking challenges recent tracking methods tend focus adopting developing eﬀective representations. however variants image representations scale invariant feature transform local binary patterns developed computer vision domain universally eﬀective wide-range vision tasks lack customizability. recent highly eﬀective approach better task-speciﬁc representations learn representations data itself. representation learning techniques seek bypass conventional labor-intensive feature engineering disentangling underlying explanatory factors observed input. thus representation learning main focus approach. objects video likely subject small transformations across frames content remains largely unchanged. work presented paper aims exploit temporal slowness principle learn image representation change slowly time thus making robust local transformations. making amount unlabeled tracked sequential data generic local features invariant transformations commonly found tracking tasks learned oﬄine. complex-cell-like autoencoder model temporal slowness constraint proposed learning separate representations invariances transformations image sequences. learn complex invariances deep learning model formed training second autoencoder convolved activations ﬁrst autoencoders larger image patches. components illustrated fig. firstly fig. tracked image patches used train deep stacked autoencoders temporal slowness constraint trained stacked autoencoders transferred adaptive observational model visual tracking based certain conditions tracking observational model updated online account appearance changes. fig. describes steps observational model update whereby logistic regression classiﬁer trained accumulative training features obtained transferred stacked autoencoders. fig. tracking performed sampling tracking candidates particle ﬁltering. learned representation trained logistic regression candidate highest predicted probability chosen target object. main contributions paper present autoencoder algorithm learn generic invariant features oﬄine visual tracking. train model perform tracking unlabeled sequential data obtain tracked image patches training data. transformation-invariant features learned enforcing strong temporal slowness between tracked image patches. subspace pooling construct complex-valued representation separates invariances transformations. another autoencoder layer construct stacked convolutional autoencoders model learning higher-level invariances. stacked autoencoders transferred visual tracking based self-taught learning paradigm learned representations propose adaptive observational model tracking. ﬁrst second layer stacked autoencoders transferred form ﬁnal tracking representation. better discriminative tracking proposed observational model equipped novel negative sampling method collects relevant negative training samples. besides alleviate visual drift propose simple technique observational model retain early recent training samples. integrate proposed adaptive observational model particle ﬁlter framework evaluate proposed tracker number challenging benchmark sequences comparing several state-of-theart trackers. results demonstrate proposed tracker performs favourably competing trackers. observational model also known appearance model undoubtedly crucial component visual tracking. section literature review done existing trackers terms common categories observational model namely generative discriminative generative approaches represent target object reconstruction error identiﬁes best matched candidate among many observations. adapt appearance changes target object recent generative tracking methods learn appearance object online. subspace learning methods learn expressive representations dimensional space. develop subspace learning-based tracker ross used principal component analysis construct incrementally update subspace model target object. liwicki formulated incremental kernel krein space learn nonlinear subspace representation tracking. account partial occlusion during tracking proposed canonical correlation analysis -based tracker considers correlations among sub-patches tracking observations. mixture models also used tracking jepson learned mixture model model appearance changes target object online expectation-maximisation algorithm. wang develop adaptive observational model joint spatial-color space using gaussian mixture model. although generative trackers work well certain circumstances inferior discriminative trackers dealing complicated environments. unlike generative trackers discriminative trackers take background information account distinguish target object background. collins selected color features online best discriminates target object current background. deal appearance changes grabner proposed online boosting classiﬁer adaptively selects discriminative features tracking. klein cremers introduced novel scaleinvariant gradient feature used boosting track target object eﬃciently. alleviate visual drift zhang song proposed tracking method based online multiple instance boosting handles ambiguously labeled samples weights positive sample diﬀerently update. besides online feature selection ensemble methods support vector machine -based tracking methods received much attention lately. tang trained multiple svms independent feature locates target object combining conﬁdence scores classiﬁers. reduce error selecting inaccurate samples updating observational model online hare presented structured output directly predict trajectory target object frames. discriminative tracking methods advantageous generally allow incorporation various kinds feature representations. however representations well formulated devastating tracking performances. representation learning emerging ﬁeld aims learn good representations input low-level representations. many representation learning techniques proven superior conventional hand-engineered representations. popular techniques representation learning dictionary learning deep learning. dictionary learning aims learn dictionary codebook atoms linearly combined well approximate given signal. considering absence object prior information existing model-free tracking methods wang performed sparse coding sift features extracted labeled object recognition datasets. learned sparse coded dictionary online unlabeled patches discriminates target object background regions using svm. deep learning relatively visual tracking research. deep learning deep layered architectures employed learn complex high-level representations terms less simpler low-level representations. using -means clustering jonghoon trained convolutional neural network oﬄine unsupervised manner tracking. method however maintains static observational model time. account object appearance changes wang yeung pre-trained stacked denoising autoencoders ﬁnetuned deep neural network online tracking. paper exploit temporal slowness learn generic invariant representations unsupervised transfer visual tracking. temporal slowness major priors representation learning successfully used object recognition tasks main motivation learning representation difﬁcult develop hand-craft exact feature extraction algorithm robust object transformations found videos. best knowledge paper ﬁrst attempt employ temporal slowness principle visual tracking. showed temporal slowness constraint simply added conventional autoencoder cost function learn invariant features also choose autoencoder representation learning model. although proposed tracker discriminative nature representations learned generative constraints. train. proposed autoencoder reconstruction cost replaces hard orthonormality constraint prevent feature degeneracy sparsity cost helps discover interesting features train autoencoder dataset consists number tracked patches formed number track sessions number frames track modiﬁed autoencoder number hidden units trained rp×d autoencoder weights weight temporal slowness cost subspace pooling representation image patch frame t-th track. l-norm minimization common achieve sparsity representation learning. progressively added autoencoder achieve temporally slow autoencoder algorithm. secondly details stacked convolution autoencoder constructed described. thirdly complex-valued representation formed subspace pooled autoencoder described visualized optimal stimuli learned features shown. lastly sequential dataset used preprocessing method obtain tracked training image patches explained. autoencoder weights activation function network biases subscripts indicate associations components encoder decoder respectively. generally latent representation learned hidden layer autoencoder regarded meaningful classiﬁcation purposes. conventional autoencoder mere reconstruction cost hardly learn useful latent representation data. allow autoencoder discover better representations many autoencoder regularization schemes denoising autoencoders contractive autoencoders proposed. however autoencoder variants static temporally uncorrelated object images learn features object recognition tasks. although approach borrowed directly visual tracking purpose contend image representation visual tracking learned speciﬁc task. propose adapt autoencoder learn invariant features tracked image sequences visual tracking. autoencoder model draws inspiration independent subspace analysis proposed learning motion invariance. unlike conventional sparse autoencoders enforce sparsity hidden layer proposed autoencoder performs subspace pooling hidden layer activations enforces sparsity pooling layer identical isa. learning features invariance achieved enforcing strong temporal slowness constraint minimize distance subspace pooling representations temporally correlated tracked image patches. preserve architectural properties biases nonlinear activation function biases omitted linear activation function chosen reconstruction cost furthermore encoder decoder weights tied reduce number free parameters second cost term minimize temporal representation diﬀerences l-norm allow invariance sparsely represented kind motion invariance represented small number features thus become specialized diﬀerent invariances. l-norm regularization applied third cost function term enforce sparsity subspace pooling layer indicates element-wise square operation subspace pooling matrix sums every adjacent features non-overlapping way. encoder diagram subspace pooling shown fig. subspace pooling successfully used temporal slowness feature learning techniques group similar features pooled units therefore achieving invariance. pairs every adjacent hidden units form complexvalued representation pair decomposed amplitude phase variables pooling units resemble complex-cells visual cortex amplitudes insensitive phase changes. proposed autoencoder temporal slowness constraint paired tracked patches illustrated fig. unlike relies inconvenient constrained optimization methods training cost function optimized eﬃciently using unconstrained optimization methods autoencoders commonly stacked form deep layered architectures learn higher-level representations low-level counterparts. paper stack train second autoencoder convolved features ﬁrst autoencoder greedy layer-wise training fashion convolutional learning architecture allows reuse ﬁrst layer features learn higher-level features bigger image patches. training second layer ﬁrst layer subspace pooling features densely extracted larger d-dimensional tracked image patches predeﬁned spatial stride step-size overlaps dense patches small. small computationally demanding overlap impedes feature performances severely. unlike object recognition tasks whereby small always subsequently convolved ﬁrst layer representation densely extracted patches ﬂattened feature vector large tracked image used training dataset train second layer. convolutional learning architecture method illustrated fig. bigger chosen reasonable number feature dimensions convolved representations thus dimensionality reduction technique required unlike strategy second layer shares autoencoder cost function ﬁrst layer diﬀer parameter settings cost term weights. formed. amplitude magnitude complex representation indicates degree presence feature invariant phase changes transformations. computed euclidean norm exactly does. amplitudes good invariant representation supervised classiﬁcations. second information obtained subspace pooled units phase. phase complex representation deﬁned interesting property learning subspace pooling temporal slowness allows phase pooled units shifted visualize transformations features invariant follow phase-shifting procedure linear combination ﬁlters visualize optimal stimuli features invariances learned stacked autoencoders. fig. shows representative optimal stimuli layers stacked autoencoders increment phase shift. shown ﬁrst layer learns gabor-like edge detectors invariant small location translations second layer learns features invariant complicated transformations out-of-plane rotation. features learned unlabeled data transferred supervised learning tasks whereby generating distribution unlabeled data diﬀerent labeled data. context paper features learned image sequences unrelated tracking benchmark sequences used gauge proposed tracker’s performances. setting analogous tracking algorithm exploits patch-level similarity transfers visual prior unlabeled dataset tracking tasks. harnessing diverse unlabeled dataset self-taught feature learning essential learned features expected generalize well unseen examples. deep learning-based tracking algorithm transfer features learned unlabeled datasets employ large dataset great amount visual diversity. paper large aggregation tracking benchmark sequences compiled klein objects scenes diverse terms appearances. importantly exhibit possibly variants tracking challenges occlusion out-ofplane rotation deformation scale variation. experimental benchmark sequences used section excluded uphold self-taught learning principle. employing tracker developed tracked image patches collected randomly sequences. chosen tracker arbitrary choice best performing tracker among trackers evaluated tracking benchmark sequences section excluding proposed tracker. learn feature invariances temporal slowness important tracked image patches accurately obtained. aﬃrm hypothesis randomly shuﬄed ﬁrst half collected tracked dataset disrupt temporal ordering subsequently adopted learning ﬁrst-layer features. thought rough simulation using weak performing tracker collect tracked patches. optimal stimuli learned ﬁrst-layer features shown fig. seen optimal stimuli made noisy patterns resemble sharp edge detectors fig. presence much uninteresting regions sequences ‘interesting’ regions performing tracking. interest point detection approaches ﬁrst considered. ﬁrst approach identifying motional pixels binary-thresholded accumulative diﬀerence pictures size ﬁlter used remove trivially small connected components among identiﬁed pixels. subsequently initial tracking regions chosen random frame numbers random spatial locations constraint initial regions must least small overlaps motional pixels. contrast second approach involves space-time harris interest point detection algorithm identiﬁes regions ‘interesting’ spatially temporally. qualitatively compare results approaches arbitrary short video segment shown fig. video segment contains running persons relatively unchanged background. noticeably stip outperforms ﬁrst approach latter takes account temporal diﬀerences ignoring much regions rich spatial information experimentally choose stip accumulative diﬀerence pictures because spatial temporal interestingnesses help learn good features good invariances respectively. higher number frames track advantageous lower number frames qualitatively evaluate features learned unlabeled dataset using frames/track frames/track settings respectively show fig. features learned frames/track setting incredibly similar frames/track setting fig. frames track setting interest points detected using number frames. object remains ‘interesting’ long period time likely multiple interest points object detected diﬀerent timesteps implicitly forms long sequence tracked patches. therefore using higher number frames track signiﬁcantly advantageous circumstance. example tracked patches track session shown fig. observational model paper discriminative adaptive. discriminative sense utilizes supervised binary classiﬁer classify tracking observations positive class negative class. also retrain classiﬁer periodically training samples keep observational model adapted appearance changes target object background time. observations represented using representations learned stacked autoencoders introduced previously. ﬁrst frame tracking preceding positive samples concatenated current tracking target form positive training set. therefore circumstance positive samples collected regions located pixels away target object. denote target object’s coordinate horizontal vertical axes respectively t-th frame. ﬁrst frame positive sample collected location larger number track sessions encourage diversity training dataset smaller number frames track session minimize occurences tracking drift dataset. although seem intuitive using dicted target object observation added positive sample replacing oldest positive sample set. considering fact early target observations likely reliable predictions tracker posiunlike positive samples single procedure collect negative samples frames. paper novel method collect negative samples proposed negative samples little overlapping regions target object somehow coherent particle ﬁlter’s dynamic model proposed tracker. rationale behind coherence tracker tracks fast-moving objects require negative samples farther target whereas tracker tracks slow-moving objects need negative samples. context visual tracking particle ﬁlter’s dynamic model generally requires normallydistributed translational aﬃne parameters model spatial translations target tribution’s standard deviations translational aﬃne parameters horizontal vertical axes respectively. t-th frame negative sample collected location second term speciﬁes maximum overlaps negative samples target object whereas third last term determines between negative samples target object. every frame training alleviate visual drift. time tracking negative samples positive samples present training causes class imbalance problem. address problem section training stacked autoencoders section ofﬂine transferred visual tracking. ﬁrst second layers stacked autoencoders used extract dense local features tracking observations training samples. performing feature extraction tracking observations normalized standard tracking template size local d-dimensional patches densely extracted tracking observations stride passed ﬁrst layer obtain ﬁrst-layer features. subsequently second-layer features densely extracted ﬁrst-layer feature stride finally convolved representations layers concatenated form ﬁnal representation visual tracking. relatively large convolution strides spatial pyramid pooling performed convolved representations. would greatly encourage translational invariance favourable visual tracking performing feature extraction positive negative samples training dataset approximate labels obtained. linear binary classiﬁer adopted distinguish target object background tracking. linear classiﬁers less prone overﬁtting high-level representations learned deep representations likely linearly separable. classiﬁer used logistic regression capability providing predictions probability estimates. probability estimates soft labels useful hard labels case want identify likely target object candidate among many candidates. taking account class imbalance problem training class-weighted logistic regression proposed weight parameter positive-class logistic cost weight parameter tributions classes inversely proportional respectively. additionally weight decay weight regularization added cost function penalize large weights therefore reducing overﬁtting. prediction stage trained logistic regression classiﬁer computes probability conﬁdence score follows maximum probability obtained threshold classiﬁer retrained current training set. small value allow fast update classiﬁer case abrupt appearance changes target object. without probability threshold update would often small trivial errors likely accumulated eventually causing tracking drift. account poorly diversiﬁed positive samples early frames maximum probfinally also circumstances whereby target object change much background changed. allow background information learned classiﬁer slow manner classiﬁer retrained target states time steps. model-free visual tracking using rectangular bounding employ aﬃne transformation parameters state elements approximate motion target object frames. dynamic model formulated state element modeled independently normal distribution centered previous state visual tracking carried adaptive observational model section integrated object state estimation method. this particle ﬁlter chosen methods nonlinearity non-gaussian assumption capability maintaining multiple hypotheses. last part section contrast proposed tracking method reviewed state-of-the-art representation learning trackers. factor trackers performance dealing various tracking challenges. paper observation likelihood exponentially proportional conﬁdence score given periodically updated linear classiﬁer time exponentiation penalize low-weighted particles less likely chosen particle resampling. excluding oﬄine training stacked autoencoders temporal slowness high-level summary proposed tracker given algorithm refer proposed tracker deep slow tracker proposed tracking method similar diﬀerent reviewed representation learning trackers ways. terms datasets used training representation learning models trains datasets unrelated tracking video sequences. allow learned ﬁlters speciﬁc tracking environment performs online training solely image patches sampled tracking sequences itself. datasets used generic unlabeled thus diﬀerent datasets speciﬁed object classes. however sake generality cannot assume objects real-world applications share similar appearances limited object classes. besides dataset used diﬀerent others sense train stacked autoencoders tracked image patches instead temporally uncorrelated object recognition datasets used trackers image patches learn representations from except extracts sift features patches bases build sparse coded dictionary. terms observational models adaptivity tracker uses non-adaptive oﬄine classiﬁer distinguish target object object. including employ linear classiﬁers independent representation learning models build adaptive observational models. wang yeung uses sophisticated classiﬁcation tracking ﬁne-tuning deep neural network using classiﬁcation error cost function. results nonlinear classiﬁer. supervised ﬁne-tuning applied proposed autoencoder advantages temporal slowness constraint might diminished result minimizing classiﬁcation errors. section describe implementation details parameter settings along experimental setups tracking experiments. tested several challenging sequences state-of-the-art trackers. present results experiments quantitative qualitative means. sequential training datasets tracked patches trains tracked patches. number track sessions number frames track next weight parameters temporally slow autoencoder cost function ﬁrst second layer respectively. convolved representation training second layer ﬁrst layer features densely extracted tracked patches spatial stride tracking observations spatial stride second layer features densely extracted ﬁrst layer feature spatial stride unconstrained optimization autoencoders employ oﬀ-the-shelf limited-memory broydenfletcher-goldfarb-shanno algorithm relatively memory-eﬃcient fast-converging optimization process stops reaches ﬁxed number iterations paper. adaptive observational model training samples binary classes collected online train linear classiﬁer. parameter collect ﬁrst-frame positive samples constant multiplier collect negative samples alleviate tracking drift positive samples early frames recent retained current training set. likewise negative samples recent retained number negative samples collected frame. classiﬁer update frequency parameters respectively. parameter allow quick observational model update probability threshold conﬁgured parameter settings ﬁxed sequence bird board bolt cardark cliﬀbar coke crossing david deer dollar faceocc football football jumping mountainbike shaking singer surfer tiger trellis walking woman average evaluate challenging benchmark sequences. part benchmark sequences compiled wang babenko comprehensive evaluation sequences include various challenges visual tracking fast motion illumination variation cluttered background occlusion pose variation object deformation. compare dst’s performances state-of-the-art trackers. competing trackers adaptive structural local-sparse appearance tracker compressive tracker deep learning tracker incremental visual tracker online discriminative feature selection tracker partial least squares tracker sparse prototypes tracker tracking-learning-detection experiments based codes provided authors. asla builds eﬃcient incremental sparse appearance model takes account structural information target object. using random measurement matrix generates compressed representation haar-like features performs tracking discriminatively. especially relevant deep denoising autoencoders learn compact representation online tracking. uses novel incremental approach generatively learn updatable subspace representation online. odfs performs online feature selection using weak classiﬁers maximize conﬁdence positive samples. hand makes binary training samples learn low-dimensional sequence bird board bolt cardark cliﬀbar coke crossing david deer dollar faceocc football football jumping mountainbike shaking singer surfer tiger trellis walking woman average discriminative subspace representation. introduces sparsity trivial templates generative subspace learning explicitly handle occlusion motion blur. meticulously combines detection tracking learning components framework long-term tracking. contrast visual tracking literatures test recent state-of-the-art trackers instead earlier ones. tracking results transcribed sequences viewed http//www.youtube.com/user /deepslowtracker/videos. particle ﬁlter-based trackers aﬃne parameter settings particle ﬁlter’s dynamic model heuristically chosen according target object’s nature benchmark sequence. grid-search deliberate optimization done obtain settings. fair comparisons conﬁgured share aﬃne parameter settings number particles. trackers employ particle ﬁlter best setting object search window parameter possible values sequence. since carries object detection densely image window search parameter tuned. parameters feature-related training sample collection parameters left default settings ﬁxed sequences like done dst. finally trackers initialized target object locations. tracking result computed area area error obtained computing euclidean distance pixels center since evaluated trackers carry random sampling trackers times sequence median results. median results obtained adding max-min normalized inverse errors trials. average errors shown table respectively. achieves best second best performance sequences terms errors. numerically fares better deep learning-based competitor many sequences. understand performances trackers time error plots tested sequences presented fig. qualitative evaluation choose number sequences benchmark sequences presented section sequences chosen span across typical tracking challenges representative sequence kind challenge. occlusion coke sequence target object undergoes partial occlusion full occlusion out-of-plane pose change illumination change. asla odfs lost track target coming heavy partial occlusion notable performs better well perform poorly holistic representations robust partial occlusion. highly invariant representation learned temporal slowness deal well pose change. furthermore training accumulation technique helps alleviate post-occlusion drift. however long full occlusion recover fully retain initial target appearance model well particle conﬁdence scores remain high much time. target object woman sequence deformable woman ﬁgure heavily occluded cars. drifts away background cluttered ﬁrst occlusion deciding point trackers make point track target well till end. overall asla perform well sequence. unlike generative trackers holistic asla able handle moderate partial occlusion part-based sparse representation. even though loses tracker object heavy partial occlusion detector component help recover object later appears unoccluded illumination variation fig. shows tracking results sequences illumination variation. sequence target undergoes illumination scale changes. although employs particle ﬁlter tracking perform well scale changes unstable incremental training deep neural networks. generative representations perform well attributed fact robust illumination changes generative trackers mainly invariant features extracted background training samples allowing relatively unchanged background candidates rejected even though target’s appearance changed. trellis sequence pose changes well frequent illumination changes target. trackers except asla drift away target experience illumination change sunlight besides illumination change tracker eﬀectively deal out-of-plane pose change novel online negative sampling method collects dipose variation deformation fig. shows tracking results bird sequence target bird walks back forth undergoing pose variation partial occlusion that target deformable object requires rectangular tracking bounding boxes include much background region. generative trackers fail track heavy partial occlusion objects similar appearances. remain accurate object undergoes large out-of-plane rotation asla recovers certain parts target become recognizable regards early appearance. uses non-adaptive appearance model second particle ﬁltering step causes drift signiﬁcant appearance change pose change occurs. discriminative nature observational model robust partial occlusion similar objects whereas invariant amplitude features learned stacked autoencoders help tracker deal pose changes. bolt sequence target object athlete sprints track undergoing shape deformation gradual pose variation. bolt sequence challenging among tested sequences except drift away target beginning performs best sequence tracks target well end. success sequence attributed highly descriptive representation formed convolutionally trained second-layer features edge detector-like ﬁrst-layer features. cluttered background cardark sequence contrast background foreground well illumination changes. odfs fail make illuminationsensitive haar-like features base features perform update every frame accumulates tracking errors easily. tld’s optical ﬂow-based tracking causes gradual drift highly cluttered environment able revert object detection mode. situations appearances target background change much perform well updated slower manner depending maximum conﬁdence score tracking observations. fig. shows representative tracking results football sequence target object helmeted head football athelete. sequence tough background cluttered atheletes similar appearances athete undergoes motion blur signiﬁcant pose change overall asla perform well. asla’s novel alignment pooling makes less prone drifting problem similar objects around. handling motion blur explicity avoids updates performs better ivt. able achieve similar result using accumulation training samples weaken contribution training samples. bird board bolt cardark cliﬀbar coke crossing david deer dollar faceocc football football jumping mountainbike shaking singer surfer tiger trellis walking woman average research relies hand-crafted representations sift lbp. main contribution paper take alternative approach learning features visual tracking. learn slow invariant features oﬄine stacked autoencoders transfer online visual tacking. isolate merits proprosed tracking representation substituting dense sift local descriptors. components proprosed tracker remain same. obtain hand-crafted descriptors tracking reputable computer vision library package known vlfeat sift spatial stride ﬁrst layer autoencoder best width sift spatial bins possible values close ﬁrst layer’s input size. then best setting terms cell sizes. settings yield best overall results selected evaluation. parameters hand-crafted feature extractions left default settings. experimentally evaluate handcrafted representation trackers proprosed tracker benchmark sequences average errors sequence shown table table shown comparable trackers substituted hand-crafted representations. well-rounded tracker achieving best results terms average error. advantage proposed representation hand-crafted representations especially evident bird bolt football shaking surfer sequences large pose changes prevalent. tracking results sequences fast motion. objects fast motion tend generate motion blur images captured common cameras. target object deer sequence head quickly moving deer abrupt location change motion blur. overall trackers able track target heuristically chosen aﬃne settings ﬁne-tuned search window settings. furthermore always contrast target background experiences slight drift attributed fact static appearance model consider appearance variations target background time. drifts confusingly similar background object cannot track realiably switch detection mode. jumping sequence target object undergoes large translation frames exhibits signiﬁcant motion blur. nearly discriminative trackers except perform poorly rely negative sampling methods consider target’s motion pattern. works reasonably well learning object appearance cautiously estimation false negatives false positives. without considering negative templates generative trackers track target well. succeeds sequence virtue novel negative sampling method adapts particle ﬁlter’s dynamic model. enforcing strong temporal slowness constraint ﬁrst second layer autoencoders. understand importance temporal slowness visual tracking train stacked autoencoders varying temporal slowness weight evaluate learned representations visual tracking tasks. autoencoder layers assessed independently ﬁxing temporal slowness weight layer varying slowness weight another. default settings autoencoder parameters follow settings varying temporal slowness strengths obtained choosing relevant values lower default parameter settings. ﬁxing sparsity reconstruction weights reduction temporal slowness forces autoencoders rely sparsity reconstruction learn features. experimentally tracking representation substituted representations varying temporal slowness strengths evaluated benchmark sequences. plots errors varying temporal slowness strengths ﬁrst second layer presented fig. plots demonstrate tracking performance improves increased temporal slowness strength either layer proposed stacked autoencoders. understand eﬀects varying temporal slowness strengths learned features visually present phase-shifted optimal stimuli varying fig. worth noting lower temporal slowness strength produces features invariant limited transformations transitions transformations sudden unsmooth. conversely round features learned higher temporal slowness strengths fig. well ﬁrst layer’s second layer’s online negative sampling online negative sampling important components discriminative tracking. propose novel negative sampling method takes account maximum overlap target object negative samples. moreover negative sampling method adapted motionpattern target objects according aﬃne parameter settings particle ﬁlter. part compare proposed negative sampling method existing methods employed dlt. draws negative samples annular region deﬁned inner outer radius inner radius radius circle minimally enclosing target outer radius ﬁxed parameter. negative samples collected locations sampled zero-mean normal distribution standard deviation ﬁxed parameter multiplied height width target object. generally collects negative samples without overlap collects overlapping negative samples near. fair assessment substitute negative sampling method methods parameter settings. name tracker negative sampling method negative sampling method experimentally evaluate proposed tracker coke faceocc jumping shaking challenging benchmark sequences. error plots presented fig. comparisons include tracking results trackers. noticeably proposed negative sampling method superior comparable overall. performs better faceocc abrupt lighting appearance changes. sequence abrupt appearance changes fast motion better comparable even without proposed negative sampling method performs better original counterparts sequences whereas performs signiﬁcantly better sequences abrupt appearance changes eﬀective deep slow representation. work exploits temporal slowness principle learn invariant representations visual tracking. temporal slowness constraint incorporated autoencoder algorithm facilitate representation learning. allow learned representations speciﬁc visual tracking tasks large number tracked image patches collected existing tracker training set. deep learning model formed stacking autoencoders learn higher-level invariances temporal slowness. transfer oﬄine learned representations observational model online visual tracking. adaptive observational formulated collects relevant negative samples utilizes accumulative training alleviate tracking drift. tracking carried particle ﬁlter framework estimate target state sequentially. compared several state-of-the-art trackers proposed tracker demonstrates favourable performances challenging benchmark sequences various tracking challenges. future work explore possibility online learning representations using temporal slowness visual tracking without relying selftaught learning paradigm", "year": 2016}