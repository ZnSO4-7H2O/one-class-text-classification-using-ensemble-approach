{"title": "Kernel Implicit Variational Inference", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.", "text": "jiaxin shi∗† †department computer science technology brain tsinghua university ‡department computer science university toronto shijxmails.tsinghua.edu.cn ssycs.toronto.edu dcszjtsinghua.edu.cn recent progress variational inference paid much attention ﬂexibility variational posteriors. promising direction implicit distributions i.e. distributions without tractable densities variational posterior. however existing methods implicit posteriors still face challenges noisy estimation computational infeasibility applied models high-dimensional latent variables. paper present approach named kernel implicit variational inference addresses challenges. know ﬁrst time implicit variational inference successfully applied bayesian neural networks shows promising results regression classiﬁcation tasks. bayesian methods playing vital roles machine learning providing principled approach generative modeling posterior inference preventing over-ﬁtting becomes common practice build deep models many parameters even important bayesian formulation capture uncertainty models. example bayesian neural networks shown promise reasoning model conﬁdence learning labeled data. another recent trend incorporate deep neural networks powerful function mapping random variables bayesian network deep generative models like variational autoencoders except simple examples bayesian inference typically challenging variational inference standard workhorse approximate true posterior traditional focuses factorized variational posteriors analytical updates recent progress ﬁeld drives stochastic differentiable amortized rely analytical updates anymore factorized posteriors still commonly used variational family. greatly restricts ﬂexibility variational posterior especially highdimensional spaces often leads biased solutions true posterior usually factorized thus family. works improve ﬂexibility variational posteriors borrowing ideas invertible transformation probability distributions works important transformation invertible ensure transformed distribution tractable density. although utilizing invertible transformation promising direction increase expressiveness variational posterior argue ﬂexible variational family constructed using general deterministic stochastic transformations necessarily invertible. common result variational posterior tractable density despite sample kind distribution called implicit distributions variational methods implicit variational posterior wild variational approximations refer implicit variational inference existing implicit methods rely discriminator produce estimates variational objective gradients. pointed many them estimates often noisy lead unstable training. besides discriminator-based approaches computationally infeasible applied nontrivial bnns. paper present approach named kernel implicit variational inference addresses noisy estimation problem previous works providing principled tuning bias-variance tradeoff. furthermore kivi rely discriminator thus computationally feasible models high-dimensional latent variables kivi applicable global local latent variable models demonstrated experiments bnns vaes. know ﬁrst time implicit successfully applied bnns shows promising results regression classiﬁcation tasks. background consider generative model denote observed latent variables respectively. variational distribution parametric family chosen approximate true posterior optimizing evidence lower bound denotes kullback-leibler divergence klp) objective lower bound log-likelihood since written klp). maximum objective achieved challenge using implicit calculating klp) requires evaluating density intractable implicit distribution. recently inspired probabilistic interpretation generative adversarial networks works extend approach posterior inference latent variable models methods implicit variational family thus categorized implicit methods. observations density ratio estimated samples distributions probabilistic classiﬁer called discriminator. ﬁrst assign class labels samples class samples class given equal class prior density ratio given point calculated qφ/p ratio class probabilities given data point. estimate this discriminator trained classify classes logistic loss outputs probability class given sufﬁciently ﬂexible optimal solution therefore divergence term elbo approximated klp) log)] called prior-contrastive forms husz´ar note ratio approximation change gradients approximation accurate shall later though incorporating discriminative power probabilistic model shown great success gans method still suffers challenging problems applied noisy density ratio estimation variational posterior gets updated iteration. shown discriminator trained optimum update. however practice inner loop training discriminator often truncated several iterations. beginning inference procedure hard discriminator catch variational posterior. noisy signal produced discriminator leads noisy gradients thus unstable training. besides even discriminator quickly achieves optimum small number iterations still another issue. notice training loss expectations. practice using samples distributions approximate support distributions high-dimensional given limited number samples variance estimate considerable i.e. discriminator tends overﬁt samples. phenomenon discriminator arrives state samples easily distinguished probabilities given discriminator near commonly observed experiments computationally infeasible high dimensional latent variables density ratio estimated discriminator samples distributions latent variables however typically used neural network discriminator cannot afford high-dimensional inputs address challenges implicit propose replace discriminator kernel method dre. advantages method closed-form solution allows explicitly tradeoff bias variance tuning regularization coefﬁcient. estimating term speciﬁcally latent variable true density ratio qφ/p. consider modeling function reproducing kernel hilbert space induced positive deﬁnite kernel similar kernel ridge regression objective composed squared loss regression plus penalty complexity function. squared loss choose form used unconstrained least square importance fitting number samples respectively. note expectation squared loss taken w.r.t. resulting form estimated without evaluating density distributions. penalty term complexity measured rkhs norm ˆrh). putting together ﬁnal objective median heuristic approximate density ratio function monte carlo estimate klp) constructed note constraint estimated density ratio non-negative. however involve optimization objective order closed-form solution indicates post-processing needed ensure property. solve issue clipping estimated density ratio. clipping values searched experiments found algorithm sensitive clipping value. accurate estimation guaranteed global optimum rkhs universal family kernels used reverse ratio trick another technique essential improve estimation term call reverse ratio trick. observation expectation squared loss taken w.r.t. whereas expectation term taken w.r.t. unless match well probabilities small squared loss always mean good estimate. solution simple trick. instead estimating choose estimate denote estimated reverse density ratio ˆrpq corresponding estimate ˆrpq. note squared loss changes probability measure term’s. shall experiments trick essential make estimation sufﬁciently accurate gradient computation consider estimate gradients term w.r.t. variational parameters first easy prove husz´ar detailed proof appendix indicates true gradients term w.r.t. density ratio function. replace ratio right side ˆrpq ∇φklp) −∇φeqφ ˆrpq. note without cannot approximation since zero gradients w.r.t. then reparameterization trick used constructed closed-form estimate term show gradients estimated reparameterization trick. note reparameterization trick also used compute gradients reconstruction term thus applied elbo. algo. complete algorithm. note number samples used reconstruction term different required estimation reduced model expensive thus compared normal reparameterized extra computational cost mainly calculating inverse matrix shall experiments tens hundred samples sufﬁcient obtain stable estimate added cost high. sample prior sample variational posterior compute density ratio ˆrpq clip ˆrpq positive zqs. ˆkl. compute estimate reparameterization trick. gradient ascent ∇φl. parameter learning gradient ascent ∇θl. kivi addresses challenges stated sec. first ratio estimates given closed-forms thus problem catching second bias-variance trade-off estimation controlled regularization coefﬁcient smaller estimation aggressive match samples. larger estimated ratio function smoother. choosing appropriate variance gradients controlled compared extreme ratio estimates given discriminators output probabilities near moreover kivi directly applicable global local latent variable models advantage nonparametric methods like particle mirror descent stein variational gradient descent task training local lvms like vaes additionally adaptive contrast technique whose details summarized appendix present example using kivi bayesian neural networks received increasing attention ability model uncertainty important factor many tasks adversarial defense reinforcement learning. however despite removed need discriminator still nontrivial apply kivi bnns need design implicit posterior outputs high-dimensional samples latent variables existing implicit posteriors based traditional fully-connected neural networks cannot handle high-dimensional output space. present matrix multiplication neural network efﬁcient architecture sampling large matrices. deploying mmnn kivi easily scale large bnns. bnns prior speciﬁed neural network parameters {wl}l weights l-th layer. given input output modeled output feed-forward network distribution parameterized regression usually gaussian mean. classiﬁcation usually discrete distribution unnormalized probabilities. however previous methods used variational posteriors limited capacity including factorized gaussian matrix variate gaussian normalizing ﬂows enabled learn implicit variational posteriors propose adopt general distribution without explicit density function form transformation parameterized treated samples challenge high dimensional moderate size neural networks. thus often cannot fully connected neural network inspired low-rank matrix factorization propose kind network called matrix multiplication neural network serve shown alg. layer mmnn input matrix left multiplied right rameter matrix multiplied parameter matrix finally passed nonlinear activation call layer mout×nout matrix multiplication layer. modeling matrix mmnn signiﬁcant computational advantages mlps low-rank property. example model weight matrix consider single-layer mmnn input matrix size parameters needed total size single fully-connected layer used parameter size much larger. thus mmnn variational posterior normal-size neural networks. tasks small networks still work closely relates works implicit generative models density ratio estimation igms drawn much attention popularity gans. general learning algorithms implicit models surveyed mohamed lakshminarayanan plays central role. connection gans also discussed uehara comprehensive review refer readers survey also refer readers many works sugiyama collaborators kliep lsif based bregman divergence work also builds upon recent methods including stochastic approximation mini-batches direct gradient optimization variational lower bounds reparameterization trick training continuous lvms following success learning igms implicit distributions applied many based discriminators divided categories prior-contrastive joint-contrastive methods discriminators distinguish samples prior variational posterior methods distinguish model joint distribution joint distribution composed data distribution variational posterior. concurrent husz´ar mescheder proposed adversarial variational bayes amortized version methods training local lvms like vaes. prior husz´ar similar ideas methods proposed bi-gan nonparametric methods svgd adapt particles towards true posterior also closely related implicit share similar advantage ﬂexible approximations. recently amortized version svgd developed idea applied mcmc shown turner core identity svgd could also employed approximate gradients implicit distributions. gaussian mixtures ﬁrstly conduct experiment approximate gaussian mixture distribution gaussian mixture distribution equally distributed unit-variance components whose means compare kivi using single gaussian posterior variational distribution used kivi generates samples propagating standard normal distribution two-layer hidden units layer output unit. shown gaussian posterior converges single mode. contrast kivi accurately approximate modes expressive variational posterior. -.±. -.±. -.±. boston -.±. -.±. -.±. concrete -.±. -.±. -.±. energy kinnm naval combined -.±. -.±. -.±. -.±. -.±. -.±. protein -.±. -.±. -.±. wine -.±. -.±. -.±. yacht -.±na year stated sec. latent variables bnns global data points usually high-dimensional ﬂexible variational family essential. compare kivi state-of-the-art methods regression classiﬁcation standard benchmarks. quantitatively measure predictive ability bnns kivi inference method standard multivariate regression benchmarks recent works probabilistic backpropagation compare state-of-the-art methods bayesian interpretation dropout stein variational gradient descent following setup bnns -unit hidden layer except large datasets i.e. protein structure year predication units used. randomly select whole dataset training leave rest testing. also gamma prior precision observation noise adaptively learn datasets batch size learning rate model trained epochs small datasets less data points epochs others. report mean errors standard deviations averaged runs except runs protein structure year predication msd. networks used tasks small scale mlps hidden layer implicit variational posterior table shows results best ones marked bold. results svgd dropout cited papers setting ours. kivi consistently outperforms svgd dropout rmse test-ll datasets. especially rmse kivi signiﬁcant improvements except wine year predication msd. suggests kivi enables implicit variational posterior capture predictive uncertainty network parameters hard fully described mixture delta distributions ﬁxed particles emphasize although nonparametric nature svgd also made approximation ﬂexible uses particles throughout inference procedure iteration kivi generates particles. thus implicit posterior learned kivi smoothed parametric model. recently normalizing ﬂows shown good performance bnns also experiment directly applying normalizing ﬂows task. results reported appendix figure results mnist classiﬁcation. left table shows test error rates. indicates results directly comparable ours used ensemble networks second part blundell changed prior scale mixture. plot right shows training lower bound mnist classiﬁcation prior-contrastive kivi. classiﬁcation present results mnist consists training images test images handwriting digits. compared datasets regression mnist much higher feature dimension introducing millions parameters even moderate-size networks brings challenges bnns. standard benchmark performance mnist improved many techniques convolution generative pre-training data augmentation etc. ensure fair comparison follow settings bayes-by-backprop focus improving performance ordinary mlps without using techniques. network structures used three mlps relu hidden layers layer sizes respectively. kivi used mmnns hidden matrix multiplication layers implicit posterior train epochs batch size initial learning rate annealed every epochs ratio used last samples training validation model selection. fig. summarizes results. kivi achieves better accuracy compared plain dropout bayes-by-backprop three types mlps. kivi even performs better bayes-by-backprop changed prior makes model ﬂexible ours. layer size result comparable ensemble networks dropconnect demonstrates implicit posterior learned account model uncertainty. also conduct experiments prior-contrastive method challenge applying posterior samples extremely high-dimensional discriminators like neural networks cause unaffordable computation cost. around this logistic regression discriminator experiment settings reported appendix g... training lower bounds methods plotted fig. beginning increase pace fails converge lower bound explosion kivi improves consistently. explosion mainly input discriminator hundreds thousands dimensions plain logistic regression cannot produce reliable density ratio estimates. also experiment layer size fail converge end. note pbp-mv used adaptive weight priors different common setting standard normal priors; former also additionally used variational dropout thus results directly comparable works discussed well ours. figure variational autoencoders gaussian posterior implicit posterior denotes fully-connected layer. used plain kivi respectively; training evaluation curves lower bounds statically binarized mnist. latent features observations output distribution modeling observations takes outputs neural network parameters. conduct experiments widely used datasets generative modeling binarized mnist celeba bernoulli distribution binarized mnist normal distribution celeba. variational lower bound form except replaced original parameterizes neural network outputs parameters normal distribution. mnist case hidden layers illustrated fig. form implicit posterior direct choice move stochastic noise output layer penultimate hidden layer illustrated fig. applying kivi crucial question expect using implicit posteriors training vaes. target could tighter lower bounds data log-likelihood algorithm searches larger variational family optimal lower bound. suggests however weak optimization larger space lead better test given optimization always arrives local optima. previously adversarial variational bayes shown results mnist comparing test plain trained using golden truths estimated annealed importance sampling however results reported model architectures used plain different leads concerns part change contributes improved likelihoods. adopt another setting better demonstrate gain implicit posteriors. observe improvement implicit posteriors objectives average much broader range posterior conﬁgurations. effect contributes larger search space contains tighter lower bound values also makes model better prevent overﬁtting. verify kivi keeps property conduct experiments statically binarized mnist dataset models prior knowledge problem typical setting leads overﬁtting. latent dimension model hidden layers size parameters kivi details found appendix shows training testing curves vaes without kivi. seen lower bound training testing curves plain much larger kivi indicates trained kivi less prone overﬁtting. epochs evaluate test test images using plain trained kivi. demonstrate kivi scales larger models trained network structure used dcgan celeba using kivi. latent dimension case implicit posterior constructed similar shown fig. bottom hidden layers symmetric decoder network visually check latent space learned kivi show reconstruction results linearly interpolating latent vectors real images epochs model converged. compared latent space learned epochs smoother interpolated images much sharper. interpolation results training process presented compared appendix also learned model generate images evaluate sample quality using fr´echet inception distance scores achieved epoch kivi respectively fact many efforts required make successfully train model celeba produce results shown ﬁgure. reported mescheder prior explicitly added discriminator kivi need much tuning need carefully design discriminator hyper-parameters clear meanings. present implicit method named kernel implicit variational inference provides principled tuning bias-variance tradeoff makes implicit computationally feasible models high-dimensional latent variables. successfully applied approach bayesian neural networks achieved superior performance regression classiﬁcation tasks. also demonstrate kivi applied learn local latent variable models like vaes. future work include applying method larger-scale networks improving kernel estimator further. work supported national natural science foundation china projects beijing natural science foundation tsinghua tiangong institute intelligent computing nvidia nvail program project siemens. yarin zoubin ghahramani. dropout bayesian approximation representing model uncertainty deep learning. international conference machine learning goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems jose miguel hernandez-lobato ryan adams. probabilistic backpropagation scalable learning bayesian neural networks. proceedings international conference machine learning martin heusel hubert ramsauer thomas unterthiner bernhard nessler g¨unter klambauer sepp hochreiter. gans trained time-scale update rule converge nash equilibrium. arxiv preprint arxiv. shohei hido yuta tsuboi hisashi kashima masashi sugiyama takafumi kanamori. statistical outlier detection using direct density ratio estimation. knowledge information systems diederik kingma salimans rafal jozefowicz chen ilya sutskever welling. improved variational inference inverse autoregressive ﬂow. advances neural information processing systems lars mescheder sebastian nowozin andreas geiger. adversarial variational bayes unifying variational autoencoders generative adversarial networks. arxiv preprint arxiv. simard steinkraus platt. best practices convolutional neural networks applied visual document analysis. seventh international conference document analysis recognition proceedings. ./icdar... casper kaae sønderby tapani raiko lars maaløe søren kaae sønderby winther. ladder variational autoencoders. advances neural information processing systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research masashi sugiyama shinichi nakajima hisashi kashima paul buenau motoaki kawanabe. direct importance estimation model selection application covariate shift adaptation. advances neural information processing systems masashi sugiyama taiji suzuki takafumi kanamori. density-ratio matching bregman divergence uniﬁed framework density-ratio estimation. annals institute statistical mathematics masatoshi uehara issei sato masahiro suzuki kotaro nakayama yutaka matsuo. generative adversarial nets density ratio estimation perspective. arxiv preprint arxiv. matthew zeiler sixin zhang yann fergus. regularization neural networks using dropconnect. proceedings international conference machine learning note although used derivation forms solution require invertible. fact zero eigenvalues objective bounded since always choose null space decrease words form solution well deﬁned condition optimal objective well deﬁned. although kivi gives rather good estimation density ratio estimation accuracy still degrades larger discrepancy problem critical local latent variable models like vaes variational model required infer posteriors local latent variables. order mitigate that adopted technique called adaptive contrast easily integrated kivi. introduces auxiliary tractable distribution resembles elbo rewritten gradients ﬁrst term w.r.t. easily computed using monte carlo gradients second term estimated using kivi. adaptive contrast gives better estimates approximates well. required tractable density commonly used adaptive distribution gaussian distribution whose mean standard derivation match practice estimated samples according invariance divergence reparameterization denotes distribution z−µr denotes standard normal distribution. reparameterization need estimate density ratio distributions zero means unit variances. multivariate regression task output sampled normal distribution mean parameter variance. variance controls likelihood model therefore choosing appropriate variance essential. place gamma prior gamma reciprocal variational posterior used gamma. elbo calculated eqeq sigmoid function. data points generated true model training data unnormalized true posterior plotted fig. baseline ﬁrst factorized normal distribution. result shown fig. clearly seen factorized normal capture position scale true posterior cannot well shape independence across dimensions. apply kivi. implicit posterior simple stochastic neural network demonstrate good result also hamiltonian monte carlo posterior samples. results plotted fig. implicit posterior learned capture strong correlation dimensions produce posterior samples similar shape samples drawn hmc. also experiment illustrate importance reverse ratio trick. figure implicit variational distribution learned optimizing klp). expect learned close prior result produced using trick compared result without latter fails work well. section present comparisons methods towards ﬂexible posteriors namely normalizing variational inference also include results ground truth. normalizing basic idea normalizing introduced section speciﬁcally given random variable following simple distribution invertible mapping probability density transformed variable calculated thus construct complex distributions composing invertible mappings keeping probability density result distribution tractable successively applying free parameters smooth element-wise nonlinearty chosen tanh following experiments. simple variational posterior made expressive parametric transformation employed. variational inference method directly minimizes kernelized stein discrepancy true posterior variational posterior conduct regression experiments section results shown table normalizing apply planar ﬂows weights match running time implicit posteriors. whether ﬂows help inference also present results using factorized normal distributions weights implicit posteriors used kivi section factorized normal distributions normalizing samples batch size except kinnm naval batch size learning rate epochs times. training parameters kivi’s. also produce ground truths. chains iterations target acceptance rate adapt step size. experiment time-consuming perform runs. table normalizing show improvements factorized normal distributions. probably optimization challenges caused limited form planar ﬂows thus inference procedure takes little beneﬁt ﬂexibility introduced ﬂow. found sensitive initialization implicit posteriors. variance input gaussian noise larger would diverge beginning training. also soon converges unsatisfying local optima optimization process starts. ﬁndings well explained conclusion magnitude functional gradient divergence saddle points original problem optimizing divergence become local optima optimizing ksd. order visually check quality uncertainty estimated kivi visualization technique named parallel coordinates plot posterior weights. compare posteriors inferred regression experiment boston housing factorized normal approximation kivi. feature dimension boston housing data used hidden layer size network weight matrices since dimensions plot posterior samples avoid visual clutter. speciﬁcally draw samples posterior goal show -dimensional samples. parallel coordinates sample plotted polyline vertices polyline represent single dimension. positions horizontal axis indices dimension vertical coordinates represent weight values. note important fact hidden neurons neural networks non-identiﬁable indicates dimensions non-identiﬁable. different inference algorithms converge different local modes caused symmetry. make visualizations comparable sort dimensions algorithm mean value posterior samples dimension. visualization could bnns factorized normal approximation signiﬁcant over-pruning problems. large proportion hidden nodes turned inference information mainly carried several others. pruned weights identiﬁed signal-to-noise ratio mean standard deviation. problem pointed previously works works explained looseness variational bound factorized approximation used contrast ground truth pruning problems. strong correlations captured observe neighboring dimensions. kivi also pruning problems could capture strong correlations across dimensions. gain accuracy still good amount uncertainty implicit posterior. despite weight dimensions non-identiﬁable could still methods arrive biased solutions compared ground truth terms scales. note necessarily problematic since typically known produce biased solutions. section know approximate gradients elbo directly related estimates. would like assess accuracy estimator. adopt settings experiment mnist comparison must also able compute ground truth term. achieve this normalizing complicated tractable density. thus good monte carlo estimate true figure compare term klp) term estimated using kivi ground truth. note since adaptive-contrast experiments term broken parts make sense look part uses density ratio estimator plotted figure estimates closely track ground truth accurate variational approximation improves time. note samples plotted drawn single chain. also nonidentiﬁability weights since different chains different initializations tend converge different local modes cannot identiﬁed other. plotted chains much visual clutter fair would need methods different initializations. figures present generated images interpolation experiments celeba training process. images generated epochs. results left produced right kivi. clearly seen avb’s training process high variance mentioned section gaussian mixture implicit posterior generates samples propagating samples standard normal distribution two-layer hidden units output unit. regularization coefﬁcient density ratio clipping threshold bayesian logistic regression inputs points randomly sampled outputs predictions randomly sampled weights prior. chains iterations each leapfrog steps. step size automatically adapted using dual averaging starting discard ﬁrst samples generated. factorized training epochs samples used. training anneal learning rate linearly according +epoch−. kivi follow setting except samples. regularization coefﬁcient minimal density ratio respectively. describe sample generation process implicit variational posterior used kivi. first random normal samples propagated fully-connected layers size producing added another random normal noise trainable variances producing finally propagate fully-connected layer size linear output layer getting variational samples. regression datasets small feature dimensions using bnns hidden layer produce high-dimensional weights. therefore still mlps implicit variational posterior samples generated propagating samples standard normal distribution mlp. datasets relu activation function. hidden layer except yacht hidden layers. list details table consist datasets weight matrices each. taking layer boston example represents random normal samples generated propagated hidden layer size output layer size note corresponds number weights ﬁrst layer bnn. mnist classiﬁcation needs larger scale network used multivariate regression. therefore adopt mmnn implicit variational posterior. denote hidden-layer size otherwise mmnn matrix multiplication layers. ﬁrst-layer weights hidden matrix multiplication layers size relu activations. output layer mmnn size linear activations. input matrices random samples size drawn standard normal distribution. second-layer weights hidden matrix multiplication layers size relu activations. output layer mmnn size linear activations. input matrices random samples size drawn standard normal distribution. third-layer weights hidden matrix multiplication layers size relu activations. output layer mmnn size linear activations. input matrices random samples size drawn standard normal distribution. variational posterior settings kivi logistic regression serves discriminator. regularization coefﬁcient minimal density ratio kivi. methods samples batch size learning rate training. decoders used mnist experiment mlps hidden relu layers. latent dimension hidden layer size implicit posterior also hidden relu layers gaussian noises dimensions added ﬁrst hidden layer. noise zero means -dimensional trainable variances. training batch size learning rate annealed factor every epochs. parameters kivi case clipping value decoders used celeba experiment exactly structure used images dcgan paper latent dimension implicit variational posterior deep convolutional neural network symmetric structure decoder except output last convolutional layer ﬂattened added gaussian noise shape last dimension. noise zero means trainable variances. last hidden layer fully-connected relu units. decoder. kivi batch size training parameters follow original code celeba. parameters kivi case clipping value learning rate kivi", "year": 2017}