{"title": "Low Rank Matrix Recovery with Simultaneous Presence of Outliers and  Sparse Corruption", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We study a data model in which the data matrix D can be expressed as D = L + S + C, where L is a low rank matrix, S an element-wise sparse matrix and C a matrix whose non-zero columns are outlying data points. To date, robust PCA algorithms have solely considered models with either S or C, but not both. As such, existing algorithms cannot account for simultaneous element-wise and column-wise corruptions. In this paper, a new robust PCA algorithm that is robust to simultaneous types of corruption is proposed. Our approach hinges on the sparse approximation of a sparsely corrupted column so that the sparse expansion of a column with respect to the other data points is used to distinguish a sparsely corrupted inlier column from an outlying data point. We also develop a randomized design which provides a scalable implementation of the proposed approach. The core idea of sparse approximation is analyzed analytically where we show that the underlying ell_1-norm minimization can obtain the representation of an inlier in presence of sparse corruptions.", "text": "instance assumes bernoulli model support element non-zero certain small probability. given arbitrary support columns/rows affected outliers. cutting-edge principal component pursuit approach developed directly decomposes rank sparse components solving convex program minimizes weighted combination nuclear norm -norm many approaches developed address problem including assumed column sparse matrix matrix decomposition algorithm proposed decompose data rank column-sparse components. -norm optimization problem replaced -norm grant robustness outliers. leveraged mutual coherence outlier data points data points apart inliers. alternative approach relies observation small subsets columns linearly dependent small subsets outlier columns given outliers typically follow low-dimensional structures. several algorithms exploit feature locate outliers rank matrix element-wise sparse matrix matrix whose non-zero columns outlying data points. date robust algorithms solely considered models either both. such existing algorithms cannot account simultaneous element-wise column-wise corruptions. paper robust algorithm robust simultaneous types corruption proposed. approach hinges sparse approximation sparsely corrupted column sparse expansion column respect data points used distinguish sparsely corrupted inlier column outlying data point. also develop randomized design provides scalable implementation proposed approach. core idea sparse approximation analyzed analytically show underlying -norm minimization obtain representation inlier presence sparse corruptions. index terms—robust sparse matrix subspace learning data outlier detection matrix decomposition unsupervised learning data sketching randomization sparse corruption standard tools principal component analysis routinely used reduce dimensionality ﬁnding linear projections high-dimensional data lower dimensional subspaces. basic idea project data along directions spread residual information loss minimized. basis much progress broad range data analysis problems including problems computer vision communications image processing machine learning bioinformatics notoriously sensitive outliers prompted substantial effort developing robust algorithms unduly affected outliers. distinct robust problems considered prior work depending underlying data corruption model namely rank plus sparse matrix decomposition outlier detection problem capital small letters used denote matrices vectors respectively. matrix column null space i.e. complement space denotes spectral norm nuclear norm singular addition matrix denotes matrix column removed. n-dimensional space vector standard basis. given vector denotes p-norm. element-wise functions sign absolute value functions respectively. paper consider generalized data model incorporates simultaneous element-wise column-wise data corruption. words given data matrix rn×n expressed rank matrix contains outliers element-wise sparse matrix arbitrary support. without loss generality assume columns corresponding nonzero columns equal zero. seek robust algorithm exactly decompose given data matrix without assumptions decomposition problem clearly ill-posed. indeed many scenarios unique decomposition exist. instance rank matrix element-wise sparse non-zero columns sparse sparse matrix rank. following brieﬂy discuss various identiﬁability issues. distinguishing identiﬁability rank plus sparse decomposition problem studied problem shown admit unique decomposition long column spaces sufﬁciently incoherent standard basis non-zero elements sufﬁciently diffused conditions intuitive essentially require rank matrix non-sparse sparse matrix rank. distinguishing outliers inliers consider outlier detection problem much research devoted study different versions problem various requirements distributions inliers outliers provided warrant successful detection outliers. authors considered scenario columnsparse i.e. data columns actually outliers established guarantees unique decomposition rank number non-zero columns sufﬁciently small. approach presented necessitate column sparsity requires small sets outliers linearly independent. assumption distribution outliers exact decomposition guaranteed even remarkable portion data columns outliers. paper make assumption distribution outliers namely assume outlier cannot obtained linear combination outliers. distinguishing sparse matrix outlier matrix suppose assume columns corresponding non-zero columns equal zero. thus columns sufﬁciently sparse nonzero columns sufﬁciently dense able locate outlying columns examining sparsity columns example suppose support follows bernoulli model parameter non-zero elements sampled zero mean normal distribution. sufﬁciently large fraction nonzero elements non-zero column concentrates around elements non-zero column nonzero high probability. best knowledge ﬁrst work account simultaneous presence sources corruption. numerical examples presented paper utilize following data model. data model given data matrix follows following model. data matrix rn×n expressed rank matrix non-zero columns. deﬁne {gi}k non-zero columns vectors {gi/gi}k i.i.d. random vectors uniformly distributed unit sphere thus non-zero column overwhelming probability. non-zero elements follow bernoulli model parameter i.e. element non-zero independently probability without loss generality assumed columns corresponding non-zero columns equal zero. remark uniform distribution outlying columns unit sphere necessary requirement proposed methods. made assumption data model ensure following requirements satisﬁed high probability similarly bernoulli distribution non-zero elements sparse matrix necessary requirement. assumption used ensure support concentrated columns/rows. needed distinguishable outlier matrix ensuring sparse matrix rank high probability. facial images different illuminations shown dimensional subspace given dataset consists sparsely corrupted face images along images random objects images random objects cannot modeled face images sparse corruption calls means recover face images robust presence random images. users rating matrix recommender systems modeled rank matrix owing similarity people’s preferences different products. account natural variability user proﬁles rank plus sparse matrix model better model data. however proﬁleinjection attacks captured matrix introduce outliers user rating databases promote suppress certain products. model captures element-wise column-wise abnormal ratings. rank plus sparse matrix decomposition algorithms consider presence applicable generalized data model given necessarily sparse matrix. also column-sparse well rank violates identiﬁability conditions approach rank plus sparse matrix decomposition illustrative example assume follows data model apply decomposition method learn obtained rank component. deﬁne hand robust algorithms solely consider column-wise corruption bound fail presence sparse corruption since crucial requirement algorithms columns lies however presence sparse corruption matrix even columns corresponding zero columns might instance assume follows data model fig. shows recovery error versus example robust algorithm presented utilized subspace recovery. clear algorithm cannot yield correct subspace recovery work paper motivated preceding shortcomings existing approaches. however method requires tuning parameters importantly inherits limitations speciﬁcally approach requires rank substantially smaller dimension data fails many outliers. also experiments shown yield accurate decomposition data matrix. illustration consider following data model columns indexed complement column support corresponding sparse component recovered table shows error recovery versus shown sparse component convex program combines cannot yield accurate decomposition knowing absence column outliers setting recover sparse component recovery error values table paper develop robust approach dubbed sparse approximation approach account types corruptions simultaneously. below provide summary contributions. sparse approximation approach approach forth -norm minimization formulation allows sparse approximation columns using sparse representation. idea used locate outlying columns which identiﬁed reduces primary problem rank plus sparse matrix decomposition. develop randomized design provides scalable implementation proposed method. learned using randomly design sampled data columns. subsequently outliers located using randomly sampled rows data. provide mathematical analysis sparse approximation idea underlying approach prove -norm minimization yield linear representation sparsely corrupted inlier. cancel moreover could show -norm relaxed -norm still able obtain sparse approximation. following lemma establishes sparse approximation recovered solving convex -norm minimization problem sufﬁciently sparse. order obtain concise sufﬁcient conditions lemma assumes randomized model distribution rows space. appendix present deterministic sufﬁcient conditions general optimization problem. assumption rows matrix i.i.d. random vectors uniformly distributed intersection space unit sphere state lemma deﬁne following oracle optimization problem thus number non-zero rows number non-zero rows orthogonal much smaller support follows random model addition small enough much smaller lemma suppose matrix rn×n full rank matrix expressed follows assumption deﬁne remark suppose support follows bernoulli model parameter small enough thus order roughly deﬁne cannot simultaneously orthogonal many non-zero rows thus much smaller therefore order assume rest paper organized follows. section idea sparse approximation explained. section presents proposed robust method section exhibits numerical experiments. proofs theoretical results provided appendix along additional theoretical investigations sparse approximation problem. suppose vector lies matrix i.e. vector linear representation respect columns least-square sense representation obtained optimal point main question section seeks address whether recover representation using convex optimization formulation sparsely corrupted. section propose norm minimization problem prove yield underlying representation. refer approach sparse approximation. proximation. deﬁnition suppose rn×n expressed sparse matrix a−iz sparse approximation b−iz. thus a−iz sparse approximation a−iz s−iz reason refer a−iz sparse approximation a−iz s−iz sparse vector sufﬁciently small. assume include sparse vectors i.e. coherent standard basis. according deﬁnition small enough lies a−iz∗ sparse approximation optimal point a−iz case regularization parameter chosen appropriately identify sparse vector -norm regularizer) also sparse. -norm functions forces non-zero values columns linear combination rank components cancel rank component linear combination sparse component yields sparse vector. word sparse vector since linear combination sparse vectors algorithm automatically puts non-zero values columns roughly sparse possible. hand i.e. outlying column since small subsets outlying columns linearly independent outlying column unlikely admit sparse representation sparsely corrupted columns linear combinations sparsely corrupted columns unlikely approximate outlying column. randomized techniques utilized reduce sample computational complexities robust algorithms algorithm solves dimensional optimization problem identify outlying columns. however show problem simpliﬁed low-dimensional subspace learning problem. uσvt compact singular value decomposition rn×r rr×r rn×r. rewrite call representation matrix. table algorithm details randomized implementation proposed method. randomized implementation ﬁrst obtained using random subset data columns. matrix rn×m consists randomly sampled columns. proposed outlier detection approach applied identify outlying columns matrix expressed {ei}n respectively. sufﬁciently sparse concluded k-th column outlying column. form index detected outlying columns. form matrix equal detected outlying columns removed. matrix decomposition obtain optimal point section sparse approximation method presented. also present randomized design reduce complexity proposed method table algorithm presents proposed algorithm. -norm functions utilized enforce sparsity representation vector residual vector main idea ﬁrst locate non-zero columns reduce problem rank plus sparse matrix decomposition. order identify outliers attempt sparse approximation data column using sparse linear combination columns d−i. certain columns approximation cannot found identify columns outliers. idea underlying approach sparsity rate used certify identity outlier sparsely corrupted inlier. providing insight consider illustrative example follows data model ﬁrst columns non-zero fig. shows outlying column clearly distinguishable. insight method gain insight consider scenario i-th column zero i.e. i-th data column sparsely corrupted inlier. corresponding columns sampled respectively. sufﬁciently large thus remove outlying columns decompose resulting matrix obtained rank component yield consists randomly sampled elements vector sparse vector accordingly equal zero however equal zero sufﬁciently large highly unlikely cannot cancel component sparse vector since remark learning step identify outlying columns sparsity dφz∗ lies null space non-zero elements according investigations chosen appropriately small number mostly smaller thus non-zero elements. practice much smaller since optimization searches sparse linear combination. step algorithm outlying columns located examining sparsity columns i-th column equal zero correctly recovered sufﬁciently large i-th column sparse vector roughly non-zero elements. accordingly appropriate threshold number dominant non-zero elements outlying columns correctly identiﬁed. algorithm randomized implementation sparse approximation method input data matrix rn×n initialization form column sampling matrix rn×m sampling matrix rn×m learning column sampling matrix samples columns given data matrix sampled outlying columns detection deﬁne equal outlying columns removed. recovery form orthonormal matrix basis learning locating outlying columns. sampling matrix samples rows given data matrix outlying column detection form index non-sparse columns obtaining rank sparse components. form columns indexed equal zero. form equal columns indexed zero. output matrices obtained rank sparse components respectively. contains indices identiﬁed outlying columns. suppose given data follows data model ﬁrst columns non-zero. matrix follows bernoulli model rank equal solve constraint vector equal {ei}n deﬁne corresponding optimal point. deﬁne |dz∗ max. also deﬁne vector whose i-th entry equal number elements greater thus i-th element number dominant non-zero elements fig. shows elements g/n. shown indices corresponding outlying columns clearly distinguishable. section present numerical experiments study performance proposed approach. first validate idea sparse approximation outlier detection study requirements. second provide phase transition plots demonstrate requirements randomized implementation i.e. sufﬁcient number randomly sampled columns/rows. finally study sparse approximation approach outlier detection real world data. presented theoretical analysis shown rank rank component sufﬁciently small sparse component sufﬁciently sparse -norm optimization problem yield sparse approximation section assume data follows data model study phase transition algorithm d-plane ﬁrst columns given data outlying columns i.e. deﬁne vector |dz∗ corresponding data column classify i-th column outlier percent elements greater fig. shows phase transition plane pair generate random realizations. ﬁgure white designates outliers detected correctly inlier misclassiﬁed outlier. observe correctly identify outliers even practice proposed method handle larger values higher sparsity levels columns rank matrix typically exhibit additional structures clustering structures simulation corresponding fig. rank matrix generated elements rn×r rr×n drawn zero mean normal distribution. thus columns distributed randomly accordingly elements must least non-zero elements yield sparse approximation. however columns union r/n-dimensional subspaces nonzero elements sufﬁcient. thus data exhibits clustering structure algorithm bear higher rank sparsity levels. example assume follows data model fig. shows sorted elements left plot columns -dimensional subspace whereas right plot columns union -dimensional subspaces. shown proposed method yields better output data admits clustering structure. section requirements algorithm studied. data matrix follows data model phase transition shows performance algorithm function trial considered successful rank equal step identiﬁes outlying columns correctly fig. entries vector different number clusters columns right plot columns union -dimensional subspaces. left plot columns dimensional subspace. increase increase. required value required approximately linear value linear fig. shows phase transition different dimensions interestingly required values nearly independent size ﬁxed vector. refer convex null space learning optimization problem. constraint vector equal null space learning optimization problem equivalent sparse approximation optimization problem hence latter special case former. note equivalence stems fact optimal point optimal point equal interestingly ﬁnding sparse vector linear subspace bearing effectively used machine learning problems including dictionary learning spectral estimation samples face images. observed images roughly follow rank plus sparse model rank plus sparse matrix decomposition algorithms successfully applied images remove shadows specularities addition sparse matrix face images. form data matrix consisting sparsely corrupted face images plus randomly sampled images caltech database outlying data points fig. shows subset images sampled caltech database. ﬁrst columns face images last columns random images. found that average number non-zero elements absolute values greater standard deviation average number absolute non-zero elements value greater standard deviation thus non-face images identiﬁed proper threshold number non-zero elements section study general theoretical problem dubbed null space learning problem sparse approximation problem special case. similar model used section assume matrix rn×n full rank matrix expressed sparse matrix columns linearly dependent. coherent standard basis expect optimal point feasible point conditions ensure optimal point cost function increased move optimal point along feasible perturbation direction. observe feasible point perturbation satisﬁes ensure optimal point lies null space coherent standard assume thus basis recall follows bernoulli model sufﬁciently |ls| case small approximate permeance statistic measure well rows distributed space permeance statistic increases rows uniformly distributed space. aligned along speciﬁc directions permeance statistic tends smaller wherefore coherent standard basis. agreement initial intuition since linear combinations columns likely form sparse vectors highly coherent. words coherence would imply could sparse even null space matrix sufﬁciently sparse ﬁrst inequality sufﬁciently sparse otherwise cardinality sufﬁciently large. requirement also conﬁrms initial intuition optimal point cannot yield sparse linear combination even lies null space unless sparse. vector incoherent recalling orthonormal bases null space space respectively factor second inequality unveils vector sufﬁciently coherent order ensure optimal point lies intuition small projection optimal point lies optimal point large euclidean norm satisfy linear constraint sense points lying would unlikely attain minimum objective function addition coherency requirement implies optimal point likely rank smaller null space would higher dimension makes likely coherent soltanolkotabi candes geometric analysis subspace clustering outliers annals statistics tsakiris vidal dual principal component pursuit proceedings ieee international conference computer vision workshops rahmani atia coherence pursuit fast simple robust principal component analysis arxiv preprint arxiv. cand`es romberg robust uncertainty principles exact signal reconstruction highly incomplete frequency information information theory ieee transactions vol. joneidi zaeemzadeh rahnavard khalilsarai matrix coherency graph tool improving sparse coding performance sampling theory applications international conference halko p.-g. martinsson tropp finding structure randomness probabilistic algorithms constructing approximate matrix decompositions siam review vol. wright ganesh compressive principal component pursuit information inference vol. rahmani atia randomized robust subspace recovery outlier detection high dimensional data matrices ieee transactions signal processing vol. march k.-c. kriegman acquiring linear subspaces face recognition variable lighting ieee transactions pattern analysis machine intelligence vol. fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding vol. m.-p. hosseini hajisami pompili real-time epileptic seizure detection signals random subspace ensemble learning autonomic computing ieee international conference hajinezhad hong nonconvex alternating direction method multipliers distributed sparse principal component analysis signal information processing ieee global conference kanade robust norm factorization presence outliers missing data alternative convex programming ieee computer society conference computer vision pattern recognition vol. ding zhou r-pca rotational invariant lnorm principal component analysis robust subspace factorization proceedings international conference machine learning. huber robust statistics. springer fischler bolles random sample consensus paradigm model ﬁtting applications image analysis automated cartography communications vol.", "year": 2017}