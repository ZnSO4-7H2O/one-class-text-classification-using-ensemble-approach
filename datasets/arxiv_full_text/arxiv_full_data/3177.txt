{"title": "Modeling Images using Transformed Indian Buffet Processes", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.", "text": "latent feature models attractive image modeling since images generally contain multiple objects. however many latent feature models ignore objects appear different locations require pre-segmentation images. transformed indian buffet process provides method modeling transformation-invariant features unsegmented binary images current form inappropriate real images computational cost modeling assumptions. combine tibp likelihoods appropriate real images develop efﬁcient inference using crosscorrelation images features theoretically empirically faster existing inference techniques. method discovers reasonable components achieve effective image reconstruction natural images. latent feature models assume data generated combining latent features shared across dataset learn latent structure unsupervised manner. models typically assume properties feature common data points—i.e. feature appears exactly across observations. often reasonable assumption. example microarray data designed cell consistently corresponds speciﬁc condition. explain ball’s every position devote less attention aspects image unable generalize across ball’s path. instead would like properties feature e.g. shape shared across data points properties e.g. location observation-speciﬁc. models discover transformation-invariant features many applications. image tracking instance discovers mislaid bags illegally stopped cars. image reconstruction restores partially corrupted images. movie compression recognizes recurring image patches caches across frames. discover features needed model data addi generalize across transformations features handle properties real images occlusion. nonparametric model comes close goals noisy-or transformed indian buffet process however likelihood model inappropriate real images. existing unsupervised models handle realistic likelihoods parametric cannot discover features. section describe models meet some criteria. section propose models fulﬁll properties combining realistic likelihoods nonparametric frameworks. section introduce novel inference algorithms dramatically improve inference transformed ibps larger datasets section show models discover features model data better existing models. discuss relationships nonparametric models extensions section figure generative process linear gaussian linear gaussian tibp masked tibp models share features across dataset observationspeciﬁc indicators determine features contribute data point tibp models transformations change features appear observation. lg-tibp features combined additively. m-tibp feature contributes pixel ﬁnal image. together global ordering features local binary mask determine feature responsible appearance given pixel. section review indian buffet process extension transformed models simple images. describe likelihood models images. models prelude models introduce section indian buffet process distribution binary matrices exchangeable rows inﬁnitely many columns. deﬁne nonparametric latent feature model unbounded number features. often matches intuitions. know many latent features expect data; neither expect possible latent features given dataset. model data must select likelihood model determines form features corresponding columns subset features selected combine generate data point. many likelihoods proposed several appropriate modeling images. location location person either foreground background. na¨ıve models would learn different features location appears appropriate model would learn observation fact transformation common feature. transformed extends accommodate data varying locations. tibp column ibp-distributed matrix associated feature. addition nonzero element associated transformation rnk. transforming features combining according likelihood model produces observations. original tibp paper features generated combined using noisy-or refer model noisy-or tibp allows feature appear different locations scales orientations. addition noisy-or another likelihood used linear gaussian model assumes images generated linear superposition features selects subset features generates observation additively superimposing features adding gaussian noise. demonstrated figure model extended adding weights non-zero elements ibp-distributed matrix incorporating spiky noise model appropriate corrupted images. want model images features occlude other linear gaussian models inappropriate. vision community images often represented overlapping layers including generative probabilistic models sprite models features gaussian-distributed ordering deﬁned features. image every active feature transformation binary mask pixel. given feature order image generated taking value pixel uppermost unmasked feature. model appealing. intuitive occlusion model; features consistent ordering; topmost feature visible. however likelihood model used parametric feature sets data number features known priori. no-tibp likelihood model incompatible real images provides foundation nonparametric models transformed features. section tibp build models combine nonparametric feature distribution transformations feature likelihood image likelihood deﬁned various ways. remainder section generic framework deﬁne concrete models parameterization transformations different likelihood models. transformations following austerweil grifﬁths consider three categories transformation translation rotation scaling. parameterize transformation using vector parameters parameterize translations transformed feature obtained shifting pixel rotations parameterized scaling parameterized practice restrict possible rotations scaling factors ﬁnite assume uniform prior transformations. linear gaussian transformed ﬁrst attempt deﬁne likelihood applicable real data based linear gaussian likelihood described section feature represented using real-valued vector image transformed features combined using superposition masked transformed lg-tibp model appropriate real-valued data cannot handle feature occlusion. address problem propose masked transformed based sprite model model feature represented gaussian feature shape vector permutation imposes ordering features. interpret feature behind feature time feature appears image sample mask bernoulli probabilities corresponding shape vector masks occlude lower layers pixel; uppermost unmasked feature contributes ﬁnal image. operator hadamard product matrices. figure shows ibp-distributed matrix transformations variables combine features form images lg-tibp m-tibp. perform inference lg-tibp m-tibp using mcmc. iteration sample gaussiandistributed features ibp-distributed binary matrix transformations hyperparameters m-tibp binary masks ordering models binary indicator matrix matrix transformations feature masks closely coupled. austerweil grifﬁths sampled explicitly marginalizing sampling rnk. however explicitly computing conditional distribution transformations feature cannot scale even moderate-sized images instead sample jointly metropolis-hastings step. efﬁcacy metropolis-hastings sampler depends quality proposal distribution. design data-driven proposal distribution qzqrqs based established pattern matching technique assigns high probability plausible states. figure cross-correlation proposal distribution per-image per-feature translation here propose translation feature best explains residual ˜xnk. note need within boundaries image borders ˜xnk indicate range possible rnk. mask proposal distribution m-tibp must also propose binary mask snk. proposal distribution conditional distribution unseen features previously unseen features sample feature proposal distribution corresponding mask obtained normalizing sampling pixel proposed mask according series bernoulli distributions parameterized normalized entries addition sampling jointly also resample values jointly resample using metropolis-hastings step proposal distribution qs). m-tibp also gibbs sample binary masks using conditional distribution assume feature order sampled uniform distribution permutations. sample feature order using metropolis-hastings step uniformly choose consecutive features propose order swap. transformation proposal distribution obtain proposal distribution translations matches intuitions true posterior look crosscorrelation feature residual ˜xnk obtained removing feature image cross-correlation standard tool classical image analysis pattern-matching. crosscorrelation real-valued images measure similarity translated lg-tibp. figure show proposal distribution feature three data points. proposal distribution peaks locations best match pattern pixels feature. locations match feature proposal distribution relatively entropic. thus crosscorrelation proposal distribution cause consider good candidates rnk. incorporate scaling rotation addition translation must increase space deﬁne metropolis-hastings proposal. small transformation space remains practical extend proposal distribution include possible scaling rotation combinations. separately obtain cross-correlations transformed features residual image concatenate resulting vectors obtain distribution possible transformations. features identity transformation. derivation assumes pixel single real number. however natural images typically color information represented three-dimensional vector pixel. model colors contribute image likelihoods. similarly proposal distribution element-wise possible channels main motivation behind algorithm proposed section allow transformed applied large data. austerweil grifﬁths calculate likelihood data every possible transformation. replacing naive approach sampler presented achieve speed-up least number rotations considered number scales considered number pixels number non-zero elements evaluating lg-tibp m-tibp likelihoods single image requires computations. since number possible translations calculating likelihood possible translations yielding total per-iteration complexity inference method used austerweil grifﬁths also values would scale contrast calculating cross-correlation feature image residual done using fast fourier transform proposal distribution described section calculated likelihood need evaluated twice metropolis-hastings step sampler scales noisy-or transformed sprite model experiments simulated data show lg-tibp m-tibp recover underlying features locations effectively ibp. data sets scaled zero mean unit variance linear gaussian models. simulated data qualitatively assess ability lgtibp m-tibp translated features generated data using four colorful features synthetic dataset contains images generated selecting features independently probability sampling transformation uniformly. since noisy-or likelihood cannot process color images data binarized no-tibp. although models cope gaussian noise no-tibp cannot noise added. experiment iterations; present features reconstructions ﬁnal iteration. figure compares performance four models dataset constructed translating four features. notibp achieves good results. struggles common structure lg-tibp m-tibp generalize across locations discover features qualitatively similar no-tibp’s. features overlap m-tibp obtains correct reconstruction; lg-tibp not. figure shows training likelihood iteration plotted accumulated time obtained using proposed metropolis hastings inference gibbs sampling lg-tibp model datasets pixel images respectively. marker indicates single iteration; plot shows iterations. time measured machine -core .-ghz memory. speed-up predicted section realtable test per-pixel per-channel rmse four datasets. number features sprite true number features dataset. lg-tibp m-tibp outperforms datasets. m-tibp works better lgtibp equally well lg-tibp worse dnk. m-tibp performs equally well sprite three datasets outperforms sprite smb. dataset trained lg-tibp m-tibp sprite randomly selected images remaining held testing. since no-tibp appropriate binary data could compare method. used features extracted training estimate test data evaluated reconstructions using test rmse. table shows lg-tibp m-tibp achieve better performance across datasets; m-tibp performs equally well sprite three datasets much better dataset. m-tibp performs better lg-tibp datasets worse lg-tibp dnk. limited occlusions black background adequately represented using simpler lg-tibp. figure shows reconstructions features obtained using sprite lg-tibp m-tibp. matches image background. contrast lg-tibp m-tibp identify shapes appear different locations. example ﬁrst column figure lg-tibp identiﬁes donkey kong ﬁreball addition background interestingly lg-tibp mis-identiﬁes ﬁreball missed actual ﬁreball. m-tibp model detected ﬁreball donkey kong background publicly available implementation sprite could detect features datasets. enable fairest comparison possible compare ﬁnite version m-tibp ﬁxed always believe equivalent sprite model although inference implementation tweaks tricks restrict kinds features learned. figure run-time likelihood comparison using metropolis hastings sampling gibbs sampling gibbs used synthetic dataset pixel images gibbs used pixels. addition trained lg-tibp m-tibp dataset features scaled rotated translated. implemented austerweil grifﬁths presumably computational cost. figure shows models successfully detected underlying features. ordering learned m-tibp matches true order except case green blue often overlap. real-world data show performance simulated data section carries real images evaluated lg-tibp m-tibp four image datasets chosen reﬂect various levels complexity simple video games static/dynamic background real-world scenes. figure reconstructions test data lg-tibp. first true image. second third fourth sixth rows reconstructed image sprite lg-tibp m-tibp respectively. fifth seventh rows features detected lg-tibp m-tibp respectively superimposed true image. color feature; colors consistent columns. pair adjacent columns images datasets respectively. super mario dataset lg-tibp extracted bush brick clearly m-tibp managed extract text denoting points earned player sprite performs poorly possibly large sparsely observed feature set. m-tibp identiﬁed blue parts feature green feature. bricks often appear center screen model learns occlude location patch sky. lg-tibp m-tibp learn features transformations m-tibp whole accurate reconstructions clearer. sprite generally reconstruct data well m-tibp extracted features less clear. possible reason sprite assumes features present image. moreover practice difﬁcult know priori number features dataset. factors mean sprite unlikely scale heterogeneous datasets smb. presented nonparametric latent feature models real-valued images presented novel efﬁcient inference scheme. section discuss applications inference paradigm discuss possible extensions models. exploitation pattern matching algorithms inference scheme uses scoring functions classical image analysis proposal distribution metropolishastings algorithm combines robustness computational appeal well-established pattern recognition tool ﬂexibility probabilistic models. approach similar methods based classical pattern recognition techniques applied across range bayesian models improve inference large state spaces. alternative modeling images real-valued vectors image codewords techniques used transformed bayesian nonparametric models build high-performing vision systems using ﬁxed codewords combination models would allow joint model infer transformations codewords feature cooccurrence patterns. rotation scaling implemented extending space cross-correlation-based proposal distribution. avenue future work investigate existing non-statistical models pattern recognition sample broder class transformations using metropolis-hastings. additional modeling directions features appear image contrary assumptions tibp. avenue future work extend model allow multiple instances feature given image. inﬁnite gamma-poisson process distribution inﬁnite non-negative integer valued matrices. used image modeling application required presegmentation images. work would allow extension non-segmented images. original tibp paper assumed transformations associated pair sampled i.i.d. distribution possible transformations. possible avenue future research allow correlations transformations different images leading image tracking model also leads efﬁcient inference restricting range feature appear neighborhood feature’s location image. idea used speed inference sprite implementation titsias williams incorporating spatial information mask distribution would also lead coherent feature appearances counteract spotty features observed mtibp. addition informative prior features could used encode domain-speciﬁc knowledge—for example data comparable walker video might make vertically oriented ellipses humanshaped features. authors would like thank joseph austerweil frank wood finale doshi-velez michalis titsias publishing implementations. research supported grant jordan boyd-graber also supported army research laboratory cooperative agreement wnf---. sinead williamson supported grant afosr grant opinions ﬁndings conclusions recommendations expressed authors’ necessarily reﬂect sponsors.", "year": 2012}