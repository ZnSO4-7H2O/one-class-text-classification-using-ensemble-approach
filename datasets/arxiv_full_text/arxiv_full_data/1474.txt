{"title": "From Images to Sentences through Scene Description Graphs using  Commonsense Reasoning and Knowledge", "tag": ["cs.CV", "cs.AI", "cs.CL", "I.2.10"], "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.", "text": "paper propose construction linguistic descriptions images. achieved extraction scene description graphs visual scenes using automatically constructed knowledge base. sdgs constructed using vision reasoning. specifically commonsense reasoning applied detections obtained existing perception methods given images commonsense knowledge base constructed using natural language processing image annotations lexical ontological knowledge resources wordnet. amazon mechanical turk-based evaluations flickrk flickrk ms-coco datasets show cases sentences auto-constructed sdgs obtained method give relevant thorough description image recent state-of-the-art image caption based approach. image-sentence alignment evaluation results also comparable recent state-of-the approaches. imagine example computer could look arbitrary scene anything sunset ﬁshing village grand central station rush hour produce verbal description. problem overwhelming difﬁculty relying ﬁnding solutions vision language integrating them. suspect scene analysis last cognitive tasks performed well computers. ﬁfteen year quote attributed rosenfeld founders ﬁeld computer vision pointed fundamental problem generating semantics visual scenes. since then researchers attempted approaches mostly centered asking what where questions commonsense reasoning commonsense knowledge many types commonsense knowledge belong different levels abstraction paper focus capturing reasoning based knowledge natural activities. scene view. methodology scenes recognized detecting inside objects objects recognized detecting parts attributes activities recognized detecting motions objects contexts involved activities recently researchers advanced viewpoint able develop semantic understanding visual scene able produce natural language descriptions semantics. given rise area ﬁeld integrates vision knowledge natural language. knowledge becomes especially important without background knowledge become increasingly hard obtain desirable level accuracy problem. knowledge often mined text problem stands intersection between computer vision natural language processing. mining knowledge storing form retains semantics reasoning using knowledge develop better understanding scenes fundamental issues addressed paper. current developments computer vision shown deep neural nets trained generate caption arbitrary scene decent success. indeed exciting achievement. however current state-of-the-art image captioning systems still drawbacks brute-force image-to-text mapping makes inconvenient conduct logical reasoning beyond inferences annotated data; lack intermediate semantic representations language-dependent; importantly system produces wrong results almost impossible trace back system analyze failure case consider humans accomplish task. human perception active selective exploratory. continuously shift gaze different locations scene. recognizing objects ﬁxate location interpret visual input using knowledge activities events objects. analyze vifigure examples positive example annotation construction worker orange safety vest working road negative example annotation bunch bananas hanging ceiling. annotations could infrequent hard logically justify contrasting outputs. sual scene visual processes continuously interact high-level knowledge represented form language. sense perception language engaged interaction exchange information leads meaning understanding. thus problem requires least modules solution vision module reasoning module interacting other. paper propose model early stages process. available datasets make impossible perform experiments consider vision active process although ultimate goal. thus question becomes vision module produces number detections much reasoning module infer scene possesses common sense abilities? turns reasoning module infer great deal. motivated intuitions present effort integrate deep learning based vision state-of-the-art concept modeling commonsense knowledge obtained text. deep learning-based perception system obtain objects scenes constituents probabilistic weights input image. predict objects interact scene build common-sense knowledge base image annotations along bayesian network capturing dependencies among commonly occurring objects abstract visual concepts precomputed resources help infer following correct correlated objects based high-conﬁdence objects detected; probable events objects participate role objects play event; given events objects constituents concept emerges information. based inferences output scene description graph depicts different entities events interact. figure show possible example image. essentially directed labeled graph among entities events enables array possibilities analysis beyond visual appearance event-entity based analysis question answering scene ﬂexible caption generation. fundamental contribution work novel algorithm uses automatically constructed knowledge base create image facilitates further reasoning caption generation. sdgs advantages ground-truth sentences easily processed machines/ai systems comparison sentences; output rich information-content; bounded speciﬁc templates often used researchers convert labels sentences also used generate sentence descriptions. also create knowledge base captures knowledge commonly-occurring concepts events entities. knowledge base used provide answers following queries event events connect entities; role entity plays event subset possible concepts involving entities connecting events. lastly inferences scenes will player holding ball able tackle blocker conditions also attempted feeding output predicates reasoning modules along additional background knowledge. related works work inﬂuenced various lines work researchers proposed approaches extract meaningful information images videos. suggests works categorized dense image annotations generating textual descriptions grounding natural language images neural networks visual language domains. according categorization share roots works generating textual descriptions. includes works retrieves ranks sentences training sets given image works generated descriptions stitching together annotations applying templates detected image content. several works shown promising efforts acquire apply commonsense different aspects scene analysis. uses abstraction discover semantically similar images. proposes learn variations pertaining concepts uses common-sense learn actions. introduced scene graphs describe scenes creates scene graphs descriptions. however automatically construct graph image believe event-entity-attribute based representation meaningful edge-labels sdgs equipped facilitate symbolic-level reasoning. figure example image possible corresponding sdg. note contain similar event wear person. omit space constraints. note that easy augment spatial information graph recent development deep neural networks based approaches revolutionized visual recognition research. different traditional hand-crafted features multilayer neural network architecture efﬁciently captures sophisticated hierarchies describing data shown superior performance standard scene recognition object recognition image captioning benchmarks. image dataset paper three image data sets popularly referred flickr flickr coco datasets three datasets images respectively. images datasets accompanied hand-annotated sentences describe image. datasets used train-test splits testing images serve testing reasoning experiments. deep object recognition trained bottom-up region proposals convolutional neural networks object detection method considers common everyday object classes trained ilsvrc dataset. apply method testing images convert object detection scores deep scene recognition trained scene classiﬁcation method classiﬁcation model trained scene categories category training samples. apply method testing images convert scene classiﬁcation scores constituent annotation collection deep constituent recognition images wild cannot always categorized limited number scene categories. however scene constituents describing properties actions objects attributes scenes occur frequently across images utilized describe image. work augment flickr image dataset human annotation constituents using amazon mechanical turks. speciﬁcally human labeler annotate objects objects properties objects. constituents reduce annotation effort. obtain standardized constituents annotations perform stop-words removal parts-of-speech processing retain nouns adjectives verbs. replace nouns superclasses father person then rank resulting phrases according frequencies. phrases grass play play person wear short etc. rest processing post-process annotations training image consider among constituents recent empirical results diverse range visual recognition tasks indicate generic descriptors extracted powerful work pre-trained image pre-trained model extract dimensional feature vector using trained multi-label constituents recognition using deep features. trained model applied testing images convert classiﬁcation scores next explain reasoning framework construct sdgs noisy labels knowledge text. provide better understanding complex system provide diagram architecture explaining reasoning process example image figure shown image perception system produces object scene constituent detection tuples. detection provided conﬁdence score. objects scores provided bounding box. scene labels constituent detections considered reasoning framework. detections quite noisy. develop elaborate reasoning framework construct sdgs noisy detections help pre-processed background knowledge. dataset-independent needs augmented object classiﬁers expands. scenes-to-avcs mapping table scene added ontological information involving abstract concepts synonyms. obtain synonyms used wordnet api. hand-annotated avcs scene learnt prior belief scene human annotations. example scene airport terminal {waiting room glass view people} list avcs terminal synonym; learn priors respectively avcs. following sub-sections ﬁrst introduce reasoning framework brieﬂy followed description construction knowledge base bayesian network lastly describe reasoning framework detail. reasoning framework equipped background knowledge stored form process objects scene constituent detections image construct appropriate following populate synonyms hypernyms hyponyms objects synonyms avcs scenes; extract entities events constituent. constituent person wear short results event wear edges labeled agent joining entity person another labeled recipient joining entity short; iii) choose avcs iteratively maximizes conditional probability given high conﬁdence objects; low-scoring objects choose sibling maximizes conditional probability given high-conﬁdence objects avcs; search compatible events connect pairs high-conﬁdence objects. events obtained constituents compatible events; given events avcs search concepts best suits events avcs also construct based high-conﬁdence objects events avcs. knowledge base idea behind representation concept inspired process aura structure process graph represents biological process aura-kb. symbolizes higher-level event encapsulates smaller events entities participate events. similarly concept represents natural activity events occur participating entities interact events. entire approach essence sequentially determining participants concept lastly concept itself. phase collect ontological information object classes object meta-data table scene classes scene metadata also store scene detection tuples human annotation images training images. create knowledge base bayesian network scenes abstract visual concepts mapping table scene detection tuples perception system previous section create scene detection tuples training images used learn bayes image annotations collect textual descriptions training images provided image datasets building knowledge base bayes however built using repository sentences describe day-to-day concepts. object meta-data object classes collected synonyms hyponyms hypernyms. list prepared using wordnet api. abstract visual concepts higher-level scene constituents describe commonly occurring visual concepts observed across images. essence compared phrasal verbs. like textual phrases necessarily follow compositionality constituent words. case happens context images. example waiting room suggests room seating area chairs people waiting etc. comparison scene constituents compared phrases retain compositionality people play person wear shorts etc. context image person wear shorts grounded objects person shorts; action wear. build visualization part given figure parsing annotated sentences flickrk consists events entities traits. total number edges distinct concepts graph conditional probability estimation sub-section describe type conditional probabilities estimate bayesian network learn estimate probabilities. conditional probability calculations steps approach inferring probable collection abstract visual concepts rectifying low-scoring erroneous objects. inferring probable collection avcs ﬁrst make list frequent avcs scenes detected test image. follow algorithm inferred concepts cinf high-scoring entities oimg scenes simg next attempt rectify low-scoring entities based high-scoring entities cinf low-scoring entity siblings i.e. children hypernyms. example bathing assigned score assigned superclass headwear children headband etc. calculate following omax argmaxo∈siblings omax high-scoring entities list paragraphs suggest need estimate conditional probabilities estimate conditional probabilities learn bayesian network using learning bayesian network capture knowledge naturally co-occurring entities abstract visual concepts learn bayesian network represents dependencies among them. create training data tuples total number entities avcs. term binary denotes entity occurs tuple. then tabu search algorithm learn structure populate conditional probability tables using r-bnlearn figure constructing knowledge base annotations. snapshot ﬁgure person bench entities connecting event. entity person trait climber. sub-graph essentially captures knowledge activity person laying bench. ﬁgure left shows edge-labels. stanford parser dependency graph. k-parser maps dependency labels using rules meaningful labels km-ontology resulting graph augmented using ontological semantic information different sources generalize graphs i.e. replace entities superclasses. merge based overlapping entities events create single graph deﬁned tuple denoting vertices edges vertex edge label. vertex three types events entities traits. events correspond verbs entities correspond superclasses nouns directly interact events traits represent nouns. edge labels exactly k-parser concepts corresponds generalized k-parser graphs sentences essentially sub-graph create training data process training image automatically detect entities avcs output tuple detect entities parse image annotations extract entities avcs people people wear shorts detected using rule-based techniques. however scenes airport-terminal unlikely avcs waiting room found human descriptions image; tend describe entities interactions. keeping idea mind scene classiﬁer system previous section consider avcs scene highest score scene-to-avc lookup table ranking inferring final concepts given relevant abstract visual concepts entities concepts image describes. this search ﬁrst events entities participate events entities together search concepts concepts rely assumptions knowledge base reﬂects more-or-less complete view relevant world knowledge hence suitable events assumption valid images come domain; examples used flickrk dataset domain corresponds pictures humans dogs natural setting; contains concepts possible given events entities. strict assumption might true even parse whole web. alleviate problem give ﬁnal outputs involving entities avcs events another concept obtained search connecting events motivation behind building knowledge base logically explain certain co-occurring events suitable combination entities. example consider entities person swimming trunks. note swimming trunks corresponds vertex trunk events sniff climb wear etc. i.e. corresponding tree-trunk others swimming-trunks. logically suitable events connecting events ﬁlter spurious events based ontological background knowledge pair entity oimg traverse path entity another graph consider eventnodes path. shown figure entities connected event. however cases could connected chain events entities. employ greedy breadth-ﬁrst search graph pairs. denote entities related event oev. ﬁltering spurious events introduce notion edge-compatible events. event edge-compatible respect entities connected event using edges compatible labels. labels well-deﬁned relations entities events km-ontology label-compatibility easy observe. example compatible pair animate entity agent. based rules event wear edge-compatible respect entities person trunk. even this still obtain events like climb etc. ﬁlter events consult table concepts know entity swimming trunks belongs superclass clothing hence retain events connected entity trunk superclass concept construction obtaining suitable events construct using following rules cinf detected events; iii) compatible edges related events has; entities following animate entity has; otherwise shortest path detected event edges path sdg. search concepts given events entities search concepts recall concept generalized k-parser graph sentence. consider concept candidate edges detected edge-compatible event present next weight candidate concept using remaining entities avcs; i.e. increase counter entity occurs graph. also calculate joint conﬁdence-score concept based values object scene constituents present concept. based counters joint conﬁdence-score rank concepts. generate sentence person wearing shorts. based edge-labels populate verb subject object adjectives sentences using simple rules. noted k-parser labels direct mapping stanford dependencies theoretically populate parts-of-speeches sentence sdg. herein lies effectiveness producing image. experiments results knowledge-structure representing scene rich information-content carry enough semantics describe image. adopted three sets experiments. first detect accuracy system detect events entities present image. perform qualitative evaluation textual descriptions generated sdgs sentences generated using amazon mechanical turkers lastly evaluate imagesentence alignment quality design image retrieval task report results image search based generated annotations. conclude provide example images sdgs. comparison purposes implementation generate textual caption testing image. method based combination image regions bidirectional recurrent neural networks sentences structured multimodal embedding. denote captions model represented tuple among these collected stored once re-used datasets. experiments re-use bayesian network learnt flickrk data datasets. though build time annotated sentences easily avoided using datasets. essence reasoning part donot require training datasets. entity event detection accuracy experiment extracted entities events constituent annotations test images flickrk. manually checked remove noise. provide baseline also extracted entities events automatically using k-parser. subsequently compared gold-standard entity-event output system image. statistics evaluation given table sentences. evaluation metrics relevance thoroughness therefore proposed empirical measures much description conveys image content much image content conveyed description engaged services judge generated descriptions based discrete scale ranging average scores deviation summarized table flickrk flickrk test images ms-coco validation images. comparison asked amts also judge random gold-standard description output state-of-the-art image captioning system. experiments found flickrk annotations used flickrk without much effect accuracy. however ms-coco datasets flickrk annotations falls short producing desired accuracy coco data much varied. similar experiments also evaluate imagesentence alignment quality using ranking experiments. withhold testing images generated sentences queries. process textual query construct gquery using procedure construct image take gimg calculate similarity query using following formula figure sdgs corresponds images respectively. detailed examples please check appendix http//bit.ly/njycko. wnsim wordnet-lin similarity words jaccard standard jaccard coefﬁcient similarity. based similarity measure give image retrieval results compared stateof-the-art results table sentences images seamlessly converge space graphical representations. could huge repercussions search image textual space storing knowledge images text together uniﬁed knowledge base. conclusion primary contributions work knowledge-structure representation bridges semantic information text images. results experiment beneﬁt intermediate representation easy observe. example images sdgs examples pick images produces objects scene recognitions comparably good conﬁdence scores. images corresponding sdgs provided figure observe information produced sdgs easily processed machines. answer questions entities interact event possible events scene entities interact scene. also mention concept-level modeling provided sdgs separates work recent approaches furthermore comparing structures k-parser output figure paper introduced reasoning module generate textual descriptions images ﬁrst constructing intermediate semantic representation namely scene description graph later used generate sentences. reasoning module uses automatically constructed knowledge base created text capture commonsense knowledge. built knowledge base proposed method obtaining sdgs noisy labels using prediction system. representation scene view integrates direct visual knowledge background commonsense knowledge. addition sdgs structure similar semantic representations sentences thus facilitating interaction vision natural language. notion great potential. used automatic creation sentences describing scene; equipped background knowledge also allows reasoning question/answering scene demonstrate effectiveness sentences constructed sdgs performed number experiments. evaluations popular datasets show sentences performs comparatively well respect state-of-the-art measures relevance thoroughness. gold-standard based evaluation shows output sdgs detect events entities comparable accuracy state-of-the-art system. lastly image retrieval experiment shows image-sentence alignment quality comparable state-of-the-art results.", "year": 2015}