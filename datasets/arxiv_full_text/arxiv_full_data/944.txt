{"title": "Unsupervised model compression for multilayer bootstrap networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Recently, multilayer bootstrap network (MBN) has demonstrated promising performance in unsupervised dimensionality reduction. It can learn compact representations in standard data sets, i.e. MNIST and RCV1. However, as a bootstrap method, the prediction complexity of MBN is high. In this paper, we propose an unsupervised model compression framework for this general problem of unsupervised bootstrap methods. The framework compresses a large unsupervised bootstrap model into a small model by taking the bootstrap model and its application together as a black box and learning a mapping function from the input of the bootstrap model to the output of the application by a supervised learner. To specialize the framework, we propose a new technique, named compressive MBN. It takes MBN as the unsupervised bootstrap model and deep neural network (DNN) as the supervised learner. Our initial result on MNIST showed that compressive MBN not only maintains the high prediction accuracy of MBN but also is over thousands of times faster than MBN at the prediction stage. Our result suggests that the new technique integrates the effectiveness of MBN on unsupervised learning and the effectiveness and efficiency of DNN on supervised learning together for the effectiveness and efficiency of compressive MBN on unsupervised learning.", "text": "recently multilayer bootstrap network demonstrated promising performance unsupervised dimensionality reduction. learn compact representations standard data sets i.e. mnist rcv. however bootstrap method prediction complexity high. paper propose unsupervised model compression framework general problem unsupervised bootstrap methods. framework compresses large unsupervised bootstrap model small model taking bootstrap model application together black learning mapping function input bootstrap model output application supervised learner. specialize framework propose technique named compressive mbn. takes unsupervised bootstrap model deep neural network supervised learner. initial result mnist showed compressive maintains high prediction accuracy also thousands times faster prediction stage. result suggests technique integrates eﬀectiveness unsupervised learning eﬀectiveness eﬃciency supervised learning together eﬀectiveness eﬃciency compressive unsupervised learning. keywords model compression multilayer bootstrap networks unsupervised learning. dimensionality reduction core problem machine learning classiﬁcation clustering regarded special cases reduce high dimensional data discrete points. paper focus unsupervised learning. traditionally dimensionality reduction categorized kernel methods neural networks probabilistic models sparse coding. kernel methods costly large-scale problems. although neural networks scalable large scale data double computational complexity bottleneck structure take input output bottleneck structure training stage slow moreover learn data distribution globally eﬀective learning local structures. multilayer bootstrap network recently proposed bootstrap method multiple nonlinear layers. layer ensemble k-centers clusterings. centers k-centers clustering randomly sampled data points input. easily implemented trained scales well large-scale problems neural networks training stage. moreover learns data distribution locally learn eﬀective representamotivated aforementioned problem recent progress compressing ensemble classiﬁers single small classiﬁer supervised learning hinton abstract paper propose unsupervised model compression framework. framework uses supervised model approximate mapping function input unsupervised bootstrap method output application unsupervised bootstrap method. specify framework taking unsupervised bootstrap method supervised model. proposed method named compressive mbn. best knowledge ﬁrst work model compression bootstrap methods unsupervised learning. algorithm basic framework. easily extend compressive techniques simply using unsupervised bootstrap techniques replace ﬁrst step potentially better performance. also many supervised learners replace third step knowledge currently already good choice. also design algorithms simply specifying second step diﬀerent applications. examples follows. compressive used visualization omit second step simply take input output input output respectively. compressive used unsupervised prediction hard clustering algorithm training predicted indicator vector training data point. example data point assigned second cluster predicted indicator vector also probabilistic output clustering. conducted initial experiment mnist. showed technique helpful reducing high computational cost unsupervised prediction problems. mnist data normalized dividing entries subsection consider generalization ability compressive mbn. instead studied visualization ability. data contained unlabeled images randomly selected training mnist used training test. reconstruction getting high-dimensional sparse representation hidden layer mapped dimensional space expectationmaximization principle component analysis training compressive omitted second step used input output input output model respectively. parameter settings follows. trained dnn. dropout rate rectiﬁed linear unit used hidden unit linear function used output unit. number training epoches batch size learning rate dimensional visualizations produced compressive shown fig. fig. respectively. ﬁgures found visualizations compressive equivalently good. used features clustering nmis methods around amazingly prediction time compressive images seconds accelerated prediction time around times parameter random feature selection parameter random reconstruction getting high-dimensional sparse representation hidden layer mapped dimensional space em-pca encoded -dimensional representations -dimensional indicator vectors k-means clustering specialization second step compressive mbn. training compressive took feature training input took -dimensional predicted indicator vectors training target dnn. parameter settings follows. trained dnn. dropout rate rectiﬁed linear unit used hidden unit sigmoid function used output unit. number training epoches batch size learning rate figure comparison generalization ability compressive clustering random reconstruction used clustering accuracy evaluated normalized mutual information. prediction time test images seconds. prediction time compressive test images seconds. figure comparison generalization ability compressive clustering random reconstruction used prediction time test images seconds. prediction time compressive test images seconds. moreover curve prediction compressive even slightly better mbn; highest prediction accuracy compressive reached terms nmi. advanced property compressive needed seconds predict images needed seconds predict images. prediction time accelerated around times. shown data small scale random reconstruction operation quite helpful however still unclear whether random reconstruction helpful data large scale since largest third experiment experimental results without random reconstruction exciting. subsection enlarged section experimental settings compressive section except mapped sparse features dimensional space em-pca. experimental results summarized fig. ﬁgure observed experimental conclusions section could also summarized here except random reconstruction used performance compressive good without random reconstruction. paper proposed general framework unsupervised model compression. framework takes case study. specialized technique named compressive uses auxiliary model modeling mapping function input prediction result given application takes dimension output input. technique aims solve problem although simple eﬀective robust eﬃcient-at-the-training-stage time consuming prediction. compressive concatenates eﬀectiveness unsupervised learning eﬀectiveness eﬃciency supervised learning together eﬀectiveness eﬃciency unsupervised learning. moreover easily extend compressive unsupervised model compression techniques.", "year": 2015}