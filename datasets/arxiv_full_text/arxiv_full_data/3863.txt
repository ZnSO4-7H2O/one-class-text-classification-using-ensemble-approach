{"title": "Boosting in the presence of outliers: adaptive classification with  non-convex loss functions", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "abstract": "This paper examines the role and efficiency of the non-convex loss functions for binary classification problems. In particular, we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data. The analysis of the role of a particular non-convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss -- the ability of the loss to efficiently adapt to the outlying data, the local convex properties of the loss and the proportion of the contaminated data. In order to use these properties efficiently, we propose a new family of non-convex losses named $\\gamma$-robust losses. Moreover, we present a new boosting framework, {\\it Arch Boost}, designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination. Along with the Arch Boosting framework, the non-convex losses lead to the new class of boosting algorithms, named adaptive, robust, boosting (ARB). Furthermore, we present theoretical examples that demonstrate the robustness properties of the proposed algorithms. In particular, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. Moreover, we present new theoretical results, based only on local curvatures, which may be used to establish statistical and optimization properties of the proposed Arch boosting algorithms with highly non-convex loss functions. Extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers.", "text": "paper examines role eﬃciency non-convex loss functions binary classiﬁcation problems. particular investigate design simple eﬀective boosting algorithm robust outliers data. analysis role particular non-convex loss prediction accuracy varies depending diminishing tail properties gradient loss ability loss eﬃciently adapt outlying data local convex properties loss proportion contaminated data. order properties eﬃciently propose family non-convex losses named γ-robust losses. moreover present boosting framework arch boost designed augmenting existing work corresponding classiﬁcation algorithm signiﬁcantly adaptable unknown data contamination. along arch boosting framework non-convex losses lead class boosting algorithms named adaptive robust boosting furthermore present theoretical examples demonstrate robustness properties proposed algorithms. particular develop breakdown point analysis inﬂuence function analysis demonstrate gains robustness. moreover present theoretical results based local curvatures used establish statistical optimization properties proposed arch boosting algorithms highly non-convex loss functions. extensive numerical calculations used illustrate theoretical properties reveal advantages existing boosting methods data exhibits number outliers. recent advances technologies cheaper faster data acquisition storage explosive growth data complexity variety research areas high-throughput genomics biomedical imaging high-energy physics astronomy economics. result noise accumulation experimental variation data inhomogeneity become substantial. therefore developing classiﬁcation methods highly eﬃcient accurate settings problem great practical importance. however classiﬁcation settings known poses many statistical challenges calls methods theories. binary classiﬁcation problems assume presence separable noiseless data belong classes adversary corrupted number observations classes independently. number setups belong general framework. random ﬂipped label design labels class membership randomly ﬂipped example occur frequently labeling prone number errors human otherwise. another example presence outliers observations small number observations classes variance larger noise rest observations. situations naturally occur heterogeneous data data corrupted subgroups behave diﬀerently; subgroup might individuals small studies would appear outliers within class data. considerable eﬀort therefore focused ﬁnding methods adapt relative error data. although resulted algorithms e.g. grünwald dawid achieve provable guarantees contamination model known multiple noisy copies data available good generalization errors test means guaranteed. problem compounded contamination model unknown outliers need detected automatically. despite progress outlier-removing algorithms signiﬁcant practical challenges remain. paper concentrate ensemble algorithms. among these adaboost proven simple eﬀective solving classiﬁcation problems many diﬀerent kinds. aesthetics simplicity adaboost forward greedy algorithms logitboost also facilitated tacit defense overﬁtting especially combined early termination algorithm friedman developed powerful statistical perspective views adaboost gradient-based incremental search good additive model using exponential loss. gradient boosting anyboost used approach generalize boosting idea wider families problems loss functions. criterion motivated fact exponential loss convex surrogate hinge loss. nevertheless presence label noise and/or outliers performance deteriorates rapidly although algorithms like logitboost madaboost log-lossboost able better tolerate noise adaboost still insensitive outliers. hence eﬃcient data observed little noise. however long servedio pointed boosting algorithm convex loss functions highly susceptible random label noise model. constructed simple example hereon denoted long/servedio problem cannot learned boosting algorithms above. center analysis work freund proposed robust boosting algorithm based boost majority brownboost algorithm uses non-convex loss function. instead maximizing margin algorithms achieve robustness allowing preassigned error margin maximization moreover step algorithms update solve diﬀerential equation update preassigned remaining time target error loss function changes iteration algorithm agree general boosting interpretation additive models. furthermore least preassigned parameters diﬃcult implement highly inconsistent respect minor changes settings. surprisingly statistical convergence properties algorithms still unknown. leads natural question develop simple eﬀective boosting algorithm non-convex loss function preserves boosting interpretation robust noise data? paper address question propose fully automatic estimator tuning parameters chosen provable guarantees. design framework explore amend drawbacks adaboost algorithm contaminated data setting. successfully identify adaboost’s sensitivity outliers comes unbounded weight assignment misclassiﬁed observations. outliers likely misclassiﬁed likely assigned large weights repeatedly reﬁtted following iterations. reﬁtting deteriorate seriously generalization performance algorithm algorithm learns incorrect data distribution. achieve robustness algorithm able abandon observations extreme incorrect side boundary. here theoretically computationally investigate applicability non-convex loss functions purpose. illustrate best weight updating rule assign weight data point appropriate non-convex loss function. tilting argument non-convex losses. shown that non-convex loss suﬃciently tilted i.e. small outliers eliminated successively. hence constant tilting trimming suﬃcient outlier removal. tilting loss function eﬀectively preserving much ﬁdelity data possible redistirbuting emphasis diﬀerent observations. propose arch boosting framework implements tilting method adjusts optimality search optimal weak hypothesis. moreover show framework avoids overﬁtting much similar adaboost. propose suﬃcient conditions needed loss function allow good properties archboost. show every non-convex function satisﬁes conditions; example sigmoid loss. however propose family loss functions balances beneﬁts non-convexity empirical risk interpretation boosting. properties boosting algorithms based convex loss functions extensively studied zhang comparatively little known non-convex losses existing techniques apply. show local convexity properties suﬃcient statistical consistency. furthermore even though proposed loss function shown enjoy aforementioned local convexity largely unknown whether numerical algorithms identify local minimizer. moreover algorithm deﬁned gradient descent algorithm require approach proof numerical convergence. develop suﬃcient optimality condition based hardness condition technical proofs. hardness property mean orthogonality reweighted classiﬁer class membership vector. furthermore address robustness eﬃciency proposed method respect outliers. although straightforward provide analysis parametric linear models computations classiﬁcation nonparametric boundaries challenging. provide novel analysis propose ﬁnite sample breakdown point theory show inﬂuence function bounded appropriate class classiﬁcation problems. best knowledge ﬁrst result regarding robustness properties boosting algorithms respect presence outliers. analysis allows convex non-convex loss function. ﬁnalize analysis proof statistical consistency proposed method includs many non-convex losses; exploring local curvatures loss. essence paper investigates eﬀects non-convex losses variety boosting algorithms presence unknown contamination data. particular focus design boosting framework order improve prediction accuracy classiﬁcation methods data outliers. rest paper organized follows. present arch boosting framework section designed augmenting boosting framework corresponding classiﬁcation algorithm signiﬁcantly adaptable outliers. section outlines family loss functions explores non-convexity present suﬃcient conditions non-convex loss robust. present theoretical analysis section numerical statistical convergence proposed algorithms discussed respectively. moreover show theoretical robust properties section breakdown point discussed section inﬂuence function section section contains numerical experiments number examples loss functions belonging introduced family γ-robust loss least squares logistic exponential truncated exponential loss. demonstrate methods practice compare alternative applying non-augmented adaboost algorithm noisy data. subsection varies parameter considers examples contaminated gaussian distribution. examples clearly illustrate methods outlined section successful existing boosting methods. section deals complex situation long/servedio data show arch boost method outperforms robust boosting method freund also discuss outlier detection examples section apply methods three real datasets section arch boost consider binary classiﬁcation problem denoting domain d-dimensional variable denoting class label equals estimate function assume training data i.i.d. copies unknown distribution. data consists samples contaminated distribution composed true data ﬁxed unknown number outliers classes. sample size i.i.d observations true probability distribution simplicity notation write e)]. note observed samples come contaminated distribution i.e. i.i.d. samples small positive contamination belongs large space weak hypotheses denoted framework presence countably inﬁnite features also known task feature induction easily established. next introduce framework boosting presence noise call arch boosting framework. design arch boosting framework stage-wise iterative minimization φ-risk however non-convex cannot done simply applying well known friedman’s gradient boosting explicit updates usually unavailable standard numerical methods like newton-raphson suitable convex functions used. instead constrain stage-wise minimization φ-risk keep additional important property boosting algorithms namely hardness condition freund schapire shown hardness condition easily distinguish outliers center data non-convex loss used. moreover allows tuning appropriate optimal hypothesis assignments minimization φ-risk approximately kept. therefore hardness condition allows simultaneously escape non-convexity minimization non-convexity separate outliers inliers. property iteration deﬁned equation explained progressing step weights updated orthogonal respect inner product deﬁned reweighed data wt+. certain sense weights chosen diﬃcult weak hypothesis weight vector hypothesis better random guess. recall goal optimal minimizes φ-risk suitable class measurable functions binary case instance associated label goal learning classiﬁer order minimize minimize sign equal every point given considering parameter denoting e)|x problem deﬁned ﬁrst order derivative classiﬁcation problems input loss function margin classiﬁer applied data point rewriting expectation terms class probabilities obtain following representation ﬁrst order optimality conditions without confusion write instead mimic equation ebove iteration steps proposed framework. details iteration current estimate +··· hand wish weak hypothesis solving following equation always accurately weak hypothesis process terminate step. however complications arise aspects. first restricted family weak hypotheses. therefore iteration approximated weak hypothesis closest approximation found. secondly equation alone cannot eﬃciently utilized since ultimate goal classiﬁcation problems unknown however propose solve approximated equation replace weighted conditional probability iteration proposed algorithm. hence ewt|x weighted conditional expectation deﬁned details iteration given solves since approximation optimal increment step instead adding multiply constant search best constant best updating classiﬁer αtht approaches optimal deﬁned equation hence deﬁne optimal solution following optimization problem then recall hardness condition adaboost algorithm weights updated weighted misclassiﬁcation error recent weak hypothesis according achieved deﬁning weights hardness condition optimality updating hypothesis satisﬁed. deﬁned weight updating rule back equation deﬁne optimal weak hypothesis ﬁnding relationship deﬁning function multiply note weak hypotheses least squares loss modiﬁed least squares loss depend current estimate weighted conditional probability diﬀerent gradient boosting lastly summarize procedure following algorithm call arch boost. assumption critical point require loss function convex hence arch boost algorithm applied many non-convex loss functions. illustrate section proposing family robust boosting algorithms based arch boosting framework. step algorithm classiﬁer used; example decision tree case proportion training samples label terminal node ends instance terminal region stands index data note step whenever pwtpwt order deal problem method update points becomes inﬁnity step keep inﬁnity update further. finally sign sign another method simply applying class probability estimations constant close data points additional variance within class data malicious data proposed algorithm adapts great success. begin section proposing loss function named two-robust loss function. furthermore characterize regularity conditions loss function needs satisfy order appropriate arch boosting framework. finally provide family loss functions convex satisﬁes newly deﬁned regularity conditions corresponding family robust boosting algorithms called arb-γ. approach non-convex functions recognized successful existing literature. freund freund utilized propose boosting methods resilient presence outliers data. however loss functions require number tuning parameters. behavior algorithm hindered optimal choice parameters. moreover optimal choices data dependent isolated universal either approach. driven need propose alternative loss function easily used many proposed loss function optimal weak hypothesis weight updating rule presented table call function two-robust loss belongs family loss functions discussed section adopting arch boosting framework algorithm illustrate nice downweighting property weight updating rule loss function. figure plot two-robust loss together exponential logistic regression losses corresponding weight updating rules. observe two-robust loss non-convex bounded therefore outliers even large size bounded inﬂuence classiﬁcation. moreover investigating weight updating rule observe misclassiﬁed data point smaller weight updating function algorithm fact abandons data points repeatedly misclassiﬁed bayes boundary. phenomenon disappears uses exponential loss logistic loss fact commonly used convex loss function. best knowledge existing loss function satisﬁes nice property without requiring a-priori ﬁxed tuning parameters. plugging two-robust loss arch boost obtain boosting algorithm named adaptive robust boost- denoted arb- hereon. next consider simple dimensional binary classiﬁcation example consisting mislabeled points near classiﬁcation boundary. example artiﬁcially created illustrate point adaptive classiﬁer outliers data. evenly points region {xij corresponding otherwise. labels step sizes algorithms constant number iterations show diﬀerence real adaboost arb- figure seen decision boundary adaboost inﬂuenced blue abnormal data points. arb- decision boundary stays abnormal points hence adapts outliers. previous section seen example robust boosting algorithm. nevertheless plenty non-convex functions leads question loss function arch boost? answer impose auxiliary conditions. section discuss kinds conditions need imposed non-convex function becomes suitable loss function arch boost framework since equation becomes convex combination therefore necessary condition loss functions unique optimal solution observe class measurable functions take real value every condition equivalent convexity loss function rather local convexity around true parameter interest next section present family non-convex loss functions possess property. hence present regularity conditions deﬁnition below. conditions together imply upper bound loss constant scaling. condition classiﬁcation calibration considered weakest possible condition imposed measurable function minimizes φ−risk also risk close minimal one; words close optimal creates bayes boundary. function convex condition satisﬁed long diﬀerentiable however considering non-convex losses regularity conditions doesn’t exists current literature. framework includes non-convex loss functions diﬀers existing literature convex losses includes additional condition lemma know logistic exponential least square modiﬁed least square loss arch boosting loss functions. diﬀerentiability loss function technical condition crucial proposed framework. hinge loss φhinge diﬀerentiable shown satisfy conditions however plug hinge loss equation cannot easily optimal solution however every diﬀerentiable non-convex loss function satisﬁes regularity conditions above. remark sigmoid loss φsig diﬀerentiable satisﬁes condition satisfy condition hence arch boosting loss function. family non-convex functions arb-γ algorithms recall optimal classiﬁer satisﬁes equation observe right hand side equation depend loss function take values positive real line hence parameterize real-valued function whose range follows surjective decreasing function classical motivation reparametrization often called link functions often uses parametric representation natural scale matching desired one. choose function constant surjection. parametrization unique admits solution following diﬀerential equation observe parameters tuning parameters rather index family non-convex losses much like huber tukey’s biweight losses are. note right hand side take real values monotonically increasing consequently corresponding loss always give unique solution name element family γ-robust loss function. later show positive parameter irrelevant algorithms note obtain nonconvex loss function moreover loss function bounded function upper bound equal therefore eﬀects outlier necessarily bounded. moreover weight updating rule also downweight largely misclassiﬁed data points. plot diﬀerent values corresponding normalized weight updating rules figure figure diﬀerent peak point weight updating rules shifts left increases. weight updating curve symmetric fact equivalent sigmoid loss function tanh moreover loss equivalent savage loss function mesnadi-shirazi vasconcelos used probability elicitation technique savage design loss functions. lemma allows plug arch boost framework obtain family robust boosting algorithms name adaptive robust boost-γ details presented algorithm computations relegated appendix note algorithm arb-γ despite substantial body existing work gradient boosting classiﬁers adaboost particular bartlett traskin freund friedman schapire schapire zhang breiman koltchinskii panchenko research robust boosting classiﬁers mostly limited methodological proposals little supporting theory kearns littlestone however whereas loss functions studied papers ﬁnite-sample versions globally convex functions many important robust classiﬁers arising freund proposed arb-γ possess convex curvature local regions even population level. paper present theoretical results based local curvatures used establish statistical optimization properties proposed arch boosting algorithms highly non-convex loss functions. koltchinskii panchenko zhang main diﬀerence authors gradient descent rule ﬁrst approximate minimization second paper hardness condition select optimal weak hypothesis {w}n recall classiﬁer data point term always stands margin. weak hypothesis denote expected margin empirical margin wyih. introduce notation used theorem family weak classiﬁers denote theorem assume arch boosting loss function. furthermore assume weak learner able provide disjoint regions domain iteration apply arch boost algorithm sample {··· iterations. arb-γ algorithm converges number iterations increases. conditions part somewhat diﬀerent compared equivalent obtained gradient boosting convex losses reference sequence needs local neighboorhood sence ¯ft−ft cannot blow rapidly. moreover size cannot smaller implying size needs converge faster polynomial additionally choice depends classical conditions guarding example converge speed. however chosen additional constraint step size choice acts penalty allowing non-convex loss functions. however unlike existing results theorem require additional algorithmic tuning parameters choices λt). results bartlett traskin provide similar bounds assumption unbounded step size boosting algorithm assume positive lower bound hessian empirical risk remark result theorem requires weak conditions. namely approximate minimization step inexact weak hypothesis iteration obtained preserving hardness property adaboost method applicable non-convex functions. certain loss function apriori point gradient descend direction empirical φ−risk. hence cannot numerical methods like newton’s method needed gradient descent provide novel weak hypothesis also suitable non-convex loss functions. then show direction weak hypothesis indeed descending direction section gave informal explanation non-convex losses lead robust algorithm. weight updating rule ﬁrst derivative loss functions plays important role robustness. unlike convex functions many non-convex functions diminishing ﬁrst derivative tends inﬁnity negative inﬁnity. section quantify robustness justify robustness arch boosting algorithms point view ﬁnite sample breakdown point well inﬂuence functions population measure robustness. section invex function properties show non-convex functions leads robust algorithms. ﬁrst recall deﬁnitions deﬁnition assume open set. diﬀerentiable function invex exists vector function well known diﬀerentiable convex diﬀerentiable rank invex craven glover proved function invex every stationary point global minimizer. second condition deﬁnition equivalent invex function exactly critical point show two-robust loss function sample empirical risk invex function write diﬀerentiable convex function diﬀerentiable vector function rank empirical risk rexpn viewed \"inﬂuence trimming\" function. current estimate observe whenever fexp otherwise fexp instead saying made severe mistake a-transformation fexp uncertain point. empirical robustness properties deﬁned breakdown point donoho huber proved successful context location scale regression problems stromberg ruppert tyler etc.). success sparked many attempts extend concept situations genton lucas davies gather etc.). however little work done classiﬁcation context. breakdown point deﬁned hampel roughly smallest amount contamination cause estimator take arbitrarily large aberrant values. breakdown points mean median reﬂect ﬁnite-sample behavior. however alternative view desired classiﬁcation context magnitude estimator relate necessarily classiﬁcation size weak hypothesis marginally related classiﬁcation boundary. henning exploits cluster stability breakdown point analysis estimators mixture models. however context boosting clear concept diﬀerentiate adaboost logisticboost algorithms; ﬁrst known robust second known better ﬁnite sample analysis. argue case gradient classiﬁcation loss takes alternating directions certain sense similar diverging estimator values regression setting. sense proposed breakdown study resembles hampel’s original deﬁnition. hence look largest number perturbed data points keep gradient risk minimization correct direction. moreover relate concept weight updating rule arch boost algorithms. {··· }∪{··· observed contaminated samples among {··· outliers. original dataset weak hypothesis iteration denote ht). also denote −gt) ynwt) negative gra= ymwt obtained embedding negative gradient weak hypothesis modiﬁed negative gradient −go. following theorem. theorem using notations above min−|pj every region then arch boost algorithm iteration conditional realizations theorem suggests arch boosting algorithm satisﬁes conditions above preserves direction descent non-contaminated empirical φ-risk hence disregarding outliers. sense oracle property minimizes risk clean data. remarks imminent. conditions theorem mild. total weight positive labels negative ones region elements corresponding points consequently inﬂuence sign moreover case main interest case data region labels hence reasonable outliers. cases theorem establishes whenever summation weights outliers larger constant times summation weights data points outliers also direction along empirical risk non-contaminated data decrease. figure shows weight updating rule archboost algorithm various clearly illustrates condition likely satisﬁed adaboost logitboost algorithm. example case archboost real adaboost arb- weight outlying data point boundary data hence seen adaboost puts times larger weight data compared arb-. dirac distribution point describes eﬀect inﬁnitesimal contamination point estimate standardized mass contamination. additionally gives eﬀect outlying observation estimator. bounded eﬀect outlier inﬁnitesimal. straightforward obtain inﬂuence functions associated parametric estimators linear regression models. however nonparametric regression models complicated. simplify analysis consider subclass binary classiﬁcation models true boundary assumed belong class functions here deﬁned reproducing kernel hilbert space bounded kernel induced norm ||h. observe archboost much like adaboost logitboost belongs broad class empirical risk minimization methods converges properly regularized hence study robustness properties consider viewed population quantity interest. here loss function tuple convenience. solution viewed every ﬁxed value regularization parameter assigns element rkhs every distribution given observe contaminated analysis herein completed risk minimization context inﬂuence population level. function also denote deﬁned inﬂuence function takes form identity mapping contamination point. display derivative deﬁned proof result refer proof theorem christmann steinwart theorem christmann steinwart convexity loss function respect second argument required invertible non-convex loss guaranteed nonnegative. however observe suﬃcient function non-negativity expectation rather second derivative itself. moreover non-negativity required local neighborhood true parameter interest following lemma show properties lemma binary classiﬁcation problem given distribution whenever twice continuous diﬀerentiable arch boosting loss function obtained measurable function comments necessary. conditions lemma satisﬁed case γ-robust loss function. example case two-robust loss function show thus observe condition restricts setting lownoise setting true probability class membership bounded away example model condition holds compact space. recall solution risk minimization problem deﬁned measurable restrict search rkhs bounded kernel functions. general. however long rich enough close sense ||fpλ ∗||∞ small following theorem. theorem treated clue robustness non-convex loss functions shows inﬂuence function bounded decreases contamination point outlier. furthermore theorem emphasizes robustness mainly comes fact magnitude margin large. theorem binary classiﬁcation problem twice continuously diﬀerentiable arch boosting loss function rkhs bounded kernel assume distribution exists ||fpλ ∗||∞ result theorem implies smooth enough classiﬁcation boundaries arch boosting algorithm bounded inﬂuence function whenever plot versus zyfpλ decrease towards constant origin like loss function redescending m-estimator moreover observe unbounded exponential loss bounded diminishing logistic loss theorem observe arch boost algorithm formulated empirical risk minimization procedure consistency established case convex loss functions however extend framework include non-convex losses exploring local curvatures. given training sample compute classiﬁer denote misclassiﬁcation error |sn). moreover bayes risk family measurable functions. three steps proving consistency include utilizing property loss whenever φ−risk converges minimal risk sample dependent function hence random variable. hence step choose deterministic reference sequence function sample size ﬁrst condition automatically satisﬁed arch boosting loss function classiﬁcation calibration condition. note inequalities true sample sample size several assumptions. ﬁrst class distribution certain rich enough class assumption true distribution example class binary trees number terminal nodes larger equal dimension next assumption loss function satisﬁes assumption know limv→∞ limv→−∞ exist class loss functions satisfy assumption incorporates class loss functions ﬁrst derivative converges zero away origin; example redescending hampel’s three part loss function. lessens eﬀect gross outliers turn leads many good robust properties resulting estimator. γ-robust loss functions satisfy condition. commonly used loss functions like least squares exponential loss logistic loss satisfy condition bounded. next state results important proving consistency arch boosting estimator. much like consistency adaboost proof hinders upon optimal choice stopping times. however statement dependent additional truncation level functions boundedness loss function nice decaying property derivative proposed arch boosting loss enables avoid additional parameters. proof given appendix theorem assume distribution satisfy assumption arch boosting loss function satisfying assumption sample non-negative sequence stopping times theorem illustrates uniform deviation φ-risk empirical φ-risk. note want fast order make sure uniform deviation converges zero moreover part theorem know exists sequence samples another word optimal classiﬁer obtained minimizing empirical risk rφfn {fn} reference sequence note depends sample size {fn} deterministic state intermediary lemma connects reference sequence arch boost estimator lemma reference sequence {fn}∞ rφnfn) rφfn) theorem ﬁnally state consistency result. corollary assume class distribution satisfy assumption arch boosting loss function satisﬁes assumption then sequence classiﬁers returned arch boost algorithm stopped step chosen theorem satisﬁes a.s. generate datasets using ’make-hastie--’ ’make-gaussian-quantiles’ ’make-hastie--’ data features standard independent gaussian ’make-gaussian-quantiles’ case dataset constructed taking -dimensional normal distribution datasets classes separated concentric multi-dimensional spheres origin roughly equal numbers samples class. test robustness arb-γ algorithms adding noise diﬀerent percentages training samples. following examples independent t-distribution noise features selected training samples. results summarized figure dataset generate samples using corresponding methods. among data training cross validation rest testing. number weak classiﬁers cross validation step sizes arb. arb- arb- arb- arb- arb- real adaboost. robustboost tune target parameter percentage errors using bisection search. ﬁgure plot average test errors corresponding conﬁdence intervals. figure several observations. first test errors arb-γ algorithms less real adaboost. moreover smaller truncated exponential loss indicating introduced non-convex family losses substantially outperforms traditional truncation losses. second percentage outliers less certain level performances arb- best. noise level higher arb-. behaves best. moreover robustboost higher test error noise level low. robustboost algorithm iterations obtain error larger zero. however robustboost long enough error rate performance converge real adaboost. robustboost arb-. similar performance. arb- worse robustboost noise level high. however note tune number tuning parameters robustboost noise level. also \"tune\" arb-γ algorithms example choose arb- noise level less arb-. otherwise figure could observe arb-γ uniformly better robustboost. illustrate importance loss function choice arch boosting method implement gradient descend boosting algorithm trimmed version exponential loss i.e. truncated exponential loss function. observe improvement adaboost extremely minor disappears dimensionality problem grows. ’makehastie--’ dataset truncated exponential loss better γ-robust losses point much indistinguishable adaboost. situation even better ’make-gaussian-quantiles’ dataset truncated exponential almost identical adaboost soon suggests arch boosting framework essential robust generalizable performance. long servedio constructed challenging classiﬁcation setting described follows. input binary features label first label chosen equal probability. given features generated according following mixture distribution xi). generate samples distribution label probability train classiﬁer noisy data test performance original clean data. ﬁrst generate datasets according distribution them randomly labels. result table record average test error also report sample deviations brackets. arb- outperforms real adaboost logitboost even better robustboost also compare performance diﬀerent arb-γ plot average test errors conﬁdence intervals figure figure arb-. behaves best dataset among algorithms. increases performance arb-γ approaches real adaboost. breakdown point higher implying smaller lead better robustness properties. breakdown point shown previous sections arb-γ algorithms robust noise. therefore robustness arb-γ able detect outliers. intuitively point outlier misclassiﬁed weak hypotheses arb-. experiment generate data points using ’make-hastie--’ randomly shuﬄe them. noise drawn t-distribution features ﬁrst percentage data points. running algorithms iterations record times data point misclassiﬁed count number points misclassiﬁed times count many actually belong noisy noise finally calculate ratio ratio describes chance data point outlier. cross-validation step size arb- real adaboost. results shown table x-axis stands index training points ranging y-axis stands times point misclassiﬁed ranging percentage outliers less arb- points misclassiﬁed times above indeed outliers real adaboost number around informally arb- conﬁdence conclude data point misclassiﬁed times outlier. test arb-γ algorithms wisconsin breast cancer data street available machine learning repository website university california irvine https//archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+. data created taking measurements digitized image needle aspirate breast mass individuals benign malignant instances. realvalued features computed cell nucleus radius texture perimeter area smoothness compactness concavity concave points symmetry fractal dimension. randomly split data equally balanced training benign samples malignant samples rest samples used testing. maximum iterations ﬁve-fold cross-validation implemented training select step size stopping time algorithm. procedure repeated times average test error reported table boxplots test error presented figure observe arb- behaves best original data arb-. outperforms others figure comparison arb- arb-. robustboost real adaboost wisconsin breast cancer dataset. subﬁgure left right plots test errors arb- arb-. robustboost real adaboost. noise. compared stefanski obtain best test error rate methods uniformly achieve smaller test error rate clean comparable test error rates perturbed datasets. compare arb- arb-. robustboost real adaboost dataset sensorless drive diagnosis also available machine learning repository https//archive.ics. uci.edu/ml/datasets/dataset+for+sensorless+drive+diagnosis. dataset contains instances features extracted electric current drive signals. range typical defects drive train applications considered diﬀerent classes present. combine data points label class rest class. time randomly choose points training cross validation testing. cross validation choose stopping time step sizes. according noise levels certain proportion labels training data points randomly ﬂipped. summarized test errors using plots figure calculated mean sample deviation table figure comparison arb- arb-. robustboost real adaboost sensorless drive diagnosis dataset. subﬁgure left right plots test errors arb- arb-. robustboost adaboost. observed arb- performs best original data without adding extra noise. robustboost behaves worse others original dataset. reason needs time terminate target error near reason cannot distinguish outliers hard inliers ﬂipped labels arb-. outperformed others ﬂipped labels robustboost behaved best. three cases noise test errors arb-. robustboost close. however arb-. need tune target parameters diﬀerent noise levels. next test algorithms dataset part ’microarray quality control project. available gene expression omnibus database accession number http //www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=gse. dataset contains newly diagnosed breast cancer patients aged years population spanning three major races mixtures. patients received months preoperative chemotherapy followed surgical resection cancer. estrogen-receptor status helps guide treatment breast cancer patients breast cancer contains many estrogen receptors. patients positive estrogen-receptor status negative estrogenreceptor status. sample described biomarker probe-sets. alleviate computational burden choose probe-sets smallest p-values two-sample t-test standardize feature. simpliﬁcation often considered high dimensional data randomly choose samples positive estrogen receptor status samples negative estrogen receptor status training rest testing set. randomly labels samples training according preassigned noise level repeat analysis times. ﬁve-fold cross-validation implemented training select stopping time step sizes. summarize results table figure dataset previously analyzed deshwar morris zhang best obtained test error respectively. however methods achieve error comparable even labels perturbed random. suggests method extremely stable even high-dimensional models. showed archboost robust alternative popular gradient boost -type algorithms. algorithmic part presented theorem works quite general class loss functions satisfy arch-boost loss properties presented deﬁnition diﬀerentiability condition imposed artiﬁcially believe avoided considering appropriate sub-diﬀerential analysis. however condition crucial analysis believe cannot relaxed. moreover robustness properties depend crucially condition too. additionally robustness part analysis summarized theorems works quite arbitrary lipschitz loss function. hence presents novel proof logitboost robust adaboost folklore observation made many experts ﬁeld. example theorem likely hold logitboost adaboost similarly likely hold archboost logitboost. contrast many existing boosting methods based gradient boosting ideas cannot directly applied purpose. order propose estimator explore recursive relationship ft−. observe rewriting equation statistical consistency proof centered around tilted loss functions non-convex particular. believe non-convex losses great unexplored potential robust high dimensional statistics. framework tilted loss functions general well explored robust variable selection estimation appropriate penalization scheme. moreover well known impact outliers multiplied case inferential problems conﬁdence intervals testing. screening many large outliers tilted losses signiﬁcantly improve upon asymptotic eﬃciency existing procedures. since convex monotone decreasing. monotone increasing. increasing decreasing constant. contradiction assumption. therefore combined monotonicity continuity know solution global minimum here show derivation possible family solutions. employing integrating factor method adapting nonlinear ordinary diﬀerential equation since made reasonable guess plugging positive empirical risk decreases adding weak hypotheses current estimate. then show weak hypothesis returned arch boost algorithm always positive empirical margin convergence. here develop ideas much similar proof lemma lemma zhang diﬀerences comparison zhang first loss non-convex function second optimal hypothesis chosen diﬀerently. contains weak hypotheses proof theorem consists careful decomposition inner product gradient vector weak hypothesis obtained complete data. decomposition done manner factors inner product gradient computed noise-free data weak hypothesis. then proof completed showing signs inner products match. proof lemma since arch boosting loss function know critical point global minimum. hence note treat input chain derivative w.r.t. rule know second argument. also nonnegative points proof theorem lemma exists ||fpλ measurable function therefore show inﬂuence function still exists form theorem christmann steinwart since theorem rewriting place needs convexity show first prove theorem result follows following lemma builds lemma bartlett traskin supplemented uniform convergence truncated loss φ-loss presented lemma proof lemma result mainly follows lemma bartlett traskin since satisﬁes assumption know exist ﬁnite maxv∈ almost surely convergence follows borel-cantelli lemma. arch boosting loss function satisfying assumption since limv→∞ limv→−∞ exist given know exists |u||w| deﬁnition convergence. choose hence discussion previous paragraph. summary |rφn |rφn |rφn a.s. therefore supf∈πζn◦f |rφn−rφ| .a.s. supf∈f |rφn−rφ| a.s. supf∈f |rφn a.s. theorem proved. classiﬁer conclusion uniform deviation follows theorem follows theorem follows theorem a.s. since arch boosting follows function know classiﬁcation-calibrated hence theorem bartlett", "year": 2015}