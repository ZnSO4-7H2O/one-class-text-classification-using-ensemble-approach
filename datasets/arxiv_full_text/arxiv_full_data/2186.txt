{"title": "From Language to Programs: Bridging Reinforcement Learning and Maximum  Marginal Likelihood", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.", "text": "figure task natural language utterances program manipulates world state. correct program captures true meaning utterances spurious programs arrive correct output wrong reasons. develop methods prevent model drawn spurious programs. process constructing program formulated sequential decision-making process feedback received sequence completed program executed. natural language processing literature common approaches handling situation reinforcement learning particularly reinforce algorithm maximizes expected reward sequence actions; maximum marginal likelihood treats sequence actions latent variable maximizes marginal likelihood observing correct program output approaches enjoyed success many tasks found work poorly task. addition sparsity correct programs task also requires weeding spurious programs incorrect interpretations goal learn semantic parser maps natural language utterances executable programs indirect supervision available examples labeled correct execution result program itself. consequently must search space programs output correct result misled spurious programs incorrect programs coincidentally output correct result. connect common learning paradigms learning maxireinforcement marginal likelihood present learning algorithm combines strengths both. algorithm guards spurious programs combining systematic search traditionally employed randomized exploration updating parameters probability spread evenly across consistent programs. apply learning algorithm neural semantic parser show signiﬁcant gains existing state-of-theart results recent context-dependent semantic parsing task. interested learning semantic parser maps natural language utterances executable programs example figure program corresponding utterance transforms initial world state world state. would like learn indirect supervision training example labeled correct output program produced outshow optimize closely related objectives. furthermore methods mechanism exploring program space search programs generate correct output. explain exploration tends quickly concentrate around short spurious programs causing model sometimes overlook correct program. address problem propose randomer learning algorithm parts first propose randomized beam search exploration strategy combines systematic beam search traditionally employed randomized off-policy exploration increases chance ﬁnding correct programs even beam size small parameters pre-trained. second observe even good exploration gradients objectives still upweight entrenched spurious programs strongly correct programs probability current model. propose meritocratic parameter update rule modiﬁcation gradient update equally upweights programs produce correct output. makes model less likely overﬁt spurious programs. apply randomer train neural semantic parser outputs programs stackbased programming language. evaluate resulting system scone context-dependent semantic parsing dataset long approach outperforms standard methods direct comparison achieves state-of-the-art results improving long three domains scone accuracy challenging one. consider semantic parsing task scone dataset illustrated figure example consists world containing several objects certain properties given initial world state sequence natural language utterances task generate program manipulates world state according utterances. utterance describes single action transforms world state world state training system receives weakly supervised examples input target ﬁnal world state dataset includes domains alchemy tangrams scene. description domain found appendix domains highlight different linguistic phenomena alchemy features ellipsis tangrams features anaphora actions scene features anaphora entities domain contains roughly training test examples. example contains utterances labeled target world state utterance target program. spurious programs. given training example goal true underlying program reﬂects meaning constraint must transform i.e. enough uniquely identify true often many satisfying experiments found least average example. alcapture meaning refer incorrect spurious programs. programs encourage model learn incorrect mapping language program operations e.g. spurious program figure would cause model learn yellow maps hasshirt. spurious programs scone. dataset utterances often reference objects different ways hence target programming language must also support different reference strategies. result even single action moving person target destination achieved many different programs selecting person destination different way. across multiple actions number programs grows combinatorially. programs actually implement correct reference strategy deﬁned utterance. problem would severe general-purpose language embedding token execution history embedding. compare options embedding execution history. standard approach simply take recent tokens zt−kt− concatenate embeddings. refer tokens experiments. also consider approach leverages ability incrementally execute programs using stack. summarize execution history embedding state stack time achieved concatenating embeddings values stack. refer stack. formulated task sequence prediction problem must still choose learning algorithm. ﬁrst compare standard paradigms reinforcement learning maximum marginal likelihood next section propose better alternative. comparing objective functions reinforcement learning. perspective given training example policy makes sequence decisions receives reward episode executes otherwise formulate program generation sequence prediction problem. represent program sequence program tokens postﬁx notation; example move leftof)) linearized yellow hashat blue hasshirt leftof move. representation also allows incrementally execute programs left right using stack constants pushed onto stack functions appropriate arguments stack push back computed result appendix lists full program tokens executed. note action always ends action token given input model generates program tokens left right using neural encoder-decoder model attention throughout generation process model maintains utterance pointer initialized generate model’s encoder ﬁrst encodes utterance vector then based previously generated tokens model’s decoder deﬁnes distribution possible values next token sampled distribution. action token generated model increments utterance pointer process terminates utterances processed. ﬁnal probability generating particular program encoder. utterance pointer encoded using bidirectional lstm ﬁxed glove word embedding word ﬁnal utterance embedding concatenation decoder. unlike bahdanau used recurrent network decoder feed-forward network simplicity. embedding previous execution history inputs taking step direction upweights probability heuristically think gradient attempting upweight reward-earning program gradient weight subsection argue qmml better guarding spurious programs propose even better alternative. policy gradient literature monte carlo integration typical approximation strategy. example popular reinforce algorithm uses monte carlo sampling compute unbiased estimate gradient programs come beam search. beam search. beam search generates programs following process. step beam search maintain beam search states. state represents partially constructed program state beam generate possible continuations maximum marginal likelihood. perspective assumes generated partially-observed random process conditioned latent program generated conditioned observation generated. implies marginal likelihood policy gradient think procedure used produce programs exploration strategy searches programs produce reward. advantage numerical integration allows decouple exploration strategy gradient weights assigned program. section illustrate spurious programs problematic commonly used methods describe problems propose solution each based insights gained comparison section spurious programs bias exploration mentioned section reinforce bsmml employ exploration strategy approximate respective gradients. methods exploration guided current model policy whereby programs high probability current policy likely explored. troubling implication programs probability current policy likely overlooked exploration. current policy incorrectly assigns probability correct program likely fail discover exploration consequently fail upweight probability repeats every gradient step keeping probability perpetually low. feedback loop also cause already highprobability spurious programs gain even probability. this exploration sensitive initial conditions rich richer poor poorer. since often thousands spurious programs correct programs spurious programs usually found ﬁrst. spurious programs head start exploration increasingly biases towards them. figure possible paths tree possible programs. path leads spurious program longer path leads correct program edge represents decision shows probability decision uniform policy. shorter program orders magnitude higher probability. model policy puts near-uniform probability decisions time step. however causes shorter programs orders magnitude higher probability longer programs illustrated figure empirically observe. sophisticated approach might involve approximating total number programs reachable point programgenerating decision tree. however instead propose reduce sensitivity initial distribution programs. solution randomized beam search solution biased exploration simply rely less untrustworthy current policy. injecting random noise exploration. reinforce common solution sample \u0001-greedy variant current policy. hand exploration beam search deterministic. however advantage reinforce-style sampling even program occupies almost probability under current policy beam search still remaining beam capacity explore least programs. contrast sampling methods repeatedly visit mode distribution. best worlds propose simple \u0001-greedy randomized beam search. like regular beam search iteration compute continuations cont sort model probability instead selecting highest-scoring continuations choose continuations without replacement cont. choosing continuation remaining pool either uniformly sample random continuation probability pick highest-scoring continuation pool probability empirically even perfect exploration visits every program gradient weights programs weighted proportional current policy probability. spurious program times higher probability figure gradient spend roughly magnitude upweighting towards towards even though programs reward. implies would take many updates catch fact never catch depending gradient updates training examples. simply increasing learning rate inadequate would cause model take overly large steps towards potentially causing optimization diverge. ﬁrst observe jmml already improves regard. gradient weight qmml policy distribution restricted renormalized rewardearning programs. renormalization makes gradient weight uniform across examples even reward-earning programs particular example model probability comz qmml always experiments jmml performs signiﬁcantly better however jmml assigns uniform weight across examples still uniform programs within example. hence propose update rule goes step pursuing uniform updates. extending qmml deﬁne β-smoothed version summary proposed approach described problems solutions reduce exploration bias using \u0001-greedy randomized beam search perform balanced optimization using β-meritocratic parameter update rule. call resulting approach randomer. table summarizes randomer combines desirable qualities reinforce bs-mml. evaluation. evaluate proposed methods three domains scone dataset. accuracy deﬁned percentage test examples model produces correct ﬁnal world state test examples also report accuracy processing ﬁrst utterances control effects randomness train instances model different random seeds. report median accuracy instances unless otherwise noted. training. following long decompose training example smaller examples. given example utterances consider length- length- substrings form training example substring e.g. models implemented tensorflow model parameters randomly initialized pre-training. adam optimizer learning rate minibatch size examples train accuracy validation converges problems concern gradient w.r.t. single example. full gradient averages multiple examples helps separate correct spurious. e.g. multiple examples mention yellow correct program parsing hashat example whereas spurious programs follow consistent pattern. consequently spurious gradient contributions cancel correct program gradients vote direction. table randomer combines qualities reinforce bs-mml. approximating expectation gradient numerical integration bs-mml. exploration strategy hybrid search off-policy sampling gradient weighting equivalent meritocratic reinforce lower values hyperparameters. models performed grid search hyperparameters maximize accuracy validation set. hyperparameters include learning rate baseline reinforce \u0001-greediness βmeritocraticness. reinforce also experimented regression-estimated baseline found perform worse constant baseline. main results comparison prior work. table compares randomer results long well baselines reinforce bsmml approach achieves state-of-the-art results signiﬁcant margin especially scene domain features complex program syntax. report results reinforce bs-mml randomer seed hyperparameters achieve best validation accuracy. also found reinforce required \u0001greedy exploration make progress. using \u0001-greedy greatly skews monte carlo approximation ∇jrl making uniformly weighted programs similar spirit using β-meritocratic gradient weights however increases uniformity reward-earning programs only rather programs. effect randomized beam search. table shows \u0001-greedy randomized beam search consistently outperforms classic beam search. even increase beam size classic beam effect β-meritocratic updates. table evaluates impact β-meritocratic parameter updates uniform upweighting across reward-earning programs leads higher accuracy fewer spurious programs especially scene. however single value performs best domains. choosing right value randomer signiﬁcantly accelerates training. figure illustrates ultimately achieve similar accuracy alchemy reaches good performance half time. since lowering reduces trust model policy helps early training current policy untrustworthy. however grows trustworthy begins price ignoring hence worthwhile anneal towards time. effect execution history embedding. table compares proposals embedding execution history tokens stack. stack performs better domains object referenced multiple ways stack directly embeds objects stack invariant pushed onto stack unlike tokens. hypothesize invariance increases robustness spurious behavior program accidentally pushes right object onto stack spurious means model still learn remaining steps program without conditioning spurious history. fitting overﬁtting training data. table reveals bs-mml randomer different strategies training data. depicted training example bs-mml actually achieves higher expected reward marginal probability randomer putting probability spurious program— form overﬁtting. contrast randomer spreads probability mass multiple rewardearning programs including correct ones. consequence overﬁtting observed test time bs-mml references people positional indices instead shirt color whereas randomer successfully learns multiple reference strategies. figure validation accuracy across training iterations alchemy. compare randomer bs-mml reinforce. vertical lines mark ﬁrst time model surpasses accuracy. randomer reaches point twice fast reinforce plateaus long time begins climb iterations training runs averaged seeds. zettlemoyer reddy pasupat liang interested initial stages training scratch getting training signal difﬁcult combinatorially large search space. also highlighted problem spurious programs capture reward give incorrect generalizations. likelihood beam search traditionally used learn semantic parsers indirect supervision. reinforcement learning. concurrently recent surge interest reinforcement learning along wide application classic reinforce algorithm troubleshooting dialog generation game playing coreference resolution machine translation even semantic parsing indeed challenge training semantic parsers indirect supervision perhaps better captured notion sparse rewards reinforcement learning. answer would better exploration take many forms including simple action-dithering \u0001-greedy entropy regularization monte carlo tree search randomized value functions methods prioritize learning environment dynamics under-explored states majority methods employ monte carlo sampling exploration. table top-scoring predictions training example scene randomer distributes probability mass numerous reward-earning programs classic beam search overﬁts spurious program giving high probability. contrast randomized beam search suitable setting explores low-probability states even policy distribution peaky. β-meritocratic update also depends fact beam search returns entire reward-earning programs rather since renormalizes reward-earning set. similar entropy regularization βmeritocratic update targeted increases uniformity gradient among rewardearning programs rather across programs. strategy using randomized beam search meritocratic updates lies closer imply nothing offer setting. simple connection established much literature exploration variance reduction directly applied problems. special interest methods incorporate value function actor-critic. maximum likelihood tempting group approach sequence learning methods interpolate supervised learning reinforcement learning methods generally seek make training easier pre-training warm-starting fully supervised learning. requires training example labeled reasonably correct output sequence. setting would amount labeling example correct program known. hence methods cannot directly applied. without access correct output sequences cannot directly maximize likelihood instead resort maximizing marginal likelihood rather proposing form pre-training argue superior substitute standard objective β-meritocratic update even better. simulated annealing. β-meritocratic update employs exponential smoothing bears resemblance simulated annealing strategy smith eisner shen however difference methods smooth objective function whereas smooth expectation gradient. underscore difference note ﬁxing method quite effective whereas total smoothing simulated annealing methods would correspond completely objective function uninformative gradient zero everywhere. neural semantic parsing. recent interest using recurrent neural networks semantic parsing modeling logical forms end-to-end execution develop neural model context-dependent setting made possible stackbased language similar riedel devin ghemawat goodfellow harp irving isard j´ozefowicz kaiser kudlur levenberg man´e monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warden wattenberg wicke zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. artzi zettlemoyer. weakly supervised learning semantic parsers mapping instructions actions. transactions association computational linguistics bellemare srinivasan ostrovski schaul saxton munos. unifying countadbased exploration intrinsic motivation. vances neural information processing systems pages bengio vinyals jaitly shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems pages branavan chen zettlemoyer barzilay. reinforcement learning mapping inassociation compustructions actions. tational linguistics international joint conference natural language processing pages glorot bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics. krishnamurthy mitchell. weakly emsupervised training semantic parsers. pirical methods natural language processing computational natural language learning pages norouzi bengio jaitly schuster schuurmans reward augmented maximum likelihood neural structured prediction. advances neural information processing systems. pages smith eisner. minimum risk aninternanealing training log-linear models. tional conference computational linguistics association computational linguistics pages sutton mcallester singh mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems push fraction color push list beakers chemical color beaker number fraction perform remove units chemical beakers perform transfer chemical beaker perform turn color chemical brown additional tokens tangrams domain tangrams world contains tangram pieces different shapes. shapes anonymized; tangram referred index history reference shape. swap additional tokens scene domain scene world linear stage positions. position occupied person colored shirt optionally colored hat. usually people stage. nohat hasshirt hashat push pseudo-color color push list people shirt color colors push list people shirt color color person push location index left right number colors perform person position shirt color color person number perform move position people perform exchange hats person perform remove stage", "year": 2017}