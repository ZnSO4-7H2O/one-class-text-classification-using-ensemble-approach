{"title": "Block-Sparse Recurrent Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80% to 90% with small loss in accuracy. This allows us to reduce the model size by roughly 10x. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32x32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.", "text": "recurrent neural networks used state-of-the-art models domains speech recognition machine translation language modelling. sparsity technique reduce compute memory requirements deep learning models. sparse rnns easier deploy devices high-end server processors. even though sparse operations need less compute memory relative dense counterparts speed-up observed using sparse operations less expected different hardware platforms. order address issue investigate different approaches induce block sparsity rnns pruning blocks weights layer using group lasso regularization create blocks weights zeros. using techniques demonstrate create block-sparse rnns sparsity ranging small loss accuracy. allows reduce model size roughly additionally prune larger dense network recover loss accuracy maintaining high block sparsity reducing overall parameter count. technique works variety block sizes block-sparse rnns eliminate overheads related data storage irregular memory accesses increasing hardware efﬁciency compared unstructured sparsity. improvements several applications speech recognition language modeling machine translation result large recurrent neural networks trained large scale datasets. datasets available train models grown model sizes. deployment large models compute memory intensive. pruning deep neural networks effective strategy reduce overall memory compute requirements models however approaches induce random unstructured sparsity weight matrices. speed-up obtained random sparsity various hardware platforms lower expected narang diamos sparse formats efﬁciently utilize hardware resources storage overheads irregular memory access inability take advantage array data-paths modern processors. block sparsity address issues. saving indices non-zero blocks instead indices non-zero elements reduces storage overhead factor block size. block-sparse formats store blocks contiguously memory reducing irregular memory accesses. block sparsity inherently allows take advantage array-data-path modern processors. order induce block sparsity rnns propose block pruning approach zeros blocks weights matrix network training. training algorithm creates block-sparse rnn. addition pruning technique examine efﬁcacy group lasso regularization induce block sparsity network. also combine group lasso regularization block pruning. demonstrate block pruning group lasso regularization pruning successful creating block-sparse rnns. inducing block sparsity blocks vanilla rnns gated recurrent units results loss accuracy compared dense baseline. model size reduces nearly block sizes scaled approach. larger blocks require lower sparsity maintain similar accuracy. also reduce accuracy loss starting larger dense matrix baseline pruning still reducing number parameters compared baseline. approach agnostic optimization algorithm require hyper-parameter retuning furthermore since approach require re-training model training time remains same. several approaches reduce network size pruning model. hanson pratt several bias techniques decay weights network. lecun hassibi hessian-based approaches prune weights certain threshold. simpler approaches like sorting thresholding used prune neural network. prune convolution neural networks maintaining high accuracy. hard threshold prune deep learning models. narang gupta prune recurrent neural networks initial training small accuracy loss using gradual pruning. unlike technique approaches induce random unstructured sparsity neural networks. several approaches exist induce structured sparsity neural networks. simple threshold based technique create structurally sparse cnns. propose scalpel prunes cnns taking account underlying target hardware architecture. alter structure long short term memory create lstms smaller memory footprint. demonstrate technique works language modeling penn tree bank dataset. approach works vanilla models trained large-scale datasets speech recognition. group lasso regularization used efﬁcient method generating sparse structures group lasso regularization induce structured sparsity convolutional neural networks. regularization known method induce sparsity deep neural networks best knowledge none approaches used rnns trained large-scale datasets. approaches reduce compute memory footprint deep learning models include quantization low-rank factorization approach orthogonal methods combined them. approach pruning deep learning models builds work narang propose weight pruning algorithm introduces random unstructured sparsity rnns. work propose pruning weights monotonically increasing threshold. pruning strategy impose structure weights. extend approach prune blocks matrix instead individual weights. order prune blocks pick weight maximum magnitude representative entire block. maximum magnitude block current threshold weights block zeros. figure depicts process generating block-sparse mask weight matrix given threshold. block-sparse mask multiplied weights generate block-sparse weight matrix. monotonically growing threshold causes blocks pruned training progress. stop pruning blocks around training completed. blocks iteration start pruning iteration increase rate pruning iteration stop pruning parameters initial rate increasing threshold rate increasing threshold ramp iteration number iterations updated narang hyper-parameters determine threshold given iteration. table provides description heuristics hyper-parameters. start slope ramp slope determine rate threshold increases. order determine start slope recommend using weights existing dense model. achieve sparsity assign weight percentile absolute values weight matrix. assuming equation determine block pruning need modify start slope take account number elements block order calculate start slope ﬁrst calculate start slope weight pruning using equation given suggest using equation determine initial slope block pruning. based empirical results found using approach allows achieve block sparsity ranging tuning hyper-parameters required achieve desired block sparsity. prune recurrent fully connected layers network using block size. pruning hyper-parameters type layer network recurrent weight layer linear/fully connected layer. group lasso type weight regularization works groups weights zero weights group. order induce block sparsity network divide weights model blocks. block loss term proportional norm block. block weights norm block total number block. norm variant general group lasso deﬁned yuan kn)/. group lasso property large enough drive weights within certain groups hard zeros. thus explore group lasso regularization produce block-structured sparsity. choose appropriate constant duration training. interpretation weight regularization less important weights driven towards zero important weights retain large absolute values. thus combine group lasso block pruning group lasso guides selection blocks prune. apply group lasso regularization coincide pruning schedule. turn regularization pruning schedule ends typically around training epochs. discussed section weights already zero remain unchanged point. group lasso related wellknown regularization. appendix discuss exploration regularization combined weight pruning. block sparsity experiments different speech recognition models amodei model consists convolutional layer followed seven bidirectional recurrent layers connectionist temporal classiﬁcation layer baseline model consists hidden units recurrent layer nearly million parameters. model consists convolutional layers three recurrent layers cells layer. baseline model consists hidden units layer total million parameters. dataset used training models consists hours english speech. validation consisting hours data. character error rate results reported independent test consisting hours english data. order introduce block sparsity rnns three different types experiments block pruning group lasso group lasso block pruning prune weights recurrent layers fully connected layers. biases batchnormalization parameters weights convolutional layers pruned since account small portion total weights network. besides pruning hyper-parameters hyper-parameter changes required sparse training runs. models trained using nesterov stochastic gradient descent momentum. models trained epochs. dense models trained without regularization. section report results different sparse models pruned blocks. section compares results different group lasso experiments. section discusses impact varying block size accuracy model. initially prune baseline models. using able reduce parameter count models nearly shown table sparse model hidden units overall block sparsity relative loss accuracy secondly train dense models fewer parameters determine sparsity reducing overﬁtting large dense baseline models. models train dense model hidden units layer resulting approximately number parameters ﬁnal sparse models. table shows dense models perform worse sparse models models. large sparse models better approach reduce parameter count dense small models. finally train sparse models hidden units recurrent layers recover accuracy. models increase hidden layer size shown table sparse worse dense baseline model. sparse models reduce overall parameter count ×and .×respectively. similarly pruning model hidden nodes reduces accuracy loss still shrinking model evaluation show inducing block sparsity baseline model allows reduce model size approximately ×with small loss accuracy. pruning model larger baseline model allows reduce accuracy loss reducing model size nearly results also indicate large sparse models result better accuracy small dense models. table highlights results experiments different models. models hidden nodes group lasso without pruning signiﬁcantly worse combining group lasso block pruning methodology. order achieve high sparsity need relatively high value. instance experiments using required approximately ×larger experiments. high regularization factor hurts model accuracy. dense baseline model trained without regularization. even without regularization dense model overﬁt training dataset. group lasso experiments underﬁt training data high value group lasso could successful inducing sparsity dense model overﬁts training dataset. experiments reduce regularization factor since pruning forces smaller magnitude weights zero. combined approach results improved accuracy maintaining high levels sparsity. table shows results varying block size pruning baseline models. increasing block size requires reducing sparsity respectively models obtain good accuracy. similar results hold true model well. large sparse blocks reduce memory overhead storing zero values take advantage array data-paths modern processors. therefore even though large blocks achieve lower sparsity result lower memory compute requirements. primary advantage block-sparse format increase hardware efﬁciency making computation regular. sparse formats incur least three types overhead indexing overhead irregular memory accesses incompatibility array-data-paths mitigated using larger block sizes. indexing overheads. sparse formats extra memory track location non-zero value. example compressed-sparse-row format uses approximately extra index values non-zero value. size extra index values depends maximum matrix size. using -bit indices incurs -bits overhead non-zero value allows matrices supported. assuming neural network weights represented -bits micikevicius overhead. block sparsity reduces overhead factor block size index shared entire block. example using block size reduces memory bloat using block size reduces overhead less irregular memory accesses. caches lines dram buffers tlbs provide best performance memory accessed relatively large contiguous units opposed ﬁne-grained random accesses. block-sparse formats store blocks contiguously memory resulting large coalesced accesses. array data-paths. fine-grained sparsity cannot directly take advantage array-data-paths tensorcore units volta described nvidia units google described jouppi signiﬁcant advantages using units example volta enable higher throughput figure speed-up sparse matrix dense matrix multiply. benchmarks titanx maxwell using cusparse library. sparse matrices represented format. matrix sizes sparsity matrix sizes sparsity results shown matrices weight pruning block pruning simd data-paths. order keep units busy block size least large hardware data-path size figure shows block-sparse matrices achieve higher speed-up unstructured sparsity large batch sizes. case speed-up achieved reducing irregular memory accesses improving load balance. blocks higher speed-up blocks. investigation needed understand behavior. figure plot pruning schedule recurrent linear layer bidirectional model trained weight pruning three algorithms pruning begins ﬁrst epoch iterations. models result sharper curve weights zero short span iterations. experiments function reduce blocks single value could cause sharpness pruning. also model reaches sparsity iterations signiﬁcantly earlier model. training encourages sparsity early training pushing blocks weights towards zero. figure shows histogram number output connections neurons network models different sparsity pruned sparse model signiﬁcantly worse sparse. model sparsity neurons output weights zero total model produced good accuracy relative dense baseline. however increasing sparsity layer results neurons zero output weights. additionally neurons smaller number non-zero output weights. using baseline model many weight block pruning experiments varying hyperparameters produce spectrum results ranging sparsity. experiments models trained epochs accuracy measured validation instead test set. therefore relative accuracy models slightly different results reported section shown figure models pruned using sparsity less relative accuracy ranging increasing sparsity model beyond results accuracy loss. accuracy cliff earlier models pruned block sparsity. block size models sparsity greater yield relative accuracy figure figure shows pruning schedule layers network models. models block size figure plots histogram number output connections neurons network using block pruning blocks. figure figure shows relative accuracy different block sizes varying sparsity model. models relative accuracy worse capped figure shows sparsity different recurrent layers network model pruned using loss higher. similarly blocks models sparsity greater accuracy loss. similar trend observed block size indicates tradeoff sparsity block size accuracy model. figure shows sparsity recurrent layers network using recurrent layers pruning hyper-parameters. layer ﬁrst recurrent layer layer ﬁnal recurrent layer cost layer. block pruning weight pruning initial layers pruned aggressively compared ﬁnal layers. increasing sparsity layers closer output results poor accuracy. additionally variance sparsity across layers increases block size. increasing variance makes harder increase block size beyond pruning hyper-parameters recurrent layers. block-sparse models signiﬁcantly fewer parameters dense baselines reducing memory requirements. block-sparse models take advantage underlying hardware efﬁciently. would like investigate pruning performed even earlier training thereby allowing train sparse models. training sparse models would allow reap beneﬁts sparsity training resulting lesser compute memory demands. work remains implement efﬁcient block-sparse matrix denese matrix/vector multiplies processors would provide increased speed-up deployment. would like thank song mohammad shoeybi markus kliegl helpful discussions related work. would also like thank varun arora creating ﬁgure paper. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos deep speech end-to-end speech recognition proceedings international conference machine learning english mandarin. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. emily denton wojciech zaremba joan bruna yann lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. corr abs/. http//arxiv.org/ abs/.. julian faraone nicholas fraser giulio gamberdella michaela blott philip leong. compressing precision deep neural networks using sparsity-induced regularization ternary networks. arxiv preprint arxiv. alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning suyog gupta ankur agrawal kailash gopalakrishnan pritish narayanan. deep learning limited numerical precision. proceedings international conference machine learning stephen jos´e hanson lorien pratt. advances neural information processing systems chapter comparing biases minimal network construction back-propagation morgan kaufmann publishers inc. francisco isbn ---. http//dl.acm.org/ citation.cfm?id=.. norman jouppi cliff young nishant patil david patterson gaurav agrawal raminder bajwa sarah bates suresh bhatia boden borchers rick boyle pierre-luc cantin clifford chao chris clark jeremy coriell mike daley matt jeffrey dean gelb tara vazir ghaemmaghami rajendra gottipati william gulland robert hagmann richard doug hogberg john robert hundt hurt julian ibarz aaron jaffey alek jaworski alexander kaplan harshit khaitan andy koch naveen kumar steve lacy james laudon james diemthu chris leary zhuyuan kyle lucke alan lundin gordon mackean adriana maggiore maire mahony kieran miller rahul nagarajan ravi narayanaswami kathy thomas norrie mark omernick narayana penukonda andy phelps jonathan ross amir salek emad samadiani chris severn gregory sizikov matthew snelham souter steinberg andy swing mercedes gregory thorson tian horia toma erick tuttle vijay vasudevan richard walter walter wang eric wilcox hyun yoon. in-datacenter performance analysis tensor processing unit. corr abs/. http//arxiv.org/abs/.. baoyuan wang hassan foroosh marshall tappen marianna pensky. sparse convolutional neural networks. proceedings ieee conference computer vision pattern recognition mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks springer international publishing cham ./---- https //doi.org/./----_. yuxiong samyam rajbhandari wenhan wang fang yiran chen learning intrinsic sparse structures within long short-term memory. arxiv preprint arxiv. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. corr abs/. http//arxiv.org/abs/. dong frank seide gang deng. exploiting sparseness deep neural networks large vocabulary speech recognition. ieee international conference acoustics speech signal processing ieee jiecao andrew lukefahr david palframan ganesh dasika reetuparna scott mahlke. scalpel customizing pruning underlying hardware parallelism. proceedings annual international symposium computer architecture prior work group lasso regularization considered regularizers induce sparsity network. regularizers individual weights could inducing unstructured sparsity network. regularization deﬁned group lasso experiments described explore regularization without pruning. weight pruning algorithm narang used along regularization. motivation group lasso block sparsity experiments either guide pruning produce sparsity directly. also explore regularization deﬁned uses regularization produce sparsity directly. gradient regular|wj|−/. term smaller weights larger magnitude. expectation ization drive unimportant weights towards zero leaving large weights relatively unaffected thus avoiding accuracy loss associated excessive regularization. experiments deep speech bidirectional baseline model described section models trained epochs internal training dataset hours. results reported independent test consisting hours. without pruning model results signiﬁcantly worse accuracy compared dense baseline. combining weight pruning allows recover loss accuracy similar sparsity. pruning model performs worse pruning model. comparing regularizers result indicates better guiding pruning suitable regularizer both. similar group lasso experiments regularization experiments require signiﬁcantly higher achieve high sparsity without pruning. suspect regularizers would successful inducing sparsity models overﬁt training training dataset.", "year": 2017}