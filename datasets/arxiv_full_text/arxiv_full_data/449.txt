{"title": "Quantifying the vanishing gradient and long distance dependency problem  in recursive neural networks and recursive LSTMs", "tag": ["cs.AI", "cs.CL", "cs.NE"], "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.", "text": "recursive neural networks recently proposed extension recursive long short term memory networks models compute representations sentences recursively combining word embeddings according externally provided parse tree. models thus unlike recurrent networks explicitly make hierarchical structure sentence. paper demonstrate rnns nevertheless suffer vanishing gradient long distance dependency problem rlstms greatly improve rnn’s problems. present artiﬁcial learning task allows quantify severity problems models. show ratio gradients highly indicative success backpropagation optimizing relevant weights tree. paper thus provides explanation existing superior results rlstms tasks sentiment analysis suggests beneﬁts including hierarchical structure including lstm-style gating complementary. recursive neural network model became popular since work socher employed tackle several tasks syntactic parsing machine translation word embedding learning like traditional recurrent neural nethowever works seems suffer vanishing gradient problem error signals propagating root parse tree child nodes shrink quickly. moreover encounters difﬁculties capturing long range dependencies information propagating child nodes deep parse tree obscured reaching root node. recurrent neural network world long short term memory architecture often used solution problems. natural extension lstm deﬁned tree structures call recursive lstm proposed independently zuidema however intensive research showing lstm architecture overcome problems compared traditional recurrent models research knowledge still absent comparison rnns rlstms. therefore current paper investigate following questions supervised learning requires annotated data often expensive collect. result examining model natural data many different aspects difﬁcult portion data speciﬁc aspect could sufﬁcient. moreover studying individual aspects separately hard since many aspects often correlated other. this unfortunately true case answering questions requires evaluate examined models datasets different tree depths nodes contain decisive information parse tree must identiﬁed. using available annotated corpora stanford sentiment treebank penn treebank thus inappropriate small purpose nodes marked. solution artiﬁcial task sentences parse trees randomly generated arbitrary constraints tree depth node’s position. rlstm model instances general framework takes sentence syntactic tree vector representations words sentence input applies composition function recursively compute vector representations phrases tree complete sentence. technically speaking given production representing compute composition function. one-layer feed-forward neural network. rlstm node represented vector resulting concatenating vector representing phrase node covers memory vector. could lstm combine concatenation vectors structure-lstm tree-lstm lstm-rnn current paper implementation zuidema examine problems vanishing gradient problem problem capture long range dependencies affect rlstm model model. propose following artiﬁcial task requires model distinguish useful signals noise. deﬁne task predict class sentence given binary parse tree label sentence determined solely keyword models need identify keyword parse tree allow information leaf node keyword affect root node. worth noting task resembles sentiment analysis simple cases sentiment whole sentence determined keyword simulating complex cases involving negation composition etc. straightforward future work. believe current task adequate answer questions raised section models rlstm implemented dimension vector representations vector memories following socher used tanh activation function initialized word vectors randomly sampling value uniform distribution trained models using adagrad method learning rate mini-batch size rlstm. development sets employed early stopping experiment randomly generated datasets. generate sentence length shufﬂe list randomly chosen non-keywords keyword. i-th dataset contains sentences lengths tokens tokens split train test sets sizes sentences. parsed sentence randomly generating binary tree whose number leaf nodes equals sentence length. test accuracies models datasets shown figure dataset model times reported highest accuracy model distribution accuracies rlstm model. model performs reasonably well short sentences however sentence length exceeds rnn’s performance drops quickly difference random guess’ performance negligible. trying different learning rates mini-batch sizes values give signiﬁcant differences. hand rlstm model achieves accuracy sentences shorter tokens. performance drops sentence length increases still substantially better random guess sentence length exceed sentence length exceeds rlstm perform similarly. periment kept tree size ﬁxed vary keyword depth. generated pool sentences lengths tokens parsed randomly generating binary trees. created datasets trees i-th dataset consists trees distances keywords roots figure shows test accuracies models datasets. similarly experiment dataset model times reported highest accuracy model distribution accuracies rlstm model. model achieves high accuracies keyword depth exceed performance drops rapidly gets close performance random guess. evidence model difﬁculty capturing long range dependencies. contrast rlstm model performs accuracy depth keyword reaches difﬁculty dealing larger depths performance always better random guess. examine whether models encounter vanishing gradient problem. looked back-propagation phase model experiment third dataset tree calculated ratio numerator norm error vector keyword node denominator norm error vector root node. ratio gives intuition error signals develop propagating backward leaf nodes ratio vanishing gradient problem occurs; else ratio observe exploding gradient problem. keyword node depth epoch training model. ratios ﬁrst epoch always small. following epoch model successfully lifts ratios steadily clear decrease depth becomes larger observable. rlstm model story somewhat different. ratios epochs rapidly even exploding error signals sent back leaf nodes. subsequently remain stable substantially less exploding error signals. interestingly concurrent performance rlstm model development seems rlstm model epoch quickly locates keyword node tree relates root building strong bond error signals. correlation keyword label root found tries stabilize training reducing error signals sent back keyword node. comparing models aligning figure figure figure figure rlstm model capable transmitting error signals leaf nodes. worth noting vanishing gradient problem happening training model figure figure suggests problem become less serious long enough training time. might depth still manageable model. fact model still doesnot perform better random guessing explained using arguments given bengio show trade-off figure ratios norms error vectors keyword nodes norms error vectors root nodes w.r.t. keyword node depth epoch training rnn. gradients gradually vanish greater depth. figure ratios norms error vectors keyword nodes norms error vectors root nodes rlstm. many gradients explode epoch stabilize later. gradients vanish even depth richard socher john bauer christopher manning andrew parsing compoproceedings sitional vector grammars. annual meeting association computational linguistics pages richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings emnlp. sheng richard socher christopher manning. improved semantic representations tree-structured long short-term memory networks. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. experimental results show rlstm superior terms overcoming vanishing gradient problem capturing long term dependencies. parallel general conclusions power lstm architecture compared traditional recurrent neural networks. future work focus complex cases involving negation composition etc. references yoshua bengio patrice simard paolo frasconi. learning long-term dependencies gradient descent difﬁcult. neural networks ieee transactions john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. journal machine learning research pages felix gers j¨urgen schmidhuber. lstm recurrent networks learn simple context-free neural networks context-sensitive languages. ieee transactions phong willem zuidema. compositional distributional semantics long short term memory. proceedings joint conference lexical computational semantics association computational linguistics. shujie yang ming zhou. recursive recurrent neural network statistical machine translation. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. richard socher christopher manning andrew learning continuous phrase representations syntactic parsing recursive neural networks. proceedings nips- deep learning unsupervised feature learning workshop.", "year": 2016}