{"title": "Modeling Relational Data with Graph Convolutional Networks", "tag": ["stat.ML", "cs.AI", "cs.DB", "cs.LG"], "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.", "text": "knowledge graphs enable wide variety applications including question answering information retrieval. despite great effort invested creation maintenance even largest remain incomplete. introduce relational graph convolutional networks apply standard knowledge base completion tasks link prediction entity classiﬁcation rgcns related recent class neural networks operating graphs developed speciﬁcally deal highly multi-relational data characteristic realistic knowledge bases. demonstrate effectiveness r-gcns stand-alone model entity classiﬁcation. show factorization models link prediction distmult signiﬁcantly improved enriching encoder model accumulate evidence multiple inference steps relational graph demonstrating large improvement fbk- decoder-only baseline. knowledge bases organize store factual knowledge enabling multitude applications including question answering information retrieval even largest knowledge bases despite enormous effort invested maintenance incomplete lack coverage harms downstream applications. predicting missing information knowledge bases main focus statistical relational learning following previous work assume knowledge bases store collections triples form consider example triple refer baryshnikov vaganova academy entities educated relation. additionally assume entities labeled types edge node label shown missing information inferred. academy marked university). convenient represent knowledge bases directed labeled multigraphs entities corresponding nodes triples encoded labeled edges consider fundamental tasks link prediction entity classiﬁcation cases many missing pieces information expected reside within graph encoded neighborhood structure i.e. knowing mikhail baryshnikov educated vaganova academy implies mikhail baryshnikov label person triple must belong knowledge graph. following intuition develop encoder model entities relational graph apply tasks. entity classiﬁcation model similarly kipf welling uses softmax classiﬁers node graph. classiﬁers take node representations supplied relational graph convolutional network predict labels. model including r-gcn parameters learned optimizing cross-entropy loss. link prediction model regarded autoencoder consisting encoder r-gcn producing latent feature representations entities decoder tensor factorization model exploiting representations type transformation shown effective accumulating encoding features local structured neighborhoods signiﬁcant improvements areas graph classiﬁcation graph-based semi-supervised learning intuitively accumulates transformed feature vectors neighboring nodes normalized sum. different regular gcns introduce relation-speciﬁc transformations i.e. depending type direction edge. ensure representation node layer also informed corresponding representation layer single self-connection special relation type node data. note instead simple linear message transformations could choose ﬂexible functions multi-layer neural networks leave future work. neural network layer update consists evaluating parallel every node graph. practice implemented efﬁciently using sparse matrix multiplications avoid explicit summation neighborhoods. multiple layers stacked allow dependencies across several relational steps. refer graph encoder model relational graph convolutional network computation graph single node update r-gcn model depicted figure regularization central issue applying highly multi-relational data rapid growth number parameters number relations graph. practice easily lead overﬁtting rare relations models large size. predict labeled edges. though principle decoder rely type factorization simplest effective factorization methods distmult observe method achieves competitive results standard benchmarks outperforming among baselines direct optimization factorization improvement especially large consider challenging fbk- dataset result demonstrates explicit modeling neighborhoods r-gcns beneﬁcial recovering missing facts knowledge bases. main contributions follows. best knowledge ﬁrst show framework applied modeling relational data speciﬁcally link prediction entity classiﬁcation tasks. secondly introduce techniques parameter sharing enforce sparsity constraints apply r-gcns multigraphs large numbers relations. lastly show performance factorization models example distmult signiﬁcantly improved enriching encoder model performs multiple steps information propagation relational graph. introduce following notation denote directed labeled multi-graphs nodes labeled edges relation type. relational graph convolutional networks model primarily motivated extension gcns operate local graph neighborhoods large-scale relational data. related methods graph neural networks understood special cases simple differentiable message-passing framework hidden state node l-th layer neural network dimensionality layer’s representations. incoming messages form accumulated passed element-wise activation function relu max. denotes incoming messages node often chosen identical incoming edges. typically chosen neural network-like function simply linear transformation weight matrix kipf welling figure depiction r-gcn model entity classiﬁcation per-node loss function. link prediction model r-gcn encoder distmult decoder takes pairs hidden node representations produces score every edge graph. loss evaluated edge. supervised classiﬁcation nodes simply stack r-gcn layers form softmax activation output last layer. minimize following cross-entropy loss labeled nodes node indices labels k-th entry network output i-th labeled node. denotes respective ground truth label. practice train model using gradient descent techniques. schematic depiction entity classiﬁcation model given figure link prediction deals prediction facts formally knowledge base represented directed labeled graph rather full edges given incomplete subset task assign scores possible edges order determine likely edges belong order tackle problem introduce graph auto-encoder model comprised entity encoder scoring function encoder maps entity real-valued vector decoder reconstructs edges graph relying vertex representations; words scores -triples function existing approaches link prediction r-gcn model. activations neighboring nodes gathered transformed relation type individually resulting representation accumulated passed activation function per-node update computed parallel shared parameters across whole graph. basis function decomposition seen form effective weight sharing different relation types block decomposition seen sparsity constraint weight matrices relation type. block decomposition structure encodes intuition latent features grouped sets variables tightly coupled within groups across groups. decompositions reduce number parameters needed learn highly multi-relational data time expect basis parameterization alleviate overﬁtting rare relations parameter updates shared rare frequent relations. overall r-gcn model takes following form stack layers deﬁned output previous layer input next layer. input ﬁrst layer chosen unique one-hot vector node graph features present. block representation one-hot vector dense representation single linear transformation. toutanova yang trouillon interpreted framework. crucial distinguishing characteristic work reliance encoder. whereas previous approaches single real-valued vector every optimized directly training compute representations r-gcn encoder similar graph auto-encoder model introduced kipf welling unlabeled undirected graphs. full link prediction model schematically depicted figure experiments distmult factorization scoring function known perform well standard link prediction benchmarks used own. distmult every relation associated diagonal matrix rd×d triple scored previous work factorization train model negative sampling. observed example sample negative ones. sample randomly corrupting either subject object positive example. optimize cross-entropy loss push model score observable triples higher negative ones entity classiﬁcation experiments here consider task classifying entities knowledge base. order infer example type entity successful model needs reason relations entities entity involved datasets evaluate model four datasets resource description framework format aifb mutag relations datasets need necessarily encode directed subject-object relations also used encode presence absence speciﬁc feature given entity. dataset targets classiﬁed properties group entities represented nodes. exact statistics datasets found table detailed description datasets reader referred ristoski vries paulheim remove relations used create entity labels employs afﬁliation aifb ismutagenic mutag haslithogenesis objectcategory material baselines baseline experiments compare recent state-of-the-art classiﬁcation results rdfvec embeddings weisfeiler-lehman kernels hand-designed feature extractors feat assembles feature vector inout-degree every labeled entity. rdfvec extracts walks labeled graphs processed using skipgram model generate entity embeddings used subsequent classiﬁcation. ristoski paulheim in-depth description discussion baseline approaches. entity classiﬁcation experiments nodes memory. results results table reported train/test benchmark splits ristoski vries paulheim aside training validation hyperparameter tuning. r-gcn report performance -layer model hidden units basis function decomposition trained adam epochs using learning rate normalization constant chosen details models hyperparameter choices provided supplementary material. table entity classiﬁcation results accuracy feature-based baseline rdfvec r-gcn test performance reported train/test splits provided ristoski vries paulheim datasets furthermore corresponds version model ﬁxed entity embeddings place r-gcn encoder described section second baseline simple neighbor-based linkfeat algorithm proposed toutanova chen compare complex hole state-ofthe-art link prediction models complex facilitates modeling asymmetric relations generalizing distmult complex domain hole replaces vector-matrix product circular correlation. finally include comparisons classic algorithms transe results provide results using commonly used evaluation metrics mean reciprocal rank hits following bordes metrics computed ﬁltered setting. report ﬁltered ﬁltered hits evaluate hyperparameter choices respective validation splits. found normalization constant deﬁned words applied across relation types work best. report results using basis decomposition basis functions single encoding layer -dimensional embeddings. fbk- found block decomposition perform best using layers block dimension -dimensional embeddings. regularize encoder edge dropout applied normalization dropout rate self-loops edges. using edge droupout makes training objective similar denoising autoencoders apply regularization decoder penalty adam optimizer learning rate baseline factorizations found parameters trouillon apart dimensionality fbk- work best though make systems comparable maintain number negative samples full-batch optimization baselines model. datasets. mutag dataset molecular graphs later converted format relations either indicate atomic bonds merely presence certain feature. dataset rock types hierarchical feature descriptions similarly converted format relations encode presence certain feature feature hierarchy. labeled entities mutag connected high-degree nodes encode certain feature. conjecture ﬁxed choice normalization constant aggregation messages neighboring nodes partly blame behavior particularly problematic nodes high degree. potential overcome limitation introduce attention mechanism i.e. replace normalization constant /cir data-dependent attention weights aijr aijr expect promising avenue link prediction experiments shown previous section r-gcns serve effective encoder relational data. combine encoder model scoring function score candidate triples link prediction knowledge bases. datasets link prediction algorithms commonly evaluated subset relational database freebase subset wordnet containing lexical relations words. toutanova chen serious observed datasets presence inverse triplet pairs training test set. reduces large part prediction task memorization affected triplet pairs. simple baseline linkfeat employing linear classiﬁer sparse feature vectors observed training relations shown outperform existing systems large margin. address issue toutanova chen proposed reduced dataset fbk- inverse triplet pairs removed. therefore choose fbk- primary evaluation dataset. since still widely used also include results datasets using splits introduced bordes table results freebase wordnet datasets. results marked taken trouillon results marks taken nickel rosasco poggio r-gcn+ denotes ensemble r-gcn distmult main text details. expected dominate performance factorizations contrasting design r-gcn model. better understand difference plot figure performance best r-gcn model baseline functions degree nodes corresponding entities considered triple seen model performs better nodes high degree contextual information abundant. observation models complementary suggests combining strengths single model refer r-gcn+. local longdistance information provide strong solutions expect r-gcn+ outperform individual model. fbk- local information less salient expect combination model outperform pure r-gcn model signiﬁcantly. test this evaluate ensemble trained r-gcn model separately trained distmult factorization model r-gcn+ r-gcn distmult selected development data. datasets r-gcn r-gcn+ outperform distmult baseline like systems underperform datasets compared linkfeat algorithm. strong result baseline highlights contribution inverse relation pairs highperformance solutions datasets. interestingly rgcn+ yields better performance complex even though r-gcn decoder explicitly model asymmetry relations opposed complex. suggests combining r-gcn encoder complex scoring function promising direction future work. choice scoring function orthogonal choice encoder; principle scoring function factorization model could incorporated decoder auto-encoder framework. table results fbk- reduced version problematic inverse relation pairs removed. transe complex evaluated using code published trouillon hole evaluated using code published nickel rosasco poggio previously discussed) inverse relation pairs removed linkfeat baseline fails generalize. here r-gcn model outperforms distmult baseline large margin highlighting importance separate encoder model. expected earlier analysis r-gcn r-gcn+ show similar performance dataset. r-gcn model compares favorably factorization methods despite relying distmult decoder shows comparatively weak performance used without encoder. numbers directly comparable reported toutanova chen pruning training testing since pruning schema fully speciﬁed code available possible replicate set-up. relational modeling encoder-decoder approach link prediction relies distmult decoder special simpler case rescal factorization effective original rescal context multi-relational knowledge bases. numerous alternative factorizations proposed studied context including linear nonlinear ones many approaches regarded modiﬁcations special cases classic tensor decomposition methods tucker; comprehensive overview tensor decomposition literature refer reader kolda bader incorporation paths entities knowledge bases recently received considerable attention. roughly classify previous work methods creating auxiliary triples added learning objective factorization model approaches using paths features predicting edges time ﬁrst direction largely orthogonal ours would also expect improvements adding similar terms loss second research line comparable; r-gcns provide computationally cheaper alternative path-based models. direct comparison somewhat complicated path-based methods used different datasets neural networks graphs r-gcn encoder model closely related number works area neural networks graphs. primarily motivated adaption previous work gcns large-scale highly multi-relational data characteristic realistic knowledge bases. early work area includes graph neural network scarselli number extensions original graph neural network proposed notably utilize gating mechanisms facilitate optimization. r-gcns seen sub-class message passing neural networks encompass number previous neural models graphs including gcns differentiable message passing interpretation. link prediction entity classiﬁcation. entity classiﬁcation problem demonstrated r-gcn model competitive end-to-end trainable graphbased encoder. link prediction r-gcn model distmult factorization decoding component outperformed direct optimization factorization model achieved competitive results standard link prediction benchmarks. enriching factorization model rgcn encoder proved especially valuable challenging fbk- dataset yielding improvement decoder-only baseline. several ways work could extended. example graph autoencoder model could considered combination factorization models complex better suited modeling asymmetric relations. also straightforward integrate entity features r-gcns would beneﬁcial link prediction entity classiﬁcation problems. address scalability method would worthwhile explore subsampling techniques hamilton ying leskovec lastly would promising replace current form summation neighboring nodes relation types data-dependent attention mechanism. beyond modeling knowledge bases r-gcns generalized applications relation factorization models shown effective acknowledgements would like thank diego marcheggiani ethan fetaya christos louizos helpful discussions comments. project supported european research council innovation center network dutch national science foundation", "year": 2017}