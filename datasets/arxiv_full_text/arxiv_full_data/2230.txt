{"title": "Neural Optimizer Search with Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classification and Google's neural machine translation system.", "text": "controller form recurrent network generate update equation optimizer. recurrent network controller trained reinforcement learning maximize accuracy particular model architecture trained ﬁxed number epochs update rule held-out validation set. process shown figure cifar- approach discovers several update rules better many commonly used optimizers adam rmsprop without momentum small convnet model. many generated update equations easily transferred architectures datasets. instance update rules found small convnet architecture improve training wide resnet architecture compared adam rmsprop momentum sgd. imagenet update rules improve top- top- accuracy state-of-the-art mobile sized model update rules also work well google’s neural machine translation system giving improvement bleu english german task. neural networks difﬁcult slow train many methods designed reduce difﬁculty lecun schraudolph martens duchi zeiler martens sutskever schaul pascanu bengio pascanu kingma present approach automate process discovering optimization methods focus deep learning architectures. train recurrent neural network controller generate string domain speciﬁc language describes mathematical update equation based list primitive functions gradient running average gradient etc. controller trained reinforcement learning maximize performance model epochs. cifar- method discovers several update rules better many commonly used optimizers adam rmsprop without momentum convnet model. introduce optimizers named powersign addsign show transfer well improve training variety different tasks architectures including imagenet classiﬁcation google’s neural machine translation system. choice right optimization method plays major role success training deep learning models. although stochastic gradient descent often works well advanced optimization methods adam adagrad faster especially training deep networks. designing optimization methods deep learning however challenging non-convex nature optimization problems. google brain. correspondence irwan bello <ibellogoogle.com> barret zoph <barretzophgoogle.com> vijay vasudevan <vrvgoogle.com> quoc <qvlgoogle.com>. recent optimization methods combine insights stochastic batch methods small minibatch similar implement many heuristics estimate diagonal second-order information similar hessian-free l-bfgs combination often yields faster convergence practical problems example adam commonly-used optimizer deep learning implements simple heuristics estimate mean variance gradient used generate stable updates training. many update rules designed borrowing ideas convex analysis even though optimization problems neural networks non-convex. recent empirical results non-monotonic learning rate heuristics suggest still many unknowns training neural networks many ideas non-convex optimization used improve goal work search better update rules neural networks space well-known primitives. words instead hand-designing update rules scratch machine learning algorithm search among update rules. goal shared recently-proposed methods andrychowicz ravi larochelle wichrowska malik learn generate numerical updates training models. difference approach generates mathematical equation update instead numerical updates. main advantage generating equation easily transferred larger tasks require training additional neural networks optimization problem. finally although method optimize memory usage update rules method discovers update rules adam rmsprop requiring less memory. concept using recurrent neural network meta-learning attempted past either genetic programming gradient descent similar recent methods approaches generate updates update equations proposed paper. related approach using genetic programming evolve update equations neural networks runarsson jonsson orchard wang genetic programming however often slow requires many heuristics work well. reason many prior studies area experimented small-scale neural networks. example neural networks used experiments approach reminiscent recent work automated model discovery reinforcement learning especially neural architecture search recurrent network used generate conﬁguration string neural architectures instead. addition applying ideas different applications work presents novel scheme combine primitive inputs much ﬂexible manner makes search novel optimizers possible. finally work also inspired recent studies keskar zhang found regularizer helps generalization. work accuracy validation reward signal thereby implicitly searching optimizers help generalization well. framework controller generates strings corresponding update rules applied neural network estimate update rule’s performance. performance used update controller controller generate improved update rules time. strings sampled controller update rule design domain speciﬁc language relies parenthesis-free notation choice domain speciﬁc language motivated observation computational graph common optimizers represented simple binary expression tree assuming input primitives gradient running average gradient basic unary binary functions. therefore express update rule string describing ﬁrst operand select second operand select unary function apply ﬁrst operand unary function apply second operand binary function apply combine outputs unary functions. output binary function either temporarily stored operand bank used ﬁnal weight update follows figure computation graph commonly used optimizers rmsprop adam. here show computation adam step steps. blue boxes correspond input primitives temporary outputs yellow boxes unary functions gray boxes represent binary functions. gradient bias-corrected running estimate gradient bias-corrected running estimate squared gradient. figure overview controller rnn. controller iteratively selects subsequences length ﬁrst selects operands unary functions apply operands ﬁnally binary function combines outputs unary functions. resulting becomes operand selected subsequent group predictions becomes update rule. every prediction carried softmax classiﬁer next time step input. limited number iterations represent subset mathematical equations. however note represent common optimizers within iteration assuming access simple primitives. figure shows commonly used optimizers represented dsl. also note multiple strings prediction scheme underlying update rule including strings different lengths feature action space corresponding mathematical expressions choice dsl. controller implemented recurrent neural network samples strings length number iterations ﬁxed training since operand bank grows iterations computed different softmax weights every step prediction. zoph train controller using vanilla policy gradient obtained reinforce known exhibit poor sample efﬁciency. using sample efﬁcient proximal policy optimization algorithm speeds convergence controller. baseline function simple exponential moving average previous rewards. speed training controller employ distributed training scheme. distributed training scheme samples generated controller added queue distributed workers connected across network. scheme different parameter server controller replicas needed controller simpliﬁes training. iteration controller samples batch update rules adds global worker queue. training child network complete accuracy held-out validation computed returned controller whose parameters updated ppo. samples generated process continues. ideally reward controller would performance obtained running model sampled optimizer convergence. however setup requires signiﬁcant computation time. help deal issues propose following trade-offs greatly reduce computational complexity. first searching optimizers small layer convolutional network provides enough signal whether optimizer would well much larger models wide resnet model. second train model modest epochs only also provides enough signal whether proposed optimizer good enough needs. techniques allow experiments quickly efﬁciently compared zoph controller experiments typically converging less using cpus compared gpus several weeks. here running exponential moving averages obtained decay rates respectively drop sets inputs probability clip clips input operations applied element-wise. additionally give controller access decay operands based current training step current training step total number training steps hyperparameter controlling number periods periodic decays. note corresponds cosine decay without restarts abbreviate experiments binary trees depths correspond strings length respectively. list operands unary functions binary function quite large address issue helpful work subsets operands functions presented above. leads typical search space sizes ranging possible update rules. also experiment several constraints sampling update rule forcing left right operands different iteration using addition ﬁnal binary function. additional constraint added force controller reuse previously computed operands subsequent iterations. constraints implemented manually setting logits corresponding forbidden operands functions across experiments controller trained adam optimizer learning rate minibatch size controller single-layer lstm hidden state size weights initialized uniformly random also entropy penalty exploration. entropy penalty coefﬁcient child network architecture sampled optimizers small layer convnet. convnet ﬁlters relu activations batch normalization applied convolutional layer. child networks trained cifar- dataset benchmarked datasets deep learning. controller trained child models trained using distributed workers also cpus worker receives optimizer controller performs basic hyperparameter sweep learning rate ranging learning rate tried epoch cifar- training examples. best learning sign provides binary signal whether direction gradient moving average agree single dimension. formula shows subcomponent many optimizers sampled controller. controller discovered fairly intuitive families update rules based formula rate epoch used train child network epochs ﬁnal validation accuracy reported reward controller. child networks batch size evaluate update rule ﬁxed held-out validation examples. setup training child model sampled optimizer generally takes less minutes. experiments typically converge within day. experiments carried using tensorflow introduce promising update rules discovered controller. note mathematical representation update rule necessarily represent found search space sampled directly operand obtained applying sign function results show controller discovers many different updates perform well epochs small convnet searched maximum accuracy also increases time. figure show learning curve controller optimizers sampled. ﬁlter optimizers well many epochs dozens optimizers epochs wide resnet architecture save computational resources aggressively early stop optimizers show less promise based simple heuristics. table appendix examples update rules perform well wide resnet architecture necessarily experiment datasets. among regularly sampled formulas sign either internal decay function training step powersign scales update parameter depending whether gradient running moving average agree. addsign scales update parameter example addsign without internal decay updates parameters gradient moving average agree. note updates correspond usual update meaning internal decays interpolate update rules usual update rule towards training. variants powersign addsign replace also sampled controller found perform well. unless speciﬁed otherwise powersign speciﬁcally refers esign∗sign addsign speciﬁcally refers sign) applying internal decay sign sign quantity refer optimizers powersign-f addsign-f overall found optimizers quite robust slight changes hyperparameters also found larger values base powersign optimizer typically lead faster convergence minor ﬁnal performance loss. advantage method discovering update equations compared previous approaches update equations found method easily transferred tasks. following experiments update equations found previous experiment different network architectures tasks. controller trained again update rules simply reused. goal test transferability optimizers completely different models tasks. adam rmsprop momentum. experiment optimizer iterations different learning rates searched logarithmic scale best performance plotted results figure show optimizer outperforms adam rmsprop close matching performance momentum task. investigate generalizability found update rules different much larger model wide resnet architecture million parameters. commonly used optimizer tuned weight decay tried different learning rates logarithmic scale. learning rate decays considered stepwise cosine decay found latter consistently perform better. optimizers weight decay learning rates centered around yielded best results controller search. applying learning rate decay optimizers outperform common optimizers size-able margin especially employing internal decays. feature optimizers advantageous tasks necessarily require learning rate decays. ﬁrst test powersign optimizer well-known rosenbrock function common testbed stochastic optimization compare performance commonly used deep learning optimizers tensorflow momentum adam rmsprop addsign-ld addsign-ld addsign-ld addsign-cd addsign-cd addsign-cd powersign powersign powersign-ld powersign-ld powersign-cd powersign-cd table performance powersign addsign optimizers standard optimizers wide-resnet architecture cifar- learning rate decay results averaged runs. additionally evaluate powersign addsign optimizers imagenet classiﬁcation task. model state-of-the-art mobile sized nasnet-a model zoph observe improvement using optimizers default rmsprop optimizer spite training pipeline initially tuned rmsprop. optimizers tried three different learning rates models including rmsprop baseline trained cosine learning rate annealing ﬁxed training steps synchronous replicas batch size zoph rest training details. results table powersign addsign achieve improvements rmsprop optimizer evaluate powersign optimizer english german task. optimizer experiment compared adam optimizer architecture interest google neural machine translation model shown achieve competitive translation quality english german task. gnmt network comprises lstm layers encoder decoder ﬁrst layer encoder bidirectional connections. gnmt model also employs attention form layer neural network. model trained distributed fashion using parameter server. twelve workers used worker using gpus minibatch size details model found change make training replace adam powersign update rule. note gnmt model’s hyperparameters weight initialization previously tuned work well adam expect tuning improve results update rule. results table show optimizer indeed generalize well achieves improvement perplexity considered decent gain task. gain training perplexity enables model obtain bleu improvement adam optimizer test validation averaged improvement points near peak values bleu. finally powersign update rule also memory efﬁcient keeps running average parameter compared running averages adam. practical implications much larger translation models adam cannot currently used memory constraints mikolov base model parameters single layer lstm variational dropout weight tying weight decay embedding state output dropout state reset probability embedding ratio tuned black-box hyperparameter tuner similarly melis clip gradient norm train model steps batch size truncated backpropagation length timesteps. compare powersign addsign adam table setup optimizers perform much better outperformed adam. hyperparameter ranges ﬁrst manually tuned experimenting adam possible could negatively affect performance optimizers. paper considers approach automating discovery optimizers focus deep neural network architectures. strength approach naturally encompasses environment optimization process happens. example method discovering optimizers perform well scenarios computations carried using bits distributed setup workers communicate bits information shared parameter server. method discovered intuitive update rules powersign addsign obtain competitive performance common optimizers variety tasks models image classiﬁcation convnets machine translation lstms. method also identiﬁed learning rate annealing scheme linear cosine decay found generally leads faster convergence cosine annealing. references abadi mart´ın barham paul chen jianmin chen zhifeng davis andy dean jeffrey devin matthieu ghemawat sanjay irving geoffrey isard michael kudlur manjunath levenberg josh monga rajat moore sherry murray derek steiner benoit tucker paul vasudevan vijay warden pete wicke martin yuan zheng xiaoqiang. tensorﬂow system large-scale machine learning. proceedings usenix symposium operating systems design implementation andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. advances neural information processing systems jimmy grosse roger martens james. distributed second-order optimization using kroneckerfactored approximations. international conference learning representations bengio samy bengio yoshua cloutier jocelyn. genetic programming search learning rule neural networks. evolutionary computation ieee world congress computational intelligence. proceedings first ieee conference ieee dean jeffrey corrado greg monga rajat chen devin matthieu mark senior andrew tucker paul yang quoc large scale distributed deep networks. advances neural information processing systems golovin daniel solnik benjamin moitra subhodeep kochanski greg karro john elliot sculley google vizier service black-box optimization. proceedings sigkdd international conference knowledge discovery data mining hochreiter sepp younger steven conwell peter learning learn using gradient descent. international conference artiﬁcial neural networks springer inan hakan khosravi khashayar socher richard. tying word vectors word classiﬁers loss frameinternational conferwork language modeling. ence learning representations keskar nitish shirish mudigere dheevatsa nocedal jorge smelyanskiy mikhail tang ping peter. large-batch training deep learning generalization sharp minima. international conference learning representations quoc ngiam jiquan coates adam lahiri ahbik prochnow bobby andrew optimization methods deep learning. proceedings international conference machine learning yarin ghahramani zoubin. theoretically grounded application dropout recurrent neural netadvances neural information processing works. systems neelakantan arvind vilnis luke quoc sutskever ilya kaiser lukasz kurach karol martens james. adding gradient noise improves learning deep networks. arxiv preprint arxiv. runarsson thomas jonsson magnus evolution design distributed learning rules. ieee symposium combinations evolutionary computation neural networks shazeer noam mirhoseini azalia maziarz krzysztof davis andy quoc hinton geoffrey dean jeff. outrageously large neural networks sparselygated mixture-of-experts layer. international conference learning representations wichrowska olga maheswaranathan niru hoffman matthew colmenarejo sergio gomez denil misha freitas nando sohl-dickstein jascha. learned international optimizers scale generalize. conference machine learning yonghui schuster mike chen zhifeng quoc norouzi mohammad macherey wolfgang krikun maxim yuan macherey klaus klingner jeff shah apurva johnson melvin xiaobing kaiser lukasz gouws stephan kato yoshikiyo kudo taku kazawa hideto stevens keith kurian george patil nishant wang young cliff smith jason riesa jason rudnick alex vinyals oriol corrado greg hughes macduff dean jeffrey. google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. zhang chiyuan bengio samy hardt moritz recht benjamin vinyals oriol. understanding deep learning requires rethinking generalization. international conference learning representations default hyperparameter values powersign addsign optimizers learning rate used usually good ﬁrst choice. usual using learning rate decay initial learning rate larger applying decay. applying linear cosine learning rate decay initial learning rate larger cosine decay. concerning internal decays applied sign sign quantity we’ve generally found powersign works best internal cosine decay addsign internal linear decay. table performance neural optimizer search standard optimizers wide-resnet architecture cifar-. final final test refer ﬁnal validation test accuracy training epochs. best corresponds best validation accuracy epochs best test test accuracy epoch validation accuracy highest. optimizer report best results seven learning rates logarithmic scale according validation accuracy. learning rate decay applied.", "year": 2017}