{"title": "Building Machines That Learn and Think Like People", "tag": ["cs.AI", "cs.CV", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.", "text": "recent progress artiﬁcial intelligence renewed interest building systems learn think like people. many advances come using deep neural networks trained end-to-end tasks object recognition video games board games achieving performance equals even beats humans respects. despite biological inspiration performance achievements systems diﬀer human intelligence crucial ways. review progress cognitive science suggesting truly human-like learning thinking machines reach beyond current engineering trends learn learn speciﬁcally argue machines build causal models world support explanation understanding rather merely solving pattern recognition problems; ground learning intuitive theories physics psychology support enrich knowledge learned; harness compositionality learning-to-learn rapidly acquire generalize knowledge tasks situations. suggest concrete challenges promising routes towards goals combine strengths recent neural network advances structured cognitive models. artiﬁcial intelligence story booms busts traditional measure success last years marked exceptional progress. much progress come recent advances deep learning characterized learning large neural-network-style models multiple layers representation. models achieved remarkable gains many domains spanning object recognition speech recognition control object recognition krizhevsky sutskever hinton trained deep convolutional neural network nearly halved error rate previous state-of-the-art challenging benchmark date. years since convnets continue dominate recently approaching human-level performance object recognition benchmarks automatic speech recognition hidden markov models leading approach since late framework chipped away piece piece replaced deep learning components leading approaches speech recognition fully neural network systems ideas deep learning also applied learning complex control problems. mnih combined ideas deep learning reinforcement learning make deep reinforcement learning algorithm learns play large classes simple video games frames pixels game score achieving human superhuman level performance many games accomplishments helped neural networks regain status leading paradigm machine learning much late early recent success neural networks captured attention beyond academia. industry companies google facebook active research divisions exploring technologies object speech recognition systems based deep learning deployed core products smart phones web. media also covered many recent achievements neural networks often expressing view neural networks achieved recent success virtue brain-like computation thus ability emulate human learning human cognition. article view excitement opportunity examine means machine learn think like person. ﬁrst review criteria previously oﬀered cognitive scientists developmental psychologists researchers. second articulate view essential ingredients building machine learns thinks like person synthesizing theoretical ideas experimental data research cognitive science. third consider contemporary light ingredients ﬁnding deep learning models incorporate many solving problems diﬀerent ways people discussing view plausible paths towards building machines learn think like people. includes prospects integrating deep learning core cognitive ingredients identify inspired part recent work fusing neural networks lower-level building blocks classic psychology computer science traditionally seen incompatible. beyond speciﬁc ingredients proposal draw broader distinction diﬀerent computational approaches intelligence. statistical pattern recognition approach treats prediction primary usually context speciﬁc classiﬁcation regression control task. view learning discovering features high value states common shared label classiﬁcation setting shared value reinforcement learning setting across large diverse training data. alternative approach treats models world primary learning process model-building. cognition using models understand world explain imagine could happened didn’t could true isn’t planning actions make diﬀerence between pattern recognition model-building prediction explanation central view human intelligence. scientists seek explain nature simply predict human thought fundamentally model-building activity. elaborate point numerous examples below. also discuss pattern recognition even core intelligence nonetheless support model-building model-free algorithms learn experience make essential inferences computationally eﬃcient. nearly long neural networks critiques neural networks critical neural networks article goal build successes rather dwell shortcomings. role neural networks developing human-like learning machines applied compelling ways many types machine learning problems demonstrating power gradient-based learning deep hierarchies latent variables. neural networks also rich history computational models cognition history describe detail next section. fundamental level computational model learning must ultimately grounded brain’s biological neural networks. also believe future generations neural networks look diﬀerent current state-of-the-art. endowed intuitive physics theory mind causal reasoning capacities describe sections follow. structure inductive biases could built networks learned previous experience related tasks leading human-like patterns learning development. networks learn eﬀectively search discover mental models intuitive theories improved models will turn enable subsequent learning allowing systems learn-to-learn using previous knowledge make richer inferences small amounts training data. also important draw distinction purports emulate draw inspiration aspects human cognition not. article focuses former. latter perfectly reasonable useful approach developing algorithms avoiding cognitive neural inspiration well claims cognitive neural plausibility. indeed many researchers proceeded article little pertinence work conducted research strategy. hand believe reverse engineering human intelligence usefully inform machine learning especially types domains tasks people excel despite recent computational achievements people better machines solving range diﬃcult computational problems including concept learning scene understanding language acquisition language understanding speech recognition etc. human cognitive abilities remain diﬃcult understand computationally including creativity common sense general purpose reasoning. long natural intelligence remains best example intelligence believe project reverse engineering human solutions diﬃcult computational problems continue inform advance inﬂuential textbook russell norvig state quest ‘artiﬁcial ﬂight’ succeeded wright brothers others stopped imitating birds started using wind tunnels learning aerodynamics. neural network network simple neuron-like processing units collectively perform complex computations. neural networks often organized layers including input layer presents data hidden layers transform data intermediate representations output layer produces response recurrent connections also popular processing sequential data. deep learning neural network least hidden layer state-of-the-art deep networks trained using backpropagation algorithm gradually adjust connection strengths. backpropagation gradient descent applied training deep neural network. gradient objective function respect model parameters used make series small adjustments parameters direction improves objective function. convolutional network neural network uses trainable ﬁlters instead fully-connected layers independent weights. ﬁlter applied many locations across image leading neural networks eﬀectively larger local connectivity fewer free parameters. model-free model-based reinforcement learning model-free algorithms directly learn control policy without explicitly building model environment model-based algorithms learn model environment select actions planning. deep q-learning model-free reinforcement learning algorithm used train deep neural networks control tasks playing atari games. network trained approximate optimal action-value function expected long-term cumulative reward taking action state optimally selecting future actions. generative model model speciﬁes probability distribution data. instance classiﬁcation task examples class labels generative model neural network predict label given data point cannot directly used sample examples compute queries regarding data. generally concerned directed generative models given causal interpretation although undirected generative models also possible. program induction constructing program computes desired function function typically speciﬁed training data consisting example inputoutput pairs. case probabilistic programs specify candidate generative models data abstract description language used deﬁne allowable programs learning search programs likely generated data. exciting recent progress forms probabilistic machine learning example researchers developed automated statistical reasoning techniques automated techniques model building selection probabilistic programming languages believe approaches play important roles future systems least compatible ideas cognitive science discuss here full discussion connections beyond scope current article. central goal paper propose core ingredients building human-like learning thinking machines. elaborate ingredients topics section brieﬂy overview ideas. ﬁrst ingredients focuses developmental start-up software cognitive capabilities present early development. several reasons focus development. ingredient present early development certainly active available well child adult would attempt learn types tasks discussed paper. true regardless whether early-present ingredient learned experience innately present. also earlier ingredient present likely foundational later development learning. focus pieces developmental start-up software first intuitive physics infants primitive object concepts allow track objects time allow discount physically implausible trajectories. example infants know objects persist time solid coherent. equipped general principles people learn quickly make accurate predictions. task physics still works way. second type software present early development intuitive psychology infants understand people mental states like goals beliefs understanding strongly constrains learning predictions. child watching expert play video game infer avatar agency trying seek reward avoiding punishment. inference immediately constrains inferences allowing child infer objects good objects bad. types inferences accelerate learning tasks. second ingredients focus learning. many perspectives learning model building hallmark human-level learning explaining observed data construction causal models world perspective earlypresent capacities intuitive physics psychology also causal models world. primary learning extend enrich models build analogous causally structured theories domains. richness eﬃciency. children come ability desire uncover underlying causes sparsely observed events knowledge beyond paucity data. might seem paradoxical people capable learning richly structured models limited amounts experience. suggest compositionality learning-tolearn ingredients make type rapid model learning possible ﬁnal ingredients concerns rich models minds build action real time remarkable fast perceive act. people comprehend novel scene fraction second novel utterance little time takes hear important motivation using neural networks machine vision speech systems respond quickly brain does. although neural networks usually aiming pattern recognition rather model-building discuss ways model-free methods accelerate slow model-based inferences perception cognition learning recognize patterns inferences outputs inference predicted without costly intermediate steps. integrating neural networks learn inference rich model-building learning mechanisms oﬀers promising explain human minds understand world well quickly. also discuss integration model-based model-free methods reinforcement learning area seen rapid recent progress. causal model task learned humans model plan action sequences maximize future reward; rewards used metric successs model-building known model-based reinforcement learning. however planning complex models cumbersome slow making speed-accuracy trade-oﬀ unfavorable real-time control. contrast model-free reinforcement learning algorithms current instantiations deep reinforcement learning support fast control cost inﬂexibility possibly accuracy. review evidence humans combine model-based model-free learning algorithms competitively cooperatively interactions supervised metacognitive processes. sophistication human-like reinforcement learning realized systems area crosstalk cognitive engineering approaches especially promising. questions whether relate human cognitive psychology older terms ‘artiﬁcial intelligence’ ‘cognitive psychology.’ alan turing suspected easier build educate child-machine fully capture adult human cognition turing pictured child’s mind notebook rather little mechanism lots blank sheets mind child-machine ﬁlling notebook responding rewards punishments similar reinforcement learning. view representation learning echoes behaviorism dominant psychological tradition turing’s time. also echoes strong empiricism modern connectionist models idea learn almost everything know statistical patterns sensory inputs. early research newell simon developed general problem solver algorithm model human problem solving subsequently tested experimentally pioneers areas research explicitly referenced human cognition even published papers cognitive psychology journals example schank writing journal cognitive psychology declared draw boundary theory human thinking scheme making intelligent machine; purpose would served separating today since neither domain theories good enough explain—or produce—enough mental capacity. much research assumed human knowledge representation symbolic reasoning language planning vision could understood terms symbolic operations. parallel developments radically diﬀerent approach explored based neuron-like subsymbolic computations representations algorithms used approach directly inspired neuroscience cognitive psychology although ultimately would ﬂower inﬂuential school thought nature cognition—parallel distributed processing name suggests emphasizes parallel computation combining simple units collectively implement sophisticated computations. knowledge learned neural networks thus distributed across collection units rather localized symbolic data structures. resurgence recent interest neural networks commonly referred deep learning share representational commitments often even learning algorithms earlier models. deep refers fact powerful models built composing many layers representation still much style utilizing recent advances hardware computing capabilities well massive datasets learn deeper models. also important clarify perspective compatible model building addition pattern recognition. original work done banner closer model building pattern recognition whereas recent large-scale discriminative deep learning systems purely exemplify pattern recognition discussed also question nature learned representations within model form compositionality transferability developmental start-up software used there. focus issues paper. guide learning. proponents approach maintain many classic types structured knowledge graphs grammars rules objects structural descriptions programs etc. useful misleading metaphors characterizing thought. structures epiphenomenal real emergent properties fundamental sub-symbolic cognitive processes compared paradigms studying cognition position nature representation often accompanied relatively blank slate vision initial knowledge representation much like turing’s blank notebook. attempting understand particular cognitive ability phenomenon within paradigm common scientiﬁc strategy train relatively generic neural network perform task adding additional ingredients necessary. approach shown neural networks behave learned explicitly structured knowledge rule producing past tense words rules solving simple balance-beam physics problems tree represent types living things distribution properties training large-scale relatively generic networks also best current approach object recognition high-level feature representations convolutional nets also used predict patterns neural response human macaque cortex well human typicality ratings similarity ratings images common objects. moreover researchers trained generic networks perform structured even strategic tasks recent work using deep q-learning network play simple video games neural networks broad application machine vision language control trained emulate rule-like structured behaviors characterize cognition need develop truly human-like learning thinking machines? relatively generic neural networks bring towards goal? cognitive science converged single account mind intelligence claim mind collection general purpose neural networks initial constraints rather extreme contemporary cognitive science. diﬀerent picture emerged highlights importance early inductive biases including core concepts number space agency objects well powerful learning algorithms rely prior knowledge extract knowledge small amounts training data. knowledge often richly organized theory-like structure capable graded inferences productive capacities characteristic human thought. present challenge problems machine learning learning simple visual concepts learning play atari game frostbite also problems running examples illustrate importance core cognitive ingredients sections follow. ﬁrst challenge concerns handwritten character recognition classic problem comparing diﬀerent types machine learning algorithms. hofstadter argued problem recognizing characters ways people handwritten printed contains fundamental challenges whether statement right highlights surprising complexity underlies even simple human-level concepts like letters. practically handwritten character recognition real problem children adults must learn solve practical applications ranging reading envelope addresses checks machine. handwritten character recognition also simpler general forms object recognition object interest two-dimensional separated background usually unoccluded. compared people learn types objects seems possible near term build algorithms structure characters people see. standard benchmark mnist data digit recognition involves classifying images digits categories ‘’-‘’ training provides images class total training images. large amount training data available many algorithms achieve respectable performance including k-nearest neighbors support vector machines convolutional neural networks best results achieved using deep convolutional nets close human-level performance error rate similarly recent results applying convolutional nets challenging imagenet object recognition benchmark shown human-level performance within reach data well humans neural networks perform equally well mnist digit recognition task large-scale image classiﬁcation tasks mean learn think way. least important diﬀerences people learn fewer examples learn richer representations comparison true learning handwritten characters well learning general classes objects people learn recognize handwritten character single example allowing discriminate novel instances drawn people similar looking non-instances moreover people learn pattern recognition learn concept model class allows acquired knowledge ﬂexibly applied ways. addition recognizing examples people also generate examples parse character important parts relations generate characters given small related characters additional abilities come free along acquisition underlying concept. even simple visual concepts people still better sophisticated learners best algorithms character recognition. people learn less capturing human-level learning abilities machines characters challenge. recently reported progress challenge using probabilistic program induction aspects full human cognitive ability remain reach. people model represent characters sequence strokes relations people figure characters challenge human-level learning novel handwritten characters abilities also illustrated novel two-wheeled vehicle single example visual concept enough information support classiﬁcation examples generation examples parsing object parts relations generation concepts related concepts. adapted lake salakhutdinov tenenbaum richer repertoire structural relations strokes. furthermore people eﬃciently integrate across multiple examples character infer optional elements horizontal cross-bar combining diﬀerent variants character single coherent representation. additional progress come combining deep learning probabilistic program induction tackle even richer versions characters challenge. second challenge concerns atari game frostbite control problems tackled mnih signiﬁcant advance reinforcement learning showing single algorithm learn play wide variety complex tasks. network trained play classic atari games proposed test domain reinforcement learning impressively achieving human-level performance games. however particular trouble frostbite games required temporally extended planning strategies. frostbite players control agent tasked constructing igloo within time limit. igloo built piece-by-piece agent jumps ﬂoes water challenge ﬂoes constant motion ﬂoes contribute construction igloo visited active state agent also earn extra points gathering avoiding number fatal hazards success game requires figure screenshots frostbite video game designed atari game console. start level frostbite. agent must construct igloo hopping ﬂoes avoiding obstacles birds. ﬂoes constant motion making multi-step planning essential success. agent receives pieces igloo jumping active ﬂoes deactivates level agent must safely reach completed igloo. later levels include additional rewards deadly obstacles temporally extended plan ensure agent accomplish sub-goal safely proceed next sub-goal. ultimately pieces igloo place agent must proceed igloo thus complete level time expires learns play frostbite atari games combining powerful pattern recognizer simple model-free reinforcement learning algorithm components allow network sensory inputs onto policy small actions mapping policy trained optimize long-term cumulative reward network embodies strongly empiricist approach characteristic connectionist models little built network apart assumptions image structure inherent convolutional networks network essentially learn visual conceptual system scratch game. mnih network architecture hyper-parameters ﬁxed network trained anew game meaning visual system policy highly specialized games trained recent work shown game-speciﬁc networks share visual features used train multi-task network achieving modest beneﬁts transfer learning play games. although interesting learns play games human-level performance assuming little prior knowledge learning play frostbite games diﬀerent people examine diﬀerences considering amount experience required learning. mnih compared professional gamer received approximately hours practice atari games trained million frames games equates approximately hours game time almost times much experience human received. additionally incorporates experience replay frames replayed approximately times average course learning. full hours unique experience additional replay achieved less human-level performance controlled test session recent variants demonstrated superior performance reaching professional gamer’s score incorporating smarter experience replay using smarter replay eﬃcient parameter sharing requires experience reach level learning curve provided schaul shows performance around hours hours hours diﬀerences human machine learning curves suggest learning diﬀerent kinds knowledge using diﬀerent learning mechanisms both. contrast becomes even dramatic look earliest stages learning. original recent variants require multiple hours experience perform reliably better random play even non-professional humans grasp basics game minutes play. speculate people inferring general schema describe goals game object types interactions using kinds intuitive theories model-building abilities model-based planning mechanisms describe below. novice players make mistakes inferring harmful rather helpful learn play better chance within minutes. humans able ﬁrst watch expert playing minutes learn even faster. informal experiments authors playing frostbite javascript emulator watching videos expert play youtube minutes found able reach scores comparable reported scores human starts measure test performance designed prevent networks memorizing long sequences successful actions single starting point. faster learning higher scores reported using metrics unclear well networks generalizing alternative metrics. figure comparing learning speed people versus deep q-networks test performance atari game frostbite plotted function game experience include additional experience replay. learning curves scores shown diﬀerent networks dqn+ dqn++ random play achieves score human starts performance measure used behavioral signatures suggest fundamental diﬀerences representation learning people dqn. instance game frostbite provides incremental rewards reaching active providing relevant sub-goals completing larger task building igloo. without sub-goals would take random actions accidentally builds igloo rewarded completing entire level. contrast people likely rely incremental scoring ﬁguring play game. frostbite possible ﬁgure higher-level goal building igloo without incremental feedback; similarly sparse feedback source diﬃculty atari games montezuma’s revenge people substantially outperform current approaches. learned network also rather inﬂexible changes inputs goals changing color appearance objects changing goals network would devastating consequences performance network retrained. speciﬁc model necessarily more precisely human expert mnih scored average points across game sessions minutes play. individual sessions lasting longer minutes author obtained scores points approximately minutes gameplay points minutes points minutes. author obtained approximately minutes gameplay minutes minutes. watched approximately minutes expert play youtube simpliﬁed held standard general human intelligence contrast human ﬂexibility striking nonetheless. example imagine tasked playing frostbite goals furthest unexplored level without regard score. discover secret easter eggs. many can. touch individual ﬂoes screen once. teach friend play eﬃciently possible. range goals highlights essential component human intelligence people learn models arbitrary tasks goals. neural networks learn multiple mappings tasks stimuli adapting outputs depending speciﬁed goal models require substantial training reconﬁguration tasks contrast people require little retraining reconﬁguration adding tasks goals repertoire relative ease. frostbite example particularly telling contrast compared human play. even best deep networks learn gradually many thousands game episodes take long time reach good performance locked particular input goal patterns. humans playing small number games span minutes understand game goals well enough perform better deep networks almost thousand hours experience. even impressively people understand enough invent accept goals generalize changes input explain game others. people diﬀerent? core ingredients human intelligence might modern machine learning methods missing? might object frostbite characters challenges draw unfair comparison speed human learning neural network learning. discuss objection detail section feel important anticipate well. paraphrase reviewer earlier draft article people solving task diﬀerently. better seen solving diﬀerent tasks. human learners unlike many deep learning systems approach problems armed extensive prior experience. human encountering years-long string problems rich overlapping structure. humans result often important domain-speciﬁc knowledge tasks even ‘begin.’ starting completely scratch. agree indeed another putting point here. human learners fundamentally take diﬀerent learning tasks today’s neural networks want build machines learn think like people machines need confront kinds tasks human learners away them. people never start completely scratch even close from scratch secret success. challenge building models human learning thinking becomes bring bear rich prior knowledge learn tasks solve problems quickly? form prior knowledge take constructed combination inbuilt capacities previous experience? core ingredients propose next section oﬀer route meeting challenge. introduction laid core ingredients intelligence. consider ingredients detail contrast current state neural network modeling. hardly ingredients needed human-like learning thought building blocks present current learning-based systems certainly present together additional attention prove especially fruitful. believe integrating produce signiﬁcantly powerful human-like learning thinking abilities currently systems. considering ingredient detail important clarify core ingredient necessarily mean ingredient innately speciﬁed genetics must built learning algorithm. intend discussion agnostic regards origins ingredients. time child adult picking character learning play frostbite armed extensive real world experience deep learning systems beneﬁt experience would hard emulate general sense. certainly core ingredients enriched experience even product experience itself. whether learned built enriched claim ingredients play active important role producing human-like learning thought ways contemporary machine learning capture. early development humans foundational understanding several core domains domains include number space physics psychology core domains cleave cognition conceptual joints domain organized entities abstract principles relating entities. underlying cognitive representations understood intuitive theories causal structure resembling scientiﬁc theory child scientist proposal views process learning also scientist-like recent experiments showing children seek data distinguish hypotheses isolate variables test causal hypotheses make data-generating process drawing conclusions learn selectively others address nature learning mechanisms section core domain target great deal study analysis together domains thought shared cross-culturally partly non-human animals. domains important augmentations current machine learning though focus particular early understanding objects agents. young children rich knowledge intuitive physics. whether learned innate important physical concepts present ages earlier child adult learns play frostbite suggesting resources used solving many everyday physics-related tasks. months possibly earlier human infants expect inanimate objects follow principles persistence continuity cohesion solidity. young infants believe objects move along smooth paths wink existence inter-penetrate distance expectations guide object segmentation early infancy emerging appearance-based cues color texture perceptual goodness expectations also guide later learning. around months infants already developed diﬀerent expectations rigid bodies soft bodies liquids liquids example expected barriers solid objects cannot ﬁrst birthday infants gone several transitions comprehending basic physical concepts inertia support containment collisions single agreed-upon computational account early physical principles concepts previous suggestions ranged decision trees cues lists rules promising recent approach sees intuitive physical reasoning similar inference physics software engine kind simulators power modern-day animations games according hypothesis people reconstruct perceptual scene using internal representations objects physically relevant properties forces acting objects relative physical ground truth intuitive physical state representation figure intuitive physics-engine approach scene understanding illustrated tower stability. engine takes inputs perception language memory faculties. constructs physical scene objects physical properties forces simulates scene’s development time hands output reasoning systems. many possible ‘tweaks’ input result much diﬀerent scenes requiring potential discovery training evaluation features tweak. adapted battaglia approximate probabilistic oversimpliﬁed incomplete many ways. still rich enough support mental simulations predict objects move immediate future either responses forces might apply. intuitive physics engine approach enables ﬂexible adaptation wide range everyday scenarios judgments goes beyond perceptual cues. example physics-engine reconstruction tower wooden blocks game jenga used predict whether tower fall ﬁnding close quantitative adults make predictions well simpler kinds physical predictions studied infants simulation-based models also capture people make hypothetical counterfactual predictions would happen certain blocks taken away blocks added table supporting tower jostled? certain blocks glued together attached table surface? blocks made diﬀerent materials blocks color much heavier colors? physical judgments require features training pattern recognition account work level model-based simulator. prospects embedding acquiring kind intuitive physics deep learning systems? connectionist models psychology previously applied physical reasoning tasks balance-beam rules rules relating distance velocity time motion networks attempt work complex scenes input wide range scenarios judgments figure recent paper facebook researchers represents exciting step direction. lerer trained deep convolutional network-based system predict stability block towers simulated images similar figure much simpler conﬁgurations three four cubical blocks stacked vertically. impressively physnet generalized simple real images block towers matching human performance images meanwhile exceeding human performance synthetic images. human physnet conﬁdence also correlated across towers although strongly approximate probabilistic simulation models experiments battaglia limitation physnet currently requires extensive training scenes learn judgments single task narrow range scenes shown generalize also limited ways contrast people require less experience perform particular task generalize many novel judgments complex scenes training required could deep learning systems physnet capture ﬂexibility without explicitly simulating causal interactions objects three dimensions? sure hope challenge take alternatively instead trying make predictions without simulating physics could neural networks trained emulate general-purpose physics simulator given right type quantity training data input experienced child? active intriguing area research faces signiﬁcant challenges. networks trained object classiﬁcation deeper layers often become sensitive successively higher-level features edges textures shape-parts full objects deep networks trained physics-related data remains seen whether higher layers encode objects general physical properties forces approximately newtonian dynamics. generic network trained dynamic pixel data might learn implicit representation concepts would generalize broadly beyond training contexts people’s explicit physical concepts consider example network learns predict trajectories several balls bouncing network actually learned something like newtonian mechanics able generalize interestingly diﬀerent scenarios minimum diﬀerent numbers diﬀerently shaped objects bouncing boxes diﬀerent shapes sizes orientations respect gravity mention severe generalization tests tower tasks discussed above also fall newtonian domain. neural network researchers take challenge hope will. whether models learned kind data available human infants clear discuss section diﬃcult integrate object physics-based primitives deep neural networks payoﬀ terms learning speed performance could great many tasks. consider case learning play frostbite. although diﬃcult discern exactly network learns solve particular task probably parse frostbite screenshot terms stable objects sprites moving according rules intuitive physics incorporating physics-engine-based representation could help dqns learn play games frostbite faster general whether physics knowledge captured implicitly neural network explicitly simulator. beyond reducing amount training data potentially improving level performance reached could eliminate need retrain frostbite network objects slightly altered behavior reward-structure appearance. object type bear introduced later levels frostbite network endowed intuitive physics would also easier time adding object type knowledge integration intuitive physics deep learning could important step towards human-like learning algorithms. intuitive psychology another early-emerging ability important inﬂuence human learning thought. pre-verbal infants distinguish animate agents inanimate objects. distinction partially based innate early-present detectors low-level cues presence eyes motion initiated rest biological motion cues often suﬃcient necessary detection agency. beyond low-level cues infants also expect agents contingently reciprocally goals take eﬃcient actions towards goals subject constraints goals socially directed; around three months infants begin discriminate anti-social agents hurt hinder others neutral agents later distinguish anti-social neutral pro-social agents generally agreed infants expect agents goal-directed eﬃcient socially sensitive fashion less agreed computational architecture supports reasoning whether includes reference mental states explicit goals. possibility intuitive psychology simply cues down though would require cues scenarios become complex. consider example scenario agent moving towards agent moves blocks reaching box. infants adults likely interpret behavior ‘hindering’ inference could captured states agent’s expected trajectory prevented completion blocking agent given negative association.’ easily calculated scenario also easily changed necessitate diﬀerent type cue. suppose already negatively associated acting negatively towards could seen good suppose something harmful didn’t know about. would seen helping protecting defending suppose knew something wanted anyway. could seen acting paternalistically. cue-based account would twisted gnarled combinations expected trajectory prevented completion blocking agent given negative association unless trajectory leads negative outcome blocking agent previously associated positive alternative cue-based account generative models action choice bayesian inverse planning models baker saxe tenenbaum naive utility calculus models jara-ettinger gweon tenenbaum schulz tauber steyvers related alternative based predictive coding kilner friston frith models formalize explicitly mentalistic concepts ‘goal’ ‘agent’ ‘planning’ ‘cost’ ‘eﬃciency’ ‘belief’ used describe core psychological reasoning infancy. assume adults children treat agents approximately rational planners choose eﬃcient means goals. planning computations formalized solutions markov decision processes taking input utility belief functions deﬁned agent’s state-space agent’s state-action transition functions returning series actions agent perform eﬃciently fulﬁll goals simulating planning processes people predict agents might next inverse reasoning observing series actions infer utilities beliefs agents scene. directly analogous simulation engines used intuitive physics predict happen next scene infer objects’ dynamical properties move. yields similarly ﬂexible reasoning abilities utilities beliefs adjusted take account agents might wide range novel goals situations. importantly unlike intuitive physics simulation-based reasoning intuitive psychology nested recursively understand social interactions think agents thinking agents. case intuitive physics success generic deep networks capturing intuitive psychological reasoning depend part representations humans use. although deep networks applied scenarios involving theory-of-mind intuitive psychology could probably learn visual cues heuristics summary statistics scene happens involve agents. underlies human psychological reasoning data-driven deep learning approach likely success domain. however seems full formal account intuitive psychological reasoning needs include representations agency goals eﬃciency reciprocal relations. objects forces unclear whether complete representation concepts could emerge deep neural networks trained purely predictive capacity. similar intuitive physics domain possible tremendous number training trajectories variety scenarios deep learning techniques could approximate reasoning found infancy even without learning anything goal-directed social-directed behavior generally. also unlikely resemble humans learn understand apply intuitive psychology unless concepts genuine. altering setting scene target inference physics-related task diﬃcult generalize without understanding objects altering setting agent goals beliefs diﬃcult reason without understanding intuitive psychology. while connectionist networks used model general transition children undergo ages regarding false belief referring scenarios require inferring goals utilities relations. tremely quickly watching experienced player minutes playing rounds themselves. intuitive psychology provides basis eﬃcient learning others especially teaching settings goal communicating knowledge eﬃciently case watching expert play frostbite whether explicit goal teach intuitive psychology lets infer beliefs desires intentions experienced player. instance learn birds avoided seeing experienced player appears avoid them. need experience single example encountering bird watching frostbite bailey bird order infer birds probably dangerous. enough experienced player’s avoidance behavior best explained acting belief. similarly consider sidekick agent expected help player achieve goals. agent useful diﬀerent ways diﬀerent circumstances getting items clearing paths ﬁghting defending healing providing information general notion helpful explicit agent representation predict agent helpful circumstances bottom-up pixel-based representation likely struggle. several ways intuitive psychology could incorporated contemporary deep learning systems. could built intuitive psychology arise ways. connectionists argued innate constraints form hard-wired cortical circuits unlikely simple inductive bias example tendency notice things move things bootstrap reasoning abstract concepts agency similarly great deal goal-directed sociallydirected actions also boiled simple utility-calculus could shared cognitive abilities. origins intuitive psychology still matter debate clear abilities early-emerging play important role human learning thought exempliﬁed frostbite challenge learning play novel video games broadly. since inception neural networks models stressed importance learning. many learning algorithms neural networks including perceptron algorithm hebbian learning rule backpropagation wake-sleep algorithm contrastive divergence whether goal supervised unsupervised learning algorithms implement learning process gradual adjustment connection strengths. supervised learning updates usually aimed improving algorithm’s pattern recognition capabilities. unsupervised learning updates work towards gradually matching statistics model’s internal patterns statistics input data. recent years machine learning found particular success using backpropagation large data sets solve diﬃcult pattern recognition problems. algorithms reached humanlevel performance several challenging benchmarks still matching human-level learning ways. deep neural networks often need data people order solve types problems whether learning recognize type object learning play game. learning meanings words native language children make meaningful generalizations sparse data children need examples concepts hairbrush pineapple lightsaber largely ‘get grasping boundary inﬁnite deﬁnes concept inﬁnite possible objects. children practiced adults learning concepts learning roughly nine words beginning speak high school ability rapid one-shot learning disappear adulthood. adult need single image movie novel two-wheeled vehicle infer boundary concept others allowing discriminate examples concept similar looking objects diﬀerent type contrasting eﬃciency human learning neural networks virtue generality highly ﬂexible function approximators notoriously data hungry benchmark tasks imagenet data object recognition provides hundreds thousands examples class hairbrushes pineapples etc. context learning handwritten characters learning play frostbite mnist benchmark includes examples handwritten digit mnih played atari video game approximately hours unique training experience cases algorithms clearly using information less eﬃciently person learning perform tasks. also important mention many classes concepts people learn slowly. concepts learned school usually challenging diﬃcult acquire including mathematical functions logarithms derivatives integrals atoms electrons gravity evolution etc. also domains machine learners outperform human learners combing ﬁnancial weather data. vast majority cognitively natural concepts types things children learn meanings words people still better learners machines. type learning focus section suitable enterprise reverse engineering articulating additional principles make human learning successful. also opens possibility building ingredients next generation machine learning algorithms potential making progress learning concepts easy diﬃcult humans acquire. even examples people learn remarkably rich conceptual models. indicator richness variety functions models support beyond classiﬁcation concepts support prediction action communication imagination explanation composition abilities independent; rather hang together interact coming free acquisition underlying concept. returning previous example novel wheeled vehicle person sketch range instances parse concept important components even create complex concept combination familiar concepts likewise discussed context frostbite learner acquired basics game could ﬂexibly apply knowledge inﬁnite frostbite variants acquired knowledge supports reconﬁguration tasks demands modifying goals game survive acquiring points possible eﬃciently teach rules friend. richness ﬂexibility suggests learning model building better metaphor learning pattern recognition. furthermore human capacity one-shot learning suggests models built upon rich domain knowledge rather starting blank slate contrast much recent progress deep learning pattern recognition problems including object recognition speech recognition video game learning utilize large data sets little domain knowledge. recent work types tasks including learning generative models images caption generation question answering learning simple algorithms discuss question answering learning simple algorithms section least image caption generation tasks mostly studied data setting odds impressive human ability generalizing small data sets diﬃcult learn neural-network-style representations eﬀortlessly generalize tasks trained additional ingredients needed order rapidly learn powerful general-purpose representations? relevant case study work characters challenge people various machine learning approaches compared ability learn handwritten characters world’s alphabets. addition evaluating several types deep learning models developed algorithm using bayesian program learning represents concepts simple stochastic programs structured procedures generate examples concept executed programs allow model express causal knowledge data formed probabilistic semantics allow model handle noise perform creative tasks. structure sharing across concepts accomplished compositional reuse stochastic primitives combine ways create concepts. note overloading word model refer framework whole well individual probabilistic models infers images represent novel handwritten characters. hierarchy models figure causal compositional model handwritten characters. types generated compositionally choosing primitive actions library combining subparts make parts combining parts relations deﬁne simple programs programs create diﬀerent tokens concept rendered binary images probabilistic inference allows model generate examples example concept shown visual turing test. example image concept shown pair grids. grid generated people samples model. grid pair generated machine? answers adapted lake salakhutdinov tenenbaum higher-level program generates diﬀerent types concepts programs generate tokens concept. here describing learning rapid model building refers fact constructs generative models produce tokens concept learning models form allows perform challenging one-shot classiﬁcation task human level performance outperform current deep learning models convolutional networks representations learns also enable generalize other creative human-like ways evaluated using visual turing tests tasks include generating examples parsing objects essential components generating concepts style particular alphabet following sections discuss three main ingredients compositionality causality learning-to-learn important success framework believe important understanding human learning rapid model building broadly. ingredients naturally within probabilistic program induction framework could also integrated deep learning models types machine learning algorithms prospects discuss detail below. approach using convolutional matching networks achieves good one-shot classiﬁcation performance discriminating characters diﬀerent alphabets directly compared evaluated one-shot classiﬁcation characters alphabet. compositionality classic idea representations constructed combination primitive elements. computer programming primitive functions combined together create functions functions combined create even complex functions. function hierarchy provides eﬃcient description higher-level functions like part hierarchy describing complex objects scenes compositionality also core productivity inﬁnite number representations constructed ﬁnite primitives mind think inﬁnite number thoughts utter understand inﬁnite number sentences learn concepts seemingly inﬁnite space possibilities compositionality broadly inﬂuential cognitive science especially pertains theories object recognition conceptual representation language. focus compositional representations object concepts illustration. structural description models represent visual concepts compositions parts relations provides strong inductive bias constructing models concepts instance novel two-wheeled vehicle figure might represented wheels connected platform provides base post holds handlebars etc. parts composed sub-parts forming partonomy part-whole relationships novel vehicle example parts relations shared reused existing related concepts cars scooters motorcycles unicycles. since parts relations product previous learning facilitation construction models also example learning-to-learn another ingredient covered below. compositionality learning-to-learn naturally together also forms compositionality rely less previous learning bottom-up parts-based representation hoﬀman richards learning models novel handwritten characters operationalized similar way. handwritten characters inherently compositional parts strokes relations describe strokes connect other. lake salakhutdinov tenenbaum modeled parts using additional layer compositionality parts complex movements created simpler sub-part movements. characters constructed combining parts sub-parts relations novel ways compositionality also central construction types symbolic concepts beyond characters spoken words created novel combination phonemes gesture dance move created combination primitive body movements. eﬃcient representation frostbite similarly compositional productive. scene game composition various object types including birds ﬂoes igloos etc. representing compositional structure explicitly economical better generalization noted previous work object-oriented reinforcement learning many repetitions objects present diﬀerent locations scene thus representing identical instance object figure perceiving scenes without intuitive physics intuitive psychology compositionality causality. image captions generated deep neural network using code github.com/karpathy/neuraltalk. image credits gabriel villena fern´andez tvbs taiwan agence france-presse photo dave martin similar examples using images reuters news found twitter.com/interesting jpg. properties important eﬃcient representation quick learning game. further levels contain diﬀerent numbers combinations objects compositional representation objects using intuitive physics intuitive psychology glue would making crucial generalizations deep neural networks least limited notion compositionality. networks trained object recognition encode part-like features deeper layers whereby presentation types objects activate novel combinations feature detectors. similarly trained play frostbite learn represent multiple replications object features facilitated invariance properties convolutional neural network architecture. recent work shown type compositionality made explicit neural networks used eﬃcient inference structured generative models explicitly represent number objects scene beyond compositionality inherent parts objects scenes compositionality also important level goals sub-goals. recent work hierarchical-dqns shows providing explicit object representations deﬁning sub-goals based reaching objects dqns learn play games sparse rewards combining sub-goals together achieve larger goals look forward seeing ideas continue develop potentially providing even richer notions compositionality deep neural networks lead faster ﬂexible learning. capture full extent mind’s compositionality model must include explicit representations objects identity relations maintaining notion coherence understanding novel conﬁgurations. coherence related next principle causality discussed section follows. concept learning scene understanding causal models represent hypothetical real world processes produce perceptual observations. control reinforcement learning causal models represent structure environment modeling state-to-state transitions action/state-to-state transitions. concept learning vision models utilize causality usually generative every generative model also causal. generative model describes process generating data least assigns probability distribution possible data points generative process resemble data produced real world. causality refers subclass generative models resemble abstract level data actually generated. generative neural networks deep belief networks variational auto-encoders generate compelling handwritten digits mark causality spectrum since steps generative process bear little resemblance steps actual process writing. contrast generative model characters using bayesian program learning resemble steps writing although even causally faithful models possible. causality inﬂuential theories perception. analysis-by-synthesis theories perception maintain sensory data richly represented modeling process generated relating data causal source provides strong priors perception learning well richer basis generalizing ways tasks. canonical examples approach speech visual perception. instance liberman cooper shankweiler studdertkennedy argued richness speech perception best explained inverting production plan level vocal tract movements order explain large amounts acoustic variability blending cues across adjacent phonemes. discussed causality literal inversion actual generative mechanisms proposed motor theory speech. learning handwritten characters causality operationalized treating concepts motor programs abstract causal descriptions produce examples concept rather concrete conﬁgurations speciﬁc muscles causality important factor model’s success classifying generating examples seeing single example concept causal knowledge also shown inﬂuence people learn concepts; providing learner diﬀerent types causal knowledge changes learn generalize. example structure causal network underlying features category inﬂuences people categorize examples similarly related characters challenge people learn write novel handwritten character inﬂuences later perception categorization explain role causality learning conceptual representations likened intuitive theories explanations providing glue lets core features stick equally applicable features wash away borrowing examples murphy medin feature ﬂammable closely attached wood money underlying causal roles concepts even though feature equally applicable both; causal roles derive functions objects. causality also glue features together relating deeper underlying cause explaining features wings feathers co-occur across objects others not. beyond concept learning people also understand scenes building causal models. human-level scene understanding involves composing story explains perceptual observations drawing upon integrating ingredients intuitive physics intuitive psychology compositionality. perception without ingredients absent causal glue binds together lead revealing errors. consider image captions generated deep neural network many cases network gets objects scene correct fails understand physical forces work mental states people causal relationships objects words build right causal model data. steps towards deep neural networks related approaches learn causal models. lopez-paz muandet scholk¨opf tolstikhin introduced discriminative data-driven framework distinguishing direction causality examples. outperforms existing methods various causal prediction tasks unclear apply approach inferring rich hierarchies latent causal variables needed frostbite challenge characters challenge. graves learned generative model cursive handwriting using recurrent neural network trained handwriting data. synthesizes impressive examples handwriting various styles requires large training corpus applied tasks. draw network performs recognition generation handwritten digits using recurrent neural networks window attention producing limited circular area image time step recent variant draw applied generating examples novel character single training example model demonstrates impressive ability make plausible generalizations beyond training examples generalizes broadly cases ways especially human-like. clear could pass visual turing tests lake salakhutdinov tenenbaum although hope draw-style networks continue extended enriched could made pass tests. incorporating causality greatly improve deep learning models; trained without access causal data characters actually produced without incentive learn true causal process. attentional window crude approximation true causal process drawing rezende attentional window pen-like although accurate model could incorporated. anticipate sequential generative neural networks could make sharper one-shot inferences goal tackling full characters challenge incorporating additional causal compositional hierarchical structure potentially leading computationally eﬃcient neurally grounded variant model handwritten characters causal model frostbite would complex gluing together object representations explaining interactions intuitive physics intuitive psychology much like game engine generates game dynamics ultimately frames pixel images. inference process inverting causal generative model explaining pixels objects interactions agent stepping deactivate crab pushing agent water deep neural networks could play role ways serving bottom-up proposer make probabilistic inference tractable structured generative model serving causal generative model imbued right ingredients. humans machines make inferences beyond data strong prior knowledge must making diﬀerence people acquire prior knowledge learning-to-learn term introduced harlow closely related machine learning notions transfer learning multi-task learning representation learning. terms refer ways learning task accelerated previous parallel learning related tasks strong priors constraints inductive bias needed learn particular task quickly often shared extent related tasks. range mechanisms developed adapt learner’s inductive bias learn speciﬁc tasks apply inductive biases tasks. hierarchical bayesian modeling general prior concepts shared multiple speciﬁc concepts prior learned course learning speciﬁc concepts models used explain dynamics human learning-to-learn many areas cognition including word learning causal learning learning intuitive theories physical social domains machine vision deep convolutional networks discriminative methods form core recent recognition systems learning-to-learn occur sharing features models learned objects models learned objects neural networks also learn-to-learn optimizing hyperparameters including form weight update rule related tasks. transfer learning multi-task learning already important themes across deep learning particular systems learn tasks rapidly ﬂexibly humans capturing human-like learning-to-learn dynamics deep networks machine learning approaches could facilitate much stronger transfer tasks problems. gain full beneﬁt humans learning-to-learn however systems might ﬁrst need adopt compositional causal forms representations argued above. many character concepts background alphabets tune representations learn character concepts test alphabets. perform well current neural network approaches require much pre-training people bayesian program learning approach still solving characters challenge. cannot sure people knowledge domain understand works think people might similar. transfers readily concepts learns object parts sub-parts relations capturing learning concept like concepts like general. crucial learning-to-learn occurs multiple levels hierarchical generative process. previously learned primitive actions larger generative pieces re-used re-combined deﬁne generative models characters transfer occurs learning typical levels variability within typical generative model; provides knowledge ways generalize seen example character could possibly carry information variance. could also beneﬁt deeper forms learning-to-learn currently does important structure exploits generalize well built prior learned background pre-training whereas people might learn knowledge ultimately human-like machine learning system well. analogous learning-to-learn occurs humans learning many object models vision cognition consider novel two-wheeled vehicle figure learning-to-learn operate transfer previously learned parts relations reconﬁgure compositionally create model concept. deep neural networks could adopt similarly compositional hierarchical causal representations expect might beneﬁt learning-to-learn. frostbite challenge video games generally similar interdependence form representation eﬀectiveness learning-to-learn. people seem transfer knowledge multiple levels low-level perception high-level strategy exploiting compositionality levels. basically immediately parse game environment objects types objects causal relations them. people also understand video games like goals often involve approaching avoiding objects based type. whether person child seasoned gamer seems obvious interacting birds change game state either good video games typically yield costs rewards types interactions types hypotheses quite speciﬁc rely prior knowledge polar bear ﬁrst appears tracks agent’s location advanced levels attentive learner sure avoid depending level ﬂoes spaced apart close together suggesting agent able cross gaps others. humans typically direct experience alphabets even related drawing experience likely amounts equivalent hundred character-like visual concepts most. pre-training characters alphabets suﬃcient perform human-level one-shot classiﬁcation generation examples. best neural network classiﬁers error rates approximately times higher humans pre-trained alphabets three times higher pre-training times much data current need extensive pre-training illustrated deep generative models rezende present extensions draw architecture capable one-shot learning. general world knowledge previous video games help inform exploration generalization scenarios helping people learn maximally single mistake avoid mistakes altogether. deep reinforcement learning systems playing atari games impressive successes transfer learning still come close learning play games quickly humans can. example parisotto presents actor-mimic algorithm ﬁrst learns atari games watching expert network play trying mimic expert network action selection and/or internal states algorithm learn games faster randomly initialized scores might taken four million frames learning reach might reached million frames practice. anecdotally humans still reach scores minutes practice requiring less experience dqns. interaction representation previous experience building machines learn fast people deep learning system trained many video games itself enough learn games quickly people system aims learn compositionally structured causal models game built foundation intuitive physics psychology could transfer knowledge eﬃciently thereby learn games much quickly. previous section focused learning rich models sparse data proposed ingredients achieving human-like learning abilities. cognitive abilities even striking considering speed perception thought amount time required understand scene think thought choose action. general richer structured models require complex inference algorithms similar complex models require data making speed perception thought remarkable. combination rich models eﬃcient inference suggests another psychology neuroscience usefully inform also suggests additional build successes deep learning eﬃcient inference scalable learning important strengths approach. section discusses possible paths towards resolving conﬂict fast inference structured representations including helmholtz-machine-style approximate inference generative models cooperation model-free model-based reinforcement learning systems. hierarchical bayesian models operating probabilistic programs equipped deal theorylike structures rich causal representations world formidable algorithmic challenges eﬃcient inference. computing probability distribution entire space programs usually intractable often even ﬁnding single high-probability program poses intractable search problem. contrast representing intuitive theories structured causal models less natural deep neural networks recent progress demonstrated remarkable eﬀectiveness gradient-based learning high-dimensional parameter spaces. complete account learning inference must explain brain much limited computational resources popular algorithms approximate inference probabilistic machine learning proposed psychological models prominently proposed humans approximate bayesian inference using monte carlo methods stochastically sample space possible hypotheses evaluate samples according consistency data prior knowledge monte carlo sampling invoked explain behavioral phenomena ranging children’s response variability garden-path eﬀects sentence processing perceptual multistability moreover beginning understand methods could implemented neural circuits monte carlo methods powerful come asymptotic guarantees challenging make work complex problems like program induction theory learning. hypothesis space vast hypotheses consistent data good models discovered without exhaustive search? least domains people especially clever solution problem instead grappling full combinatorial complexity theory learning discovering theories slow arduous testiﬁed long timescale cognitive development learning saltatory fashion characteristic aspects human intelligence including discovery insight development problem-solving epoch-making discoveries scientiﬁc research discovering theories also happen much quickly person learning rules frostbite probably undergo loosely ordered sequence moments learn jumping ﬂoes causes change color changing color ﬂoes causes igloo constructed piece-by-piece birds make lose points make gain points change direction cost igloo piece little fragments frostbite theory assembled form causal understanding game relatively quickly seems like guided process arbitrary proposals monte carlo inference scheme. similarly described characters challenge people quickly infer motor programs draw character similarly guided processes. domains program theory learning happens quickly possible people employ inductive biases evaluate hypotheses also guide hypothesis selection. schulz suggested abstract structural properties problems contain information abstract forms solutions. even without knowing answer question where deepest point paciﬁc ocean? still knows answer must location map. answer inches question what year lincoln born? invalidated priori even without knowing correct answer. recent experiments tsividis tenenbaum schulz found children high-level abstract features domain guide hypothesis selection reasoning distributional properties like ratio seeds ﬂowers dynamical properties like periodic monotonic relationships causes eﬀects might eﬃcient mappings questions plausible subset answers learned? recent work spanning deep learning graphical models attempted tackle challenge amortizing probabilistic inference computations eﬃcient feed-forward mapping also think learning inference independent ideas learning model building discussed previous section. feed-forward mappings learned various ways example using paired generative/recognition networks variational optimization nearest-neighbor density estimation implication amortization solutions diﬀerent problems become correlated sharing amortized computations; evidence inferential correlations humans reported gershman goodman trend avenue potential integration deep learning models probabilistic models probabilistic programming training neural networks help perform probabilistic inference generative model probabilistic program another avenue potential integration diﬀerentiable programming ensuring program-like hypotheses diﬀerentiable thus learnable gradient descent possibility discussed concluding section introduced mnih used simple form model-free reinforcement learning deep neural network allows fast selection actions. indeed substantial evidence brain uses similar model-free learning algorithms simple associative learning discrimination learning tasks particular phasic ﬁring midbrain dopaminergic neurons qualitatively quantitatively consistent reward prediction error drives updating model-free value estimates. model-free learning however whole story. considerable evidence suggests brain also model-based learning system responsible building cognitive environment using plan action sequences complex tasks model-based planning essential ingredient human intelligence enabling ﬂexible adaptation tasks goals; rich model-building abilities discussed previous sections earn value guides action. argued discussion frostbite design numerous variants simple video game identical except reward function governed identical environment model state-action-dependent transitions. conjecture competent frostbite player easily shift behavior appropriately little additional learning hard imagine model-based planning approach environment model modularly combined arbitrary reward functions deployed immediately planning. boundary condition ﬂexibility fact skills become habitized routine application possibly reﬂecting shift model-based model-free control. shift arise rational arbitration learning systems balance trade-oﬀ ﬂexibility speed similarly probabilistic computations amortized eﬃciency plans amortized cached values allowing model-based system simulate training data model-free system process might occur oﬄine suggesting form consolidation reinforcement learning consistent idea cooperation learning systems recent experiment demonstrated model-based behavior becomes automatic course training thus marriage ﬂexibility eﬃciency might achievable human reinforcement learning systems guidance. intrinsic motivation also plays important role human learning behavior much previous discussion assumes standard view behavior seeking maximize reward minimize punishment externally provided rewards reinterpreted according internal value agent depend current goal mental state. also intrinsic drive reduce uncertainty construct models environment closely related learning-to-learn multi-task learning. deep reinforcement learning starting address intrinsically motivated learning discussing arguments paper colleagues three lines questioning critiques come frequently. think helpful address points directly maximize potential moving forward together. seem unfair compare neural networks humans amount training experience required perform task learning play atari games learning handwritten characters humans extensive prior experience networks beneﬁted from. people many hours playing games experience reading writing many handwritten characters mention experience variety loosely related tasks. neural networks pre-trained experience argument goes might generalize similarly humans exposed novel tasks. rationale behind multi-task learning transfer learning strategy long history shown promising results recently deep networks furthermore deep learning advocates argue human brain eﬀectively beneﬁts even experience evolution. deep learning researchers trying capture equivalent humans’ collective evolutionary experience would equivalent truly immense pretraining phase. agree humans much richer starting point neural networks learning tasks including learning concept play video game. point developmental start-up software building blocks argued creating richer starting point. less committed particular story regarding origins ingredients including relative roles genetically programmed experiencedriven developmental mechanisms building components early infancy. either fundamental building blocks facilitating rapid learning sparse data. learning-to-learn across multiple tasks conceivably route acquiring ingredients simply training conventional neural networks many related tasks suﬃcient generalize human-like ways novel tasks. argued section successful learningto-learn least human-level transfer learning enabled models right representational structure including building blocks discussed paper. learningto-learn powerful ingredient powerful operating compositional representations capture underlying causal structure environment also building intuitive physics psychology. finally recognize researchers still hold hope enough training datasets suﬃciently rich tasks enough computing power beyond tried deep learning methods might suﬃcient learn representations equivalent evolution learning provides humans with. sympathize hope believe deserves exploration although sure realistic one. understand principle evolution could build brain cognitive ingredients discuss here. stochastic hill-climbing slow require massively parallel exploration millions years innumerable dead-ends build complex structures complex functions willing wait long enough. contrast trying build representations scratch using backpropagation deep q-learning stochastic gradient-descent weight update rule ﬁxed network architecture unfeasible regardless much training data available. build representations scratch might require exploring fundamental structural variations network’s architecture gradient-based learning weight space prepared although deep learning researchers explore many architectural variations devising increasingly clever powerful ones recently researchers driving directing process. exploration creative innovation space network architectures made algorithmic. perhaps could using genetic programming methods structure-search algorithms think would fascinating promising direction explore acquire patience machine learning researchers typically express algorithms dynamics structure-search look much like slow random hill-climbing evolution smooth methodical progress stochastic gradient-descent. alternative strategy build appropriate infant-like knowledge representations core ingredients starting point learning-based systems build learning systems strong inductive biases guide direction. regardless developer chooses main points orthogonal objection. core cognitive ingredients human-like learning thought. deep learning models could incorporate ingredients combination additional structure perhaps additional learning mechanisms part approach human-like whether based deep learning likely gain incorporating ingredients. focused cognitive science motivate guide eﬀorts engineer human-like contrast advocates deep neural networks cite neuroscience inspiration. approach guided pragmatic view clearest path computational formalization human intelligence comes understanding software hardware. case article proposed ingredients software previous sections. nonetheless cognitive approach intelligence ignore know brain. neuroscience provide valuable inspirations cognitive models researchers centrality neural networks model-free reinforcement learning proposals thinking fast prime exemplars. neuroscience also principle impose constraints cognitive accounts cellular systems level. deep learning embodies brain-like computational mechanisms mechanisms incompatible cognitive theory argument cognitive theory favor deep learning. unfortunately know brain clear-cut. many seemingly well-accepted ideas regarding neural computation fact biologically dubious uncertain best thus disqualify cognitive ingredients pose challenges implementation within approach. example neural networks form gradient-based hebbian learning. long argued however backpropagation biologically plausible; crick famously pointed backpropagation seems require information transmitted backwards along axon realistic models neuronal function prevented backpropagation good connectionist models cognition building deep neural networks neural network researchers must regard good thing case concerns biological plausibility hold back research particular algorithmic approach learning. strongly agree although neuroscientists found mechanisms implementing backpropagation brain neither produced deﬁnitive evidence existing data simply oﬀer little constraint either backpropagation obviously great value engineering today’s best pattern recognition systems. hebbian learning another case point. form long-term potentiation spiketiming dependent plasticity hebbian learning mechanisms often cited biologically supported however cognitive signiﬁcance biologically grounded form hebbian learning unclear. gallistel matzel persuasively argued critical interstimulus interval orders magnitude smaller intervals behaviorally relevant forms learning. fact experiments simultaneously manipulate interstimulus intertrial intervals demonstrate critical interval exists. behavior persist weeks months whereas decays baseline course days learned behavior rapidly reacquired extinction whereas facilitation observed relevantly focus would especially challenging implement ingredients described article using purely hebbian mechanisms. claims biological plausibility implausibility usually rest rather stylized assumptions brain wrong many details. moreover claims usually pertain cellular synaptic level connections made systems level neuroscience subcortical brain organization understanding details matter requires computational theory moreover absence strong constraints neuroscience turn biological argument around perhaps hypothetical biological mechanism viewed skepticism cognitively implausible. long optimistic neuroscience eventually place constraints theories intelligence. believe cognitive plausibility oﬀers surer foundation. said little article people’s ability communicate think natural language distinctively human cognitive capacity machine capabilities strikingly. certainly could argue language included short list ingredients human intelligence instance mikolov featured language prominently recent paper sketching challenge problems road moreover natural language processing active area research deep learning widely recognized neural networks implementing human language abilities. question develop machines richer capacity language? believe understanding language role intelligence goes hand-in-hand understanding building blocks discussed article. also true language builds core abilities intuitive physics intuitive psychology rapid learning compositional causal models focus capacities place children master language provide building blocks linguistic meaning language acquisition hope better understanding earlier ingredients implement integrate computationally better positioned understand linguistic meaning acquisition computational terms explore ingredients make human language possible. human modes thought recursion kind recursive structure building ability ability reuse symbols name ability understand others intentionally build shared intentionality version things aspects capacities already present infants? important questions future work potential expand list ingredients; intend list complete. finally keep mind ways acquiring language extends enriches ingredients cognition focus article. intuitive physics psychology infants likely limited reasoning objects agents immediate spatial temporal vicinity simplest properties states. language older children become able reason much wider range physical psychological situations language also facilitates powerful learning-to-learn compositionality allowing people learn quickly ﬂexibly representing concepts thoughts relation existing concepts ultimately full project building machines learn think like humans must language core. last decades machine learning made remarkable progress computer programs beat chess masters; systems beat jeopardy champions; apps recognize photos friends; machines rival humans large-scale object recognition; smart phones recognize speech. coming years promise still exciting applications areas varied self-driving cars medicine genetics drug design robotics. ﬁeld proud accomplishments helped move research academic journals systems improve daily lives. also mindful achieved not. pace progress impressive natural intelligence still best example intelligence. machine performance rival exceed human performance particular tasks algorithms take inspiration neuroscience aspects psychology follow algorithm learns thinks like person. higher worth reaching potentially leading powerful algorithms also helping unlock mysteries human mind. comparing people current best algorithms machine learning people learn less data generalize richer ﬂexible ways. even relatively simple concepts handwritten characters people need examples concept able recognize examples generate examples generate concepts based related ones abilities elude even best deep neural networks character recognition trained many examples concept ﬂexibly generalize tasks. suggest comparative power ﬂexibility people’s inferences come causal compositional nature representations. thought incorporate psychological ingredients including outlined paper. before closing discuss recent trends promising developments deep learning trends hope continue lead important advances. recent interest integrating psychological ingredients deep neural networks especially selective attention augmented working memory experience replay ingredients lower-level cognitive ingredients discussed paper suggest promising trend using insights cognitive psychology improve deep learning even furthered incorporating higher-level cognitive ingredients. paralleling human perceptual apparatus selective attention forces deep learning models process perceptual data series high-resolution foveal glimpses rather once. somewhat surprisingly incorporation attention substantial performance gains variety domains including machine translation object recognition image caption generation attention help models several ways. helps coordinate complex outputs attending speciﬁc aspects input allowing model focus smaller sub-tasks rather solving entire problem shot. instance caption generation attentional window shown track objects mentioned caption network focus frisbee producing caption like throws frisbee attention also allows larger models trained without requiring every model parameter aﬀect every output action. generative neural network models attention used concentrate generating particular regions image rather whole image could stepping stone towards building causal generative models neural networks neural version bayesian program learning model could applied tackling characters challenge researchers also developing neural networks working memories augment shorterterm memory provided unit activation longer-term memory provided connection weights developments also part broader trend towards diﬀerentiable programming incorporation classic data structures random access memory stacks queues gradient-based learning systems example neural turing machine successor diﬀerentiable neural computer neural networks augmented random access external memory read write operations maintains end-to-end diﬀerentiability. trained perform sequence-to-sequence prediction tasks sequence copying sorting applied solving block puzzles ﬁnding paths nodes graph additionally neural programmer-interpreters learn represent execute algorithms addition sorting fewer examples observing input-output pairs well execution traces model seems learn genuine programs examples albeit representation like assembly language high-level programming language. generation neural networks tackle types challenge problems introduced paper diﬀerentiable programming suggests intriguing possibility combining best program induction deep learning. types structured representations model building ingredients discussed paper objects forces agents causality compositionality help explain important facets human learning thinking also bring challenges performing eﬃcient inference deep learning systems shown work representations demonstrated surprising eﬀectiveness gradient descent large models high-dimensional parameter spaces. synthesis approaches able perform eﬃcient inference programs richly model causal structure infant sees world would major step forward building human-like another example combining pattern recognition model-based search comes recent research game considerably diﬃcult chess recently computer program alphago ﬁrst beat world-class player using combination deep convolutional neural networks monte carlo tree search components made gains artiﬁcial real players notion combining pattern recognition model-based search goes back decades games. showing approaches integrated beat human champion important accomplishment important however questions directions opens long-term project building genuinely human-like worthy goal would build system beats world-class player amount kind training human champions receive rather overpowering google-scale computational resources. alphago initially trained million positions moves unique games played human experts; improves reinforcement learning playing million games itself. publication silver facing world champion sedol alphago iteratively retrained several times way; basic system always learned million games played successively stronger versions itself eﬀectively learning million games altogether contrast probably played around games entire life. looking numbers like these impressive even compete alphago all. would take build professional-level learns games? perhaps system combines advances alphago complementary ingredients intelligence argue would route end. could also gain much trying match learning speed ﬂexibility normal human players. people take long time master game frostbite characters challenges humans learn basics game quickly combination explicit instruction watching others experience. playing games teaches human enough beat someone learned rules never played before. could alphago model earliest stages real human learning curves? human players also adapt learned innumerable game variants. wikipedia page figure system playing combining deep convolutional network model-based search monte-carlo tree search convnet used predict next moves given current board. search tree current board state root current win/total statistics node. mcts rollout selects moves along tree according mcts policy reaches leaf next move chosen convnet. there play proceeds game’s according pre-deﬁned default policy based pachi program based mcts. end-game result leaf used update search tree. adapted tian permission. structures board torus mobius strip cube diamond lattice three dimensions. holes board regular irregular ways. rules adapted known first capture nogo time money players receive bonuses creating certain stone patterns capturing territory near certain landmarks. could four players competing individually teams. variants eﬀective play needs change basic game skilled player adapt simply relearn game scratch. could alphago? techniques handling variable sized inputs convnets help playing diﬀerent board sizes value functions policies alphago learns seem unlikely generalize ﬂexibly automatically people many variants described would require signiﬁcant reprogramming retraining directed smart humans programmed alphago system itself. impressive alphago beating world’s best players standard game extremely impressive fact cannot even conceive variants alone adapt autonomously sign understand game humans human players understand variants adapt explicitly represent game goal beat adversary playing achieve goal governed rules stones placed board board positions scored. humans represent strategies response constraints game changes begin adjust strategies accordingly. presents compelling challenges beyond matching world-class human performance trying match human levels understanding generalization based kinds amounts data explicit instructions opportunities social learning aﬀorded people. learning play quickly ﬂexibly people drawing cognitive ingredients paper laid out. learning-to-learn compositional knowledge. using core intuitive psychology aspects intuitive physics like alphago also integrating model-free pattern recognition model-based search. believe systems could built things potentially capturing better humans learn understand game. believe would richly rewarding cognitive science pursue challenge together systems could compelling testbed principles paper argues well building progress date alphago represents. paper suggested ingredients building computational models humanlike learning thought. principles explained context characters frostbite challenges special emphasis reducing amount training data required facilitating transfer novel related tasks. also ways ingredients spur progress core problems practical applications. oﬀer speculative thoughts scene understanding. deep learning moving beyond object recognition towards scene understanding evidenced ﬂurry recent work focused generating natural language captions images current algorithms still better recognizing objects understanding scenes often getting objects right causal relationships wrong compositionality causality intuitive physics intuitive psychology playing increasingly important role reaching true scene understanding. example picture cluttered garage workshop screw drivers hammers hanging wall wood pieces tools stacked precariously work desk shelving boxes framing scene. order autonomous agent eﬀectively navigate perform tasks environment agent would need intuitive physics properly reason stability support. holistic model scene would require composition individual object models glued together relations. finally causality helps infuse recognition existing tools understanding helping connect diﬀerent object models proper scene includes people acting interacting nearly impossible understand actions without thinking thoughts especially goals intentions towards objects agents believe present. autonomous agents intelligent devices. robots personal assistants cannot pre-trained possible concepts encounter. like child learning meaning words intelligent adaptive system able learn concepts small number examples encountered naturally environment. common concept types include spoken words gestures activities human-like system would able learn recognize produce instances small number examples. like handwritten characters system able quickly learn concepts constructing pre-existing primitive actions informed knowledge underlying causal process learning-to-learn. autonomous driving. perfect autonomous driving requires intuitive psychology. beyond detecting avoiding pedestrians autonomous cars could accurately predict pedestrian behavior inferring mental states including beliefs desires similarly drivers road similarly complex mental states underlying behavior type psychological reasoning along types model-based causal physical reasoning likely especially valuable challenging novel driving circumstances little relevant training data creative design. creativity often thought pinnacle human intelligence chefs design dishes musicians write songs architects design buildings entrepreneurs start businesses. still developing systems tackle types tasks compositionality causality central goal. many commonplace acts creativity combinatorial meaning unexpected combinations familiar concepts ideas illustrated figure novel vehicles created combination parts existing vehicles similarly novel characters constructed parts stylistically similar characters familiar characters re-conceptualized novel styles case free combination parts enough compositionality learning-to-learn provide parts ideas causality provides glue gives coherence purpose. since birth people wanted build machines learn think like people. hope researchers machine learning cognitive science accept challenge problems testbed progress. rather building systems recognize handwritten characters play frostbite result asymptotic process suggest deep learning computational paradigms tackle tasks using little training data people need also evaluate models range human-like generalizations beyond task model trained hope ingredients outlined article prove useful working towards goal seeing objects agents rather features building causal models recognizing patterns recombining representations without needing retrain learning-to-learn rather starting scratch. grateful peter battaglia matt botvinick y-lan boureau shimon edelman nando freitas anatole gershman george kachergis leslie kaelbling andrej karpathy george konidaris tejas kulkarni tammy kwan michael littman gary marcus kevin murphy steven pinker shafto david sontag pedro tsividis four anonymous reviewers helpful comments early versions manuscript. schaul helpful answering questions regarding learning curves frostbite scoring. work supported center minds brains machines award ccf- mooresloan data science environment nyu.", "year": 2016}