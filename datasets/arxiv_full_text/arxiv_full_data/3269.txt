{"title": "Image Data Compression for Covariance and Histogram Descriptors", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Covariance and histogram image descriptors provide an effective way to capture information about images. Both excel when used in combination with special purpose distance metrics. For covariance descriptors these metrics measure the distance along the non-Euclidean Riemannian manifold of symmetric positive definite matrices. For histogram descriptors the Earth Mover's distance measures the optimal transport between two histograms. Although more precise, these distance metrics are very expensive to compute, making them impractical in many applications, even for data sets of only a few thousand examples. In this paper we present two methods to compress the size of covariance and histogram datasets with only marginal increases in test error for k-nearest neighbor classification. Specifically, we show that we can reduce data sets to 16% and in some cases as little as 2% of their original size, while approximately matching the test error of kNN classification on the full training set. In fact, because the compressed set is learned in a supervised fashion, it sometimes even outperforms the full data set, while requiring only a fraction of the space and drastically reducing test-time computation.", "text": "computing nearest neighbor simply comparing pair histograms matrices non-trivial. histogram descriptors certain bins individually similar/dissimilar bins. therefore euclidean distance often poor measure distance cannot measure bin-wise dissimilarity. case covariance descriptors matrices convex half-cone— non-euclidean riemannian manifold embedded inside euclidean space. measuring distances matrices straight-forward euclidean metric ignores underlying manifold structure data tends systematically under-perform classiﬁcation tasks histogram covariance descriptors excel underlying structure incorporated distance metric. recently number proposed histogram distances although yield strong improvements classiﬁcation accuracy distances often costly compute similarly matrices specialized geodesic distances algorithms developed operate covariance manifold. constraint methods often require signiﬁcantly time make predictions test data. especially true computation geodesic distance along manifold requires eigen-decomposition individual pairwise distance— computation needs repeated training inputs classify single test input. cherian improved running time test classiﬁcation approximating riemmanian distance symmetrized log-determinant divergence. dimensional data bregman ball trees adapted however performance deteriorates quickly dimensionality increases. paper develop novel technique speed k-nearest neighbor applications covariance histogram image features used concert many speedup methods. methods called stochastic covariance compression stochastic histogram compression learn compressed training size approximately covariance histogram image descriptors provide effective capture information images. excel used combination special purpose distance metrics. covariance descriptors metrics measure distance along non-euclidean riemannian manifold symmetric positive deﬁnite matrices. histogram descriptors earth mover’s distance measures optimal transport histograms. although precise distance metrics expensive compute making impractical many applications even data sets thousand examples. paper present methods compress size covariance histogram datasets marginal increases test error k-nearest neighbor classiﬁcation. speciﬁcally show reduce data sets cases little original size approximately matching test error classiﬁcation full training set. fact compressed learned supervised fashion sometimes even outperforms full data requiring fraction space drastically reducing test-time computation. absence sufﬁcient data learn image descriptors directly inﬂuential classes feature descriptors histogram covariance descriptor. histogram descriptors ubiquitous computer vision descriptors designed capture distribution image gradients throughout image result visual bagof-words representation covariance descriptors generally symmetric positive deﬁnite matrices often used describe structure tensors diffusion tensors region covariances latter particularly well suited task object detection variety viewpoints illuminations. matches performance classiﬁer original data. data consist original training samples; instead contains artiﬁcially generated inputs explicitly designed error training data. original training discarded training test-time k-nearest neighbors among artiﬁcial samples. drastically reduces computation time shrinks storage requirements. facilitate learning compressed data sets borrow concept stochastic neighborhoods used data visualization metric learning leverage recent results machine learning community data compression euclidean spaces make three novel contributions derive methods compression covariance histogram data; devise efﬁcient methods solving optimizations using cholesky decomposition normalized change variable; carefully evaluate methods several real world data sets compare extensive state-of-the-art baselines. experiments show often compress covariance data original size without increase test error. cases match test error training size—leading order-of-magnitude speedups test time. finally learn compressed explicitly minimize error cases even outperforms full data achieving lower test error. assume given d-dimensional feature vectors x|f|} computed single input image. features compute covariance histogram image descriptors. covariance descriptors represent covariance matrix features vectorial data ‘nearµ ness’ often computed euclidean distance learned mahalanobis metric however euclidean/mahalanobis distance covariance matrices poor approximation true distance along manifold matrices. natural distance covariance matrices afﬁne-invariant riemannian metric geodesic distance manifold. deﬁnition positive deﬁnite cone matrices rank afﬁne-invariant riemannian metric matrices logf airm accurately describes dissimilarity covariances along manifold requires eigenvalue decomposition every input metric becomes intractable compute even moderatelysized covariance matrices demonstrate nearest neighbor classiﬁcation using jbld distance performance nearly identical airm much faster practice asymptotically requires computation histogram descriptors popular alternative covariance representations. assume ddimensional features image x|f|}. further collection features images training fi∪. .∪fn construct visual bag-of-words representation cluster features centroids centroids often referred codebook using codebook visual bag-of-words representation image k-dimensional vector element count many features nearest centroid. arguably successful histogram distances earth mover’s distance used achieve impressive results image classiﬁcation retrieval constructs distance histograms ‘lifting’ bin-to-bin distance called ground distance full histogram distance. speciﬁcally histogram vectors distance solution following linear program transportation matrix vector ones. element describes amount mass moved vectors match exactly. example ground distance visual bag-of-words representation euclidean distance centroid vectors ground distance metric shown also metric practice limitation distance high computational complexity. cuturi therefore introduce sinkhorn distance involves regularized version optimization problem objective. inspired neighborhood components analysis compute probability input classiﬁed correctly stochastic nearest neighbor classiﬁer compressed ˆxm} ideally implying compressed yields perfect predictions training set. kldivergence ideal -distribution simply log. objective minimize kl-divergences respect compressed covariance matrices ˆxm} gradient. ensure learned matrices decompose matrix unique cholesky decomposition upper triangular matrix. ensure remains perform gradient descent w.r.t. gradient w.r.t. analogous covariance compression also compress histogram descriptors refer stochastic histogram compression learn compressed histograms ˆhm} labels training histograms =−tr) entropy transport sinkhorn distance solution solution arbitrarily close upper bound exact solution optimization problem shown least order magnitude faster compute linear program speciﬁcally cuturi introduce simple iterative algorithm solve time practice algorithm iteration matrix scaling computation performed multiple histograms simultaneously. means algorithm parallel efﬁciently computed modern hardware architectures section detail covariance compression technique stochastic covariance compression uses stochastic neighborhood compress training input covariances ‘compressed’ covariances. learning original training discarded future classiﬁcations made using compressed inputs. since complexity testity computing single jbld distance assume given training covariance matrices rd×d corresponding labels goal learn compressed covariance matrices ˆxm} rd×d labels ˆym. initialize randomly sample covariance matrices training data copy associated labels optimize synthetic inputs minimize classiﬁcation error. classiﬁcation error non-continuous non-differentiable respect introduce stochastic neighborhood proposed hinton roweis soften neighborhood assignment allow optimization error. speciﬁcally place radial basis function around input proceed nearest prototypes assigned randomly. given probability picked nearest neighbor denoted datasets. evaluate covariance compression method benchmark data sets. dataset images object categories pictured solid blue background. category exemplar objects exemplar camera placed different positions. covariance descriptors segment images per-pixel color texture features construct covariances. ethz dataset low-resolution images surveillance cameras sizes original task identify person given image different individuals. original dataset multiple classes fewer individuals. therefore better demonstrate wide range compression ratios ﬁlter dataset include popular classes resulting individual images pixel-wise features construct covariance matrices. feret face recognition dataset gray-scale images faces individuals oriented various angles. majority individuals fewer images training also limit dataset popular individuals larger compression ratios gabor-ﬁlter covariances version rgbd object dataset contains point cloud frames objects three different views. task classify object object categories. covariance features consist intensity depth-map gradients well surface normal information. scene data hn}⊂ labels -dimensional simplex. objective. place stochastic neighborhood distribution compressed histograms deﬁne probability nearest neighbor sinkhorn distance normalizes valid probability. deﬁne probability training histogram predicted correctly summing compressed inputs share label. minimize kl-divergence perfect distribution learn compressed histograms ˆhj. gradient. covariance setting gradient objective w.r.t. compressed histogram gradient sinkhorn distance w.r.t. introduces challenges distance nested optimization problem; learned vector must remain well-deﬁned histogram throughout optimization i.e. must non-negative s.t. ﬁrst address gradient nested optimization problem w.r.t. i.e. primal formulation stated histogram occurs within constraints complicates gradient computation. instead form dual corresponding dual variables. strong duality primal dual formulations identical optimum however dual formulation unconstrained. gradient dual objective linear w.r.t. ˆhj. consider ﬁxed follows optimal value optimum optimal dual variable easily computed iterative sinkhorn algorithm mentioned section function approximation ignores reasonable approximation small step-sizes. address second problem simultaneously perform gradient descent ensuring always lies simplex propose change variable redeﬁne compressed histogram positive normalized quantity figure montages datasets used evaluations down left right objects different orientations; ethz person recognition; rgbd objects point clouds; feret face detection; kth-tipsb material categorization scene scene classiﬁcation. consists black white images different indoor outdoor scenes. split dataset training test sets create covariance features compute dense sift descriptors centered pixel image. sift features bins horizontal vertical directions orientation bins producing covariance descriptor. work learn rank-r projection matrix rd×r reduce size covariance matrices transformation uxiu. bayesian optimization select values covariance size well hyperparameters minimizing error small validation set. kth-tipsb dataset material classiﬁcation dataset materials total images. material different samples ﬁxed poses scales lighting conditions. follow procedure extract covariance descriptors using color information gabor ﬁlter responses. experimental setup. compare methods test error -nearest neighbor classiﬁcation uses entire training set. results depend random initialization sampling report average standard deviation across random runs datasets rgbd ethz report results averaged different train/test splits. baselines. compare method stochastic covariance compression number methods aimed reducing size training adapt covariance feature setting using full training using class-based subsampled training initialization condensed nearest neighbor reduced nearest neighbor random mutation hill climbing fast fcnn select subsets training leaveone-out training error full training well-known fast literature. works postprocessing output improve training error rmhc random subset selection method. fcnn must make modiﬁcation accommodate covariance matrix features. speciﬁcally fcnn requires computing centroid class regular intervals selection. centroid class given solving following optimization jbld divergence. cherian give efﬁcient iterative procedure solving optimization covariance fcnn implementation. classiﬁcation error. figure compares test error baselines sizes compressed equal training set. dataset feret large number classes larger compression ratios although fcnn output single reduced training plot intermediate test errors method compression ratios well. dataset able reduce test error nearly applied full dataset using less equal training data. ethz rgbd could match full error significance however error rates marginally higher. small compression ratios superior baselines well subsampling initialization. datasets ethz ﬁnal outputs fcnn roughly equivalent curve. however notable downside algorithms control size ﬁnal sets feret large contrast allows regulate compressed size precisely. rmhc also able regulate compressed size. however based random sampling performance poor feret. surprisingly scene learning compressed covariance dataset reduces test error error full training set. suspect occurs because training amount label noise partially alleviated subsampling essentially learns supervised covariance representation versus label-agnostic covariance descriptors. datasets reason shrink dataset original size discard original data—yielding speedups test-time. test-time speedup. table shows speedup classiﬁcation using full training various compression ratios learned scc). general speedups roughly denotes compression ratio. results match exceed accuracy blue. compression datasets compression ratio match test error full classiﬁcation. effect removed neighbor redundancies dataset gained factor roughly speedup. much larger speedups obtained compression ratio— although small increase classiﬁcation error. data many classes loss-free compression still yield speedup training time. table describes average training times denotes results). maximum compression training time order minutes. size compressed gets larger time increases small amounts indeed longest training time hours rgbd compression. furthermore entire compression done completely off-line prior testing. contributions training points gradient independent high computation memory load ratio. training could therefore potentially sped signiﬁcantly parallelization clusters gpus. evaluate technique compressing histogram datasets stochastic histogram compression current baseline methods constructing reduced training set. benchmark compare k-nearest neighbor accuracies compressed sets different sizes report test-time speedups achieved method. start describing datasets comparison. datasets. coil dataset consists grayscale image objects background masked black. object rotated degrees image taken every degrees yielding images class. construct histogram features follow procedure extract shape context log-polar histograms using randomly sampled edge points yielding histograms dimensionality ground distance distance bins log-polar histogram. mpeg dataset different shape classes images. image black background sold white shape cellular phone fountain octopus among others. follow procedure coil dataset extract shape context histograms also used mpeg dataset. ground distance also bins. kylberg texture dataset -class dataset different surfaces. used dataset without rotations contains images class. follow feature-extraction technique uses ﬁrst second order image-gradient features every pixels resizing. construct visual bag-of-words representation ﬁrst clustering features codewords. represent image -dimensional count vector entry corresponds number times gradient feature closest codeword. ground distance bins euclidean distance pair codewords. benchmark comparison methods test error -nearest neighbor classiﬁcation full training set. similarly dataset report results different train/test splits. algorithm bayesian optimization tune parameter deﬁnition well initial gradient descent learning rate minimize training error. additionally initialize results rmhc covariance setting appears largely outperform subsampling approach. exact baselines covariance features except sinkhorn distance dissimilarity measure. subtlety fcnn needs able compute centroid histograms respect sinkhorn distance. centroid histogram measures respect called wasserstein barycenter shown compute barycenter sinkhorn distance accelerated gradient approach solve sinkhorn centroid. classiﬁcation error. figure shows average test error standard deviation compression ratios coil kylberg many-class dataset mpeg. datasets. covariance setting outperforms matches error compression trade-offs baseline methods throughout evaluated settings. speciﬁc speedup full classiﬁcation able achieve lowest test error throughout. kylberg reduce training size without increase test error. coil mpeg ﬁnal compressed sets fcnn high compression ratio lead modest speedups. evaluate arguably least interesting settings nevertheless show error rates baselines completeness. test-time speedup. test time speedups full training shown table datasets). similar speedups reach maximum compression still reach order magnitude worst case mpeg data many classes. kylberg error lower full data set—even compression ratio. training time. training times shown table fast especially considering solving nested optimization problem compressed histograms sinkhorn distance. believe speed implementation improved gpus sinkhorn computation approximate second-order hill-climbing methods. training reduction considered context vector data euclidian distance three primary methods training consistent sampling prototype generation prototype positioning training consistent sampling iteratively adds inputs training reduced ‘reference set’ until reference perfectly classiﬁed training set. precisely technique condensed nearest neighbors number extensions notably reduced nearest neighbor searches smallest subset result correctly classiﬁes training data. additionally fast ﬁnds close training time linear size training set. prototype generation creates inputs represent training usually clustering prototype positioning learns reduced training optimizing appropriate objective. method similar recently proposed stochastic neighbor compression uses stochastic neighborhood learn prototypes euclidean space finally bucilua ﬁrst study model compression machine learning algorithms compressing neural networks. knowledge ﬁrst methods explicitly consider training reduction covariance histogram descriptors. work towards speeding test-time classiﬁcation covariance-valued data somewhat limited. jbld divergence proposed speed individual distance computations. cherian show possible adapt bregman ball trees generalization euclidean ball tree bregman divergences jbld divergence. done using clever iterative kmeans method followed leaf node projection technique onto relevant bregman balls. techniques complementary dataset compression method. large amount work devoted toward improving complexity earth mover’s distance using approximations instance point upper bound placed transport bins solved much efﬁciently. ling okada show ground distance distance bins reformulated exactly tree-based optimization problem unknown variables constraints sinkhorn approximation added advantage unconstrained dual formulation. many classiﬁcation settings sheer amount distance computations previously prohibited covariance histogram features. shown data sets compressed small fraction original sizes often slightly increasing test error. drastically speeds nearest neighbor search potential unlock applications covariance histogram features large datasets.", "year": 2014}