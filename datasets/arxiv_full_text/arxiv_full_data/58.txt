{"title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.", "text": "semantic matching aims determine matching degree texts fundamental problem many applications. recently deep learning approach applied problem signiﬁcant improvements achieved. paper propose view generation global interaction texts recursive process i.e. interaction texts position composition interactions preﬁxes well word level interaction current position. based idea propose novel deep architecture namely match-srnn model recursive matching structure. firstly tensor constructed capture word level interactions. spatial applied integrate local interactions recursively importance determined four types gates. finally matching score calculated based global interaction. show that degenerated exact matching scenario match-srnn approximate dynamic programming process longest common subsequence. thus exists clear interpretation match-srnn. experiments semantic matching tasks showed effectiveness match-srnn ability visualizing learned matching structure. introduction semantic matching critical task many applications natural language processing including information retrieval question answering paraphrase identiﬁcation target semantic matching determine matching score given texts. taking task question answering example given pair question answer matching function created determine matching degree texts. traditional methods feature based learning models usually rely exact matching patterns determine degree thus suffer vocabulary mismatching problem recently deep learning approach applied area well tackled vocabulary mismatching problem. existing work focus representing text several dense vectors calculate matching score based similarity vectors. examples include dssm cdssm arc-i cntn lstm-rnn multigrancnn mv-lstm however usually difﬁcult methods model complicated interaction relationship texts representations calculated independently. address problem deep methods proposed directly learn interaction relationship texts including deepmatch arc-ii matchpyramid etc. models conducts matching hierarchical matching structure global interaction texts composition different levels local interactions word level phrase level interactions. methods mechanism generation complicated interaction relationship texts clear thus lack interpretability. paper propose tackle problem recursive manner. speciﬁcally view generation global interactions recursive process. given texts w··· v··· interaction position composition interactions three interactions preﬁxes s∼s) word level interaction position stands preﬁx consisting previous words text compared previous hierarchical matching structure recursive matching structure capture interactions nearby words also take long distant interactions account. based idea propose novel deep architecture namely match-srnn model recursive matching structure. firstly similarity tensor constructed capture word level interactions texts element stands similarity vector words different texts. spatial recurrent neural network gated recurrent units applied tensor. speciﬁcally representation position viewed interactions preﬁxes i.e. determined four factors hi−j hij− hi−j− input word level interaction depending corresponding gates respectively. finally matching score produced linear scoring function representation global interaction obtained aforementioned spatial rnn. show match-srnn well approximate dynamic programming process longest common subsequence problem furthermore simulation experiments show clear matching path obtained backtracking maximum gates position similar lcs. thus clear interpretation global interaction generated match-srnn. conducted experiments question answering paper citation tasks evaluate effectiveness model. experimental results showed match-srnn signiﬁcantly outperform existing deep models. moreover visualize learned matching structure showed matching path texts sampled real data. proposal deep architecture namely matchsrnn model recursive matching structure. experimental results showed match-srnn significantly improve performances semantic matching compared existing deep models. dense vector compute matching score based similarity vectors. example dssm uses multi-layer fully connected neural network encode query vector. cdssm arc-i utilize convolutional neural network lstmrnn adopts recurrent neural network long short term memory units better represent sentence. different work cntn uses neural tensor network model interaction sentences instead using cosine function. capture complex matching relations. methods even match sentences multiple representations words phrases sentences level representations. examples include bicnn multigrancnn mvlstm general idea behind approach consistent users’ experience matching degree sentences determined meanings well captured. however usually difﬁcult methods model complicated interaction relationship texts especially already represented compact vector paradigm turns directly model interaction relationship texts. speciﬁcally interaction represented dense vector matching score produced integrating interaction. existing work paradigm create hierarchical matching structure i.e. global interaction texts generated compositing local interactions hierarchically. example deepmatch models generation global interaction texts integrating local interactions based hierarchies topics. matchpyramid uses model generation global interaction abstraction word level phrase level interactions. deﬁning matching structure hierarchically limitations since hierarchical matching structure usually relies ﬁxed window size composition long distant dependency local interactions cannot well captured kind models. recursive matching structure existing methods mechanism semantic matching complicated hard interpret. mathematics computer science facing complicated object common method simpliﬁcation divide problem subproblems type solve problems recursively. well-known thinking recursion. paper propose tackle semantic matching problem recursively. recursive rule deﬁned follows. deﬁnition given texts s={w··· s={v··· interaction preﬁxes s={w··· s={v··· hij) composited interactions sub-preﬁxes well word level interaction current position shown following equation figure illustrates example recursive matching structure sentences s={the mat} s={the played balls ﬂoor}. considering interaction s={the sat} s={the played balls} recursive matching structure deﬁned indicates composition interactions preﬁxes word level interaction ‘sat’ ‘balls’ stands interaction s={the cat} s={the played balls} denotes interaction s={the sat} s={the played} denotes interaction s={the cat} s={the played}. i.e. s={the played} utilized representing consists well human understanding. therefore expected recursive matching structure well capture complicated interaction relationship texts interactions preﬁxes taken consideration. compared hierarchical recursive matching structure able capture long-distant dependency among interactions. match-srnn section introduce deep architecture namely match-srnn model recursive matching structure. shown figure match-srnn consists three components neural tensor network capture word level interactions; spatial applied word interaction tensor obtain global interaction; linear scoring function obtain ﬁnal matching score. neural tensor network match-srnn neural tensor network ﬁrst utilized capture basic interactions texts i.e. word level interactions. speciﬁcally word ﬁrst represented distributed vector. given words vectors interaction represented vector slice tensor parameters parameters linear part. non-linear function rectiﬁer paper. interaction also represented similarity score cosine. adopt neural tensor network capture complicated interactions spatial second step match-srnn apply spatial word level interaction tensor. spatial also referred dimensional special case multi-dimensional according spatial given representations interactions preﬁxes function different choices. basic usually uses non-linear full connection layer type function easy computing often suffers gradient vanishing exploding problem therefore many variants proposed long short term memory gated recurrent units grid lstm here adopt since easy implement close relationship discussed following sections. proposed utilize several gates tackle aforementioned problems basic shown excellent performance tasks machine translation paper extend traditional sequences spatial gru. figure describes clearly extensions. d-gru given sentence stands embedding t-th words representation position i.e. computed follows =σxt ht−) r=σxt ht−) =φxt+u ht−)) ht−+z representation position parameters updating gate tries control whether propagate information states write generated information states reset gate tries reset information stored cells generating candidate hidden states. parameters model including parameters word embedding neural tensor network spatial jointly trained backpropagation stochastic gradient descent. speciﬁcally adagrad parameters training process. linear scoring function since spatial recursive model scanning input left right bottom obtain last representation right bottom corner. reﬂects global interaction texts. ﬁnal matching score obtained linear function denote parameters. optimization different tasks need utilize different loss functions train model. taking regression example square loss optimization discussion section show relationship matchsrnn well known longest common subsequence problem. theoretical analysis goal problem longest subsequence common sequences sequences many applications detection lengths used deﬁne matching degree between sequences. formally given sequences e.g. s={x··· s={y··· represents length length between obtained following recursive progress step determined four factors i.e. matching i{xi=yj}) i{xi=yj} indicator function equal otherwise. match-srnn strong connection lcs. show this ﬁrst degenerate match-srnn model exact matching problem replacing neural tensor network simple indicator function returns words identical otherwise i.e. sij=i{xi=yj}. dimension spatial cells also reset gates spatial disabled since length accumulated depending past histories. thus equation degenerated hij− hi−j hi−j− hij− hi−j hi−j− correspond terms i{xi=yj} equation respectively. please note calculated softmaxbyrow thus approximate operation equation appropriately setting parameters matchsrnn hi−j− approximate simple addition operation hi−j−+sij hi−j− correspond i{xi=yj} respectively. therefore computation well approximate simulation results conducted simulation experiment verify analysis result shown above. dataset constructed many random sampled sequence pairs sequence composed characters sampled vocabulary dataset collected yahoo answers community question answering system users propose questions system users submit answers whole dataset contains pairs question accompanied best answer. select pairs questions best answers length dataset contains pairs form positive pairs. question ﬁrst best answer query retrieval results whole answer lucene search engine. randomly select answers construct negative pairs. task match papers citation relationship. dataset constructed paper abstract information citation network collected commercial academic website. negative pairs randomly sampled whole dataset. finally positive negative instances. effectiveness match-srnn compared match-srnn several existing deep learning methods including arc-i arc-ii cntn lstmrnn multigrancnn mv-lstm matchpyramid. also compared popular strong baseline semantic matching information retrieval. match-srnn also implemented bidirectional version comparison also scans right bottom left word interaction tensor denoted bi-match-srnn. experiments parameters baselines follows. word embeddings used model baseline deep models initialized skipgram wordvec following previous practice word embeddings trained whole question answering data dimension batch size trainable parameters initialized randomly uniform distribution scale selected according performance validation set. initial learning rates adagrad also selected validation. dimension neural tensor network spatial best validation results among settings parameters baseline methods taking values original papers. task formulated ranking problem. therefore hinge loss optimization shown section results evaluated typical ranking measures precision mean reciprocal rank figure simulation result match-srnn. figure shows matching degree path discovered lcs. figure shows simulation results matchsrnn. figure shows backtracing process ﬁnding gray path match-srnn. firstly dynamic programming algorithm conducted sequence pair normalized length matching degree sequence pair. simulation split data training testing trained matchsrnn regression loss. simulation results sequences shown figure figure shows results scores position stands gray path indicates process ﬁnding sequences obtained backtracing dynamic programming process. figure gives results match-srnn score position stands representation scores produced matchsrnn identical obtained reveals relationship match-srnn lcs. gray path figure shows main path local interactions composited global interaction generated backtracing gates. figure shows path generation process three values positions stands three gates e.g. zl=. zt=. position considering last position matching signals passed direction largest value gates i.e. therefore move position position largest value gates zd=. therefore move position path induced match-srnn identical dynamic programming. analysis gives clear explanation mechanism semantic matching problem addressed match-srnn. number testing ranking lists positive sentence ranking list denotes rank sentence ranking list indicator function. task formulated binary classiﬁcation modeling recursive matching structure matchsrnn signiﬁcantly improve performances compared baselines. taking task example compared improvement terms compared mv-lstm best among deep learning methods focusing learning sentence representations improvement compared deep models using hierarchical composition structures improvements least task match-srnn also achieves best results though improvements smaller compared task. task much easier even simple model produce good result. analysis recursive matching structure help improve results semantic matching. figure representative interaction learned matchsrnn brightness dependent interaction value position path denotes information diffussion process generated backtracing maximum gates. tion. speciﬁcally values different dimensions gates identical convenient backtracing process. since hidden dimension used match-srnn obtain values hij. choose visualize feature dimension largest weight producing ﬁnal matching score. similar visualization obtained dimensions omit space limitation. visualization results shown figure brightness position stands interaction strength. recursive matching structure shown clearly. strong word level interaction happened position interaction texts strengthened thus bottom-right side position becomes brighter. interactions strengthened strong word level interactions i.e. bottomright side matching positions become even brighter. backtracing gates obtain matching path crosses points strong word interactions shown curve figure gives clear interpretation match-srnn conducted semantic matching real example. conclusions paper propose recursive thinking tackle complicated semantic matching problem. speciﬁcally novel deep learning architecture namely match-srnn proposed model recursive matching structure. matchsrnn consists three parts neural tensor network obtain word level interactions spatial generate global interactions recursively linear scoring function output matching degree. analysis reveals interesting connection match-srnn lcs. finally experiments semantic matching tasks showed matchsrnn signiﬁcantly outperform existing deep learning methods. furthermore visualized recursive matching structure discovered match-srnn real example. acknowledgments work funded program china grants program china grants national natural science foundation china grants research program chinese academy sciences grant kgzd-ew-t- youth innovation promotion association grants kyunghyun bart merrienboer aglar g¨ulc¸ehre fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. corr abs/. john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. jmlr alex graves j¨urgen schmidhuber. ofﬂine handwriting recognition multidimensional recurrent neural networks. nips pages hamid palangi deng yelong shen jianfeng xiaodong jianshu chen xinying song rabab ward. deep sentence embedding using long short term memory network analysis application information retrieval. corr abs/. richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders paraphrase detection. nips pages richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. nips pages richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. emnlp pages", "year": 2016}