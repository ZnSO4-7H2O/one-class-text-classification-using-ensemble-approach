{"title": "Hierarchical Compound Poisson Factorization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF.", "text": "non-negative matrix factorization models based hierarchical gamma-poisson structure capture user item behavior effectively extremely sparse data sets making ideal choice collaborative ﬁltering applications. hierarchical poisson factorization particular proved successful scalable recommendation systems extreme sparsity. however suffers tight coupling sparsity model response model limits expressiveness latter. here introduce hierarchical compound poisson factorization favorable gammapoisson structure scalability highdimensional extremely sparse matrices. importantly hcpf decouples sparsity model response model allowing choose suitable distribution response. hcpf capture binary non-negative discrete non-negative continuous zero-inﬂated continuous responses. compare hcpf nine discrete three continuous data sets conclude hcpf captures relationship sparsity response better hpf. matrix factorization central subject statistics since introduction principal component analysis goal embed data lower dimensional space minimal loss information. dimensionality reduction aspect matrix factorization become increasingly important exploratory data analysis dimensionality data exploded. alternative non-negative matrix factorization ﬁrst developed factorizing matrices face recognition idea behind contributions feature factor non-negative. although motivation behind choice roots cerebral representations objects non-negativeness found great appeal applications collaborative ﬁltering document classiﬁcation signal processing collaborative ﬁltering data highly sparse user item response matrices. example netﬂix movie rating data includes users movies ratings meaning matrix entries missdonors-choose data user reing. sponse item quantiﬁes monetary donation project; matrix includes donors projects donations meaning matrix entries missing. collaborative ﬁltering literature school thoughts treat missing entries. ﬁrst assumes entries missing random; observe uniformly sampled subset data factorization done premise response value provides information needed. second method assumes matrix entries missing random instead underlying mixture model ﬁrst coin ﬂipped determine entry missing. entry missing value zero; missing response drawn speciﬁc distribution framework postulate absence entry carries information item user information exploited improve overall quality factorization. difﬁcult part representing connection absence response numerical value response concerned problem sparse matrix factorization data missing random. extensions hint model addresses problem. recent work showed objective function equivalent factorized poisson likelihood authors proposed bayesian treatment poisson model gamma conjugate priors latent factors laying foundation hierarchical poisson factorization gamma-poisson structure also used earlier work matrix factorization favorable behavior long tailed gamma priors found powerful capturing underlying user item behavior collaborative ﬁltering problems applying strong shrinkage values near zero allowing non-zero responses escape shrinkage extension poisson factorization non-discrete data using data augmentation considered along similar lines connection beta divergences compound poisson gamma distribution exploited develop non-negative matrix factorization model sparse positive data recent work introduced stochastic variational inference algorithm scalable collaborative ﬁltering using models factor contribution drawn poisson distribution long tail gamma prior. thanks additive property poisson contributions poisson random variable used model response. collaborative ﬁltering problem treats missing entries true zero responses applied missing non-missing entries overwhelming majority matrix missing posterior estimates poisson factors close zero. mechanism profound impact response model poisson parameter zero truncated poisson distribution approaches zero converges degenerate distribution words condition fact entry missing predicts response value high probability. since response model sparsity model tightly coupled suitable sparse binary matrices. using full matrix might binarize data improve performance however binarization ignores impact response model absence. instance user likely watch movie similar movie gave high rating relative rated lower. information ignored model. paper introduce hierarchical compound poisson factorization gamma-poisson structure model equally computationally tractable. hcpf differs ﬂexibility decouples sparsity model response model allowing hcpf accurately model binary non-negative discrete non-negative continuous zero-inﬂated continuous responses context extreme sparsity. unlike distribution concentrate around instead converges response distribution choose. words effectively decouple sparsity model response model. decoupling imply independence instead ability capture distributional characteristics response accurately presence extreme sparsity. hcpf still retains useful property expected nonmissing response value related probability nonabsence allowing sparsity model exploit information responses. first generalize handle non-discrete data. section introduce additive exponential dispersion models class probability distributions. show member additive exponential dispersion model family including normal gamma inverse gaussian poisson binomial negative binomial zero-truncated poisson used response model. section prove compound poisson distribution converges element additive distribution sparsity increases. section describes generative model hierarchical compound poisson factorization mean ﬁeld stochastic variational inference algorithm hcpf allows hcpf data sets millions rows columns quickly section show favorable behavior hcpf compared twelve data sets varying size including ratings data sets social media activity data sets activity data music data biochemistry data ﬁnancial data sets genomics data additive exponential dispersion models generalization natural exponential family nonzero dispersion parameter scales log-partition function ﬁrst give formal deﬁnition additive present seven useful members deﬁnition family distributions table seven common additive exponential dispersion models. normal gamma inverse gaussian poisson binomial negative binomial zero truncated poisson distributions written additive form variational distribution poisson variable corresponding compound poisson additive edm. gamma distribution parametrized shape rate make following remark decoupling theorem. later show degenerate form model using remark. remark compound poisson random variable ordinary poisson random variable parameter element distribution degenerate distribution present decoupling theorem. theorem shows distribution zero truncated compound poisson random variable converges element distribution poisson parameter goes zero. zero truncated theorem compound poisson random variable element random variable zero support table generalized deﬁnition zero truncated poisson density i.i.d. random variables ordinary distribution expressed formula last column table shows variational update poisson parameter compound poisson additive discussed section start general deﬁnition compound poisson distribution discuss compound poisson additive distributions. present decoupling theorem explain implications theorem detail. deﬁnition poisson distributed random variable parameter i.i.d. random variables distributed element distribution called compound poisson random variable. general closed form expression well deﬁned probability distribution. conditional form usually easier manipulate calculate marginal distribution integrating degenerate distribution zero. furthermore element distribufigure zero truncated compound poisson random variable various sparsity levels scale. color coded darker colors correspond greater density. response distribution degenerate zero truncated poisson gamma distribution vertical lines mark sparsity levels various data sets. compound poisson variable element random distribution zero truncated theorem study probability density function various sparsity levels respect average sparsity levels discrete data sets continuous data sets. importantly element distribution nearly identical across levels sparsity. model entry full sparse matrix meaning including missing nonmissing entries. zero truncated random variable corresponds non-missing response. ordinary poisson variable. remark theorem suggest levels extreme sparsity almost probability mass concentrates predicts that entry missing value high probability. ﬂexible response model might regularize appropriate gamma priors; however approach would degrade performance sparsity model. hand compound poissonztp random variable extreme sparsity levels virtually effect distribution response using hcpf free choose additive distribution response model. discrete response data might degenerate poisson binomial negative binomial continuous response data might select gamma inverse gaussian normal distribution. furthermore hcpf explicitly encodes relationship non-absence expected non-missing response value deﬁned choice response model. degenerate hcpf model deﬁnes relationship leads poor behavior outside binary sparse matrices. along ﬂexibility choosing natural element distribution hcpf capable decoupling sparsity response models still encoding data-speciﬁc relationship sparsity model values nonzero responses expectation. next describe generative process hcpf gamma-poisson structure. explain intuition behind choices long-tailed gamma priors. present stochastic variational inference algorithm hcpf. tail gamma prior assumption users allows users unusually high responses. instance donations data might expect observe donors make extraordinarily large donations projects. appropriate movie ratings since maximum rating substantial number non-missing ratings ﬁves. long tail gamma prior items allow items receive unusually high average responses. useful property data sets imagine projects attract particularly large donations blogs receive likes movies receive unusually high average rating. choice long tail gamma priors different implications terms sparsity model. gamma prior particular user models active many items responses for. long tail assumption sparsity model implies unusually active users long tail assumption items corresponds popular items large number responses. note movies ratings necessarily highest rated movies. leverage fact contributions poisson factors written multinomial distribution using this write stochastic variational inference algorithm hcpf learning rate delay learning rate power note hcpf conjugate model variational updates need statistics purpose calculate explicitly ntr. choice truncation value depends λui. using expected range well ﬁxed hcpf reduces δyui. speciﬁc form different additive edms given table performed matrix factorization different data sets different levels sparsity response characteristics sizes rating data sets include amazon food ratings movielens netﬂix yelp responses star rating exception movielens maximum rating social media data sets include wordpress tencent response number likes non-negative integer value. commercial data sets include bestbuy response number user visit product page. biochemchoice long tail gamma priors substantial implications response model collaborative ﬁltering framework.the effect gamma prior particular user’s responses effectively characterize average response. similarly gamma prior particular item models average users’ response item. long identify best response model hcpf seven additive distributions table ﬁrst hcpf models sampling full matrix. calculated lcnm section compare hcpf second analysis used non-missing entries training calculated section compared lcnm ﬁrst analysis second analysis. analysis quantify well models capture sparsity response behavior ultra sparse matrices. movie ratings data question becomes ‘can predict user would rate given movie rating would give?’. report test likelihood twelve data sets. hcpf normal gamma inverse gaussian element distributions data sets. discrete data sets additionally hcpf poisson binomial negative binomial zero truncated poisson element distributions. ratings data sets hcpf signiﬁcantly outperforms relative performance difference even pronounced sparser data sets break test likelihood missing non-missing parts that sparser data sets relative performance non-missing entries much weaker less sparse data sets. expected response coupling stronger sparser data sets forcing response variables zero. opposite true hcpf performance improves increasing data sparsity. istry data sets include merck captures molecules chemical characteristics echonresponse chemical activity. response number times user listened song. donation data sets donation donorschoose includes donors projects response total amount donation dollars. genomics data geuvadis includes genes individuals response gene expression level user gene bestbuy merck nearly binary matrices meaning vast majority non-missing entries one. hand donation donorschoose geuvadis continuous response variable. held non-missing entries testing validation respectively. also sampled equal number missing entries testing validation. calculating test validation likelihood likelihood missing entries adjusted reﬂect true sparsity ratio. test likelihood missing non-missing entries well test likelihood non-missing entry conditioned missing calculated hcpf empirical study smaller data sets. hyperparameters maximum likelihood estimates element distribution parameters nonmissing entries. number non-missing entries training data inferred sparsity level effectively estimating empirically every user-item pair). used factorization hyperparameters create heavy tails uninformative gamma priors assumed contribution factor equal table test likelihood. per-thousand-entry test likelihood hcpf trained full matrix twelve data sets. discrete hcpf models applicable continuous data sets element distribution acronyms table table non-missing test likelihood. non-missing entry test likelihood hcpf trained full matrix non-missing entries trained full matrix conditional non-missing test likelihood reported. ponentially decaying characteristic. ﬁrst might seem good model data; however sparsity level high non-missing point mass concentrates best seen comparison hcpf-ztp. although poisson distribution seems capture response characteristic effectively test performance degrades zero included part response model bestbuy merck data sets response near binary hcpf performances similar. conﬁrms observation sufﬁciently good model sparse binary data sets. ﬁnancial data sets gamma inverse gaussian better distributional choices normal element distribution. question becomes ‘can predict rating user would give movie given know rated movie?’. table report conditional non-missing test likelihood models trained full matrix test likelihood models trained non-missing entries. first note training non-missing entries results better response model training full matrix. exception bestbuy conditional non-missing test likelihood near perfect. near binary structure data set. know entry missing fairly conﬁdent value secondly investigate modeling missing entries explicitly helps response model. compare trained non-missing entries hcpf trained full matrix. hcpf-bi outperforms ratings data sets except movielens. similar pattern seen social media music data sets hcpf-ig seems phenomenon attributed better identiﬁcation relationship sparsity model response model. although trained full matrix also capture relationship high sparsity levels force near-zero poisson parameters hurting prediction response. ratings data sets hcpf captures notion likely people consume item higher responses are. movielens netﬂix instance know watched movies tend higher ratings. social media data sets relation reach popularity captured. followers blog content likely produce reaction get. similar correlation exists users well. active users also responsive ones. summary hcpf capture relationship non-missingness entry actual value entry non-missing. algorithm makes missing random assumption would miss relationship. might argue perhaps good model because underlying response distribution possion like. comparing rows marked ‘nm’ observe truth argument. netﬂix yelp data sets hcpf-bi outperforms hpf; however also training hcpf-bi full matrix even better. similar argument made hcpf-ga social media data sets ﬂexibility hcpf factor identifying underlying response distribution. ability model sparsity response time gives hcpf edge modeling response. movie ratings data purchasing data important question ‘can predict user would rate given movie certain item?’. understand quality performance task evaluated sparsity model separately computing area curve ﬁtting hcpf models sampling full matrix binarized full matrix calculate true label whether data missing non-missing used estimated probability non-missing entry model prediction. discussed section full matrix compromise performance sparsity response. hcpf hand enjoys decoupling effect preserving relationship expectation data sets improvement somewhat surprising specialized task; illustrates beneﬁt coupling sparsity response models expectation. better modeling response would naturally lead better sparsity model. paper ﬁrst proved zero truncated compound poisson distribution converges element distribution sparsity increases. implication theorem non-missing response distribution concentrates good response model unless binary data set. inspired convergence theorem introduce hcpf. similar hcpf favorable gamma-poisson structure model longtailed user item activity. unlike hcpf capable modeling binary non-negative discrete non-negative continuous zero-inﬂated continuous data. importantly hcpf decouples sparsity response models allowing specify suitable distribution non-missing response entries. show decoupling effect improves test likelihood dramatically compared high-dimensional extremely sparse matrices. hcpf also shows superior performance trained exclusively non-missing entries terms modeling response. finally show hcpf better sparsity model despite targeting sparsity behavior. future directions investigate implications decoupling theorem bayesian settings. also explore hierarchical latent structures element distribution. funded sloan faculty fellowship. funded part princeton innovation insley blair pyne fund award. would like thank robert schapire valuable discussions. lappalainen sammeth friedlnder m.r. hoen monlong rivas m.a. gonzlezporta kurbatova griebel ferreira p.g. barann transcriptome genome sequencing uncovers functional variation humans. nature simsekli cemgil yilmaz learning beta-divergence tweedie compound poisson mainternational conference trix factorization models. machine learning", "year": 2016}