{"title": "A Brief Survey of Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.", "text": "cover seminal recent developments conveying innovative ways neural networks used bring closer towards developing autonomous agents. comprehensive survey recent efforts including applications areas natural language processing refer readers overview deep learning enables scale decision-making problems previously intractable i.e. settings high-dimensional state action spaces. amongst recent work ﬁeld outstanding success stories. ﬁrst kickstarting revolution development algorithm could learn play range atari video games superhuman level directly image pixels providing solutions instability function approximation techniques work ﬁrst convincingly demonstrate agents could trained high-dimensional observations solely based reward signal. second standout success development hybrid system alphago defeated human world champion paralleling historic achievement ibm’s deep blue chess decades earlier ibm’s watson deepqa system beat best human jeopardy players unlike handcrafted rules dominated chess-playing systems alphago composed neural networks trained using supervised reinforcement learning combination traditional heuristic search algorithm. algorithms already applied wide range problems robotics control policies robots learned directly camera inputs real world succeeding controllers used handengineered learned low-dimensional features robot’s state. step towards even capable agents used create agents meta-learn allowing generalise complex visual environments never seen figure showcase domains applied ranging playing video games indoor navigation video games interesting challenge learning play goal drl. driving forces behind vision creating systems capable learning adapt real world. managing power consumption picking stowing objects stands increase amount physical tasks automated learning. however stop there general approaching optimisation problems trial error. designing state-of-the-art machine translation models constructing optimisation functions already used approach manner machine learning tasks. abstract—deep reinforcement learning poised revolutionise ﬁeld represents step towards building autonomous systems higher level understanding visual world. currently deep learning enabling reinforcement learning scale problems previously intractable learning play video games directly pixels. deep reinforcement learning algorithms also applied robotics allowing control policies robots learned directly camera inputs real world. survey begin introduction general ﬁeld reinforcement learning progress main streams value-based policybased methods. survey cover central algorithms deep reinforcement learning including deep q-network trust region policy optimisation asynchronous advantage actor-critic. parallel highlight unique advantages deep neural networks focusing visual understanding reinforcement learning. conclude describe several current areas research within ﬁeld. primary goals ﬁeld artiﬁcial intelligence produce fully autonomous agents interact environments learn optimal behaviours improving time trial error. crafting systems responsive effectively learn long-standing challenge ranging robots sense react world around them purely software-based agents interact natural language multimedia. principled mathematical framework experience-driven autonomous learning reinforcement learning although successes past previous approaches lacked scalablity inherently limited fairly low-dimensional problems. limitations exist algorithms share complexity issues algorithms memory complexity computational complexity case machine learning algorithms sample complexity witnessed recent years—the rise deep learning relying powerful function approximation representation learning properties deep neural networks—has provided tools overcoming problems. advent deep learning signiﬁcant impact many areas machine learning dramatically improving state-of-the-art tasks object detection speech recognition language translation important property deep learning deep neural networks automatically compact low-dimensional representations high-dimensional data crafting inductive biases neural network architectures particularly hierarchical representations machine learning practitioners made effective progress addressing curse dimensionality deep learning similarly accelerated progress deep learning algorithms within deﬁning ﬁeld deep reinforcement learning survey fig. range visual domains. classic atari video games freeway seaquest arcade learning environment range supported games vary genre visuals difﬁculty become standard testbed algorithms discuss later several benchmarks used standardise evaluation torcs racing simulator used test algorithms output continuous actions utilising potentially unlimited amount training data amassed robotic simulators several methods transfer knowledge simulator real world four robotic tasks designed levine screwing bottle placing shaped block correct hole. levine able train visuomotor policies end-to-end fashion showing visual servoing could learned directly camera inputs using deep neural networks. real room wheeled robot trained navigate building given visual input must corresponding location natural image captioned neural network uses reinforcement learning choose look processing small portion image every word generated network focus attention salient points. figures reproduced respectively. examining contributions deep neural networks introduce ﬁeld general. essence learning interaction. agent interacts environment upon observing consequences actions learn alter behaviour response rewards received. paradigm trial-and errorlearning roots behaviourist psychology main foundations inﬂuence optimal control lent mathematical formalisms underpin ﬁeld. set-up autonomous agent controlled machine learning algorithm observes state environment timestep agent interacts environment taking action state agent takes action environment agent transition state based current state chosen action. state sufﬁcient statistic environment thereby comprises necessary information agent take best action include parts agent position actuators sensors. optimal control literature states actions often denoted respectively. best sequence actions determined rewards provided environment. every time environment transitions state also provides scalar reward agent feedback. goal agent learn policy maximises expected return given state policy returns action perform; optimal policy policy maximises expected return environment. respect aims solve problem optimal control. however challenge agent needs learn consequences actions environment trial error unlike optimal control model state transition dynamics available agent. every interaction environment yields information agent uses update knowledge. perception-actionlearning loop illustrated figure fig. perception-action-learning loop. time agent receives state environment. agent uses policy choose action action executed environment transitions step providing next state well feedback form reward rt+. agent uses knowledge state transitions form order learn improve policy. general policy mapping states probability distribution actions episodic i.e. state reset episode length sequence states actions rewards episode constitutes trajectory rollout policy. every rollout policy accumulates rewards environment γtrt+. goal optimal policy achieves maximum expected return states also possible consider non-episodic mdps situation prevents inﬁnite rewards accumulated. furthermore methods rely complete trajectories longer applicable ﬁnite transitions still are. concept underlying markov property— current state affects next state words future conditionally independent past given present state. means decisions made based solely rather st−}. although assumption held majority algorithms somewhat unrealistic requires states fully observable. generalisation mdps partially observable mdps agent receives observation distribution observation dependent current state previous action control signal processing context observation would described measurement/ observation mapping state-space-model depends current state previously applied action. pomdp algorithms typically maintain belief current state given previous belief state action taken current observation. common approach deep learning utilise recurrent neural networks which unlike feedforward neural networks dynamical systems. approach solving pomdps related problems using dynamical systems state space models true state estimated instructive emphasise challenges faced optimal policy must inferred trial-and-error interaction environment. learning signal agent receives reward. agents must deal long-range time dependencies often consequences action materialise many transitions environment. known credit assignment problem illustrate challenges context indoor robotic visual navigation task goal location speciﬁed able estimate distance remaining unlikely know exactly series actions robot needs take reach goal. robot must choose navigates building decisions inﬂuence rooms sees hence statistics visual sequence captured. finally navigating several junctions robot dead end. range problems learning consequences actions balancing exploration introduced formalism used brieﬂy noted challenges following distinguish different classes algorithms. main approaches solving problems methods based value functions methods based policy search. also hybrid actor-critic approach employs value functions policy search. explain approaches useful concepts solving problems. value functions available optimal policy could retrieved choosing among actions available picking action maximises est+∼t setting transition dynamics unavailable. therefore construct another function state-actionvalue quality function similar except initial action provided followed succeeding state onwards learning rate temporal difference error; here target standard regression problem. sarsa on-policy learning algorithm used improve estimate using transitions generated behavioural policy results setting γqπ. qlearning off-policy instead updated transitions necessarily generated derived policy. instead q-learning uses maxa directly approximates arbitrary generalised policy iteration policy iteration consists policy evaluation policy improvement. policy evaluation improves estimate value function achieved minimising errors trajectories experienced following policy. estimate improves policy naturally improved choosing actions greedily based updated value function. instead performing steps separately convergence generalised policy iteration allows interleaving steps progress made rapidly. sampling instead bootstrapping value functions using dynamic programming methods monte carlo methods estimate expected return state averaging return multiple rollouts policy. this pure monte carlo methods also applied non-markovian environments. hand used episodic mdps rollout terminate return calculated. possible best methods combining learning monte carlo policy evaluation done algorithm similarly discount factor used interpolate monte carlo evaluation bootstrapping. demonstrated figure results entire spectrum methods based around amount sampling utilised. another major value-function based method relies learning advantage function unlike producing absolute state-action values instead represents relative state-action values. learning relative values akin removing baseline average level signal; intuitively easier learn action better consequences another learn actual return taking action. represents relative advantage actions simple relationship also closely related baseline method variance reduction within gradient-based policy search methods idea advantage updates utilised many recent algorithms policy search policy search methods need maintain value function model directly search optimal policy typically parameterised policy chosen whose parameters updated maximise expected return using either gradient-based gradient-free optimisation neural networks encode policies successfully trained using gradient-free gradientbased methods. gradient-free optimisation effectively cover low-dimensional parameter spaces despite successes applying large networks gradient-based training remains method choice algorithms sample-efﬁcient fig. dimensions algorithms based backups used learn construct policy. extremes dimensions dynamic programming exhaustive search one-step learning pure monte carlo approaches. bootstrapping extends -step learning n-step learning methods pure monte carlo approaches relying bootstrapping all. another possible dimension variation choosing sample actions versus taking expectation choices. recreated constructing policy directly common output parameters probability distribution; continuous actions could mean standard deviations gaussian distributions whilst discrete actions could individual probabilities multinomial distribution. result stochastic policy directly sample actions. gradient-free methods ﬁnding better policies requires heuristic search across predeﬁned class models. methods evolution strategies essentially perform hill-climbing subspace policies whilst complex methods compressed network search impose additional inductive biases perhaps greatest advantage gradient-free policy search also optimise non-differentiable policies. policy gradients gradients provide strong learning signal improve parameterised policy. however compute expected return need average plausible trajectories induced current policy parameterisation. averaging requires either deterministic approximations stochastic approximations sampling deterministic approximations applied model-based setting model underlying transition dynamics available. common modelfree setting monte carlo estimate expected return determined. gradient-based learning monte carlo approximation poses challenge since gradients cannot pass samples stochastic function. therefore turn estimator gradient known reinforce rule elsewhere known score function likelihood-ratio estimator latter name telling using estimator similar practice optimising fig. actor-critic set-up. actor receives state environment chooses action perform. time critic receives state reward resulting previous interaction. critic uses error calculated information update actor. recreated log-likelihood supervised learning. intuitively gradient ascent using estimator increases probability sampled action weighted return. formally reinforce rule used compute gradient expectation function random variable respect parameters computation relies empirical return resulting gradients possess high variance. trajectory introducing unbiased estimates less noisy possible reduce variance. general methodology performing subtract baseline means weighting updates advantage rather pure return. simplest baseline average return taken several episodes many options available possible combine value functions explicit representation policy resulting actor-critic methods shown figure actor learns using feedback critic methods trade variance reduction policy gradients bias introduction value function methods actor-critic methods value function baseline policy gradients fundamental difference actor-critic methods baseline methods actor-critic methods utilise learned value function. reason later discuss actor-critic methods subset policy gradient methods. planning learning sample trajectories heuristic search even perform exhaustive search sutton barto deﬁne planning method utilises model produce improve policy. includes distribution models include sample models samples transitions drawn. focus learning without access underlying model environment. however interactions environment could used learn value functions policies also model. model-free methods learn directly interactions environment model-based methods simulate transitions using learned model resulting increased sample efﬁciency. particularly important domains interaction environment expensive. however learning model introduces extra complexities always danger suffering model errors turn affects learned policy; common partial solution latter scenario model predictive control planning repeated small sequences actions real environment although deep neural networks potentially produce complex rich models sometimes simpler dataefﬁcient methods preferable considerations also play role actor-critic methods learned value functions rise many successes based scaling prior work high-dimensional problems. learning low-dimensional feature representations powerful function approximation properties neural networks. means representation learning deal efﬁciently curse dimensionality unlike tabular traditional non-parametric methods instance convolutional neural networks used components agents allowing learn directly highdimensional visual inputs. general based training deep neural networks approximate optimal policy and/or optimal value functions although successes gradientfree methods vast majority current works rely gradients hence backpropagation algorithm primary motivation available gradients provide strong learning signal. reality gradients estimated based approximations sampling otherwise craft algorithms useful inductive biases order tractable. beneﬁt backpropagation view optimisation expected return optimisation stochastic function function comprise several parts—models policies value functions—which combined various ways. individual parts value functions directly optimise expected return instead embody useful domain. example using differentiable model policy possible forward propagate backpropagate entire rollouts; hand innacuracies accumulate long time steps pertinent instead value function summarise statistics rollouts previously mentioned representation learning function approximation success also true ﬁeld deep learning inspired ways thinking following review partition next part survey value function policy search methods starting well-known deep qnetwork sections focus stateof-the-art techniques well historical works built upon. focus state-of-the-art techniques state space conveyed visual inputs e.g. images video. conclude examine ongoing research areas open challenges. well-known function approximation properties neural networks naturally deep learning regress functions agents. indeed earliest success stories td-gammon neural network reached expert-level performance backgammon early using methods network took state board predict probability black white winning. although simple idea echoed later work progress research favoured explicit value functions capture structure underlying environment. early value function methods took simple states input current methods able tackle visually conceptually complex environments function approximation begin survey value-function-based algorithms pictured figure achieved scores across wide range classic atari video games comparable professional video games tester. inputs four greyscale frames game concatenated time initially processed several convolutional layers order extract spatiotemporal features movement ball pong breakout. ﬁnal feature convolutional layers processed several fully connected layers implicitly encode effects actions. contrasts traditional controllers ﬁxed preprocessing steps therefore unable adapt processing state response learning signal. dqn—neural ﬁtted iteration —involved training neural network return qvalue given state-action pair later extended train network drive slot using visual inputs camera race track combining deep autoencoder reduce dimensionality inputs separate branch predict q-values although previous network could trained reconstruction tasks simultaneously reliable computationally efﬁcient train parts network sequentially. fig. deep q-network network takes state—a stack greyscale frames video game—and processes convolutional fully connected layers relu nonlinearities layer. ﬁnal layer network outputs discrete action corresponds possible control inputs game. given current state chosen action game returns score. uses reward—the difference score previous one—to learn decision. precisely reward used update estimate error previous estimate estimate backpropagated network. lange ﬁrst algorithm demonstrated work directly visual inputs wide variety environments. designed ﬁnal fully connected layer outputs action values discrete actions—in case various directions joystick button. enables best action argmaxa chosen single forward pass network also allows network easily encode action-independent knowledge lower convolutional layers. merely goal maximising score video game learns extract salient visual features jointly encoding objects movements importantly interactions. using techniques originally developed explaining behaviour cnns object recognition tasks also inspect parts view agent considers important fig. saliency trained playing space invaders backpropagating training signal image space possible neural-network-based agent attending frame salient points—shown overlay—are laser agent recently ﬁred also enemy anticipates hitting time steps. true underlying state game contained within bytes atari ram. however designed directly learn visual inputs impractical represent exactly lookup table combined possible actions obtain qtable size even feasible create table would sparsely populated information gained state-action pair cannot propagated state-action pairs. strength lies ability compactly represent high-dimensional observations q-function using deep neural networks. without ability tackling discrete atari domain visual inputs would impractical. addressed fundamental instability problem using function approximation techniques experience replay target networks. experience replay memory stores transitions form cyclic buffer enabling agent sample train previously observed data ofﬂine. massively reduce amount interactions needed environment batches experience sampled reducing variance learning updates. furthermore sampling uniformly large memory temporal correlations adversely affect algorithms broken. finally practical perspective batches data efﬁciently processed parallel modern hardware increasing throughput. whilst original algorithm used uniform sampling later work showed prioritising samples based errors effective learning note although experience replay typically thought model-free technique could actually considered simple model second stabilising method introduced mnih target network initially contains weights network enacting policy kept frozen large period time. rather calculate error based rapidly ﬂuctuating estimates q-values policy network uses ﬁxed target network. training weights target network updated match policy network ﬁxed number steps. experience replay target networks gone used subsequent works q-function modiﬁcations considering components function approximator q-function beneﬁt fundamental advances hasselt showed single estimator used q-learning update rule overestimates expected return maximum action value approximation maximum expected action value. double-q learning provides better estimate double estimator whilst doubleq learning requires additional function learned later work proposed using already available target network algorithm resulting signiﬁcantly better results small change update step radical proposal bellemare actually learn full value distribution rather expectation; provides additional information whether potential rewards come skewed multimodal distribution. although resulting algorithm—based learning categorical distributions—was used construct categorical beneﬁts potentially applied algorithm utilises learned value functions. another adjust architecture decompose q-function meaningful functions constructing adding together separate layers compute state-value function advantage function rather come accurate qvalues actions duelling beneﬁts single baseline state form easier-tolearn relative values form combination duelling prioritised experience replay state-of-the-art techniques discrete action settings. insight properties modify convex advantage layer extended algorithm work sets continuous actions creating normalised advantage function algorithm. beneﬁting experience replay target networks advantage updates several state-of-the-art techniques continuous control problems domains recommender systems large discrete action spaces hence difﬁcult directly deal with. dulac-arnold proposed learning action embeddings large original actions using k-nearest neighbors produce proto-actions used traditional methods. idea using representation learning create distributed embeddings particular strength successfully utilised purposes another related scenario many actions need made simultaneously specifying torques many-jointed robot results action space growing exponentially. naive reasonable approach factorise policy treating action independently alternative construct autoregressive policy action single timestep predicted conditionally state previously chosen actions timestep metz used idea order construct sequential allowing discretise large action space outperform naf—which limited quadratic advantage function— continous control problems. broader context rather dealing directly primitive actions directly choose invoke subpolicies higher-level policies concept known hierarchical reinforcement learning discussed later. policy search methods directly policies means gradient-free gradient-based methods. prior current surge interest several successful methods eschewed commonly used backpropagation algorithm favour evolutionary algorithms gradient-free policy search algorithms. evolutionary methods rely evaluating performance population agents. hence expensive large populations agents many parameters. however black-box optimisation methods used optimise arbitrary non-differentiable models naturally allow exploration parameter space. combination compressed representation neural network weights evolutionary algorithms even used train large networks; technique resulted ﬁrst deep neural network learn task straight high-dimensional visual inputs recent work reignited interest evolutionary methods potentially distributed larger scales techniques rely gradients backpropagation stochastic functions workhorse however remains backpropagation previously discussed reinforce rule allows neural networks learn stochastic policies taskdependent manner deciding look image track classify caption objects cases stochastic variable would determine coordinates small crop image hence reduce amount computation needed. usage make discrete stochastic decisions inputs known deep learning literature hard attention compelling uses basic policy search methods recent years many applications outside traditional domains. generally ability backpropagate stochastic functions using techniques reinforce reparameterisation trick allows neural networks treated stochastic computation graphs optimised concept algorithms stochastic value gradients compounding errors searching directly policy represented neural network many parameters difﬁcult suffer severe local minima. around guided policy search takes sequences actions another controller learns using supervised learning combination importance sampling corrects off-policy samples approach effectively biases search towards good optimum. works loop optimising policies match sampled trajectories optimising trajectory distributions match policy minimise costs. initially used train neural networks simulated continuous problems later utilised train policy real robot based visual inputs research levine showed possible train visuomotor policies robot end-to-end straight pixels camera motor torques hence seminal works drl. commonly used method trust region optimisation steps restricted within region approximation true cost function still holds. preventing updated policies deviating wildly previous policies chance catastrophically update lessened many algorithms trust regions guarantee practically result monotonic improvement policy performance. idea constraining policy gradient update measured kullback-leibler divergence current proposed policy long history newer algorithms line work trust region policy optimisation shown relatively robust applicable domains high-dimensional inputs achieve this trpo optimises surrogate objective function—speciﬁcally optimises advantage estimate constrained using quadratic approximation divergence. whilst trpo used pure policy gradient method simple baseline later work schulman introduced generalised advantage estimation proposed several advanced variance reduction baselines. combination trpo remains stateof-the-art techniques continuous control. however constrained optimisation trpo requires calculating secondorder gradients newer proximal policy optimisation algorithm performs unconstrained optimisation requiring ﬁrst-order gradient information main variants include adaptive penalty divergence heuristic clipped objective independent divergence less expensive whilst retaining performance trpo means gaining popularity range tasks instead utilising average several monte carlo returns baseline policy gradient methods actorcritic approaches grown popularity effective means combining beneﬁts policy search methods learned value functions able learn full returns and/or errors. beneﬁt improvements policy gradient methods value function methods target networks last years actor-critic methods scaled learning simulated physics tasks real robotic visual navigation tasks directly image pixels. recent development context actor-critic algorithms deterministic policy gradients extend standard policy gradient theorems stochastic policies deterministic policies. major advantages dpgs that whilst stochastic policy gradients integrate state action spaces dpgs integrate state space requiring fewer samples problems large action spaces. initial work dpgs silver introduced demonstrated off-policy actor-critic algorithm vastly improved upon stochastic policy gradient equivalent high-dimensional continuous control problems. later work introduced deep utilised neural networks operate highdimensional visual state spaces vein dpgs heess devised method calculating gradients optimise stochastic policies reparameterising stochasticity away network thereby allowing standard gradients used resulting methods ﬂexible used svg) without value function critics svg) without models. later work proceeded integrate dpgs svgs rnns allowing solve continuous control problems pomdps learning directly pixels value functions introduce broadly applicable beneﬁt actor-critic methods—the ability off-policy data. onpolicy methods stable whilst off-policy methods data efﬁcient hence several attempts merge earlier work either utilised on-policy off-policy gradient updates used off-policy data train value function order reduce variance on-policy gradient updates recent work uniﬁed methods interpolated policy gradients resulting newest state-of-theart continuous algorithms also providing insights future research area. together ideas behind ipgs svgs form algorithmic approaches improving learning efﬁciency drl. orthogonal approach speeding learning exploit parallel computation. particular methods training networks asynchronous gradient updates developed single machines distributed systems keeping canonical parameters read updated asynchronous fashion multiple copies single network computation efﬁciently distributed processing cores single across cpus cluster machines. using distributed system nair developed framework training multiple dqns parallel achieving better performance reduction training time. however simpler asynchronous advantage actor-critic algorithm developed single distributed machine settings become popular techniques recent times. combines advantage updates actor-critic formulation relies asynchronously updated policy value function networks trained parallel several processing threads. multiple agents situated independent environments stabilises improvements parameters conveys additional beneﬁt allowing exploration occur. used standard starting point many subsequent works including work applied robotic navigation real world visual inputs. simplicity underlying algorithm used agent termed advantage actor-critic alternatively segments trajectories multiple agents collected processed together batch batch processing efﬁciently enabled gpus; synchronous version also goes name adds retrace off-policy bias correction q-value-based allowing experience replay order improve sample complexity. others attempted bridge value policy-based utilising theoretical advancements improve upon original finally growing trend towards exploiting auxiliary tasks improve representations learned agents hence improve learning speed ﬁnal performance agents conclude highlight current areas research challenges still remain. previously focused mainly model-free methods examine model-based algorithms detail. model-based algorithms play important role making data-efﬁcient trading exploration exploitation. tackling exploration strategies shall address imposes inductive bias ﬁnal policy explicitly factorising several levels. available trajectories controllers used bootstrap learning process leading imitation learning inverse ﬁnal topic speciﬁc look multi-agent systems special considerations. bring attention broader areas— rnns transfer learning—in context drl. examine issue evaluating current benchmarks drl. model-based idea behind model-based learn transition model allows simulation environment without interacting environment directly. model-based assume speciﬁc prior knowledge. however practice incorporate prior knowledge speed learning. model learning plays important role reducing amount required interactions environment limited practice. example unrealistic perform millions experiments robot reasonable amount time without signiﬁcant hardware wear tear. various approaches learn predictive models dynamical systems using pixel information. based deep dynamical model high-dimensional observations embedded lower-dimensional space using autoencoders several model-based algorithms proposed learning models policies pixel information sufﬁciently accurate model environment learned even simple controllers used control robot directly camera images learned models also used guide exploration purely based simulation environment deep models allowing techniques scaled high-dimensional visual domains compelling insight beneﬁts neural-networkbased models overcome problems incurred planning imperfect models; effect embedding activations predictions models vector agent obtain information ﬁnal result model rollouts also learn downplay information believes model inaccurate efﬁcient though less principled bayesian methods propagating uncertainty another make ﬂexiblity neural-network-based models decide plan given ﬁnite amount computation whether worth modelling long trajectory several short trajectories anything in-between simply take action real environment although deep neural networks make reasonable predictions simulated environments hundreds timesteps typically require many samples tune large amount parameters contain. training models often requires samples simpler models. reason train locally linear models algorithm— continuous equivalent improve algorithm’s sample complexity robotic domain samples expensive. order spur adoption deep models model-based necessary strategies used order improve data efﬁciency less common potentially useful paradigm exists model-free model-based methods—the successor representation rather picking actions directly performing planning models learning replaced learning expected future occupancies linearly combined order calculate optimal action; decomposition makes robust model-free methods reward structure changes work extending deep neural networks demonstrated usefulness multitask settings whilst within complex visual environment exploration exploitation greatest difﬁculties fundamental dilemma exploration versus exploitation agent non-optimal actions order explore environment exploit optimal action order make useful progress? off-policy algorithms typically simple \u0001-greedy exploration policy chooses random action probability optimal action otherwise. decreasing time agent progresses towards exploitation. although adding independent noise exploration usable continuous control problems sophisticated strategies inject noise correlated time order better preserve momentum observation temporal correlation important osband propose bootstrapped maintains several q-value heads learn different values combination different weight initialisations bootstrapped sampling experience replay memory. beginning training episode different head chosen leading temporally-extended exploration. usunier later proposed similar method performed exploration policy space adding noise single output head using zero-order gradient estimates allow backpropaone main principled exploration strategies upper conﬁdence bound algorithm based principle optimism face uncertainty idea behind pick actions maximise standard deviation return therefore encourages exploration regions high uncertainty moderate expected return. whilst easily achievable small tabular cases powerful density models conversely hashing allowed algorithm scale high-dimensional visual domains drl. technique trading exploration exploitation context bayesian optimisation future work beneﬁt investigating successful techniques used bayesian optimisation. also considered implementing intrinsic motivation general concept advocates decreasing uncertainty/making progress learning environment several algorithms intrinsic motivation minimising model prediction error maximising information gain hierarchical deep learning relies hierarchies features relies hierarchies policies. early work area introduced options which apart primitive actions policies could also policies approach allows top-level policies focus higher-level goals whilst subpolicies responsible control. several works attempted using top-level policy chooses subpolicies division states goals subpolicies achieved either manually automatically help construct subpolicies focus discovering reaching goals speciﬁc states environment; often locations agent navigate whether utilised discovery generalisation goals also important area ongoing research imitation learning inverse given sequence optimal actions expert demonstrations possible supervised learning straightforward manner—a case learning demonstration. indeed possible known behavioural cloning traditional literature. taking advantage stronger signals available supervised learning problems behavioural cloning enjoyed success earlier neural network research notable success alvinn earliest autonomous cars however behavioural cloning cannot adapt situations small deviations demonstration execution learned policy compound lead scenarios policy unable recover. generalisable solution provided trajectories guide learning suitable state-action pairs ﬁne-tune agent using alternatively expert still available query training agent active learning gather extra data unsure allowing learn states away optimal trajectories applied deep learning setting trained visual navigation task active learning signiﬁcantly improved upon pure imitation learning baseline goal estimate unknown reward function observed trajectories characterise desired solution used combination improve upon demonstrated behaviour. using power deep neural networks possible learn complex nonlinear reward functions ermon showed policies uniquely characterised occupancies allowing reduced problem measure matching. insight able generative adversarial training facilitate reward function learning ﬂexible manner resulting generative adversarial imitation learning algorithm. gail later extended allow applied even receiving expert trajectories different visual viewpoint agent complementary work baram exploit gradient information used gail learn models within process. multi-agent usually considers single learning agent stationary environment. contrast multi-agent considers multiple agents learning often non-stationarity introduced agents changing behaviours learn focus enabling communication agents allows co-operate. several approaches proposed purpose including passing messages agents sequentially using bidirectional channel allto-all channel addition communication channels natural strategy apply marl complex scenarios preclude usual practice modelling cooperative competing agents applied elsewhere marl literature works note marl investigate effects learning sequential decision making game theory memory attention earliest works spawned many extensions. ﬁrst extensions converting allows network better deal pomdps integrating information long time periods. like recursive ﬁlters recurrent connections provide efﬁcient means acting conditionally temporally distant prior observations. using recurrent connections hidden units deep recurrent q-network introduced hausknecht stone able successfully infer velocity ball game pong even frames game randomly blanked out. improvements gained introducing attention— technique additional connections added recurrent units lower layers—to drqn resulting deep attention recurrent q-network attention gives network ability choose part next input focus allowed darqn beat drqn games require longer-term planning. however outperformed drqn darqn games requiring quick reactions q-values ﬂuctuate rapidly. taking recurrent processing further possible differentiable memory allows ﬂexibly process information working memory traditional rnns recurrent units responsible performing calculations storing information. differentiable memories large matrices purely used storing information accessed using differentiable read write operations analagously computer memory. key-value-based memory q-network constructed agent could solve simple maze built minecraft correct goal episode indicated coloured block shown near start maze. especially sophisticated variants signiﬁcantly outperformed drqn baselines highlighting importance using decoupled memory storage. recent work memory given structure order resemble spatial hints future research specialised memory structures developed address speciﬁc problems navigation alternatively differentiable memories used approximate hash tables allowing algorithms store retrieve successful experiences facilitate rapid learning even though algorithms process high-dimensional inputs rarely feasible train agents directly visual inputs real world large number samples required. speed learning possible exploit previously acquired knowledge related tasks comes several guises transfer learning multitask learning curriculum learning name few. much interest transferring learning task another particularly training physics simulators visual renderers ﬁne-tuning models real world. achieved naive fashion directly using network simulated real phases sophisticated training procedures directly mitigate problem neural networks catastrophically forgetting knowledge adding extra layers transferring domain approaches involve directly learning alignment simulated real visuals even different camera viewpoints different form transfer utilised help form multitask training especially neural networks supervised unsupervised learning tasks help train features used agents making optimising objective easier achieve. example unsupervised reinforcement auxiliary learning acbased agent additionally trained pixel control plus reward prediction value function learning experience replay meanwhile ac-based agent mirowski additionally trained construct depth given inputs helps task learning navigate environment. ablation study mirowski showed predicting depth useful receiving depth extra input lending support idea gradients induced auxiliary tasks extremely effective boosting drl. transfer learning also used construct dataparameter-efﬁcient policies. student-teacher paradigm machine learning ﬁrst train powerful teacher model guide training less powerful student model. whilst originally applied supervised learning neural network knowledge transfer technique known distillation utilised transfer policies learned large dqns smaller dqns transfer policies learned several dqns trained separate games single together combination multitask transfer learning improve sample efﬁciency robustness current algorithms important topics wish construct agents accomplish wide range tasks since naively training multiple objectives infeasible. challenges ﬁeld machine learning developing standardised evaluate techniques. although much early work focused simple custom mdps shortly emerged control problems could used standard benchmarks testing algorithms cartpole mountain domains. however problems limited relatively small state spaces therefore failed capture complexities would encountered realistic scenarios. arguably initial driver provided interface atari video games code access games provided initial release video games vary greatly still present interesting challenging objectives humans provide excellent testbed agents. ﬁrst algorithm successfully play range games directly visuals secured place milestone development algorithms. success story started trend using video games standardised testbeds several interesting options available. vizdoom provides interface doom ﬁrst-person shooter echoing popularity esports competitions vizdoom competitions held yearly ieee conference computational intelligence games. facebook’s torchcraft deepmind’s starcraft learning environment respectively provide interfaces starcraft starcraft real-time strategy games presenting challenges micromanagement long-term planning. provide ﬂexible environments deepmind developed quake arena ﬁrst-person shooter engine microsoft’s project malmo exposed interface minecraft sandbox game environments provide customisable platforms approaches focus discrete actions solutions also developed continuous control problems. many papers continuous control used mujoco physics engine obtain relatively realistic dynamics multi-joint continuous control problems effort standardise problems help standardisation reproducibility aforementioned domains made available openai library online service allows people easily interface publicly share results algorithms domains vii. conclusion beyond pattern recognition despite successes many problems need addressed techniques applied wide range complex real-world problems recent work generative causal models demonstrated superior generalisation standard algorithms benchmarks achieved reasoning causes effects environment example schema networks kanksy trained game breakout immediately adapted variant small wall placed front target blocks whilst progressive networks failed match performance schema networks even training domain. although already combined techniques search planning deeper integration traditional approaches promises beneﬁts better sample complexity generalisation interpretability time also hope theoretical understanding properties neural networks improve currently lags behind practice. conclude worth revisiting overarching goal research creation general-purpose systems interact learn world around them. interaction environment simultaneously advantage disadvantage whilst many challenges seeking understand complex everchanging world allows choose explore effect endows agents ability perform experiments better understand surroundings enabling learn even high-level causal relationships. availability high-quality visual renderers physics engines enables take steps direction works learn intuitive models physics visual environments challenges remain possible real world steady progress made agents learn fundamental principles world observation action. perhaps away systems learn human-like ways increasingly complex environments. authors would like thank reviewers broader community feedback survey; particular would like thank nicolas heess clariﬁcations several points. arulkumaran would like acknowledge arulkumaran dilokthanakul murray shanahan anil anthony bharath. classifying options deep reinforcement learning. ijcai workshop deep reinforcement learning frontiers challenges dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio. actorcritic algorithm sequence prediction. iclr andrew barto richard sutton charles anderson. neuronlike adaptive elements solve difﬁcult learning control problems. ieee trans. systems cybernetics charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich k¨uttler andrew lefrancq simon green v´ıctor vald´es amir sadik deepmind lab. arxiv. marc bellemare sriram srinivasan georg ostrovski schaul david saxton r´emi munos. unifying count-based exploration intrinsic motivation. nips paul christiano zain shah igor mordatch jonas schneider trevor blackwell joshua tobin pieter abbeel wojciech zaremba. transfer simulation real world learning deep inverse dynamics model. arxiv. jeffrey dean greg corrado rajat monga chen matthieu devin mark andrew senior paul tucker yang quoc large scale distributed deep networks. nips marc deisenroth gerhard neumann peters. survey policy search robotics. foundations trends robotics misha denil pulkit agrawal tejas kulkarni erez peter battaglia nando freitas. learning perform physics experiments deep reinforcement learning. iclr duan john schulman chen peter bartlett ilya sutskever pieter abbeel. fast reinforcement learning slow reinforcement learning. nips workshop deep reinforcement learning gabriel dulac-arnold richard evans hado hasselt peter sunehag timothy lillicrap jonathan hunt timothy mann theophane weber thomas degris coppin. deep reinforcement learning large discrete action spaces. arxiv. david ferrucci eric brown jennifer chu-carroll james david gondek aditya kalyanpur adam lally william murdock eric nyberg john prager building watson overview deepqa project. magazine goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. nips shixiang timothy lillicrap zoubin ghahramani richard turner bernhard sch¨olkopf sergey levine. interpolated policy gradient merging on-policy off-policy gradient estimation deep reinforcement learning. nips nicolas heess srinivasan sriram lemmon josh merel greg wayne yuval tassa erez ziyu wang eslami martin riedmiller emergence locomotion behaviours rich environments. arxiv. todd hester matej vecerik olivier pietquin marc lanctot schaul bilal piot andrew sendonaris gabriel dulac-arnold osband john agapiou learning demonstrations real world reinforcement learning. arxiv. jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reiclr inforcement learning unsupervised auxiliary tasks. sham kakade. natural policy gradient. nips kansky silver david m´ely mohamed eldawy miguel l´azaro-gredilla xinghua nimrod dorfman szymon sidor scott phoenix dileep george. schema networks zero-shot transfer generative causal model intuitive physics. icml hilbert kappen. path integrals symmetry breaking optimal michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. tejas kulkarni karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. nips tejas kulkarni ardavan saeedi simanta gautam samuel gershman. deep successor reinforcement learning. nips workshop deep reinforcement learning leung herbert robbins. asymptotically efﬁcient adaptive allocation rules. advances applied mathematics brenden lake tomer ullman joshua tenenbaum samuel gershman. building machines learn think like people. behavioral brain sciences page sergey levine pieter abbeel. learning neural network policies guided policy search unknown dynamics. nips sergey levine vladlen koltun. guided policy search. iclr sergey levine peter pastor alex krizhevsky deirdre quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. iser jitendra malik. learning optimize. xiujun lihong jianfeng xiaodong jianshu chen deng recurrent reinforcement learning hybrid approach. arxiv. timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. iclr volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. iclr anusha nagabandi gregory kahn ronald fearing sergey levine. neural network dynamics model-based deep reinforcement learning model-free fine-tuning. arxiv. arun nair praveen srinivasan blackwell cagdas alcicek rory fearon alessandro maria vedavyas panneershelvam mustafa suleyman charles beattie stig petersen massively parallel icml workshop methods deep reinforcement learning. deep learning andrew adam coates mark diel varun ganapathi jamie schulte eric berger eric liang. autonomous inverted helicopter flight reinforcement learning. experimental robotics pages emilio parisotto ruslan salakhutdinov. neural structured memory deep reinforcement learning. arxiv. emilio parisotto jimmy ruslan salakhutdinov. actormimic deep multitask transfer reinforcement learning. iclr razvan pascanu yujia oriol vinyals nicolas heess lars buesing sebastien racani`ere david reichert th´eophane weber daan wierstra peter battaglia. learning model-based planning scratch. arxiv. peng peng ying yaodong yang quan yuan zhenkun tang haitao long wang. multiagent bidirectionally-coordinated nets emergence human-level coordination learning play starcraft combat games. arxiv. alexander pritzel benigno uria sriram srinivasan adri`a puigdom`enech oriol vinyals demis hassabis daan wierstra charles blundell. neural episodic control. icml andrei rusu sergio gomez colmenarejo caglar gulcehre guillaume desjardins james kirkpatrick razvan pascanu volodymyr mnih koray kavukcuoglu raia hadsell. policy distillation. iclr andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv. andrei rusu matej vecerik thomas roth¨orl nicolas heess razvan pascanu raia hadsell. sim-to-real robot learning pixels progressive nets. corl john schulman philipp moritz sergey levine michael jordan pieter abbeel. high-dimensional continuous control using generalized advantage estimation. iclr bobak shahriari kevin swersky ziyu wang ryan adams nando freitas. taking human loop review bayesian optimization. proc. ieee david silver lever nicolas heess thomas degris daan wierstra martin riedmiller. deterministic policy gradient algorithms. icml david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature satinder singh diane litman michael kearns marilyn walker. optimizing dialogue management reinforcement learning experiments njfun system. jair gabriel synnaeve nantas nardelli alex auvolat soumith chintala timoth´ee lacroix zeming florian richoux nicolas usunier. torchcraft library machine learning research real-time strategy games. arxiv. haoran tang rein houthooft davis foote adam stooke chen duan john schulman filip turck pieter abbeel. exploration study count-based exploration deep reinforcement learning. nips whye victor bapst wojciech marian czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu. distral robust multitask reinforcement learning. nips gerald tesauro rajarshi chan jeffrey kephart david levine freeman rawson charles lefurgy. managing power consumption performance computing systems using reinforcement learning. nips eric tzeng coline devin judy hoffman chelsea finn xingchao peng sergey levine kate saenko trevor darrell. towards adapting deep visuomotor representations simulated real environments. wafr nicolas usunier gabriel synnaeve zeming soumith chintala. episodic exploration deep deterministic policies application starcraft micromanagement tasks. iclr alexander vezhnevets volodymyr mnih simon osindero alex graves oriol vinyals john agapiou koray kavukcuoglu. strategic attentive writer learning macro-actions. nips alexander sasha vezhnevets simon osindero schaul nicolas heess jaderberg david silver koray kavukcuoglu. feudal networks hierarchical reinforcement learning. icml oriol vinyals timo ewalds sergey bartunov petko georgiev alexander sasha vezhnevets michelle alireza makhzani heinrich k¨uttler john agapiou julian schrittwieser starcraft challenge reinforcement learning. arxiv. jane wang kurth-nelson dhruva tirumala hubert soyer joel leibo r´emi munos charles blundell dharshan kumaran matt botvinick. learning reinforcement learn. cogsci ziyu wang nando freitas marc lanctot. dueling network ziyu wang victor bapst nicolas heess volodymyr mnih r´emi munos koray kavukcuoglu nando freitas. sample efﬁcient actor-critic experience replay. iclr th´eophane weber s´ebastien racani`ere david reichert lars buesing arthur guez danilo jimenez rezende adria puigdom`enech badia oriol vinyals nicolas heess yujia imaginationaugmented agents deep reinforcement learning. nips paul john werbos. beyond regression tools prediction analysis behavioral sciences. technical report harvard university applied mathematics markus wulfmeier peter ondruska ingmar posner. maximum entropy deep inverse reinforcement learning. nips workshop deep reinforcement learning kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml volume yuke roozbeh mottaghi eric kolve joseph abhinav gupta fei-fei farhadi. target-driven visual navigation indoor scenes using deep reinforcement learning. icra barret zoph quoc neural architecture search arulkumaran ph.d. candidate department bioengineering imperial college london. received b.a. computer science university cambridge m.sc. biomedical engineering imperial college london research intern twitter magic pony microsoft research research focus deep reinforcement learning transfer learning visuomotor control. marc peter deisenroth lecturer statistical machine learning department computing imperial college london prowler.io. received m.eng. computer science university karlsruhe ph.d. machine learning karlsruhe institute technology awarded imperial college research fellowship received best paper awards icra iccas recipient google faculty research award microsoft ph.d. scholarship. research centred around data-efﬁcient machine learning autonomous decision making. miles brundage ph.d. candidate human social dimensions science technology arizona state university research fellow university oxford’s future humanity institute. received b.a. political science george washington university research focuses governance issues related artiﬁcial intelligence. anil anthony bharath reader department bioengineering imperial college london fellow institution engineering technology. received b.eng. electronic electrical engineering university college london ph.d. signal processing imperial college london academic visitor signal processing group university cambridge co-founder cortexica vision systems. research interests deep architectures visual inference.", "year": 2017}