{"title": "Geometry of Polysemy", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.", "text": "jiaqi suma bhat pramod viswanath department electrical computer engineering university illinois urbana champaign urbana {jiaqimuspbhatpramodv}illinois.edu vector representations words heralded transformational approach classical problems nlp; popular example wordvec. however single vector sufﬁce model polysemous nature many words i.e. words multiple meanings. paper propose three-fold approach unsupervised polysemy modeling context representations sense induction disambiguation lexeme representations. feature work ﬁnding sentence containing target word well represented rank subspace instead point vector space. show subspaces associated particular sense target word tend intersect line disambiguate senses using clustering algorithm harnesses grassmannian geometry representations. disambiguation algorithm call k-grassmeans leads procedure label different senses target word corpus yielding lexeme vector representations unsupervised manner starting large corpus english. apart several prototypical target examples host empirical studies intuit justify various geometric representations validate algorithms standard sense induction disambiguation datasets present state-of-the-art results. distributed representations embeddings words real vector space achieved appropriate function models interaction neighboring words sentences log-bilinear models co-occurrence statistics approach strikingly successful capturing syntactic semantic similarity words simple linear algebraic relations corresponding vector representations. hand polysemous nature words i.e. phenomenon surface form representing multiple senses central feature creative process embodying natural languages. example large tall machine used moving heavy objects tall long-legged long-necked bird share surface form crane. vast majority words especially frequent ones polysemous word taking anywhere dozen different senses many natural languages. instance wordnet collects polysemous english words average senses naturally single vector embedding appropriately represent polysemous word. since hand-crafted lexical resources sometimes reﬂect actual meaning target word given context importantly resources lacking many languages focus second approach paper; approach inherently scalable potentially plausible right ideas. indeed human expects contexts particular sense speciﬁc word successful unsupervised sense representation sense extraction algorithms would represent progress broader area representation natural language. goals work. firth’s hypothesis word characterized company keeps motivated development single embeddings words also suggests multiple senses target word could inferred contexts task naturally broken three related questions represent contexts induce word senses represent lexemes vectors. existing works address questions exploring latent structure contexts. inspired work hypothesizes global word representation linear combination sense representations models contexts ﬁnite number discourse atoms recovers sense representations sparse coding vectors vocabulary works perform local context-speciﬁc sense induction introduces sense-based language model disambiguate word senses learn lexeme representations incorporating chinese restaurant process label word senses clustering contexts based average context word embeddings learn lexeme representations using labeled corpus. retains representation contexts average word vectors improves previous approach jointly learning lexeme vectors cluster centroids. grassmannian model depart linear latent models prior works presenting nonlinear geometric property contexts. empirically observe hypothesize context word representations surrounding target word reside roughly dimensional subspace. hypothesis speciﬁc sense representation target word reside subspaces contexts word means sense. note subspaces need cluster word launch sense beginning initiating endeavor could used large variety contexts. nevertheless hypothesis large semantic units reside dimensional subspaces implies subspaces contexts target word shares meaning intersect non-trivially. implies exists direction close subspaces treat intersection vector representation group subspaces. context representation deﬁne context target word left right non-functional words target word including target word itself represent low-dimensional subspace spanned context word representations; sense induction disambiguation induce word senses contexts partitioning multiple context instances groups target word sense within group. group associated representation intersection direction group found k-grassmeans novel clustering method harnesses geometry subspaces. finally disambiguate word senses context instances using respecive group representations; lexeme representation lexeme representations obtained running offthe-shelf word embedding algorithm labeled corpus. label corpus hard decisions soft decisions motivated analogous successful approaches decoding turbo ldpc codes wireless communication. experiments lexical aspect algorithm well novel geometry jointly allow capture subtle shades senses. instance hear you’re air. great moments live television isn’t representation able capture quantitative demonstration latent geometry captured methods evaluate proposed induction algorithm standard word sense induction tasks. algorithm outperforms state-of-the-art datasets semeval- task whose word senses obtained ontonotes custom-built dataset built repurposing polysemous dataset terms lexeme vector embeddings representations evaluations comparable state-of-the-art standard tasks word similarity task scws signiﬁcantly better subset scws dataset focuses polysemous target words police lineup task contexts refer entire sentences consecutive blocks words sentences surrounding target word. efﬁcient distributed vector representations sentences paragraphs active topics research literature much emphasis appropriately relating individual word embeddings sentences reside scenario contexts studied similar sense constitute long semantic units similar sentences different considering semantic units common target word residing inside them. instead straightforward application existing literature sentence vector embeddings setting deviate propose non-vector space representation; representation central results paper best motivated following simple experiment. given random word contexts principle component analysis project context word embeddings every context n-dimensional subspace measure dimensional nature context word embeddings. randomly sampled words whose occurrence larger extracted contexts wikipedia plotted histogram variance ratios captured rank-n figure make following observations even rank- captures least energy context word representations rank- capture least half energy almost surely. comparison note average number context words roughly rank- random collection words would expected capture energy word vectors trained wikipedia corpus dimension using skip-gram program wordvec experiment immediately suggests low-dimensional nature contexts contexts represented space subspaces i.e. grassmannian manifold represent context point grassmannian manifold subspace spanned principle components turn sense induction basic task explores polysemy task sentences partitioned target word used sense sentences within partition. number partitions relates number senses identiﬁed target word. geometry subset representations plays role algorithm start next. consider target word context sentence containing word empirical experiment previous section allows represent n-dimensional subspace vectors words since much smaller number words suspects representation associated wouldn’t change much target word expurgated i.e. reason hypothesis made context monosemous words case word vector representation pure polysemous words really different words surface form. empirical validation intersection hypothesis illustrate intersection phenomenon another experiment. consider monosemous word typhoon consider contexts wikipedia corpus word appears represent context rank-n subspace vectors associated words context consider intersection. subspaces dimensional cosine similarity vector associated typhoon context subspace small average standard deviation comparison randomly sample contexts average standard deviation corroborates hypothesis target word vector intersecton context subspaces. visual representation geometric phenomenon figure projected d-dimensional word representations -dimensional vectors -dimensional word vectors subspaces contexts table plot subspaces -dimensional planes. figure context subspaces roughly intersect common direction thus empirically justifying intersection hypothesis. powerful typhoon affected southern japan july sixth named storm second typhoon paciﬁc typhoon season originating area pressure near wake island july precursor maon gradually developed typhoon ineng powerful typhoon affected southern japan july sixth named storm second typhoon paciﬁc typhoon season originating area pressure near wake island july precursor crossing gulf south china patsy weakened tropical storm joint typhoon warning center ceased following system morning august made landfall near city completely wiped airplanes local airports damaged ﬁrst active paciﬁc typhoon season record typhoon barbara formed march moved west strengthened brieﬂy category three recovering intersection direction algorithmic approach robustly discover intersection direction involves ﬁnding direction vector closest subspaces; propose solving following optimization problem solved taking ﬁrst principle component {un}w∈cn=...n property context subspaces monosemous word intersect direction naturally generalizes polysemy polysemous word crane either mean large tall machine used moving heavily objects tall long-legged long-necked bird. list four contexts sense crane table repeat experiment conducted monosemous word typhoon visualize context subspaces senses figure respectively. figure plots direction intersections. immediately suggests contexts crane stands bird intersect direction contexts crane stands machine intersect different direction visualized dimensions. figure geometry contexts polysemous word crane contexts crane means machine roughly intersect direction; contexts crane means bird roughly intersect another direction; directions representing crane machine bird. june anchor towed ashore lifted mobile crane tank made concrete built ground speciﬁcally purpose conserving anchor. company deck covered lighters stick lighters steam cranes heavy lift crane barges providing single agency delaware valley shippers. claimed hydraulic crane could unload ships faster cheaply conventional cranes. large pier built harbour accommodate heavy lift marine crane would carry components northumberland strait installed. blue crane also known stanley crane paradise crane national bird south africa. sarus crane easily distinguished cranes region overall grey colour contrasting head representation senses intersection directions context subspaces unsupervised sense induction supposing target polysemous word senses goal partition contexts associated target word groups within target polysemous word shares sense. fact groups context subspaces corresponding different senses intersect different directions motivates geometric algorithm note contexts belongs group associated nearest intersection direction serves prototype group. part task also identify appropriate intersection direction vectors associated group. task represents form unsupervised clustering formalized optimization problem below. given target polysemous word contexts containing number indicating number senses would like partition contexts sets problem analogous objective k-means clustering vectors solving exactly worst case shown np-hard. propose natural algorithm repurposing traditional k-means clustering built vector spaces grassmannian space follows algorithm k-grassmeans ensure good performance randomize intersection directions multiple different seeds output best terms objective function step analogous random initialization conducted kmeans++ classical clustering literature qualitative feel algorithm work consider exemplar target word columbia senses. considered sentences extracted wikipedia corpus. goal sense induction partition contexts groups within group target word columbia sense. k-grassmeans target word extract intersect vectors sample sentence group given table example ﬁrst group corresponds british columbia canada second corresponds columbia records third corresponds columbia university york fourth corresponds district columbia ﬁfth corresponds columbia river. performance k-grassmeans context target word columbia described detail context sense disambiguation note algorithm speciﬁc target word makes efﬁcient online sense induction; relevant information retrieval applications sense query words need found real time. feel good k-grassmeans sense induction task following synthetic experiment randomly pick monosemous words merge surface forms create single artiﬁcial polsemous word collect contexts corresponding monosemous words replace every occurrence monosemous words single artiﬁcial polysemous word. k-grassmeans algorithm contexts artiﬁcial polysemous word target word recover original labels figure shows clustering performances realization artiﬁcial polysemous word made monastery figure )shows clustering performance monosemous words employers exiled grossed incredible unreleased merged together. repeat experiment trials varying accuracy sense induction reported figure experiments k-grassmeans performs well qualitatively quantitatively. quantitative experiment large standardized real dataset well comparison algorithms literature detailed section k-grassmeans outperforms state art. contexts research centres canada located patricia former british columbia highway sidney british columbia vancouver island west victoria international airport institute paired canadian coast guard base break performing arthur godfrey show since released series successful singles columbia records hechtlancaster productions ﬁrst published music marty april june cromwell music fellow merrill school journalism university maryland college park visiting scholar columbia university center study human rights haddad completed master international policy signed president benjamin harrison march site national conservatory district columbia never selected much less built school continued function york city existing solely philanthropy cowlitz county washington caples community located west woodland along caples road east shore columbia river across river columbia city oregon caples community part woodland school district figure synthetic experiment study performances k-grassmeans. monosemous words monastery phd; monosemous words employers exiled grossed incredible unreleased; accuracy versus intersection directions represent senses ready disambiguate target word sense given context using learned intersection directions speciﬁc target word context instance polysemous word goal identify sense word means context. approach three-fold represent context dimensional subspace approximation linear span word embeddings non-functional words context orthogonal projection distance intersection vector context subspace ﬁnally output minimizes distance i.e. refer hard decoding word senses since outputs deterministic label. times makes sense consider soft decoding algorithm output probability distribution. probability takes k-th sense given context deﬁned applied target word columbia sentences listed table probability distributions returned soft decoding algorithm optimal k∗’s returned hard decoding algorithm provided table table even though hard decoding algorithm outputs correct label information missing return single label example since take bag-of-words model k-grassmean words provided table suggest meaning columbia instance might also columbia university. function words reﬂects probability distribution returned soft decoding algorithm probability columbia instance means columbia university around misleading result mainly comes bag-of-words model resolve remains open. induction disambiguation important tasks themselves several downstream applicatons distributed vector representation multiple senses associated target word. word representations expect distributed lexeme representations semantic meanings similar lexemes represented similar vectors. might seem natural represent lexeme given word intersection vector associated k-th sense group i.e. idea supported observation intersection vector close word representation vector many monosemous word. perform another experiment directly conﬁrm observation randomly sample monosemous words occur least times word compute intersection vector check cosine similarity intersection vector corresponding word representation monosemous words. average cosine similarity score high small standard deviation despite empirical evidence somewhat surprisingly lexeme representation using intersection vectors turns good idea reason fairly subtle. turns intersection vectors concentrated relatively small surface area sphere cosine similarity random intersection vectors among intersection vectors average standard deviation quite contrast analogous statistics word embeddings wordvec algorithm cosine similarity random word vectors average standard deviation indeed word vector representations known approximately uniformly scattered unit sphere intersection vectors cluster together quite isotropic still able distinguish different senses shown empirical studies qualitative experiments prototypical examples geometric mismatch word vectors intersection directions corresponding mismatch linear algebraic properties expected distributed representations appropriate intersection direction lexeme vector representation. word representations isotropic intersection vectors cluster close intriguing open question detailed empirical study phenomenon theoretical exploration generative models might mathematically explain behavior exciting future directions research beyond scope paper. addition geometric mismatch intersection vectors perhaps appropriate represent lexemes reasons intersections represent directions lack information magnitudes; context subspaces noisy since initial phase polysemous word represented single vector. light propose learn lexeme representations alternate procedure ﬁrst label polysemous words corpus using proposed disambiguation algorithm section standard word embedding algorithm labeled corpus yielding lexeme embeddings. several possibilities regarding labeling discussed next. label corpus using disambiguation algorithm special label representing don’t know introduced avoid introducing many errors labeling phase since approach based bag-of-words model cannot guarantee label every sense correctly; clear errors affect existing word embedding algorithms. label introduced checking closest distance context subspace intersection directions i.e. closest intersection vector context label instance otherwise hyperparameter. detailed algorithm chart sense disambiguation corpus labeling provided appendix label includes instances words means rare sense confusing sense requires disambiguation context words labeling procedure inspired analogous scenarios wireless communication likelihood ratio bits close zero practice better labeled erasures treating informative overall decoding task another labeling using absolute scores k-grassmeans disambiguation sense target work speciﬁc context equation soft decoding involves generating random corpus sampling sense every occurrence polysemous word according probability distribution lexeme representations obtained application standard word embedding algorithm labeled corpus. since consider words frequent enough sense polysemous word sampled enough times allow robust lexeme representation high probability. soft decoding beneﬁts scenarios context enough information disambiguation concentrates one) random sampling high chance making correct decision. context ambiguous random sampling chance making wrong decision. throughout paper conducted multiple qualitative empirical experiments highlight motivate various geometric representations. section evaluate algorithms empirically datasets literature allowing quantitative feel performance large datasets well afford comparison algorithms literature. algorithms unsupervised operate large corpus obtained wikipedia dated wikiextractor extract plain text. skip-gram model wordvec word embedding algorithm default parameter setting. context window size rank pca. choose experiment. disambiguation algorithm main comparisons algorithms conduct unsupervised polysemy disambiguation speciﬁcally sense clustering method multi-sense skip gram model different parameters sparse coding method global dictionary able download word sense representations online trained word sense representations corpus used algorithms. word sense induction tasks conduct following test given context instances containing target word asked partition context instances groups within group target word shares sense. test induction algorithm k-grassmeans datasets standard custom-built. semeval- test semeval- shared task contains polysemous nouns polysemous verbs whose senses extracted ontonotes total instances expected disambiguated. context instances extracted various sources including abc. makes-sense- several word senses semeval- ﬁne-grained view creates noise reduces performance algorithms required senses perhaps useful downstream applications. example guarantee labeled four different meanings following four sentences provided legal guarantee development china’s cross cause connections international cross movement signifying china’s cross cause entered historical phase. clearer. repurposing recent dataset created part police lineup task. dataset contains polysemous words together senses borrowed generate testing instances target word extracting occurrences wikipedia corpus analyzing wikipedia annotations grouping annotations ﬁnally merging annotations target word shares sense. since senses quite readily distinguishable perspective native/ﬂuent speakers english disambiguation variability among human raters tested dataset negligible evaluate performance algorithms task according standard measures literature v-measure paired f-score evaluation metrics also feature semeval- task v-measure entropy-based external cluster evaluation metric. paired f-score evaluates clustering performance converting clustering problem binary classiﬁcation problem given instances belong cluster not? metrics operate contingency table {atk} number instances manually labeled algorithmically labeled detailed description given appendix completeness. metrics range perfect clustering gives score empirical statistics show v-measure favors larger number cluster paired f-score favors smaller number cluster. table shows detailed results experiments k-grassmeans strongly outperforms others. main reason behind better performance seems k-grassmeans disambiguates subtle senses others cannot. example following three sentences containing observed enough information contained sentence inform ﬁrst broadcasting second region ground third mixture gases. k-grassmeans distinguish three algorithms cannot. requirement lexeme representations properties word embeddings i.e. similar lexemes represented similar vectors. hence evaluate lexeme representations standard word similarity task focusing context-speciﬁc scenarios stanford contextual word similarities dataset addition word similarity task scws also evaluate lexeme representations police lineup task proposed word similarity scws task follows given pair target words algorithm assigns measure similarities pair words. algorithm evaluated checking degree agreement similarity measure given algorithm similarity measure given humans terms spearman’s rank correlation coefﬁcient. although scws dataset meant speciﬁcally polysemy repurpose tasks since asks similarity words given sentential contexts also large standard dataset literature human rated similarity scores rated integral scale take average human scores represent human judgment. propose measures given respective contexts based hard decoding algorithm based soft one. hardsim softsim deﬁned table shows detailed results task. conclude general lexeme representations similar performance state-of-the-art soft hard decisions. worth mentioning vanilla wordvec representation also comparable performance makes sense since words scws monosemous closer inspection results shows vanilla wordvec representation also performs fairly well polysemous words hypothesize wordvec global representation actualy close individual sense representations senses occur frequently enough corpus. hypothesis supported linear algebraic structure uncovered αkvk word semantically close k-th sense know vtv) αkvkv). since senses irrelevant assume vkv) k-th sense frequent enough assume putting together conclude vtv) vktv) therefore inner product captures similarity k-th sense separate effect combination monosemous polysemous words scws dataset expurgate monosemous words corpus creating smaller version denoted scws-lite. scws-lite consider pairs sentences target words pair surface form different contexts scws-lite contains pairs words roughly original dataset. performances various algorithms detailed table representations outperform state-of-the-art showcases superior representational performance comes context-speciﬁc polysemy disambiguation. police lineup task task proposed evaluate efﬁcacy sense representations testbed contains polysemous words senses sense deﬁned eight related words. example tool/weapon sense axes represented handle harvest cutting split tool wood battle chop. task following given polysemous word algorithm needs identify true senses word group senses outputting senses group. algorithm evaluated corresponding precision recall scores. task offers another opportunity test representations others literature also provide insight representations themselves. possible algorithm simply proposed replace sense representations ours denote sense denote words representing sense. deﬁne similarity figure shows precision recall curve polysemy test vary first observe representations uniformly better precisionrecall curve state although relatively small margin. soft decoding performs slightly better hard decoding task. second surprising ﬁnding baseline create using vanilla wordvec representations performs well state-of-the-art described careful look shows algorithm outputs highly correlated make correct calls obvious instances make mistakes confusing instances. believe following algorithm highly correlated wordvec since overall similarity measure uses linear combination similarity measures associated sense vector word vector wordvec embeddings appear ability capture different senses polysemous word instances errors occur seem either genuinely subtle rare domain embeddings trained main approaches model polysemy supervised uses linguistic resoures unsupervised inferring senses directly large text corpus approach belongs latter category. differing approaches harness hand-crafted lexical resources leverages gloss deﬁnition lexeme uses model disambiguate word senses. models sense lexeme representations ontology wordnet. approaches natural interesting inherently limited coverage wordnet wordnet covers polysemous words senses polysemous words complete domain-agnostic ﬁne-grained nature wordnet wordnet senses appear times pedantic useful practical downstream applications unsupervised methods suffer idiosyncracies linguistic resources inherently challenging pursue since rely latent structures word senses embedded inside contexts. existing unsupervised approaches divided categories based aspects contexts target words used global structures contexts local structures contexts. global structure hypothesizes global word representation linear combination sense vectors. linear algebraic hypothesis validated surprising experiment wherein single artiﬁcial polysemous word created merging random words. experiment ingenious ﬁnding quite surprising restricted setting single artiﬁcial polysemous word created merging random words. upon enlargening parameters better suit landscape polysemy natural language linear-algebraic hypothesis fragile figure plots linearity function number artiﬁcial polysemous words created also function many words merged create polysemous word. linearity worsens fairly quickly number polysemous words increases scenario typical natural languages. main reason effect appears linearity quite sensitive interaction word vectors caused polysemous nature words. linear algebraic hypothesis mathematically justiﬁed section terms rand-walk generative model three extra simpliﬁcations. generalize proof handle multiple artiﬁcial words time appears particularly relevant simpliﬁcation continue hold. simpliﬁcation step involved assumption random words merged occur together context window word occur single wi’s context windows. simpliﬁcation step clearly longer holds increases especially nears size vocabulary. however latter scenario basis sparse-coding algorithm proposed latent structure multiple senses modeled corpus discourse atoms every atom interacts others. experiment whose results depicted figure designed mimic underlying simpliﬁcations proof train word vectors skip-gram version wordvec using following steps. initialize newly generated artiﬁcial polysemous words random vectors; initialize update vector representations words existing word vectors. embeddings learnt wikipedia dump tokenized wikiextractor words occur less times ignored; words merged chosen randomly proportion frequencies. computational constraints instance mergers subjected single wordvec algorithm. local structure model context average constituent word embeddings average vector feature induce word senses partitioning context instances groups disambiguate word senses context instances. models senses target word given context chinese restaurant process models contexts also averaging constituent word embeddings applies standard word embedding algorithms skip-gram). approach broadly similar spirit approaches local lexical-level model conceived depart several ways prominent modeling contexts subspaces paper study geometry contexts polysemy propose three-fold approach model target polysemous words unsupervised fashion represent context rank subspace induce word senses clustering subspaces terms distance intersection vector representing lexeme labeling corpus. representations novel involve nonlinear geometry subspaces clustering algorithms designed harness speciﬁc geometry. overall performance method evaluated quantitatively standardized word sense induction word similarity tasks present state-of-the-art results. several avenues research natural language representations arise ideas work discuss items detail below. interactions polysemous words ﬁndings work experiments section polysemous words interact corpus. natural harness intersections hence sharpen k-grassmeans iterative labeling procedure. hard decoding soft decoding beneﬁt iterations. hard decoding labels resolved multiple iterations since rare senses become dominant senses major senses already labeled confusing sense disambiguated polysemous words context disambiguated. soft decoding probability expected concentrate sense since iteration yields precise context word embeddings. hypothesis inspired success procedures inherent message passing algorithms turbo ldpc codes reliable wireless communication share fair commonality setting polysemy disambiguation quantitative tool measure disambiguation improvements iterations stop iterative process interesting research direction detailed algorithm design iterative decoding implementation; beyond scope paper. dimensional context representation surprising ﬁnding work contexts contain common target word tend reside dimensional subspace justiﬁed empirical observations figure understanding geometrical phenomenon context generative model able explain this) basic problem interest several relevant applications semantic parsing sentences understanding could also provide ideas topical subject representing sentences paragraphs eventually combining document/topic modeling methods latent dirichlet allocation combining linguistic resources presently methods lexeme representation either exclusive external resource-based entirely unsupervised. unsupervised method k-grassmeans reveals robust clusters senses identiﬁed sense). hand wordnet lists detailed number senses frequent robust many others grained; lack accompanying metric relates frequency robustness sense really makes resource hard make computational least within context polysemy representations. interesting research direction would combine k-grassmeans existing linguistic resources automatically deﬁne senses multiple granularities along metrics relating frequency robustness identiﬁed senses. sanjeev arora yuanzhi yingyu liang tengyu andrej risteski. latent variable model approach pmi-based word embeddings. transactions association computational linguistics javier artiles enrique amig´o julio gonzalo. role named entities people search. proceedings conference empirical methods natural language processing emnlp august singapore meeting sigdat special interest group pages bar-haim dagan bill dolan lisa ferro danilo giampiccolo bernardo magnini idan szpektor. second pascal recognising textual entailment challenge. proceedings second pascal challenges workshop recognising textual entailment volume pages xinxiong chen zhiyuan maosong sun. uniﬁed model word sense representation disambiguation. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages asaf cidon kanthi nagaraj sachin katti pramod viswanath. flashback decoupled lightweight wireless control. sigcomm conference sigcomm helsinki finland august pages dagan oren glickman bernardo magnini. pascal recognising textual entailment challenge. machine learning challenges evaluating predictive uncertainty visual object classiﬁcation recognizing textual entailment first pascal machine learning challenges workshop mlcw southampton april revised selected papers pages firth. synopsis linguistic theory studies linguistic analysis. philological society oxford reprinted palmer selected papers firth longman harlow. danilo giampiccolo bernardo magnini dagan bill dolan. third pascal recognizing textual entailment challenge. proceedings acl-pascal workshop textual entailment paraphrasing pages association computational linguistics eric huang richard socher christopher manning andrew improving word representations global context multiple word prototypes. annual meeting association computational linguistics proceedings conference july jeju island korea volume long papers pages rukmini iyer mari ostendorf robin rohlicek. language modeling sentencelevel mixtures. human language technology proceedings workshop held plainsboro jerey march kenter alexey borisov maarten rijke. siamese cbow optimizing word embeddings sentence representations. proceedings annual meeting association computational linguistics august berlin germany volume long papers yoon kim. convolutional neural networks sentence classiﬁcation. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages quoc tomas mikolov. distributed representations sentences documents. proceedings international conference machine learning icml beijing china june pages omer levy yoav goldberg. neural word embedding implicit matrix factorization. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages jiwei jurafsky. multi-sense embeddings improve natural language understandproceedings conference empirical methods natural language ing? processing emnlp lisbon portugal september pages suresh manandhar ioannis klapaftis dmitriy dligach sameer pradhan. semeval task word sense induction &disambiguation. proceedings international workshop semantic evaluation semevalacl uppsala university uppsala sweden july pages tomas mikolov martin karaﬁ´at luk´as burget cernock´y sanjeev khudanpur. recurrent neural network based language model. interspeech annual conference international speech communication association makuhari chiba japan september pages andriy mnih geoffrey hinton. three graphical models statistical language modelling. machine learning proceedings twenty-fourth international conference corvallis oregon june pages arvind neelakantan jeevan shankar alexandre passos andrew mccallum. efﬁcient non-parametric estimation multiple embeddings word vector space. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group pages jeffrey pennington richard socher christopher manning. glove global vectors proceedings conference empirical methods word representation. natural language processing emnlp october doha qatar meeting sigdat special interest group pages sameer pradhan nianwen xue. ontonotes solution. human language technologies conference north american chapter association computational linguistics proceedings june boulder colorado tutorial abstracts pages joseph reisinger raymond mooney. multi-prototype vector-space models word human language technologies conference north american chapter meaning. association computational linguistics proceedings june angeles california pages andrew rosenberg julia hirschberg. v-measure conditional entropy-based external emnlp-conll proceedings joint concluster evaluation measure. ference empirical methods natural language processing computational natural language learning june prague czech republic pages sascha rothe hinrich sch¨utze. autoextend extending word embeddings embeddings synsets lexemes. proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages sheng richard socher christopher manning. improved semantic represenproceedings tations tree-structured long short-term memory networks. annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing july beijing china volume long papers pages v-measure paired f-score clustering algorithms partition data points clusters given ground truth i.e. another partition data another clusters performance clustering algorithm evaluated based contingency table representing clustering algorithm number data whose ground truth label algorithm label intrinsic properties desirable evaluation measures algorithm similarity score thought mean value word similarities target word deﬁnition word given sense take power mean algorithm adapted different choice i.e. algorithm algorithm take account atom represents sense target word thus atoms might generate senses output candidates. sophisticated algorithm required address issue.", "year": 2016}