{"title": "Paranom: A Parallel Anomaly Dataset Generator", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we present Paranom, a parallel anomaly dataset generator. We discuss its design and provide brief experimental results demonstrating its usefulness in improving the classification correctness of LSTM-AD, a state-of-the-art anomaly detection model.", "text": "help manage this paranom provides uniqueness controls data generation hard soft. define hard uniqueness data property must met. hard uniqueness requested user-specified number tries paranom unsuccessfully generated unique datum paranom terminate execution. define soft uniqueness data property might user-specified number tries paranom continue execution using last generated datum entry. anomalous data scarce duplication already small number data points result model overfitting address this paranom provides data uniqueness controls normal data anomalous data. enables user generate soft unique non-anomalous data hard unique anomalous data vice-versa needed. data generation paranom supports following ways data generation. stochastic variables. variables support controlled randomization developers define specified range stochasticism anomalous non-anomalous values generate. paranom handles value generation. callback variables. developer requires full control value generation define callback variable requires construction callback functions variable anomalous data generation non-anomalous. runtime appropriate callback function invoked value generation variable unique discrete timestamp. abstract paper present paranom parallel anomaly dataset generator. discuss design provide brief experimental results demonstrating usefulness improving classification correctness lstm-ad state-of-the-art anomaly detection model. dataset collection data usually manipulated single unit necessary many machine learning techniques context deep learning shown larger richer dataset greater potential accuracy model built this possessing large high-quality dataset usually first step building model. contrarily practical development models usually requires various sizes data beginning items initial model construction billions items model deployment. early development small rich datasets useful enable rapid model tuning topological changes without suffering significant performance penalties practice means many models generated iteratively training re-training growing dataset. manipulation performed manually increases engineering overhead potential data manipulation errors. anomaly detection process identifying outliers specific domain adds even complexity dataset problem least reasons. first anomalies definition infrequent therefore building accurate anomaly detection model challenging scarcity anomalous data second anomalies tend continuous events means data presented must usually periodic time-series ordered form reasons growing importance streaming systems building large rich meaningful datasets anomaly detection open increasingly important problem brief overview paranom’s technical design. illustration paranom’s synthetic data used lstm-ad state-of-the-art anomaly detection model improving accuracy using real data. data uniqueness synthetic data generation data uniqueness cannot always guaranteed. illustrate this consider example user requests unique anomalous data providing possible discrete values data stochasticism paranom provides seedable randomization ensure repeatable stochasticism dataset generation. useful iterative training small large datasets previously seen data guaranteed remain unchanged dataset grows size. paranom also provides controls anomalous non-anomalous data randomly present dataset. addition providing controls absolute number data points paranom also provides controls stochastic frequency anomalous non-anomalous data. parallel run-time execution ameliorate performance overhead possibly generating billions entries dataset paranom designed goal perfectly parallel multithreaded synchronization used generation data although paranom’s data generation perfectly parallel user level synchronization used needed within user-defined callback variables. useful generating unique data sequentially dependent time-series data among things. experimental evaluation section describe used paranom improve accuracy lstm-ad state-of-the-art anomaly detection model malhotra compared using real data space shuttle valve sensor anomaly experimental setup. trained lstm-ad models tensorflow used real space shuttle data. used portions real non-anomalous data conjunction anomalous data solely generated paranomas described below. similarities. models constrained training data size tested real test data. aspects lstm-ad model well training iterations identical settings. used paranom generate data dozen different anomalous scenarios. however limited space include example malhotra al.’s lstm-ad paper paranom also used generate non-anomalous data useful zero-positive learning differences. model trained using paranom’s generated training data real-world anomalous training data. instead created paranom callback variable would generate anomalous data uniquely different real-world nonanomalous data stochastically chosen value range. paranom inject synthetic anomalies frequency variable anomaly duration range similar real anomalous events. results. differences original training data paranom generated data seen figures respectively. figure shows original data used train lstm-ad including real anomalies also shows lstm-ad predictions test data trained original data. figure shows paranom generated training data includes paranom’s synthetically generated stochastic anomalies. trained real training data paranom’s training data tested lstm-ad models original testing data. original model identified anomalies. paranom model identified five anomalies. seen figure paranom lstm-ad model improved accuracy recall score compared model trained real data. lstm-ad result improved precision. paranom lstm-ad model introduced false positives. conclusion paper briefly presented paranom’s design parallel execution model. provided empirical illustration showing benefit using paranom’s synthetically created data improve robustness lstm-ad state-of-the-art anomaly detection model order magnitude recall using paranom’s data using real data. martin abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard manjunath kudlur josh levenberg rajat monga sherry moore derek murray benoit steiner paul tucker vijay vasudevan pete warden martin wicke yuan xiaoqiang zheng. tensorflow system large-scale machine learning. usenix symposium operating systems design implementation aggarwal. outlier analysis. springer. mohammad mejbah alam justin gottschlich abdullah muzahid. autoperf generalized zero-positive learning system detect software performance anomalies. technical report. http//arxiv.org/abs/./ lavin ahmad. evaluating real-time anomaly detection algorithms numenta anomaly benchmark. ieee international conference machine learning applications malhotra shroff agarwal. long short term memory networks anomaly detection time series. european symposium artificial neural networks computational intelligence machine learning", "year": 2018}