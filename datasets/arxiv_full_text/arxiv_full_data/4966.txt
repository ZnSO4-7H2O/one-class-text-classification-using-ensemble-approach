{"title": "Generalized Value Iteration Networks: Life Beyond Lattices", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value iteration algorithm by using a novel graph convolution operator, which enables GVIN to learn and plan on irregular spatial graphs. We propose three novel differentiable kernels as graph convolution operators and show that the embedding based kernel achieves the best performance. We further propose episodic Q-learning, an improvement upon traditional n-step Q-learning that stabilizes training for networks that contain a planning module. Lastly, we evaluate GVIN on planning problems in 2D mazes, irregular graphs, and real-world street networks, showing that GVIN generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms a naive generalization of VIN (discretizing a spatial graph into a 2D image).", "text": "recent work value iteration networks combines recurrent convolutional neural networks max-pooling emulate process value iteration learns environment plan shortest paths unseen mazes. input data deep learning systems usually associated regular structures. example speech signals natural language underlying sequential structure; images underlying lattice structure. take advantage regularly structured data deep learning uses series basic operations deﬁned regular domain convolution uniform pooling. however data contained regular structures. urban science trafﬁc information associated road networks; neuroscience brain activity associated brain connectivity networks; social sciences users’ proﬁle information associated social networks. learn data irregular structure recent works extended lattice structure general graphs redeﬁned convolution pooling operations graphs; however works evaluate data ﬁxed given graph. addition lack ability generalize unseen environments. paper enable agent self-learn plan optimal path unseen spatial graphs using model-based graph-based techniques. task relevant many real-world applications route planning self-driving cars crawling/navigation. proposed method general classical extending irregular structures. furthermore proposed method scalable handles various edge weight settings adaptively learns environment model. note optimal path self-deﬁned necessarily shortest one. additionally proposed work differs conventional planning algorithms; example dijkstra’s algorithm requires known model gvin aims learn general model trial error apply said model unseen irregular graphs. create gvin generalize aspects. first work irregular graphs propose graph convolution operator generalizes original convolution operator. graph convolution operator proposed paper introduce generalized value iteration network end-to-end neural network planning module. gvin emulates value iteration algorithm using novel graph convolution operator enables gvin learn plan irregular spatial graphs. propose three novel differentiable kernels graph convolution operators show embedding-based kernel achieves best performance. furthermore present episodic q-learning improvement upon traditional n-step q-learning stabilizes training gvin. lastly evaluate gvin planning problems mazes irregular graphs realworld street networks showing gvin generalizes well arbitrary graphs unseen graphs larger scale outperforms naive generalization reinforcement learning technique solves sequential decision making problems lacks explicit rules labels recent developments deep reinforcement learning lead enormous progress autonomous driving innovation robot control humanlevel performance atari games board game given reinforcement learning task agent explores underlying markov decision process attempts learn mapping high-dimensional state space data optimal policy maximizes expected return. reinforcement learning categorized model-free model-based approaches model-free approaches learn policy directly trial-and-error attempt avoid bias caused suboptimal environment model majority recent architectures follow model-free approach model-based approaches hand allow agent explicitly learn mechanisms environment lead strong generalization abilities. network captures basic concepts spatial graphs direction distance edge weight. also able transfer knowledge learned graph others. second improve reinforcement learning irregular graphs propose reinforcement learning algorithm episodic q-learning stabilizes training gvin. original trained either imitation learning requires large number ground-truth labels reinforcement learning whose performance relatively poor. proposed episodic q-learning network performs signiﬁcantly better reinforcement learning mode. since proposed network generalizes original model call generalized value iteration network proposed architecture gvin generalizes handle regular structures irregular structures. gvin offers end-to-end architecture trained reinforcement learning section framework; proposed graph convolution operator generalizes convolution learns concepts direction distance enables gvin transfer knowledge graph another; section graph convolution; proposed reinforcement learning algorithm episodic q-learning extends classical n-step q-learning monte carlo control signiﬁcantly improves performance reinforcement learning irregular graphs; section training reinforcement learning; intensive experiments demonstrate generalization ability gvin within imitation learning episodic q-learning various datasets including synthetic maze data irregular graphs real-world maps show gvin signiﬁcantly outperforms discretization input irregular structures; section experimental results. markov decision process. consider environment deﬁned contains states actions reward function series transition probabilities pssa probability moving current state next state given action goal policy maximizes γkrt+k rt+k immediate reward time stamp discount rate. policy probability taking action state value state policy expected return starting following value taking action state policy expected return starting taking action following least policy better equal policies called optimal policy optimal policy maxπ optimal state-value function optimal action-value function obtain usually consider solving bellman equation. value iteration differentiable planning module. employs embedded differentiable planning architecture trained end-toend imitation learning bellman equation encoded within convolutional neural networks policy obtained backpropagation. however limited regular lattices; requires imitation learning maximum performance trained separately reactive policy. recent work memory augmented control network combines model memory augmented controller backtrack history previous trajectories. however shown later table gvin outperform macn performance problem scales. different model-based work predictron uses learning planning model simulates markov reward process architecture unrolls \"imagined\" plan predictron core. however predictron limited markov rewards process relatively computationally expensive compared vin. deep learning graphs. number recent works consider using neural networks handle signals supported graphs principal idea generalize basic operations regular domain ﬁltering pooling graph domain based spectral graph theory. example introduce hierarchical clustering graphs spectrum graph laplacian neural networks; generalizes classical convolutional neural networks using graph coarsening localized convolutional graph ﬁltering; considers semi-supervised learning graphs using graph-based convolutional neural networks; investigate learning graph structure gated recurrent unit; considers message passing framework uniﬁes previous work recent overviews propose model-based framework gvin takes general graph starting node goal node inputs outputs designed plan. goal gvin learn underlying summarizes optimal planning policy applied arbitrary graphs requires gvin capture general knowledge planning structure transition invariant depend speciﬁc graph structure. component transition matrix needed solve bellman equation. train general transition matrix works arbitrary graphs similar treat graph convolution operator parameterize using graphbased kernel functions represents unique action pattern. train parameters gvin using episodic q-learning makes reinforcement learning irregular graphs practical. figure architecture gvin. left module emulates value iteration obtains state values; right module responsible selecting action based \u0001-greedy policy greed policy emphasize contributions including graph convolution operator episodic q-learning blue blocks. input gvin graph starting node goal node. training phase gvin trains parameters trial-and-error various graphs; testing phase gvin plans optimal path based trained parameters. framework includes planning module action module shown figure planning module emulates value iteration iteratively operating graph convolution max-pooling. action module takes greedy action according value function. mathematically consider directed weighted spatial graph node node embeddings embedding node edge rn×n adjacency matrix element representing edge weight nodes. consider graph signal mapping nodes real values. graph signal encode goal node one-sparse activates goal node. reward graph signal state-value graph signal action-value graph signal respectively. represent entire process matrixvector form follows feature-extraction step encoded become robust reward feature-extract function convolutional neural network case regular graphs identity function operating irregular graphs; step graph convolution operator channel graph convolution operators trained based graph described section graph convolution; value iteration emulated using graph convolution obtain action-value graph signal channel max-pooling obtain state-value graph signal training parameters parameterize respectively. shown figure repeat graph convolution operation max-pooling iterations obtain ﬁnal state-value graph signalv. signal action module. original extracts node varies. solve this consider convertingv pseudo action-value graph signalq whose element isqs maxs∈neivs representing action value moving neighbors. advantages approach come following three aspects ﬁnal state value node obtained using maximum action values across channels robust small variations; pseudo action-value graph signal considers unique action node depend number actions; node agent queries state values neighbors always moves highest value; pseudo action-value graph signal considers local graph structure next state always chosen neighbors current state. pseudo action-value graph signal used episodic q-learning learns trial-and-error experience backpropagates update training parameters. episodic q-learning episode obtained follows given starting node agent move sequentially \u0001-greedy strategy; deﬁned discount factor immediate return time stamp additional details algorithm discussed section training reinforcement learning. testing phase obtain action greedily selecting maximal state value; conventional takes image input lattice graph. node pixel local structure sitting grid connecting eight neighbors. case convolution operator easy obtain. irregular graphs however nodes form diverse local structures making challenging obtain structured translation invariant operator transfers knowledge graph another. fundamental problem convolution operator works arbitrary local structures. solve learning spatial kernel function provides transition probability distribution space according evaluate weight edge obtain graph convolution operator. spatial kernel function assigns value position space reﬂects possibility transit corresponding position. mathematically transition probability starting position another position spatial kernel function speciﬁed later. deﬁnition spatial kernel function shift invariant satisﬁes shift invariance requires transition probability depend relative position transfer learning; words matter starting position transition probability distribution invariant. based shift-invariant spatial kernel function graph adjacency matrix obtain graph convolution operator rn×n element ·kwp kernel function parameterized embeddings node. graph convolution operator follows graph connectivity spatial kernel function. shift-invariant property spatial kernel function leads local transition distribution node; graph adjacency matrix works modulator select activations graph convolution operator. edge edge high high; words transition probability node node higher edge weight high inﬂuence node node bigger graph convolution. note sparse matrix sparsity pattern corresponding adjacency matrix ensures cheap computation. shown graph convolution matrix-vector multiplication graph convolution operator graph signal γvn; figure note work lattice graph appropriate kernel function graph convolution operator nothing matrix representation conventional convolution words special case gvin underlying graph lattice; details supplementary graph-based kernel functions. kernel coefﬁcient direction edge connecting thejth nodes computed node embeddings directional kernel order reference direction reﬂecting center activation. hyperparameters include number directional kernels order reﬂecting directional resolution figure kernel coefﬁcient reference direction training parameters distance nodes computed node embeddings spatial kernel reference distance reference direction indicator function i|d−d|≤\u0001 otherwise. hyperparameters include number directional kernels order reference distance distance threshold kernel coefﬁcient reference direction training parameters embedding-based kernel. directional kernel spatial kernel manually design kernel provide hints gvin learn useful direction-distance patterns. directly feed node embeddings allow gvin automatically learn implicit hidden factors general planning. element graph convolution operator then ·kemb indicator function ii=j otherwise embedding-based kernel function kemb mnnet mnnet standard multi-layer neural network. training parameters weights multi-layer neural network. practice graph weighted also include graph adjacency matrix input multilayer neural network. theorem proposed three kernel functions directional kernel spatial kernel embedding-based kernel shift invariant. train gvin episodic q-learning modiﬁed version n-step q-learning. difference episodic q-learning n-step q-learning nstep q-learning ﬁxed episode duration updates training weights steps; episodic q-learning episodic terminates agent reaches goal maximum step threshold reached update trainable weights entire episode. experiments found regular irregular graphs policy planned original q-learning keeps changing converge frequent updates. similar monte carlo algorithms episodic q-learning ﬁrst selects actions using exploration policy goal reached. afterwards accumulate gradients entire episode update trainable weights allowing agent stable plan complete entire episode. simple change greatly improves performance pseudocode algorithm presented algorithm section evaluate proposed method three types graphs mazes synthesized irregular graphs real road networks. ﬁrst validate proposed gvin comparable original mazes regular lattice structure. next show proposed gvin automatically learns concepts direction distance synthesized irregular graphs reinforcement learning setting finally pre-trained gvin model plan given starting point goal location consider planning shortest paths mazes; figure example. generate mazes using scripts used. conﬁguration consider four comparisons gvin action-value based imitating learning statevalue based imitating learning direction-guided gvin unguided gvin reinforcement learning. four metrics used quantify planning performance including prediction accuracy—the probability taking ground-truth action state success rate—the probability successfully arriving goal start state without hitting obstacles path difference—the average length difference predicted path ground-truth path expected reward—the average accumulated reward overall testing results summarized table gvin. gvin performs competitively especially gvin uses direction-aware actionvalue based imitation learning outperforms others four metrics. figure shows value learned gvin direction-unaware state-value based imitation learning. negative values obstacles positive values around goal similar value reported action-value state-value. action-value imitation learning slightly outperforms state-value imitation learning. similarly gvin action-value based imitation learning slightly outperforms gvin state-value based imitation learning. results suggest action approximation method impact performance maintaining ability extended irregular graphs. direction-aware gvin unaware gvin. directionaware gvin slightly outperforms direction-unaware gvin reasonable ﬁxed eight directions ground truth regular mazes. remains encouraging gvin able ground-truth directions imitation learning. shown later direction-unaware gvin outperforms direction-aware gvin irregular graphs. figures show planning performance improves kernel exponential increases resolution reference direction small. figure compares kernel reference direction different kernel orders. kernel activates wide-range directions; kernel focuses small-range directions higher resolution. table maze performance comparison gvin. gvin achieves similar performance mazes state-value imitation learning achieves similar performance action-value imitation learning. based kernel direction-aware direction-unaware scale generalization reinforcement learning imitation learning. performance metrics previously discussed maze experiments. directional kernel spatial kernel embeddingbased kernel. ﬁrst train gvin imitation learning. table shows embedding-based kernel outperforms kernel methods terms action prediction path difference indicating embedding-based kernel captures edge weight information within neural network weights better methods. spatial kernel demonstrates higher accuracy success rate compared directional kernel suggests effectiveness using sampling. direction-unaware method shows slightly better results spatial kernel larger success rate gain directional kernel. figure shows visualization learned value shares similar properties regular graph value map. also train converting graph image. shown table fails signiﬁcantly figures show planning performance irregular domain kernel order increases. results show larger irregular domain opposite effect compared regular domain. observation reasonable irregular domain direction neighbor extremely variable larger kernel order creates narrower direction range vin. table shows episodic q-learning algorithm outperforms training method used results reported table able train using algorithm epochs trpo curriculum learning took epochs train reported shown figure episodic q-learning algorithm shows faster convergence better overall performance compared q-learning. table performance comparison amongst three different kernels gvin. experiments except macn tested -node irregular graphs. note last column trained using episodic q-learning. stands imitate learning reinforcement learning respectively. similar experimental settings macn achieves success rate -node graphs gvin achieves success rate -node graphs. details training irregular graphs sees section irregular graphs supplementary material. graph edge weights. also test gvin handles edge weights. true weighted shortest path distance nodes edge weight. shown table imitation learning trained -node graphs reinforcement learning trained -node. also examine gvin excluding edge weights input effects performance. table shows reinforcement learning edge weights slightly help agent suitable policy; imitation learning input edge weights cause signiﬁcant failure. demonstrate generalization capabilities gvin evaluate real-world maps minnesota highway contains nodes representing intersections edges representing roads york city street contains nodes representing intersections edges representing roads. models trained graphs containing nodes embedding-based kernel using episodic q-learning section exploring irregular graphs separately. normalize data coordinates recurrence parameter randomly pick start points goal points different times. algorithm baseline. table shows generalize well large scale data. policy could reach goal position experiments. sample planned path shown supplementary reinforcement learning. train gvin using episodic q-learning compare imitation learning. baseline also train gvin using standard deep qlearning techniques including using experience replay buffer target network. networks kernel function conﬁgurations. figure shows comparison algorithms’ success rate expected rewards training. clearly episodic q-learning converges high success rate high expected rewards standard deep q-learning techniques fail achieve reasonable results. scale generalization. also examine scale generalization training -node graphs testing -node graphs using embedding-based kernel. gvin trained -node graphs imitation learning performance signiﬁcantly hindered shown table gvin trained using episodic q-learning table shows excellent generalization abilities outperform imitation learning based results success rate expected rewards. compared imitation introduced gvin differentiable novel planning module capable regular irregular graph navigation impressive scale generalization. also introduced episodic q-learning designed stabilize training process gvin. proposed graph convolution applied many graph-based applications navigation point cloud processing molecular analysis left future works.", "year": 2017}