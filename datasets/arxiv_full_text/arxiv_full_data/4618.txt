{"title": "Submodular Optimization for Efficient Semi-supervised Support Vector  Machines", "tag": ["cs.LG", "cs.AI"], "abstract": "In this work we present a quadratic programming approximation of the Semi-Supervised Support Vector Machine (S3VM) problem, namely approximate QP-S3VM, that can be efficiently solved using off the shelf optimization packages. We prove that this approximate formulation establishes a relation between the low density separation and the graph-based models of semi-supervised learning (SSL) which is important to develop a unifying framework for semi-supervised learning methods. Furthermore, we propose the novel idea of representing SSL problems as submodular set functions and use efficient submodular optimization algorithms to solve them. Using this new idea we develop a representation of the approximate QP-S3VM as a maximization of a submodular set function which makes it possible to optimize using efficient greedy algorithms. We demonstrate that the proposed methods are accurate and provide significant improvement in time complexity over the state of the art in the literature.", "text": "practical success theoretical robustness large margin methods general specially support vector machines drawn attention semi-supervised support vector machines however problem challenging non-convexity objective function. paper propose approximate-svm formulation result standard quadratic programming problem namely approximate qp-svm solved directly using shelf optimization packages. important aspect proposed formulation uncovers connection density separation method graph based algorithms helpful step towards unifying framework furthermore present formulation loss based problems. formulation represents problems functions theory submodular functions optimization solve efﬁciently. speciﬁcally present submodular function equivalent proposed approximate qpsvm solve efﬁciently using greedy approach well established optimizing submodular functions section provides preliminaries notations used throughout paper. proposed approximate qp-svm detailed section section present submodular formulation approximate qp-svm. experimental results provided section followed conclusion section abstract—in work present quadratic programming approximation semi-supervised support vector machine problem namely approximate qp-svm efﬁciently solved using shelf optimization packages. prove approximate formulation establishes relation density separation graph-based models semi-supervised learning important develop unifying framework semi-supervised learning methods. furthermore propose novel idea representing problems submodular functions efﬁcient submodular optimization algorithms solve them. using idea develop representation approximate qp-svm maximization submodular function makes possible optimize using efﬁcient greedy algorithms. demonstrate proposed methods accurate provide signiﬁcant improvement time complexity state literature. recent advances information technology imposes serious challenges traditional machine learning algorithms classiﬁcation models trained using labeled samples. data collection storage nowadays never easier therefore using enormous volumes data infer reliable classiﬁcation models utmost importance. meanwhile labeling entire data sets train classiﬁcation models longer valid option high cost experienced human annotators. despite recent efforts make annotation large data sets cheap reliable using online workforce collected labeled data never keep cheap collection unlabeled data. semi-supervised learning handles issue utilizing large amount unlabeled samples along labeled samples build better performing classiﬁers. assumptions form basis usefulness unlabeled samples discriminative methods cluster assumptions smoothness assumption although assumptions idea samples close distance metric assume label inspire different categories algorithms namely density separation methods graph-based methods density separation methods unlabeled samples used better estimate boundaries class. graph-based methods labeled unlabeled samples construct graph representation data information propagated labeled samples unlabeled samples dense problem simpliﬁed combinatorial {+−} continuous proceed dual form. deriving lagrangian continuous formulation problem applying karush-kuhn-tucker conditions obtained dual form presented problem ones vector length |l|. similarly |u|. αilagrangian multiplier labeled loss constraint γjlagrangian multiplier unlabeled loss constraint βjlagrangian multiplier unlabeled loss constraint using derived dual form problem propose approximate optimization based minimizing upper bound maxabγidual. proposed upper bound speciﬁed following theorem. theorem proposed upper bound maxabγidual solution eqn. result ﬁnding optimal separating hyperplane labels assigned unlabeled samples loss labeled unlabeled samples controlled parameters reﬂect conﬁdence labels cluster assumption respectively. algorithms solve eqn. broadly divided combinatorial continuous optimization algorithms. continuous optimization algorithms given ﬁxed optimal simply obtained sgnw problem comes continuous optimization problem hand combinatorial optimization algorithms given optimization standard problem. therefore deﬁne function solving eqn. lead degenerate solutions unlabeled samples assigned class. usually handled literature enforcing balancing constraint makes sure certain ratio unlabeled samples assigned class eqn. combinatorial formulation optimizes labels minimize loss associated unlabeled sample. overcome hard combinatorial problem loss setting denoted assigned variable variable indicates probability loss setting denoted given probability −pj. balancing constraint form r|u|. modiﬁed formulation following fig. plot fig. small values i.e. means large values i.e. means qjqj minimize assign small large valued means unlabeled samples close large assigned small valued force assume label i.e. hand small assign large words unlabeled samples close small assigned different classes setting large i.e. qjqj easy minimizing basically implements clustering assumption semi-supervised learning algorithms unlabeled samples form clusters samples cluster label. notice minimization smaller minimum value achievable unlabeled samples assigned label therefore however degenerate solution balancing constraint important approximate formulation problem split terms associated labeled samples necessary dependence interpretation labels since minimizing involves assigning small i.e. large values vice versa small valued assigned large i.e. words unlabeled sample close i.e. large labeled sample unlabeled sample label labeled sample hand unlabeled sample from i.e. small labeled sample examining upper bound theorem objective function value optimizing standard supervised labeled samples therefore constant well term c∗|u|. rest upper bound function optimal values obtainable following optimization problem. problem quadratic programming approximation semisupervised support vector machines proposed approximate formulation quadratic programming problem variables order avoid trivial solutions problem variables zero. constraint r|u| makes sure certain ratio unlabeled samples assigned class section analyze approximate model obtained problem necessary ensure approximate model deviate original problem. ﬁrst term eqn. expanded follows negative quadratic minimizing enforces values either words minimizing help making clear assignments labels unlabeled samples. understand implications minimizing solution problem start plotting pjpj) shown fig.. unlabeled sample opposite label labeled sample notable balancing constraint used smaller value minimum achievable unlabeled samples assigned label argument holds minimizing unlabeled samples large/small similarity labeled sample assigned small/large i.e. respectively. process jointly minimizing implements clustering assumption semi-supervised learning unlabeled samples assigned labels similarity labeled samples results formulation follows intuition behind label propagation algorithms semi-supervised learning. labeling process chooses dense regions propagate labels unlabeled samples. therefore provided approximate formulation problem deviate general paradigm semi-supervised learning problem. meanwhile provided formulation provides insight connection avoiding dense regions semi-supervised algorithms include graph-based algorithms. approximate qp-svm formulation proposed problem simple intuitive. however fact quadratic minimization concave function computational complexity ﬁnding solution become hindering issue specially semi-supervised learning problems inherently large scale. section concepts submodular functions provide simple efﬁcient algorithm proposed approximate qp-svm problem. submodular functions play central role combinatorial optimization considered discrete analog convex functions continuous optimization sense structural properties beneﬁted algorithmically. also emerge natural structural form classic combinatorial problems maximum coverage maximum facility location location analysis well maxcut problems graphs. recently submodular functions become concepts machine learning problems feature selection active learning solved maximizing submodular functions core problems like clustering learning structures graphical models formulated submodular function minimization discussed section solution approximate qp-svm provides value variable associated unlabeled sample section different perspective problem. perspective problem binary semi-supervised classiﬁcation general concerned choosing subset pool unlabeled samples unlabeled samples assigned label rest them assigned label possible subset assigned value function optimal solution terms original semi-supervised classiﬁcation problem. makes reformulation semi-supervised learning functions interesting function monotonic submodular many algorithms solve problem efﬁciently following give background concept submodularity functions employ solve problem efﬁciently. function deﬁned xn}. monotonicity submodularity deﬁned follows deﬁnition sets function monotonic well acknowledged result nemhauser theorem below establishes lower bound performance simple greedy algorithm algorithm used maximize monotone submodular function subject cardinality constraint. simple greedy algorithms basically works adding element maximally increases objective value according theorem simple procedure guaranteed achieve least constant fraction optimal solution natural exponential. theorem given ﬁnite monotonic submodular function following maximization problem section illustrate accuracy efﬁciency proposed qp-svm submodular optimization compare performance qp-svm s-qp-svm three competitive algorithms namely transductive support vector machine deterministic annealing semi-supervised kernel machines tsvm experiments performed intel core machine ram. experiments performed several real world data table selected achieve diversity terms dimensionality distribution properties. accuracy transductive learning experiment considered challenging setup number labeled samples exceed available unlabeled data data sets percentage labeled/unlabeled samples splitting process repeated times average reported table illustrate value using unlabeled samples semi-supervised setting results standard trained using labeled samples presented. experiments linear kernel feature-wise normalized data. ratio positive samples output correct ratio unlabeled samples. clear table qp-svm s-qp-svm superior terms accuracy tsvm tsvm. table provide cpu-time comparison qp-svm s-qp-svm tsvm tsvm. clear time complexity perspective s-qp-svm efﬁcient competitors. solving approximate qp-svm problem. towards goal propose following submodular maximization problem equivalent approximate qp-svm problem problem submodular maximization formulation equivalent problem problem basically maximizes negative discrete version objective function eqn.. correspondence ﬁrst three terms eqn. straightforward. however term design added ensure monotonicity submodularity shown theorem constant maximum value kernel matrix. therefore radial basis function kernels. data feature-wise normalized highly recommended practice values linear kernel equal number dimensions used data average number non-zero features since ﬁxed value constant optimal solution obtained optimizing affected adding words depends cardinality contents. theorem function problem monotone submodular shown monotonic submodular means greedy maximization algorithm used used optimize problem performance guarantee theorem holds true. summarize proposed equivalent submodular maximization problem deﬁned subsets samples belonging class labeled efﬁcient greedy algorithm algorithm used solve problem efﬁciently. optimum solution determined rest unlabeled samples i.e. u\\a∗ belong class labels proposed algorithm transductive setting semi-supervised learning. however inductive setting needed standard supervised training performed give ﬁnal hyperplane first follows directly deﬁnition eqn. summations elements therefore sake simplicity consider special case however extension general values fairly straightforward. next prove monotonicity property. using deﬁnition show increase objective value adding proved efﬁcient solve using standard optimization techniques. major contribution proposed qp-svm establishes link major paradigms semi-supervised learning namely density separation methods graph-based methods. link considered signiﬁcant step towards unifying framework semisupervised learning methods. furthermore propose novel formulation semi-supervised learning problems terms submodular functions authors knowledge ﬁrst time idea presented. using formulation present methodology submodular optimization techniques efﬁciently solve proposed qp-svm problem. finally idea representing semisupervised learning problems submodular functions great impact many learning schemes open door using arsenal algorithms theoretical guarantees efﬁcient performance. authors already making progress extending presented work multiclass semi-supervised formulations well examining relationship submodular optimization different matroids interpretation terms semi-supervised learning. last intriguing point proposed work samples assigned classes case positive class sequentially. opens door possible ways estimate ratio positive samples automatically learning process still problem semisupervised techniques specially exists difference ratio labeled unlabeled samples. gr¨otschel lov´asz schrijver geometric algorithms combinatorial optimization second corrected edition ser. algorithms combinatorics. springer vol. narasimhan bilmes submodular-supermodular procedure applications discriminative structure learning uncertainty artiﬁcial intelligence edinburgh scotland morgan kaufmann publishers july krause guestrin nonmyopic active learning gaussian processes exploration-exploitation approach icml proceedings international conference machine learning. york asuncion newman machine learning repository c.-w. c.-c. chang c.-j. practical guide support vector classiﬁcation department computer science national taiwan university tech. rep. thus monotonicity property holds true. prove submodularity assuming using element used earlier i.e. need show adding effect adding stated deﬁnition since narayanan belkin niyogi relation density separation spectral clustering graph cuts advances neural information processing systems sch¨olkopf platt hoffman eds. cambridge press chapelle sindhwani keerthi branch bound semi-supervised support vector machines twentieth annual conference neural information processing systems cambridge joachims transductive inference text classiﬁcation using support vector machines proceedings icml- international conference machine learning bratko dzeroski eds. bled morgan kaufmann publishers francisco sindhwani keerthi chapelle deterministic annealing semi-supervised kernel machines icml proceedings international conference machine learning york", "year": 2011}