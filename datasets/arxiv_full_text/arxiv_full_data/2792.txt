{"title": "Safe Policy Improvement with Baseline Bootstrapping", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A common goal in Reinforcement Learning is to derive a good strategy given a limited batch of data. In this paper, we adopt the safe policy improvement (SPI) approach: we compute a target policy guaranteed to perform at least as well as a given baseline policy. Our SPI strategy, inspired by the knows-what-it-knows paradigms, consists in bootstrapping the target policy with the baseline policy when it does not know. We develop two computationally efficient bootstrapping algorithms, a value-based and a policy-based, both accompanied with theoretical SPI bounds. Three algorithm variants are proposed. We empirically show the literature algorithms limits on a small stochastic gridworld problem, and then demonstrate that our five algorithms not only improve the worst case scenarios, but also the mean performance.", "text": "common goal reinforcement learning derive good strategy given limited batch data. paper adopt safe policy improvement approach compute target policy guaranteed perform least well given baseline policy. strategy inspired knows-what-it-knows paradigm consists bootstrapping target policy baseline policy know. develop computationally efﬁcient bootstrapping algorithms value-based policy-based accompanied theoretical bounds. three algorithm variants proposed. empirically show literature algorithms limits small stochastic gridworld problem demonstrate algorithms improve worst case scenario mean performance. reinforcement learning consists discovering trial-and-error unknown uncertain environment action valuable particular situation. online learning setting trialand-error works optimally good outcome brings policy improvement even error leads learning lesser cost. however real-world algorithms widely deployed independent devices/systems policies cannot updated often online learning would require. ofﬂine setting batch algorithms applied trial-and-error paradigm shows limits policy updates rare commitment trial strong error impact severe. paper endeavour build batch algorithms safe regard. notion safety deﬁned several contexts notions uncertainty internal parametric deﬁned ghavamzadeh internal uncertainty reﬂects uncertainty return stochastic transitions rewards single known parametric uncertainty reﬂects uncertainty unknown parameters transition reward distributions. short internal uncertainty intends guarantee certain level return individual trajectory critical view potential harmful behaviour catastrophe avoidance scenarios paper focus speciﬁcally parametric uncertainty order guarantee given expected return trained policy batch setting speciﬁcally seek high conﬁdence trained policy approximately outperforms behavioural policy called baseline policy onwards. goal therefore improve policy even worst case scenario. such family algorithms seen pessimistic optimism face uncertainty counterpart. section recalls necessary background mdps safe policy improvement section presents novel optimization formulation baseline bootstrapping consists bootstrapping trained policy baseline policy state-action pair transitions probed sufﬁciently often dataset. develop novel computationally efﬁcient spibb algorithms value-based policy-based accompanied theoretical bounds. expense theoretical guarantees implement three additional algorithms variants. then develop related work positioning argue algorithms found literature impractical intractable non-small mdps and/or make unreasonable assumptions dataset size distribution. then section empirically validates theoretical results gridworld problem algorithms compared state algorithms. results show algorithms signiﬁcantly outperform competitors mean performance worst case scenario computationally efﬁcient standard model-based algorithm. markov decision processes widely used framework address problem optimizing sequential decision making. work assume true environment modelled unknown state space action space true bounded stochastic reward function true transition probability reﬂecting quality decision. process repeated episode. denote policy corresponds decision making mechanism assigns actions states. denote stochastic policies probability distributions actions evaluates performance policy function starting state goal reinforcement learning algorithm discover unique optimal state value function deﬁne performance policy expected value policy given policy subset said π-optimal maximises performance maxπ∈π later known also make notation vmax rmax upper bound return absolute value. pmdp posterior probability parameters high probability metaparameter error meta-parameter. petrik admissible high probet bound constraint considerability parameters estimator error function parametrised dataset meta-parameter unfortunately prove np-hard problem. propose algorithms approximating solution without formal proof. first approximate robust baseline regret minimizatrion assumes error transition probabilities baseline policy hazardous assumption. also considering high complexity difﬁcult empirically assess percentile criterion safety. second robust solver uses ξ-worst-case safety test guarantee safety conservative. evoked section endeavour section reformulate percentile criterion order efﬁcient provably-safe policy within tractable amount computer time. criterion consists optimising policy respect performance pairs variance value estimators/policies obtained baseline policy train policy. implement next subsections novel spibb algorithms. show approach safe prove bounds. derive three additional spibb variants work better extent experiments. obtained averaging returns obtained dataset after transitions. indeed constraint estimation precision probability grows logarithmically state size proposition state action pairs then probability least rest subsection assume inequality satisﬁed every state-action pair bootstrap uncertain state-action pairs q-function estimates baseline policy creating qπb-bootstrapped described earlier formalised algorithm then solve estimated denote optimal policy. hereinbelow theorem provides bounds near optimality theorem offers guarantees improving baseline policy theorem optimal policy reward maximization problem paper extend previous result allowing constraint partially satisﬁed subset complementary subset uncertain stateaction pairs called bootstrapped pairs denoted following. dependent state action dataset parameter depends three parameters return precision level equivalently model precision level ||e||∞ high probability discount factor ease notation dependencies omitted. pseudocode construction bootstrapped state-action pairs presented algorithm trained estimated qπb-bootstrapped performed algorithm computational efﬁciency. utilization casting environment qπb-bootstrapped version means qπbbootstrapped state-action pair performed reached state terminal baseline policy take control trajectory end. practical shortcomings means must known take advantage fact deﬁned state-action pairs expected efﬁcient consequence despite lack theoretical guarantees experimental section also assesses empirical safety continuing control trajectory after choosing bootstrapping action. variants value-based spibb respectively referred qπbspibb- qπb-spibb-∞. policy-based spibb baseline policy. section adopt policy bootstrapping. precisely state-action pair rarely seen dataset i.e. batch algorithm unable assess performance instead relies baseline policy copying probability take action particular situation algorithm provides pseudo-code baseline policy bootstrapping. consists constructing allowed policies search πb-optimal policy model estimated dataset practice optimisation process performed policy iteration current policy evaluated next iteration policy made greedy respect constraint belonging similarly theorems qπb-spibb- near πb-optimality baseline policy proven πb-spibb theorem policies constraint following πb-optimal policy reward algorithm referred πb-spibb tendency reproduce rare actions baseline policy. even though allows guarantee performance algood baseline policy’s prove toxic baseline policy already near optimal reasons visited state-action pairs generally actions behavioural policy probability lower meaning actions likely baseline exploratory strategies fall category copying baseline policy case reproducing strategies. another look problem therefore consider rare actions must avoided risky therefore force policy assign probability perform action. algorithm remains unchanged except policy search space replaced deﬁned follows empirical analysis section shows variant referred π-spibb often proves unsafe. believe better policy-improvement spibb lays inbetween space policies search constrained give weight actions tried enough signiﬁcantly assess performance still leave possibility completely performing actions even though evaluation uncertain. resulting algorithm referred π≤b-spibb. again algorithm reused replacing policy search space deﬁned follows algorithm appendix describes greedy projection π≤b. despite lack theoretical guarantees experiments π≤b-spibb proves safe outperforming πb-spibb. however growing batch setting might valuable keep exploring πb-spibb does. table shows difference policy projection policy improvement step policy iteration process. shows baseline policy probability mass redistributed among different actions according three policy-based spibb algorithms. observe πbspibb boostrapped state-action pairs probabilities untouched. opposite π-spibb removes mass boostrapped state-action pairs. ﬁnally π≤bspibb lies in-between. table summarizes algorithms strengths weaknesses. high-conﬁdence refers family algorithms introduced thomas rely ability produce high-conﬁdence policy evaluation trained policy known high variance consequence formally rely policy improvement safety test unlikely positive without immense dataset. verge optimal policy proof relies unpractical assumption sampled every state-action pair huge number times arbrm assumes transition model known around baseline policy strong assumption cannot made practical problems. arbrm robust lesser extent suffer high complexity even ﬁnite state space np-hard reduced approximation polynomial time; also lack safety guarantees respect approximation make tractable. finally reward-adjusted mdp’s algorithm proven safety relies consequence safety test similarly high-conﬁdence berkenkamp assume existence local stable policy safe lyapunov algorithm allows safely explore outside safe region without ever leaving exclusively addresses stabilization tasks online scenario. develop robust versions main model-free algorithms offering algorithms robustly converge online optimal policy. help general batch setting trust regions inﬁnite state-action pair never observed. papini propose adapt batch size ensure gradients safely improve policy. main shortcomings requires policy gradient updates less sample-efﬁcient model-based methods commands batch size usually feature system control ofﬂine setting like ours. three recent algorithms different assumptions ours. table brief summary safe improvement algorithms. columns left right name algorithm existence safe improvement guarantees rely policy improvement test? baseline policy need known? constraint action-state pair counts? safety basic model-based proved petrik arbrm robust reward-adjusted respectively algorithms petrik high-conﬁdence general approach policy improvement guaranteed off-policy evaluation conﬁdence intervals defended thomas side. initial ﬁnal states respectively bottom left right corners. reward hitting wall ﬁnal state reached. consists generating dataset training policy evaluating trained policy. gridworld domain justiﬁed fact basic model-based already fails safe simple environment empirical worst-case evaluation requires runs algorithms dataset sizes values. basic several q-functions initializations investigated optimistic null pessimistic ﬁrst yielded awful perforalgorithms take inspiration arbrm idea ﬁnding policy guaranteed improvement realization uncertain parameters. still like arbrm taking account estimation error function state-action pair counts. instead searching analytic optimum goes straightforwardly solution improving baseline policy guarantee improvement bootstrapping baseline policy uncertainty high. knows-what-it-knows algorithm asking help baseline policy know whether knows. consequence proofs require policy improvement safety test. qπb-spibb- proved safe conditions application widely relaxed compared basic model-based theorem πb-spibb algorithm rely condition application else knowing baseline policy. additionally contrary robust/safe batch algorithms literature spibb algorithms maintain computational cost equal basic algorithms. experimental evaluation gridworld setting case study straightforward discrete stochastic gridworld four actions down left right. transition function stochastic actions move agent speciﬁed direction chances opposite direction chances baseline policies used generate dataset bootstrap literature benchmark performed ﬁrst strong softmax exploration around optimal q-function. spibb benchmark performed second differs favours walking along walls although avoid prevent stochastic transitions. baseline constructed order demonstrate unsafety algorithm π-spibb. results presented forms mean performance runs; worst-case performance worst runs. spibb algorithms values tested. state-of-the-art algorithms batch assume every state-action experienced certain amount times subsection empirically demonstrate assumption generally transgressed even simple gridworld domain. collect millions trajectories ﬁrst baseline. state-action count logarithm shows unbalanced counts transitions experienced trajectory every million trajectories even never seen once. moreover actions rarely chosen likely dangerous ones ones model might lead catastrophic policy. figure displays expected number transitions seen exactly dataset function size. curve decreases slowly trajectories collected. look speciﬁcally dangerous transitions i.e. ones direct agent wall observe peak around trajectories. next subsection strongly affects basic safety surprisingly models trained trajectories yield better returns ones trained trajectories average. conjecture issue faced practical applications too. instance dialogue collected human dialogue transitions relevant discussed. figure shows literature benchmark results best algorithm π≤b-spibb. basic algorithm performs reasonably well average fails safe sometimes outputs policy disastrous. notice performance reaches valley datasets around trajectories. interpret consequence rare pair count effect developed previous subsection. neither robust reward adjusted seem improve safety safety test omitted. curves make relevant comparison test appears always negative wide conﬁdence interval. also worth mentioning reward adjusted algorithm tends become suicidal environments killed indeed intrinsic penalty adjustment overwhelming environment reward optimal strategy stop trajectory fast possible. algorithm π≤b-spibb safe. worst decile performance even signiﬁcantly higher algorithms mean performance. spibb algorithms empirical results lengthily discussed hereinbelow. spibb algorithms efﬁcient safe shift dataset size range window. safety assessed worst-centile measure mean performance worst runs. basic worst centile appear. wide range values evaluated main lessons safety improvement baseline much impacted choice higher implies spibb algorithms conservative bootstrap often baseline. complete results refer interested reader appendix. even though theory would advise higher values report best empirical results value-based spibb algorithms qπb-spibb- qπbspibb-∞ fail safe small datasets. reason algorithms rely assumption even bootstrapped state-action pairs must experienced small amount times. also notice that despite lack guarantees qπb-spibb-∞ improves safety compared qπb-spibb-. πb-spibb π≤b-spibb worst case scenario points baseline partially explained variance evaluation likely consequence policy. π-spibb lacks safety small datasets tends completely abandon actions sampled enough dataset regardless performance. results higher values show dataset size π-spibb tends optimal actions causes strong performance drop worst case scenario mean performance πbspibb conservative fails improve fast policy-based spibb algorithms safely. π≤b-spibb best worlds safe although still capable cutting actions even small number samples. however growing batch settings might better keep trying actions sufﬁciently explored πb-spibb might best algorithm setting. paper tackle problem batch reinforcement learning safety. reformulate percentile criterion without compromising safety expense optimality safe solution. gain allows implement algorithms qπb-spibb- πbspibb fast basic model-based algorithm generating provably safe policy improvement known baseline three spibb algorithms derived without safety guarantees qπbspibb-∞ π-spibb π≤b-spibb. empirical analysis shows that even simple domain basic algorithm fails safe state-of-the-art safe batch algorithms better policy improvement safety test omitted. safety test also proven always negative tests consequently preventing improvement baseline. spibb algorithms show signiﬁcantly better results worst-centile performance even surpassing basic mean performance settings. future work includes developing model-free versions algorithms order ease continuous state complex real-world applications state representation approximation using density networks compute pseudo-counts similar optimism-motivated online future work also includes designing bayesian policy projection take account uncertainty local policy evaluation demonstrating algorithms used conjunction imitation learning compute baseline policy estimate. auer peter ortner ronald. logarithmic online regret bounds undiscounted reinforcement learning. proceedings annual conference neural information processing systems bellemare marc srinivasan sriram ostrovski georg schaul saxton david munos remi. unifying count-based exploration intrinsic motivation. proceedings advances neural information processing systems berkenkamp felix turchetta matteo schoellig angela krause andreas. safe model-based reinforcement learning stability guarantees. proceedings advances neural information processing systems brafman ronen tennenholtz moshe. r-max-a general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research lipton zachary kumar abhishek jianfeng lihong deng combating deep reinforcement learning’s sisyphean curse reinforcement learning. arxiv preprint arxiv. mandel travis yun-en levine sergey brunskill emma popovic zoran. ofﬂine policy evaluation across representations applications educational proceedings international congames. ference autonomous agents multi-agent systems papini matteo pirotta matteo restelli marcello. proadaptive batch size safe policy gradients. ceedings advances neural information processing systems petrik marek ghavamzadeh mohammad chow yinlam. safe policy improvement minimizing robust baseline regret. proceedings advances neural information processing systems schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. proceedings international conference machine learning sutton richard precup doina singh satinder. between mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence proofs make matrix representation q-function -function policy transition reward discount rate functions. q-functions matrices |x||a| columns. -functions matrices columns. policy matrices |x||a| columns. even though policy generally deﬁned function shouldbe represented compact matrix rows columns order simple matrix operators need policy matrix output distribution state-action pairs. consequently policy matrix obtained following expansion diagonal transition matrices rows |x||a| columns. reward matrices |x||a| columns. discount rate matrices rows |x||a| columns. expression matrix product matrices column dimensions coincide. expression element-wise product matrices dimension. denotes identity matrix dimension given context. denotes column unit vector zeros everywhere except element index equals instance denotes value performing action state proof. construction proposition petrik ﬁrst condition satisﬁed every state-action pair individually probability second third inequalities obtained similarly. proof detailed third inequality hereinafter given two-sided hoeffding’s inequality lemma consider transition functions reward functions bootstrapping q-function used bootstrap mdps consider policy state value function policy given respectively. policy decomposed aggregation partial policies non-boostrapped actions probabilities bootstrapped actions probabilities. then difference value functions line explained fact bootstrapping action lead terminal state therefore line passing second term left-hand side equation factorised divided factor. using holder’s inequality state-action pair have proves lemma. contrast lemma additional factor might require discussion. provides fact control fact maximum rmax might vmax semi-mdp setting control factor front second term. consequence surmise tighter bound \u0001vmax decomposed aggregation partial policies containing non-bootstrapped actions probabilities state bootstrapped actions probabilities. denote policies bootstrapped semi mdps. initialization function determines states option available. option policy followed length option. finally termination function deﬁning probability option terminate state. please notice states might available options okay since every option termination function equal states meaning unreachable. avoid situation beginning trajectory notion starting option trajectory starts void option construction i.e. condition state-action counts proposition fulﬁlled. also policy implemented policy bootstrapped semi-mdp. inversely policy admits policy original mdp. note also construction transition reward functions deﬁned pairs convention pairs. corresponding q-functions therefore well. means lemma applied algorithms greedy projection policy-based spibb algorithms rely policy iteration process requires policy improvement step constraint generated policy belong π≤b. respectively described algorithms algorithm greedy projection data baseline policy data last iteration value function data bootstrapped state-action pairs data current state action initialize return algorithm greedy projection data baseline policy data last iteration value function data bootstrapped state-action pairs data current state action initialize return algorithm greedy projection data baseline policy data last iteration value function data bootstrapped state-action pairs data current state action sort decreasing order action values initialize", "year": 2017}