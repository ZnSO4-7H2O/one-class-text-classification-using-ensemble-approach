{"title": "Bayesian Joint Matrix Decomposition for Data Integration with  Heterogeneous Noise", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery. While only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods.", "text": "abstract—matrix decomposition popular fundamental approach machine learning data mining. successfully applied various ﬁelds. matrix decomposition methods focus decomposing data matrix single source. however common data different sources heterogeneous noise. matrix decomposition methods extended multi-view data integration pattern discovery. methods designed consider heterogeneity noise multi-view data data integration explicitly. propose joint matrix decomposition framework models heterogeneity noise gaussian distribution bayesian framework. develop algorithms solve model variational bayesian inference algorithm makes full posterior distribution; another maximum posterior algorithm scalable easily paralleled. extensive experiments synthetic real-world datasets demonstrate bjmd considering heterogeneity noise superior competitive state-of-the-art methods. plays fundamental roles machine learning data mining. know basic task machine learning data mining discover knowledge primitive data. many cases primitive datasets observations organized matrix forms. example image stored matrix pixels; corpus often represented document-term matrix describes frequency terms occur collection documents. primitive datasets generally noisy matrices. extracting useful information primitive datasets directly often infeasible. therefore matrix decomposition becoming powerful tool compact meaningful representation observed matrices. matrix decomposition successfully applied various ﬁelds including image analysis gene expression analysis collaborative ﬁltering text mining many areas. generally matrix decomposition methods assume data matrix single data source. conceptual idea underlying matrix decomposition observed data matrix approximated product low-rank matrices. given observed data matrix general matrix decomposition method summarized follows chihua zhang shihua zhang* ncmis cems rcsds academy mathematics systems science chinese academy sciences beijing china school mathematics sciences university chinese academy sciences beijing china. correspondence addressed. email zshamss.ac.cn. divergence function. notice w·khkj number columns i.e. column observed matrix approximated linear combination columns therefore often referred basis matrix often referred coefﬁcient matrix. generally different matrix decomposition methods placing different constraints example k-means clustering considered simplest matrix decomposition method k-means clustering contains clustering centroid matrix restriction; binary matrix column indicates cluster membership single one. seminonnegative matrix factorization relaxes restriction k-means. restricts nonnegative. nonnegative restriction usually improves interpretability basis matrix. decompose nonnegative data like images nonnegative matrix factorization explored restricts nonnegative. nonnegative constraints lead part-based feature extraction sort sparseness naturally moreover effectiveness clustering data representation extended many variant forms including various sparse models regularized models however several main drawbacks standard matrix factorization methods. first prone overﬁtting observed data matrices. second regularization parameters models require carefully tuned. bayesian approaches imposed matrix factorization address weaknesses incorporating priors model parameters hyperparameters. third methods focus data matrix source. shared patterns multi-view data matrices different sources reﬂected shared basis matrix. easily jnmf mathematically equivalent setting therefore jnmf ignores heterogeneity noise different data sources. stra˘zar extended jnmf adding orthogonality-regularized terms coefﬁcient matrices predict protein-rna interactions orthogonality regularization prevents multicollinearity ionmf stated outperform models predicting protein-rna interactions. however heterogeneity noise among different data types still ignored. multinmf extends jnmf multi-view clustering requires coefﬁcient matrices learned various views approximately common. speciﬁcally formulated follows λc||h h∗|| ||w·k consensus coefﬁcient matrix weight parameter tune relative importance among different views. image processing jing proposed supervised joint matrix factorization model image classiﬁcation annotation snmfca factorizes region-image matrix annotation-image matrix simultaneously incorporates label information network-regularized term. snmfca also uses weight hyperparameters balance importance data source. recently chalise fridley proposed intnmf extension jnmf specifying weight hyperparameter data source. speciﬁcally formulated follows user predeﬁned weight parameter. methods weight parameters regularize importance data source. however choose weight parameters still sufﬁciently studied literature. glad bayesian methods matrix decomposition studied bishop proposed bayesian principal component analysis automatically determine effective number principal components part inference procedure. later bishop gives effective variational inference method bpca. welling kurihara introduced bayesian k-means retains ability selecting model structure incorporating prior model parameters. schmidt proposed bayesian variant introducing exponential priors basis matrix coefﬁcient matrix respectively moreover bnmf assumes gaussian noise product basis sources common. data different sources might complementary other. moreover usually different quality make full data important challenging topic. many studies devoted efforts developing mathematical methods tackle underlying issues data integration. example zhang proposed joint perform integrative analysis multiple types genomic data discover combinatorial patterns would ignored single type data. jnmf assumes different data shares common basis matrix data set. moreover zhang also developed sparsity-constrained network-regularized joint multiple genomics data integration. developed multi-view multi-view clustering. unlike jnmf others shared one-side factor among multi-view data matrices multinmf uses consensus coefﬁcient matrix give multi-view clustering results. recently chalise fridley proposed weighted joint model integrative analysis assigns higher weight data reliable source. however methods address heterogeneity noise explicitly. jnmf ignores noise heterogeneity multinmf intnmf address problem implicitly giving lower weight noisy data source weight source difﬁcult decide. thus powerful computational methods well consider heterogeneous noise multi-view data data integration urgently needed. propose powerful joint matrix decomposition framework models heterogeneity noise explicitly gaussian distribution bayesian framework. mathematically still remains interpretability sparsity basis matrix regularized matrix decomposition does. develop algorithms solve model variational bayesian inference algorithm makes full proposal distribution approximate posterior distribution; another maximum posterior algorithm turns iterative optimization algorithm enabling efﬁcient scalable. extensively compare algorithms discuss advantages disadvantages. extensive experiments synthetic real-world datasets show effectiveness bjmd modeling noise demonstrate considering heterogeneity noise leads superior performance state-of-the-art methods. related work joint variants joint natural extension integrating multiple datasets data matrices )m×n )m×nc optimization format jnmf formulated matrix coefﬁcient matrix. however gaussian noise lead entry data matrix negative. collaborative ﬁltering salakhutdinov mnih proposed bayesian probabilistic matrix factorization places gaussian prior basis coefﬁcient matrices predict user preference movies. bpmf attracted great interests researchers diverse ﬁelds. moreover bayesian methods matrix decomposition collaborative ﬁltering ﬁeld also proposed glad mixed-membership model utilizes three typical distributions glad described matrix decomposition model laplace prior basis matrix dirichlet prior coefﬁcient matrix gaussian noise observed data matrix. speciﬁcally glad assumes observed data matrix xm×n generated follows element basis matrix column coefﬁcient matrix indicates gaussian noise mathematically laplace prior enforces sparsity basis matrix. dirichlet distribution providing distribution clusters column inference glad focuses estimating posterior distribution latent variables however non-conjugate prior intractable calculate posterior distribution involves high dimensional integral. glad seeks non-conjugate variational inference instead calculating integral exactly uses distribution parameterized approximate true posterior distribution. speciﬁcally aims minimizing kullback-leibler divergence approximation distribution true posterior distribution denotes divergence denotes family probability distribution functions make integral trackable. involving posterior distribution divergence still difﬁcult optimize. instead maximize evidence lower bound ﬁrst term expectation joint distribution approximation distribution second term entropy approximation distribution. therefore expectation calculated analytically turns inference problem optimization problem. unfortunately expectation still difﬁcult compute glad. address issue glad introduces laplace approximation address expectation elbo. inference glad main drawbacks. first approximates elbo approximation true posterior. consequently approximation distribution true posterior distribution might widened. second still computationally expensive involving non-linear optimization problem iteration. thus glad takes considerable time converge even small data makes unrealistic realworld datasets. non-negative restriction coefﬁcient matrix semi-nmf leads better interpretability basis matrix. however columns basis matrix usually comparable without proper normalization weakens interpretability. fact semi-nmf higher weight data points source lower noise level. different weight hyperparameters intnmf multinmf automatically inferred optimization process. second term equation l-norm regularization basis matrix regularizes sparsity last term regularization term coefﬁcient matrices. omit ﬁrst terms last term minimized actually expectation dirichlet prior. therefore last term enforces expectation prior reduces risk overﬁtting. variational inference subsection adopt variational inference algorithm bjmd model. comparison ﬁrst extend glad multi-view data matrices shared basis matrix different coefﬁcient matrices. note glad extended glad model multi-view data full bayesian framework. models treat variance gaussian distribution hyperparameter rather distribution. thus refer extended glad model joint matrix decomposition later. easily inference algorithm glad naturally extended jmd. however extended inference algorithm still suffers aforementioned problems glad approximation elbo might good approximation true posterior; expensive computational cost makes unrealistic real-world datasets. recent advances stochastic variational methods including black variational inference auto-encoding variational bayes automatic differentiation variational inference provide powerful tools tackle problems. stochastic variational inference methods avoid calculating elbo directly. instead take derivative elbo push derivative operator expectation denotes observed data denotes latent variable denotes variational distribution parameterized denotes joint distribution latent variables observed data denotes elbo. gradient elbo expressed expectation variational distribution approximated monte carlo method implement advi optimizing bjmd model python library pymc advi converts constrained latent variables unconstrained ones uses gaussian distributions approximate elbo unconstrained random variables. following previous notations advi brieﬂy summarized follows firstly advi transforms constrained latent variables unconstrained real-valued variables invariant column scaling scaling consider diagonal matrix size equal ss−h. additionally restrict column coefﬁcient matrices summing one. support dirichlet distribution unit simplex place dirichlet prior column coefﬁcient matrices complete bayesian model also need introduce prior distribution variance gaussian distribution. convenience choose conjugate prior hyperparameters inverse gamma distribution small positive value noninformative fashion could help understand optimization perspective. hence write log-likelihood. simplicity formulate complete negative loglikelihood retaining terms involving follows absolute value factor laplace distribution inconvenient posterior inference within bayesian framework address computational issue reformulate model utilizing two-level hierarchical representation laplace distribution. speciﬁcally random variable following laplace distribution therefore replace laplace prior two-level hierarchical representation eliminate absolute value factors. useful technique simplify computation used studies reformulation complete likelihood note absolute value bars eliminated reformulation. instead original lnorm regularized term converted weighted l-norm regularized term easier optimize. fig. shows graphical model reformulated bjmd. furthermore advi reparameterizes gradient terms standard gaussian. transformation turns monte carlo procedure sampling standard gaussian distribution enabling sampled effectively. finally advi uses noisy gradients optimize variational distribution. transformation mentioned former three steps one-to-one mappings variational parameters concern variational distribution original space constrained latent variable advi automatically infer probabilistic models steps. notice variational distribution optimized noisy gradients thus possible advi escape local optimum overﬁtted. disadvantage noisy gradients elbo converge. instead elbo ﬂuctuate small interval. therefore difﬁcult decide stop iteration. empirically stopping iteration immediately elbo becoming stable often leads poor performance. coefﬁcient matrices clustering representation stop iteration coefﬁcient matrices stable. advi empirically takes tens thousands iterations well-trained model. checking relative change basis matrices iteration leads expensive computational cost. thus check coefﬁcient matrices every iterations ensure model sufﬁciently trained ||)l|| tolerance threshold small positive value coefﬁcient matrices iteration note relative change converge zero noisy gradient. thus iteration stopped criterion satisﬁed iteration exceeds itermax itermax user predeﬁned maximum times iteration. maximum posterior advi makes possible apply bjmd framework realworld datasets still takes tens thousands iterations stopping criterion satisﬁed. efﬁcient scalable algorithm develop maximum posterior algorithm directly maximizes posterior treat optimization problem. difference that approximates posterior distribution family distribution; approximates posterior distribution mode. makes full posterior distribution pays expensive computational cost. contrast point estimation less computational cost. present algorithm bjmd model following. convex optimization problem. otherwise would difﬁcult solve problem non-convexity. always following experiments. note problem separable. divide subproblems column optimize subproblem parallel manner. subproblem column follows subproblem close quadratic programming problem unit simplex constraints. objective function quadratic term coupled logarithmic barrier function. difference that constraint quadratic programming problem smooth parameter enforce path interior within feasible region. usually small gradually decreasing case hyper-parameter related dirichlet prior. conﬁdent prior knowledge necessarily small. update rules here propose powerful iterative update procedure solve problem easily paralleled. note optimization problem convex. therefore optimize block-coordinate manner. speciﬁcally update latent variable others ﬁxed convergence. updating ﬁxing others partial derivative zeros ∂zik simple quadratic equation variable. considering constraint have diag denotes diagonal function element-wise root square. subproblem close ridge regression difference elements vector equal ridge regression. diagonal matrix zi·)− special form case diag zero system fα·j solved newtonlike method. iteration choose good direction jacobian matrix linear approximation. then following linear equation solved stop criterion easy step algorithm convex. value objective function ensured monotonically non-increasing. stop iteration relative change objective function small enough computational complexity brieﬂy discuss computational complexity proposed algorithm bjmd model. iteration computational expensive step inferring involves solving constrained problem. infer iteration subalgorithm needs solve linear system stops iterations total cost updating infer linear system solved leads cost total. computational cost updating variance respectively. altogether iteration total complexity algorithm thus upper bound iteration. generally empirically small wi·s optimized parallel. therefore proposed algorithm scalable large-scale problems. output repeat update update satisﬁed subalgorithm interior-point algorithm input initialized parameters output column coefﬁcient matrix repeat ﬁrst evaluate bjmd algorithms advi using synthetic data. demonstrate methods estimate variance noise accurately bjmd leads superior performance compared jmd. apply methods three real-world data including -source text dataset extended yale face database metabric breast cancer expression dataset compare several state-of-the-art methods including k-means joint-nmf semi-nmf bnmf. experiments performed desktop computer intel xeon card memory ubuntu evaluate effect methods terms clustering ability adopt evaluation metric based area under curve speciﬁcally coefﬁcient matrix source corresponding true label indicator matrix binary equals means sample data source belongs cluster. note sample belongs clusters thus zero column. deﬁne following metric constant denotes number non-zero entries ﬁrst columns denotes length coherence basis wi−· wi·. note entries column equal zero. next generate entries coefﬁcient matrix k×nc source follows bernoulli distribution parameter namely column entries equal zero normalize column equals one. finally generate observed data matrix following experiments applied vibjmd map-bjmd synthetic data. also concatenated column combined data matrix. applied glad vi-bjmd map-bjmd concatenated data matrix comparison. denote results algorithms vi-catbnjmd mapcatbjmd respectively clarity. value average best runs fig. illustration convergence behavior vi-bjmd using small-scale synthetic dataset σ=.. elbo versus iteration numbers; relative change coefﬁcient matrices percentage aucs versus iteration numbers source. convergence behavior vi-bjmd ﬁrst investigate convergence behavior variational inference bjmd numerical simulation. applied vibjmd onto small-scale synthetic dataset σ=.. elbo becomes stable begins ﬂuctuate small interval around iterations relative change coefﬁcient matrices stays stable around iterations also plot steps iteration versus performance vibjmd source observed performance vi-bjmd source increasing iteration progressed. moreover decreases correspondingly noise level increases. however performance still increasing elbo gets stable iterations. therefore stop iteration immediately elbo becoming stable model relative poor performance terms aucs. elbo lower bound evidence approximates posterior distribution. thus possible variational distribution approaching true posterior distribution elbo stays stable. coefﬁcient matrix predictions obvious performance vi-bjmd stay stable relative change coefﬁcient matrices become stable. therefore stopping criterion variational algorithm better choice examining change elbo. note relative change coefﬁcient matrices approach zero noisy gradients optimization. therefore stop iteration tolerance satisﬁed maximum times iteration itermax exceeded. following experiments always itermax clearly methods considering heterogeneity noise demonstrate signiﬁcantly better performance others terms aucs moreover proposed methods map-bjmd vi-bjmd obtain better performance glad jmd. also glad cost considerable time even small synthetic data thus impractical apply large-scale realworld datasets. utilized card accelerate variational inference. synthetic data small vi-bjmd utilize cores card. concatenating matrices signiﬁcantly accelerate speed iteration. situation vi-catbjmd spends around third running time vi-bjmd. size data matrix source enough signiﬁcant difference vi-bjmd vicatbjmd terms iteration speed. map-bjmd mapcatbjmd much faster methods. besides map-bjmd gets performance vi-bjmd. performance large-scale synthetic dataset consistent small-scale synthetic dataset vi-catbjmd exceeds maximum iteration itermax map-bjmd map-catbjmd take less steps converge note mapbjmd vi-bjmd outperform map-catbjmd vicatbjmd respectively besides vi-bjmd obtains better performance map-bjmd. since coefﬁcient matrices relative large iteration speed vi-catbjmd slightly faster vi-bjmd. vi-catbjmd vi-bjmd take seconds still applicable real-world data. algorithm bjmd much faster one. scalable largescale data. erogeneity noise assumption necessary verify whether proposed models estimate noise level data different sources accurately. verify ability discovering heterogeneity noise different data sources applied vi-bjmd map-bjmd small-scale synthetic dataset σ=.. clearly estimated probability density curves close true noise histograms thus three methods estimate variance gaussian distribution accurately following conduct experiments demonstrate considering heterogeneity noise different data sources leads superior performance. evaluate methods small large-scale synthetic datasets respectively. glad omitted large-scale synthetic data huge computational cost. order facilitate comparison plot objective function value versus iteration number likelihood versus iteration number respectively natural methods considering heterogeneity noise better objective function values methods extra parameters model noise source. iteration variational inference calculates noisy gradient monte carlo sampling signiﬁcantly accelerated takes thousands iterations satisfy stopping criterion. contrast map-bjmd computational cost iteration converge iterations. aucs vi-bjmd map-bjmd source source inﬂuenced increasing noise level source indicates proposed algorithms suitable integrating data different noise level. aucs vi-catbjmd source source suffers rapid decreasing noise level source greater reason noise level source greater variances noise source source overestimated thus gradients coefﬁcient matrices source source smaller true gradient. consequence vi-catbjmd needs much iterations. aucs vi-bjmd slightly better map-bjmd. vi-bjmd approximates posterior distribution variational distribution map-bjmd approximates posterior distribution mode density function. result robust model misspeciﬁed. observe aucs vi-catbjmd better map-catbjmd noise level source results large-scale synthetic datasets consistent small-scale ones still observe considering heterogeneity noise leads superior performance. noise level source performance vi-catbjmd decreases sharply. compared result small-scale ones vi-bjmd outperforms map-bjmd signiﬁcantly. performance curves vi-bjmd source source stable noise level source increasing. implies vi-bjmd successfully protects source source inﬂuence highly noisy source. however performance curves map-bjmd ﬂuctuates slightly. moreover performance vi-bjmd consistently better map-bjmd sources. real-world experiments subsection applied vi-bjmd map-bjmd three real-world datasets compared performance k-means joint-nmf bnmf semi-nmf. glad omitted huge computational cost. three datasets data preprocessing summarized so-called sources text data obtained http//mlg.ucd.ie/ datasets/sources.html. dataset collected three well-known online news sources reuters guardian contains news articles covering distinct news stories. story annotated manually labels. reuters fig. performance comparison vi-bjmd map-bjmd respective version ignoring heterogeneous noise small-scale synthetic datasets. standard deviation gaussian noise ﬁxed source source noise level source ranges performance algorithms considering difference noise level plotted dashed line others plotted solid line. fig. performance comparison vi-bjmd map-bjmd large-scale synthetic datasets. standard deviation gaussian noise ﬁxed source source noise level source ranges guardian represented term-document matrices size respectively. conduct integrative clustering combined terms data matrices. result data matrices sizes respectively. note dataset sparse. article represented -dimensional vector entries nonzeros. extended yale face database dataset next dataset so-called extended yale face database data contains images individuals collected different illumination conditions. image illumination condition recorded azimuth elevation angles source light. used images whose azimuth elevation angles smaller degrees resized images divided groups center lighting source light near center azimuth angle images total; side lighting light side azimuth angle images total. therefore data matrices center lighting side lighting sizes respectively. normalized data matrices dividing average analysis. images collected side lighting shadow thus natural assumes noise level groups different. metabric breast cancer dataset third dataset so-called metabric data contains large breast cancer patient cohort around samples detailed clinical measurements genome-wide molecular proﬁles. using gene expression classify invasive breast cancers biologically clinically distinct subtypes studied decade. parker developed efﬁcient classiﬁer called distinguish intrinsic subtypes breast cancer commonly employed breast cancer studies thus used subtypes reference evaluate results clustering. cellularity breast tumor concerned clinical indicator treatment therefore used cellularity divide samples groups. ﬁrst mapped probes gene expression gene names average pooling. removed samples subtypes cellularity available. genes samples left preprocessing. samples divided groups cellularity moderate cellularity high cellularity size respectively. number features dataset much larger number samples. applied algorithms three datasets. experiments dirichlet prior laplace prior tolerance map-bjmd vi-bjmd respectively. used k-means joint-nmf bnmf semi-nmf comparison. among four algorithms bnmf bayesian approach. experiment reported average best runs. table shows estimated noise level dataset. table show clustering performance sources extended yale face database metabric dataset respectively. aucs running time reported. vi-bjmd takes much time metabric dataset thus reported algorithm semi-nmf involves calculating inverse matrix. mentioned earlier sources data sparse. therefore semi-nmf robust dataset. map-bjmd involves calculating inverse matrix updating basis matrix laplace priors protect inverse matrix singular. bayesian methods including bnmf vibjmd map-bjmd outperform methods sources dataset. suggests superiority bayesian priors sparse data. performance metabric data. suggests map-bjmd robust methods. note number features much larger number samples metabric dataset. believe feature selection needed improve situation. although vi-bjmd outperforms map-bjmd synthetic experiments. however vi-bjmd guaranteed outperform map-bjmd real-wold data. realworld data much complicated vi-bjmd often exceeds maximum steps. adaptive feature selection subsection discuss estimated noise level select features adaptively. biological applications features often redundant. common thousands features tens hundreds samples. thus feature selection needed. perhaps straightforward approach select features choose variance greater threshold. however difﬁcult specify threshold value unsupervised tasks. luckily estimated noise level threshold conduct adaptive feature selection. speciﬁcally map-bjmd much faster vi-bjmd map-bjmd estimate variance gaussian noise source. variance feature less variance noise. natural assume signiﬁcant effect clustering. procedure adaptive feature selection summarized follows given dataset sources ﬁrst map-bjmd estimate variance source. then source conduct individual hypothesis tests null hypothesis variance feature tailed alternative hypothesis finally given signiﬁcant level simply bonferroni correction choose features variances signiﬁcant greater background noise shown table feature selection improves performance methods compared table implies feature selection procedure effective. vi-bjmd map-bjmd outperform algorithms. moreover feature selection procedure reduces size data matrices. therefore running time vi-bjmd acceptable outperforms methods terms aucs. discussion conclusion paper proposed bayesian joint matrix decomposition framework models heterogeneity noise explicitly gaussian distribution. develop algorithms solve framework. approximates posterior accurately scalable. algorithms estimate noise level data source accurately. experimental results synthetic datasets show considering heterogeneity noise different sources brings improvement clustering performance protects data source noise level inﬂuence noisy data sources. experiments real-world datasets demonstrate methods achieve better competitive performance state-ofthe-art methods. well reminded several questions remain investigated future studies. first vi-bjmd consistently outperforms map-bjmd synthetic experiments distinctly observed real-world data experiments. possibly implies advi work well real data complicated synthetic data. remains problem infer model efﬁcient effective variational methods. second bjmd models heterogeneous noise gaussian distribution. however noise real-world might structured complicated. instead assuming noise distribution follows family parametric distribution nonparametric bayesian methods ﬂexible might complicated noise real-world data. suggest extend current bjmd model. seung learning parts objects nonnegative matrix factorization nature vol. hoyer non-negative matrix factorization sparseness constraints machine learning research vol. huang constrained nonnegative matrix factorization image representation ieee trans. pattern anal. mach. intell. vol. j.-p. brunet tamayo golub mesirov metagenes molecular pattern discovery using matrix factorization proc. nat’l academy sciences vol. carmona-saez pascual-marqui tirado carazo pascual-montano biclustering gene expression data non-smooth non-negative matrix factorization bioinformatics vol. witten tibshirani hastie penalized matrix decomposition applications sparse principal components canonical correlation analysis biostatistics vol. park sparse non-negative matrix factorizations alternating non-negativity-constrained least squares microarray data analysis bioinformatics vol. non-negative matrix factorization manifold proc. ieee int’l conf. data mining huang graph regularized nonnegative matrix factorization data representation ieee trans. pattern anal. mach. intell. vol. bishop bayesian advances neural information rodr´ıguez-lescure ebbert prat mun´arriz rowe miller ruiz-borrego anderson lyons breast cancer subtyping rt-qpcr concordance standard clinical molecular markers medical genomics vol. nielsen parker leung voduc ebbert vickery davies snider stijleman reed comparison intrinsic subtyping immunohistochemistry clinical prognostic factors tamoxifen-treated estrogen receptor–positive breast cancer clin. cancer res. ellis suman hoog snider prat parker deschryver allred randomized phase neoadjuvant comparison letrozole anastrozole exemestane postmenopausal women estrogen receptor– rich stage breast cancer clinical biomarker outcomes predictive value baseline pam-based intrinsic subtypeacosog clin. oncol. vol. chang wooten tsimelzon hilsenbeck gutierrez y.-l. tham kalidas elledge mohsin osborne patterns resistance incomplete response docetaxel gene expression proﬁling breast cancer patients clin. oncol. vol. rajan poniecka smith yang frye pusztai fiterman gal-gombos whitman rouzier change tumor cellularity breast carcinoma neoadjuvant chemotherapy variable pathologic assessment response cancer vol. hatakenaka soeda yabuuchi matsuo kamitani tsuneyoshi honda apparent diffusion coefﬁcients breast tumors clinical application magn. reson. med. sci. vol. zhang zhou novel computational framework simultaneous integration multiple types genomic data identify microrna-gene regulatory modules bioinformatics vol. joseph hammonds celniker frise stability-driven nonnegative matrix factorization interpret spatial gene expression build local gene networks proc. nat’l academy sciences vol. georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern anal. mach. intell. vol. curtis shah s.-f. chin turashvili rueda dunning speed lynch samarajiwa yuan genomic transcriptomic architecture breast tumours reveals novel subgroups nature vol.", "year": 2017}