{"title": "A Geometric Framework for Convolutional Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.NE", "I.5.1; I.2.6"], "abstract": "In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks.", "text": "paper geometric framework neural networks proposed. framework uses inner product space structure underlying parameter perform gradient descent component-based form coordinate-free manner. convolutional neural networks described framework compact form gradients standard higher-order loss functions calculated layer network. approach applied network structures provides basis create networks. machine learning algorithms long worked multi-dimensional vector data parameters exploited underlying inner product space structure. recent paper deep learning nature called paradigms involving operations large vectors propel ﬁeld forward. approach taken describe convolutional neural network paper. particular layers described vector-valued maps gradients maps respect parameters layer taken coordinate-free manner. approach promotes greater understanding network coordinate-based approach allows loss function gradients calculated compactly using coordinate-free backpropagation error. paper also considers higher-order loss function algorithms compute iteration gradient descent provided types loss functions clarify application developed theory. precise notation developed throughout paper provides mathematical standard upon deep learning researched overcoming inconsistent notation currently employed across ﬁeld. framework developed paper ﬂexible extended cover types network structures even inspire developments deep learning. symbol exclusively used denote tensor product linear maps vector space denoted linear adjoint denoted linear deﬁned relationship ⟨l∗f vector bilinear deﬁne linear map∶ derivative viewed deﬁned denote adjoint that⟨w v⟩=⟨d∗f consider written denote derivative respect evaluated denote derivative respect evaluated easy verify ∇f∈l adjoints denoted respectively. sometimes written instead emphasize differentiation second derivative respect evaluated bilinear deﬁned follows used second equality. shows error−y) propagates backward composition i.e. second derivative given second derivative given e)+df) equivalent following ﬁxed df)\u0001 adjoint yields higher-order backpropagation error loss function depend parameters. section describe framework applied convolutional neural networks; refer example theory cnns. first actions layer generic described extended multiple layers. coordinate-free gradient descent algorithm also described. note section bases inner product space assumed orthonormal. rn×ℓ called feature corresponds abstract representation input generic layer. rp×q ﬁlter used convolution r¯n×¯ℓ bias term. actions layer feature maps rn×ℓ maps deﬁned assist calculation derivative elementwise ﬁrst second derivatives maps dimension deﬁned replaced ¯σ′′ formulation respectively. furthermore consider bilinear comes taking derivative adjoint relationship ○ft. note d∗ft calculated using theorem since update given algorithm method calculating explicitly shown derivation simpler version contribution input point note learning rate. ance network direction useful image classiﬁcation example class image expected invariant respect rotation. case would inﬁnitesimal generator rotation. term added create loss function work developed geometric framework convolutional neural networks. input data parameters deﬁned vector space equipped inner product. parameters learned using gradient descent algorithm acts directly inner product space avoiding individual coordinates. derivatives higher-order loss functions also explicitly calculated coordinate-free manner providing basis gradient descent algorithm. mathematical framework extended types deep networks including recurrent neural networks autoencoders deep boltzmann machines. another interesting future direction expand capabilities automatic differentiation coordinate-free realm strengthening hierarchical approach paper shown express particular deep neural network end-to-end precise format. however framework limited expressing previous results written simply derivative calculation method. stronger mathematical understanding neural networks provided work promote expansion types networks.", "year": 2016}