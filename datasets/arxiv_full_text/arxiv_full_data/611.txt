{"title": "Going Wider: Recurrent Neural Network With Parallel Cells", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Recurrent Neural Network (RNN) has been widely applied for sequence modeling. In RNN, the hidden states at current step are full connected to those at previous step, thus the influence from less related features at previous step may potentially decrease model's learning ability. We propose a simple technique called parallel cells (PCs) to enhance the learning ability of Recurrent Neural Network (RNN). In each layer, we run multiple small RNN cells rather than one single large cell. In this paper, we evaluate PCs on 2 tasks. On language modeling task on PTB (Penn Tree Bank), our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English translation task, our model increases BLEU score for 0.39 points than baseline model.", "text": "abstract recurrent neural network widely applied sequence modeling. hidden states current step full connected previous step thus influence less related features previous step potentially decrease model’s learning ability. propose simple technique called parallel cells enhance learning ability recurrent neural network layer multiple small cells rather single large cell. paper evaluate tasks. language modeling task model outperforms state models decreasing perplexity chinese-english translation task model increases bleu score points baseline model. .introduction recurrent neural network powerful sequence models natural language processing. many important applications achieve state performance including language modeling machine translation features learned stored hidden states. time step cell extracts features data updates hidden states. left side figure concisely shows transition hidden states naïve rnn. units previous step fully connected units current step. thus pair features affect other. design reality many features related. influence unrelated features harm learning ability models. expect learning models automatically weight unnecessary connections zero. however practice data size limited algorithms strong unrelated connections harm learning ability. example feature selections training. address problem many successful neural models benefit replacing global connection local connection. example long short-term memory gated recurrent units popular cells. core models gates control data flow allow part connections activated. another example convolution neural networks successful models deep learning nowadays. uses local receptive fields extract features previous feature map. local receptive fields neurons extract elementary visual features oriented edges end-points corners. line propose model named rnn-pcs improve learning ability meanwhile largely reduce number parameters. model replaces global recurrent connections small local connections. replace single large cell many smaller ones right part figure hidden states longer full connected connected local manner. figure shows unrolled layer parallel cells. layer small cell extracts saves features outputs previous layer independently. outputs small cells concatenated output current layer. design several advantages. first cell extracts features independently. features cell transferred recurrent connection impacted features cells. second parameters recurrent connection decrease significantly. also cell placed entirely gpu. multiple small cells rather single larger cell thus place cells different gpus optimize training. thirdly parallel cells modify inner structure cells result used along type cells lstm. indeed idea rnn-pcs inspired multi-filter mechanisms cnn. uses multi filters generate multi feature maps. next feature maps stacked input following steps. rnn-pcs works similar manner. multi cells extract different features current inputs. outputs stacked inputs following steps. empirical stage shows different filters running different features. also make empirical study language modeling task show different cells offer different features. evaluate rnn-pcs tasks language modeling chinese-english translation. language modeling task conducted penn tree bank model outperforms state systems decreasing perplexity task chinese-english machine translation proposed model achieves bleu score outperforms strong baseline model bleu score rest paper organized follows. section describes models including background models parallel cells. section reports experiments. section based task language modeling make empirical study observe behavior rnn-pcs. finally section draws conclusions. .methods background models briefly background models used paper including recurrent neural network long-short term memories variants. naïve recurrent neural network cell basic computation component rnn. cell mainly composed three parts input vector，hidden states recurrent connection. generally output cell hidden states. dynamics viewed deterministic transition past hidden states current states given current input. recurrent connection defines transition procedure happens. subscript denote time step subscript denote layers. h\u0000\u0000∈ℝ\u0000 hidden states cell time step layer linear transform bias e.g. y=wx+bfor bwhere x∈ℝ\u0000 y∈ℝ\u0000. sigmoid tanh relu non-linear activate functions. element-wise multiplication. time step layer hidden state determined previous hidden state h\u0000\u0000\u0000\u0000 current input x\u0000\u0000. learning long range dependencies augmenting memory cell vector c\u0000\u0000∈ℝ\u0000. variants commonly used variants basic architecture bidirectional multilayer rnn. bidirectional contains cells parallel input sequence reverse input sequence. time step hidden states forward backward concatenated output. multilayer multilayer layer contains single cell. output lower layer upper layer input. parallel cells idea replace single large cell several small parallel cells. figure shows basic cell counterpart parallel cells solution. left side basic cell whose hidden states units. cell’s recurrent connection. input vector hidden vector respectively. right side shows counterpart parallel cells solution. small parallel cells unit hidden states. thus left right side equal total unit hidden states retain information past. small cells accept input generate outputs h_h_…h_n respectively. final output concatenated h_h_…h_n. easily used along lstm complex cells. shown figure parallel cells modify internal structure cells. example lstm cell also accepts input outputs hidden units. mainly modification lstm memory vector recurrent connection impact pcs. compared naïve hyper-parameter wide denotes amount parallel cells. wide= model exactly naïve rnn. note replace cell many parallel small cells equally dividing units hidden states. really arbitrary setting convenience turning. maybe delicate splitting method helpful improving learning ability. parameters amount basic cell units parameters total parameter size m\u0000+m. suppose wide=n t\u0000\u0000\u0000\u0000 total number parameters \u0000\u0000\u0000+m. lstm units parameters t\u0000\u0000\u0000\u0000 calculating ifog total parameter size m\u0000+m. suppose parallel cells wide=n t\u0000\u0000\u0000\u0000\u0000\u0000 total \u0000\u0000\u0000\u0000 large wide=n reduce parameters .experiments present results language modeling machine translation part-of-speech tagging. language modeling task dataset conduct word-level word prediction experiments penn tree bank dataset. data tomas mikolov’s webpage. exactly training data test data researchers training testing. vocabulary size symbol rest rare words. model training details zaremba achieved state performance layer regularized lstm. time step word mapped length word vector lstm. lstm outputs probability distribution next word. models regularized lstm pcs. convenience comparison follow zaremba’s settings. lstms experiments layers unrolled steps. initial states final states previous batch initial states next batch. size batch cross entropy loss function stochastic gradient decent optimization. model parameters initialized uniformly train model learning rate first epochs apply weight decay rest epochs. clip norm gradients dropout recurrent connections avoid overfitting. results table shows results previous systems model. zaremba achieved perplexity state result language modeling significantly outperformed previous works. model hidden units. proposed model pure character input comparable perplexity model achieves perplexity wide= hidden units decreases perplexity state results. table show performance model different wide. wide= model plain layer lstm. wide= models similar performance. note wide= lstm cell contains parameters reduce perplexity plain lstm. however continue parallel cells harm performance. wide= perplexity increases suggest reduce unnecessary connections unrelated features. cell getting small afford necessary feature sets overall performance decrease. plain lstm setting units hidden states larger improve performance. perplexity grows however parallel cells still benefit increasing units hidden states. recurrent parameters size machine translation task dataset conduct experiments chinese-english translation task. purpose experiment evaluate whether substantial improvement large rnn. consideration speed include training testing samples sides contain less words. limit source target vocabularies frequent words chinese english including symbol words symbol padding sentence. training data translation task consists sentence pairs extracted corpora. development dataset nist dataset contains sentence pairs. test datasets nist datasets contains sentence pairs. case insensitive -gram nist bleu score evaluating translation task. model training details bahdanau proposed encoder-decoder model attention machine translation. encoder decoder rnns cell type gate recurrent units. encoder read source words final hidden states encoded information source sequences. hidden states decoder rnn. time step took states aligned source states previous generated target words inputs decoder output current target word. used bidirectional layers encoder catch forward backward information. used lstm cells translation. speed parallel running consideration used bidirectional first layer encoder. systems also comparable state systems. baseline model machine translation paper bahdanau’ work minus differences. first first layer encoder bidirectional. second cells used rnns lstm. implement encoder decoder rnn. bidirectional layer layers encoder share parameters. encoder contains layers bidirectional bottom layer single directional layers. decoder contains layers. word embedding dimension size hidden states cross entropy loss function stochastic gradient decent optimization. batch size train model initial learning rate apply weight decay every batches cost validation decrease. clip norm gradients dropout recurrent connections. train model batches. results table shows results baseline model model. baseline model takes batches best bleu score set. translation model parallel cells takes batches best bleu score. parallel cells translation model gets improvement bleu score. empirical study previous shown substantially improve performance less parameters. section make empirical study language modeling task show whether parallel cells work manner expected e.g. difference cells learning different features. also compare performance model average techniques. different cells different features unlike conditional random fields support vector machine neural network explicit features. moreover texts easy visualized pictures kind features extracted. result investigate indirect seeing result changes mask different cells. suppose predict word requires features offered cells. result mask cell prediction target word depends heavily features cell perplexity increase sharply vice versus. single layer lstm unit hidden states wide= dropout rate=. ptb. test mask parallel cells turns changes. figure shows mask cells. figure show three group samples selected test set. words selected frequency word set. seen similar words masked models similar performance perplexity. example consider copular verbs generally mask gets lowest average perplexity words mask gets almost highest perplexity. conclude predicting copular verbs cell provides less important features cell. figure results masked models. vertical axis shows average perplexity test set. horizon axis maski represents model masking cell comparison model averaging pcs-rnn hidden units parallel cells consumes much computing memory resource small naïve hidden units. thus want compare performance pcs-rnn counterpart ensemble models consumes resource. table compares results model averaging. model cost much resource model. however model outperform model model averaging. model achieve previous state perplexity model averaging. however outperform using model. conclude that model averaging still substantially outperforms naïve equivalent less computing memory resource. model parallel_cells_lstm_h_wide table parallel cells hidden units. model lstm-h table naïve lstm hidden units. model naïve lstm hidden states. data model model results reported zaremba al.. conclusion paper proposed parallel cells technique rnn. parallel cells reduce impact unrelated features coming recurrent connections. evaluation tasks models significantly improvement results less parameters. investigated results language modeling task masking cells. results show strong evidence different cells coping different features. also found even model averaging still beat baseline model state perplexity. much work done line pcs. example parallel cells different hidden units maybe better equally dividing strategy really arbitrary. also test performance many downstream models based rnn. mikolov karafiát burget cernocký khudanpur. recurrent neural network based language model. interspeech pages zaremba sutskever vinyals. recurrent neural network regularization. arxiv. luong manning minh-thang luong christopher manning. achieving open vocabulary neural machine translation hybrid word-character models. pages yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv.. hochreiter sepp schmidhuber jürgen. long short-term memory. neural computation kyunghyun bart merrienboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. emnlp pages lecun bottou bengio haffner. gradient-based learning applied document recognition. proceedings ieee burt attention mechanisms vision dynamic world. international conference pattern recognition. pages vol.. denil bazzani larochelle learning attend deep architectures image tracking. neural computation pages dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. iclr thang luong hieu pham christopher manning. effective approaches attention based neural machine translation. proceedings conference empirical methods natural language processing pages chung gulcehre empirical evaluation gated recurrent neural networks sequence modeling. eprint arxiv graves alex. generating sequences recurrent neural networks. arxiv preprint arxiv. yoon yacine jernite david sontag alexander rush. character-aware neural language models. aaai pages srivastava hinton krizhevsky dropout simple prevent neural networks overfitting. journal machine learning research kishore papineni salim roukos todd ward wei-jing zhu. bleu method automatic evaluation machine translation. hinton srivastava krizhevsky sutskever salakhutdinov. improving neural networks preventing coadaptation feature detectors. arxiv. michael collins. discriminative training methods hidden markov models theory experiments perceptron algorithms. proceedings emnlp pages stroudsburg usa. wang qian soong unified tagging solution bidirectional lstm recurrent neural network word embedding. arxiv preprint arxiv.", "year": 2017}