{"title": "Exact and Approximate Inference in Associative Hierarchical Networks  using Graph Cuts", "tag": ["cs.AI", "cs.CV"], "abstract": "Markov Networks are widely used through out computer vision and machine learning. An important subclass are the Associative Markov Networks which are used in a wide variety of applications. For these networks a good approximate minimum cost solution can be found efficiently using graph cut based move making algorithms such as alpha-expansion. Recently a related model has been proposed, the associative hierarchical network, which provides a natural generalisation of the Associative Markov Network for higher order cliques (i.e. clique size greater than two). This method provides a good model for object class segmentation problem in computer vision. Within this paper we briefly describe the associative hierarchical network and provide a computationally efficient method for approximate inference based on graph cuts. Our method performs well for networks containing hundreds of thousand of variables, and higher order potentials are defined over cliques containing tens of thousands of variables. Due to the size of these problems standard linear programming techniques are inapplicable. We show that our method has a bound of 4 for the solution of general associative hierarchical network with arbitrary clique size noting that few results on bounds exist for the solution of labelling of Markov Networks with higher order cliques.", "text": "markov networks widely used computer vision machine learning. important subclass associative markov networks used wide variety applications. networks good approximate minimum cost solution found eﬃciently using graph based move making algorithms alphaexpansion. recently related model proposed associative hierarchical network provides natural generalisation associative markov network higher order cliques method provides good model object class segmentation problem computer vision. within paper brieﬂy describe associative hierarchical network provide computationally eﬃcient method approximate inference based graph cuts. method performs well networks containing hundreds thousand variables higher order potentials deﬁned cliques containing tens thousands variables. size problems standard linear programming techniques inapplicable. show method bound solution general associative hierarchical network arbitrary clique size noting results bounds exist solution labelling markov networks higher order cliques. machine learning computer vision. interest large amount work problem estimating maximum posteriori solution random ﬁeld however research eﬀort focused inference pairwise markov networks. particular interest families associative pairwise potentials connected variables assumed likely share label. inference algorithms targeting associative potentials include truncated convex costs metrics semi metrics often carry bounds guarantee cost solution found must within bound speciﬁed ﬁxed factor cost minimal solution. although higher order markov networks used obtain impressive results number challenging problems computer vision problem bounded higher order inference largely ignored. paper address problem performing graph based inference model associative hierarchical networks includes higher order associative markov networks potentials robust model special cases derive bound family ahns successfully applied diverse problems object class recognition document classiﬁcation texture based video segmentation obtain state results. note earlier work ladicky problem inference discussed all; shows hierarchical models used scene understanding learning possible assumption model tractable. functions deﬁned cliques size unary potentials denote subscript denotes index variable potential deﬁned. similarly functions deﬁned cliques size referred pairwise points within paper want distinguish original variables energy function whose optimal values attempting auxiliary variables introduce convert higher order function pairwise one. refer original variables base layer auxiliary variables level hierarchy denoted indices variables constituting level hierarchy denote denoted existing higher-order models taskar proposed higher order potentials encourage entirety clique take label discusses applied predicting protein interactions document classiﬁcation. potentials introduced computer vision along eﬃcient graph based method inference strict potts model approach proposed generalisation kohli observed image labelling problem pixels belonging image segments computed using unsupervised clustering/segmentation algorithm take object label. proposed higher order segment based cliques. energy took form pairwise amns auxiliary variables. containing higher order cliques deﬁned terms also seen pairwise deﬁned terms propose move making algorithms pairwise energy important property transformational optimality. move making algorithms function eﬃciently searching candidate labellings proposing optimal candidate i.e. lowest energy move candidates updated algorithm repeats till convergence. experimentally transformationally optimal algorithms converge faster better solutions standard approaches α-expansion. moreover unlike standard approaches transformationally optimal algorithms always exact solution binary ahns. outline paper section introduce notation used rest paper. existing models generalised associative hierarchical network full deﬁnition ahns given section section discuss work eﬃcient inference show pairwise form associative hierarchical networks minimised using α-expansion algorithm derive bounds approach. section discusses application novel move making algorithms energies show formulation moves robust model become equivalent general form range moves unordered sets. derive transformational optimality results hierarchies potentials guaranteeing optimality moves proposed. experimentally verify eﬀectiveness approach methods section conclude section general formulation scheme described extended allowing pairwise higher order potentials deﬁned corresponds higher order potentials deﬁned layer higher order energy corresponding general hierarchical network written using following recursive function hierarchical formulation taskar’s kohli’s models understood mathematical convenience allows fast eﬃcient bounded inference earlier work used true multi-scale inference modelling constraints deﬁned many quantisations image. inference pairwise networks although problem inference np-hard associative pairwise functions deﬁned labels real world problems many conventional algorithms provide near optimal solutions grid connected networks however dense structure hierarchical networks results frustrated cycles makes traditional reparameterisation based message passing algorithms inference loopy belief propagation tree-reweighted message passing slow converge unsuitable many frustrated cycles eliminated cycle inequalities signiﬁcantly increasing time algorithm. graph based move making algorithms suffer problem successfully used minimising pairwise functions deﬁned densely connected networks encountered vision. potentials understood truncated majority voting scheme base layer. possible encourage entirety clique assume consistent labelling. however beyond certain threshold disagreement implicitly recognise consistent labelling likely occur penalty paid increasing heterogeneity. demonstrate higher order potentials robust model represented equivalent pairwise function deﬁned level hierarchical network addition single auxiliary variable every clique auxiliary variable take values extended label {lf} taking approach analogous factor graphs adding single multi-state auxiliary variable. however general higher order functions requires addition auxiliary variable exponential sized label fortunately class higher order potentials concerned compactly described ahns auxiliary variables take similar sized label base layer permitting fast inference. examples move making algorithms include αexpansion applied metrics swap applied semi-metrics range moves truncated convex potentials. moves diﬀer size space searched optimal move. expansion swap search space size minimising function variables range moves explores much larger space parameter energy details). move making approaches swap directly applied associative hierarchical networks term metric truncated convex. methods start arbitrary initial solution problem proceed making series changes leads solution lower energy step algorithms project candidate moves boolean space along energy function. resulting projected energy function submodular pairwise exactly minimised polynomial time solving equivalent st-mincut problem. optima mapped back original space returning optimal move within move set. move algorithms procedure convergence iteratively picking best candidate diﬀerent choices range cycled through. minimising higher order functions number researchers worked problem inference higher order amns. proposed approximation methods make eﬃcient inference possible higher order mrfs. followed recent works potetz tarlow showed belief propagation eﬃciently performed networks containing moderately large cliques. however methods based quite slow took minutes hours converge lack bounds. perform inference models kohli ﬁrst showed certain projection higher order model transformed submodular pairwise functions containing auxiliary variables. used formulate higher order expansion swap move making algorithms existing work addresses problem bounded higher order inference showed theoretical bounds could derived given move making algorithms proposed optimal moves exactly solving sub-problem. application used approximate moves exactly solve sub-problems proposed. consequentially bounds derive hold methods propose. however analysis applied model inference techniques propose optimal moves bounds compare results. label {lf}. true original variables base hierarchy take label artiﬁcially augment label label associate inﬁnite unary cost secondly make inter-layer pairwise potentials symmetric performing local reparameterisation operation. provides bound higher order potentials strict model largest clique network. using approach bounds possible general class robust models associative hierarchical networks. consider generalisation swap expansion moves proposed boykov standard swap move moves considered subset variables currently taking label change labels either range-swap moves considered allow variables taking labels change state similarly normal expansion move allows variable change state range expansion allows variable change states deﬁned move algorithm gives improved solutions hierarchical energy function used formulating object segmentation problem. improve upon algorithm. novel construction computing optimal moves explained following section based upon original energy function strong transformational optimality property. ﬁrst xaux∗ represents labelling auxiliary variables hierarchy. note move proposed transformationally optimal algorithms minimises original higher order energy show applied hierarchical networks range moves transformationally optimal. move optimality guarantee transformational optimality need constrain higher order potentials. consider clique associated auxiliary variable labelling labelling diﬀers variable takes label clique potential hierarchically consistent satisﬁes constraint property hierarchical consistency also required computer vision cost associated hierarchy remain meaningful. labelling auxiliary variable within hierarchy reﬂected state clique associated energy hierarchically consistent possible optimal labelling regions hierarchy reﬂect labelling base layer. hierarchy consistent. given labelling base layer hierarchy optimal labelling auxiliary variable associated clique must labels induction choice labelling clique must also decision labels describe construction three label range move hierarchical network show section reasonable assumptions methods equivalent swap expansion move exactly minimises equivalent higher order energy deﬁned base variables hierarchical network explain construction submodular quadratic pseudo boolean move function range expansion. construction swap based move function derived range move. equal expressible submodular potential. this create function deﬁned variables associate states taking state current state state prohibit state incorporating pairwise term proof follows inspection function. note cost always ﬁrst terms take cost third term cost associated unless similarly cost associated unless also takes label note variants unordered range moves guaranteed global optima label space contains states. case standard forms expansion swap auxiliary variables take three states. swap based optimality requires additional constraint namely pairwise connections variables level hierarchy except base layer. auxiliary variable take label cost associated taking label dependent upon weighted average child variables taking state remains constant. hence clique variables optimal labelling swap currently taking state transform states range moves exactly transformations move proposed must transformationally optimal consequently best possible swap energy case range-expansion move maintain transformational optimality incorporating pairwise connections hierarchy provided condition holds energy exactly represented submodular moves. evaluate α-expansion swap trw-s belief propagation iterated conditional modes expansion swap based variants unordered range moves problem object class segmentation msrc data-set pixel within image must assigned label representing class grass water boat cow. express problem three layer hierarchy. pixel represented random variables base layer. second layer formed performing multiple unsupervised segmentations image associating auxiliary variable segment. children variables variables contained within segment pairwise connections formed adjacent segments. third layer formed manner second layer clustering image segments. details given ladicky tested algorithm test images average pixels/variables base layer variables clique either convergence maximum iterations. table ﬁgure compare ﬁnal energies obtained algorithm showing number times achieved energy lower equal average ratio e/e. empirically message passing algorithms trw-s appear ill-suited inference dense hierarchical networks. comparison graph based move making algorithms higher resulting energy higher memory usage exhibited slower convergence. appear unreasonable test message passing approaches hierarchical energies higher order formulations exist note simplest hierarchy contains additional layer nodes pairwise connections second layer higher order hierarchical message-passing approaches equivalent inference trees represent higher order potentials exact. similar relative performance message passing schemes observed cases. further application approaches general form would require computation exact min-marginals diﬃcult problem itself. obr´azek left typical behaviour methods along lower bound obtained trw-s image msrc data set. dashed lines right graph represent ﬁnal converged solutions. right comparison methods testing images. left right columns show number times achieved best energy average paper α-expansion reparameterisation section transformationally optimal range expansion swap signiﬁcantly outperformed existing inference methods speed accuracy. supplementary materials examples. formed trw-s icm. later methods obtained minimal cost labellings images optimal solution found contained label i.e. entirely labelled grass water. comparison also shows unordered range move variants usually outperform vanilla move making algorithms. higher number minimal labellings found range-move variant swap comparison vanilla α-expansion explained large number images labels strongly dominate unlike standard α-expansion range move algorithms guaranteed global optima label sub-problem typical behaviour methods alongside lower bound trw-s seen ﬁgure further alongside qualitative results supplementary materials. paper shows higher order amns intimately related pairwise hierarchical networks. observation allowed characterise higher order potentials solved novel reparameterisation using conventional move making expansion swap algorithms derive bounds approaches. also gave transformationally optimal family algorithms performing eﬃcient inference higher order inherits bounds.", "year": 2012}