{"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "cs.NE", "physics.soc-ph"], "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.", "text": "encountering novel objects humans able infer wide range physical properties mass friction deformability interacting goal driven way. process active interaction spirit scientist performing experiments discover hidden facts. recent advances artiﬁcial intelligence yielded machines achieve superhuman performance atari natural language processing complex control problems; however clear systems rival scientiﬁc intuition even young child. work introduce basic tasks require agents estimate properties mass cohesion objects interactive simulated environment manipulate objects observe consequences. found deep reinforcement learning methods learn perform experiments necessary discover hidden properties. systematically manipulating problem difﬁculty cost incurred agent performing experiments found agents learn different strategies balance cost gathering information cost making mistakes different situations. also compare learned experimentation policies randomized baselines show learned policies lead better predictions. work inspired empirical ﬁndings theories psychology indicating infant learning thinking similar adult scientists important view developmental science babies endowed small number separable systems core knowledge reasoning objects actions number space possibly social interactions object core system covering aspects cohesion continuity contact enables babies animals solve object related tasks reasoning oclusion predicting objects behave. core knowledge research motivated development methods endow agents physics priors perception modules infer intrinsic physical properties rapidly data instance using physics engines mental simulation becomes possible infer quantities mass visual input early stages life infants spend time interacting objects seemingly random manner interact objects multiple ways including throwing pushing pulling breaking biting. quite possible process actively engaging objects watching consequences actions helps infants understand different physical properties object cannot observed directly using sensory systems. seems infants series physical experiments enhance knowledge world performing experiment useful quickly adapting agent’s policy environment understanding object properties holistic manner. despite impressive advances artiﬁcial intelligence superhuman performance atari natural language processing still unclear systems behind advances rival scientiﬁc intuition even small child. draw inspiration child development must emphasized purpose provide account learning thinking humans rather explore similar types understanding might learned artiﬁcial agents grounded way. show build agents learn experiment learn representations informative physical properties objects using deep reinforcement learning. conducting experiment involves agent belief world updates observing consequences actions performs. investigate ability agents learn perform experiments infer object properties environments—which heavier towers. heavier environment agent able apply forces blocks must infer blocks heaviest. towers environment agent’s task infer many rigid bodies tower composed knocking down. unlike assume agent prior knowledge physical properties objects laws physics hence must interact objects order learn answer questions properties. results indicate case heavier environment agents learn experimentation strategies similar would expect algorithm designed knowledge underlying structure environment. towers environment show agents learn closed loop policy adapt varying time scale. environments show using learned interaction policies agents accurate often take less time produce correct answers following randomized interaction policies. unusual paper present model propose algorithm. reinforcement learning task core experiments algorithm models solve many existing approaches expected perform equally well substituted setting. paper step towards agents understand objects intuitive reasoning physical worlds. best agents currently fail simple control tasks simple games montezuma’s revenge look screen ladder skull don’t immediately know keys open doors skulls probably hazardous best avoided ladders allow defy gravity etc. understanding physics relations objects enables children solve seemingly simple problems best existing agents come close begin solve. endowing agents knowledge objects would help enormously planning reasoning exploration trivial. object? turns question straightforward answer paper based around idea staring thing enough understand children understand world engaging poking something soft tasting discover delicious hitting falls down. much knowledge people world result interaction. vision open loop perception alone enough. paper introduces tasks evaluate ability agents learn hidden properties objects. requires environments tasks depend properties also probe understanding agents complete tasks. previous approaches problem relied either explicit knowledge underlying structure environment exploiting correlations material appearance physical properties contributions paper show agents still learn properties objects even connection material appearance physical properties broken. setting allows show agents merely learning blocks heavy; learning check blocks heavy. none previous approaches give complete account agents could come understand physical properties world around them. specifying model manually difﬁcult scale generalize ground perception. making predictions visual properties fail distinguish objects look similar certainly unable distinguish sack full rocks sack full tennis balls. pose problem experimentation answering questions non-visual properties objects present environment. design environments questions properties providing rewards agent able infer correctly train agents answer questions using reinforcement learning. reward agent produces labeling action environment responds reward positive correct answer negative incorrect episode terminates. episode terminates automatically negative reward agent produce labeling action maximum time limit reached. crucially transition interaction labeling happen ﬁxed time initiated agent. achieved providing agent ability produce either interaction action labeling action every time step. allows agent decide enough information gathered forces balance trade-off answering given current knowledge delaying answer gather information. optimal trade-off information gathering risk answering incorrectly depends factors. ﬁrst factor difﬁculty question second cost information. difﬁculty environment speciﬁc addressed later describe environments. cost information generically controlled varying discount factor learning. small discount factor places less emphasis future rewards encourages agent answer quickly possible. hand large discount factor encourages agent spend time gathering information order increase likelihood choosing correct answer. questions answers differs terms used elsewhere literature. sutton talk value function question agent provides answer form approximation value. answer incorporates agent’s knowledge match actual value agent’s approximation grounds means knowledge accurate. usage environment question answers come form labeling actions. episode correct answer whose semantics grounded sign reward function accuracy agents knowledge assessed frequency able choose correct answer. using reward ground semantics means straightforward questions depend agent’s behavior. example easily question which block heaviest? without making question contingent particular information acquisition strategy. basic agent architecture training procedure experiments making minimal modiﬁcations order adapt agents different observation spaces actuators. experiments train recurrent agents using lstm hidden units. working features feed observations lstm directly. training pixels ﬁrst scale observations pixels feed three convolution layers figure left diagram heavier environment. blocks always arranged line mass different blocks changes episode episode. right mass distributions different settings used experiments. followed relu non-linearity. three layers square ﬁlters sizes applied strides respectively. train agents using asynchronous advantage actor critic ensure unroll length always greater timeout length agent network unrolled entirety episode. heavier environment designed question relative masses different objects scene. assign masses objects uncorrelated appearance order ensure task solvable without interaction. environment diagrammed left panel figure consists four blocks constrained move vertically. blocks always size vary mass episodes. agent’s strength remains constant episodes. question answer environment four blocks heaviest. since mass block randomly assigned episode agent must poke blocks observe respond order make determination. assigning masses randomly ensures possible solve task vision alone since appearance identity block imparts information mass current episode. obtain information masses blocks interact watch respond. heavier environment designed encode latent bandit problem physical lens. block corresponds bandit reward obtained pulling proportional mass block. identifying heaviest block seen best identiﬁcation problem best identiﬁcation well studied problem experimental design understanding optimal solution latent bandit behave used guide analysis agents train task. important emphasize cannot simply apply standard bandit algorithms here impose much higher level prior ignorance algorithms setting allows. bandit algorithms assume rewards observed directly whereas agents observe mass role dynamics maintain bandit setting could imagine parameterizing transformation reward observation perhaps even learning mapping well; however requires explicitly acknowledging mapping design learning algorithm avoid doing. moreover acknowledging mapping requires a-priori recognition existence latent bandit structure. perspective learning algorithm mere existence structure also lies beyond veil ignorance. figure learning curves typical agent trained heavier environment varying difﬁculty settings. y-axes show probability agent producing correct answer episode times out. plot shows agents started random seeds identical hyperparameter settings. light lines show learning curves individual agents dark lines show median performance across displayed runs difﬁculty. left agents trained features. right agents trained pixels. following scheme controlling difﬁculty heavier environment. first select blocks uniformly random heavy block designate remaining three light blocks. sample mass heavy block beta mass light blocks beta. single parameter effectively controls distribution mass gaps large values leading easier problems. figure shows distribution mass gaps three values experiments. distinguish problem level instance level difﬁculty domain. instance level difﬁculty refers size mass single episode. mass small harder determine block heaviest episode difﬁcult another comparing mass gaps. problem level difﬁculty refers shape generating distribution mass gaps distribution puts mass conﬁgurations small mass tend generate episodes difﬁcult instance level distribution difﬁcult another likely generate instances small mass gaps. control problem level difﬁculty incorporate problem instance level difﬁculty analysis. obvious choice actuation physical domains kind hand based manipulator. however controlling hand quite challenging requiring fair amount dexterity part agent. manipulation problem interesting right orthogonal goals work. therefore avoid problem learning dexterous manipulation providing agent much simpler form actuation. call actuation strategy environment direct actuation allows agent affect forces different blocks directly. every time step agent output eight possible actions. ﬁrst four actions result application vertical force ﬁxed magnitude center mass four blocks respectively. remaining actions labeling actions correspond agent’s selection heaviest block. ﬁrst experiment sanity check show train agents successfully heavier environment using features pixels. experiment designed simply show task solvable illustrate changing problem difﬁculty make task hard. present additional experiments showing varying difﬁculty leads differentiated behavior problem level instance level. cases knowledge latent bandit problem allows make predictions experimenting agent behave experiments designed show qualitatively correct behavior obtained agents spite a-priori ignorance underlying bandit problem. show increase problem difﬁculty learned policies transition guessing immediately heavy block found strongly preferring poke blocks making decision. corresponds observation unlikely give high reward high reward likely best. also observe agents adapt behavior difﬁculty individual problem instances. show single agent tend spend longer gathering information particular problem instance difﬁcult. corresponds observation best arms similar reward information required accurately distinguish them. finally conduct experiment comparing learned information gathering policies randomized baseline method. experiment shows agents reliably produce correct label following learned interaction policies observing environment driven random actions. success learning experiment trained several agents three different difﬁculties corresponding problem difﬁculty trained agents feature observations includes coordinate four blocks; also using pixels providing pixel rendering scene agent. representative learning curves condition shown figure curves smoothed time show running estimate probability success rather showing reward directly. agents reach perfect performance task difﬁcult problems plateauing progressively lower performance. explained looking distributions instance level difﬁculties generated different settings shown right panel figure higher difﬁculties substantial probability generating problem instances mass near makes distinguishing heaviest blocks difﬁcult. population strategy differentiation experiment trained agents three different difﬁculties corresponding using discount factor corresponds relatively high cost gathering information. trained three agents difﬁculty show results aggregated across different replicas. training agent steps conditions exposed training. record number length episodes executed testing period well outcome episode. episodes terminated timeout steps vast majority episodes terminated steps agent producing label. since episodes vary length agents complete number episodes testing. left plot figure shows histograms episode lengths broken task difﬁculty. dashed vertical line indicates episode length four interaction steps minimum number actions required agents interact every block. task difﬁculty agents appear learn simply search single heavy block however task difﬁculty strong bias away terminating episode taking least four exploratory actions. figure left histograms episode lengths different task difﬁculty settings. transition agents answer eagerly soon heavy block agents conservative answering acted enough poke blocks least once. right episode lengths function normalized mass gap. units x-axes scaled range possible masses y-axis shows number steps agent takes labeling action. black dots show individual episodes line shows linear trend error bars show histogram estimate standard deviations. plot shows testing episodes single trained agent. training agent steps conditions exposed training. record length episode well mass heaviest blocks episode. distribution mass gaps measure task difﬁculty mass single episode measure difﬁculty speciﬁc problem instance. exclude analysis small proportion episodes terminate timeout. right plots figure show relationship mass episode length across testing runs different agents. plots single agent learned adapt behavior based difﬁculty single problem instance. although variance high clear correlation mass length episodes. behavior reﬂects would expect solution latent bandit problem; information required identify best second best nearly good. randomized interaction experiment trained several agents using feature pixel observations three task difﬁculties discount total trained sets agents experiment. training agent steps conditions used training. record outcome episode well number steps taken agent chooses label. agent repeat experiment using agent’s learned interaction policy well randomized interaction policy. randomized interaction policy obtained follows step agent chooses candidate action using learned policy. candidate action labeling action passed environment unchanged however candidate action interaction action replace agent action interaction action chosen uniformly random available action set. following randomized interaction policy agent control information gathering process still controls episode ends label chosen. figure compares learned interaction policies randomized interaction baselines. results show effect episode length small consistent bias towards longer shorter episodes across difﬁculties observation types. however learned interaction policies produce accurate labels across permutations. figure comparison agents heavier environment following learned interaction policies randomized interaction policy baseline. x-axes show difﬁcultyobservation combinations left episode lengths gathering information using different interaction policies. right probability choosing correct label different conditions dashed line shows chance performance. towers environment designed agents count number cohesive rigid bodies scene. environment designed initial conﬁguration possible determine number rigid bodies vision features alone. environment diagrammed left panel figure consists tower blocks move freely three dimensions. initial block tower always conﬁguration episode bolt together different subsets blocks form larger rigid bodies shown ﬁgure. question answer environment many rigid bodies formed primitive blocks. since blocks bound together randomly assigned episode binding forces invisible agent must poke tower observe falls order determine many rigid bodies composed parameterize environment distribution number separate blocks tower uniform. ensures single action strategy achieves high reward. towers environment used actuators direct actuation similar heavier environment; actuator described below. case direct actuation agent output actions. every time step agent apply force ﬁxed magnitude either direction blocks. blocks glued together blocks move effect force. towers blocks results different possible actions. remaining actions labeling actions used agent indicate number distinct blocks tower. large spherical object agent actuate setting velocities horizontal plane. unlike direct actuation agent cannot apply direct forces objects constitute tower manipulate pushing hitting ﬁst. every time step agent output nine actions. ﬁrst four actions corresponds setting velocity constant amount directions respectively. remaining actions labeling actions used agent indicate number distinct blocks tower. order investigate agent learns strategy stopping ﬁxed number time steps whether integrates sensory information non-trivial manner used notion control time step. idea control time step similar action repeats physics simulation time step control time step means action repeated times. direct actuators episode timeout steps actuator types. figure example trajectory block tower knocked using actuator. left diagram hidden structure towers environment. tower left composed blocks could decompose rigid objects several ways distinguished interacting tower. right behavior single trained agent using actuators varying control time step. x-axis shows different control time step lengths blue line shows probability agent correctly identifying number blocks. line shows median episode length error bars showing conﬁdence intervals computed episodes. shaded region shows control time step around median. second experiment shows agents learn wait observation identify number rigid bodies producing answer. designed show agents closed loop strategy counting number rigid bodies. alternative hypothesis would agents learn wait number steps time take best guess. third experiment compares learned policy randomized interaction policy shows agents able determine correct number blocks tower quickly reliably using learned policy gather information. success learning experiment trained several agents towers environment using different pairings actuators perception. features observations include position primitive block training using pixels provide pixel rendering scene agent observation. figure shows learning curves combination actuator observation type. cases obtain agents solve task nearly perfectly although training pixels range hyperparameters train successfully narrower training features. interestingly actuators lead fastest learning spite fact agent must manipulate blocks indirectly ﬁst. possible explanation affect multiple blocks action step whereas direct actuation block affected time step. waiting information experiment trained agent pixel observations actuator towers task control time step seconds examine behavior test time smaller delay actions. reducing control time step means agent perspective time slowed down. moving ﬁxed amount distance takes longer waiting block tower collapse hit. training agent steps range different control time steps. record outcome episode well number steps taken agent chooses label. none test episodes terminate timeout include analysis. plot figure shows probability answering correctly well median length episode measured seconds. terms absolute performance small drop compared training setting agent essentially perfect agent performance remains good even substantially smaller control timesteps used training. figure learning curves agents trained towers environment different conditions. y-axes show probability agent producing correct answer episode times out. different plots show different pairings observations actuators indicated plot titles. plot shows runs random seeds identical hyper-parameter settings. black lines show learning curves individual agents lines show median performance displayed runs. also observe episodes different time steps take approximate amount real time across majority tested range. corresponds large change episode length measured number agent actions since control time step agent must execute many actions cover amount real time compared control time step used training. infer agent learned wait informative observation producing label opposed simpler degenerate strategy waiting ﬁxed amount steps answering. randomized interaction experiment trained several agents combination actuator observation type examine behavior observing environment driven random interaction policy. randomized interaction policy identical randomized baseline used heavier environment. training agent steps. record outcome episode well number steps taken agent chooses label. agent repeat experiment using agent’s learned interaction policy well randomized interaction policy. figure compares learned interaction policies randomized interaction baselines. results show agents tend produce labels quickly following learned interaction policies also labels produce much accurate. deep learning techniques conjunction vast labeled datasets yielded powerful models image classiﬁcation speech recognition recent years approached human level performance tasks strong interest computer vision ﬁeld moving beyond semantic classiﬁcation tasks require deeper nuanced understanding world. inspired developmental studies recent works focused learning representations predicting physical embodiment quantities ego-motion instead symbolic labels. extending realm things-to-be-predicted include quantities beyond class labels viewer centric parameters poses humans within scene shown improve quality feature learning scene understanding. researchers looked cross modal learning example synthesizing sounds visual images using summary statistics audio learn features object recognition image colorization inverting prediction tower another line work focused learning visual world synthesizing rather analyzing images. major cornerstones recent work area include variational autoencoders kingma welling generative adversarial networks figure comparison agents towers environment following learned interaction policies randomized interaction policy baseline. x-axes show different observationactuator combinations left episode lengths gathering information using different interaction policies. right probability choosing correct label different conditions dashed line shows chance performance. building models single image synthesis many works predicting evolution video frames time approached problem designing variational autoencoder architecture uses latent stochastic units make choices direction motion objects generates future frames conditioned choices. different form uncertainty video prediction arise effect actions taken agent. environments deterministic dynamics accurate action-conditional predictions future frames made introducing actions prediction process amounts learning latent forward dynamics model exploited plan actions achieve novel goals works frame synthesis plays role regularizer preventing collapse feature space dynamics model lives. agrawal break dependency frame synthesis dynamics learning replacing frame synthesis inverse dynamics model. forward model plays role earlier works feature space collapse prevented ensuring model decode actions pairs time-adjacent images. several works including agrawal assael mentioned also pinto pinto gupta levine gone coupling feature learning dynamics. learned dynamics models used control learning also learning process order collect data targeted shown improve speed quality learning robot manipulation tasks. challenge learning dynamics collecting appropriate data. ingenious solution import real world data physics engine simulate application forces order generate ground truth data. approach taken mottaghi generate interactable data scenes generate static data image force pairs along ground truth trajectory target object response application indicated force. purpose learning intuitive understanding dynamics possible interesting work entirely synthetic data lerer show convolutional networks learn make judgments stability synthetic block towers based single image tower. also show model trained synthetic data able generalize make accurate judgments photographs similar block towers built real world. making intuitive judgments block towers extensively studied psychophysics literature. substantial evidence connecting behavior human judgments inference explicit latent physics model humans infer mass watching movies complex rigid body dynamics major component line work analysis synthesis understanding physical process obtained learning invert observations assumed generated explicitly parameterized generative model true physical process provide constraints inference process parameters model. analysis synthesis approach extremely inﬂuential power explain human judgments generalization patterns variety situations galileo particularly relevant instance tying together analysis synthesis deep learning understanding dynamics. system ﬁrst infers physical parameters variety blocks watching videos sliding slopes colliding blocks. stage system uses off-the-shelf object tracker ground inference parameters physical simulator inference achieved matching simulated observed block trajectories. inferred physical parameters used train deep network predict physical parameters initial frame video. test time system evaluated using deep network infer physical parameters blocks physics engine used answer questions behaviors observed training time. physics extension galileo fully embraces deep learning. instead using ﬁrst pass analysis synthesis infer physical parameters based observations deep network trained regress output object tracker directly relevant physical laws encoded directly architecture model. authors show latent intrinsic physical properties inferred make novel predictions. approach encoding physical models architecture constraints also proposed stewart ermon many works discussed thus including galileo physics restricted passive sensing. pinto pinto gupta agrawal levine exceptions learn models using sequential greedy data collection bootstrapping strategy. active sensing appears important aspect visual object learning toddlers argued bambach providing motivation approach presented here. computer vision well known recognition performance improved moving acquire views object scene. jayaraman grauman example apply deep reinforcement learning construct agent chooses acquire views object classify semantic category related work section surveys many efforts active vision. jayaraman grauman others share deep reinforcement learning active sensing common work goal learn policy applied images make decisions based vision. contrast goal paper study agents learn experiment continually learn representations answer questions intrinsic properties objects. particular focus tasks solved interaction vision alone. despite recent advances artiﬁcial intelligence machines still lack common sense understanding physical world. impressive progress recognizing objects segmenting object boundaries even describing visual scenes natural language. however tasks enough machines infer physical properties objects mass friction deformability. introduce deep reinforcement learning agent actively interacts physical objects infer hidden properties. approach inspired ﬁndings developmental psychology literature indicating infants spend early time experimenting objects random exploration letting agents conduct physical experiments interactive simulated environment learn manipulate objects observe consequences infer hidden object properties. demonstrate efﬁcacy approach important physical understanding tasks—inferring mass counting number objects strong visual ambiguities. empirical ﬁndings suggest agents learn different strategies tasks balance cost gathering information cost making mistakes different situations. scientists children able probe environment discover things also leverage ﬁndings answer questions. paper shown agents trained gather knowledge answer questions hidden properties addressed larger issue theory building transfer information. given agents make judgments mass numerosity enticed leverage knowledge solve tasks? another important aspect understanding interaction shape interactions inﬂuences behavior. touched towers environment looked different actuation styles much done here. thinking along lines leads naturally exploring tool use. showed agents make judgments object mass hitting them could train agent make similar judgments using scale? would like thank matt hoffman several enlightening discussions bandits. would also like thank iclr reviewers whose helpful feedback allowed greatly improve paper. john-alexander assael niklas wahlstr¨om thomas sch¨on marc peter deisenroth. dataefﬁcient learning feedback policies image pixels using deep dynamical models. arxiv preprint arxiv. vincent delaitre david fouhey ivan laptev josef sivic abhinav gupta alexei efros. scene semantics long-term observation people. european conference computer vision springer carl doersch abhinav gupta alexei efros. unsupervised visual representation learning context prediction. proceedings ieee international conference computer vision david fouhey vincent delaitre abhinav gupta alexei efros ivan laptev josef sivic. people watching human actions single view geometry. international journal computer vision goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems jessica hamrick peter battaglia joshua tenenbaum. internal physics models guide probabilistic judgments object dynamics. proceedings annual conference cognitive science society mary hegarty. mechanical reasoning mental simulation. trends cognitive sciences geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury. deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. neural information processing systems andrew owens jiajun josh mcdermott william freeman antonio torralba. ambient sound provides supervision visual learning. european conference computer vision springer marcaurelio ranzato arthur szlam joan bruna michael mathieu ronan collobert sumit chopra. video modeling baseline generative models natural videos. arxiv preprint arxiv. elizabeth spelke katherine kinzler. core knowledge. developmental science nitish srivastava elman mansimov ruslan salakhutdinov. unsupervised learning video representations richard sutton joseph modayil michael delp thomas degris patrick pilarski adam white doina precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. international conference autonomous agents multiagent systems-volume international foundation autonomous agents multiagent systems manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems", "year": 2016}