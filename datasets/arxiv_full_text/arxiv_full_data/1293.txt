{"title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods.", "text": "comparison heterogeneous samples extensively exists many applications especially task image classiﬁcation. paper propose simple effective coupled neural network called deeply coupled autoencoder networks seeks build deep neural networks coupled every corresponding layers. dcan deep structure developed stacking multiple discriminative coupled auto-encoders denoising auto-encoder trained maximum margin criterion consisting intra-class compactness inter-class penalty. single layer component makes model simultaneously preserve local consistency enhance discriminative capability. increasing number layers coupled networks gradually narrow views. extensive experiments cross-view image classiﬁcation tasks demonstrate superiority method state-of-the-art methods. real-world objects often different views might endowed semantic. example face images captured different poses reveal identity object; images face also different modalities pictures different lighting condition pose even sketches artists. many computer vision applications image retrieval interests taken comparing types heterogeneous images come different views even different sensors. since spanned feature spaces quite different difﬁcult classify images across views directly. decrease discrepancy across views previous works endeavored learn view-speciﬁc linear transforms project cross-view samples common latent space employed newly generated features classiﬁcation. though lots approaches used learn viewspeciﬁc projections divided roughly based whether supervised information used. unsupervised methods canonical correlation analysis partial least square employed task cross-view recognition. attempt linear mappings project samples common space correlation maximized considers variations rather correlation target space. besides mutual information coupled informationtheoretic encoding method developed narrow inter-view speciﬁc photo-sketch recognition task. semi-coupled dictionary used bridge views. methods consider reduce discrepancy views however label information explicitly taken account. label information available many methods further developed learn discriminant common space instance discriminative canonical correlation analysis proposed extension cca. additional local smoothness constraints linear projections simultaneously learnt common discriminant feature extraction also methods large margin approach coupled spectral regression recently multi-view analysis developed jointly learn multiple speciﬁc-view transforms multiple views available. although methods extensively applied cross-view problem encouraging performances employed linear transforms capture shared features samples views. however linear discriminant analysis methods usually depend assumption data class agrees gaussian distribution data real world usually much complex distribution indicates linear transforms insufﬁcient extract common features cross-view images. it’s natural consider learning nonlinear features. recent topic interest nonlinear learning research deep learning. deep learning attempts learn nonlinear representations hierarchically deep structures applied successfully many computer vision problems. classical deep learning methods often stack compose multiple basic building blocks yield deeper structure. recent review deep learning algorithms. lots basic building blocks proposed including sparse coding restricted boltzmann machine autoencoder etc. speciﬁcally autoencoder shown effectiveness image denoising domain adaptation audio-visual speech classiﬁcation etc. known kernel method kernel canonical correlation analysis also widely used approach learn nonlinear representations. compared kernel method deep learning much ﬂexible time-saving transform learned rather ﬁxed time needed training inference process beyond limit size training set. inspired deep learning works above intend solve cross-view classiﬁcation task deep networks. it’s natural build single deep neural network samples views kind network can’t handle complex data totally different modalities suffer inadequate representation capacity. anway learn different deep neural networks samples different views. however independent networks project samples different views different spaces makes comparison infeasible. hence building neural networks coupled seems better solution. work propose deeply coupled autoencoder networks method learns common representations conduct cross-view classiﬁcation building neural networks deeply coupled respectively view. build dcan stacking multiple discriminative coupled auto-encoders denoising auto-encoder maximum margin criterion. discriminative coupled auto-encoder similar input corrupted reconstructive error minimized mechanism denoising auto-encoder proposed modiﬁed adding maximum margin criterion. kind criterion used previous works like etc. note counterparts views added maximum margin criterion simultaneously since come class naturally couples corresponding layer deep networks. schematic illustration seen fig.. proposed dcan related multimodal autoencoders multimodal restricted boltzmann machines deep canonical correlation analysis ﬁrst methods tend learn single network layers connected views predict view view deep canonical correlation analysis build deep networks view representations highest layer constrained correlated. therefore difference learn deep networks coupled representations layer great beneﬁts dcan learn separate deep encodings also makes better data views. what’s more differences allow model handle recognition task even data impure insufﬁcient. rest paper organized follows. section details formulation solution proposed deeply coupled auto-encoder networks. experimental results section demonstrate efﬁcacy dcan. section conclusion given. section ﬁrst present basic idea. second part gives detailed description discriminative coupled auto-encoder. then describe stack multiple layers build deep network. finally brieﬂy describe optimization model. shown fig. deeply coupled auto-encoder networks consists deep networks coupled other view. network structures deep networks like left-most right-most parts fig. circles means units layers arrows denote full connections adjacent layers. middle part fig. illustrates whole network projects samples different views common space gradually enhances separability increasing layers. deep networks built stacking multiple similar coupled single layer blocks single coupled layer might insufﬁcient method stacking multiple layers training layer greedily proved efﬁcient lots previous works number layers increased whole network compactly represent signiﬁcantly larger transforms shallow networks gradually narrow discriminative capacity enhanced. discriminative coupled auto-encoders trained maximum margin criterion single layer component. concretely incorporate additional noises training process maximizing margin criterion makes learnt mapping stable well discriminant. note maximum margin criterion also works coupling corresponding layers. formally discriminative coupled auto-encoder written follows denote inputs views denote hidden representations views respectively. transforms intend learn denote reconstructive error maximum margin criterion described detailedly next subsection.ε threshold maximum margin criterion. problem cross-view types heterogenous samples. without loss generality denote samples view view sample sizes. noted corresponding labels known denote hidden representations views want learn. dcan attempts learn nonlinear transforms project samples views discriminant common space respectively local neighborhood relationship well class separability well preserved view. auto-encoder like structure stands preserving local consistency denoising form enhances robustness learnt representations. however discrimination isn’t taken consideration. therefore modify denoising autoencoder adding maximum margin criterion consisting intra-class compactness inter-class penalty. best nonlinear transformation trade-off between local consistency preserving separability enhancing. structive error formulated follows figure illustration proposed dcan. left-most right-most schematic show structure coupled network respectively. schematic middle illustrates whole network gradually enhances separability increasing layers pictures solid line border denote samples view dotted line border denote samples view different colors imply different subjects. calculates expectation corrupted versions examples obtained corruption process speciﬁes nonlinear transforms weight matrix bias encoder decoder respectively calculated decoder process moreover maximum margin criterion consisting intra-class compactness inter-class penalty constraint term used realize coupling since samples class treated similarly matter view from. function illustrated middel part fig.. projected common space denoted compactness term shown ellipse works pulling intra-class samples together penalty term shown black ellipse tend push adjacent inter-class samples away. training process above model original sample space preliminary discriminant subspace eliminated build hidden representation trade-off approximate preservation local consistency distinction projected data. since real-world data highly complicated using single coupled layer model vast complex real scenes might insufﬁcient. choose stack multiple coupled network layers described subsection number layers increased whole network compactly represent signiﬁcantly larger transforms shallow networks gradually narrow discriminative ability enhanced. training deep network coupled nonlinear transforms achieved canonical greedy layer-wise approach precise training single layer coupled network compute feature encoder feed next layer network input feature. practice stacking multiple layers gradually reduce improve recognition performance poses expression ﬂush illumination sessions selected validate method. randomly choose images pose subject randomly partition data parts training subjects testing rest subjects. cuhk face sketch feret dataset contains types face images photo sketch. total images collected lighting variations feret dataset subject sketch drawn shape exaggeration. according conﬁguration ﬁrst subjects training data rest subjects testing data. images multi-pie cufsf cropped pixels without preprocess. compare proposed dcan method several baselines stateof-the-art methods including kernel deep cdfe mvda ﬁrst seven methods pairwise methods cross-view classiﬁcation. mvda jointly learns transforms multiple views utilized achieved state-of-the-art results reports principal component analysis used dimension reduction. experiments default dimensionality preservation energy except deep cdfe dimensionality tuned best performance. methods report best performance tuning related parameters according papers. firstly kernel experiment gaussian kernel polynomial kernel adjust parameters best performance. deep strictly follow algorithms tune possible parameters performance inferior cca. possible reason deep considers correlations training data learnt mode overly training data thus leads poor generality testing set. besides parameter respectively traversed cdfe parameter ﬁrst term reconstruction error second term maximum margin criterion last term shrinkage constraints called tikhonov regularizers utilized decrease magnitude weights help prevent over-ﬁtting. balance parameter local consistency empirical separability. called weight decay parameter usually small value e.g. .e-. optimize objective function backpropagation calculate gradient employ limited-memory bfgs method often used solve nonlinear optimization problems without constraints. l-bfgs particularly suitable problems large amount variables moderate memory requirement. utilize l-bfgs need calculate gradients object function. obviously object function differential parameters back-propagation method derive derivative overall cost function. setting objective function achieve fast convergence described performance cufsf database varied parameters shown fig.. following experiments increasing layers number hidden neurons gradually reduced i.e. four layers. first explicitly illustrate learnt mapping conduct experiment multi-pie dataset projecting learnt common features space principal component analysis shown fig.. classical method roughly align data principal directions state-of-the-art method mvda attempts merge types data seems fail. thus argue linear transforms little stiff convert data views ideal common space. three diagrams shows dcan gradually separate samples different classes increase layers described analysis. figure learning common features crossview methods project features space using principal components pca. depicted samples randomly chosen form multi-pie dataset. points come views respectively. different color points belong different classes. dcan-k proposed method stacked k-layer neural network. multi-pie data set. since images acquired seven poses multi-pie data total comparison experiments need conducted. detailed results shown table where poses used gallery probe rank- recognition rate reported. further mean accuracy pairwise results methods also reported table table supervised methods except signiﬁcantly superior label information. nonlinear methods except deep signiﬁcantly superior nonlinear methods nonlinear transforms. compared proposed dcan layer network perform better improvement. increasing layers accuracy dcan reaches climax stacking three layer networks. reason degradation dcan four layers mainly effect reduced dimensionality dimensions layer network. obvitable results cdfe mvda dcan multipie dataset terms rank- recognition rate. dcan-k means stacked k-layer network. space limitation results methods cannot reported here mean accuracies shown table figure performance varied parameter values proposed dcan. sketch photo images cufsf respectively used gallery probe set. varied ﬁxed varied ﬁxed ously compared two-view based methods proposed dcan three layers improves performance greatly besides mvda also achieves considerably good performance using samples poses. unfair compare two-view based methods mvda latter implicitly uses additional views information except current compared views. method performs better mvda observed table three-layer dcan achieves largely improvement compared ccafdacdfe cross-view cases mvda cross-view cases. results shown table table photo-sketch recognition conducted cufsf dataset. samples come views photo sketch. comparison results provided table shown table since views utilized case mvda degrades comparable performance previous two-view based methods. proposed dcan three layer networks achieve even better improvement indicates dcan beneﬁts nonlinear multi-layer structure. neighborhood structures. this alain theoretically prove learnt representation auto-encoder recover local properties view manifold. further validate that employ ﬁrst photo images cufsf database perform nonlinear self-reconstruction auto-encoder. hidden presentations local neighbors neighbors preserved probability respectively. thus auto-encoder intrinsically reduces complexity discriminant model makes learnt model better generality testing set. deep structure generates gradual model makes learnt transform robust. layer model can’t represent complex data well. layers goes deeper coupled networks learn transforms much ﬂexible hence allowed handle complex data. paper propose deep learning method deeply coupled auto-encoder networks gradually generate coupled discriminant common representation cross-view object classiﬁcation. layer take local consistency discrimination projected data consideration. stacking multiple coupled network layers dcan gradually improve learnt shared features common space. moreover experiments cross-view classiﬁcation tasks demonstrate superior method state-of-the-art methods.", "year": 2014}