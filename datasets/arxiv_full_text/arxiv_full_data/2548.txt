{"title": "Easy Monotonic Policy Iteration", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A key problem in reinforcement learning for control with general function approximators (such as deep neural networks and other nonlinear functions) is that, for many algorithms employed in practice, updates to the policy or $Q$-function may fail to improve performance---or worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the lower bound on policy improvement, a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework.", "text": "problem reinforcement learning control general function approximators that many algorithms employed practice updates policy q-function fail improve performance—or worse actually cause policy performance degrade. prior work addressed policy iteration deriving tight policy improvement bounds; optimizing lower bound policy improvement better policy guaranteed. however existing approaches suffer bounds hard optimize practice include norm terms cannot efﬁciently estimated differentiated. work derive better policy improvement bound norm policy divergence replaced average divergence; leads algorithm easy monotonic policy iteration generates sequences policies guaranteed non-decreasing returns easy implement sample-based framework. following success deep q-network approach surge interest using reinforcement learning control nonlinear function approximators particularly deep neural networks; policy q-function represented approximator. examples include variants double-dqn deep recurrent q-learning well methods neural network policies neural network value functions asynchronous advantage actor-critic algorithm however despite empirical successes algorithms come theoretical guarantees continued policy improvement training policies represented arbitrary nonlinear function approximators. approach serves inspiration work seeks maximize lower bound policy performance guarantee improvement. method roots conservative policy iteration extended separately pirotta schulman pirotta schulman derived similar policy improvement bounds consisting parts expected advantage policy respect policy penalty divergence policy policy. divergence penalty cases quite steep involves maximum policy divergence states. makes particularly difﬁcult apply bounds usual situations function approximation desirable domains model—and hence total states—is unknown and/or state space large pirotta developed algorithms primarily apply case model known approximation present advantage estimation address issue. schulman addressed issue proposing solve approximate form problem maximum divergence penalty replaced trust-region constraint average divergence; algorithm called trust region policy optimization found trpo worked quite well number domains successfully optimizing neural network policies play atari pixels control simulated robot locomotion tasks. work derive policy improvement bound penalty policy divergence goes average rather maximum divergence. allows propose easy monotonic policy iteration algorithm exploits bound generate sequences policies guaranteed non-decreasing returns easy implement sample-based framework. also enables give theoretical justiﬁcation trpo able show iteration trpo worst-case degradation policy performance depends hyperparameter algorithm. contributions present entirely theoretical empirical results testing empi appear future version work. next examine useful properties become apparent vector form ﬁnite state spaces. r|s| denote vector components r|s|×|s| denote transition matrix components identity nice reasons. first pick approximator value function relates true discounted return policy estimate policy return onpolicy average td-error approximator; aesthetically satisfying. second shows reward-shaping effect translating total discounted return es∼µ] ﬁxed constant independent policy; illustrates ﬁnding reward shaping change optimal policy. markov decision process tuple states actions reward function s×a×s transition probability function probability transitioning state given previous state agent took action starting state distribution. policy distribution actions state probability selecting state consider problem picking policy maximizes expected inﬁnite-horizon discounted total reward discount factor denotes trajectory shorthand indicating trajectories drawn distributions induced deﬁne on-policy value function action-value function advantage function usual lemma makes many ideas explored before; special case strategy leads directly policy improvement bounds previously obtained pirotta schulman form given general however allows freedom choosing remark. maximizes lower bound here. turns −es∼dπa∼πhaπ′ exactly equals thus recover exact equality. practically useful provides insight penalty coefﬁcient divergence captures information mismatch quick observations connect result prior work. clearly could bound expectation es∼dπ maxs this picking bounding second factor maxs recover policy improvement bounds given pirotta corollary schulman theorem weaker versions bound bound allows generate sequence policies nondegrading performance restricted class policies i.e. policies represented neural networks function approximators. we’ll denote arbitrary restricted class policies understand usually means policies smoothly parametrized parameters algorithm gives general template easy monotonic policy iteration obtains monotonic improvements using empi indeed monotonic observe feasible point optimization deﬁning objective value certiﬁcate objective optimum although specify solve optimization problem note objective differentiable respect parameters candidate policy result gradient-based method used. furthermore neural network policies vector parameters take value optimization unconstrained. sense consider algorithm ‘easy’ implement. usually would interested applying method problems large state unknown state spaces exact calculation objective function feasible. objective deﬁned almost entirely terms expectations policy however samplebased approximation sense algorithm ‘easy.’ challenge estimatcorollary tells theoretically justiﬁed using advantage estimators long increase penalty policy divergence appropriately. also estimator high-quality small) take larger policy improvement steps makes sense. advantage estimator poor probably make much progress; reﬂected bound. practical form general algorithm obtained choosing particular form reward-shaping functions possibly approximating terms objective replacing expectations objective sample estimates. we’ll choose base optimization problem becomes suppose estimator advantage instead true advantage example learned neural network value function approximator perhaps might generalized advantage estimator observe that ea∼π] every state iteration algorithm exploits bound generate sequences policies guaranteed non-decreasing returns easy implement sample-based framework. showed implement empi theoretically justiﬁed advantage estimators optimization inner loop. lastly showed policy improvement bound gives theoretical foundation trust region policy optimization algorithm approximate monotonic policy improvements proposed schulman shown perform well empirically wide variety tasks; here able bound worst-case performance iteration trpo. future version work give experimental results implementing empi range reinforcement learning benchmarks including high-dimensional domains like atari.", "year": 2016}