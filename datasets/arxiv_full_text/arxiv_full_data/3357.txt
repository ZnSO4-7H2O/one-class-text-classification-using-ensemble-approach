{"title": "On the Modeling of Error Functions as High Dimensional Landscapes for  Weight Initialization in Learning Networks", "tag": ["cs.LG", "cs.CV", "physics.data-an", "stat.ML"], "abstract": "Next generation deep neural networks for classification hosted on embedded platforms will rely on fast, efficient, and accurate learning algorithms. Initialization of weights in learning networks has a great impact on the classification accuracy. In this paper we focus on deriving good initial weights by modeling the error function of a deep neural network as a high-dimensional landscape. We observe that due to the inherent complexity in its algebraic structure, such an error function may conform to general results of the statistics of large systems. To this end we apply some results from Random Matrix Theory to analyse these functions. We model the error function in terms of a Hamiltonian in N-dimensions and derive some theoretical results about its general behavior. These results are further used to make better initial guesses of weights for the learning algorithm.", "text": "learning input data mixture labelled unlabelled samples. desired prediction problem model must learn structures organize data well make predictions. neural networks major developments ﬁeld machine learning. popularity substantial learning capacity adaptability various application domains. building blocks called neurons processing nodes. nodes arranged layers make network. layers called input hidden output layer based function visibility programmer. layers interconnected synaptic links associated synaptic weights. pictorial representation neuron feed forward neural network shown fig. fig. respectively. training refers tuning synaptic weights implement given function. function computed neuron where dimension input sample. element input sample weight vector respectively. weight associated bias input shown fig. differentiable non-linear function. popular non-linear functions used sigmoid tanh relu etc. training neuron samples training database associated labels used train synaptic weights. tuning weights performed minimize error function function difference predicted output actual output given abstract—next generation deep neural networks classiﬁcation hosted embedded platforms rely fast efﬁcient accurate learning algorithms. initialization weights learning networks great impact classiﬁcation accuracy. paper focus deriving good initial weights modeling error function deep neural network high-dimensional landscape. observe inherent complexity algebraic structure error function conform general results statistics large systems. apply results random matrix theory analyse functions. model error function terms hamiltonian n-dimensions derive theoretical results general behavior. results used make better initial guesses weights learning algorithm. machine learning discipline correlations drawn samples used adaptive algorithms extract critical relevant information helps classiﬁcation. interplay formulations learning models primal/dual spaces great impact theoretical analysis practical implementation system emerging embedded platforms play host variety classiﬁcation systems applications. clearly demand performance efﬁciency accuracy machine learning system paramount interest. learning supervised unsupervised even combination two. face recognition systems instance machine learning typically supervised since trained using sample faces. whereas data analytics machine learning unsupervised since priori knowledge features/information associated data. terminology machine learning classiﬁcation considered instance supervised learning i.e. learning training correctly identiﬁed observations available unsupervised learning model prepared deducing structures present input data project general rules. could mean identifying mathematical method/process data organization systematically reduces redundancy. corresponding unsupervised procedure known clustering involves grouping data categories based measure inherent similarity distance. semi-supervised gradient error function. comes training multi-layer feed forward gradient descent involves layer-wise computation gradients tuning weights accordingly. method popularly known back propagation. train multi-layer using back propagation main design parameters chosen. first learning rate visualized step size search minima error landscape. training dynamic update learning rate shown perform better learning practical examples. second important design parameter chosen initial synaptic weights start back propagation. initialized weights shown affect number iterations required convergence along classiﬁcation performance trained network weight initialization methods xavier method nguyen-widrow method used deep learning frameworks like caffe matlab neural network toolbox faster efﬁcient learning. majority deep learning frameworks employ stochastic gradient descent algorithms arrive optimal weight vector accurate classiﬁcation. deep neural networks recently emerged area interest researchers ﬁeld machine learning. strength lies multiple layers neurons together capable representing large complex functions. although numerous applications training dnns always challenge large number layers. addition deep networks witnessed problem vanishing gradients encouraged researchers explore better methods initializations. good weight initialization shown play crucial role achieving better minima faster learning dnns. therefore need better weight initialization methods play major role training emerging deep neural networks. paper explore statistical methods random matrix theory large systems apply concepts explore high dimensional landscapes error functions fast learning. methods applied problems physics study complex energy levels heavy nuclei ﬁnancial analytics stock correlations communication theory wireless systems array signal processing ﬁrst time method applied learning systems. rest paper organized follows. section give brief introduction applicability learning systems context prior work. refer analysis results literature basis development approach faster learning. section describe approach applies problem learning neural networks. section contains analysis different parameters improve learning ability network along related results support theory. conclude section lately random matrix theory applied effectively various ﬁelds science engineering fact little knowledge sufﬁcient application encouraged considerable research work towards exploring applicability different application domains. wigner’s semicircle stated follows consider ensemble real symmetric matrices independent identically distributed random variables ﬁxed probability distribution mean variance moments ﬁnite. problems regarding motion particle highdimensional landscape occur throughout physics various disciplines spin-glass theory string theory theory supercooled liquids etc. dynamics system n-dimensional potential described functional form potential interest. high-dimensional landscape characterized stationary points. stationary points points landscape particle moving equilibrium. stationary points gradient potential vanishes. probability ﬁnding local minima given equivalent ﬁnding probability maximal eigenvalue λmax study maximal eigenvalue random matrix thus appreciable interest disciplines study high-dimensional landscapes. tracy widom showed distribution maximal eigenvalue ensemble random matrices given given eigenvalue equation limits integration entire real line. equations manifests integral possible minima consider ﬁnite limits integral equation equations would mean we’re searching minima range deﬁned limits. thus deﬁned n-dimensional hypercube dimension random matrix consideration. tracy widom thus provides powerful analytical tool study minima large random landscape main subject paper. fyodorov state ﬁnding total number stationary points spatial domain random landscape difﬁcult efﬁcient techniques available perform task. however possible perform calculation gaussian ﬁelds isotropic covariance structure i.e. covariance dependent euclidean distances estimation stationary points spatial domain equivalent evaluating mean density eigenvalues gaussian orthogonal ensemble real random matrices well established closed form expression landscape using stochastic method based followed standard stochastic gradient descent method learning. believe approach/solution indeed based general phenomena ﬁnding minima high dimension error landscape conjectured lecun authors suggest method second-derivatives intrinsically overcomes issues pathological curvature optimization. also show pretrained initialization beneﬁt problem underﬁtting. however method provides probabilistic guarantees nature minima optimization converges would lead believe process weight initialization improve efﬁciency optimization algorithm used training neural networks. section model error function random landscape deﬁned equation minima high dimensional landscape optimal initial weights gradient descent propose method based rmt. compute weights using theory used initialize synaptic weights prior back propagation. consider application face recognition analysis justiﬁcation approach. neural network trained identify input images categories/classes training database. detailed description given initially consider model single layer neurons input nodes nclass output nodes dimension input sample nclass number classes/categories training database. simplicity initially consider linear activation function output nodes. link connecting input node output node associated synaptic weight wij. output output node given layer-sequential unit-variance initialization method weight initialization connection standard stochastic gradient descent proposed leads state-of-the-art thin deep neural nets. experimental validation authors establish proposed initialization leads learning deep nets produces networks test accuracy better equal standard methods comparable complex schemes fitnets highway authors propose greedy layer-wise unsupervised training strategy deep multi-layer neural networks targets initializing weights region near good local minimum giving rise internal distributed representations high-level abstractions input resulting better generalization. authors draw parallel machine learning problems deep networks mean ﬁeld spherical spin glass model terms ﬁnding ground state energy mean ﬁeld system’s hamiltonian though reality systems mathematically analogous. hint systems special cases general phenomenon discovered. experiments teacher-student networks mnist dataset authors report gradient descent proposed stochastic gradient descent methods equally efﬁcient identical number steps. eigenvalue spectrum arbitrary random matrix. quantum theory treats physical observables operators. energy physical system corresponds result action hamiltonian operator system. quantum theory provides formalism hamiltonian system realized hermitian matrix. thus ﬁnding energy system boils solving eigenvalue problem hamiltonian matrix. since hamiltonian large complicated inﬁnite dimensional matrix makes sense talk statistically. provides necessary mathematical tools pixel associated gaussian values mean. consider mean pixels image. high resolution images lots pixels. therefore mean pixel must also conform gaussian c.l.t. redeﬁne origin coordinates number provided means variance succeeded producing independent mean-zero gaussian random variables. observe introduced constraint requiring means variance. ensure constraint considering similar images i.e. images belonging class similar images deﬁnition don’t ‘vary’ much other. would like impress constraint doesn’t effect generality results. choose train output node neural network independently. finding minima high dimensional error landscape minima error function equation look analogy error function dimensional g.o.e. equation representing terms ﬁrst term equation random function write n-dimensional hamiltonian form tuning parameter n-dimensional random landscape covariance given equation indeed complex function approximated mean-zero gaussiandistributed ﬁeld. assume follows distribution helps ﬁnding minima error landscape. training samples face images rotation translation invariant assume covariance matrix also isotropic. conﬁrmed basic requirements apply method described section approach compute weight vectors connecting output node independently using samples corresponding class. thus compute nclass n-dimensional landscape minima train network. equation conclude mean number energy minima function train output node required solution value maximum mean number minima indicating presence optimum point. looking equation function covariance function. last term equation logarithm derivative solution painlev´e equation observed negligible compared difference ﬁrst terms large therefore equation behaves critical point every dimension representing point majority minima. gradient computed covariance matrix times square root diagonal elements assigned respective dimensions. equations observed training nclass output nodes independent other. target arrive favourable initial synaptic weights gradient descent believe weights computed training output node samples corresponding class. ﬁrst term r.h.s equation quadratic terms r.h.s complicated terms observe always deﬁne transformation n-dimensions. true coordinates represent different ways looking landscape. deﬁned ‘position-ﬁeld’ whereas deﬁned ‘transform-ﬁeld’. thus error function quadratic term complicated ndimensional function variables since already n-dimensions dealing error function complicated terms consider applying following trick. replace complicated part error functionej random function n-dimensions mean-zero gaussian random variables. assumption allows apply tools random matrix theory analyze error function discuss section iii-b assumption begs following question know coordinates indeed meanzero gaussian variables? look statistics answer query. represents pixel face image dataset. given large database central limit theorem pixel takes gaussian values. answers gaussian part let’s consider mean-zero part. average number minima spatial domain integrated range dimension. randomly selected vector hypercube shows high density minima give solution vector implies initialization weights would sufﬁce dimension becomes maximum. fig. shows three dimensional case cube constructed based dimension. finally computed scaled range input pixels synaptic weights. section analyse method implementation perspective select parameters achieve accurate robust classiﬁcation. addition extend work multi-layer weight initialization promising results. functional veriﬁcation based weight initialization method face image databases extended yale database database consists classes containing face images. extended yale database consists classes face images. images preprocessed described section iv-a using weight computation. matlab tool verify effectiveness weights. default matlab tool uses nguyen-widrow weight initialization method conjugate gradient descent training weights. apply weights computed using based method network training analyse compare performance terms number epochs convergence ﬁnal recognition accuracy different recognition accuracies number epochs tool multiple times initialization weights. therefore experiment multiple times report arithmetic mean maximum values paper. obtained performing principle component analysis test samples faces. eigenspace deﬁned faces serves reduce dimension inputs also helps deﬁning error landscape g.o.e. principal components/axes. also possible carry feature extraction following random matrix theory approach reported dimension reduction. resultant feature vectors mean centred divided standard deviation make mean-zero unit variance. section iii-b clear critical parameter responsible variations ﬁxed three dimensional plot fig. shows variation different values components different dimensions becomes maximum based ratio. fig. shows plot indices maximum varying plot saturates beyond point computed values values linear region plot cover whole query region landscape effective weight computation. therefore select tuning parameter ratio falls required range. established theory model network single layer neurons. comes training network large face database complex features single layer neurons sufﬁcient achieve required classiﬁcation performance. make theory applicable conditions extend theory multi-layer networks requires computation weights. multi-layer neural networks synaptic weights associated neurons hidden layers identify patterns input samples. patterns used collectively classiﬁcation subsequent layers. back propagation weights tune identifying pattern input samples. behaviour hidden nodes help extending theory multi-layer networks. here similar model bias inputs layers. multi-layer networks application method straight forward. method relies class-wise computation weights needs equal number nodes hidden layer output layer shown fig. number nodes input hidden node respectively. initially apply method ﬁrst layer neurons compute weight matrix dimension analogous training ﬁrst layer hidden node samples respective class. look like unorthodox training believe help achieve better weight initialization. weight matrix ﬁrst layer computed compute output ﬁrst hidden layer multiplying input sample nsamp sigmoid operation. although theory include sigmoid unit neuron outputs experimentally observed computed weights performed well sigmoid units included network. computed outputs dimension nsamp×n ﬁrst hidden layer inputs second layer associated labels. similarly compute synaptic weights rest hidden layers output layer. previously model synaptic weights neuron computed considering samples respective class. however method extended multi-layer brings indirect dependency improve classiﬁcation performance. e.g. ﬁrst node second layer gets inputs nodes previous layer thus dependent samples used nodes computing weights. plots fig. compare recognition accuracy number epochs convergence weight initialization initialization method used matlab tool. maximum average values number runs comparison. fig. fig. observe large number classes average method weight initialization resulted better recognition accuracy. addition difference recognition accuracy method standard methods increases number classes. finding better minima error landscape result additional steps weight updates observed fig. fig. multi-layer network described limitation hidden output layer must nclass number nodes. practical networks valid scenario number nodes layer differs based training database available resources. addition inputs large dimensions small number classes network cannot represent classiﬁcation function efﬁciently. address issue using method clustering algorithm introduced layer. k-means clustering unsupervised clustering algorithm experiments. samples clustered algorithm based similarity target number clusters. apply method set-up consider cluster single class. believe valid assumption samples cluster similar adjacency feature space. addition hidden similar patterns samples different classes explicitly brought together neuron. fig. shows varying number nodes layer. performance multi-cluster network classes database different methods weight initializations shown using plots fig. fig. observe based initialization method outperforms methods terms recognition accuracy. present work focuses ﬁnding initial weights multi-layered shallow networks. theory applies deeper networks convolution networks target future work. shallow networks analyzed performance algorithm respect number epochs recognition accuracy face recognition application. although shown performance single application standard face databases believe algorithm behaves similarly recognition applications database obeys conditions necessary applicability mentioned section iii. natural image recognition problems fall category. observed based weight initialization method results better recognition accuracy compared standard weight initializations. although relatively standard initializations lower complexity investment additional computations brings returns higher recognition accuracy. computations involved ﬁnding initial weights layer using covariance computation) search maximum value number nodes layer equal number classes additional computations involve k-means clustering k=number clusters). intend replace k-means clustering relatively simpler clustering method future work. case large databases typically training performed off-line. learning systems hosted multi-core platforms latent parallelism training algorithm. additional computations required based weight initialization also sufﬁciently parallelized platforms hence signiﬁcantly impact overall training time. internet things devices on-line training size database much smaller. cases better recognition accuracy achieved method trade-off cost additional computations. best possible initialization choose empirically optimal values µ/µc. presents uneasy situation suffers lack theoretical validation. however following schematic provides insight choices works. existing literature talks phase transition energy landscape hamiltonian phase exponential number minima phase minima. phase transition region characterised sub-exponential number minima. conjecture fig. nature density minima different values case landscape characterized exponential number minima. parabola deﬁned small value tends broad. randomness potential therefore tends produce minima. case parabola sharper number minima sub-exponential. phase transition region deﬁned choice therefore search minima landscape likely true minima. opposed case exponential number minima gradient decent leads converge minima. believe paper presents developing robust neural networks. authors plan validate conjecture immediate future. accurate classiﬁcation goal multi-layer large neural network. initialization weights signiﬁcant impact convergence learning algorithms. provided statistical method based random matrix theory weight initialization probabilistic guarantees nature minima reached high dimension landscape deﬁned error function. experimentally substantiated novelty method obtaining higher classiﬁcation accuracy well known initialization methods adopted deep learning frameworks. glorot bengio understanding difﬁculty training deep feedforward neural networks proceedings international conference artiﬁcial intelligence statistics society artiﬁcial intelligence statistics nguyen widrow improving learning speed -layer neural networks choosing initial values adaptive weights international joint conference neural networks shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture international conference multimedia york available http//doi.acm.org/./. hochreiter vanishing gradient problem learning recurrent neural nets problem solutions int. uncertain. fuzziness knowl.-based syst. vol. apr. available http//dx.doi.org/./s division commission conference neutron physics time-of-ﬂight held gatlinburg tennessee november ridge national laboratory available https//books.google.co.in/books?id=khwfgvneekc fyodorov nadal critical behavior number minima random landscape glass transition point tracy-widom distribution phys. rev. lett. vol. available http//link.aps.org/doi/./physrevlett. srivastava greff schmidhuber training deep networks advances neural information processing systems cortes lawrence sugiyama garnett eds. curran associates inc. available http//papers.nips.cc/paper/-training-very-deep-networks.pdf bengio lamblin popovici larochelle greedy layerwise training deep networks advances neural information processing systems sch¨olkopf platt hoffman eds. press available http//papers.nips. cc/paper/-greedy-layer-wise-training-of-deep-networks.pdf martens deep learning hessian-free optimization. mahale mahale nandy ranjani refresh redeﬁne face recognition using sure homogeneous cores ieee transaction parallel distributed systems", "year": 2016}