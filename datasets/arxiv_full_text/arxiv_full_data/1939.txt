{"title": "Learning by Asking Questions", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.", "text": "figure learning-by-asking paradigm. present open-world visual question answering setting agent interactively learns asking questions oracle. unlike standard training assumes ﬁxed dataset questions agent potential learn quickly asking good questions much like bright student class. alter test-time setup vqa. by-asking training time learner receives images decides questions ask. questions asked learner answered oracle test-time evaluated exactly like using well understood metrics. interactive nature requires learner construct meta-knowledge knows select supervision needs. successful facilitates sample efﬁcient learning using ﬁxed dataset learner redundant questions. explore proposed paradigm context clevr dataset artiﬁcial universe number unique objects attributes relations limited. synthetic setting little prior work asking questions images clevr allows perform controlled study algorithms needed asking questions. hope transfer insights obtained study real-world setting. building interactive learner questions challenging task. first learner needs language model form questions. second needs understand input image ensure question relevant introduce interactive learning framework development testing intelligent visual systems called learning-by-asking explore context visual question answering task. differs standard training questions observed training time learner must questions wants answers thus closely mimics natural learning potential dataefﬁcient traditional setting. present model performs clevr dataset show automatically discovers easy-to-hard curriculum learning interactively oracle. generated data consistently matches outperforms clevr train data sample efﬁcient. also show model asks questions generalize state-of-the-art models novel test time distributions. machine learning models remarkable progress visual recognition. however training data models crucially important typically treated predetermined static information. current models passive nature rely training data curated humans control supervision. stark contrast humans learn interacting environment gain information. interactive nature human learning makes sample efﬁcient also yields learning curriculum paper argue next-generation recognition systems need agency ability decide information need explore context visual question answering instead training ﬁxed large-scale dataset propose alternative interactive setup called learningcoherent. finally order sample efﬁcient learner able evaluate knowledge questions help learn information world. supervision learner receives interaction answer questions poses. present study model combines ideas visually grounded language generation curriculum learning vqa. speciﬁcally develop epsilon-greedy learner asks questions uses corresponding answers train standard model. learner focuses mastering concepts rapidly improve upon moving types questions. demonstrate model asks meaningful questions also matches performance human-curated data. model also sample efﬁcient interactively asking questions reduces number training samples needed obtain baseline question-answering accuracy related work visual question answering surrogate task designed assess system’s ability thoroughly understand images. gained popularity recent years release several benchmark datasets motivated well-studied difﬁculty analyzing results real-world datasets johnson recently proposed controlled synthetic dataset adopt work. current approaches follow traditional supervised learning paradigm. large number image-questionanswer triples collected subset data randomly selected training. learning-by-asking uses alternative challenging setting training images drawn distribution learner decides question needs learn most. learner receives answer level supervision interactions. must learn formulate questions well model knowledge remove redundancy question-asking. also potential generalize open-world scenarios. also signiﬁcant progress building models using lstms convolutional networks stacked attention networks module networks relational networks others independent backbone model used existing architecture. visual question generation recently proposed alternative image captioning work related sense require learner generate questions images however objective different. whereas focuses asking questions relevant image content requires learner questions relevant figure examples invalid questions images clevr universe. even syntactically correct questions invalid variety reasons referring absent objects incorrect object properties invalid relationships scene ambiguous etc. informative learner answered. positive side effect circumvents difﬁculty evaluating quality generated questions question-answering accuracy ﬁnal model directly correlates quality questions asked. evaluation also used recent works language community active learning involves collection unlabeled examples learner selects samples labeled oracle common selection criteria include entropy boosting margin classiﬁers expected informativeness setting different traditional settings multiple ways. first unlike agent selects image labeled agent selects image generates question. second instead asking single image level label setting allows richer questions objects relationships etc. single image. simple predeﬁned template questions templates offer limited expressiveness rigid query structure. approach questions generated learned language model. expressive language models like used work likely necessary generalizing real-world settings. however also introduce challenge many ways generate invalid questions learner must learn discard exploratory learning centers settings acquire supervision agent explores environment studied context among others computer games navigation multi-user games inverse kinematics motion planning humanoids exploratory learning problems generally framed reinforcement learning agent receives rewards used learn policy maximizes expected rewards. difference setting sparse delayed rewards. contextual multi-armed bandits anclass reinforcement learning algorithms closely resemble setting. however unlike bandits clevr universe oracle program interpreter uses ground-truth scene information produce answers. oracle understands questions form programs question proposal answering modules represent questions programs. however unlike exploit prior knowledge clevr programming language modules; instead treated simple means required communicate oracle. supplementary material examples programs details oracle. model asks invalid question oracle returns special answer indicating question invalid whether objects appear question present image. question proposal module question proposal module aims generate diverse questions relevant given image. found training single model meet requirements resulted limited diversity questions. thus employ subcomponents question generation model produces questions question relevance model predicts whether generated question relevant image figure shows examples irrelevant questions need ﬁltered question generation relevance models used repeatedly produce question proposals question generation model imagecaptioning model uses lstm conditioned image features generate question. increase diversity generated questions also condition lstm question type training speciﬁcally ﬁrst sample question type qtype uniformly random sample question lstm using beam size sampling temperature image ﬁlter questions previously answered oracle. question relevance model takes questions generator input ﬁlters irrelevant questions construct question proposals special answer provided oracle whenever invalid question asked serves online learning signal relevance model. speciﬁcally model trained predict whether imagequestion pair valid whether objects mentioned question present image. questions predictions positive setting. denote image assume exists possible questions possible answers training time learner receives input training images dtrain sampled distribution ptrain; access oracle outputs answer given question image small bootstrap tuples denoted binit. learner receives budget answers request oracle. using oracle consultations learner aims construct function predicts score answer question image small bootstrap provided learner initialize various model components; show experiments training binit alone yields poor results. challenge setting implies that training time learner must decide question image supervision oracle provides answers. number oracle requests constrained budget learner must questions maximize learning signal image-question pair sent oracle. test time assume standard setting evaluate models question-answering accuracy. agent receives input pairs images questions dtest sampled distribution ptest. images test sampled distribution trainq∈q ptest ptrain. agent’s goal maximize proportion test questions answers correctly maximize propose agent built three modules question proposal module generates question proposals input image; question answering module predicts answers pairs; question selection module looks answering module’s state proposal module’s questions pick single question oracle. receiving oracle’s answer agent creates tuple used online learning signal figure approach learning-by-asking setting vqa. given image agent generates diverse questions using question generator ﬁlters irrelevant questions using relevance model produce list question proposals. agent answers questions using model predicted answers self-knowledge past performance selects question proposals answered oracle. oracle provides answer-level supervision agent learns informative questions subsequent iterations. expected accuracy improvement informativeness value learner uses pick question helps improve rapidly particular selection policy uses informativeness scores select question oracle using epsilon-greedy policy greedy part selection policy implemented argmaxqp∈qp encourage exploration. empirically policy automatically discovers easy-to-hard curriculum experiments whenever st−∆ training phases model trained three phases initialization phase generation relevance models pre-trained small bootstrap binit tuples; online learning-by-asking phase model learns interactively asking questions updates ofﬂine phase model voﬄine trained scratch union bootstrap tuples obtained querying oracle online phase. online training phase. step phase proposal module picks image training dtrain uniformly random. generates relevant question proposals image. answering module tries answer question proposal. selection module uses state answering module along answer distributions obtained evaluating answering module pick informative question question proposal set. question answering module standard model learns predict answer given image-question pair answering module trained online using supervision signal oracle. requirement selecting good questions oracle model’s capability self-evaluate current state. capture state model round keeping track model’s questionanswering accuracy answer training data obtained far. state captures information answering module already knows; used question selection module. question selection module deﬁnes policy selects informative question oracle question proposals select informative question question selection module uses current state answering module difﬁculty question proposals. quantities obtained state beliefs current model image-question pair respectively. state contains information current knowledge answering module. difference state values current round past round measures fast answering module improving answer. inspired curriculum learning difference select questions answering module improve fastest. speciﬁcally compute expected accuracy improvement anquestion asked oracle provides answer generate training example training example used perform single gradient step parameters answering module relevance model language generation model remains ﬁxed oracle provide direct learning signal process repeated training budget oracle answer requests exhausted. ofﬂine training phase. evaluate quality asked questions training model voﬄine scratch union bootstrap binit tuples generated phase. ofﬂine training model leads slightly improved question-answering accuracy reduces variance. lstm hidden units. linear projection image features ﬁrst hidden state. input discrete variable representing question type ﬁrst token lstm starting generation. following preﬁx-tree program representation questions. implement relevance model model using stacked attention network architecture using implementation modiﬁcation make concatenate spatial coordinates image features computing attention share weights generate invalid pairs bootstrapping relevance model permute pairs bootstrap binit assume permuted pairs invalid. note bootstrap special answer indicating whether invalid questions objects present image answers obtained online phase. models image features resnet- pre-trained imagenet conv_ layer network. adam ﬁxed learning rate optimize models. additional implementation details presented supplementary material. experiments datasets. evaluate approach clevr universe provides training images tuples. tuples bootstrap binit. evaluate quality data collected measuring questionanswering accuracy ﬁnal model voﬄine clevr validation set. clevr train identical answer questiontype distributions gives models trained clevr train inherent advantage. thus also measure figure clevr accuracy models trained clevr train lba-generated data bottom accuracy clevr-humans models. shaded regions denote standard deviation accuracy. clevr-humans sample efﬁcient clevr train. question-answering accuracy clevr-humans dataset different distribution; figure models. unless stated otherwise stacked attention model answering module evaluate three different choices ﬁnal ofﬂine model voﬄine cnn+lstm encodes image using question using lstm predicts answers using mlp. cnn+lstm+sa extends cnn+lstm stacked attention model described section default answering module film uses question features modulate image features layer. unless stated otherwise cnn+lstm+sa models ablation analysis experiments even though lower performance film trains much faster voﬄine models training hyperparameters respective papers. table clevr accuracy budgets condition generator image image question type vary generator sampling temperatures three different relevance models. re-run pipeline settings. analysis question proposal module analyzing generator evaluate diversity generated questions looking distribution corresponding answers. figure ﬁnal model generate questions image training set. plot histogram answers questions generators without question type conditioning. histogram shows conditioning generator question type leads better coverage answer space. also note generated questions invalid programming language syntax. observe rows table increased question diversity translates improved question-answering accuracy. diversity also controlled sampling temperature used rows show lower temperature gives less diverse question proposals negatively impacts ﬁnal accuracy. analyzing relevance model figure displays percentage invalid questions sent oracle different time steps online training. invalid question rate decreases training even though question complexity appears increasing result indicates relevance model improves signiﬁcantly training. also decouple effect relevance model rest setup replacing perfect relevance model ﬂawlessly ﬁlters invalid questions. table shows accuracy sample efﬁciency differences perfect relevance model relevance model small suggests model performs well. datasets. ﬁgure shows clevr accuracy clevr-humans accuracy. plots draw four observations. using bootstrap alone yields poor accuracy provides signiﬁcant learning signal. quality lba-generated training data least good clevr train. impressive result given clevr train dual advantage matching distribution clevr human curated training models. despite advantages matches sometimes surpasses performance. importantly shows better generalization clevr-humans different answer distribution data sometimes sample efﬁcient clevr train instance clevr clevr-humans. cnn+lstm+sa model requires tuples achieve accuracy model trained clevr train. finally also observe agents variance sampled point training. shaded error bars show standard deviation computed independent runs using different random seeds. important property drawing meaningful conclusions interactive training environments qualitative results. figure shows samples lba-generated data various iterations provide insight curriculum discovered agent. initially model asks simple questions colors shapes also makes basic mistakes answering module improves selection policy asks complex questions spatial relationships counts figure example questions asked agent different iterations agent asks increasingly sophisticated questions training progresses starting simple color questions moving shape count questions. also invalid questions become increasingly complex. thus tested policy type answering module cnn+lstm+sa. verify works choices implementing film model rerunning lba. section evaluate lba-generated questions training three voﬄine models. results table suggest selection policy generalizes choice investigate role selection policy compare four alternatives random selection question proposals; using prediction entropy answering module proposal four forward passes dropout using variation ratio prediction; curriculum policy section training different random seeds report mean accuracy stdev cnn+lstm+sa model selection policy figure line results prior work entropy-based policies perform worse random selection. contrast curriculum policy substantially outperforms random selection questions. figure plots normalized informativeness score training question-answering accuracy grouped answer type). plots provide insight behavior curriculum selection policy speciﬁcally observe delayed pattern peak informativeness score answer type followed uptick accuracy answer type. also observe policy’s informativeness score suggests easy-to-hard ordering questions initially selection policy prefers asking easier color questions gradually moves size shape questions eventually difﬁcult count questions. emphasize easy-to-hard curriculum learned automatically without extra supervision. varying size bootstrap data vary size bootstrap binit used initializing models analyze effect generated data. table show accuracy ﬁnal voﬄine model clevr val. smaller bootstrap results reduced performance. also less clevr training dataset bootstrap asks questions match performance using entire clevr training set. empirically observed generator performs well smaller bootstrap sets. however relevance model needs enough valid invalid tuples bootstrap ﬁlter irrelevant question proposals. result smaller bootstrap affects sample efﬁciency lba. away traditional passively supervised settings human annotators provide training data interactive setting learner seeks supervision needs. passive supervision driven progress visual recognition appear well suited general tasks visual question answering curating large amounts diverse data generalizes wide variety questions difﬁcult task. results suggest interactive settings facilitate learning higher sample efﬁciency. high sample efﬁciency crucial move increasingly complex visual understanding tasks. important property distribution questions answers seen training time distribution test time. closely resembles real-world deployment systems distribution user-posed questions system unknown difﬁcult characterize beforehand clevrhumans distribution figure example this. issue poses clear directions future work need develop models less sensitive distributional variations test time; evaluate single test distribution second major direction future work develop real-world version system clevr images replaced natural images oracle replaced human annotator. relative current approach several innovations required achieve goal. importantly requires design effective mode communication learner human oracle. current approach learner uses simple programming language query oracle. real-world system needs communicate humans using diverse natural language. efﬁciency learners improved letting oracle return privileged information answer image-question pair also explains right wrong answer leave structural design privileged information future work. acknowledgments authors would like thank arthur szlam jason weston saloni potdar abhinav shrivastava helpful discussions feedback manuscript; soumith chintala adam paszke help pytorch.", "year": 2017}