{"title": "Clustering with Simultaneous Local and Global View of Data: A message  passing based approach", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.SI"], "abstract": "A good clustering algorithm should not only be able to discover clusters of arbitrary shapes (global view) but also provide additional information, which can be used to gain more meaningful insights into the internal structure of the clusters (local view). In this work we use the mathematical framework of factor graphs and message passing algorithms to optimize a pairwise similarity based cost function, in the same spirit as was done in Affinity Propagation. Using this framework we develop two variants of a new clustering algorithm, EAP and SHAPE. EAP/SHAPE can not only discover clusters of arbitrary shapes but also provide a rich local view in the form of meaningful local representatives (exemplars) and connections between these local exemplars. We discuss how this local information can be used to gain various insights about the clusters including varying relative cluster densities and indication of local strength in different regions of a cluster . We also discuss how this can help an analyst in discovering and resolving potential inconsistencies in the results. The efficacy of EAP/SHAPE is shown by applying it to various synthetic and real world benchmark datasets.", "text": "classical requirement clustering algorithms consider global view data also require algorithm provide additional local information. example provide meaningful representatives data points reveal typical characteristics data points cluster. algorithm also highlight potential inconsistencies results analysis. call features information helps getting better understanding internal structure cluster local view. table also qualitatively analyze well known pairwise similarity based algorithms w.r.t. requirements. different algorithms different shortcomings. able recognize clusters various shapes fails provide local view data. also suffers high computational complexity. hand dbscan quadratic complexity also identify clusters various shapes suffers possibility allocating many data points cluster. also provide local view data. proposed alternative k-medoids require number clusters explicit input. like k-medoids also provides exemplars clusters reveal meaningful information typical characteristics data points cluster. however case k-medoids also restricted discovering star-shaped clusters severely limits applicability. work modify incorporate desired features mentioned table choose modification attractive mathematical framework factor graphs message passing algorithms based. framework provides flexible incorporate requirements algorithm. sec. start giving overview sec. first modify cost function adapt algorithm include desired features. introduce slightly different variants algorithm call shape. also discuss parameters algorithm easily successively tuned. finally sec. using synthetic real world benchmark datasets show eap/shape able discover clusters arbitrary shapes. also discuss detail local information provided eap/shape exploited recognize different cluster densities potential inconsistencies local strength different regions cluster. abstract good clustering algorithm able discover clusters arbitrary shapes also provide additional information used gain meaningful insights internal structure clusters work mathematical framework factor graphs message passing algorithms optimize pairwise similarity based cost function spirit done affinity propagation. using framework develop variants clustering algorithm shape. eap/shape discover clusters arbitrary shapes also provide rich local view form meaningful local representatives connections local exemplars. discuss local information used gain various insights clusters including varying relative cluster densities indication local strength different regions cluster also discuss help analyst discovering resolving potential inconsistencies results. efficacy eap/shape shown applying various synthetic real world benchmark datasets. reference format rayyan ahmad khan rana amjad martin kleinsteuber. clustering simultaneous local global view data message passing based approach. proceedings london conference york pages. https//doi.org/./ nnnnnnn.nnnnnnn introduction clustering important problems data mining. refers task grouping data points points belonging group similar another ones groups. clustering widespread applications various disciplines machine learning computer vision networks bioinformatics. work focus clustering based pairwise similarities. table outlines properties desire clustering algorithm. beyond ability discover clusters arbitrary shapes permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights third-party components work must honored. uses contact owner/author. london’ august london united kingdom copyright held owner/author. isbn -x-xxxx-xxxx-x/yy/mm. https//doi.org/./nnnnnnn.nnnnnnn refers exemplar selected data point represents preference data point become exemplar. case additional priori information assigned value points parameter algorithm. note impose constraints pairwise similarities. constraint mentioned known consistency constraint forces point declared exemplar point must choose exemplar hence promoting compact clusters. np-hard combinatorial optimization solves sub-optimally computations using max-sum algorithm purpose reformulate problem follows define matrix }n×n binary variables since every data point choose exemplar objective function written related work since first published modified different ways. hierarchical affinity propagation introduced proposes layered structure exemplars previous optimization layer considered data points next layer. tries cluster data hierarchically without making hard decisions hierarchical layer. although local exemplars obtained meaningful clusters obtained still star-shaped layer information local structure clusters beyond local exemplars. multiexemplar affinity propagation another approach closely related layers authors propose exemplars super-exemplars. exemplars select super-exemplars representatives super-exemplars forced select themselves. meap shares drawbacks hap. soft-constraint affinity propagation another relevant extension. unlike variants scap allows exemplars select points representative. result scap discover wider variety cluster shapes. hand since scap connects local exemplars identify clusters corresponds expanding cluster based local centroids hence leads sub-optimal clustering. furthermore local information available local exemplars. natural approach identify arbitrarily shaped clusters combine sub-clusters corresponding local exemplar exploring connections sub-cluster boundaries. approach take work. provides better robust clustering results also provides additional information relative cluster density strength along local exemplars. affinity propagation developing algorithm sec. first give overview section describe mathematical foundations well notation needed subsequent sections. given data points along pairwise similarities find clusters pairwise similarities data points exemplars maximized factor graph fig. algorithm gives sets equations/messages updated iterative manner convergence criterion achieved. messages exchanged variable node factor/constraint nodes shown fig. consists messages variable node constraint node/factor node messages i.e. ρij. consists messages opposite direction i.e. ηij. equations describing messages detailed derivation messages given note that max-sum algorithm message variable node constraint/factor node always computed summing incoming messages except message hand computation message factor node variable node depends constraint attached factor node exemplars clusters decision messages updated value looking accumulated belief given incoming messages hij. value decided thresholding i.e. case αij. matrix accumulated beliefs. convergence achieved values diagonal elements change specified number iterations. message passing converges choose exemplars exemplar point assigned exemplar similar complexity also provides cluster representatives. unfortunately unable capture global view data limited discovering star-shaped clusters. nevertheless aforementioned appealing features well flexible well established underlying framework factor graphs message passing optimization algorithms make suitable candidate modify order develop algorithm provides good global local view data. goal section. order include desired characteristics mentioned table need modify cost function/message passing phase decision mechanism basic principle driving approach allow multiple local exemplars cluster permit data points connect multiple local exemplars close enough them. boundary connections local exemplars enable decision mechanism discover global clusters arbitrary shapes. moreover local exemplars together boundary connections provide meaningful insight internal structure cluster later sec. message passing constraint forces representative data point. allow data point connect possibly multiple local exemplars hence form boundary connections local exemplars modify follows connect however affect selection first exemplar still hard requirement outgoing message computations factor node still complexity current choice thus keeping unless chosen complexity algorithm lead complexity message computations exponential case additional information q∀i. modified outgoing messages derivation given appendix seen complexity computing outgoing messages remains case also look intuitive perspective comparing obtained negating maximum incoming messages minimum value clipped hence limiting maximum penalty choosing additional local exemplar thus negative effect accumulated belief limited encourages potential local exemplars. recover setting suppose dataset lies metric space. point selected exemplar points close also high probability selected exemplars. however forces exemplar appear suppresses others. relaxed points close local exemplar also tend become local exemplars. affect global cluster assignments used estimate relative cluster density sec. however obscures local information well separated local representatives would convey. method solve modifying local exemplars chosen must located enough another. however results algorithm complexity undesirable. propose possible solutions problem shown figure local exemplar every data point {k|sjk i.e. ϵ-neighborhood around well separated exemplars obtained enforcing maximum exemplar introducing following refer approach extended affinity propagation. constraints involve diagonal entries since entries hold accumulated belief whether point local exemplar not. since point belong ϵ-neighbourhood multiple points multiple constraints attached local exemplar attached neighbourhoods local exemplar. fig. shows messages constraint. outgoing message derivation given appendix complexity computing outgoing messages hence overall complexity algorithm still stays provides intuitive insight effect constraint candidate local exemplar neighborhood suppress another sending maximum negative confidence possible. scenario candidate maximum survive competition. candidate exemplar simply outgoing messages diagonal variable nodes updated according standard message passing rules. final equations iteratively updated hierarchical approach second method uses hierarchical approach following architecture used layers unlike first layer involves modified constraint still boundary connections. second layer apply potential local exemplars. enough apply second layer since interested merging potential local exemplars appeared close together dense regions purpose effectively fulfilled recognizing star-shaped clusters well. finally data points selected certain potential local exemplar assigned local exemplar local exemplar chosen second layer. i.e. data points associated close potential local exemplars merged together mapped single local exemplar second layer. also possible greedy approach first discovering potential local exemplars optimizing running group them. however heirarchical approach benefits greedy approach avoids hard decisions layers refer extension soft hierarchical affinity propagation extension decision mechanism like keep checking values diagonal entries given incoming messages hii. i.e. +fi. decision phase follows convergence messages described alg. clusters discovered extracting connected components graph adjacency matrix symmetrized version contain local information clusters. desired complexity affected decision phase. parameters involves three parameters. preference serves break dataset small spherical sub-clusters associated local exemplar. penalty allows formation boundary connections join sub-clusters creating closely located potential local exemplars. order avoid have case third parameter ensures well separated local exemplars spread across cluster. case shape need second layer preference instead smaller value causes bigger spherical clusters obscure local well global structure data. hand value high make algorithm susceptible noise. comparison choose higher allow formation enough small star-shaped sub-clusters dataset later merged alg. based boundary connections. selection also easy purpose simply define neighborhood around every point interested local exemplar. initial guess chosen x-percentile similarity matrix suppresses similar representatives shape keeps information first layer joins similar representatives second layer. shape advantage solution gives local information. however always easy choose parameter tuning discussed detail appendix worth mentioning successive tuning possible without compromising performance. makes parameter selection/tuning much easier. experiments results synthetic real world datasets section generated using shape also yields similar results. implementations shape along clustering results available github show algorithm discover clusters arbitrary shapes accurately also discuss exploit local information returned alg. order various perspectives insights internal structure cluster. define following notation denote number points number local exemplars cluster whereas represents total number local exemplars. synthetic datasets chosen datasets given addition also generated dataset consisting gaussian blobs different densities. since euclidean space choose pairwise similarity negative pairwise distance. col. fig. shows datasets along local global information extract using shape. refers dataset. datasets highlight important features approach. synthetic datasets compare algorithm well known algorithms fact none algorithms provide local global perspective simultaneously. furthermore comparison would obscure discussion aspects algorithm want convey using synthetic datasets section. global view. col. shows results discovering clusters different datasets points belonging cluster dataset represented colour. looking figures make qualitative statement able recognize clusters arbitrary shapes. quantitative analysis geometric mean accuracy. measures well ground truth cluster represented best matching estimated cluster vice versa respectively. details quantities results presented table. results using metrics local view. section explain different ways local information provided alg. used gain insight internal structure cluster. providing different perspectives local information show results different values long varied reasonable range global results vary minimally. case values discussed section hence results discussed sec. remain valid. local exemplars opposed shape multiple local exemplars cluster. case well separated local exemplars obtained tuning shape local exemplar represents typical characteristics data points subregion cluster. synthetic data sets observed looking local exemplars col. fig. meaning local exemplars become clear real world examples sec. furthermore clustered data points provided data point asked assign cluster efficiently comparing local exemplars assigning cluster closest local exemplar. operation would computational complexity opposed naively comparing data points. number local exemplars connected particular point different values reveals different information. corresponding optimization first layer shape connections used identify different relative densities among clusters. number local exemplars connected data point computed data point belong cluster hence obtain histograms col. fig. complexity showing spread many data points connected many local exemplars whole dataset. colors histograms refer data points different clusters. instance blue cluster aggregation higher density compared clusters clearly seen histogram data points blue cluster connections local exemplars thus right part histogram. similarly clusters density notice bars co-located histogram. larger values corresponds local exemplars second layer shape connections reveal traits data point. example data point connected local exemplar likely much similar local exemplar compared local exemplars. hand data point connected well separated exemplars could mean data point properties exemplars. also illustrated real world examples sec. indicator human inspection required. common points local exemplars greater local cluster strength region. exemplar pair common points indicator algorithm confident decision connect cluster. help pruning clusters locations local exemplars less common points pruning cluster local exemplars achieved removing common links them. case assignment every common point removed less similar local exemplars. complexity computing number data points connecting local exemplars cluster local exemplars belong compare local exemplar nearest local exemplars belong cluster total complexity compute histograms col. shows number local exemplar pairs number links them. help analyst identifying locations weak internal strength clusters. note local exemplar pairs connected data point play role evaluating notion local strength hence shown histograms. optdigits -dimensional dataset handwritten digits consisting separate training testing sets samples respectively. test dataset only. similarity matrix generated negative pairwise euclidean distances matrix. dataset taken repository. mnist dimensional dataset handwritten digits consisting samples. random samples. similarity matrix generated negative pairwise euclidean distances matrix. proteins saccharomyces cerevisiae gold standard dataset yeast protein complexes choose weighted similarity matrix whose entities given protein-protein interaction values given paper proposes probabilistic measure interactions maximum value threshold interactions file global view. compare algorithm since none algorithms mentioned table provides local exemplars except k-medoids k-medoids requires number clusters explicit input rendering comparison unfair. fair comparison sweep parameters ranges dataset choose best results. accuracy values achieved given table observed enhances score i.e. average ground truth clusters able find good representations estimated clusters. ability discover clusters arbitrary shapes. local view. highlight results mnist optdigits datasets illustrate discussion local perspective sec. using real world examples. fig. shows images digit optdigits left images digit mnist right. images borders local exemplars images black borders data-points shared local exemplars. notice local exemplar exhibits different writing style. similarly notice images connected local exemplars ones shown fig. share traits writing styles. observations line discussion sec. conclusion work used message-passing framework develop variants clustering algorithm call shape. shape complexity i.e. unlike discover clusters arbitrary shapes. discussed local information provided shape differs information used different perspectives internal structure cluster. acknowledgements thank authors patiently answering queries regarding proteins dataset. work rayyan ahmad khan martin kleinsteuber funded mercateo. work rana amjad supported german federal ministry education research framework alexander humboldt-professorship. references sylvain brohée jacques helden. evaluation clustering algorithms protein-protein interaction networks. bioinformatics hong chang dit-yan yeung. robust path-based spectral clustering. martin ester hans-peter kriegel density-based algorithm discovering clusters density-based algorithm discovering clusters large spatial databases noise limin enzo medico. flame novel fuzzy clustering method analysis microarray data. bioinformatics aristides gionis heikki mannila panayiotis tsaparas. clustering daniel tarlow inmar givoni richard zemel. hop-map efficient message passing high order potentials. proceedings thirteenth international conference artificial intelligence statistics. well-separated exemplars. noted constraint allows exemplars density regions reverse true hap. choice boils user choice exemplars distribution. also holds information scope local exemplar find local representatives combined. case weakly linked local exemplars analyst option points responsible link. however tuning second layer preference always easy choice epsilon intuitive might need search range find good second layer preference. nevertheless value exceed first layer preference objective combine similar exemplars. remark another simple achieve multiple local exemplars keep constraint standard append priori confidence accumulated belief bij. convergence keep checking every iteration. convergence apply decision phase mentioned section practice selected constant i.e. appending facilitate entries cross threshold. achieved choosing suppress discovery even exemplar point. effect increasing keeping parameters constant shown last figure appending priori belief simple finding structure data. however give rich connections local exemplars robust way. growing clusters using link different clusters loosely connected noisy data points. appendix parameter tuning take half moons synthetic dataset show effects tweaking parameters one. dataset consists points grouped clusters different densities. figure shows effect tuning parameter keeping others fixed. sub-plot shows scatter plot local exemplars drawn circles non-exemplar points drawn markers. first shows increases local exemplars start grow dense sparse regions. thus algorithm remains robust noise. second shows tuning keeping parameters fixed. decreasing increases size neighbourhoods increases separation local exemplars. third shows tuning second layer preference obtain", "year": 2018}