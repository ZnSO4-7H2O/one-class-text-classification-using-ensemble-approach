{"title": "Improved Semantic Representations From Tree-Structured Long Short-Term  Memory Networks", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "text": "superior ability preserve sequence information time long short-term memory networks type recurrent neural network complex computational unit obtained strong results variety sequence modeling tasks. underlying lstm structure explored linear chain. however natural language exhibits syntactic properties would naturally combine words phrases. introduce tree-lstm generalization lstms tree-structured network topologies. treelstms outperform existing systems strong lstm baselines tasks predicting semantic relatedness sentences sentiment classiﬁcation introduction models distributed representations phrases sentences—that models realvalued vectors used represent meaning—fall three classes bag-of-words models sequence models tree-structured models. bag-of-words models phrase sentence representations independent word order; example generated averaging constituent word representations contrast sequence models construct sentence representations order-sensitive function sequence tokens lastly tree-structured models compose phrase sentence representation constituent subphrases according given syntactic structure sentence fully capture semantics natural language inability account differences meaning result differences word order syntactic structure therefore turn ordersensitive sequential tree-structured models. particular tree-structured models linguistically attractive option relation syntactic interpretations sentence structure. natural question then following extent better tree-structured models opposed sequential models sentence representation? paper work towards addressing question directly comparing type sequential model recently used achieve state-of-the-art results several tasks tree-structured generalization. natural choice sequence modeling tasks. recently rnns long short-term memory units re-emerged popular architecture representational power effectiveness capturing long-term dependencies. lstm networks review sec. successfully applied variety sequence modeling prediction tasks notably machine translation speech recognition image caption generation program execution paper introduce generalization standard lstm architecture tree-structured network topologies show superiority representing sentence meaning sequential lstm. standard lstm composes hidden state input current time step hidden state lstm unit previous time step tree-structured lstm tree-lstm composes state input vector hidden states arbitrarily many child units. standard lstm considered special case tree-lstm internal node exactly child. evaluations demonstrate empirical strength tree-lstms models representing sentences. evaluate tree-lstm architecture tasks semantic relatedness prediction sentence pairs sentiment classiﬁcation sentences drawn movie reviews. experiments show tree-lstms outperform existing systems sequential lstm baselines tasks. implementations models experiments available https// github.com/stanfordnlp/treelstm. overview recurrent neural networks able process input sequences arbitrary length recursive application transition function hidden state vector time step hidden state function input vector network receives time previous hidden state ht−. example input vector could vector representation t-th word body text hidden state interpreted dunfortunately problem rnns transition functions form training components gradient vector grow decay exponentially long sequences problem exploding vanishing gradients makes difﬁcult model learn long-distance correlations sequence. schmidhuber addresses problem learning long-term dependencies introducing memory cell able preserve state long periods time. numerous lstm variants described describe version used zaremba sutskever deﬁne lstm unit time step collection vectors input gate forget gate output gate memory cell hidden state entries gating vectors refer memory dimension lstm. input current time step denotes logistic sigmoid function denotes elementwise multiplication. intuitively forget gate controls extent previous memory cell forgotten input gate controls much unit updated output gate controls exposure internal memory state. hidden state vector lstm unit therefore gated partial view state unit’s internal memory cell. since value gating variables vary vector element model bidirectional lstm. bidirectional lstm consists lstms parallel input sequence reverse input sequence. time step hidden state bidirectional lstm concatenation forward backward hidden states. setup allows hidden state capture past future information. multilayer lstm. multilayer lstm architectures hidden state lstm unit layer used input lstm unit layer time step here idea higher layers capture longerterm dependencies input sequence. limitation lstm architectures described previous section allow strictly sequential information propagation. here propose natural extensions basic child-sum tree-lstm lstm architecture n-ary tree-lstm. variants allow richer network topologies lstm unit able incorporate information multiple child units. standard lstm units tree-lstm unit contains input output gates memory cell hidden state difference standard lstm unit tree-lstm units gating vectors memory cell updates dependent states possibly many child units. additionally instead single forget gate treelstm unit contains forget gate child allows tree-lstm unit selectively incorporate information child. example tree-lstm model learn emphasize semantic heads semantic relatedness figure composing memory cell hidden state tree-lstm unit children labeled edges correspond gating indicated gating vector dependencies omitted compactness. standard lstm tree-lstm unit takes input vector applications vector representation word sentence. input word node depends tree structure used network. instance tree-lstm dependency tree node tree takes vector corresponding head word input whereas treelstm constituency tree leaf nodes take corresponding word vectors input. model learn ﬁne-grained conditioning states unit’s children childsum tree-lstm. consider example constituency tree application left child node corresponds noun phrase right child verb phrase. suppose case advantageous emphasize verb phrase representation. parameters trained components close components close forget gate parameterization. deﬁne parameterization child’s forget gate contains off-diagonal paramk parameterizaeter matrices tion allows ﬂexible control information propagation child parent. example allows left hidden state binary tree either excitatory inhibitory effect forget gate right child. however large values additional parameters impractical tied ﬁxed zero. constituency tree-lstms. naturally apply binary tree-lstm units binarized constituency trees since left right child nodes distinguished. refer application binary tree-lstms constituency tree-lstm. note constituency tree-lstms node receives input vector leaf node. remainder paper focus special cases dependency tree-lstms constituency tree-lstms. architectures fact closely related; since consider binarized constituency trees parameterizations models similar. difference application compositional parameters dependent head dependency tree-lstms left child right child constituency tree-lstms. unit input hidden states unit’s children. example dependency tree application model learn parameters components input gate values close semantically important content word given input values close input relatively unimportant word dependency tree-lstms. since childsum tree-lstm unit conditions components child hidden states wellsuited trees high branching factor whose children unordered. example good choice dependency trees number dependents head highly variable. refer child-sum tree-lstm applied dependency tree dependency tree-lstm. n-ary tree-lstms n-ary tree-lstm used tree structures branching factor children ordered i.e. indexed node write hidden state memory cell child respectively. n-ary tree-lstm transition equations following evaluate tree-lstm architectures tasks sentiment classiﬁcation sentences sampled movie reviews predicting semantic relatedness sentence pairs. comparing tree-lstms sequential lstms control number lstm parameters varying dimensionality hidden states. details model variant summarized table sentiment classiﬁcation task predict sentiment sentences sampled movie reviews. stanford sentiment treebank subtasks binary classiﬁcation sentences ﬁne-grained classiﬁcation classes negative negative neutral positive positive. standard train/dev/test splits binary classiﬁcation subtask ﬁne-grained classiﬁcation subtask parse tree could correspond property phrase spanned node. node softmax classiﬁer predict label given inputs {x}j observed nodes subtree rooted classiﬁer takes hidden state node input semantic relatedness sentence pairs given sentence pair wish predict real-valued similarity score range integer. sequence ordinal scale similarity higher scores indicate greater degrees similarity allow real-valued scores account ground-truth ratings average evaluations several human annotators. ﬁrst produce sentence representations sentence pair using tree-lstm model sentence’s parse tree. given sentence representations predict similarity score using neural network considers distance angle pair absolute value function applied elementwise. distance measures empirically motivated combination outperforms either measure alone. multiplicative measure interpreted elementwise sequential lstm baselines predict sentiment phrase using representation given ﬁnal lstm hidden state. sequential lstm models trained spans corresponding labeled nodes training set. classiﬁcation model described sec. dependency tree-lstms constituency tree-lstms structured according provided parse trees. dependency tree-lstms produce dependency parses sentence; node tree given sentiment label span matches labeled span training set. semantic relatedness given pair sentences semantic relatedness task predict human-generated rating similarity sentences meaning. sentences involving compositional knowledge dataset consisting sentence pairs train/dev/test split. sentences derived existing image video description datasets. sentence pair annotated relatedness score indicating sentences completely unrelated indicating sentences related. label average ratings assigned different human annotators. table test accuracies stanford sentiment treebank. experiments report mean accuracies runs fine-grained -class sentiment classiﬁcation. binary positive/negative sentiment classiﬁcation. initialized word representations using publicly available -dimensional glove vectors sentiment classiﬁcation task word representations updated training learning rate semantic relatedness task word representations held ﬁxed observe signiﬁcant improvement representations tuned. models trained using adagrad learning rate minibatch size model parameters regularized per-minibatch regularization strength sentiment classiﬁer additionally regularized using dropout dropout rate observe performance gains using dropout semantic relatedness task. table test results sick semantic relatedness subtask. experiments report mean scores runs results grouped follows semeval submissions; baselines; sequential lstms; tree-structured lstms. sentiment classiﬁcation results summarized table constituency tree-lstm outperforms existing systems ﬁne-grained classiﬁcation subtask achieves accuracy comparable state-of-theart binary subtask. particular outperforms dependency tree-lstm. performance least partially attributable fact dependency tree-lstm trained less data labeled nodes constituency tree-lstm. difference dependency representations containing fewer nodes corresponding constituency representations inability match dependency nodes corresponding span training data. found updating word representations training yields signiﬁcant boost performance ﬁne-grained classiﬁcation subtask gives minor gain binary classiﬁcation subtask gains expected since glove vectors used initialize word representations originally trained capture sentiment. compare models number non-lstm baselines. mean vector baseline computes sentence representations mean representations constituent words. dt-rnn sdt-rnn models compose vector representations nodes dependency tree afﬁnetransformed child vectors followed nonlinearity. sdt-rnn extension dtrnn uses separate transformation dependency relation. baselines including lstm models similarity model described sec. also compare four topperforming systems submitted semeval semantic relatedness shared task ecnu meaning factory unal-nlp illinois-lh systems heavily feature engineered generally using combination surface form overlap features lexical distance features derived wordnet paraphrase database list strongest results able task; cases results stronger ofﬁcial performance team shared task. example listed result zhao stronger submitted system’s pearson correlation score figure fine-grained sentiment classiﬁcation accuracy sentence length. plot accuracy test sentences length window examples tail length distribution batched ﬁnal window figure pearson correlations predicted similarities gold ratings sentence length. plot pairs mean length window examples tail length distribution batched ﬁnal window tems without additional feature engineering best results achieved dependency tree-lstm. recall task treelstm models receive supervision root tree contrast sentiment classiﬁcation task supervision also provided intermediate nodes. conjecture setting dependency tree-lstm beneﬁts compact structure relative constituency tree-lstm sense paths input word vectors root tree shorter aggregate dependency treelstm. table list nearest-neighbor sentences retrieved -sentence sample sick test set. compare neighbors ranked dependency tree-lstm model baseline ranking cosine similarity mean word vectors sentence. dependency tree-lstm model exhibits several desirable properties. note dependency parse second query sentence word ocean second-furthest word root depth regardless retrieved sentences semantically related word ocean indicates treelstm able preserve emphasize information relatively distant nodes. additionally tree-lstm model shows greater robustness differences sentence length. given query playing guitar treelstm associates phrase playing guitar longer related phrase dancing singing front crowd effect sentence length hypothesis explain empirical strength tree-lstms tree structures help mitigate problem preserving state long sequences words. true would expect greatest improvement sequential lstms longer sentences. figs. show relationship sentence length performance measured relevant taskspeciﬁc metric. data point mean score runs error bars omitted clarity. observe dependency treelstm signiﬁcantly outperform sequential counterparts relatedness task longer sentences length also achieves consistently strong performance shorter sentences. suggests unlike sequential lstms tree-lstms able encode semantically-useful structural information sentence representations compose. ranking mean word vector cosine similarity woman slicing potatoes woman cutting potatoes woman slicing herbs woman slicing tofu waving young runners ocean standing bottom stairs ranking dependency tree-lstm model woman slicing potatoes woman cutting potatoes potatoes sliced woman tofu sliced woman waving young runners ocean group playing ball beach pennington found wide applicability variety tasks. following success substantial interest area learning distributed phrase sentence representations well distributed representations longer bodies text paragraphs documents approach builds recursive neural networks abbreviate tree-rnns order avoid confusion recurrent neural networks. tree-rnn framework vector representation associated node tree composed function vectors corresponding children node. choice composition function gives rise numerous variants basic framework. treernns used parse images natural scenes compose phrase representations word vectors classify sentiment polarity sentences paper introduced generalization lstms tree-structured network topologies. tree-lstm architecture applied trees arbitrary branching factor. demonstrated effectiveness tree-lstm applying architecture tasks semantic relatedness sentiment classiﬁcation outperforming existing systems both. controlling model dimensionality demonstrated tree-lstm models able outperform sequential counterparts. results suggest lines work characterizing role structure producing distributed representations sentences. thank anonymous reviewers valuable feedback. stanford university gratefully acknowledges support natural language understanding-focused gift google inc. defense advanced research projects agency deep exploration filtering text program force research laboratory contract fa---. opinions ﬁndings conclusion recommendations expressed material authors necessarily reﬂect view darpa afrl government.", "year": 2015}