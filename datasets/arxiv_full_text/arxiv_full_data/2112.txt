{"title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive  Models", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.", "text": "achieving efﬁcient scalable exploration complex domains poses major challenge reinforcement learning. bayesian pac-mdp approaches exploration problem offer strong formal guarantees often impractical higher dimensions reliance enumerating state-action space. hence exploration complex domains often performed simple epsilon-greedy methods. paper consider challenging atari games domain requires processing pixel inputs delayed rewards. evaluate several sophisticated exploration strategies including thompson sampling boltzman exploration propose exploration method based assigning exploration bonuses concurrently learned model system dynamics. parameterizing learned model neural network able develop scalable efﬁcient approach exploration bonuses applied tasks complex high-dimensional state spaces. atari domain method provides consistent improvement across range games pose major challenge prior methods. addition gamescores also develop auc- metric atari learning domain evaluate impact exploration benchmark. reinforcement learning agents acting unknown environments face exploration versus exploitation tradeoff. without adequate exploration agent might fail discover effective control strategies particularly complex domains. pac-mdp algorithms mbie-eb bayesian algorithms bayesian exploration bonuses managed tradeoff assigning exploration bonuses novel states. methods novelty state-action pair derived number times agent visited pair. approaches offer strong formal guarantees requirement enumerable representation agent’s environment renders impractical large-scale tasks. such exploration large tasks still often performed using simple heuristics epsilon-greedy strategy inadequate complex settings. paper evaluate several exploration strategies scaled complex tasks high-dimensional inputs. results show boltzman exploration thompson sampling signiﬁcantly improve na¨ıve epsilon-greedy strategy. however show biggest consistent improvement achieved assigning exploration bonuses based learned model system dynamics learned representations. describe method learns state representation observations trains dynamics model using representation concurrently policy uses misprediction error model asses novelty state. novel states expected disagree strongly model states visited frequently past assigning exploration bonuses based disagreement produce rapid effective exploration. using learned model dynamics assess state’s novelty presents several challenges. capturing adequate representation agent’s environment dynamics predictions accomplished training model predict next state previous ground-truth state-action pair. however would expect pixel intensity values adequately capture salient features given state-space. provide suitable representation system’s state space propose method encoding state space lower dimensional domains. achieve sufﬁcient generality scalability modeled system’s dynamics deep neural network. allows on-the-ﬂy learning model representation easily trained parallel learning policy. main contribution scalable efﬁcient method assigning exploration bonuses large problems complex observations well extensive empirical evaluation approach simple alternative strategies boltzman exploration thompson sampling. approach assigns model-based exploration bonuses learned representations dynamics using observations actions. scale large problems bayesian approaches exploration become impractical show achieves signiﬁcant improvement learning speed task learning play atari games images approach achieves state-of-the-art results number games achieves particularly large improvements games human players strongly outperform prior methods. aside achieving high ﬁnal score method also achieves substantially faster learning. evaluate speed learning process propose auc- benchmark evaluate learning progress atari domain. consider inﬁnite-horizon discounted markov decision process deﬁned tuple ﬁnite states ﬁnite actions transition probability distribution reward function initial state distribution discount factor. interested ﬁnding policy maximizes expected reward time. maximization accomplished using variety reinforcement learning algorithms. work concerned online reinforcement learning wherein algorithm receives tuple step. here previous state previous action state reward collected result transition. reinforcement learning algorithm must tuple update policy maximize longterm reward choose action at+. often insufﬁcient simply choose best action based previous experience since strategy quickly fall local optimum. instead learning algorithm must perform exploration. prior work suggested methods address exploration problem acting optimism uncertainty. assumes reinforcement learning algorithm tend choose best action encouraged visit state-action pairs frequently seen augmenting reward function deliver bonus visiting novel states. accomplished augmented reward function novelty function designed capture novelty given state-action pair. prior work suggested variety different novelty functions e.g. based state visitation frequency. methods offer number appealing guarantees near-bayesian exploration polynomial time require concise often discrete representation agent’s stateaction space measure state visitation frequencies. approach employ function approximation representation learning devise alternative requirements. would like encourage agent exploration giving agent exploration bonuses visiting novel states. identifying states novel requires supply representation agent’s state space well mechanism representation assess novelty. unsupervised learning methods offer promising avenue acquiring concise representation state good similarity metric. accomplished using dimensionality reduction clustering graphbased techniques work draw recent developments representation learning neural networks discussed following section. however even good learned state representation maintaining table visitation frequencies becomes impractical complex tasks. instead learn model task dynamics used assess novelty state. formally denote encoding state dynamics predictor parameterized takes encoded version state time agent’s action time attempts predict encoded version agent’s state time parameterization discussed next section. state transition attempt predict using predictive model prediction error approach motivated idea that ability model dynamics particular state-action pair improves come understand state better hence novelty lower. don’t understand state-action pair well enough make accurate predictions assume knowledge particular area model dynamics needed hence higher novelty measure assigned. using learned model dynamics assign novelty functions allows address exploration versus exploitation problem non-greedy way. appropriate representation even encounter state-action pair expect accurate long enough similar state-action pairs encountered. q-learning actor-critic algorithms. method summarized algorithm step receive tuple compute euclidean distance encoded state prediction made model at). used compute exploration-augmented reward rbonus using equation tuples stored memory bank every step. every step policy updated. epoch corresponding observations implementation dynamics model updated improve accuracy. desired representation encoder also updated time. found retraining every epochs sufﬁcient. approach modular compatible representation well reinforcement learning method updates policy based continuous stream observation action reward tuples. incorporating exploration bonuses make reinforcement learning task nonstationary though major issue practice shown experimental evaluation. following section discuss particular choice learning policies playing atari games images. deep learning architectures though dynamics model encoder previous section parametrized appropriate method found using deep neural networks achieved good empirical results atari games benchmark. section discuss particular networks used implementation. direct learning dynamics model directly predict state next time step atari games benchmark corresponds next frame’s pixel intensity values. however directly predicting pixel intensity values unsatisfactory since expect pixel intensity capture salient features environment robust way. experiments dynamics model trained predict frames exhibited extremely poor behavior assigning exploration bonuses near equality time steps discussed experimental results section. overcome difﬁculties seek function encodes lower dimensional representation state task representing atari frames found autoencoder could used successfully obtain encoding function achieve dimensionality reduction feature extraction autoencoder hidden layers followed euclidean loss layer figure left autoencoder used input space. circle denotes hidden layer extracted utilized input dynamics learning. right model learning architecture. computes distance output features original input image. hidden layers reduced dimension maximal compression occurs units. this activations decoded passing hidden layers increasingly large size. train dynamic initialize epsilon-greedy strategy collect images actions agent acts policy learning algorithm. epochs train auto encoder data. continue collect data periodically retrain auto encoder parallel policy training algorithm. found reconstructed input achieves small non-trivial residual test regardless auto encoder training technique utilized suggesting cases learns underlying features state space avoiding overﬁtting. obtain lower dimensional representation agent’s state space snapshot network’s ﬁrst layers saved. sixth layer’s output utilized encoding original state space. construct encoding running ﬁrst hidden layers autoencoder taking sixth layers output practice found using sixth layer’s output obtained best model learning results. appendix discussion result. equipped encoding consider task predicting model dynamics. task much simpler layer neural network sufﬁces. takes input encoded version state time along agent’s action seeks predict encoded next frame loss computed euclidean loss layer regressing ground truth model initially learns representation close identity function consequently loss residual similar state-action pairs. however approximately epochs begins learn complex dynamics consequently better identify novel states. evaluate quality learned model appendix. exploration intensely studied area reinforcement learning. many pioneering algorithms area achieve efﬁcient exploration scales polynomially number parameters agent’s state space however size state spaces increases methods quickly become intractable. number prior methods also examine various techniques using models prediction incentivize exploration however methods typically operate directly transition matrix discrete provide straightforward extension large continuous spaces function approximation required. number prior methods also proposed incorporate domain-speciﬁc factors improve exploration. doshi-velez proposed incorporating priors policy optimization lang developed method speciﬁc relational domains. finally schmidhuber developed curiosity driven approach exploration uses model predictors control several exploration techniques proposed extend readily large state spaces. among these methods c-pace metric-e require good metric state space satisﬁes assumptions algorithm. corresponding representation learning issue parallels representation problem address using autoecoder unclear appropriate metric prior methods acquired automatically tasks sensory input atari games experimental evaluation. methods based monte-carlo tree search also scale gracefully complex domains indeed previous work applied techniques task playing atari games screen images however approach computationally intensive requires access generative model system order perform tree search always available online reinforcement learning. hand method readily integrates online reinforcement learning algorithm. finally several recent papers focused driving value higher. authors network dropout perform thompson sampling. boltzman exploration positive probability assigned possible action according expected utility according temperature parameter methods focus controlling values rather model-based exploration. comparison provided next section. evaluate approach games arcade learning environment task consists choosing actions atari emulator based images screen. previous work tackled task using q-learning epsilon-greedy exploration well monte carlo tree search policy gradient methods deep networks reinforcement learning algorithm within method compare performance method using epsilon-greedy exploration boltzman exploration thompson sampling approach. results games arcade learning environment presented table chose games particularly challenging prior methods ones human experts outperform prior learning methods. evaluated versions approach; using either autoencoder trained advance running epsilon-greedy q-learning collect data using autoencoder trained concurrently model policy image data table also shows results implementation reported previous work along human expert performance game note implementation attain score games prior work shorter running time. since primarily concerned rate learning ﬁnal results consider deﬁciency. directly evaluate beneﬁt including exploration bonuses compare performance approach primarily implementation prior scores provided reference. addition raw-game scores learning curves also analyze results benchmark named area curve game benchmark computes area game-score learning curve area normalized times score maximum game score achieved represents epochs play best-known levels. metric effectively captures improvements game’s learning rate require running games epochs reason suggest alternative metric game-score. bowling policy without exploration tended ﬁxate pattern nocking pins frame. bonuses added dynamics learner quickly became adept predicting outcome thus encouraged explore release points. frostbite game’s dynamics changed substantially addition extra platforms player progressed. dynamics complex systems well understood system encouraged visit often seaquest submarine must surface bouts ﬁghting sharks. however player resurfaces soon suffer penalty effects game’s dynamics. since effects poorly understood model learning algorithm resurfacing receives high exploration bonus hence agent eventually learns successfully resurface correct time. q∗bert exploration bonuses resulted lower score. q∗bert background changes color level one. dynamics predictor unable quickly adapt dramatic change environment consequently exploration bonuses assigned near equality almost every state visited. negatively impacts ﬁnal policy. learning curves games shown figure note exploration bonus algorithms learn signiﬁcantly faster epsilon-greedy q-learning often continue learning even epsilon-greedy strategy converges. games inputs normalized according table comparison maximum scores achieved different methods. static trains state-space auto encoder game frames prior policy optimization dynamic retrains auto encoder epoch using last images training set. note exploration bonuses help achieve state results bowling frostbite. games provides signiﬁcant exploration challenge. bolded numbers indicate best-performing score among experiments. note score sometimes lower score reported prior work implementation one-tenth long results show nuanced exploration strategies generally improve naive epsilon greedy approach boltzman thompson sampling methods achieving best results three games. however exploration bonuses achieve fastest learning best results consistently outperforming three methods games terms auc-. paper evaluated several scalable efﬁcient exploration algorithms reinforcement learning tasks complex high-dimensional observations. results show method based assigning exploration bonuses consistently achieves largest improvement range challenging atari games particularly human players outperform prior learning methods. exploration method learns model dynamics concurrently policy. model predicts learned representation state function prediction error added reward exploration bonus encourage policy visit states high novelty. limitations approach misprediction error metric assumes misprediction state caused inaccuracies model. true determinstic environments stochastic dynamics violate assumption. extension approach stochastic systems requires nuanced treatment distinction stochastic dynamics uncertain dynamics hope explore future work. another intriguing direction future work examine learned dynamics model incorporated policy learning process beyond providing exploration bonuses. could principle enable substantially faster learning purely model-free approaches. recall trained auto-encoder encode game’s state space. trained predictive model next auto-encoded frame rather directly training pixel intensity values next frame. obtain encoded space state eight layer auto-encoder training utilized auto-encoder’s sixth layer encoded state space. chose sixth layer rather bottleneck fourth layer found that iterations seaquest epochs iteration using layer encoding delivered measurably better performance using bottleneck layer. results experiment presented below. figure game score averaged seaquest iterations various choices state-space encoding layer. notice choosing sixth layer encode state space signiﬁcantly outperformed bottleneck layer. evaluating quality learned dynamics model somewhat difﬁcult system rewarded achieving higher error rates. dynamics model converges quickly useful exploration bonuses. nevertheless plot mean normalized residuals across games trials used experiments errors learned dynamics models continually decrease time. mean normalized residual epochs approximately half maximal mean achieved. suggests dynamics model able correctly learn properties underlying dynamics given game. table auc- computed comparing area game-score learning curve epochs play area rectangle dimensions maximum score game achieved integral approximated trapezoid rule. holistically captures games learning rate require running games epochs reason suggest alternative metric game-score.", "year": 2015}