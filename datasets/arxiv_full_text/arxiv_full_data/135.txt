{"title": "Synthesizing Deep Neural Network Architectures using Biological Synaptic  Strength Distributions", "tag": ["cs.NE", "cs.AI", "cs.CV", "stat.ML"], "abstract": "In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance.", "text": "inspired aforementioned observations perform exploratory study different uncorrelated correlated probabilistic generative models synaptic strength formation deep neural networks potential inﬂuence different distributions modelling performance particularly scenario associated small data sets. model synaptic strength distribution deep neural network synaptic order explore effect different probabilistic generative models synaptic formation modelling inference performance focused manner study restrict network architecture convolutional neural network architecture. speciﬁcally synaptic strengths convolutional layers synthesized based ﬁne-tuned whereas synaptic strengths fully connected layers synthesized trained reach complete modelling capabilities. setup allows localize effect synaptic strengths fairly compare modelling inference performance different synaptic formation drawn various underlying biologicallyinspired probability distributions. furthermore random variable corresponding synaptic strength denoted drawn probabilistic generative model study explore three different distribution models based past biological studies approach synapse strength formation enable drastic reduction number parameters need trained important factor scenarios small number training data. recently witnessed explosive growth machine learning research focused modelling real-world inference problems. notably deep learning models deep neural networks particularly powerful biologically inspired class learning algorithms consistently demonstrated state-of-the-art performance tasks object recognition image classiﬁcation image segmentation speech recognition. particular type proven effective recent year convolutional neural networks architecturally made layers neurons modelled simple complex cells visual cortex. order train task classiﬁcation synaptic strengths network optimized based training data. optimizing large-scale artiﬁcial neural architecture classiﬁcation generalizable manner however requires large number input image samples. prohibitive many practical scenarios labeled data limited. ameliorate dependence explore whether possible sidestep training large portion learnable parameters—synaptic strengths—in neural network. particularly motivated strong modelling inference performance exhibited random synaptic strengths leveraged modelling functional brain computationally. suggests inherent structure deep neural networks enough elicit powerful modelling inference performance even formation synaptic strengths random. particular draw inspiration number studies investigated distribution synaptic strengths biological brain. example observed synaptic strengths certain synapses excitatory synapses well modelled random variables following well-known distributions truncated gaussians furthermore song found underlying synaptic strengths follows lognormal distributions. studies suggested correlated relationship synaptic strengths earlier layers visual cortex speciﬁcally circular concentric receptive ﬁelds modelled lateral geniculate cells. table impact different probabilistic generative models synaptic strength generation modelling performance small datasets synaptic strengths convolutional layers generated distributions describing synaptic strengths visual cortex. convolutional layer synapses frozen trained whereas fully connected layers trained over. highest performing setups bold. exploratory study examined three standard publicly available object classiﬁcation datasets including mnist hand-written digits street view house numbers svhn cifar- object recognition dataset scenario small training datasets. mimic scenario samples class label randomly selected available training data dataset form small dataset. however compute test accuracy models tested available testing samples. reported results computed based three runs. table summarizes results experiments. also report classiﬁcation performance architecture datasets completely trained synaptic strengths ﬁne-tuned. expected small number training samples results cnn’s relatively poor classiﬁcation performance evident right-most column table named fully trained. interestingly sampling convolutional synaptic strengths normal gaussian distribution yields classiﬁcation performance comparable fully trained cifar- svhn. surprising preliminary results seen log-normal center-surround columns. possibility results suggest sampling synaptic strengths well-known distributions model synaptic strengths visual cortex result classiﬁcation system potentially outperforms carefully ﬁne-tuned cnns small datasets. suggest scenario little data learning generalizable classiﬁcation system worth effort training performance outperformed random convolutional synaptic strengths. result powerful ﬁrst step towards designing deep neural networks require many data samples learn sidestep reduce burden current training procedures maintaining boosting classiﬁcation modelling performance. future work excited explore effect deeper networks synapses investigate whether synaptic strength distributions used design efﬁcient architectures training algorithms. work supported natural sciences engineering research council canada ontario ministry economic development innovation canada research chairs program. authors also thank nvidia hardware used study nvidia hardware grant program.", "year": 2017}