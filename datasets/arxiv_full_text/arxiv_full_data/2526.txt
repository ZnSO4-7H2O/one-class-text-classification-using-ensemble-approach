{"title": "The Human Kernel", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.", "text": "bayesian nonparametric models gaussian processes provide compelling framework automatic statistical modelling models high degree ﬂexibility automatically calibrated complexity. however automating human expertise remains elusive; example gaussian processes standard kernels struggle function extrapolation problems trivial human learners. paper create function extrapolation problems acquire human responses design kernel learning framework reverse engineer inductive biases human learners across behavioral experiments. learned kernels gain psychological insights extrapolate humanlike ways beyond traditional stationary polynomial kernels. finally investigate occam’s razor human gaussian process based function learning. truly intelligent systems learn make decisions without human intervention. therefore surprising early machine learning efforts perceptron neurally inspired recent years probabilistic modelling become cornerstone machine learning approaches applications neural processing human learning probabilistic perspective ability model automatically discover patterns perform extrapolation determined support inductive biases ideally want model able represent many possible solutions given problem inductive biases extract intricate structure limited data. example performing character recognition would want support contain large collection potential characters accounting even rare writing styles inductive biases reasonably reﬂect probability encountering character support inductive biases wide range probabilistic models thus ability models learn generalise implicitly controlled covariance kernel determines similarities pairs datapoints. example bayesian basis function regression splines inﬁnite neural networks exactly represented gaussian process particular kernel function moreover fisher kernel provides mechanism reformulate probabilistic generative models kernel methods paper wish reverse engineer human-like support inductive biases function learning using gaussian process based kernel learning formalism. particular create human function learning datasets including novel function extrapolation problems multiple-choice questions explore human intuitions simplicity explanatory power. participate experiments view demonstrations http//functionlearning.com/ develop statistical framework kernel learning predictions model conditioned information model given. ability sample multiple sets posterior predictions model input locations choice given dataset choice provides unprecedented statistical strength kernel learning. contrast standard kernel learning involves ﬁtting kernel ﬁxed dataset viewed single realisation stochastic process. framework leverages spectral mixture kernels non-parametric estimates. exploit framework directly learn kernels human responses contrasts prior work human function learning compares ﬁxed model human responses. moreover consider individual rather averaged human extrapolations. interpret learned kernels gain scientiﬁc insights human inductive biases including ability adapt information function learning. also learned human kernels inspire types covariance functions enable extrapolation problems difﬁcult conventional gaussian process models. study occam’s razor human function learning compare gaussian process marginal likelihood based model selection show biased towards under-ﬁtting. provide expressive quantitative means compare existing machine learning algorithms human learning mechanism directly infer human prior representations. work intended preliminary step towards building probabilistic kernel machines encapsulate human-like support inductive biases. since state machine learning methods perform conspicuously poorly number extrapolation problems would easy humans efforts potential help automate machine learning improve performance wide range tasks including settings difﬁcult humans process finally presented framework considered general context wishes efﬁciently reverse engineer interpretable properties model predictions. describe related work section section introduce framework learning kernels human responses employ framework section supplement provide background gaussian processes recommend review. historically efforts understand human function learning focused rule-based relationships interpolation based similarity learning grifﬁths ﬁrst note gaussian process framework used unify perspectives. introduced model mixture polynomial kernels reﬂect human ability learn arbitrary smooth functions still identifying simple parametric functions. applied model standard evaluation tasks comparing predictions simple functions averaged human judgments interpolation performance human error rates. lucas extended model accommodate wider range phenomena using inﬁnite mixture gaussian process experts lucas used model shed light human predictions given sparse data. work complements pioneering gaussian process models prior work human function learning many features distinguish previous contributions rather iteratively building models comparing human predictions based ﬁxed assumptions regularities humans recognize directly learning properties human model advanced kernel learning techniques; essentially models function learning including past models evaluated averaged human responses setting aside individual differences erasing critical statistical structure data. contrast approach uses individual responses; many recent model evaluations rely relatively small heterogeneous sets experimental data. evaluation corpora using recent reviews limited small parametric forms i.e. polynomial power-law logistic logarithmic exponential sinusoidal detailed analyses tend involve linear quadratic logistic functions. projects collected richer detailed data sets aware coarse-grained qualitative analyses using data. moreover experiments depart simple parametric functions tend noisy data. thus unsurprising participants tend revert prior mode arises almost function learning experiments linear functions especially slope- intercept- departure prior work create original function learning problems simple parametric description noise obvious human learners cannot resort simple rules acquire human data ourselves. hope novel datasets inspire detailed ﬁndings function learning; learn kernels human responses provide insights biases drive human function learning human ability progressively adapt information enable human-like extrapolations problems difﬁcult conventional gaussian process models; investigate occam’s razor human function learning nonparametric model selection. rule-based associative theories human function learning uniﬁed part gaussian process framework. indeed gaussian processes contain large array probabilistic models non-parametric ﬂexibility produce inﬁnitely many consistent dataset. moreover support inductive biases gaussian process encaspulated covariance kernel. goal learn gaussian process covariance kernels predictions made humans function learning experiments gain better understanding human learning inspire machine learning models improved extrapolation performance minimal human intervention. learner given access data training inputs makes predictions testing inputs assume predictions samples learner’s posterior distribution possible functions following results showing human inferences judgments resemble posterior samples across wide range perceptual decision-making tasks assume obtain multiple draws given standard gaussian process applications access single realisation data performs kernel learning optimizing marginal likelihood data respect covariance function hyperparameters described supplementary material. however single realisation data highly constrained ability learn expressive kernel function requiring make strong assumptions covariances extract useful information data. simulating datapoints known kernel visualising empirical estimate known covariance matrix empirical estimate cases look nothing like however perhaps surprisingly even small number multiple draws recover wide array covariance matrices using empirical estimator ¯y¯y data matrix draws vector empirical means. typical goal choosing kernel minimize loss function evaluated training data ultimately minimize generalisation error. want reverse engineer kernel model based training data predictions model given training data. single sample extrapolation test inputs based training points gaussian noise probability given posterior predictive distribution gaussian process probability utility function kernel learning much like marginal likelihood. supplementary material details distributions. objective instance view different human extrapolations multiple draws common generative model. clearly assumption entirely correct since different people different biases naturally suits purposes interested differences people shared inductive biases assuming multiple draws common generative model provides extraordinary statistical strength learning shared biases. ultimately consider modelling human responses separately collectively studying differences similarities responses. option learning prediction kernel specify ﬂexible parametric form learn optimizing chosen objective functions. approach choose recent spectral mixture kernels wilson adams model wide range stationary covariances intended help automate kernel selection. however note objective function readily applied parametric forms. also consider empirical non-parametric kernel estimation since non-parametric kernel estimators ﬂexibility converge positive deﬁnite kernel thus become appealing signal strength provided multiple draws stochastic process. wish discover kernels capture human inductive biases learning functions extrapolating complex ambiguous training data. start testing consistency kernel learning procedure section section study progressive function learning. indeed humans participants different representation different observed data examining representations progressively adapt information shed light prior biases. section learn human kernels extrapolate tasks difﬁcult gaussian processes standard kernels. section study model selection human function learning. human participants recruited using amazon’s mechanical turk experimental materials described supplement demonstrations provided http//functionlearning.com/. considering stationary ground truth kernels spectral mixture kernel learning; otherwise non-parametric empirical estimate. simulations known ground truth test consistency kernel learning procedure effects multiple posterior draws converging kernel used make predictions. sample datapoints kernel krbf) exp||/) random input locations. conditioned data sample multiple posterior draws containing datapoints spectral mixture kernel components prediction kernel deliberately trained data kernel. reconstruct prediction kernel learn parameters randomly initialized spectral mixture kernel components figure compares learned kernels different numbers posterior draws data kernel prediction kernel single posterior draw learned kernel captures high-frequency component prediction kernel fails reconstructing low-frequency component. multiple draws learned kernel capture longerrange dependencies. fact learned kernel converges prediction kernel different data kernel shows consistency procedure could used infer aspects human inductive biases. asked humans extrapolate beyond training data sets functions drawn known kernels. learners extrapolated problems sequence thus opportunity progressively learn underlying kernel set. test figure reconstructing kernel used predictions training data generated kernel multiple independent posterior predictions drawn spectral-mixture prediction kernel number posterior draws increases learned spectral-mixture kernel converges prediction kernel. figure progressive function learning. humans shown functions sequence asked make extrapolations. observed data black human predictions blue true extrapolations dashed black. observed data drawn rational quadratic kernel identical data learned human kernels alone seeing data true data generating rational quadratic kernel shown red. observed data drawn product spectral mixture linear kernels identical data empirical estimate human posterior covariance matrix responses true posterior covariance matrix progressive function learning repeated ﬁrst function experiment functions set. asked extrapolation judgments provide information inductive biases interpolation pose difﬁculties conventional gaussian process kernels show learned human kernel data generating kernel human kernel learned spectral mixture kernel trained data figures respectively corresponding figures initially human learners kernel show heavy tailed behaviour bias decreasing correlations distance input space human learners high degree variance. time seen figure conﬁdent predictions accurately able estimate true signal variance function. visually extrapolations look conﬁdent reasonable. indeed human learners adapt representations data. however figure human learners still over-estimating tails kernel perhaps suggesting strong prior bias heavy-tailed correlations. learned kernel contrast cannot capture heavy tailed nature training data gaussian parametrization. moreover learned kernel underestimates signal variance data overestimates noise variance explain away heavy tailed properties data second rows consider problem highly complex structure participants. here functions drawn product spectral mixture linear kernels. participants functions appear expect linear trends become similar predictions. figures show learned true predictive correlation matrices using empirical estimators indicate similar correlation structure. experiments reported section follow general procedure described section case human participants asked extrapolate single training sets counterbalanced order sawtooth function step function training data shown dashed black lines. types functions notoriously difﬁcult standard gaussian process kernels sharp discontinuities non-stationary behaviour. figures used agglomerative clustering process human responses three categories shown purple green blue. empirical covariance matrix ﬁrst cluster shows dependencies sawtooth form characterize cluster. figures sample learned human kernels following colour scheme. samples appear replicate human behaviour purple samples provide reasonable extrapolations. contrast posterior samples spectral mixture kernel trained black data case quickly revert prior mean shown data sufﬁciently sparse non-differentiable non-stationary spectral mixture kernel less inclined produce long range extrapolation human learners attempt generalise small amount information. step function clustered human extrapolations based response time total variation predicted function. responses took seconds vary units shown figure appeared reasonable. responses shown figure empirical covariance matrices sets predictions figures show characteristics responses. ﬁrst matrix exhibits block structure indicating step-functions second matrix shows fast changes positive negative dependencies characteristic high frequency responses. posterior sample extrapolations using empirical human kernels shown figures figures show posterior samples spectral mixture kernels trained black data spectral mixture kernel able extract structure overconﬁdent unconvincing compared human kernel extrapolations. kernel unable learn much structure data. figure learning unconventional kernels. sawtooth function three clusters human extrapolations. empirically estimated human covariance matrix corresponding posterior draws empirically estimated human covariance matrices. posterior predictive draws spectral mixture kernel learned dashed black data. empirically estimated human covariance matrices posterior samples using matrices. respectively spectral mixture kernel extrapolations data black. entirely consistent data. occam’s razor describes natural tendency favour simplest hypothesis data foundational importance statistical model selection. example mackay argues occam’s razor automatically embodied marginal likelihood performing bayesian inference indeed number sequence example marginal likelihood computations show millions times probable even prior odds equal. occam’s razor vitally important nonparametric models gaussian processes ﬂexibility represent inﬁnitely many consistent solutions given problem avoid overﬁtting bayesian inference. example marginal likelihood gaussian process separates automatically calibrated model model complexity terms sometimes referred automatic occam’s razor marginal likelihood probability randomly sample parameters would create dataset simple models generate small number datasets marginal likelihood must normalise generate datasets high probability. complex models generate wide range datasets typically probability. given dataset marginal likelihood favour model appropriate complexity. argument illustrated illustrates principle gps. examine occam’s razor human learning compare gaussian process marginal likelihood ranking functions consistent data human preferences. generated figure bayesian occam’s razor. marginal likelihood possible datasets. vertical black line corresponds example dataset posterior mean functions kernel short large maximum marginal likelihood length-scales. data denoted crosses. dataset sampled kernel presented users subsample points well seven possible function internally labelled follows predictive mean maximum marginal likelihood hyperparameter estimation; generating function; predictive means larger smaller length-scales repeated procedure four times create four datasets total acquired human rankings each total rankings. participant shown unlabelled functions different random orderings. datasets along participant instructions supplement available http//functionlearning.com/. figure human occam’s razor. number ﬁrst place votes function. average human ranking functions compared ﬁrst place ranking deﬁned average human ranking average marginal likelihood ranking functions. ‘ml’ marginal likelihood optimum ‘truth’ true extrapolation. blue numbers offsets length-scale marginal likelihood optimum. positive offsets correspond simpler solutions. figure shows number times function voted best data. proportion ﬁrst place votes function follows internal ordering deﬁned above. maximum marginal likelihood solution receives ﬁrst place votes. functions received similar numbers ﬁrst place votes; choices strongly favoured total votes maximum marginal likelihood solution. solutions smaller length-scale marginal likelihood best represented functions received relatively small number ﬁrst place votes. ﬁndings suggest average humans prefer overly simple overly complex explanations data. moreover participants generally agree marginal likelihood’s ﬁrst choice preference even true generating function. however data also suggest participants wide array prior biases leading different people often choosing different looking functions ﬁrst choice furthermore participants responded ﬁrst ranked choice likely generated data looks very similar would imagined. it’s possible highly probable solutions underrepresented figure might imagine example particular solution never ranked ﬁrst always second. figure compared show average rankings standard deviations improbable speciﬁc preferences functions finally figure compare average human rankings average marginal likelihood rankings. clear trends humans agree marginal likelihood best empirically decreasing length-scale best value monotonically decreases solution’s probability; humans penalize simple solutions less marginal likelihood function receiving last place ranking marginal likelihood. despite observed human tendency favour simplicity marginal likelihood gaussian process marginal likelihood optimisation surprisingly biased towards under-ﬁtting function space. generate data known length-scale mode marginal likelihood average over-estimate true length-scale unconstrained estimating covariance matrix converge maximum likelihood estimator degenerate therefore biased. parametrizing covariance matrix length-scale restricts matrix low-dimensional manifold full space covariance matrices. biased estimator remain biased constrained lower dimensional manifold long manifold allows movement direction bias. increasing length-scale moves covariance matrix towards degeneracy unconstrained maximum likelihood estimator. data low-dimensional manifold becomes constrained less inﬂuenced under-ﬁtting bias. shown human learners systematic expectations smooth functions deviate inductive biases inherent kernels used past models function learning; possible extract kernels reproduce qualitative features human inductive biases including variable sawtooth step patterns; human learners favour smoother simpler functions even comparison models tend over-penalize complexity; possible build models extrapolate human-like ways beyond traditional stationary polynomial kernels. focused human extrapolation noise-free nonparametric relationships. approach complements past work emphasizing simple parametric functions role noise kernel learning might also applied settings. particular iterated learning experiments provide draw samples reﬂect human learners’ priori expectations. like function learning experiments past experiments presented learners sequential data. approach following little shiffrin instead presents learners plots functions. method useful reducing effects memory limitations sources noise possible people show different inductive biases across presentation modes. future work using multiple presentation formats underlying relationships help resolve questions. finally ideas discussed paper could applied generally discover interpretable properties unknown models predictions. encounters fascinating questions intersection active learning experimental design information theory. kyunghee david meyer. function learning induction continuous stimulusresponse relations. journal experimental psychology learning memory cognition edward delosh jerome busemeyer mark mcdaniel. extrapolation sine abstraction function learning. journal experimental psychology learning memory cognition jerome busemeyer eunhee byun edward delosh mark mcdaniel. learning functional relations based experience input-output pairs humans artiﬁcial neural networks. concepts categories c.e. rasmussen ghahramani. inﬁnite mixtures gaussian process experts. advances neural information processing systems proceedings conference volume page press mark mcdaniel jerome busemeyer. conceptual basis function learning extrapolation comparison rule-based associative-based models. psychonomic bulletin review michael kalish thomas grifﬁths stephan lewandowsky. iterated learning intergenerational knowledge transmission reveals inductive biases. psychonomic bulletin review samuel johnson andy frank keil. simplicity goodness-of-ﬁt explanation case intuitive curve-ﬁtting. proceedings annual conference cognitive science society pages andrew gordon wilson elad gilboa arye nehorai john cunningham. fast kernel learning multidimensional pattern extrapolation. advances neural information processing systems supplementary material available http//www.cs.cmu.edu/˜andrewgw/ humansupp.pdf provide brief review gaussian processes additional experiments regarding under-ﬁtting property maximum marginal likelihood estimation kernel lengthscales. also provide instructions questions asked human experiments. participate exact experiments http//www.functionlearning.com.", "year": 2015}