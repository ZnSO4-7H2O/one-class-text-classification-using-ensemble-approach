{"title": "Deep Bayesian Active Semi-Supervised Learning", "tag": ["cs.LG", "cs.CV", "stat.ML", "68T10, 62H30, 62H35"], "abstract": "In many applications the process of generating label information is expensive and time consuming. We present a new method that combines active and semi-supervised deep learning to achieve high generalization performance from a deep convolutional neural network with as few known labels as possible. In a setting where a small amount of labeled data as well as a large amount of unlabeled data is available, our method first learns the labeled data set. This initialization is followed by an expectation maximization algorithm, where further training reduces classification entropy on the unlabeled data by targeting a low entropy fit which is consistent with the labeled data. In addition the algorithm asks at a specified frequency an oracle for labels of data with entropy above a certain entropy quantile. Using this active learning component we obtain an agile labeling process that achieves high accuracy, but requires only a small amount of known labels. For the MNIST dataset we report an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These results are obtained without employing any special network architecture or data augmentation.", "text": "proaches obtain strong predictive models using labels kingma pitelis rasmus rifai weston assume situation complete data large labels known small fraction ﬁeld semi-supervised learning long history. already suddarth unlabeled data injected training neural networks order improve generalization performance. approaches rely expectation maximization technique clustering algorithm. semisupervised context used assign unlabeled data ﬁnite number clusters initially deﬁned small labeled samples. initial model trained then using resulting model labels assigned unlabeled data turn used train model. pseudo-label approach introduced technique. uses labels predicted neural network viewed well auxiliary loss training phase inserted reduce classiﬁcation entropy unlabeled data. well known strategy works well presence density class separation. thus unclear approach able adequately classify samples high classiﬁcation uncertainty. case quantitative measure classiﬁcation uncertainty deﬁned unlabeled data used training uncertainty small. however samples typically retain high uncertainty semi-supervised training cycle. active learning comes play valuable acquire ground truth labels samples high classiﬁcation uncertainty training. active semi-supervised learning complement naturally. learning approaches beneﬁt good uncertainty quantiﬁcation mechanisms. advent monte-carlo dropout instrument hand makes feasible construct sensitive metrics monitor classiﬁcation uncertainty. bayesian inference used active deep learning approach introduced many applications process generating label information expensive time consuming. present method combines active semi-supervised deep learning achieve high generalization performance deep convolutional neural network known labels possible. setting small amount labeled data well large amount unlabeled data available method ﬁrst learns labeled data set. initialization followed expectation maximization algorithm training reduces classiﬁcation entropy unlabeled data targeting entropy consistent labeled data. addition algorithm asks speciﬁed frequency oracle labels data entropy certain entropy quantile. using active learning component obtain agile labeling process achieves high accuracy requires small amount known labels. mnist dataset report error rate using labels labels. results obtained without employing special network architecture data augmentation. recent years deep learning shown great potential solving classiﬁcation regression tasks increasing complexity diﬃculty. academic purposes several labeled data sets associated tasks available support facilitate research machine learning. though many practical applications data available abundance labeled information readily available process generating labels time consuming expensive. therefore development methods provide strong predictive models labels possible ﬁeld high interest. recent years also eﬀorts designing specialized network architectures incorporate components like denoising auto-encoders rasmus also deep generative models used semi-supervised learning kingma paper present deep bayesian active semi-supervised learning approach based deep learning approach classiﬁcation tasks paired active learning component approximate bayesian uncertainty. ﬁrst train convolutional neural network small sample labeled training data. afterwards employ technique i.e. iteratively predict classes assign pseudo-labels unlabeled data set. then train epoch pseudo-labeled data ground-truth-labeled data. make sure prediction accuracy ground truth remains high. process algorithm asks oracle additional label information neural network shows increased classiﬁcation uncertainty e.g. high classiﬁcation entropy. predictions uncertainty estimations incorporate dropout inference. remainder work structured follows section classify method respect existing approaches literature. introduce method detail section including necessary notations. using simple example section motivate combination active semi-supervised learning. using mnist dataset compare settings section hand unlabeled data present training beginning hand unlabeled data added incrementally. settings combined diﬀerent label acquisition policies. concluding experiments compare method semi-supervised active learning approaches. eﬀort possible. combine components active learning semi-supervised learning approximate bayesian uncertainty quantiﬁcation construct robust method achieves high accuracy. related semi-supervised deep learning method including dropout inference published hyams incrementally assigns labels data highest predicted class probability chosen threshold adds respective data training data facilitate active learning components. hand active deep learning approach making bayesian uncertainty introduced work stresses importance approximate bayesian model uncertainty active learning shows comparisons semi-supervised methods. however approach make semisupervised learning. rasmus so-called ladder network denoising components introduced achieved outstanding results mnist dataset. specialized network architecture however trivially generalizable complex tasks like e.g. object classiﬁcation/detection semantic segmentation state-of-the-art networks huge. deep generative models employed successfully semisupervised learning suﬀer scalability issues well kingma presented approach show combination active semi-supervised learning techniques able achieve similar performance much simpler scalable network architecture. active semi-supervised learning approach introduced method synthetic aperture radar image recognition accepts every iteration chosen number pseudo-labels highest conﬁdence asks oracle chosen number samples lowest conﬁdence. conﬁdence measured terms highest classiﬁcation probability approximate bayesian uncertainty employed approach. observe tests compared average classiﬁcation entropy available initial ground-truth-labeled data plenty unlabeled data classiﬁcation entropy threshold. therefore beginning many thousands unlabeled samples automatically labeled added trainfreedom deﬁnition pseudo-labels choice regularization parameter practical implementation details discussed sections ingredients prescription algorithm pseudo-labels state generic version algorithm specify next paragraph. monte-carlo dropout inference disregarding nature given data prediction task practical performance algorithm strongly depends three factors initial accuracy achieved line pseudolabel quality depends lines producing tiny fraction false positives i.e. incorrect labels. furthermore also address question whether necessary pseudo-label unlabeled data incrementally once. expectation maximization expectation maximization algorithm widely used clustering approach. original unsupervised context clustering algorithm initialized model predeﬁned number classes random parameters. semi-supervised learning random initial model replaced model trained scarce ground truth labels. second phase always unsupervised applies model unlabeled data order cluster here clustering metric provided neural network classiﬁcation entropy. turn viewed adding additional term loss function. afterwards neural network trained self-aﬃrmation manner towards predictions ground truth labels unlike original never reassigned model predictions. denote collection input samples denotes data associated labels ﬁnite label space containing classes following identiﬁed i.e. denote numbers deterministic probability distribution i.e. further denote denote neural network funcc= weights denote softmax e.g. hyams acquisition policy line demanding additional paraground truth graph focus latter aspects. proposed hyams dropout inference generating pseudo-labels semi-supervised setting. active learning setting dropout used evaluate uncertainty prediction thus decide samples label next help oracle. combine approaches follows. figure experiments algorithm training labels class labels pseudo-labels rest active learning initial labels time active semi-supervised learning initial labels pseudo-labels rest labels time. network architecture parameters competing objectives work considering suitable network architecture deepbass. hand employ strong regularization order avoid overﬁtting learning small initially labeled samples. hand initial model re-adjust ground truth obtained oracle iteration requires ﬂexibility. thus addition strong regularization model needs equipped enough learnable parameters. hence tests shown section fully connected neural network input neurons hidden layers neurons output neurons. hidden layer leakyrelu activation function i.e. intentional overﬁtting ground truth choice regularization parameter loss function plays important role. chosen small unlabeled data barely eﬀect chosen large classiﬁcation accuracy ground truth labeled data decrease iteration algorithm proceeds. thus choose rather overﬁt scarce ground truth data order allow algorithm clustering followed dropout srivastava dropout rate. models trained evaluated using keras chollet tensorﬂow backend abadi layers regularized regularization parameter available crossed out. background color gradients depict neural network predicts blue class respectively. classiﬁcation boundary white represents region classiﬁcation uncertainty high. figures diﬀer choice labels. choice easy handle choice represents rather ill-posed case. middle left panel continue semisupervised training adding ground truth labels. resulting model cases sure decision indicated background colors saturated. however classiﬁcation boundary worse compared left panel. ﬁgures also show happen semi-supervised learning perform well especially density distribution class boundaries present. center right panel active learning. every second iteration demand ground truth labels. keep iterating samples labeled i.e. iterations. right hand panel both active semi-supervised learning. active learning performs well active semi-supervised learning latter clearly summarizing tests state classiﬁcation accuracies averaged runs four tests table complement results purely supervised learning using labels respectively. corresponding models trained validation accuracy stagnates. split training validation images. again models trained evaluated using keras tensorﬂow backend. architecture generic building block containing following components dense layer outputs ﬁnal softmax activation. resulting network equipped learnable parameters. convolutional layers trained regularization regularization parameter adam known ground truth perform forward passes. test perform iterations algorithm perform test times re-sampling initial samples. presented results averages runs ground truth up-sampling factor initial neural network trained balanced data containing number samples class. default start labeled samples i.e. class. presenting ground truth labeled data times obtain training accuracy roughly iterations algorithm track performance monitoring validation accuracy. perform active learning acquire labels every iterations. note added labels necessarily class-balanced. experiments entropy thresholding label acquisition policy. figure shows behavior algorithm course iterations averaged runs. left panel study inﬂuence threshold i.e. compare case unlabeled data used training right start strategy pseudo-labeled data added step-wise according threshold comparison made using diﬀerent label acquisition policies. hand labels unlabeled latter strategy motivated fact might happen exclusively acquiring data high classiﬁcation entropy could result overﬁtting data nonseparable distributions consequently slowing convergence algorithm tests initial ground truth labels four approaches share initial models average validation accuracy training labeled samples evenly distributed classes. left panel shows data max. entropy approach slightly superior acquisition policy unlabeled data added step-wise. believe reason part wellbehaved nature mnist dataset. avg. acquisition policy slightly inferior works better available unlabeled data used beginning. summarizing data combined max. entropy acquisition policy reaches accuracy average. result average runs stopping iterations order understand much beneﬁt combining active learning semisupervised learning compare best approach left panel algorithm without active learning component. labels beginning perform semi-supervised learning without adding ground truth labels. results depicted right hand panel show active learning indeed beneﬁcial using algorithm semi-supervised learning. pure semi-supervised approach labels ends average accuracy less active semisupervised learning. note achieve semi-supervised learning labels. results section summarized table complemented result supervised semisupervised learning diﬀerent numbers labels. compared data max. entropy approach pure supervised learning random sample labeled data requires times many labels. samples standard deviation reveals approach robust data resampling. data augmentation. except paragraph tests work performed without data augmentation. however practical setting might make sense data augmentation well. mnist dataset using data augmentation ground truth labeled data slight rotations less degrees slight image scaling height width observe labels enough achieve competitive initial validation accuracies around comparison methods. section provide overview methods semisupervised deep learning active deep learning tests mnist dataset performed. referred works provide numbers labels results labels scarce. compare results data max. entropy deep bayesian active semisupervised learning approach. comparison method times samples ground truth labeled average validation accuracies achieving validation accuracy. comprehensive comparison stated table clearly approach using labeled samples competitive upper spectrum reported results. though main advantages reported previous sections ability yield high accuracies even labeled samples. note full ladder model rasmus sophisticated model incorporating denoising autoencoder structures might lack scalability semi-supervised part method labels reaches validation accuracy similar approach without dropout achieved observed similar results tests without dropout inference reveals impact. semi-supervised weston semi-supervised embedding weston transductive pitelis atlasrbf rifai manifold tangent classiﬁer pseudo-label hyams self training dyn. conf. kingma deep generative models rasmus ladder γ-model virtual adversarial rasmus ladder full active bald entropy ratios active semi-supervised deepbass simple classiﬁcation entropy based uncertainty quantiﬁcation presence approximate bayesian inference well combination semi-supervised learning active learning constitute strength method outperforms state-of-the-art general approaches advanced network architectures. validation data available approach tuned respect thresholding acquisition policy. fact implies additional meta-learning extensions could developed. minor concern might data added active part approach prone overﬁtting. clean restart ﬁnal data splitting tuning could additionally improve performance method. ghahramani dropout bayesian approximation representing model uncertainty deep learning proceedings international conference international conference machine learning volume icml’. york jmlr.org learning discrete representations information maximizing self-augmented training icml. vol. proceedings machine learning research. pmlr hyams greenfeld bank self-training. corr pitelis russell agapito semi-supervised learning using unsupervised atlas machine learning knowledge discovery databases. calders berlin heidelberg springer berlin heidelberg suddarth kergosien ruleinjection hints means improving network performance learning time neural networks. almeida wellekens. berlin heidelberg springer berlin heidelberg weston deep learning semisupervised embedding neural networks tricks trade second edition. montavon k.-r. m¨uller. berlin heidelberg springer berlin heidelberg", "year": 2018}