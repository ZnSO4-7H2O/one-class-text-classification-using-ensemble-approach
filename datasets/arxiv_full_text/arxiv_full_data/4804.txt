{"title": "Shaping Proto-Value Functions via Rewards", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper, we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are available during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs.", "text": "learning value function important sub-problem solving given reinforcement learning task. choice representation value function directly affects learning. widely used representation value function linear architecture wherein value function written linear combination ‘pre-selected’ basis functions. scenario choosing right basis function crucial achieving success. often basis functions either selected ad-hoc manner choice based domain knowledge speciﬁc given task. however desirable able choose basis functions task-independent manner. proto-value functions taskindependent basis functions based topology state space. eigen functions random walk operator proto-value functions capture connectivity neighborhood information. contrast supervised learning agent performing task needs learn rewards. however goal-based tasks rewards delayed i.e. agents receive feedback reaching goal state delay cause poor learning rates. reward shaping mechanism providing additional rewards correct behavior non-goal states thereby aiding learning process. paper combine task-dependent reward shaping task-independent protovalue functions obtain reward dependent proto-value functions constructing rpvfs making immediate rewards avaialble sampling phase used construction. show experiments learning rpvf based representation better learning reward shaping pvfs. particular state space symmetrical rewards asymmetrical rpvf capture asymmetry better pvfs. reinforcement learning tasks problems cast framework markov decision processes setting dynamics underlying environment evolves within states called state-space agent performs actions control state system. agent receives reward dependent state action performs. agent aims maximize discounted inﬁnite rewards obtained result actions. formally action selection mechanism known policy agent aims learn optimal policy. order learn optimal behavior/policy agent ﬁrst needs evaluate current behavior. value function corresponding given policy state space real numbers captures total discounted reward agent collects following policy knowledge value function agent improve behavior hence learning value function efﬁcient manner assumes importance. agent’s choice representing value function affects learning process. desirable property representation compact linear function representation widely used wherein value function represented linear combination basis functions. general choice basis guided task-speciﬁc knowledge task-speciﬁc information primitive functions radial basis functions polynomial bases tile coded bases chosen. neverthless desirable able construct basis functions little information task. proto-value functions bases constructed task-independent manner applied wide variety domains. pvfs obtained diagonalizing symmetric diffusion operators empirically learned graph representing underlying state space. diffusion model random walk undirected graph probability transitioning vertex neighbor proportional degree intended capture information graph. pvfs equivalent fourier bases shown capture intricate connectivity underlying state space primitive bases fourier polynomials capture. presented representational policy iteration algorithm combining basis construction least squares policy iteration whilst topology dictated underlying graph constitutes form domain knowledge researchers also looked means enable faster learning. reward shaping process providing additional rewards learning agent guide learning process. reward function chosen preserves optimal policy. reward shaping task-speciﬁc since shaping function dependent state space varies depending goal reward structure. contribution paper combine ideas task-independent construction task-speciﬁc reward shaping construct reward based proto-value functions idea behind construction observation actual neighborhood interested generated value function itself. while topologically near states might similar values also true value affected immediate rewards. general reward though immediate rewards obtained sampling phase used construction. modify diffusion operator using immediate rewards order construct rpvfs. show success rpvfs experiments benchmark tasks. highlights experiments demonstrate similarity matrices diffusion matrix used generate features reward shaping beniﬁt features chosen. particular state space symmetrical rewards asymmetrical rpvf capture asymmetry better pvfs. organization ﬁrst present overview reinforcement learning paradigm emphasizing need learn value function. discuss proto-value functions brief. next present algorithm following discuss objective reward shaping. finally present rpvf experimental results. environment dynamics underlying environment captured framework markov decision process -tuple state space action space probability transition kernel reward function. probability transition kernel speciﬁes probability transitioning state state action reward function speciﬁes reward obtained performing action state denoted agent behavior behavior agent captured actions makes every state. parlance action selection mechanism called policy. formally policy mean sequence functions describe manner action picked given state time important types policies also useful stationary randomized policy given probability distribution actions stationary deterministic policy given state space action space. value function deﬁne inﬁnite horizon discounted reward value function discount factor similarly also deﬁne inﬁnite horizon discounted reward stateaction value function agent agent three important building blocks sub-functions namely sample collection representation learning algorithm. learner represents state environment time point feature space. learning algorithm makes samples obtained environment past behavior learn. behavior agent dictated policy makes choose actions. clear order compute optimal behavior agent needs learn even case agent wants improve given policy evaluate substituting lead improved policy thus learning value function central learning correct behavior. learning dependent agent’s representing value functions. value function representation widely used representation linear function representation wherein value state-action pair expressed weighted comi= φiwπ feature state learned weight vector. linear representation compactly represented feature matrix basis functions. classical numerical schemes value iteration policy iteration linear programming choose look table representation. look table representation standard basis chosen i.e. thus many basis functions number state-action pairs result might always efﬁcient. learning algorithm least squares policy iteration algorithm widely used algorithm makes linear representation learn value function. lspi makes least squares temporal difference learning computes solving approximate ﬁxed-point equation given matrix specifying probability transitioning state-action pairs. re-arranging αhπφwπ least-squares regression know dπ)−φdπr diagonal matrix whose entries stationary distributions various state-action pairs further letting φdπr follows −bπ. general good idea select basis functions using domain knowledge. domain knowledge absent representations based well known primitive functions radial polynomial fourier used. however representations based primitive functions might yield good results.hence desirable able choose basis functions task-independent manner. proto-value functions task-independent basis functions based topology state space. eigen functions random walk operator proto-value functions capture connectivity/neighborhood information. observe neighborhood information also affected reward structure. denote graph edge vertex denote adjacency matrix vertices connected connected. deﬁne following matrices graphical representation vertices states system adjacency matrix treated measure similarity eigen vectors chosen representation basis function. instance spectral clustering technique uses eigen-value similarity matrix perform dimensionality reduction. well used methods choosing eigen vector corresponding second smallest eigenvalue normalized laplacian. spectrally similar alternative choose eigen vector corresponding highest eigen-values random walk diffusion matrix represent transition probability vertex neighbor vertex proportional degree. note largest eigen values. thus value function approximated linear combination eigen vectors corresponding higher higher values largest eigen values transition matrix however absence knowledge transition matrix make diffusion matrix obtained graph adjacency matrix. figure three room task. agent needs start reach goal-state room size walls separate room cause discontinuities makes representation based primitive functions ineffective. consider three-room problem wherein agent move starting position top-left side ﬁrst room goal state bottom-right third room. task particularly difﬁcult presence walls create discontinuity i.e. representation based primitive functions polynomial radial would account discontinuity. authors demonstrated power proto-value functions approximating complicated value function. representational policy iteration algorithm template algorithm using experiments. since model information available lstd algorithm learns samples trajectories. present lspi algorithm makes lstdq variant lstd algorithm). fundamental difference reinforcement learning tasks supervised learning that agent needs learn using feedback obtained form rewards receives actions. learning feedback makes tasks challenging supervised learning problems wherein correct/right actions provided learner training stage problem. difﬁculty learning feedback pronounced especially case goal based tasks wherein agent reach goal-state part state space however agent receives reward states goal-state. thus behavior states goal-state clear results slower convergence algorithms. scenario rewarding correct behavior agents intermediate states helpful. mechanism providing external rewards right behavior addition rewards obtained environment called reward shaping. reward shaping serves indicator agent’s progress seen improvement algorithmic part learning agent. reward-shaping ﬁrst introduced wherein authors furnished conditions reward shaping preserves optimal policies. particular known reward shaping functions potential functions well preserve structure theorem given reward shaping function optimal policies pvfs well reward shaping though conceptually different ultimately help efﬁcient value function learning. pvfs capture underlying neighborhood information making connectivity graph associated mdp. however reality care nearness associated topological neighborhood state space nearness value functions. nearness observe also affected underlying reward structure. also pvfs constructed sampling state space phase also observe immediate rewards. case goal oriented tasks immediate rewards might hold true mdps general reward structure. immediate rewards indicators agent’s preference actions locally. consider instance goal-based however negative rewards certain states. given agent needs move step closer goal stage states negative rewards equivalent making additional steps agent’s immediate action prefer states least negative reward amongst immediate neighbors. given adjacency matrix state intuitive model agent’s actions positive constant models afﬁnity. paper construct reward based proto-value functions looking diffusion matrix combines task-dependent rewards task-independent connectivity information. demonstrate following experiments section. similarity matrices diffusion matrix used generate features show gaussian kernel matrix generated using optimal value function data points also yields good features. further show proto-value functions three-room problem recoverd even walls absent assigns appropriate negative rewards cells corresponding ‘wall’ states. short show using equivalent case. reward shaping work features show irrespective whether additional reward shaping used proﬁle learnt value function limited choice basis. particular state space symmetrical rewards asymmetrical rpvf capture asymmetry better pvfs. gaussian kernel matrix given positive scaling constant. note similarity matrix assigns nearby states higher value fact evident spectral clustering technique involves computing eigen-vectors obtain k-dimensional embedding component eigen-vector. interested data points kernel matrix entries observe second eigen-vector kernel matrix close approximation optimal value function. eigen-vectors graph laplacian -room graph laplacian reward based diffusion matrix simple grid negative rewards place wall shown figure shows optimal value function learned value functions grid world domain notice learning using rpvf better learning using with/without reward shaping. reaching goal state actions intermediate states receive reward. allowable actions move down right left. state space grid given denotes bottom-left cell top-right cell. evident learning process sped agent rewarded actions take either right. right table even case proﬁle learnt value function change resulting policy performed moderately. algorithm results shown case proﬁle learnt value function resembles optimal value function. further also observed policy returned case performed better i.e. made based representation grid world problem optimal value function shown plot chose i.e. eigen functions corresponding largest eigen values diffusion matrix constructed adjacency matrix. algorithm result shown notice value function learnt algorithm quite resemble proﬁle optimal value function consequently resulted moderately good policy. evaluated further also retaining however eigen functions corresponding ﬁrst largest eigen values cases. however third fourth eigen functions differed difference eigen function shows difference proﬁles corresponding learnt value functions. also compared performance pvfs rpvfs variant grid world problem where addition goal-state certain mine states negative rewards. chose mine states random compared performances across different random grid world problems problem averaged result across initial policies rpi. observed systems rpvf features signiﬁcantly outperforms policy learnt using pvf. figure compares performance rpvfs pvfs different mine-grid tasks. performances averaged different initail policies. policy returned pvfs rpvfs respectively. combined task-independent proto-value function construction task-speciﬁc reward shaping obtain reward based proto-value functions rpvf construction made immediate rewards avaialble sampling phase used construction. also observed rpvfs perform better pvfs goal-based tasks. salient feature rpvfs captured asymmetry value function induced reward structure better pvfs. interesting future direction look extending rpvf continous domains.", "year": 2015}