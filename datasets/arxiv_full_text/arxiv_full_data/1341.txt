{"title": "Implementation of Training Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep learning refers to the shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we gave a detailed analysis of the process of CNN algorithm both the forward process and back propagation. Then we applied the particular convolutional neural network to implement the typical face recognition problem by java. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.", "text": "deep learning refers shining branch machine learning based learning levels representations. convolutional neural networks kind deep neural network. study concurrently. article gave detailed analysis process algorithm forward process back propagation. applied particular convolutional neural network implement typical face recognition problem java. then parallel strategy proposed section. addition measuring actual time forward backward computing analysed maximal speed parallel efficiency theoretically. deep learning refers subfield machine learning based learning levels representations corresponding hierarchy features factors concepts higher-lever concepts defined lower-lever ones lower-lever concepts help define many higher-lever concepts. representation abstraction helps understand data images audio text. concept deep learning comes study artificial neural network multilayer perceptron contains hidden layers deep learning structure. late invention back propagation algorithm used artificial neural network brings hope machine learning creates trend machine learning based statistical models. variety shallow learning models proposed support vector machines boosting logistic regression structure models seen hidden node hidden nodes models gained great success theoretical analysis applications. geoffrey hinton professor university toronto canada dean machine learning students ruslan salakhutdinov published article science trend machine learning academia industry. article points artificial neural network multiple hidden layers excellent ability characteristic learning. facilitate characteristics obtained learning essential description data visualization classification. difficulties deep neural network training overcome layer-wise pre-training. article implementation layer-wise pre-training achieved unsupervised learning. feedforward neural network multilayer perceptron multiple hidden layers artificial neural networks usually known deep neural networks convolutional neural networks kind feedforward neural network. hubel wiesel researched neurons used local sensitive orientation-selective cat’s visual system found special network structure effectively reduce complexity feedback neural networks proposed convolution neural network. efficient recognition algorithm widely used pattern recognition image processing. many features simple structure less training parameters adaptability. become topic voice analysis image recognition. weight shared network structure make similar biological neural networks. reduces complexity network model number weights. generally structure includes layers feature extraction layer input neuron connected local receptive fields previous layer extracts local feature. local features extracted positional relationship features also determined. feature layer computing layer network composed plurality feature map. every feature plane weight neurons plane equal. structure feature uses sigmoid function activation function convolution network makes feature shift invariance. besides since neurons mapping plane share weight number free parameters network reduced. convolution layer convolution neural network followed computing layer used calculate local average second extract unique feature extraction structure reduces resolution. mainly used identify displacement zoom forms distorting invariance two-dimensional graphics. since feature detection layer learns training data avoids explicit feature extraction implicitly learns training data cnn. furthermore neurons feature plane identical weight network study concurrently. major advantage convolution network respect neuronal network connected other. special structure cnn’s local shared weights makes unique advantage speech recognition image processing. layout closer actual biological neural network. shared weights reduces complexity network. particular multi-dimensional input vector image directly enter network avoids complexity data reconstruction feature extraction classification process. face recognition biometric identification technology based facial features persons. study face recognition system began late development computer technology optical imaging techniques improved; late truly entered stages initial applications. practical applications monitoring system collected face images captured cameras often resolution great pose variations. affected pose variation resolution performance face recognition degrades sharply. pose variations bring great challenge face recognition. bring nonlinear factors face recognition. existing machine learning method article convolution neural network solve face recognition. overcome influence pose resolution face recognition. long training time large recognition computing difficult meet real-time requirements delay exceeds range tolerance. cloud platform concurrently speed computing process. convolutional neural networks applied many different areas. yann lecun team specially designed convoutional neural networks deal variability shapes shown outperform techniques. team present fast fully parameterizable implementation convolutional neural network variants image classification. another team proposes novel frontends robust language identification using convolutional neural network trained automatic speech recognition what’s more convolutional neural networks used visual recognition many areas facial point detection house numbers digit classification multi-digit number recognition street view imagery. besides works many teams focusing speed convnets. example multi-gpu training convnets. work facebook group consider standard architecture trained imagenet dataset classification investigate methods speed convergence parallelizing training across multiple gpus. principle convelutional neural networks methodology convolution neural network algorithm multilayer perceptron special design identification two-dimensional image information always layers input layer convolution layer sample layer output layer. addition deep network architecturethe convolution layer sample layer multiple. restricted boltzmann machine need layer neurons adjacent layer connections convolution neural network algorithms neuron need feel global image feel local area image. addition neuron parameter same namely sharing weights namely neuron convolution kernels deconvolution image. sampling process pixels neighborhood pooling steps become pixel scalar weighting weighted bias activation function produce narrow times feature sharing weights sampling time space extract feature reduce size training parameters.the advantage algorithm avoid explicit feature extraction implicitly learn training data;the neuron weights surface feature mapping thus network learn parallelly reduce complexity network;adopting sampling structure time space deformation displacement;input information network topology good match unique advantages speech recognition image processing. algorithm need experience architecture design need debug unceasingly practical application order obtain suitable particular application architecture cnn. based gray image input preprocess stage turning size image. design depth layer convolution model input layer convolution layer sampling layer convolution layer sampling layer hidden layer output layer view input preprocessing total different pictures. layer convolution convolution layer adopts convolution kernels size convolution kernels produce feature feature contains neurons. point total parameters trained feature contains neurons. sampling window matrix sampling step size layer contains connections. every feature layer contains weights bias total parameters trained feature graph feature graph contains neurons adopts full connection namely characteristic figure used belong convolution kernels characteristics sample layer convolution feature graph contains weights bias. layer figure. contains total parameters sampling layer containing feature feature contains neurons total containing neurons. characteristic figure sampling window trainable parameters. time parallel execution speed-up ratio speed-up efficiency=speed-up ratio/n images nodes time forward pass training picture time backward propagation training picture time updating weight bias convolution neural network data used yale face database. choose images analysis. algorithm need divide phases. first need train algorithm. purpose phase determine minimum error used next phase. must ensure algorithm converge certain point. training process error reduced becomes constant. constant used threshold next step. figure shows error change repeating times. best error horizontal axis represents number iterations vertical axis represents error. seconds constant obtained first step threshold judge whether algorithm stop. table shows training process successful time consumed algorithm. table represents algorithm succeed contrast represents algorithm failed. average time used algorithm work accomplish face recognition using deep learning algorithm. mainly apply algorithm convolution neural network excavate deep information multi-layer network process face recognition .and also utilize algorithm make parallel computing cloud platform accelerating process face recognition analyzing theoretical acceleration ratio experimental verification. experimental results show achieved good results. course parallelism coarse-grained still many modules fine-grained algorithm. focus future continue improve work. time work together complete course task prof. chen takes much effort offer guide help. first person must offer thanks mr.chen. time major team leader tianyi also deserves sincerest thanks. done much work organize teammates coordinate everyone’s work. lastlythanks everybody team reach consensus make concerted efforts complete work time publish website lecun bottou bengio gradient-based learning applied document recognition. proceedings ieee ciresan ueli meier jonathan masci flexible high performance convolutional neural networks image classification. proceedings twenty-second ternational joint conference artificial intelligence hinton imagenet classification deep convolutional neural networks nips. advances neural information processing systems yadan adams taigman multi-gpu training convnets. eprint arxiv ferrer lawson application convolutional neural networks language ntification noisy conditions//proc. speaker odyssey workshop wang tang deep convolutional network cascade facial point detection/ computer vision pattern recognition ieee conference ieee abdel-hamid mohamed jiang applying convolutional neural networks cepts hybrid nn-hmm model speech recognition// acoustics speech signal proces sing ieee international conference ieee boureau ponce lecun theoretical analysis feature pooling visual recognit ion. international conference machine learning haifa israel lecun kavukcuoglu farabet convolutional networks applications vision// circuits systems proceedings ieee international symposium ieee lecun convolutional neural networks applied housenumbers digit classification pattern recognition international conference ieee goodfellow bulatov ibarz multi-digit number recognition street view gery using deep convolutional neural networks. arxiv preprint arxiv.", "year": 2015}