{"title": "Context Encoders: Feature Learning by Inpainting", "tag": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "text": "figure qualitative illustration task. given image missing region human artist trouble inpainting automatic inpainting using context encoder trained reconstruction loss shown using adversarial losses possible learn predict structure using convolutional neural networks class models recently shown success across variety image understanding tasks. given image missing region train convolutional neural network regress missing pixel values call model context encoder consists encoder capturing context image compact latent feature representation decoder uses representation produce missing image content. context encoder closely related autoencoders shares similar encoder-decoder architecture. autoencoders take input image present unsupervised visual feature learning algorithm driven context-based pixel prediction. analogy auto-encoders propose context encoders convolutional neural network trained generate contents arbitrary image region conditioned surroundings. order succeed task context encoders need understand content entire image well produce plausible hypothesis missing part. training context encoders experimented standard pixel-wise reconstruction loss well reconstruction plus adversarial loss. latter produces much sharper results better handle multiple modes output. found context encoder learns representation captures appearance also semantics visual structures. quantitatively demonstrate effectiveness learned features pre-training classiﬁcation detection segmentation tasks. furthermore context encoders used semantic inpainting tasks either stand-alone initialization non-parametric methods. visual world diverse highly structured humans uncanny ability make sense structure. work explore whether state-of-the-art computer vision algorithms same. consider image shown figure although center part image missing easily imagine content surrounding pixels without ever seen exact scene. even draw shown figure ability comes fact natural images despite diversity highly structured humans able understand structure make visual predictions even seeing parts scene. paper show reconstruct passes low-dimensional bottleneck layer obtaining compact feature representation scene. unfortunately feature representation likely compresses image content without learning semantically meaningful representation. denoising autoencoders address issue corrupting input image requiring network undo damage. however corruption process typically localized low-level require much semantic information undo. contrast context encoder needs solve much harder task large missing areas image can’t hints nearby pixels. requires much deeper semantic understanding scene ability synthesize high-level features large spatial extents. example figure entire window needs conjured thin air. similar spirit wordvec learns word representation natural language sentences predicting word given context. like autoencoders context encoders trained completely unsupervised manner. results demonstrate order succeed task model needs understand content image well produce plausible hypothesis missing parts. task however inherently multi-modal multiple ways missing region also maintaining coherence given context. decouple burden loss function jointly training context encoders minimize reconstruction loss adversarial loss. reconstruction loss captures overall structure missing region relation context adversarial loss effect picking particular mode distribution. figure shows using reconstruction loss produces blurry results whereas adding adversarial loss results much sharper predictions. evaluate encoder decoder independently. encoder side show encoding context image patch using resulting feature retrieve nearest neighbor contexts dataset produces patches semantically similar original patch. validate quality learned feature representation ﬁne-tuning encoder variety image understanding tasks including classiﬁcation object detection semantic segmentation. competitive state-of-the-art unsupervised/selfsupervised methods tasks. decoder side show method often able realistic image content. indeed best knowledge ﬁrst parametric inpainting algorithm able give reasonable results semantic hole-ﬁlling context encoder also useful better visual feature computing nearest neighbors nonparametric inpainting methods. computer vision made tremendous progress semantic image understanding tasks classiﬁcation object detection segmentation past decade. recently convolutional neural networks greatly advanced performance tasks success models image classiﬁcation paved tackle harder problems including unsupervised understanding generation natural images. brieﬂy review related work sub-ﬁelds pertaining paper. unsupervised learning cnns trained imagenet classiﬁcation million labeled examples learn features generalize well across tasks however whether semantically informative generalizable features learned images alone without labels remains open question. earliest work deep unsupervised learning autoencoders along similar lines denoising autoencoders reconstruct image local corruptions make encoding robust corruptions. context encoders could thought variant denoising autoencoders corruption applied model’s input spatially much larger requiring semantic information undo. weakly-supervised self-supervised learning recently signiﬁcant interest learning meaningful representations using weakly-supervised self-supervised learning. useful source supervision temporal information contained videos. consistency across temporal frames used supervision learn embeddings perform well number tasks another consistency track patches frames video containing task-relevant attributes coherence tracked patches guide training ego-motion read non-vision sensors used supervisory signal train visual features closely related present paper efforts exploiting spatial context source free plentiful supervisory signal. visual memex used context nonparametrically model object relations predict masked objects scenes used context establish correspondences unsupervised object discovery. however approaches relied hand-designed features perform representation learning. recently doersch used task predicting relative positions neighboring patches within image train unsupervised deep feature representations. share high-level goals doersch fundamentally differ approach whereas solving discriminative task context encoder solves pure prediction problem interestingly similar distinction exist using language context learn word embeddings collobert weston advocate discriminative approach whereas wordvec formulate word prediction. important beneﬁt approach supervisory signal much richer context encoder needs predict roughly real values training example compared option among choices likely part difference context encoders take less time train moreover context based prediction also harder cheat since low-level image features chromatic aberration provide meaningful information contrast chromatic aberration partially solves task. hand clear requiring faithful pixel generation necessary learning good visual features. image generation generative models natural images enjoyed signiﬁcant research interest recently radford proposed convolutional architectures optimization hyperparameters generative adversarial networks producing encouraging results. train context encoders using adversary jointly reconstruction loss generating inpainting results. discuss detail section dosovitskiy rifai demonstrate cnns learn generate novel images particular object categories rely large labeled datasets examples categories. contrast context encoders applied unlabeled image database learn generate images based surrounding context. inpainting hole-ﬁlling important point hole-ﬁlling task cannot handled classical inpainting texture synthesis approaches since missing region large local non-semantic methods work well. computer graphics ﬁlling large holes typically done scene completion involving cut-paste formulation using nearest neighbors dataset millions images. however scene completion meant ﬁlling holes left removing whole objects struggles arbitrary holes e.g. amodal completion partially occluded objects. furthermore previous completion relies hand-crafted distance metric gist nearest-neighbor computation inferior learned distance metric. show method often able inpaint semantically meaningful content parametric fashion well provide better feature nearest neighbor-based inpainting methods. figure context encoder. context image passed encoder obtain features connected decoder using channel-wise fully-connected layer described section decoder produces missing regions image. introduce context encoders cnns predict missing parts scene surroundings. ﬁrst give overview general architecture provide details learning procedure ﬁnally present various strategies image region removal. overall architecture simple encoder-decoder pipeline. encoder takes input image missing regions produces latent feature representation image. decoder takes feature representation produces missing image content. found important connect encoder decoder channelwise fully-connected layer allows unit decoder reason entire image content. figure shows overview architecture. encoder encoder derived alexnet architecture given input image size ﬁrst convolutional layers following pooling layer compute abstract dimensional feature representation. contrast alexnet model trained imagenet classiﬁcation; rather network trained context prediction from scratch randomly initialized weights. however encoder architecture limited convolutional layers information directly propagate corner feature another. convolutional layers connect feature maps together never directly connect locations within speciﬁc feature map. present architectures information propagation handled fullyconnected inner product layers activations directly connected other. architecture latent feature dimension encoder decoder. because unlike autoencoders reconstruct original input hence need smaller bottleneck. however fully connecting encoder decoder would result explosion number parameters extent efﬁcient training current gpus would difﬁcult. alleviate issue channel-wise fully-connected layer connect encoder features decoder described detail below. channel-wise fully-connected layer layer essentially fully-connected layer groups intended propagate information within activations feature map. input layer feature maps size layer output feature maps dimension however unlike fully-connected layer parameters connecting different feature maps propagates information within feature maps. thus number parameters channel-wise fully-connected layer compared parameters fully-connected layer followed stride convolution propagate information across channels. decoder discuss second half pipeline decoder generates pixels image using encoder features. encoder features connected decoder features using channel-wise fullyconnected layer. channel-wise fully-connected layer followed series up-convolutional layers learned ﬁlters rectiﬁed linear unit activation function. up-convolutional simply convolution results higher resolution image. understood upsampling followed convolution convolution fractional stride intuition behind straightforward series up-convolutions non-linearities comprises non-linear weighted upsampling feature produced encoder roughly reach original target size. loss function train context encoders regressing ground truth content missing region. however often multiple equally plausible ways missing image region consistent context. model behavior decoupled joint loss function handle continuity within context multiple modes output. reconstruction loss responsible capturing overall structure missing region coherence regards context tends average together multiple modes predictions. adversarial loss hand tries make prediction look real effect picking particular mode distribution. ground truth image context encoder produces output binary mask corresponding dropped image region value wherever pixel dropped input pixels. training masks automatically generated image training iterations described section describe different components loss function. element-wise product operation. experimented losses found signiﬁcant difference them. simple loss encourages decoder produce rough outline predicted object often fails capture high frequency detail stems fact loss often prefer blurry solution highly accurate textures. believe happens much safer loss predict mean distribution because minimizes mean pixel-wise error results blurry averaged image. alleviated problem adding adversarial loss. adversarial loss adversarial loss based generative adversarial networks learn generative model data distribution proposes jointly learn adversarial discriminative model provide loss gradients generative model. parametric functions maps samples noise distribution data distribution learning procedure two-player game adversarial discriminator takes prediction ground truth samples tries distinguish them tries confuse producing samples appear real possible. objective discriminator logistic likelihood indicating whether input real samfigure semantic inpainting results held-out images context encoder trained using reconstruction adversarial loss. first three rows examples imagenet bottom rows paris streetview dataset. results author’s project website. method recently shown encouraging results generative modeling images thus adapt framework context prediction modeling generator context encoder; i.e. customize gans task could condition given context information; i.e. mask however conditional gans don’t train easily context prediction task adversarial discriminator easily exploits perceptual discontinuity generated regions original context easily classify predicted versus real samples. thus alternate formulation conditioning generator context. also found results improved generator conditioned noise vector. hence adversarial loss context encoders ladv ladv currently adversarial loss inpainting experiments alexnet architecture training diverged joint adversarial loss. details follow sections region masks input context encoder image regions dropped out; i.e. zero assuming zero-centered inputs. removed regions could shape present three different strategies here central region simplest shape central square patch image shown figure works quite well inpainting network learns level image features latch onto boundary central mask. level image features tend generalize well images without masks hence features learned general. classiﬁcation pooling provides spatial invariance detrimental reconstruction-based training. consistent prior work still original alexnet architecture feature learning results. evaluate encoder features semantic quality transferability image understanding tasks. experiment images datasets paris streetview imagenet without using accompanying labels. section present visualizations demonstrating ability context encoder semantic details images missing regions. section demonstrate transferability learned features tasks using context encoders pretraining step image classiﬁcation object detection semantic segmentation. compare results tasks unsupervised self-supervised methods demonstrating approach outperforms previous methods. train context encoders joint loss function deﬁned equation task inpainting missing region. encoder discriminator architecture similar discriminator decoder similar generator however bottleneck units supplementary material. used default solver hyper-parameters suggested λrec λadv however things crucial training model. condition adversarial loss noise encoder. higher learning rate context encoder adversarial discriminator. emphasize consistency prediction context predict slightly larger patch overlaps context training higher weight reconstruction loss overlapping region. qualitative results shown figure model performs generally well inpainting semantic regions image. however region ﬁlled lowlevel textures texture synthesis methods often perform better semantic inpainting compare nearest neighbor inpainting show random block prevent network latching constant boundary masked region randomize masking process. instead choosing single large mask ﬁxed location remove number smaller possibly overlapping masks covering image. example shown figure however random block masking still sharp boundaries convolutional features could latch onto. random region completely remove boundaries experimented removing arbitrary shapes images obtained random masks pascal dataset deform shapes paste arbitrary places images covering image. note completely randomize region masking process expect want correlation source segmentation mask image. merely regions prevent network learning low-level features corresponding removed mask. example figure practice found region random block masks produce similarly general feature signiﬁcantly outperforming central region features. random region dropout feature based experiments. pipeline implemented caffe torch. used recently proposed stochastic gradient descent solver adam optimization. missing region masked input image ﬁlled constant mean value. hyper-parameter details discussed sections pool-free encoders experimented replacing pooling layers convolutions kernel size stride. overall stride network remains same results ﬁner inpainting. intuitively reason pooling reconstruction based networks. figure semantic inpainting using different methods held-out images. context encoder well aligned sharp. using adversarial loss results sharp coherent. joint loss alleviate weaknesses them. last columns results plug-in best nearest neighbor patch masked region. reconstructions well-aligned semantically seen figure also shows joint loss signiﬁcantly improves inpainting reconstruction adversarial loss alone. moreover using learned features nearest-neighbor style inpainting sometimes improve results hand-designed distance metrics. table reports quantitative results streetview dataset. alexnet architecture encoder. unfortunately manage make adversarial loss converge alexnet used reconstruction loss. networks trained constant learning rate center-region masks. however random region corruption found learning rate perform better. apply dropout rate channel-wise fully connected layer since parameters layers might prone overﬁtting. training process fast converges iterations hours titan gpu. figure shows inpainting results context encoder trained random region corruption using reconstruction loss. evaluate quality features nearest neighbors masked part image using features context figure note none methods ever center part image whether query dataset image. features retrieve decent nearest neighbors context even though actual prediction blurry loss. alexnet features also perform decently trained labels semantic tasks hand fail semantics. experiment ﬁne-tune standard alexnet classiﬁer pascal number supervised self-supervised unsupervised initializations. train classiﬁer using random cropping evaluate using random crops test image. average classiﬁer output random crops. table shows standard mean average precision score compared methods. random initialization performs roughly imagenet-trained model; however labels. context encoders competitive concurrent self-supervised feature learning methods significantly outperform autoencoders agrawal second quantitative results involves using features object detection. fast r-cnn framework replace imagenet pre-trained network context encoders particular take pre-trained encoder weights pool layer re-initialize fullyfigure context nearest neighbors. center patches whose context close embedding space different methods note appearance center patches never seen methods. method brings close context. table quantitative comparison classiﬁcation detection semantic segmentation. classiﬁcation fast-rcnn detection results pascal test set. semantic segmentation results pascal validation evaluation described section using additional training data removing overlapping images validation results test pascal detection challenge reported table context encoder pre-training competitive existing methods achieving signiﬁcant boost baseline. recently kr¨ahenb¨uhl proposed data-dependent method rescaling pre-trained model weights. signiﬁcantly improves features doersch classiﬁcation detection. however rescaling doesn’t improve results methods including ours. results pascal validation reported table setting outperform randomly initialized network well plain autoencoder trained simply reconstruct full input. context encoders trained generate images conditioned context advance state semantic inpainting time learn feature representations competitive models trained auxiliary supervision. last quantitative evaluation explores utility context encoder training pixel-wise semantic segmentation. fully convolutional networks proposed end-to-end learnable method predicting semantic label pixel image using convolutional network pre-trained imagenet classiﬁcation. replace classiﬁcation pre-trained network used method acknowledgements authors would like thank amanda buster artwork fig. well shubham tulsiani saurabh gupta helpful discussions. work supported part darpa afrl intel muri award awards iis- iis- berkeley vision learning center berkeley deep drive.", "year": 2016}