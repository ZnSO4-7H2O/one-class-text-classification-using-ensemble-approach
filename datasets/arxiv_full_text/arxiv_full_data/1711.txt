{"title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with  Structure", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach.", "text": "recently deep architectures recurrent recursive neural networks successfully applied various natural language processing tasks. inspired bidirectional recurrent neural networks representations summarize past future around instance propose novel architecture aims capture structural information around input label instances. apply method task opinion expression extraction employ binary parse tree sentence structure word vector representations initial representation single token. conduct preliminary experiments investigate performance compare sequential approach. deep connectionist architectures involve many layers nonlinear information processing. allows incorporate meaning representations succeeding layer potentially abstract meaning. recent advancements efﬁciently training deep neural networks enabled application many problems including natural language processing advance application tasks invention word embeddings represent single word dense low-dimensional vector meaning space numerous problems beneﬁted work interested deep learning approaches sequence tagging tasks goal label token given input sentence. recurrent neural networks constitute important class naturally deep architecture applied many sequential prediction tasks. context recurrent neural networks view sentence sequence tokens. view successfully applied tasks language modeling spoken language understanding since classical recurrent neural networks incorporate information past bidirectional variants proposed incorporate information past future bidirectionality especially useful tasks since information provided following tokens usually helpful making decision current token. even though bidirectional recurrent neural networks rely information preceding following words capturing long term dependencies might difﬁcult vanishing gradient problem relevant information distant token investigation might lost. hand depending task relevant token might structurally close token investigation even though away sequence. example verb corresponding object might away terms tokens many adjectives object would close parse tree. addition distance-based argument structure also provides different computing representations allows compositionality i.e. meaning phrase determined composition meanings words comprise result believe many tasks might beneﬁt explicitly incorporating structural information associated token. recursive neural networks compose another class architecture operates structured inputs. applied parsing sentence-level sentiment analysis paraphrase detection given structural representation sentence e.g. parse tree recursively generate parent representations bottom-up fashion combining tokens produce representations phrases eventually producing whole sentence. sentence-level representation used make ﬁnal classiﬁcation given input sentence e.g. whether conveys positive negative sentiment. since recursive neural networks generate representations internal nodes structured representation directly applicable token-level labeling tasks interested. propose explore architecture aims represent structural information associated single token. particular extend traditional recursive neural network framework generates representations subtrees whole sentence upward also propagates downward representations toward leaves carrying information structural environment word. method naturally applicable type labeling task word level however limit opinion expression extraction task work. addition although method applicable type positional directed acyclic graph structure limit attention binary parse trees opinion expression identiﬁcation aims detect subjective expressions given text along characterizations intensity sentiment opinion opinion holder target topic important tasks require ﬁne-grained opinion analysis opinion oriented question answering opinion summarization. work focus detection direct subjective expressions expressive subjective expressions opinion holders targets deﬁned dses consist explicit mentions private states speech events expressing private states eses consist expressions indicate sentiment emotion etc. without explicitly conveying them. example sentence appropriate labels given table refused make statements explicitly expresses opinion holder’s attitude usual indirectly expresses attitude writer. previously opinion extraction tackled sequence labeling problem. approach views sentence sequence tokens labeled using conventional tagging scheme indicates beginning opinion-related expression used tokens inside opinion expression indicates tokens outside opinion-related class. variants conditional random ﬁeld-based approaches successfully applied view similar crf-based methods recurrent neural networks applied problem opinion expression extraction sequential interpretation. however approach ignores structural neural network-based approaches require vector representation input token. natural language processing common representing single token vector one-hot vector token dimensionality vocabulary size corresponding entry vector others results high dimensional sparse representation. alternatively distributed representation maps token real valued dense vector smaller size generally representations learned unsupervised manner large corpus e.g. wikipedia. various architectures explored learn embeddings might different generalization capabilities depending task geometry induced word vector space might interesting semantic properties work employ word vector representations. recurrent neural network class neural network recurrent connections allows form memory. makes applicable sequential prediction tasks arbitrary spatiotemporal dimension. description many natural language processing tasks single sentence viewed sequence tokens. work focus attention elman type networks elman-type network hidden layer time step computed nonlinear transformation current input layer previous hidden layer ht−. then ﬁnal output computed using hidden layer interpret intermediate representation summarizing past used make ﬁnal decision current input. formally nonlinearity sigmoid function output nonlinearity softmax function weight matrices input hidden layer among hidden units respectively output weight matrix bias vectors connected hidden output units respectively. observe deﬁnition information past making decision limiting tasks. simple work around problem include ﬁxed size future context around single input vector. however approach requires tuning context size ignores future information outside context window. another incorporate information future bidirectionality architecture forward weight matrices before backward counterparts them output matrices biases. setting interpreted summary past future respectively around time step make decision input vector employ initial representation intermediate representations past future. therefore bidirectional case perfect information sequence whereas classical elman type network uses partial information. note forward backward parts network independent output layer. means training backpropagating error terms output layer hidden layer parts thought separate trained classical backpropagation time recursive neural networks comprise architecture weights recursively applied structural setting given positional directed acyclic graph visits nodes topological order recursively applies transformations generate representations previously computed representations children. fact recurrent neural network simply recursive neural network particular structure. even though applied positional directed acyclic graph limit attention recursive neural networks positional binary trees given binary tree structure leaves initial representations e.g. parse tree word vector representations leaves recursive neural network computes representations internal node follows left right children weight matrices connect left right children parent bias vector. given square matrices distinguishing whether leaf internal nodes deﬁnition interesting interpretation initial representations leaves intermediate representation nonterminals space. parse tree example recursive neural network combines representations subphrases generate representation larger phrase meaning space depending task ﬁnal output layer root output weight matrix bias vector output layer. supervised task supervision occurs layer. thus learning initial error incurred backpropagated root towards leaves extend aforementioned deﬁnition recursive neural networks propagates information rest tree every leaf node structure. allow make decisions leaf nodes summary surrounding structure. parent weight matrices connect downward representations parent left right children respectively weight matrix connects upward representation downward representation node bias vector downward layer. intuitively node contains information subtree rooted contains information rest tree since every node tree contributon computation thought complete summaries structure around leaves output layer make ﬁnal decision output weight matrices output bias vector. supervised task supervision occurs output layer. then training error backpropagates upwards downward layer downwards upward layer employing backpropagation structure method desired backpropagated errors used update initial representation allows possibility tuning word vector representations setting. note deﬁnition structurally similar unfolding recursive autoencoder however goals architectures different. unfolding recursive autoencoder downward propagates representations well. however intention reconstruct initial representations. hand want downward representations different possible upward representations since capture information rest tree rather particular subtree investigation. thus unfolding recursive autoencoder computing whereas bidirectional recursive neural network does. depending task might want employ sequential context around input vector well task sequential view addition structure. combine bidirectional recurrent neural network bidirectional recursive neural network. allows sequential information structural information around token produce ﬁnal decision architecture seen extension recurrent recursive neural network. training error term backpropagates output layer individual errors combined architectures handled separately allows previously noted training methods architecture. cast problem detecting dses eses separate -class classiﬁcation problems. also experiment joint detection dses opinion holders opinion target -class classiﬁcation problem outside class beginning inside class dses opinion holders opinion targets. compare bidirectional recurrent neural network described section bidirectional recursive network described section combined architecture described section stanford pcfg parser extract binary parse trees sentences precision recall f-measure performance evaluation. since boundaries expressions hard deﬁne even human annotators soft notions measures binary overlap counts every overlapping match predicted true expression correct proportional overlap imparts partial correctness proportional overlapping amount match manually annotated mpqa corpus sentences total. detection separate sentences test -fold cross validation. joint detection opinion holder target manually annotated sentences separate test -fold cross validation. validation used pick best regularization parameter simply coefﬁcient penalizes norm. standard stochastic gradient descent updating weights minibatches sentences. epochs training. furthermore learning rate every architecture instead tuning cross validation since initial experiments showed setting every architecture successfully converges without oscillatory behavior. initial representations tokens pre-trained collobert-weston embeddings initial experiments tuning word vector representations presented severe overﬁtting hence keep word vectors ﬁxed experiments. hidden layers rectiﬁer linear activation max{ experimentally rectiﬁer activation gives better performance faster convergence sparse representations. note recursive network apply transformation leaf nodes internal nodes interpretation belong meaning space. employing rectiﬁer units upward layer causes upward representations internal nodes always nonnegative sparse whereas initial representations dense might negative values causes conﬂict. test impact this experimented sigmoid activation upward layer rectiﬁer activation downward layer caused degradation performance. therefore loss interpretation rectiﬁer activation layers experiments. experimental results detection given tables recurrent network topology means input dimensionality forward hidden layer dimensionality backward dimensionality recursive network means input dimensionality upward layer dimensionality downward layer dimensionality combined network means input upward layer dimensionality downward layer dimensionality forward backward layer dimensionalities asterisk indicates performance statistically signiﬁcantly better others group respect sided paired t-test observe bidirectional recurrent neural network better performance bidirectional recursive combined architectures task detection respect proportional overlap metrics observe signiﬁcant difference respect binary overalp metrics. might explained fact dses tend shorter furthermore since dses exhibit explicit subjectivity neccessarily require contextual investigation around phrase. time detected looking particular phrase. task detection combined network signiﬁcantly better binary f-measure compared others furthermore combined network signiﬁcantly better proportional precision architectures insigniﬁcant loss proportional recall. terms binary measures bi-recursive network precision high recall might suggest complementary behavior architectures. eses tend longer relative dses might explain results. aditionally unlike dses eses often require contextual information interpretation. instance given example table clear usual labeled unless looks context presented sentence. experimental results joint detection opinion holder target given table here combined architecture insigniﬁcantly better performance detecting dses signiﬁcantly better performance detecting opinion holders whereas bi-recurrent network better detecting targets again possible explanation might better utilization contextual information. decide whether named entity opinion holder must link entity opinion expression. therefore possible decide looking particular named entity. joint detection task also investigate performance subset sentences sentence least opinion holder seperated distance. attempt explore impact token-level sequential distance opinion holder opinion expression. results given figure separation distance increases average detection performance combined architecture steady combined network compared bi-recurrent network. might suggest structural information helps better capture cues opinion holders expression. note distance-based subset instances strictly smaller since fewer number sentences conforming constraints causes increase variance. proposed extension recursive neural network carry labeling tasks token level. investigated performance opinion expression extraction task. experiments showed that depending task employing structural information around token might contribute performance. bidirectional recursive neural network downward layer built upward layer whereas bidirectional recurrent neural network forward backward layers separate. causes supervision occur higher level recursive network relative recurrent network makes training relatively difﬁcult. alleviate difﬁculty unsupervised pre-training upward layer similar semi-supervised training might employed future research direction. tuning word vector representations pre-training might positive impact performance recursive network since learned representations phrases might structurally meaningful compared representations learned sequential context window based approaches. future work address observations investigate effective training bidirectional recursive network explore impact different word vector representations architecture. work supported part darpa deft grant fa--- gift boeing. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied darpa u.s. government.", "year": 2013}