{"title": "Measuring the non-asymptotic convergence of sequential Monte Carlo  samplers using probabilistic programming", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A key limitation of sampling algorithms for approximate inference is that it is difficult to quantify their approximation error. Widely used sampling schemes, such as sequential importance sampling with resampling and Metropolis-Hastings, produce output samples drawn from a distribution that may be far from the target posterior distribution. This paper shows how to upper-bound the symmetric KL divergence between the output distribution of a broad class of sequential Monte Carlo (SMC) samplers and their target posterior distributions, subject to assumptions about the accuracy of a separate gold-standard sampler. The proposed method applies to samplers that combine multiple particles, multinomial resampling, and rejuvenation kernels. The experiments show the technique being used to estimate bounds on the divergence of SMC samplers for posterior inference in a Bayesian linear regression model and a Dirichlet process mixture model.", "text": "limitation sampling algorithms approximate inference difﬁcult quantify approximation error. widely used sampling schemes sequential importance sampling resampling metropolishastings produce output samples drawn distribution target posterior distribution. paper shows upper-bound symmetric divergence output distribution broad class sequential monte carlo samplers target posterior distributions subject assumptions accuracy separate gold-standard sampler. proposed method applies samplers combine multiple particles multinomial resampling rejuvenation kernels. experiments show technique used estimate bounds divergence samplers posterior inference bayesian linear regression model dirichlet process mixture model. paper builds growing body work begun estimating upper bounds divergences sampler’s output distribution posterior. variational inference divergence variational approximation variational lower bound log-evidence. recognized certain stochastic inference markov chains including annealed importance sampling single-particle treated variational approximations extended space includes auxiliary random choices sampler. similar insight introduced independently also showed estimate upper bounds log-evidence datasets simulated model using generalizations harmonic mean estimator introduced bidirectional monte carlo technique ‘sandwiching’ log-evidence upper bounds variational lower bounds. related approach sandwiching partition function previously used statistical physics literature finally recognized bounds serves upper bound divergence sampler allowing bdmc used measuring sampler accuracy simulated datasets. independent papers built develop technique different ways. previous paper took probabilistic programming perspective showed estimate divergence bound described general samplers using ‘meta-inference’ sampler generates sampler execution histories. also provided meta-inference samplers sampling importance resampling particle ﬁltering without mcmc rejuvenation kernels. also introduced upper bound symmetric divergence sampler output posterior analyzed optional approximate ‘reference’ samples surrogates exact posterior samples related tightness bounds accuracy meta-inference sampler. closely related independent work introduced bounding divergences reverse annealing uses upper bound symmetric divergence given showed evaluate single-particle approximate inference quality using bound. bread also includes heuristic scheme applicable hierarchical bayesian statistical models generating simulated datasets whose divergence proﬁles used proxies divergence proﬁles real-world datasets. also integrated technique existing probabilistic programming platforms. main contribution current work meta-inference construction generic samplers related conditional generalizes existing meta-inference constructions single-particle particle ﬁltering. handling broad class samplers construction increases relevance real world problems. construction allows analysis samplers rely mcmc rejuvenation kernels good inference quality permitting multiple particles tighten divergence bounds. background subjective divergence ﬁrst review subjective divergence procedure denote approximate inference sampling program samples output suppose also comes endowed side-procedure evaluates probability sampler produces given output denote posterior distribution πz˜π denote unnormalized posterior distribution. suppose access samples following unbiased monte carlo estimate symmetric divergence unfortunately often possible efﬁciently evaluate sampling programs sample auxiliary random choices execution including mcmc sampling algorithms approximate bayesian inference. denote joint distribution auxiliary random choices output intractable marginalize auxiliary random choices exponentially large number terms therefore instead compute following unbiased estimate upper bound symmetric divergence using ‘meta-inference’ sampler program samples execution histories sampler given output upper bound estimated symmetric divergence extended space includes auxiliary variables sampler. shown tightness bound governed well approximates average samples gold-standard approximate inference ‘reference sampler’ used place posterior samples validity bound subject accuracy reference sampler probabilistic programming interface subjective divergence clarify procedures associated sampler needed subjective divergence estimation. particular introduce following probabilistic programming interface consists stochastic procedures denoted .simulate .regenerate distributions simulate procedure runs sampler joint distribution execution histories output returns regenerate procedure takes potential sampler output input runs ‘regeneration’ sampler samples execution history original sampler. procedures also return log-weight. logweight returned simulate interpreted harmonic mean estimate log-weight returned regenerate interpreted importance sampling estimate sampler inference sampler call regeneration sampler ‘meta-inference’ sampler. seen relationship original sampler regeneration sampler analogous relationship conditional note auxiliary random variables exposed interface. also note sampler tractable marginal output probability trivially implements interface log/q) reduces output probability auxiliary variables algorithm shows procedure computes equation using interface. algorithm subjective divergence estimation using simulate regenerate requiresampler package implementing simulate regenerate; posterior sampler reference sampler unnormalized posterior probability function procedure estimate-kl-bound algorithm shows implement simulate regenerate generic sampler template introduced independent resampling. sampler template permits mcmc kernels provided corresponding ‘backward kernels’ deﬁned weights computed. note simulate sample backward kernels. building analysis used auxiliary variables sampler random choices made execution resampling choices output sampler values intermediate particles denoted stochastic regeneration template given output samples execution history sampler ﬁrst choosing ancestral particle indices output sampling backward kernels reverse output ﬁnally running order deﬁne ancestral particle values ﬁxed. related conditional forward ancestral indices values update differs output particle full particle trajectory required input. log-weight sampler regeneration pair simplify algorithm simulate regenerate samplers independent resampling requirenumber steps hypothesis spaces unnormalized target distributions deﬁned sampler initialization kernel deﬁned samplers kernels indexed deﬁned sampler kernel indexed deﬁned samplers kernels indexed deﬁned sampler kernel evaluator procedures weight functions procedure simulate speciﬁed implement simulate regenerate generic variant estimate subjective divergences smc. illustrate algorithm algorithm estimate subjective bounds symmetric divergences samplers black variational approximations posterior figure note optimized performance variational inference implementations separately relative runtimes approaches meant informative. figure show estimated elbo lower bounds estimated upper bounds divergence posterior respectively samplers black variational inference programs bayesian linear regression inference problem. samplers single-site independent metropolis-hastings rejuvenation kernels single-site random-walk rejuvenation kernels. bbvi bbvi optimize different variational families. show estimated elbo lower bounds subjective upper bounds divergence single-site gibbs rejuvenation kernels cluster assignments dirichlet process mixture model problem collapsed cluster parameters. samplers problems sequential observation deﬁne sequence target distributions parameterized number particles number applications mcmc rejuvenation kernels target distribution updates particles initialized prior. exact posterior reference samples used bound estimation samples gold-standard approximate mcmc reference sampler used lieu posterior samples bound estimation random-walk kernels appear effective independent kernels. increasing number particles tightens divergence bound effect rejuvenation kernels already saturated research supported darpa iarpa ofﬁce naval research army research ofﬁce gifts analog devices google. research conducted government support awarded force ofﬁce scientiﬁc research national defense science engineering graduate fellowship salimans diederik kingma welling. markov chain monte carlo variational inference bridging gap. proceedings international conference machine learning icml lille france july pages john hunter william reinhardt thomas davis. ﬁnite-time variational method determining optimal paths obtaining bounds free energy changes computer simulations. journal chemical physics recall auxiliary random choices sampler algorithm resampling values intermediate choices joint probability auxiliary random choices output particles execution smc’s simulate ﬁrst factor rand-ancestry. first note true requirements using requirements gives ﬁrst consider since ensure deﬁned output next assume deﬁned output t}}. also weight deﬁned appendix sequential observation detailed balance kernels experiments programs deﬁned follows. hypothesis space corresponding ‘global’ latent variables. additional hypothesis space extensions corresponding ‘local’ latent variables observations deﬁne indexing notation denote model’s joint probability assume given observation target distribution algorithm conditional distribution deﬁne intermediate target distributions unnormalized target probability functions deﬁne initialization kernel kernel samples model’s prior distribution global latents local latents ﬁrst observation. suppose exist ‘detailed balance kernel collection distributions elements indexed kernels’ elements detailed balance kernel must satisfy detailed balance property respect intermediate target distribution yt−) deﬁne collection distributions indexed elements xt−. possible sample sampling detailed balance kernel sampling model prior distribution local yt−). intuitively kernel ﬁrst performs inference targeting latents extends hypothesis space include values local latent variables observation sampling prior. deﬁne intuitively kernel performs inference targeting ﬁnal target distribution deﬁne ‘backward kernels’ et−) kernel collection distributions indexed elements sample simply sample detailed balance kernel dt−. finally deﬁne first follows show detailed balance requirement fact argument applies note require detailed balance kernels ergodic. example given kernel update components given deﬁnitions weight functions become finally algorithm shows simulate regenerate specialized sequential observation detailed balance kernels used experiments. algorithm parenthesized superscripts indicate step algorithm whereas subscripts indicate observation indices value local latents observation particle step smc). appendix using cycles detailed balance kernels recall require detailed balance kernels ergodic. particular update subset random variables show algorithm used without modiﬁcation kernels utilize instead cycles detailed balance kernels targeting distribution provided corresponding kernels sample cycle reverse order. note cycle detailed balance kernels satisfy detailed balance. suppose ˜pr− suppose dt−; dt−; detailed balance kernel targeting ˜pr− weights consider modifying modifying rand-ancestry categorical) replace uniform joint probability simulate divided probability excluded random choices deterministically joint probability regenerate divided since weight probability excluded random choices s−r+ weight expression therefore unchanged simpliﬁed concise implementation modiﬁed procedures steps collapsed step cycle detailed balance kernels taking role single simulate regenerate reverse cycle dr−) taking role corresponding regenerate. also replaced cycles detailed balance kernels consider introducing random variables simulate regenerate follows joint probability expression replace replace replaced cycle detailed balance kernels targeting reversed cycle respectively without modifying expression returned log-weight value still computed using", "year": 2016}