{"title": "Semantically Consistent Regularization for Zero-Shot Recognition", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.", "text": "role semantics zero-shot learning considered. effectiveness previous approaches analyzed according form supervision provided. learn semantics independently others supervise semantic subspace explained training classes. thus former able constrain whole space lacks ability model semantic correlations. latter addresses issue leaves part semantic space unsupervised. complementarity exploited convolutional neural network framework proposes semantics constraints recognition.although trained classiﬁcation transfer ability encouraged learning hidden semantic layer together semantic code classiﬁcation. forms semantic constraints introduced. ﬁrst loss-based regularizer introduces generalization constraint semantic predictor. second codeword regularizer favors semantic-to-class mappings consistent prior semantic knowledge allowing learned data. signiﬁcant improvements state-of-the-art achieved several datasets. signiﬁcant advances object recognition recently achieved introduction deep convolutional neural networks main limitation approach effort required collect annotate millions images necessary train models complexity training scratch. fact recent computer vision papers adapt small popular models alexnet googlenet learned imagenet dataset hence interest techniques transfer learning model learned dataset used recognize object classes represented ideally transfer learning methods would replicate human ability recognize objects example images even description terms concepts semantic vocabulary. motivated introduction semantic representations object recognition rely predeﬁned vocabulary visual concepts deﬁne semantic space classiﬁers image space. scores classiﬁers used semantic features object classiﬁcation. furthermore simple rules thumb designed priori describe object classes terms semantics image mapping exploited recognize previously unseen objects. known zeroshot learning fundamental difﬁculty training canguided goal classiﬁer. recognizer learned training classes must provide accurate predictions image classiﬁcation non-overlapping unseen zero-shot classes. historically early efforts devoted identiﬁcation good semantics zsl. motivated collection datasets containing images annotated respect semantics visual attributes subsequent works addressed design semantic space using strategies previously proposed semantic representation literature. ﬁrst recognition using independent semantics consists learning independent classiﬁer semantic simplicity became widely popular attribute recognition literature notwithstanding efforts discriminant attribute discovery modeling uncertainty learning semantics independently proved weak guarantee reliable predictions. motivated shift second strategy ties design goal recognition learning single multi-class classiﬁer optimally discriminates between training classes difﬁculty extending approach semantics interest classes themselves. proposed effective solution problem noting ﬁxed linear transformation embedding semantics interest class labels speciﬁed hand even classes. accomplished using label embedding function class vector space attributes. recently various works proposed variations approach refer class methods recognition using semantic embeddings learning semantics simultaneously rule able leverage dependencies concepts thus addressing main limitation ris. work investigate advantages disadvantages approaches implementations based deep learning cnns. show that context methods reduce constraints architecture learns bank independent cnns rule uses single ﬁxed weights ﬁnal layer. follows performance approaches constrained form supervision provided space image attributes. provides supervision along dimension independently rule along subspace spanned label embedding directions number attributes usually larger classes exposes strengths weaknesses approaches. hand supervises attributes cannot model dependencies. other rule models dependencies leaves large number dimensions unconstrained. exploit complementarity propose framework denoted semantically consistent regularization leverages advantages rule. achieved recognizing methods exploit semantics constraints recognition. enforces ﬁrst-order constraints rule focuses second-order however suboptimal zsl. ignores recognition training classes sacriﬁcing modeling semantic dependencies rule ignores large subspace ﬁxes network weights. score addresses problems exploiting view optimal classiﬁer respect multidimensional classiﬁcation code implemented layer. interprets code mapping semantics classes enforces ﬁrst secondorder regularization constraints combination like loss-based regularizer constraints semantic predictions codeword regularizer favors classiﬁcation codes consistent rule embeddings. measurable visual property discrete continuous numerical categorical. given semantic vocabulary semantic feature space deﬁned cartesian product vector spaces associated semantic classiﬁer denoted semantic operates example animal recognition semantic vocabulary containing visual attributes e.g. {furry legs brown etc.} usually deﬁned along corresponding vector spaces. case since semantics binary large positive values indicate attribute presence large negative values absence. early approaches semantic recognition used image classes recognized semantic vocabulary. rationale create feature space high-level abstraction operations image search classiﬁcation performed robustly. recently substantial interest semantic feature spaces transfer learning auxiliary semantic vocabulary deﬁned mid-level visual concepts. three main categories concepts explored including visual attributes hierarchies word vector representations. attributes introduced quickly adopted many works semantic concepts extracted hierarchies/taxonomies later explored vector representations words/entities zero-shot learning current solutions fall main categories rule. early approaches adopted strategy. popular among direct attribute prediction method learns attributes independently using svms infers predictions maximum posteriori rule assumes attribute independence. several enhancements proposed account attribute correlations posteriori e.g. using crfs model attribute/class correlations directed bayesian networks merge attribute predictions class scores random forests learned mitigate effect unreliable attributes recently proposed multiplicative framework enables class-speciﬁc attribute classiﬁers learns independent attributes previously discovered wordvec representations. rule alternative strategy exploits one-toone relationship semantics object classes. central idea deﬁne embedding maps class q-dimensional vector attribute states identiﬁes bilinear compatibility function deep-ris deep-rule advantages disadvantages observed comparing risks since attributes quantities interest useful understand methods provide supervision space attributes. deep-ris provides supervision individual attributes since vector canonical basis supervision along canonical directions hand depend projections along vector encodings training classes. hence rule provides supervision column space practice often regime figure number attributes larger number training classes follows fairly dimensional left null space fairly high dimensional hence constraints attributes rule leaves attribute dimensions unconstrained. case classes semantic codes misaligned cannot expected accurately predicted. limit rule completely inadequate discriminate classes perpendicular figure suggests superiority rule. however supervises attributes independently ability learn attribute dependencies e.g. attributes wings lives water strong negative correlation. dependencies thought constraints reduce effective dimensionality attribute space. imply attribute vectors natural images span effective attribute subspace dimension learning deeprule provides supervision explicitly space. suggests deep-rule outperform deep-ris. class ﬁrst implementation rule learned variant structured svm. several variants proposed addition different regularization terms least-squares losses faster training improved semantic representations objects learned multiple text sources discuss implementation rule. simplicity assume attribute semantics. sections extend treatment concepts. quick consultation table summarizes important notation used rest paper. deep-ris independence assumption underlies implementation reduces learning independent attribute predictors. inspired success multitask learning advantageous share parameters across attributes rely common feature extractor parameters implemented popular cnns literature. thus attribute predictor deep-ris takes form implementation rule follows immediately bilinear form note ﬁxed mapping space attributes space class labels. example binary attributes class labels dimensional vector encodes presence/absence attributes class denote semantic code class implement sufﬁces popular models compute fully-connected layer units parameters vector attribute scores deﬁne class outputs lagrange multiplier regularizer favors score functions complexity. common usages include shrinkage sparse representations weight decay since approaches simply favor solutions complexity form task-insensitive regularization. type regularization indeed used control variance semantic scores backward projections object embeddings feature space well suppress noisy semantics work rather generic penalty complexity propose task-sensitive form regularization favors score functions added functionality attribute prediction. regularization implemented complimentary mechanisms introduced next sections. codeword regularization denotes inner product predictor class codewords. denote classiﬁcation code class example binary classiﬁcation algorithms boosting simply choose codewords positive/negative class.similarly c-ary classiﬁcation neural networks multi-class svms rely one-hot encodings lead typical decision rule maxj∈{...c} however reason limited classical sets. comparing score functions deeprule interpreted learning optimal predictor classiﬁcation code given i.e. hence deep-rule seen form strict regularization ﬁnal fully-connected layer semantic codes. general ﬁxing network weights undesirable better results usually obtained learning data. avoid using semantic codes loose regularization constraints framework similarly deep-rule learn predictor using cross-entropy empirical risk score functions form overall relative performance approaches depends overlap subspaces covered training classes denoted respectively. contains directions deﬁne deep-rule outperform deep-ris. classes deﬁned directions contained deep-ris likely outperform deep-rule. previous section relative performance deep-ris deep-rule depends alignment subspaces deﬁne training classes ideal scenario class however unlikely happen datasets tractable size subsets scenario deep-ris deep-rule compliment other. deep-ris enforces ﬁrst-order constraints statistics single attributes deep-rule enforces second-order constraints constraining statistics linear attribute combinations. strategies combined deep-rule explain attribute dependencies appear training classes leaving deep-ris task constraining attribute distribution remainder space. thus natural combine strategies. accomplish mapping regularization constraints. recognition regularization vector conﬁdence scores assignment class class prediction. score function usually learned minimizing empirical risk complexity constraint improve generalization i.e. second mechanism denoted loss-based regularization aims constraint attributes beyond provides explicit regularization attribute predictions. implemented introducing auxiliary risk optimization i.e. replacing attribute prediction risks drives score function produce accurate attribute predictions addition classiﬁcation. depending value multipliers score learn standard deep-ris deep-rule. regularization constraints disregarded classiﬁer standard recognizer training classes. increasing improves transfer ability. hand regardless increasing makes score like deep-ris. limit ﬁrst summation plays role optimization trivially minimized reduced deep-ris optimization problem hand maintaining increasing makes score similar deep-rule. large values learning algorithm emphasizes similarity classiﬁcation semantic codes trading classiﬁcation performance semantic alignment. finally nonzero score learns classiﬁer best satisﬁes corresponding trade-off three goals recognition attribute predictions alignment semantic code. support different degrees certainty class/attribute associations continuous attributes also easily implemented making beyond binary semantics semantic state codewords. then semantic code class built concatenating similarly binary case predictor learned codeword attempt approximate images class state semantic thus recovered codeword maxi=...sk state semantic corresponding subspace many semantic state codewords deﬁned. provide examples. taxonomies work consider taxonomic encodings emphasize node speciﬁc decisions interpreting node semantic concept. illustrated figure semantic state codeword deﬁned node state codewords identify possible children nodes plus reject option. example codeword node contains codewords whale plus reject codeword other. taxonomic encoding semantic code identiﬁes relevance node figure deep-score. feature extraction based common architectures. classiﬁcation performed ﬁrst computing semantic scores codewords combining class scores using known class/semantics relations class internal node ancestor contributes codeword corresponding branch selection node. node ancestor contributes reject codeword. example figure class bear receives code concat remains deﬁne codeword sets could used reﬂect semantic information. tree figure could encode attributes distinguish aquatic terrestrial aerial animals legs wings. work since semantic information available beyond taxonomy itself rely maximally separated codeword sets procedure q-ways decision mapped codewords deﬁned vertices q-sided regular polygon dimensions centered origin. wordvec wordvec procedure generate word embeddings. word mapped high-dimensional vector neural network trained large text corpora reconstruct linguistic contexts words. semantic annotation mapping used semantic code i.e. class encoded vector work skip-gram architecture proposed mikolov embeddings determined parameters size encoding layer window size deﬁnes context word. rather relying single model learn wordvec embeddings using different combinations parameters. creates codeword sets semantic code represents class string resulting vectors using could computed separately structure allows shared computation. accomplished adding layers semantic predictor denote semantic encoding layers. shown figure used compute predictor similarly deep-ris deep-rule implemented linear transformation feature vector computed popular models. ﬁrst layer consists parallel fully-connected layers compute semantic scores semantics. weights branch contain classiﬁcation codewords learned codeword regularizer second layer selects class single output branch corresponding state semantic class outputs added obtain class recognition score easily implemented fully connected layer predetermined sparse weights remain ﬁxed throughout training. learning consider training three-tuples image vector semantic states class label shown figure state vectors used supervisory signals ﬁrst layer labels supervisory signals second. supervisory signals semantic codes used compute lagrangian risk parameters optimized back-propagation using caffe toolbox deep-score models trained tuning pretrained cnns using stochastic gradient descent momentum weight decay learning rate chosen empirically experiment. section discuss several experiments carried evaluate performance deepscore. source code available https//github.com/ pedro-morgado/score-zeroshot. experimental setup datasets three datasets considered animals attributes caltech-ucsd birds subset imaging flowcytobot dataset. table summarizes statistics. partition source target classes speciﬁed respectively. ifcb ﬁrst used classes partitioned randomly. separate validation classes also drawn randomly tune score parameters. images resized image representation pixels exception ifcb aspect ratios differ widely resizing introduces considerable distortion. instead image ﬁrst resized along longest axis shortest axis padded average pixel value preserve aspect ratio. typical data augmentation techniques used training center crop used testing. three architectures used implement alexnet googlenet semantics three sources semantics evaluated. visual attributes continuous attributes shown superior binary counterparts used cub. ifcb attributes deﬁned previously list visual attributes assembled annotated expert binary labels using several sources oceanographic community taxonomies created pruning wordnet tree training classes eliminating dummy nodes containing single child. rare situations wordnet ﬁne-grained enough distinguish classes taxonomy expanded simply assigning object leaf. gains regularization started evaluating codeword loss-based regularization. importance regularizers assessed separately datasets using visual attributes googlenet. cases measured gains deep-rule classiﬁcation codewords gains loss-based regularization evaluated increasing keeping setting classiﬁer converges deep-ris limit conversely gains codeword regularization measured increasing keeping case classiﬁer converges unrestricted object recognizer deep-rule figure presents absolute improvement mean class accuracy deep-rule function lagrange multipliers. regularizers produced gains deep-rule absolute gains high zs-mca points. demonstrates importance learning classiﬁcation codewords rather ﬁxing them. note that codeword regularization best results obtained intermediate values encourage consistency seusing three architectures alexnet googlenet vgg. although results vary drastically clear deep-score outperforms previous approaches datasets achieving impressive gains stateof-the-art every architecture alexnet googlenet respectively. multiple semantics ﬁnally studied performance deep-score attributes taxonomies wordvec embeddings. figure compares deep-score variants popular rule approaches literature es-zsl approaches implemented semantic codes section best results obtained deep-score also shown. figure supports main conclusions. first shown attributes enable effective transfer. surprising since attributes tend discriminant properties various object classes. taxonomies wordvec informative grouping contextual information. second approaches rely regularization nature regularization matters. task-sensitive regularization deep-score always outperformed taskinsensitive regularization es-zsl combination loss-based codeword regularization always outperformed ﬁxed semantic code loss-based regularization work analyzed type supervision provided previous approaches. complementarity found class semantic supervision lead introduction procedure denoted score learned together semantic codeword forms semantic constraints loss-based codeword regularization. state-of-the-art zero-shot performance achieved various datasets. mantic classiﬁcation codes leave enough ﬂexibility learn classiﬁcation code superior semantic counterpart. cases score much superior conﬁrming importance modeling attribute dependencies ﬁrst term finally score performance also superior unrestricted cnn. demonstrates beneﬁts regularization. interestingly case always underperformed unrestricted rule achieved results ifcb. section hypothesized loss-based regularization becomes important alignment subspaces spanned training classes decreases. test hypothesis measured alignment computing average orthogonal distance semantic codeword class subspace spanned codewords training classes. average distances ifcb indicating transfer easiest hardest ifc. consistent plots figure show largest gains loss-based regularization ifcb followed cub. comparisons state-of-the-art methods comparison literature trivial since methods differ implementation train/zs class partitioning semantic space representation. mitigate differences focused attribute semantics available results. methods alternative semantics unlabeled images classes training disregarded comparison. deep-score hyper-parameters tuned subset training classes.", "year": 2017}