{"title": "Unsupervised Submodular Rank Aggregation on Score-based Permutations", "tag": ["cs.LG", "cs.AI"], "abstract": "Unsupervised rank aggregation on score-based permutations, which is widely used in many applications, has not been deeply explored yet. This work studies the use of submodular optimization for rank aggregation on score-based permutations in an unsupervised way. Specifically, we propose an unsupervised approach based on the Lovasz Bregman divergence for setting up linear structured convex and nested structured concave objective functions. In addition, stochastic optimization methods are applied in the training process and efficient algorithms for inference can be guaranteed. The experimental results from Information Retrieval, Combining Distributed Neural Networks, Influencers in Social Networks, and Distributed Automatic Speech Recognition tasks demonstrate the effectiveness of the proposed methods.", "text": "unsupervised rank aggregation score-based permutations widely used many applications deeply explored yet. work studies submodular optimization rank aggregation score-based permutations unsupervised way. speciﬁcally propose unsupervised approach based lovasz bregman divergence setting linear structured convex nested structured concave objective functions. addition stochastic optimization methods applied training process efﬁcient algorithms inference guaranteed. experimental results information retrieval combining distributed neural networks inﬂuencers social networks distributed automatic speech recognition tasks demonstrate effectiveness proposed methods. unsupervised rank aggregation task combining multiple permutations candidates ranking list better permutation candidates. speciﬁcally different permutations total elements ranking list denote either relative orders numeric values candidates. previous work unsupervised rank aggregation focuses social choice theory relative orders candidates assigned elements ranking list consensus among ranking lists result pursuit. numerous aggregated methods unsupervised methods based local kemeny optimality distance-based mallows models consensus order minimize distances permutations consensus. however framework rank aggregation score-based permutations elements ranking lists represent numerical values combined list aggregated scores used obtain relative orders candidates sorting values thus methods used rank aggregation order-based permutations simply generalized unsupervised rank aggregation score-based permutations. best knowledge borda count-based unsupervised learning algorithm rank aggregation particularly designed unsupervised rank aggregation score-based permutations although naive methods like averaging majority vote also applied. lovasz bregman divergence rank aggregation score-based permutations initially introduced divergence derived generalized bregman divergence parameterized lovasz extension submodular function submodular function deﬁned diminishing return property speciﬁcally function said submodular item subsets satisﬁes inequality many discrete optimization problems involving submodular functions solved polynominal time e.g. exact minimization approximate maximization addition lovasz extension submodular function ensures convex function although introduction divergence rank aggregation score-based permutations brieﬂy discussed existing formulation divergence rank aggregation related algorithms limited supervised manner. thus formulations divergence unsupervised rank aggregation score-based permutations well related algorithms still lacking. since divergence capable measuring divergence score-based permutation order-based permutation work applies divergence unsupervised rank aggregation score-based permutations. addition efﬁcient algorithms associated divergence framework proposed accordingly. signiﬁcance unsupervised rank aggregation score-based permutations ground truths relevance scores associated candidates required supervised cases quite expensive obtain practice. example ﬁeld information retrieval difﬁcult annotate relevance scores retrieval documents prevents learning rank methods. similarly ﬁeld speech recognition machine translation ground truth normally unknown thus multiple n-best hypotheses generated distributed deep learning models combined unsupervised way. thus necessary design theories algorithms unsupervised rank aggregation score-based permutations. divergence utility function ﬁrstly proposed used measuring divergence score-based permutation order-based permutation. related deﬁnitions divergence brieﬂy summarized follows deﬁnition submodular function order-based permutation chain sets deﬁnition given submodular function associated lovasz extension deﬁne divergence measuring divergence permutations score-based permutation order-based permutation divergence shown next study apply divergence obtain normalized discounted cumulative gain loss function used unsupervised rank aggregation score-based permutations. deﬁnition given order-based permutation candidates ground relevance scores associated ndcg obviously found normalized applied utility function ndcg loss measurement normalization term constant. addition lovasz bregman divergence guarantees upper bound ndcg loss function shown proposition proposition given score-based permutation concave function divergence deﬁned provides constant upper bound ndcg loss function. speciﬁcally maxij marginal gain. furthermore setting obtain maxj applying submodular diminishing return property minj thus noting mini σxδg proof comsetting pleted. ﬁrst unsupervised learning framework based divergence based linear structure. suppose queries total score-based permutations associated query assumed random variable subsumes i||πq) computes divergence possible permutations divergence approximated metropolis-hasting sampling method. speciﬁcally whether sample selected depends following steps given weight score-based permutations time suppose state lies probability generating normalization term independent permutation. update goes queries several iterations repeated reaching convergence. proposition linear structured framework given number candidates number permutations computational complexity training stage inference process formulated follows given test data query represents score-based permutation deﬁned above associated query estimate optimal order-based permutation deﬁned refers weight vector trained learning stage. generally problem np-hard combinatorial problem divergence provides close-form solution complete inference algorithm shown algorithm note order-based permutation ﬁnally obtained sorting numeric values permutation associated returned decreasing order. linear structured framework involves several potential problems ﬁrst problem score-based permutations {xi}k interact other since permutation might partially redundant another; second problem lies fact permutation tends become dominant rest. overcome problems additional hidden layer utilized construct nested structured framework shown figure objective function nested structured framework formulated separately represent numbers units input hidden layers rk∗k denote weights bottom upper layers respectively refer regularization terms. setting increasing concave functions objective function becomes concave function thus needs maximized respect need update weights bottom layer temporary variables ﬁrstly computed respectively updated note metropolis-hasting sampling method applied denotes number sampling permutations refers learning rate. proposition nested structured framework given numbers input hidden layers respectively number candidates computational complexity entire training process tested algorithms four different applications report results section. addition another unsupervised rank aggregation method based ulara tested purpose comparison divergence-based methods. introduced section applications include information retrieval combining distributed neural networks inﬂuencers social networks distributed automatic speech recognition experiments sigmoid function chosen discounted factor learning rate regularization terms ﬁxed given query expected candidates associated potential relevance scores ndcg score deﬁned lies interval tool evaluating quality ranking retrieved documents. larger ndcg score represents ranking result conﬁdence. experiments conducted letor dataset package benchmark data sets research learning rank. dataset contains standard features relevance judgements data partitioning several baselines. letor dataset involves query sets named short. queries queries query document candidates associated given relevant scores. applied unsupervised rank aggregation methods dataset compared ndcg results learning rank approaches. learning rank approaches include ranksvm listnet adarank rankboost setup divergence-based methods permutations candidates associated query total number units hidden layer nested structure ndcg results datasets shown figure linearlbd nested-lbd represent linear nested structured divergence-based methods unsupervised rank aggregation respectively. results show nested structured divergencebased method comparable learning rank methods even obtains better results potential candidates considered. data requires deep learning architectures distributed example automatic speech recognition machine translation. study combine hypothesis outputs distributed neural networks aggregated result. result expected close ground truth possible corresponds higher accuracy prediction. outputs distributed neural networks seen score-based permutations task combining distributed neural networks taken unsupervised rank aggregation score-based permutations. combination distributed neural networks conducted digit recognition task mnist dataset. distributed neural networks constructed shown figure distributed system consisted neural networks total ﬁrst deep neural networks middle convolutional neural networks last multi-layer perceptrons stacked auto-encoder initialization technique permutations total distributed deep model architecture permutation scores candidates addition neural networks different architectures changes number units hidden layers size neuron units. shown figure unsupervised rank aggregation layer added distributed system combine outputs neural networks. table shows detailed architecture conﬁgurations corresponding different neural networks results based digit error rates conﬁgurations aggregated methods number permutations conﬁgured number units hidden layer nested structured framework output corresponding ﬁnal aggregated result. note experiments conducted deep learning toolkit ders table state-of-the-art results show unsupervised rank aggregation score-based permutations lower ders. particularly method based nested structured divergence obtains maximum gain ulara performs even better method based linear structured divergence simple averaging method. people pair-wisely connected social networks pair-wise preference individuals provided. study inﬂuencers social networks aims predict human judgement inﬂuential high accuracy. studied unsupervised submodular rank aggregation methods approaching baseline results obtained supervised logistic regression. receiver operating characteristic curve used evaluate methods. score interval higher value means higher prediction accuracy. data task provided kaggle competition task comprise standard pair-wise preference learning task. pair-wise preference data points combined feature pre-computed pre-computed non-negative numeric features based twitter activity include volume interactions number followers. binary label represents human judgement individuals inﬂuential. goal task predict human judgement inﬂuential high accuracy. speciﬁcally unsupervised rank aggregation task purpose assign likelihood candidate aggregating features candidate. labeled data points randomly split training testing rest. baseline system based supervised logistic regression used compare unsupervised methods. conﬁguration divergence-based methods number permutations number units hidden layer nested structured framework numbers candidates training testing respectively. results based scores shown table note results average testing results based different partitioned datasets. although results based unsupervised submodular rank aggregation methods baseline result approach based nested structured divergence close baseline. last application based unsupervised submodular rank aggregation refers distributed automatic speech recognition system. illustration distributed system shown figure compared combination deep learning models different structure conﬁguration deep learning models share initial setup including types number layers. training dataset partitioned non-overlapping subsets means robust submodular data partitioning methods since subsets employed training particular dnn-based acoustic model different dnn-based acoustic models ﬁnally collected. evaluation stage test data acoustic models outputs distributed system expected aggregated combined result higher accuracy. traditionally supervised adaboost method applied expensive difﬁcult obtain ground truth practice. although approximated clustered triphone labels obtained forced-alignment labels perfectly correct supervise adaboost training. thus unsupervised submodular rank aggregation score-based permutations attempted replace supervised adaboost method. following steps submodular partitioning functions composed according prior phonetic knowledge triphone corresponds different biphones based phonetic knowledge including ‘place articulation’ ‘production manner’ ’voicedness’ ‘miscellaneous’. training dnn-based acoustic model entire dataset split disjoint data subsets formulating problem robust submodular data partitioning problem shown partition ﬁnite denotes sets corresponding algorithm applied obtain approximated solutions problem. experiments conducted timit database. training data consist utterances total. development test data composed utterances respectively. data preprocessing included extracting -dimensional frequency cepstral coefﬁcient features correspond speech signals. addition mean variance speaker normalization also applied. acoustic models initialized clustered triphones modeled -state left-to-right hidden markov models state emission modeled gaussian mixture model targets consisted clustered triphone states. -gram language model used decoding. subsets data partitioned submodular functions used training dnns parallel. units input layer correspond long-context feature vector generated concatenating consecutive frames primary mfcc feature followed discrete cosine transformation thus dimension initial long-context feature reduced addition hidden layers setup dnn. parameters hidden layers initialized restricted boltzmann machine pre-training ﬁne-tuning back-propagation algorithm besides feature-based maximum likelihood linear regression applied speaker adaptation training dnn-based acoustic models done ﬁnal posteriors clustered triphones associated training data separately obtained dnn-based acoustic models. posteriors taken permutation data training unsupervised rank aggregation models. testing stage posteriors collected dnn-based acoustic models combined permutation expected close ground truth possible. conﬁguration unsupervised submodular rank aggregation formulations number permutations dimension permutation conﬁgured matches number clustered triphones. besides number units hidden layer nested structured framework output corresponding ﬁnal aggregated permutation. table shows decoding results dnn-based acoustic models table presents combined ones based different aggregation methods. results suggest unsupervised submodular rank aggregation method based nested structured formulation achieves better result baseline system based adaboost method whereas others worse baseline. marginal gain nested structured formulation arises potential bias caused forced-alignment. study focuses several algorithms unsupervised submodular rank aggregation scorebased permutations based divergence linear nested structured frameworks. information retrieval combining distributed neural networks inﬂuencers social networks distributed automatic speech recognition tasks suggest nested structured divergence obtain signiﬁcantly gains. however gains obtained respect approaches lower varying degrees. addition methods scalable large-scale datasets computational complexity. future work study generalize nested structure deeper structure hidden layers. although convexity objective function deep structure maintained message-passing method deeper layers cannot obtain satisfying result applications. therefore better unsupervised learning approach training divergence objective function deeper structure formulation necessary. tejedor javier. robust submodular data partitioning distributed speech recognition. ieee international conference acoustics speech signal processing rosti antti-veikko ayan necip fazil xiang bing matsoukas spyridon schwartz richard dorr bonnie combining outputs multiple machine translation systems. hlt-naacl vincent pascal larochelle hugo lajoie isabelle bengio yoshua manzagol pierre-antoine. stacked denoising autoencoders learning useful representation deep network local denoising criterion. journal machine learning research theorem given monotone submodular function permutation inequality equation holds. minj maxij proof. decompose minj minj notice |ri| moreover hence since σ||. finally note minj combining these ||r|| result.", "year": 2017}