{"title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based  Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.", "text": "ﬁrst task. example machine learning system trained convex objective always reach conﬁguration training second task regardless initialized. means trained diﬀerent tasks completely forget perform ﬁrst task. whenever able correctly classify example original task chance similarities tasks. well-supported model biological learning human beings suggests neocortical neurons learn using algorithm prone catastrophic forgetting neocortical learning algorithm complemented virtual experience system replays memories stored hippocampus order continually reinforce tasks recently performed machine learning researchers lesson glean acceptable learning algorithms suﬀer forgetting need complementary algorithms reduce information loss. designing complementary algorithms depends understanding characteristics forgetting experienced contemporary primary learning algorithms. paper investigate extent catastrophic forgetting aﬀects variety learning algorithms neural network activation functions. neuroscientiﬁc evidence suggests relationship between task strongly inﬂuences outcome successive learning experiences consequently examine three diﬀerent types relationship tasks tasks functionally identical diﬀerent formats input tasks similar tasks dissimilar. catastrophic forgetting problem faced many machine learning models algorithms. trained task trained second task many machine learning models forget perform ﬁrst task. widely believed serious problem neural networks. here investigate extent catastrophic forgetting problem occurs modern neural networks comparing established recent gradient-based training algorithms activation functions. also examine eﬀect relationship ﬁrst task second task catastrophic forgetting. always best train using dropout algorithm– dropout algorithm consistently best adapting task remembering task best tradeoﬀ curve between extremes. different tasks relationships tasks result diﬀerent rankings activation function performance. suggests choice activation function always cross-validated. catastrophic forgetting problem aﬀects neural networks well learning systems including biological machine learning systems. learning system ﬁrst trained task trained second task forget perform ward neural nets. choice activation function less consistent eﬀect–diﬀerent activation functions preferable depending task relationship tasks well whether places greater emphasis adapting task retaining performance task. training dropout maxout activation function consistently appear somefrontier performance tradeoﬀs tasks considered. however maxout best function points along tradeoﬀ curve consistent performance trained withdropout still advisable cross-validate choice activation function particularly training without dropout. cases dropout increases optimal size resistance forgetting explained mostly larger nets greater capacity. however eﬀect consistent using dissimilar task pairs dropout usually decreases size net. suggests dropout subtle beneﬁcial eﬀects characterize future. catastrophic forgetting well-studied property neural networks recent years. property well-studied past received much attention since deep learning renaissance began srivastava repopularized idea studying aspect modern deep neural nets. however main focus work study catastrophic forgetting experiments limited. neural network trained case. networks used hyperparameters heuristically chosen stopping point. pair tasks employed clear whether ﬁndings apply pairs tasks kind degree similarity whether ﬁndings generalize many kinds pairs tasks. training algorithm standard gradient descent employed. move beyond limitations training multiple nets diﬀerent hyperparameters stopping using validation evaluating using three task pairs diﬀerent task similarity proﬁles including dropout algorithm experiments. dropout recently introduced training algorithm neural networks. dropout designed regularize neural networks order improve generalization performance. dropout training modiﬁcation standard stochastic gradient descent training. example presented network learning input states hidden unit states network multiplied binary mask. zeros mask cause units removed network. mask generated randomly time example presented. element mask sampled independently others using ﬁxed probability test time units dropped weights going unit multiplied compensate unit present often training. dropout seen extremely eﬃcient means training exponentially many neural networks share weights averaging together predictions. procedure resembles bagging helps reduce generalization error. fact learned features must work well context many diﬀerent models also helps regularize model. dropout eﬀective regularizer. prior introduction dropout main ways reducing generalization error neural network simply restrict capacity using small number hidden units. dropout enables training noticeably larger networks. example performed random hyperparameter search experiments case best two-layer rectiﬁer network classifying mnist dataset. training dropout best network according validation parameters best network trained withdropout. hypothesize increased size optimally functioning dropout nets means less prone catastrophic forgetting problem traditional neural nets regularized constraining capacity barely suﬃcient perform ﬁrst task. hidden layers neural networks transforms input vector output vector cases done ﬁrst computing presynaptic activation matrix learnable parameters vector learnable parameters. presynaptic activation transformed post-synaptic activation activation function provided input next layer. making fair comparisons diﬀerent deep learning methods diﬃcult. performance deep learning methods complicated non-linear function multiple hyperparameters. many applications state performance obtained human practitioner selecting hyperparameters this deviation implementation srivastava break ties using smallest index. used approach easier implement theano. think deviation previous implementation acceptable able reproduce previously reported classiﬁcation performance. deep learning method. human selection problematic comparing methods human practitioner skillful selecting hyperparameters methods familiar with. human practitioners also conﬂict interest predisposing selecting better hyperparameters methods prefer. automated selection hyperparameters allows fair comparison methods complicated dependence hyperparameters. however automated selection hyperparameters challenging. grid search suﬀers curse dimensionality requiring exponentially many experiments explore highdimensional hyperparameter spaces. work random hyperparameter search instead. method simple implement obtains roughly state results using experiments simple datasets mnist. sophisticated methods hyperparameter search bayesian optimization able obtain better results found random search able obtain state performance tasks consider think greater complication using methods justiﬁed. sophisticated methods hyperparameter feedback also introduce sort bias experiment methods study satisﬁes modeling assumptions hyperparameter selector. deﬁnition tasks suite experiments kinds algorithms stochastic gradient descent training dropout training. algorithms four diﬀerent activation functions logistic sigmoid rectiﬁer hard lwta maxout. eight conditions randomly generate random sets hyperparameters. code accompanying paper details. cases model hidden layers followed softmax classiﬁcation layer. hyperparameters search include magnitude maxnorm constraint layer method used initialize weights layer hyper-parameters associated search hyperparameters good values reasonably well-known. example dropout best probability dropping hidden unit known usually around best probability dropping visible unit known usually around used wellknown constants experiments. reduce maximum possible performance able obtain using search makes search function much better experiments since fewer experiments fail dramatically. best keep hyperparameter searches comparable diﬀerent methods. always used hyperparameter search dropout. diﬀerent activation functions slight diﬀerences hyperameter searches. diﬀerences related parameter initialization schemes. lwta maxout always initial biases since randomly initializing bias unit make unit within group often resulting dead ﬁlters. rectiﬁers sigmoids randomly select initial biases using diﬀerent distributions. sigmoid networks beneﬁt signiﬁcantly negative initial biases since encourages sparsity initializations fatal rectiﬁer networks since signiﬁcantly negative initial bias prevent unit’s parameters ever receiving non-zero gradient. rectiﬁer units also beneﬁt slightly positive initial biases help prevent rectiﬁer units getting stuck known reason believe helps sigmoid units. thus diﬀerent range initial biases rectiﬁers sigmoids. necessary make sure method able achieve roughly state performance experiments random search. likewise diﬀerences initialize weights activation function. activation functions initialize weights uniform distribution small values least cases. maxout lwta always method use. rectiﬁers sigmoids hyperparameter search also choose initialization method advocated martens sutskever method weights going unit remaining relatively large random values. maxout lwta method performs poorly diﬀerent ﬁlters within group cases ﬁrst train task validation error improved last epochs. restore parameters corresponding best validation error begin training task. train error union validation validation improved epochs. running randomly conﬁgured experiments conditions make possibilities frontier curve showing minimum amount test error task obtaining amount test error task. speciﬁcally plots made drawing curve traces lower left frontier cloud points pairs encountered models during course training task point generated pass training set. note test errors computed training subset training data train validation set. possible improve also training validation care relative performance diﬀerent methods necessarily obtaining state results. example consider learning understand italian already learning understand spanish. tasks share deeper underlying structure natural language understanding problem furthermore italian spanish similar grammar. however speciﬁc words language diﬀerent. person learning italian thus beneﬁts having pre-existing representation general structure language. challenge learn words structures without damaging ability understand spanish. ability understand spanish could diminish learning algorithm inadvertently modiﬁes abstract deﬁnition language general rather exploiting pre-existing deﬁnition learning algorithm removes associations individual spanish words pre-existing concepts test kind learning problem designed simple pair tasks tasks same diﬀerent ways formatting input. specifically used mnist classiﬁcation different permutation pixels task task. tasks thus beneﬁt concepts like penstroke detectors concept penstrokes combined form digits. however meaning individual pixel diﬀerent. must learn associate collections pixels penstrokes without signiﬁcantly disrupting higher level concepts erasing connections pixels penstrokes. classiﬁcation performance results presented fig. using dropout improved two-task validation performance models task pair. show eﬀect dropout optimal model size fig. nets able basically succeed task don’t believe mapping diﬀerent sets pixels pre-existing concepts. visualized ﬁrst layer weights best apparent semantics noticeably change training task concludes training task begins. suggests higher layers changed able accomodate relatively arbitrary projection input rather remaining lower layers adapted input format. next considered happens tasks exactly same semantically similar using input format. test case used sentiment analysis product categories amazon reviews tasks. classiﬁcation performance results presented fig. using dropout improved two-task validation performance models task pair. show eﬀect dropout optimal model size fig. next considered happens tasks semantically similar. test case used amazon reviews task mnist classiﬁcation another. order give tasks output size used classes mnist dataset. give validation size randomly subsampled remaining examples mnist validation amazon dataset preprocessed earlier input features mnist give tasks input size reduced dimensionality amazon data pca. forgetting explained part large model sizes trained dropout. input-reformatted task pair similar task pair dropout never decreased size optimal model four activation functions tried. however dropout seems additional properties help prevent forgetting explanation for. dissimilar tasks experiment dropout improved performance reduced size optimal model activation functions task pairs occasionally eﬀect optimal model size. recent previous work catastrophic forgetting argued choice activation function signiﬁcant eﬀect catastrophic forgetting properties particular hard lwta outperforms logistic sigmoid rectiﬁed linear units respect trained stochastic gradient descent. extensive experiments found choice activation function less consistent eﬀect choice training algorithm. performed experiments diﬀerent kinds task pairs found ranking activation functions problem dependent. example logistic sigmoid worst conditions best experiments shown training dropout always beneﬁcial least relatively small datasets used paper. dropout improved performance eight methods three task pairs. dropout works best terms performance task performance task points along tradeoﬀ curve balancing extremes three task pairs. dropout’s resistance conditions. suggests always cross-validate choice activation function long computationally feasible. also reject idea hard lwta particular resistant catastrophic forgetting general makes standard training algorithm resistant catastrophic forgetting. example training input reformatting task pair hard lwta’s possibilities frontier worse activation functions except sigmoid points along curve. similar task pair lwta worst eight methods considered terms best performance task best performance task terms attaining points close origin possibilities frontier plot. however hard lwta perform best circumstances suggests worth including hard lwta many activation functions hyperparameter search. lwta however never leftmost point three task pairs probably useful sequential task settings forgetting issue. computational resources limited experiment multiple activation functions recommend using maxout activation function trained dropout. method appears lower-left frontier performance tradeoﬀ plots three task pairs considered. would theano pylearn would also like thank nserc compute canada calcul qu´ebec providing computational resources. goodfellow supported google fellowship deep learning.", "year": 2013}