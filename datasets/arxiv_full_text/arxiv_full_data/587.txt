{"title": "Domain Specific Author Attribution Based on Feedforward Neural Network  Language Models", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at https://github.com/zge/authorship-attribution/.", "text": "authorship attribution refers task automatically determining author based given sample text. problem long history wide range application. building author proﬁles using language models successful methods automate task. language modeling methods based neural networks alleviate curse dimensionality usually outperform conventional n-gram methods. however much research applying authorship attribution. paper present novel setup neural network language model apply database text samples different authors. investigate nnlm performs task moderate author size relatively limited training test data topics text samples affect accuracy. nnlm achieves nearly reduction perplexity measurement ﬁtness trained language model test data. given random test sentences also increases author classiﬁcation accuracy average compared n-gram methods using srilm tools. open source implementation methodology freely available https//github.com/zge/authorship-attribution/. authorship attribution refers task identifying text author given text sample ﬁnding author’s unique textual features. possible author’s proﬁle style embodies many characteristics including personality cultural educational background language origin life experience knowledge basis etc. every person his/her style sometimes author’s identity easily recognized. however often identifying author challenging author’s style vary signiﬁcantly topics mood environment experience. seeking consistency consistent evolution variation always easy task. much research area. juola stamatatos example surveyed state proposed recommendations move forward. text data become available computational linguistic models using statistical methods mature opportunities challenges arise area many statistical models successfully applied area latent dirichlet allocation topic modeling dimension reduction naive bayes text classiﬁcation multiple discriminant analysis support vector machines feature selection classiﬁcation methods based language modeling also among popular methods authorship attribution neural networks deep learning successfully applied many applications speech recognition object detection natural language processing pattern recognition classiﬁcation tasks neural network based language models surpassed performance traditional n-gram purported generalize better smaller datasets paper propose similar nnlm setup authorship attribution. performance proposed method depends highly settings experiment particular experimental design author size data size work focused small datasets within speciﬁc text domain sizes training test datasets author limited. often leads contextbiased models accuracy author detection highly dependent degree topics training test sets match experiments conceive based closed dataset i.e. test author also appears training task simpliﬁed author classiﬁcation rather detection. paper organized follows. sec. introduces database used project. sec. explains methodology nnlm including cost function deﬁnition forward-backward propagation weight bias updates. sec. describes implementation nnlm provides classiﬁcation metrics compares results conventional baseline n-gram models. finally sec. presents conclusion suggests future work. database selection course transcripts coursera largest massive open online course platforms. ensure author detection less replying domain information courses selected speciﬁc text domain technical science engineering ﬁelds covering areas algorithm data mining information technologies machine learning mathematics natural language processing programming digital signal processing table lists details course database number sentences words number words sentence vocabulary sizes multiple stages. privacy reason exact course titles instructor names concealed. however purpose detecting authors necessary point courses taught different instructors except courses done intentionally allow investigate topic variation affects performance. transcripts course originally collected short phrases various lengths shown time bottom video lectures. ﬁrst concatenated segmented sentences using straight-forward boundary determination punctuations. sentence-wise datasets stemmed using porter stemming algorithm control vocabulary size words occurring entire course frequency less considered negligible inﬂuence outcome pruned mapping out-of-vocabulary mark unk. ﬁrst graph figure shows vocabulary size course dataset shrinks stemming pruning. words among datasets mapped however vocabulary sizes signiﬁcantly reduced average bottom graph provides proﬁle instructor terms word frequency i.e. database coverage frequent words stemming pruning example frequent words cover least words datasets. language model trained using feed-forward neural network illustrated figure given sequence words training text network trains weights predict word designated target word position sequence using information provided rest words formulated argmax similar classic n-gram language model primary task predict next word given previous words. however network trained predict target word position given neighboring words. network contains different types layers word layer embedding layer hidden layer output layer. weights adjacent i.e. word-to-embedding weights wword−emb embedding-to-hidden weights wemb−hid hidden-to-output weights whid−out need trained order transform input words predicted output word. following sub-sections brieﬂy introduce nnlm training procedure ﬁrst deﬁning cost function minimized describing forward backward weight bias propagation. implementation details regarding parameter settings tuning discussed sec. cross-entropy output node ﬁnal output layer network i.e. probability selecting word predicted word. parameter target label -of-v multiclass classiﬁcation problem target value rest forward propagation process compute outputs layer neural function inputs computed using outputs previous layer weights layer layer bias current layer weight bias initialization neural network training starts forward propagating word inputs outputs ﬁnal layer. word layer given word context size target word position input words represented binary index column vector length equal vocabulary size contains particular position differentiate words. word transformed distributed representation socalled embedding layer equation word−emb wword−emb word-to-embedding weights size used computation zemb different words nemb dimension embedding space. zemb column word−emb representing word process simply table look forward propagating input words ﬁnal output yout network next task backward propagate error derivatives output layer input know directions magnitudes update weights layers. iteration forward-backward propagation weights biases updated reduce cost. denote general form weight matrices wword−emb wemb−hid whid−out averaged version weight gradient carries information previous iterations initialized zeros weights updated with momentum determines percentage weight gradients carried previous iteration learning rate determine step size update weights towards direction descent. biases updated similarly replacing models share general parameters number epochs epoch learning rate decay starts learning rate decay factor however model parameters searched optimized within certain ranges using multi-resolutional optimization scheme dimension embedding space nemb nodes hidden layer nhid learning rate momentum mini-batch size optimization process time consuming worthwhile since course unique proﬁle terms vocabulary size word distribution database size etc. model adapted proﬁle perform better later classiﬁcation. statistical language models provide tool compute probability target word given context words normally target word word context words preceding words. denote word sequence using chain rule probability probability sequence formulated simpliﬁed neural network terms weights biases gets updated iteration mini-batch training. gradient normal∂wi ized cycle feeding data called epoch given appropriate training parameters learning rate momentum normally requires epochs well-trained network. next present procedure training nnlm. includes components described before ﬂexibility change training parameters different epochs includes early termination criterion. general parameters mini-batch size number epochs model parameters word context size target word position number nodes layer etc.; section covers implementation details authorship attribution system n-way classiﬁcation problem using nnlm. results compared baseline n-gram language models trained using srilm toolkit database courses randomly split training validation test sets ratio compensate model variation limited data size segmentation performed times different randomization seeds mean variation performance measured. course project trained different -gram nnlm i.e. context size predict word using preceding words. project classiﬁcation performed measuring perplexity test word sequences terms sentences using trained nnlm course. denote candidate courses/instructors selected classiﬁer. expressed classiﬁcation performance nnlm also compared baselines n-gram backmodel kneser-ney smoothing. perplexities computed without insertions start-ofsentence end-of-sentence tokens srilm nnlm. evaluate ﬁtness different training methods table lists training-to-test perplexities courses averaged different database segmentations. line table test set. figure shows graphically accuracy number sentences particular course accuracies obtained different methods uniqram -gram nnlm -gram. number randomly selected sentences range particular number sentences trials performed mean accuracies standard deviations shown ﬁgure. mentioned earlier sec. courses taught instructor courses excluded courses/instructors used compute -way classiﬁcation accuracies. figure demonstrates mean accuracy courses. -gram nnlm -gram achieve similar accuracy variation. however nnlm -gram slightly accurate -gram. shows mean perplexities standard deviation n-gram methods plus nnlm -gram method. illustrates among n-gram methods -gram slightly better tri-gram -gram nnlm method achieves even lower perplexities average. test classiﬁcation accuracy particular course instructor sentence-wise perplexity computed trained nnlms different classes. sentences randomly selected figure compares accuracies models. provides accuracies difﬁculty stages given test sentences. perform differently along course/instructor datasets. however nnlm -gram average slightly better -gram especially number sentences less. besides classiﬁcation accuracy confusion between different course/instructors also investigated. figure shows confusion matrices courses/instructors computed randomly picked test sentence methods. probabilities scale better visualization. confusion value column probability assigning course/instructor one. since course taught instructor surprising values larger others row. addition instructors taught courses ﬁeld courses courses likely confused other. topic text play role authorship attribution. since nnlm -gram assigns higher values -gram biased towards author rather content sense. paper investigates authorship attribution using nnlm. experimental setup nnlm detailed mathematical elaboration. results terms ﬁtness perplexity classiﬁcation accuracies confusion scores promising compared baseline n-gram methods. performance competitive state-of-the-art terms classiﬁcation accuracy testing sensitivity i.e. length test text used order achieve conﬁdent results. previous work listed sec. best reported results date achieved either accuracy similar author pool size authors limited training date author. shown figure work achieves nearly perfect accuracies test sentences given. however since baseline nnlm methods achieves nearly perfect accuracies limited test data current database sufﬁciently large challenging probably consistency training test sets contribution topic distinction. future algorithm tested using datasets larger author sizes greater styling simisince purely topic-neutral text data even exist developing general author mixed-topic data adapting particular topics also desirable. could particularly helpful topics text data available. compensate relatively small size training also trained group authors adapt individuals. nnlm assigns unique representation single word difﬁcult model words multiple meanings thus combining nnlm n-gram models might beneﬁcial. recurrent nnlm captures context size current feed-forward model also worth exploring. seroussi zukerman bohnert authorship attribution latent dirichlet allocation. proceedings ﬁfteenth conference computational natural language learning pages association computational linguistics. socher manning parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning pages stamatatos survey modern authorship attribution methods. journal american society information science technology stolcke srilm-an extensible language modcoyotl-morales villase˜nor-pineda montes-y g´omez rosso authorship attribution using word sequences. progress pattern recognition image analysis applications pages springer. hinton deng dahl mohamed a.r. jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared signal processing views four research groups. magazine ieee keˇselj peng cercone thomas n-gram-based author proﬁles authorship attribuproceedings conference paciﬁc assotion. ciation computational linguistics pacling volume pages luyckx daelemans authorship attribution veriﬁcation many authors limited data. proceedings international conference computational linguistics-volume pages association computational linguistics. mikolov karaﬁ´at burget cernock`y khudanpur recurrent neural network interspeech based language model. makuhari chiba japan september pages", "year": 2016}