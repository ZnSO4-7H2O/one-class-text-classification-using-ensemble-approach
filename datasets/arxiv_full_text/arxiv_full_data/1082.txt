{"title": "DropIn: Making Reservoir Computing Neural Networks Robust to Missing  Inputs by Dropout", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The paper presents a novel, principled approach to train recurrent neural networks from the Reservoir Computing family that are robust to missing part of the input features at prediction time. By building on the ensembling properties of Dropout regularization, we propose a methodology, named DropIn, which efficiently trains a neural model as a committee machine of subnetworks, each capable of predicting with a subset of the original input features. We discuss the application of the DropIn methodology in the context of Reservoir Computing models and targeting applications characterized by input sources that are unreliable or prone to be disconnected, such as in pervasive wireless sensor networks and ambient intelligence. We provide an experimental assessment using real-world data from such application domains, showing how the Dropin methodology allows to maintain predictive performances comparable to those of a model without missing features, even when 20\\%-50\\% of the inputs are not available.", "text": "ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. appear proceedings international joint conference neural networks abstract—the paper presents novel principled approach train recurrent neural networks reservoir computing family robust missing part input features prediction time. building ensembling properties dropout regularization propose methodology named dropin efﬁciently trains neural model committee machine subnetworks capable predicting subset original input features. discuss application dropin methodology context reservoir computing models targeting applications characterized input sources unreliable prone disconnected pervasive wireless sensor networks ambient intelligence. provide experimental assessment using realworld data application domains showing dropin methodology allows maintain predictive performances comparable model without missing features even inputs available. increasing diffusion networks pervasively distributed environmental personal sensor devices requires computational models capable dealing continuous streams sensor data form time series measurements. machine learning models context serve make sense multivariate sequences heterogeneous sensor information providing predictions supporting context-awareness ambient intelligence functions. numerous applications developed modeling regression classiﬁcation tasks sensor streams including event recognition fault anomaly detection human activity recognition general supporting robotic intenet-of-things applications providing adaptivity context awareness mechanisms. recurrent neural networks popular effective means deal sequential information thanks ability encoding history past inputs within network state. however networks well feedforward counterpart require ﬁxed input features available training well test time. predictive performance tends abruptly decline input features trained missing querying model predictions many practical ubiquitous computing scenarios comprise information generated networks loosely connected devices often battery-operated communicating wireless channels little quality service guarantees. scenarios unlikely deal missing information might result performing recognition prediction tasks missing part inputs. goal work propose experimentally assess principled approach make neural networks robust missing inputs query time focusing particular efﬁcient models reservoir computing paradigm. large majority works literature addresses problem missing inputs solely respect information missing training time. context dealing missing information amounts ﬁnding best strategies impute values certain features missing training samples provides recent survey problem. work instead consider scenario able train using complete data part inputs become unavailable even long time system operation typical approaches deal problem exploits imputation i.e. substituting missing feature inferring information. instance spatial-temporal replacement scheme proposed fuzzy adaptive resonance theory neural network used anomaly detection wireless sensor network missing input information related node imputed based majority voting readings devices spatially closer failing one. similarly uses k-nearest neighbor replace missing sensor values spatially temporally correlated sensor values. approach major drawback fact assumes sensor devices homogenous type possible spatially/temporally related sensor type failing replace measurements. alternative approach problem based ﬁtting probability distribution missing features given observable inputs e.g. using parzen window estimator sample replacement measurements missing information proposed discriminative solution uses model static problems recurrent layers used estimate values missing features. rather different approach information imputation uses committee machines model ensembles models accommodate data missing features training ensemble predictors random subsets total number available features. none approaches described attempt make original model robust missing information. either impute missing information enrich original model training multiple instances different input conﬁgurations considerable impact training time number input features increases. propose novel solution missing input problem exploits dropout regularization technique become widely popular context deep learning latter years. dropout prevents overﬁtting randomly selecting training subset neurons synapses dropped current training sample. intuition guiding work effects dropout train full neural model ensemble thinned models obtained random dropout disconnections. such believe exploited build model robust missing inputs ensemble thinned models obtained applying dropout input neurons connections alone. following discuss application technique reservoir computing family efﬁcient models quite popular applications dealing sensor streams robustness noise computational efﬁciency e.g. embedded implementation low-power devices. networks based separation recurrent dynamical layer i.e. reservoir non-recurrent output layer i.e. readout. particular focus echo state network model output layer connections trained input recurrent weights left randomly initialized. note ﬁrst application dropout techniques models make networks robust missing inputs. choice reference model motivated hand relevance popularity ambient intelligence pervasive computing applications. hand would like assess effect dropout approach referred dropin onwards challenging conditions i.e. output connections adaptive beneﬁcial effect input dropout propagates them. nevertheless proposed dropin technique applied recurrent feedforward neural network standard dropout technique. describes dropin application section provides experimental assessment proposed model real-world datasets ambient intelligence pervasive computing applications. section concludes paper. dropout regularization technique fully connected neural network layers adaptive synaptic weights intuition dropout prevent units coadapting randomly disconnecting subset network training. dropout also though model combination ensembling technique efﬁciently combine exponential number neural network architectures corresponding thinned networks obtained randomly dropping neuron subsets. dropout training works randomly determining units kept given training instant; unit retained ﬁxed probability independently units dropped probability retention probability typically determined validation. dropped unit temporarily removed network along incoming outgoing connections practice amounts zeroing dropped unit output. respect neural network units ensemble thinned networks whose total number parameters still bound since share weights. test time feasible reproduce network thinning process compute ensemble prediction models. however shows efﬁciently approximated using original un-thinned network weights scaled retention probability dropout approach later generalized dropconnect model instead dropping single units dropconnect sets random subsets network weights zero unit receives input portion neurons previous layer. exploits again retention probability independently applied single elements neural network connectivity matrix. shown enhance ensembling effect respect dropout number thinned network dropconnect matrix randomly selected zero ones used weight masking reservoir computing paradigm based separation recurrent part network reservoir feedforward neurons including output layer referred readout. reservoir encodes history input signals. activations neurons combined readout layer compute network predictions. aspect training efﬁciency readout layer trained part network whereas input reservoir connections randomly initialized conditions ensuring contractivity left untrained. among different approaches focus popular model suitability ambient intelligence applications considered work. comprises input layer units reservoir layer units readout units. reservoir large sparsely-connected layer recurrent non-linear units used perform contractive encoding history driving input signals state space. readout comprises feedforward linear neurons computing predictions weighted linear combination reservoir activations. work focus leaky integrator variant standard model applies exponential moving average reservoir state space values. allows better handling input sequences change slowly respect sampling frequency shown work best practice dealing sensor data streams time step activation reservoir activation li-esn computed whx) vector inputs time input-to-reservoir weight matrix recurrent reservoir weight matrix component-wise reservoir activation function. term leaking rate controls speed li-esn state dynamics larger values denoting faster dynamics. reservoir parameters left untrained random initialization subject constraints given called echo state property requiring network state asymptotically depends driving input signal dependency initial conditions progressively lost. provided necessary sufﬁcient condition esp. sufﬁcient condition states largest singular value reservoir weight matrix must less li-esn model condition applies leaky integrated matrix largest singular value necessary condition hand says spectral radius i.e. largest absolute eigenvalue matrix network asymptotically unstable null states hence lacks esp. sufﬁcient condition considered restrictive practical purposes. instead matrix often initialized satisfy necessary condition i.e. values spectral radius typically close stability threshold input weights randomly chosen uniform distribution typically uniform distribution scaled holds. wout reservoir-to-readout weight matrix. readout output computed time step subset depending application training model amounts learning values wout matrix implies solution linear least squares minimization problem typically achieved efﬁcient batch linear methods moorepenrose pseudo-inversion ridge regression describe novel dropout technique principled approach make neural networks robust missing inputs prediction time. name approach dropin based unit-wise dropout level input neurons network. rationale inspiring work observation application dropout input units essentially training committee machine thinned networks shared weights. sense approach recalls committee machine differently work method require deﬁne speciﬁc committee machine architecture induces increase parametrization original model. choice using unit-wise dropout place connection-wise dropconnect motivated fact latter approach would make neural network robust losing inputs; rather would make network somewhat less focused exploitation speciﬁc input which principle discriminant task. preliminary experiments performed dropconnect approach conﬁrm intuition performs signiﬁcantly worse dropout tasks. dropin approach general applied neural network model dropout applies. here focus dropin context particular model. motivation choice twofold. first models popular approach ambient intelligence pervasive computing applications areas deal input information collected low-ﬁdelity devices transmitted loose communication channels thus involving high likelihood faults missing data second motivation related exploration effect dropin algorithm dropin training require dataset pairs input-output sequences properly initialized model; forgetting factor regularization term input unit retention probability init loop algorithm ensures readout weights modiﬁed take account reservoir activations corresponding certain inputs missing. clearly training sequence expected processed next epochs time different input features missing. standard dropout decision drop input unit taken independently input feature probability input masking easily obtained zeroing elements current input using masking vector equations innermost loop algorithm implement standard algorithm whose details found dropinesn model algorithm implemented matlab reference source code used experimental assessment section found here. model trained using dropin used prediction without weight re-scaling. note different standard dropout works. dropin-esn fact masking input units bear effect input-to-reservoir weights whose values frozen random initialization. readout weights ones affected dropin effect indirect mediated reservoir activations. preliminary experiments effect weight re-scaling show negatively affects predictive performance. reasons underlying effect deserve studies possibly fact dropin networks require re-scaling might associated fact fig. model dropin application input masking vector generated time training sequence network. decision retain input unit taken independently feature random sampling uniform probability retention probability learning paradigm perhaps least straightforward terms dropout application. characteristics fact makes beneﬁt poorly dropout terms regularization minor part network weights trained. fact extent knowledge ﬁrst work dropout techniques used here seek determine application dropout inputs untrained weights effect propagating throughout untrained reservoir readout trained part network hence part effect dropin recorded. application dropin requires trained iterative learning algorithm. fact order make network robust missing inputs need process single training sequences multiple times time ﬁxed probability independently missing single inputs retention probability dropout. discusses section ii-b training entails solving least squares minimization problem readout weights wout. simplest approach would tackle problem gradient descent direction minimizing instantaneous squared error y∗−y desired prediction time alternatively recursive least squares algorithm proposed fast online learning approach. time step minimizes discounted error dropin-esn model embeds input units dropout within steps incremental learning. schematic view procedure provided pseudo-code algorithm architecture depicted fig. assuming provided properly initialized procedure runs several epochs dataset error stability criterion maximum number epochs reached. dropin mask fig. computed processing whole sequence hence mask elements sequence applications involve simulate effect sensor devices faults prediction time. sake work assume instead sufﬁcient amount training data without missing information available tackling missing information training outside scope work. section iv-a brieﬂy describes case studies summarizes experimental setup section iv-b analyzes results experiments. ﬁrst dataset human movement prediction benchmark based information originally presented available download repository. task deals prediction room changes ofﬁce environment based received signal strength packets exchanged mote worn user four anchor devices deployed ﬁxed positions corners rooms. experimental data consists different setups work refer homogenous setup described original paper. data composed sequences concerning trajectories different users moving room couples separated hallway. trajectories start different corners rooms lead user changing room others user remaining room. task requires learning model predict whether user going change room stay current based information recorded starting point trajectory. prediction concerning room change taken marker point located door movements; therefore different paths cannot distinguished based values collected benchmark formalized binary classiﬁcation task time series. collected measurements denote samples gathered sending beacon packet anchors mobile regular intervals times second using full transmission power iris-type mote. values four anchors input features model organized sequences varying length corresponding trajectory measurements starting point marker target classiﬁcation label associated input sequence indicate whether user change room classiﬁcation second dataset referred kitchen cleaning concerns ambient assisted living scenario cleaning robot operates home environment located ¨angen senior residence facilities ¨orebro universitet. task learning model required learn predict user preference modelled robotic planner domain knowledge fact robot cleaning kitchen user task part larger effort assess self-adaptation abilities robotic ecology planner developed part rubicon project including also automated feature selection mechanisms time series experimental scenario consists real-world sensorized rfid ﬂoor mobile robot range-ﬁnder localization mote-class devices. device equipped light temperature humidity passive infrared presence sensors. kitchen cleaning experiment consider inputs deemed relevant feature selection analysis readings ﬁrst motes plus information position robot environment measured rangeﬁnder localization system. dataset contains information robot initiating kitchen cleaning tasks consist robot moving base station living room kitchen user might present might enter kitchen robot navigation. information recorded robot localization system across whole robot navigation towards kitchen thus collecting time series input features. half associated cleaning task correctly completed remainder associated task failures presence user kitchen. target learn kitchen cleaning task preference target output equal user enters kitchen point cleaning task kitchen free. differently previous dataset target values associated element input sequence prediction thus performed time step experimental assessment intended confront performance standard li-esn trained without dropout respect dropin li-esn model using different input unit retention probabilities latter value particular added assess performance break-point introducing input dropout. model selection scheme set-up assess learning performance. first dataset hold-out total sequences extracted create external test set. k-fold cross-validation approach applied remaining data model selection purposes ﬁrst second dataset respectively. standard li-esn dropin li-esn trained hyperparameters varying follows number reservoir neurons connectivity leaky parameter term value ﬁxed discussed conﬁguration hyperparameters generated three random reservoir topologies random weight initializations using uniform distribution input reservoir weights along lines reservoir weights re-scaled satisfy necessary condition spectral radius validation error computed hyperparameter conﬁguration select best model trained full training data tested hold-out information. experimental assessment assess effect missing inputs predictive performance li-esn trained without dropin using different unit retention probabilities. ﬁrst provide baseline results models without missing inputs. then show effect increasing number missing input features performing test predictions input features removed. instance performance single missing input obtained removing single input feature test sequence time network prediction computed process iterated input features average prediction performance computed. similarly testing multiple missing inputs perform predictions using possible combinations missing features provide performance statistics averaged combinations. table shows baseline results model selected conﬁguration ﬁrst dataset. model performance assessed classiﬁcation accuracy. baseline results show li-esn model trained without dropin achieves higher validation test accuracies decreasing levels input retention probabilities yield decreasing predictive accuracies inputs present. surprising instance retention probability task means essentially model trained perform predictions using information single input randomly selected time available four. using dropin performance standard liesn performance difference explained terms excessive regularization dropin-esn subject yields underﬁtting. clearly seen confronting training accuracy models standard li-esn achieves classiﬁcation accuracy dropin-esn stops figure hand shows clearly advantage dropin training dealing missing inputs. predictive performance standard li-esn drops considerably already single input missing. contrary performance dropin network increases respect baseline reaching remaining well standard li-esn model even inputs lost. reason performance increase respect baseline sought fact dropping single input test time basically allowing network work training conditions i.e. missing average input time. performance dropin using also better li-esn whereas clearly breakpoint amount inputs dropped training task. table shows baseline results kitchen cleaning data performance assessed using mean absolute error targets outputs available element sequence. dataset beneﬁcial effect dropin training even evident. strong regularization effect case ensures dropin networks yield better test performances standard li-esn even using features despite fact latest model achieves lowest validation error dropin network less training data generalizes better test set. might speciﬁcs task which particular characterized presence inputs high degree redundancy other hence favouring constrained model. advantage dropin approach evident especially considering effect missing inputs shown plot fig. again standard li-esn model cope well already single missing input feature jumping raising inputs missing. hand dropin model best missing inputs conﬁgurations particular loss single input shift signiﬁcantly error respect baseline table proposed novel approach deal problem missing input information prediction time neural networks related well known dropout regularization. approach named dropin applies dropout level input neurons. exploit ensembling effect produced dropout train neural network interpreted committee subnetworks capable making predictions using subset available inputs determined neuron retention probability. proposed approach principled general seamlessly applied artiﬁcial neural network model dropout applies require deﬁne ad-hoc committee architectures deﬁne data imputation algorithms models. further dropin simple embed training phase neural model minor computational impact. assessed dropin performance conjunction reservoir computing neural models. least straightforward terms dropout application untrained nature input recurrent connection weights makes recording dropin effect synaptic connections quite challenging. time extent knowledge ﬁrst time dropout techniques applied models well ﬁrst work speciﬁcally addressing problem missing input prediction time rnns. experimental assessment provides snapshot potential approach applied real-world predictive tasks comprising input information collected networks distributed fragile loosely coupled sensor devices. particular shown models trained gest dropin approach indeed effective dealing missing inputs prediction time although might come cost slightly reduced performance inputs available depending characteristics task. terms lesson learned seems reasonable deploying predictor subject missing inputs train models ﬁrst standard predictor trained without dropin work maximum predictive performance inputs available. second dropin trained model queried inputs missing maintain highest achievable predictive performances. seems feasible practical approach given computational impact dropin procedure. nevertheless suggested performance second dataset dropin trained model also good candidate sole predictor deployed dragone amato bacciu chessa coleman rocco gallicchio gennaro lozano maguire mcginnity micheli ohare renteria safﬁotti vairo vance cognitive robotic ecology approach self-conﬁguring evolving systems engineering applications artiﬁcial intelligence vol. bacciu gallicchio micheli rocco safﬁotti learning context-aware mobile robot navigation home environments iisa international conference information intelligence systems applications july bacciu micheli sperduti compositional generative mapping tree-structured data part bottom-up probabilistic modeling trees ieee trans. neural netw. learning syst. vol. dropin maintain good predictive performance even pair input features missing e.g. sensor fault. conversely model trained standard procedure neat predictive performance degradation already missing single input. experimental outcome suggests dropin become useful methodology developing ambient intelligence pervasive computing applications need continuously stream predictions despite inputs missing device communication faults. future developments work consider extending application assessment dropin methodology neural models recurrent static also conjunction different case studies considered paper. longer term instead interested studying technique exploited efﬁciently train recursive neural network models deal structured data different samples characterized different connectivity topology work partially supported industrial research project funded biobeats dipartimento informatica universit`a pisa. bacciu would like acknowledge support italian ministry education university research project list-it tresp neuneier ahmad efﬁcient methods dealing missing data supervised learning advances neural information processing systems. morgan kaufmann tresp hofmann missing noisy data nonlinear timeseries prediction neural networks signal processing proceedings ieee workshop. ieee bengio gingras recurrent neural networks missing asynchronous data advances neural information processing systems bacciu barsocchi chessa gallicchio micheli experimental characterization reservoir computing ambient assisted living applications neural computing applications vol.", "year": 2017}