{"title": "Dimensionality Reduction by Local Discriminative Gaussians", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.", "text": "present local discriminative gaussian dimensionality reduction supervised dimensionality reduction technique classiﬁcation. objective function approximation leave-one-out training error local quadratic discriminant analysis classiﬁer thus acts locally training point order mapping similar data discriminated dissimilar data. state-ofthe-art linear dimensionality reduction methods require gradient descent iterative solution approaches solved single eigen-decomposition. thus scales better datasets large number feature dimensions training examples. also adapt transfer learning setting show achieves good performance test data distribution diﬀers training data. dimensionality reduction mapping highdimensional data lower-dimensional space retaining much information content data possible. preprocessing step supervised classiﬁcation algorithms dimensionality reduction achieves several important goals. reduces storage requirements algorithm complexity reducing input space data. improve performance learning algorithms rejecting spurious noisy features prior training testing. dimensionality reduction also protect overﬁtting reducing number parameters learned present method supervised dimensionality reduction based local discriminative gaussian criterion. discriminative gaussian criterion smooth approximation leave-one-out cross-validation error quadratic discriminant analysis classiﬁer seeks mapping quadratic boundary separates classes. goal separation class diﬃcult achieve globally criterion instead operates locally training point. considered objective function non-convex analytical solution; however present approximation solved maximal eigenvalue decomposition. simplicity solution advantage state-of-the-art dimensionality reduction techniques require iterative solution methods complex generalized eigenvalue decompositions. perform experiments supervised dimensionality reduction dimensionality reduction transfer learning. show datasets large number feature dimensions state-of-the-art algorithms either intractably slow exhibit numerical instability whereas able extract useful mapping even number features original data thousands. also show easily extended transfer learning setting training data drawn diﬀerent distribution test data. experiments show eﬀective setting well. straint constraint simpliﬁes making covariance gaussians mapped space independent furthermore makes unique solution. taking gaussians arrive objective diﬀerent terms which minimized individually give maximum likelihood solution incorrect-class gaussian i.e. gaussian distribution trained local neighbors coming diﬀerent class. therefore viewed regularized maximum likelihood estimate regularization term attempts minimize likelihood incorrect classes. discontinuity indicator function makes diﬃcult minimize. order arrive smooth diﬀerentiable objective function approximates substitute indicator learned parameters shown improve classiﬁcation performance parameters learned maximimum likelihood estimation. work motivated maximizing mutual information class labels feature vectors. estimate parameters gaussian point class ﬁnding nearest class neighbors euclidean distance training point using points estimate gaussian’s maximum likelihood mean covariance. reduce estimation variance model covariance matrix scaled identity properly sized identity matrix. therefore non-convex analytical solution. gradient-descent global optimization used become computationally expensive number classes training samples dimensionality large. therefore propose tractable approximation analytical solution. minimizes approximation used directly lfda found largest number ﬁnal dimensions submatrices optimal solution fewer dimensions. finally large body work distance metric learning feature selection related linear dimensionality reduction. distance metric learning addresses problem best determine distance feature vectors linear distance metric learning primarily concerned ﬁnding positive semi-deﬁnite mahalanobis metric gives distance tion thought ﬁnding rank mahalanobis metric approaches given propose convex optimization problems ﬁnding methods suﬀer drawback rank constraints non-convex thus typically rank. however perform dimensionality reduction rewriting mahalanobis metric lλlt using feature selection method resulting λ/lt proposed perform experiments compare several diﬀerent dimensionality reduction methods lfda information theoretic metric learning feature selection using maximum-relevance minimum redundancy criterion lfda itml mrmr feature selection code provided authors. evaluate performance dimensionality reduction methods k-nn classiﬁcation accuracy done preprocessing step standard normalize training data feature mean zero standard deviation one. choose number neighbors used estimate local gaussians dimensionality reduction ﬁvefold cross-validation using local classiﬁer original data. fisher discriminant analysis supervised technique chooses maximize ratio between-class covariance within-class covariance solution choose eigenvectors generalized eigendecomposition νsλ. drawbacks. first perform poorly multi-modal data single linear boundary separates data class. second between-class covariance local fisher discriminant analysis alleviates drawbacks fda. lfda generalizes adding weight based pairwise sample distances between-class within-class covariance matrices. thus lfda able separate multi-modal data. change also results neighbourhood components analysis dimensionality reduction technique based smooth approximation leave-one-out k-nn error. dimensionality reduction found shown provide good classiﬁcation accuracy; however suﬀers figure mean classiﬁcation accuracy random splits data. diamonds mark methods statistically best statistically diﬀerent best conﬁdence dimensionality. lfda itml plotted datasets reasons given section table mean training time seconds mean classiﬁcation accuracy number dimensions chosen cross-validation. bold font highlights methods statistically best statistically diﬀerent best conﬁdence. symbol legend method converge three hours training/test split computable. imizes k-nn leave-one-out cross-validation error dimensionality equal number classes plus ﬁve. found that general dimensions number classes present data good dimensionality choose case ties select largest value mrmr requires discretize itml features feature selection thresholding mean recommended authors’ code. perform experiments ﬁfteen datasets average accuracy random splits training test data datasets found either machine learning repository machine learning dataset repository. p-mutants dataset contained large degree class asymmetry. therefore randomly sampled inactive class samples discarded rest order make split inactive active class data figure table show small datasets comparable state-of-the-art methods. however provides clear advantage datasets largest feature dimensionality. also outperforms lfda datasets. provide dimensionality fewer number classes limperformance ionoshpere ringnorm datasets. furthermore lfda exhibit numerical instability datasets large feature dimensionality fact withinclass covariance matrix underdetermined. thus generalized eigenvalue decomposition algorithms solve fails discriminative dimensions. lfda returns complex eigenvalues arcene dexter datasets dexter dataset; thus lfda results computable datasets. table show average classiﬁcation accuracy dimensionality chosen leave-one-out cross-validation. increasing dimensionality cross-validation accuracy decreases adding another dimension. run-time numbers measure mean time takes seconds method produce dimensions shown figure select best dimensionality. classify test data drawn unknown target domain distribution feature vectors class labels training examples. however assume plenty training examples source domain diﬀers target domain thought useful learning. example experiments treat resized mnist handwritten digits source usps handwritten digits target goal transfer learning achieve high classiﬁcation accuracy target domain training classiﬁer using sets training data. therefore goals dimensionality reduction matrix target domain data separated according class. however added goal wish mapping source target domain distributions show examples diﬀerent one-dimensional spaces mapped higher-dimensional space left plot mapping target domain data separated according class source target domain distributions similar. right plot target domain data separated additionally source domain data distribution similar target domain data. therefore source target domain data used train classiﬁer test data using right-side mapping. estimate parameters gaussian distribution target domain point using nearest source domain training examples. ﬁrst term primary transfer term. denominator term ﬁnds maximizes likelihood target domain data gaussian distribution trained using local source domain data thus ﬁnding brings same-class source target domain data close together. conversely numerfigure examples one-dimensional mappings transfer dimensionality reduction. example target domain separated right example target domain data matches source domain data making transfer learning eﬀective. second term normal objective function source domain data only. thus similar standard dimensionality reduction acting pooled source target domain data. include term source target domain distributions similar train using much data possible. choose cross-validating target domain training data. case ties choose largest value thereby defaulting using much data possible. make approximations section analogous approximation makes distances examples across domains small same-class data large diﬀerent-class data. mrmr feature selection dimensionality reduction matrix conduct transfer learning experiments different classiﬁcation problems. ﬁrst classify images according category object found images thirty class problem datasets three domains amazon product images images taken high-resolution dslr camera images taken low-resolution webcam. examples back packs category three domains shown figure dataset ﬁrst used saenko preprocessing techniques described featurize images results features image. second problem diﬀerent domains consist grayscale digit images mnist usps datasets. image features pixel values preprocessing resize mnist images pixels match usps images. show examples images domain figure compare four dimensionality reduction techniques performance using nearest-neighbor classiﬁcation. ﬁrst transfer choose k-nn cross-validation dimensionality equal number classes plus ﬁve. compare pooled pooled dimensionality reduction. approaches ignore difference domains pooling training data domain performing standard fda. finally compare linear itml transfer learning described mrmr feature selection. also show results itml dimensionality reduction. source domain training data consists available data particular domain standard normalize mean zero standard deviation one. target domain training data consists exactly examples class standard normalized independently source domain training data. remove features exhibit zero variance source target domain. figure plots accuracy averaged random splits target domain test training data. results show statistically best tied best many dimensions experiments. pooled performs well datasets amazon images source fails perform well datasets. pooled performs poorly datasets. show results best dimensionality chosen cross-validation similar table space constraints. note dimensions plotted itml achieves highest accuracy dimensionality reduction still match best performance ldg. presented dimensionality reduction technique maps data space classes separated locally training point. solved simple maximal eigenvalue decomposition thus scales better iterative methods lfda large datasets. furthermore shown dimensionality reduction applied transfer learning problems good results.", "year": 2012}