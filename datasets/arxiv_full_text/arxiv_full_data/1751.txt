{"title": "Learning the Dimensionality of Word Embeddings", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We describe a method for learning word embeddings with data-dependent dimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is made dynamic by employing techniques used by Cote & Larochelle (2016) to define an RBM with an infinite number of hidden units. We show qualitatively and quantitatively that SD-SG and SD-CBOW are competitive with their fixed-dimension counterparts while providing a distribution over embedding dimensionalities, which offers a window into how semantics distribute across dimensions.", "text": "describe method learning word embeddings data-dependent dimensionality. stochastic dimensionality skip-gram stochastic dimensionality continuous bag-of-words nonparametric analogs mikolov al.’s well-known ‘wordvec’ models. vector dimensionality made dynamic employing techniques used côté larochelle deﬁne inﬁnite number hidden units. show qualitatively quantitatively sd-sg sd-cbow competitive ﬁxed-dimension counterparts providing distribution embedding dimensionalities offers window semantics distribute across dimensions. introduction word embeddings received wide-spread attention ability capture surprisingly detailed semantic information without supervision. however despite success still deﬁciencies. embedding dimensionality must user thus requires form cross-validation performed. issue implementation. words naturally vary semantic complexity since vector dimensionality standardized across vocabulary diﬃcult allocate appropriate number parameters word. instance meaning word race varies context meaning regatta rather speciﬁc invariant. seems unlikely race regatta’s representations could contain number parameters without overﬁtting underﬁtting. better capture semantic variability words propose novel extension popular skip-gram continuous bag-of-words models allows vectors stochastic data-dependent dimensionality. employing mathematical tools allow deﬁnition inﬁnite restricted boltzmann machine deﬁne log-bilinear energy-based models named stochastic dimensionality skip-gram stochastic dimensionality continuous bag-of-words ﬁxed dimensional counterparts. training sd-sg sd-cbow allow word representations grow naturally based well predict context. behavior among things enables vectors speciﬁc words dimensions vectors vague polysemous words elongate capture wide semantic range needed. aware ﬁrst word embedding method allows vector dimensionality learned vary across words. ﬁrst review original skip-gram continuous bag-of-words architectures describing novel extensions. following model deﬁnitions d-dimensional real-valued vector representing input word vector representing context word appearing k-sized window around instance training corpus distribution random positive integer denoting maximum index compute embedding inner product also known partition function. deﬁne energy function weight penalty. notice sd-sg essentially except three modiﬁcation inner product index random variable term introduced penalize vector length additional term introduced ensure inﬁnite dimensions results convergent geometric series convergence turn yields ﬁnite partition function; appendix underlying assumptions derivation. maximum index contains non-zero value. must exist sparsity assumption makes partition function ﬁnite assumption gives information value. work-around constant would restrict model make crucial hyperparameter. better option sample values rely randomness make learning dynamic tractable. grow arbitrarily vectors retain ﬁnite number non-zero elements. thus write loss terms expectation notice evidence bound widely used variational inference except equality bound variational distribution posterior tractable. sampling desire comes size context vocabulary. stochastic gradient descent used update also hierarchical softmax negative sampling commonly used approximate normalization term denominator introduced ﬁxed-dimensional embedding techniques cbow deﬁne extensions make vector dimensionality random variable. order embedding growth unconstrained word vectors context vectors considered inﬁnite dimensional initialized ﬁrst dimensions non-zero rest zero. score function estimate gradient produce dynamic vector growth vectors constrained grow dimension time done sd-sg sampling dimensional multinoulli related work mentioned introduction aware previous work deﬁnes embedding dimensionality random variable whose distribution learned data. much existing work increasing ﬂexibility embeddings auxiliary parameters. instance vilnis mccallum represent word multivariate gaussian distribution. gaussian embedding’s mean parameter serves much like traditional vector representation method’s novelty arises covariance parameter’s ability capture word speciﬁcity related work proposes using multiple embeddings word order handle polysemy homonymy bartunov adagram model closest spirit sd-sg sd-cbow uses dirichlet process learn unconstrained data-dependent number embeddings word. contrast sd-sg/-cbow dimensionality embedding still userspeciﬁed. evaluation evaluate sd-sg sd-cbow quantitatively qualitatively original cbow. experiments models trained billion word subset wikipedia samples drawn note presence term—the term said problematic equation since known. compute term monte carlo objective setting largest value sampled point training. presence boon because since depend need control variates stabilize typically high variance term there’s still problem therefore large dimensionality could sampled resulting gradient incurring painful computational costs. remedy situation value greater current value sampled restricting model grow dimension time. constraining growth computationally eﬃcient since drawn -dimensional multinoulli distribution parameters intuition model sample dimension less equal already suﬃciently large draw option choosing increase model’s capacity. hyperparameter controls growing behavior approaches right approaches inﬁnity. stochastic dimensionality continuous bag-ofwords model conditional gibbs distribution center word random positive integer denoting maximum index before given multiple context words ergy function deﬁned sd-sg admits ﬁnite partition function using arguments. primary diﬀerence sdsg sd-cbow latter assumes words appearing together window vector dimensionality. sd-sg figure subﬁgure shows results semantic similarity tasks. spearman’s rank correlation model human scores calculated wordsim- datasets. subﬁgure shows histogram expected vector dimensionalities training sd-cbow. subﬁgures show distribution dimensionalities sd-sg learned words race player. quantitative evaluation. test model’s ability rank word pairs according semantic similarity task commonly used gauge quality wes. evaluate embeddings standard test sets wordsim typical evaluation measure spearman’s rank correlation similarity scores produced model produced humans. correlations models reported subtable figure sd-sg sdcbow perform better dimensional counterparts worse dimensional counterparts. scores relatively competitive though separated qualitative evaluation. observing sdsg sd-cbow models perform comparatively ﬁnite versions somewhere dimensions qualitatively examine distributions vector dimensionalities. subﬁgure figure shows histogram expected dimensionality—i.e. ez|wc—of vector after training sd-cbow model. expected distribution long-tailed vague words occupy tail speciﬁc words found head. shown annotations word photon expected dimensionality homograph race note expected dimensionality correlates word frequency—due fact multi-sense words deﬁnition used frequently—but follow strictly. instance word william frequently occurring word corpus expected length closer lengths much rarer words similarly frequent words. subﬁgures figure plot quantity homographs race player learned sd-sg order examine multiple meanings conspicuous distribution dimensionalities. race distribution indeed least modes ﬁrst around dimensions represents racing determined computing nearest neighbors dimension cutoﬀ second around dimensions encodes anthropological meaning. propose modest modiﬁcations cbow allow embedding dimensionality learned data probabilistically principled way. models preserve performance semantic similarjoseph turian ratinov yoshua bengio. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics. association computational linguistics pages appendix finite partition function stochastic dimensionality skip-gram’s partition function containing countably inﬁnite values would seem divergent thus incomputable. however properties ﬁrst proposed deﬁne restricted boltzmann machine inﬁnite number hidden units penalty ensures word context vectors must ﬁnite two-norm iterative gradient-based optimization ﬁnite initial condition. words proper optimization method could converge inﬁnite solution vectors initialized ﬁnite number non-zero elements references sergey bartunov dmitry kondrashkin anton osokin dmitry vetrov. breaking sticks ambiguities adaptive skip-gram. international conference artiﬁcial intelligence statistics finkelstein evgeniy gabrilovich yossi matias ehud rivlin zach solan gadi wolfman eytan ruppin. placing search context concept revisited. proceedings international conference world wide web. pages eric huang richard socher christopher manning andrew improving word representations global context multiple word prototypes. annual meeting association computational linguistics tomas mikolov ilya sutskever chen greg corrado dean. distributed representations words phrases compositionaladvances neural information processing ity. systems. pages arvind neelakantan jeevan shankar alexandre passos andrew mccallum. eﬃcient nonparametric estimation multiple embeddings word vector space. conference empirical methods natural language processing joseph reisinger raymond mooney. multiprototype vector-space models word meaning. human language technologies annual conference north american chapter association computational linguistics. association computational linguistics pages tian hanjun jiang bian zhang enhong chen tie-yan liu. probabilistic model learning multi-prototype word embeddings. proceedings coling. pages sparsity penalty allows split e−e)) z=l+ e−e)) index factored second term remaining terms zero. steps algebra reveal presence convergent geometric series. intuitively think second quantifying data’s need term expand model’s capacity given", "year": 2015}