{"title": "Hierarchical Actor-Critic", "tag": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "abstract": "The ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning -- sample efficiency. Hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level. In this paper, we present a novel approach to hierarchical reinforcement learning called Hierarchical Actor-Critic (HAC) that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales. HAC has two key advantages over most existing hierarchical learning methods: (i) the potential for faster learning as agents learn short policies at each level of the hierarchy and (ii) an end-to-end approach. We demonstrate that HAC significantly accelerates learning in a series of tasks that require behavior over a relatively long time horizon and involve sparse rewards.", "text": "ability learn different resolutions time help overcome main challenges deep reinforcement learning sample efﬁciency. hierarchical agents operate different levels temporal abstraction learn tasks quickly divide work learning behaviors among multiple policies also explore environment higher level. paper present novel approach hierarchical reinforcement learning called hierarchical actor-critic enables agents learn break problems involving continuous action spaces simpler subproblems belonging different time scales. advantages existing hierarchical learning methods potential faster learning agents learn short policies level hierarchy end-to-end approach. demonstrate signiﬁcantly accelerates learning series tasks require behavior relatively long time horizon involve sparse rewards. despite major successes simulated real-world tasks problem many deep reinforcement learning algorithms slow. learning particularly slow rewards granted agents sparse. major reason reinforcement learning’s poor sample efﬁciency many existing algorithms force agents learn lowest level temporal abstraction. instance simulated robot agent given task involving locomotion agent need learn entire sequence joint torques accomplish task instead trying break problem higher level. learning exclusively levels abstraction slows learning reasons. first agents must learn longer sequences actions order achieve desired behavior. problematic policies involving longer sequences actions difﬁcult learn particularly rewards sparse. process propagating back q-values actions produce sparse reward preceding actions takes longer. issue long-term credit assignment also becomes severe actionvalue function needs learn q-values larger portion state-action space. second learning lowest level restrains exploration. agents propose higher level subgoals quickly determine distant states helpful achieving certain behavior goals. faster exploration state space environment speed process learning robust policy. existing hierarchical methods provide approach breaking tasks involving continuous action spaces guarantees shorter policies level abstraction end-to-end. current hierarchical approaches enable agents learn higher levels action space discrete further many existing hierarchical learning approaches implement hierarchical agents equitably divide work learning behavior among agent’s multiple policies. instance many approaches choose decompose problems smaller state spaces rather smaller time scales problematic continuous action space environments policy acts within small region state space need lengthy sequence actions escape region. existing hierarchical approaches also require non-trivial manual work including designing non-sparse reward functions preselecting possible higher level subgoals rather learning experience staggering training different policies paper introduce novel approach hierarchical reinforcement learning called hierarchical actor-critic algorithm enables agents learn divide tasks involving continuous state action spaces simpler problems belonging different time scales. achieves objective implementing agents learn multiple policies parallel. successive policy hierarchy responsible learning break problems subproblems increasingly time resolutions. figure provide intuition agents learn different time scales. ﬁgure shows agent action. goal-based actor network learns limited length policies operate different time resolutions critical feature algorithm time limits. actor network certain number actions achieve higher level input goal. section explains time limits enable actor network specialize different time scale. another advantage provides end-toend hierarchical learning approach. learns separate goals subgoals using agent’s experience algorithm requires sparse reward functions. hierarchical policies also learned parallel need learned different phases. paper series experiments comparing performance agents hierarchical actor-critic algorithm. tasks examined include pendulum reacher cartpole pick-andplace environments. task agents used hierarchical actor-critic signiﬁcantly outperformed not. tasks hierarchical actorcritic appears difference consistently solving task rarely solving task. video showing results experiments available https //www.youtube.com/watch?v=meyebpgepo. hierarchical actor-critic builds three techniques reinforcement learning literature deep deterministic policy gradient learning algorithm universal value function approximators hindsight experience replay ddpg serves learning infrastructure within hierarchical actor-critic. ddpg actor-critic algorithm thus uses neural networks enable agents learn experience. actor network learns deterministic policy maps states actions critic network approximates q-function action-value function current policy discounted future rewards γi−tri. thus critic network maps pairs expected long-term reward order learn near-optimal policy results large expected long-term reward ddpg follows cyclical process composed steps policy evaluation policy improvement. policy evaluation phase agent ﬁrst interacts environment period time using noisy policy normal distribution. transitions experienced stored tuples replay buffer. agent updates approximation q-function figure example hierarchy. agent ﬁgure uses three policies learn behavior. policy specializes breaking problems subproblems ﬁner time resolutions. uses three policies accomplish behavior. solid vertical lines represent time resolutions subgoals output policy. distance consecutive vertical lines time agent achieve subgoal. low-level policy outputs actual agent actions vertical lines low-level policy interpreted subgoals requiring action. ﬁgure high-level policy breaks goal three subgoals relatively large time resolutions. mid-level policy specializes dividing subgoal high-level policy three subgoals belonging shorter time scales. finally low-level policy specializes decomposing subgoal mid-level policy three agent actions represent smallest time resolution. crucial beneﬁt policy specialize breaking goals particular time scale subgoals certain smaller time scale policies learned limited length. beneﬁcial shorter policies learned quickly longer ones. further multiple policies operate different levels temporal abstraction helpful enables high-level exploration also accelerate learning. hierarchical actor-critic helps agents learn hierarchy policies similar figure using actor-critic networks. actor-critic network responsible learning policies within hierarchy. policies actor networks learned goal-based meaning take input current state goal output current policy performing mini-batch gradient descent loss function target bellman estimate q-function γq). policy improvement phase agent modiﬁes policy based updated approximation action-value function. actor function trained moving parameters direction gradient w.r.t. actors parameters. universal value function approximators second idea critical hac. uvfa extends action-value function incorporate goals. q-function represents expected long-term reward taking action given current state goal goal reward function discount function agent state achieves prescribed goal current state viewed terminating one. goals critical goals often hierarchical broken subgoals. goals also useful used easily sparse binary reward functions. hindsight experience replay another component reinforcement learning literature integral hierarchical actor-critic. helps agents learn goal-based policies quickly sparse reward functions used. idea behind even though agent failed achieve given goal episode agent learn sequence actions achieve different objective hindsight state agent ﬁnished. learning achieve different goals goal space help agent better determine achieve original goal. hindsight experience replay implemented creating separate copy transitions occurred episode replacing original goal goal achieved hindsight original reward appropriate value given goal. introduce hierarchical approach called hierarchical actor-critic. algorithm helps agents learn long time horizon tasks involving continuous action spaces sparse rewards quickly enabling agents learn break tasks easier subtasks belonging different time scales. directly addresses issue lengthy policies hinder many existing non-hierarchical hierarchical approaches agents learn limited policies level temporal abstraction. approach also end-to-end learns subgoal policies different levels temporal abstraction parallel requires sparse reward functions. objective algorithm learn hierarchical policy like shown figure hierarchical policy composed multiple goal-based policies actor networks. actor network takes input current state higher level goal outputs action belonging particular time scale. subgoal actor networks bottom network figure action proposed subgoal. proposed subgoal desired future state future states agent. actor network operating lowest level abstraction network figure action agent’s actual output. experiments trained agents used hierarchical policies composed three actor networks. additional layers easily added. actor network critic network replay buffer learn near-optimal policy. actor networks figure shown connected respective critic networks figure critic network approximates q-function associated policy using bellman equation target γgqπi gt). sparse binary granted agent reached state within certain distance goal. described prescribed goal achieved terminating state reached. different time scales. order achieve last state reached action agent could chosen every state subgoal state. valid breakend goal reached subgoals subgoal achieved actions agent. breakdown higher level network’s replay buffer receives sequence transitions showing subgoals belonging larger time resolutions achieve hindsight goal. similarly low-level network’s replay buffer receives sequences transitions showing actions belonging smallest time scale achieve subgoals. course many episodes actor network learns achieve goals actions belonging respective time scale. thus time limits critical helping actor network specialize different time resolution provide simple divide sequence actions achieved goal multiple sequences belonging different time scales. learning limited policies different time resolutions presents signiﬁcant beneﬁts well challenges agent. beneﬁt easier learn multiple shorter policies parallel long policy. credit assignment less problem critic function policy needs learn q-values limited region state-action-goal space. learning faster reinforcement learning agents essentially learn beginning sparse rewards used. backwards learning process occurs quickly policy requires fewer actions. however limited policies also results signiﬁcant dilemma subgoal actor networks conﬂicting missions. subgoal actor networks need learn policy simultaneously achieve higher level goal actions possible also output subgoals achieved lower-level actor network limited number steps. producing subgoals ambitious achievable lower level layers specialize limited policies. overly ambitious subgoals could thus result failure higher level actor network achieve goal. simultaneously solve conﬂicting objectives upper lower level layers need coordinate upper level needs understand types subgoals lower level accomplish. take steps incentivize subgoal actor network output subgoals achieved actor network actions. first example discussed above experience transitions passed replay buffers subgoal actor networks contain actions actually achieved succeeding lower level actor network within maximum number actions. second penalizes proposed subgoals achieved. certain percentage time deﬁned hyworks take certain number actions achieve higher level goal. lowest level actor network means execute certain number actual agent actions achieve subgoal. higher level subgoal actor networks policy limit means network must achieve higher level goal within maximum number subgoals. policy length limit actor network controlled hyper parameter experiments generally used value actor network within hierarchical policy. limit thus restricts actor network learning achieve goals accomplished within certain number actions shortens goal-based policy learned. combination time limits hindsight experience replay enables actor network specialize different time resolutions. actor networks learn operate different time scales largely result hindsight learning agent learns achieve goal states agent actually reached episode help her. following example provide clarity. consider agent using layer hierarchical policy meaning agent must achieve subgoal agent actions achieve goal subgoals. even worst case scenario agent fails agent actions achieve subgoals goal agent still able learn could divided task achieving ﬁnal state problems belonging parameter agents test subgoals adding noise subgoals actions prescribed hierarchical policy. noise needs removed testing subgoals agent miss subgoal noise added lower level actions. subgoals cannot achieved agent’s current noise-free policy penalized. experiments layer proposed subgoal achieved layer received reward −ti. addition subgoal missed testing q-value missed proposed subgoal depend q-value different state. reward proposing subgoal must less. otherwise proposing missed subgoal larger q-value subgoals efﬁciently guide agent goal subgoal actor network update policy result. observed extra penalty proposing missed subgoal works intended subgoal layer would adjust proposing conservative subgoals lower-level layer achieve. passed actor network’s replay buffer. order learn policies certain length actor networks need passed sequences transitions length. also discussed above order subgoal actor network learn output effective subgoals belong respective time resolution certain transitions need passed actor network. proceeds follows. initial state goal passed actor network triggers passing series subgoals higher level actor networks lower level actor networks. agent subgoal testing phase noise added subgoals. cascade subgoals terminates last subgoal passed lowest level actor function located innermost loop. lowest level actor attempts achieve provided subgoal. action lowest level actor network copies transitions created result call processtrans function shown algorithm ﬁrst transition placed replay buffer transition indicates whether action taken state able achieve goal second transition awaits processing hindsight experience replay. lowest level actor attempts achieves higher-level goals gt...g hindsight experience replay executed. random transitions selected serve hindsight goals. result selected transition preceding transitions goal component replaced function transforms array state space goal space. thus interpreted subgoal achieved hindsight. reward component replaced appropriate value given updated goal. purpose transitions thus help agents learn sequences actions used achieve hindsight goals even though hindsight goals different original target goals. executed lowest level actor network control passed preceding higher level actor network. completion innermost loop consists subgoal action actor network different transitions created depending whether original proposed subgoal achieved actor network whether agent currently testing phase. proposed subgoal achieved transition passed replay buffer transition indicates whether subgoal achieved hindsight able achieve higher level goal proposed subgoal missed agent currently testing subgoals actor network penalized transition hand subgoal achieved actor network transition passed finally additional transition sti+ created hindsight experience replay. actor network proposes second subgoal actor network process begins again. insight second component action component every transition passed subgoal actor networks identiﬁes subgoal achieved lower level actor network steps except transitions penalize subgoals. result subgoal actor network learns output actions belong particular time resolution helpful towards achieving actor network’s higher level goal. interacting environment episodes ﬁlling replay buffers transitions agent learns experience updating actorcritic functions. mini-batch gradient descent performed critic network push q-function closer bellman estimates. next parameters actor function moved direction gradient q-function w.r.t lowest level actor network updating actor-critic network enable actor efﬁcient routes achieving shorter term goals. subgoal actor-critic layer updating actor-critic network means ﬁnding better balance conﬂicting objectives ﬁnding sequence subgoals quickly solve higher level goal achieved steps succeeding actor layer. evaluated hierarchical actor-critic approach total tasks pendulum-climb pendulum-balance reacher cartpole -object pick-and-place. environment compared performance agents using subgoal layers. agents using subgoal layers goal task agent swing pendulum maximum height marked yellow cube. agent needs touch yellow sphere located peak need balance pole. found efﬁcient policy could solve task around low-level actions. goal environment balance pendulum peak near yellow sphere. thus achieve goal pole must located near peak angular velocity near figure shows frames successful episode. found efﬁcient policy could solve task around low-level actions. goal task agent learn move randomly designated point marked yellow cube. agent task simulated robotic arm. make task require longer time horizon goal location always quadrant front opposite starting location gripper. found efﬁcient policy could solve task around individual actions. goal task swing pole yellow cube. order achieve goal angular velocity pole must also near position cart must yellow cube. found efﬁcient policy could solve task around low-level actions. idea task assess hierarchical actor-critic would perform task natural hierarchy. objective task pick blue move yellow rod. agent joint robot worm. efﬁcient agents solve task around steps. network within stack sparse reward function. experiment reward granted actor network took action complete goal reward action successful. also larger negative reward issued subgoal networks proposed subgoal lower level layers could achieve. rewards granted layer agent moved within certain distance layer’s goal. further rewards granted network independent other. instance agent miss low-level subgoal achieve high-level subgoal. case transition passed low-level replay buffer still list negative reward taking action given current state subgoal action unsuccessful. goals subgoals table shows subgoals goals task. represent cartesian joint positions joint velocities respectively. subgoals experiments essentially desired future states agent learns achieve along reaching goal. experiments used lower dimensional version state subgoal. goal shown last column table depends behavior user would like agent learn. speciﬁc desired behavior similar goal state. instance pendulum climb task agent needed learn swing peak actions possible. agent need balance pole. thus task goal -dimensional includes coordinates maximum height pendulum. hand pendulum balance task agent needed swing pole peak maintain near angular velocity pole upright. thus goal task -dimensional included coordinates maximum height pendulum desired angular velocity accompanying video figure show results experiments. figure chart plots percentage goals achieved agents using subgoal layers testing period. testing periods separated episodes testing period consists episodes. plot represents average performance runs task. agents learn task scratch thus begin pre-training. episode agents thereby using random policies. result work hierarchical actor-critic resulted substantial improvement performance. environments agents used learned robust policy signiﬁcantly faster. indeed many tasks agents used subgoal layers able consistently solve task. video frames figure also demonstrate algorithm working intended agents learning break problems easier subproblems. pendulum balance environment subgoals represented purple spheres show agent learned wants balance pole upright ﬁrst needs swing back forth. reacher environment goal location marked yellow cube away agent using subgoal layers agent often high-level subgoal marker green cube halfway goal. low-level subgoals marked purple cubes appear guide agent toward high-level subgoal. indicates agent learned effectively separate high-level goals easier low-level goals. similar hierarchical behavior evident pickand-place environments. -object pick-and-place high-level subgoal represented green sphere figure plots compare performance agents baseline ddpg+her method subgoal layers method subgoal layers tasks. performance measured percentage goals achieved testing period consists episodes. often appear vicinity blue blue picked blue attached high-level subgoal moves yellow rod. low-level subgoal represented purple sphere guide agent green high-level subgoal. also observed trials task except pick-and-place task agents using subgoal layers learned signiﬁcantly faster agents using subgoal layer. agent using subgoal layers learn extra policy three policies specialize learning shorter policies policies learned agents using subgoal layer. result supports main premise algorithm hierarchical agents learn shorter policies parallel outperform agents learning fewer longer policies. hierarchical topic ongoing research popular hierarchical reinforcement learning approach feudal reinforcement learning feudal reinforcement learning state space divided increasingly small regions level abstraction. dayan hinton present grid world example maze continually divided quarters level. level managers charge providing goals rewards sub-managers below. sub-managers need learn complete goals learning give tasks sub-managers. dayan hinton show feudal structure outperforms non-hierarchical q-learning approach. difference feudal reinforcement learning latter breaks problems along spatial dimension instead temporal dimension. problematic reasons. first unclear state space would divided high-dimensional continuous state spaces. second even divide highdimensional continuous state space feudal approach guarantee hierarchical policies learned short. small region continuous state space difﬁcult maneuver require many actions manager. hand motivates actor networks learn shorter policies accelerate learning. another popular framework hierarchical reinforcement learning options framework approach generally uses hierarchy layers enable agents break problems down. low-level layer consists multiple options policy solve speciﬁc task. high-level layer responsible learning sequence speciﬁc policies achieve task. uses different approach breaking problems down. instead high-level policy select many speciﬁc low-level policies high-level network provides subgoal single low-level network trained achieve variety subgoals learns goal-based policy. using low-level goal-based policy network instead several non-goal-based policies provide efﬁciency advantages learning achieve subgoal often help learning achieve different subgoals. instance pick-and-place task learning pick drop object certain location help agent learn pick drop object different target location. kulkarni proposed approach similarities options framework hac. algorithm named hierarchical-dqn aims help agents solve tasks environments discrete action spaces. agents implemented h-dqn break tasks using value functions. high-level layer attempts learn sequence subgoals accomplish task. low-level layer attempts learn sequence individual actions achieve provided subgoal. low-level layer thus learns goal-based policy value functions similar hac. however unlike hierarchical actor-critic method h-dqn enable agents learn sequence high-level subgoals scratch using sparse reward functions. papers montezumas revenge example agent provided possible subgoals included objects game doors ladders keys. agent responsible learning order items needed reached. external reward function also used help agent quickly order subgoals. reason hierarchical actor-critic need aids like sets subgoals manually-engineered reward functions hindsight experience replay. long agent occasionally achieve goals nearby intended goal agent chance learn desired behavior. introduced technique called hierarchical actorcritic uses temporal abstraction break complex problems easier subproblems. results indicate using policy learn challenging behavior environment sparse rewards problematic. better approach learn policies operating different time resolutions work together learn behavior. references andrychowicz marcin crow dwight alex schneider jonas fong rachel welinder peter mcgrew tobin josh pieter abbeel openai zaremba wojciech. hindsight experience replay. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems curran associates inc. http//papers.nips.cc/paper/ -hindsight-experience-replay.pdf. dayan peter hinton geoffrey feudal reinforcehanson cowan ment learning. giles advances neural information processing systems morgan-kaufmann http//papers.nips.cc/paper/ -feudal-reinforcement-learning. pdf. dietterich thomas maxq method hierarchical reinforcement learning. proceedings fifteenth international conference machine learning morgan kaufmann kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum josh. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. corr abs/. http//arxiv.org/abs/.. schaul horgan daniel gregor karol silver david. universal value function approximators. bach francis blei david proceedings international conference machine learning volume proceedings machine learning research lille france pmlr. http//proceedings.mlr. press/v/schaul.html. vezhnevets alexander sasha osindero simon schaul heess nicolas jaderberg silver david kavukcuoglu koray. feudal networks hierarchical reinforcement learning. corr abs/. http//arxiv.org/abs/..", "year": 2017}