{"title": "Kernel Task-Driven Dictionary Learning for Hyperspectral Image  Classification", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Dictionary learning algorithms have been successfully used in both reconstructive and discriminative tasks, where the input signal is represented by a linear combination of a few dictionary atoms. While these methods are usually developed under $\\ell_1$ sparsity constrain (prior) in the input domain, recent studies have demonstrated the advantages of sparse representation using structured sparsity priors in the kernel domain. In this paper, we propose a supervised dictionary learning algorithm in the kernel domain for hyperspectral image classification. In the proposed formulation, the dictionary and classifier are obtained jointly for optimal classification performance. The supervised formulation is task-driven and provides learned features from the hyperspectral data that are well suited for the classification task. Moreover, the proposed algorithm uses a joint ($\\ell_{12}$) sparsity prior to enforce collaboration among the neighboring pixels. The simulation results illustrate the efficiency of the proposed dictionary learning algorithm.", "text": "dictionary learning algorithms successfully used reconstructive discriminative tasks input signal represented linear combination dictionary atoms. methods usually developed sparsity constrain input domain recent studies demonstrated advantages sparse representation using structured sparsity priors kernel domain. paper propose supervised dictionary learning algorithm kernel domain hyperspectral image classiﬁcation. proposed formulation dictionary classiﬁer obtained jointly optimal classiﬁcation performance. supervised formulation task-driven provides learned features hyperspectral data well suited classiﬁcation task. moreover proposed algorithm uses joint sparsity prior enforce collaboration among neighboring pixels. simulation results illustrate efﬁciency proposed dictionary learning algorithm. hyperspectral imagery increasingly become popular remote sensing applications target detection material identiﬁcation among several algorithms used classiﬁcation shown sparse representation classiﬁcation achieve superior results purpose dictionary usually constructed collecting training samples i.e. labeled pixels underlying assumption test pixel approximated dictionary atoms i.e. test pixel lies low-dimensional subspace formed training samples label test pixel. however sparse coefﬁcients generated become unstable high coherency dictionary atoms situation alleviated enforcing similarity sparse codes neighboring pixels usually similar spectral features appropriate structured sparsity prior particular joint sparsity prior assumes neighboring pixels low-dimensional recently shown learning dictionary rather constructing using training samples signiﬁcantly improve performance sparse representation-based algorithms reconstructive discriminative tasks dictionary learning algorithms generally categorized groups unsupervised supervised methods. unsupervised dictionary learning aimed ﬁnding dictionary yields minimum errors reconstruction tasks deniosing supervised dictionary learning algorithms utilize labels minimizing misclassiﬁcation cost recently shown task-driven formulation achieve state-ofthe-art performance several classiﬁcation tasks jointly learning dictionary classiﬁer similar machine learning methods kernelized sparse representation algorithms input higher-dimensional feature space using kernel function result signiﬁcant performance improvements compared linear counterpart rational data different classes projected kernel induced feature space classes become separable samples classes typically cluster together subspaces resulting discriminative sparse codes. purpose kernelized dictionary learning algorithms proposed unsupervised learning proposed kernelizing well-known k-svd algorithm object recognition. supervised formulation proposed based hilbert schmidt independence criterion maximize dependency data corresponding class labels. however classiﬁcation task preference utilize labeled data minimize misclassiﬁcation cost paper kernelized task-driven dictionary learning algorithm proposed dictionary trained optimal classiﬁcation. proposed algorithm generalizes task-driven formulation important ways. first enforces correlation among neighboring pixels using joint sparsity prior. second generalizes algorithm providing kernelized formulation. proposed dictionary learning obtained solving bi-level optimization problem shows that underlining joint sparse coding non-smooth bi-level optimization cost differentiable. simulation results demonstrate proposed algorithm achieve state-of-the-art performance classiﬁcation tasks. dictionary learning widely used various tasks reconstruction classiﬁcation compressive sensing rn×n collection training pixels number spectral bands. unsupervised formulation dictionary rn×d usually obtained minimizer following cost kernel methods usually used project data higher dimensional feature space make different classes become linearly separable. mapping feature space possibly inﬁnitedimensional. assumed hilbert space allows mercer kernels carry projection implicitly. mercer kernel function deﬁned inner product operator among commonly used kernel functions gaussian kernel polynomial kernel kx−xk kernel parameters. columns note kφ−φαk k−αt explicit mapping feature space required solve optimization problem. discussed previous section neighboring pixels usually similar spectral features robust sparse codes obtained jointly reconstructed neighboring pixels centered denoted {xs} paper. joint sparsity enforces neighboring pixels represented subspace optimal sparse coefﬁcients obtained solving following optimization problem sparse code pixel kaj→k aj→’s rows optimization problem encourages sparsity therefore neighboring pixels enforced jointly reconstructed sparse code pattern section extends task-driven dictionary learning algorithm using joint sparsity prior enforces collaboration among neighboring pixels. moreover extend algorithm kernel domain provides general framework task-driven dictionary learning using arbitrary kernel functions. notations previous section without loss generality input signal consist neighboring pixels {xs} centered label optimal value sparse coding problem regularizing parameters. assumed data drawn ﬁnite probability distribution usually unknown. stationary point optimization problem efﬁciently obtained online optimization algorithm trained dictionary used reconstruct inputs reconstruction error usually robust measure classiﬁcation tasks trained dictionary feature learning sparse code obtained solution used input feature training classiﬁer classical expected risk optimization framework however shown discriminative features generally obtained learning dictionary classiﬁer jointly following task-driven formulation binary vector representing ground truth label input c-class classiﬁcation problem convex loss function measures well predict given feature model parameters regularizing parameter. paper quadratic loss used deﬁned algorithm stochastic gradient descent algorithm kernelized task-driven dictionary learning joint sparisty prior input kernel function neighborhood size regularization parameters learning rate parameters number iterations initial dictionary initial model parameter performance proposed classiﬁcation algorithm evaluated indian pine image generated airborne visible/infrared imaging spectrometer university pavia image. indian pine image contains classes spread pixels pixel bands ranging .µm. bands corresponding water absorption removed processing image. similar setup randomly select pixels form training rest pixels used testing. university pavia image urban image spectral bands ranging .µm. contains classes spread pixels. noisiest bands removed. dataset standard training test split used training consists ﬁrst column minimizer optimization problem sparse code center pixel deﬁned noted chosen quadratic loss simplicity formulation easily extended convex cost functions used expectation taken respect joint probability distribution inputs {xs} label main difﬁculty optimizing nondifferentiability however shown sparse coefﬁcients differentiable almost everywhere. prove that optimality condition shown active locally constant small perturbation {xs} therefore locally differentiated. moreover similar procedure shown points active changes measure gradients computed using chain rule. detailed proof involved omitted space limitation. algorithm optimal dictionary model parameter classiﬁcation described algorithm special case linear kernel chosen proposed algorithm reduces task-driven dictionary learning algorithm theory needs select strictly positive guarantees linear equation algorithm unique solution. words easy show matrix algorithm positive deﬁnite given however practice observed setting zero yields satisfactory results. nonconvex optimization problem algorithm initialized properly yield poor performance. paper used unsupervised dictionary learning stochastic gradient descent initialize dictionary initialized initial value solving respect convex optimization problem. pixels rest pixels used testing. dictionary learning algorithms size dictionary chosen atoms class. regularization parameters gaussian kernel parameter selected using cross-validation sets respectively zero. learning parameters selected similar procedure outlined performance proposed kernelized dictionary learning algorithm compared linear task-driven dictionary learning algorithm proposed purpose report results proposed algorithm using three different settings named sdl-ℓ-k sdl-ℓ-l sdl-ℓ-k. sdl-ℓ-k extension sdl-ℓ-l kernel domain. sdl-ℓ-l enforcing collaboration neighboring pixel using joint sparsity linear domain. finally sdlℓ-k setting neighboring pixels jointly reconstructed kernel domain. also evaluate performance proposed algorithm linear kernel namely svm-l svm-k respectively well sparse-based representation classiﬁcation algorithms. latter training samples used construct dictionary results reported using priors linear kernel domains named src-ℓ-l src-ℓ-k src-ℓ-l src-ℓ-k accordingly. classiﬁcation results indian pine university pavia hyperspectral images shown table table respectively. expected kernelized formulations usually achieve better classiﬁcation performance. moreover consistently observed using joint sparsity prior enforce collaboration among neighboring pixels improves performance. proposed sdl-ℓ-k achieves best performance competitive algorithms datasets. comparing performances dictionary-learning based algorithms dictionary constructed collecting training samples also note difference dictionary sizes. proposed task-driven formulations achieve better performances compact dictionaries translates computationally efﬁcient processing test samples. paper kernelized task-driven dictionary learning algorithm proposed supervised classiﬁcation. proposed formulation enjoys joint sparsity prior enforces collaboration among neighboring pixels robust sparse representation. shown proposed algorithm equipped compact dictionary achieves state-of-the-art performances classiﬁcation indian pine university pavia hyperspectral images. proposed formulation provides general framework nonlinear supervised dictionary learning readily applied classiﬁcation tasks. future research topics includes extension proposed algorithm include structured sparsity priors testing different classiﬁcation tasks. camps-valls tuia bruzzone benediktsson advances hyperspectral image classiﬁcation earth monitoring statistical learning methods ieee signal process. mag. vol. jan. j.m. bioucas-dias plaza semisupervised hyperspectral image segmentation using multinomial logistic regression active learning ieee trans. geosci. remote sens. vol. nov. chen n.m. nasrabadi t.d. tran hyperspectral image classiﬁcation using dictionary-based sparse representation ieee trans. geosci. remote sens. vol. oct. mousavi srinivas u.and monga tran multi-task image classiﬁcation collaborative hierarchical spike-and-slab priors ieee intl. conf. image process. n.m. nasrabadi t.d. tran structured priors sparse-representation-based hyperspectral image classiﬁcation ieee geosci. remote sens. lett. vol. jul. bahrampour nasrabadi jenkins quality-based multimodal classiﬁcation using tree-structured sparsity proc. ieee conf. comput. vision pattern recognition srinivas mousavi jeon monga hattel jayarao simultaneous sparsity model histopathological image representation classiﬁcation ieee trans. med. imag. vol.", "year": 2015}