{"title": "An Entropy-based Learning Algorithm of Bayesian Conditional Trees", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "This article offers a modification of Chow and Liu's learning algorithm in the context of handwritten digit recognition. The modified algorithm directs the user to group digits into several classes consisting of digits that are hard to distinguish and then constructing an optimal conditional tree representation for each class of digits instead of for each single digit as done by Chow and Liu (1968). Advantages and extensions of the new method are discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which offer a different entropy-based learning algorithm are shown to rest on inappropriate assumptions.", "text": "article liu's learning handwritten fied algorithm directs several classes consisting hard distinguish optimal tion class digits single method discussed. wong wang offer different learning algorithm propriate information measure found example chow liu's algorithm efficient; dence tree algorithm identifies weights. appropriate neous mutual information much work recently bitrary verma investigations purpose network represents measurements. article digit recognition handwrit bayesian networks specific conditional ficient antee similar algorithm small cutset orates accurate given possible distribution measurements. requires exponential infeasible likely binatorial explosion conditionally experiments recognition rate obtained tional independence notice eral dependence single collection multinet apparently ditional tree represent sumptions encoded different article learning algorithm. user group digits hard distinguish digits structing optimal class digits done chow and.liu. imum weight spanning weight found conditional mutual information modification main digits class similar tree constructed using features judged helpful tinctions digits limited-sized samples help qualitative knowledge prototypical bines entropy-based heckerman scheme contribution another made herein relationship error rate divergence lated works wong wang poon offer different learning algorithm assumptions. although assumed definition works. details representable tional tree hand notation equivalently sentable conditional network node node removed work becomes probability called components point section optimization lows. find probability form dictated learning algorithm conditional mutual conditioned selects tree according previous section. obviously joint sample space small enough known advance nodes. rebane pearl extend chow liu's gorithm tions polytree-asomorph. extended small cutset algorithm formation mutual information procedure feasible section lows build single building liu. justification duction obtained single quite small probabilities remains lesser penalty quite high different gether yields ample phenomena poon genuine provided larity networks text medical rally many pattern servation recognition tinguish ratio distinguish help distinguish advantageous equality forced although finite sample noise. qualitative space therefore consider larger example ation network five values similarity uses cover cs}. variable measurement helps distingui vari­ able variable guish among note many parameters ions explicitly joint distribu given. example qual local network similarity works conditional constructed local networks previous section weights first step heuristics connected cover select appropriate selection defined dinality uses algorithm previous modifications. tion beckerman's prevent loops step best explained network bayesian provides heckerman main benefit algorithm small fraction tinguish ·logf> logf>. shows max­ imi�ing equivalent shows upper bound bayes error rate. wong wang's optimization sible computational tional propriate. stated minimizes consequently lent minimizing approximate components becomes sharper measurements' vector product mization vergence) method minimizing practical reducing clear substitute attain optimization mutual information additive form. complexity wong wang introduced wong poon chow liu's algorithm derived minimizing instead minimizing vergence however chow liu's criteria minimizing actually equivalent different minimizing seen closer look wong wang's result reveals assume remains selections constant teria indeed identical learning algorithm presented combines entropy-based networks. algorithm looks promising domains pattern recognition structed manually. critical provides insight relationship plexity various entropy-based", "year": 2013}