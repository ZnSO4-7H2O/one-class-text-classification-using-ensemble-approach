{"title": "Bi-directional Attention with Agreement for Dependency Parsing", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.", "text": "develop novel bi-directional attention model dependency parsing learns agree headword predictions forward backward parsing directions. parsing procedure direction formulated sequentially querying memory component stores continuous headword embeddings. proposed parser makes soft headword embeddings allowing model implicitly capture high-order parsing history without dramatically increasing computational complexity. conduct experiments english chinese languages conll shared task showing proposed model achieves state-of-the-art unlabeled attachment scores languages. recently several neural network models developed efﬁciently accessing long-term memory discovering dependencies sequential data. memory network framework studied context question answering language modeling whereas neural attention model encoder-decoder framework applied machine translation constituency parsing frameworks learn latent alignment source target sequences mechanism attention encoder viewed soft operation memory. although already used encoder capturing global context information bi-directional recurrent neural network employed decoder. bi-directional decoding expected advantageous previously developed uni-directional counterpart former exploits richer contextual information. intuitively separate uni-directional rnns constructs respective attended encoder context vectors computing hidden states. however drawback approach decoder would often produce different alignments resulting discrepancies forward backward directions. paper design training objective function enforce attention agreement directions inspired alignmentby-agreement idea liang specifically develop dependency parser using bi-directional attention model based memory network. given golden alignment observed dependency parsing training stage derive simple interpretable approximation agreement objective makes natural connection latent observed alignment cases. proposed biatt-dp parses sentence linear order sequentially querying memory component stores continuous embeddings headwords. words consider possible arcs parsing. formulation adopted graph-based parsers mstparser consideration possible arcs makes proposed biatt-dp different many recently developed neural dependency parsers transitionbased algorithm modeling parsing procedure sequence actions buffers. moreover unlike graph-based parsers suffer high computational complexity utilizing high-order parsing history proposed biatt-dp implicitly inject information model keeping computational complexity order sentence words. achieved feeding query component soft headword embedding computed probability-weighted headword embeddings memory component. best knowledge ﬁrst attempt apply memory network models graphbased dependency parsing. moreover ﬁrst extension neural attention models unidirection multi-direction enforcing agreement alignments. experiments english chinese languages conll shared task show biatt-dp achieve competitive parsing accuracy several state-of-the-art parsers. furthermore model achieves highest unlabeled attachment score chinese czech dutch german spanish turkish. proposed parser ﬁrst encodes word sentence continuous embeddings using bidirectional performs types operations i.e. headword predictions based bidirectional parsing history relation prediction conditioned current modiﬁer predicted headword embedding space. following ﬁrst present token embeddings constructed. then components proposed parser i.e. memory component query component discussed detail. lastly describe parsing algorithm using bidirectional attention model agreement. token embeddings proposed biatt-dp memory query components share token embeddings. notion additive token embedding utilize available information token e.g. word form lemma part-of-speech morphological features. speciﬁcally token embedding computed ei’s one-hot encoding vectors word parameters learned store continuous embeddings corresponding feature. note one-hot encoding vectors different dimensions depending individual vocabulary sizes ﬁrst dimension different second dimension. additive token embeddings allow easily integrate variety information. moreover need make single decision dimensionality token embedding rather combination decisions word embeddings embeddings concatenated token embeddings used chen manning dyer weiss reduces number model parameters tuned especially lots different features used. experiments word form ﬁne-grained always used whereas features used depending availability dataset. singleton words lemmas tags replaced special tokens. projection matrix shared memory query components well. activation function projection layer leaky rectiﬁed linear function slope negative part. remaining part paper refer token embedding word position note subscript substituted memory query components respectively. shown figure proposed biatt-dp three components i.e. memory component leftto-right query component right-to-left query component. given sentence length parser ﬁrst uses bi-directional construct headword embeddings reserved root symbol. query component uni-directional attention model. query component sequence modiﬁer embeddings constructed recursively conditioning headword embeddings. address vanishing gradient issue rnns gated recurrent unit proposed update gate reset gate employed control information ﬂow. replace hyperbolic tangent function lrel function faster compute achieves better parsing accuracy preliminary studies. following refer headword modiﬁer embeddings memory query vectors respectively. memory component proposed biatt-dp uses bi-directional obtain memory vectors. time step current hidden state vecj re/) computed non-linear transformation based current input vector previous hidden state vecj+) i.e. ideally recursive nature allows capture context information one-side bi-directional thus capture context information sides. concatenate hidden layers left-to-right right-to-left word posi. tion memory vector memory vectors expected encode words context information headword space. query component query component single-directional obtain query vectors qj’s hidden state vectors rnn. used query memory component returning association scores stj’s word position headfigure structure biatt-dp. ﬁgure illustrates parsing process time step has. blue yellow circles memory query vectors respectively. purple circles represent headword probabilities predicted corresponding query components. green circles represent soft headword embeddings. black arrowed lines connections carrying weight matrices. indicate element-wise multiplication addition respectively. simplicity ignore token embedding connected hidden layers element-wise hyperbolic tangent function rh×e rh×d model parameters. then obtain probabilities at··· headwords sentence normalizing stj’s using softmax function takes soft headword embedding input addition token embedmr ding formally forward case computed although able capture long-span context information extent local context easily dominate hidden state. therefore additional soft headword embedding allows model access long-span context information different channel. hand recursively feeding query vector soft headword embedding model implicitly captures high-order parsing history information potentially improve parsing accuracy however graph-based dependency parser utilizing parsing history features computationally expensive. example k-th order mstparser complexity sentence words. contrast biatt-dp implicitly captures high-order parsing history keeping complexity order i.e. direction. compute pair-wise probabilities paper choose soft headword embeddings rather making hard decisions headwords. latter case beam search potentially improve parsing accuracy cost higher computational complexity i.e. beam width using soft headword embeddings need perform beam search. moreover straightforward incorporate parsing history directions using query components cost cannot easily achieved using beam search. parsing decision made directly based attention weights query components further rescored maximum spanning tree search algorithm. parsing attention agreement bi-directional attention model underlying probability distributions agree order encourage agreeother. ment mathematically convenient metric kl-divergence. complete derivation provided appendix optimization safely drop constant scaler square root operation upper bound leading following loss function tries minimize distance golden alignment intersection directional attention alignments every time step. therefore inference headword prediction word time step obtained seeking agreement query components. parsing procedure also similar exhaustive left-to-right modiﬁer-ﬁrst search algorithm described enhanced additional right-to-left search agreement enforcement. alternatively treat score corresponding search achieved chuliu-edmonds algorithm implemented dense graphs according tarjan practice search slows parsing speed however forces parser produce valid tree observe slight improvement parsing accuracy cases. learning rate halved iteration loglikelihood decreases. whole training procedure terminates log-likelihood decreases second time. learning parameters except bias terms initialized randomly according gaussian distribution experiments tune initial learning rate step size choose best based log-likelihood ﬁrst epoch. empirically selected initial learning rates fall range hidden layer size tend larger using smaller hidden layer size i.e. hidden layer size around training data randomly shufﬂed every epoch. section present parsing accuracy proposed biatt-dp languages. report labeled attachment score obtained conll-x eval.pl script ignores punctuation symbols. headword predictions made search slightly improves overall proposed biatt-dp achieves competitive parsing accuracy languages state-of-the-art parsers obtains better languages. also show impact using tags pre-trained word embeddings. moreover different variants full model compared section. data work english treebank- dataset chinese treebank-. dataset languages conll shared task datasets exactly setup speciﬁcally convert english chinese data using stanford parser pennmalt tool respectively. t-th word sentence length denote random variables representing predicted headword forward backward parsing directions respectively. also denote random variable representing dependency relation joint probability headword relation predictions written n|wn) time step assume head-modiﬁer relations headwords directions independent conditioned global knowledge whole sentence. note long-span context high-order parsing history information injected model |wn) disp observe equivalent maximizing log-likelihood golden dependency tree training sentence i.e. deﬁned respectively relationt headt golden relation headword labels respectively. gradients computed back-propagation algorithm errors come labels whereas source errors headword labels back-propagated errors stochastic gradient descent adam algorithm proposed whereas chinese gold segmentation tags. constructing token embeddings english chinese word form used. also initialize eform pretrained word embeddings. languages randomly hold training data set. addition word form ﬁnd-grained tags extra features lemmas coarse-grained tags morphemes available dataset. pre-trained word embeddings used languages. model conﬁgurations hidden layer size kept across rnns proposed biatt-dp. also require dimension token embeddings hidden layer size. note concatenate hidden layers rnns constructing thus weight matrices respectively project vectors dimension equivalent english chinese since dimension pretrained word embeddings dimension embedding parameters e’s. languages square matrices embedding parameters e’s. languages tune hidden layer size choose according set. selected hidden layer sizes languages results ﬁrst compare parser state-of-the-art neural transition-based dependency parsers ctb. english also compare stateof-the-art graph-based dependency parsers. results shown table table respectively. seen biatt-dp outperforms graph-based parsers ptb. compared transition-based parsers achieves better accuracy chen manning uses feed-forward neural network dyer uses three stack lstm networks. compared integrated parsing tagging models biatt-dp outperforms bohnet nivre small alberti achieves best similar las. caused relation vocabulary size relatively smaller average sentence length biases joint objective sensitive uas. parsing speed around sents/sec measured desktop intel core .ghz using single thread. next table show parsing accuracy proposed biatt-dp languages conll shared task including comparison state-of-the-art parsers. speciﬁcally show rd-order rbgparser reported since also uses low-dimensional continuous embeddings. however several major differences rbgparser biatt-dp. first lowdimensional continuous embeddings derived table languages conll shared task also report corresponding squared brackets. results rd-order rbgparser reported best published results dataset terms among study effectiveness parser dealing non-projectivity follow compute recall crossed uncrossed arcs gold tree well percentage crossed arcs. low-rank tensors. second rbgparser uses combined scoring arcs including traditional features mstparser turboparser third rbgparser employs third-order parsing algorithm based although also implements ﬁrst-order parsing algorithm achieves lower general. table show proposed biatt-dp outperforms rbgparser languages except japanese slovene swedish. observed table biattdp highly competitive parsing accuracy stateof-the-art parsers. moreover achieves best languages. remaining seven languages gaps biatt-dp state-of-the-art parsers within except swedish. arguably fair comparison biattdp mstparser since biatt-dp replaces scoring function arcs uses exactly search algorithm. space limit refer readers results mstparsers biatt-dp consistently outperforms parser absolute score. uses graph-based non-projective parsing algorithm interesting evaluate performance crossed arcs result non-projectivity dependency tree. last three columns table show recall crossed arcs uncrossed arcs percentage crossed arcs test set. pitler mcdonald reported numbers data dutch german portuguese slovene paper. four languages biatt-dp achieves better reported importantly observe improvement recall crossed arcs much signiﬁcant uncrossed arcs indicates effectiveness biatt-dp parsing languages non-projective trees. study impact using pre-trained word embeddings tags well bidirectional query components model. first start best model english uses token embedding dimension hidden layer size. keep model parameter dimensions unchanged analyze different factors comparing parsing accuracy set. table parsing accuracy different variants full model. init refers using pre-trained word embddings initialize eform. refers using tags token embeddings. respectively indicate whether left-to-right right-to-left query components. means query component drops soft headword embeddings constructing hidden states. results summarized table comparing models observed without using pre-trained word embeddings drop without using tags token embeddings numbers drop around las. terms query components using single query component degrades .–.% around compared model model soft headword embedding used label predictions next hidden state around worse model supports hypothesis usefulness parsing history information. also implement variant model produces instead using gets indicating naively applying bi-directional enough. neural dependency parsing recently developed neural dependency parsers mostly transition-based models read words sequentially buffer stack incrementally build parse tree predicting sequence transitions feed-forward neural network used represent current state selected elements words stack buffer. element encoded concatenated embeddings words tags labels. dependency parser achieves improvement accuracy parsing speed. weiss improve parser using semi-supervised structured learning unlabeled data. model extended integrate parsing tagging hand dyer develop stack lstm architecture uses three lstms respectively model sequences buffer states stack states actions. unlike transition-based formulation proposed biatt-dp directly predicts headword dependency relation time step. speciﬁcally explicit representation actions headwords model. model learns retrieve relevant information input memory make decisions headwords head-modiﬁer relations. graph-based dependency parsing addition transition-based parsers another line research dependency parsing uses graph-based models. graph-based parser usually build dependency tree directed graph learns scoring possible arcs. nature nonprojective parsing done straightforwardly graph-based dependency parsers. mstparser turboparser examples graphbased parsers. mstparser formulates parsing searching whereas turboparser performs approximate variational inference factor graph. rbgparser proposed also viewed graph-based parser scores arcs using low-dimensional continuous features derived low-rank tensors well features used mstparser/turboparser. also employs sampler-based algorithm parsing neural attention model proposed biattdp closely related memory network question answering well neural attention models machine translation constituency parsing query memory component obtain soft headword embeddings essentially attention mechanism. however different studies alignment information latent dependency parsing modiﬁer table scores st-order order mstparsers languages conll shared task numbers reported numbers brackets indicate absolute improvement proposed biatt-dp mstparsers. references chris alberti david weiss slav petrov slav petrov. improved transition-based parsing tagging neural networks. proc. conf. empirical methods natural language process. pages daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov michael collins. globally normalized transition-based neural networks. proc. annu. meeting assoc. computational linguistics dzmitry bahdanau kyunghyun yoshua ben. neural machine translation jointly proc. int. conf. miguel ballesteros chris dyer noah smith. improved transition-based parsing model. proc. characters instead words lstms. conf. empirical methods natural language process. pages headword known training. thus utilize labels attention weights. similar idea employed pointer network used solve three different combinatorial optimization problems. paper develop bi-directional attention model encouraging agreement latent attention alignments. simple interpretable approximation make connection latent observed alignments training model. apply bi-directional attention model incorporating agreement objective training proposed memory-network-based dependency parser. resulting parser able implicitly capture high-order parsing history withsuffering issue high computational complexity graph-based dependency parsing. carried empirical studies languages. parsing accuracy proposed model highly competitive state-of-the-art dependency parsers. english proposed biattdp outperforms graph-based parsers. also achieves state-of-the-art performance languages terms demonstrating effectiveness proposed mechanism bi-directional attention agreement dependency parsing. bernd bohnet joakim nivre. transitionbased system joint part-of-speech tagging laproc. beled non-projective dependency parsing. conf. empirical methods natural language process. pages kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahadanau fethhi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statisproc. conf. empirical tical machine translation. methods natural language process. pages marie-catherine marneffe bill maccartney christopher manning. generating typed dependency parses phrase structure parses. proc. int. conf. language resources evaluation chris dyer miguel ballesteros wang ling austin matthews noah smith. transitionbased dependency parsing stack long short-term memory. proc. annu. meeting assoc. computational linguistics pages terry alexander rush michael collins tommi jaakkola david sontag. dual decomposition parsing non-projective head automata. proc. conf. empirical methods natural language process. pages yuan zhang regina barzilay tommi jaakkola. low-rank tensors scorproc. annu. meeting dependency structures. assoc. computational linguistics pages percy liang tasker klein. alignproc. human language ment agreement. technology conf. conf. north american chapter assoc. computational linguistics pages andr`e martins noah smith eric xing. turbo parsers dependency parsing approximate variational inference. proc. conf. empirical methods natural language process. pages andr`e martins miguel almeida noah smith. turing turbo fast third-order proc. annu. meetnon-projective turbo parsers. assoc. computational linguistics pages ryan mcdonald fernando pererira kiril ribarov hajiˇc. non-projective dependency parsing using spanning tree algorithms. proc. human language technology conf. conf. empirical methods natural language process. pages tomas mikolov chen greg corrado jeffrey dean. efﬁcient estimation word representations vector space. proc. workshop int. conf. learning representations. zhang ryan mcdonald. generalized higher-order dependency parsing cube pruning. proc. conf. empirical methods natural language process. computational natural language learning pages zhang ryan mcdonald. enforcing structural diversity cube-pruned dependency parsing. proc. annu. meeting assoc. computational linguistics pages zhang liang huang zhao ryan mcdonald. online learning inexact hypergraph proc. conf. empirical methods natural search. language process. pages yuan zhang regina barzilay tommi jaakkola amir golberson. steps excellence simple inference reﬁned scoring dependency proc. annu. meeting assoc. computatrees. tional linguistics pages emily pitler ryan mcdonald. linear-time translation system crossing interval trees. proc. conf. north american chapter assoc. computational linguistics pages alexander rush slav petrov. vine pruning efﬁcient multi-pass dependency parsing. proc. conf. north american chapter assoc. computational linguistics human language technologies pages kristina toutanova klein christopher manning yoram singer. feature-rich part-of-speech proc. tagging cyclic dependency network. human language technology conf. conf. north american chapter assoc. computational linguistics pages oriol vinyals lukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. proc. annu. conf. neural inform. process. syst. pages david weiss chris alberti michael collins slav petrov. structured training neural network transition-based parsing. proc. annu. meeting assoc. computational linguistics pages", "year": 2016}