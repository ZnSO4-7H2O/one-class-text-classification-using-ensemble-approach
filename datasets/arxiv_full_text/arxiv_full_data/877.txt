{"title": "Efficient Representation of Low-Dimensional Manifolds using Deep  Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. We first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. We then extend these results to more general manifolds.", "text": "figure illustrate embedding manifold deep network using famous swiss roll example dots represent color coded input data. center data divided three parts using hidden units represented yellow cyan planes. part approximated monotonic chain linear segments. additional hidden units also depicted planes control orientation next segments chain. second layer network ﬂattens chain euclidean plane assembles common representation dimensional euclidean space. fact linear segment approximating manifold represented single additional hidden unit leading representation manifold data cases nearly optimal consider ability deep neural networks represent data lies near low-dimensional manifold high-dimensional space. show deep networks efﬁciently extract intrinsic low-dimensional coordinates data. ﬁrst show ﬁrst layers deep network exactly embed points lying monotonic chain special type piecewise linear manifold mapping lowdimensional euclidean space. remarkably network using almost optimal number parameters. also show network projects nearby points onto manifold embeds little error. extend results general manifolds. deep neural networks achieved state-of-the-art results variety tasks. remarkable success fully explained possibility hierarchical layered structure allow capture geometric regularities commonplace data. support hypothesis exploring ways networks handle input data near low-dimenisonal manifold. many problems example face recognition data near manifolds much lower dimension input space represent intrinsic degrees variation data. study representational power deep networks applied manifold data. demonstrate initial layers networks take inputs manifold high-dimensional space approximate manifold piecewise linear functions economically output coordinates points embedded lownumber parameters system. means subsequent layers deep network could build upon early layers operating lower dimensional spaces naturally represent input data. beyond scope paper study problem training networks build representations. however results describe novel representations might sought existing networks might suggest architectures networks. moreover feel results provide intuitions role individual units network play shaping function computing. ﬁrst show embedding done efﬁciently manifolds consisting monotonic chains linear segments. show primitives combined form linear approximations complex manifolds. process illustrated figure further show data lies sufﬁciently close linear approximation error embedding small. constructions feed-forward network rectiﬁed linear unit activation. consider fully connected layers although treatment complex manifolds divided pieces modular resulting many zero weights. realistic learning problems e.g. vision applications speech processing involve high dimensional data. data often governed many fewer variables producing manifold-like sub-structures high dimensional ambient space. large number dimensionality reduction techniques principle component analysis multi-dimensional scaling isomap local linear embedding introduced. underlying manifold assumption states different classes separate manifolds also guided design clustering semi-supervised learning algorithms number recent papers examine properties neural nets light manifold assumption. speciﬁcally rifai trained contractive auto-encoder represent atlas manifold charts. shaham demonstrate -layer network efﬁciently represent function manifold trapezoidal wavelet decomposition. both chart represented independently requiring representation independent projection input space onto chart. show monotonic chains reduce size representation near optimal exploiting geometric relations neighboring projection matrices additional chart requires single hidden unit. another family networks attempt learn semantic distance metric training pairs often using siamese network assume input space mapped non-linearly network produce desired distances lower dimensional feature space. shows even feed-forward neural network random gaussian weights embeds input data output space preserving distances input items. suggest training improve embedding quality. another outstanding question extent deep networks represent data handle classiﬁcation problems efﬁciently shallow networks single hidden layer. earlier work showed shallow networks universal approximators however recent work demonstrates deep networks exponentially efﬁcient representing certain functions hand show empirically many practical cases shallow network trained mimic behavior deep network. construction produce exponential gains show early layers network efﬁciently reduce dimensionality data feeds later layers. paper construct networks perform dimensionality reduction data lies near manifold. focus feed-forward networks relu activation i.e. max. clearly output networks continuous non-negative piecewise linear functions input. therefore natural whether embed piecewise-linear manifolds low-dimensional euclidean space accurately efﬁciently. section construct efﬁcient networks class manifolds call monotonic chains afﬁne subspaces deﬁned shortly. serve building blocks handling general chains well sets data decomposed monotonic chains. handling complex cases require deeper networks. subsequent sections discuss complex manifolds show addition networks used approximate data near non-linear manifolds. output layer units. denote weights ﬁrst layer matrix bias vector second layer weights captured matrix total number weights layers layer network maps point embedding space simple example manifold represented efﬁciently neural network occurs data lies single m-dimensional afﬁne subspace embedding done case layer matrix size containing rows basis parallel afﬁne space. relu needed required bias accordingly feasible data points non-negative coordinates. simple extend example handle chains encoding linear segment separately. encoding require units addition units relu separate segment rest segments. related representation used e.g. show monotonic chains encoded much efﬁciently. next show construct network encode monotonic chains. below notation denote matrix formed ﬁrst rows vector containing ﬁrst entries matrix including ﬁrst columns therefore express output network ﬁrst hidden units used. recover intrinsic coordinates points ﬁrst segments relu ensures subsequent hidden units affect output points segments. construction consider pull-back standard basis chain producing geodesic basis manifold expressed collection column-orthogonal matrices matrix provides orthogonal basis segments. construct network inductively. suppose components non-negative. clearly orthogonal projection matrix shows network projects orthonormal basis ﬁrst segment orthonormal basis next show implies connected form chain suppose every subsequent segments intersect intersection lies -dimensional afﬁne subspace. assume chains ﬂattened represented note curve mapped curve length ﬂattened chain. next consider special case chains call monotonic show handled using networks hidden layers. deﬁnition chain afﬁne subspaces monotonic exist half-spaces bounded hyperplane contains intersection complement intuitively half-spaces divides chain connected pieces boundary linear segment. consider half-space represent hidden unit active subset regions. monotonic chain active units grows monotonically that additionally always deﬁne units active regions. distortion projection. show network extends basis throughout monotonic chain consistent way. next suppose used m+k− units construct ﬁrst segments. etc.) construct adding node ﬁrst hidden layer. weights incoming edges node encoded appending vector scalar weights outgoing edges encoded appending column vector assign values vectors scalar extend embedding induction assume embedded distortion constructed segments embedded consistent orientations. show also translated properly create continuous embedding. consider point denote projection onto scalar denoting embedded coordinates monotonicity assume dimensional exists hyperplane normal contains intersection lying completely side direction lies opposite side point determine ﬁrst rotate bases common matrix i.e. providing orthogonal basis parallel note induction assumption next note finally note proposed representation monotonic chains neural network efﬁcient uses parameters beyond degrees freedom needed deﬁne chains. particular deﬁnition chain requires specifying basis vectors linear segment parameters) additional segment speciﬁed direction segment direction previous segment replaced total number degrees freedom chain therefore minimum possible number parameters required specify monotonic chain. construction requires parameters. speciﬁcally note choice parameters figure black show monotonic chain three segments. show three hidden units ﬂatten chain line. note hidden unit corresponds hyperplane separates segments connected components. third hyperplane must almost parallel third segment. leads large errors noisy points near −\u0001/n separator goes easily veriﬁed setup points ﬁrst segment mapped points second segment mapped points third segment mapped ideally would want embedded point clearly baδ. readily veriﬁed that conditions therefore error embedding error embedding small bounded error embedding huge since −r/r n/\u0001. next section show happen large angle segment normal previous separating hyperplane. show noise often quite limited consider class monotonic chains total curvature segments less equal angle denote angle θk−. before drop subscript write speciﬁcally deﬁne deﬁning similarly express conconsider points exactly monotonic chain. expect happen noise approximating non-linear manifold piece-wise linear segments. point segment perturbed small noise vector perpendicular produce point ideally network would represent using coordinates effect network would project points onto monotonic chain. analyze error occur projection. analysis assumes small enough region; side hyperplanes deﬁned hidden units. ﬁrst show section arbitrary monotonic chain error unbounded. sounds show section happen hyperplanes separate monotonic chain segments must poorly chosen sense. show many reasonable cases error bounded times small constant. show error unbounded consider simple case piecewise linear manifold consists three connected line segments vertices respectively large small since three segments compose manifold three hidden units deﬁning three hyperplanes needed represent manifold. addition single output unit results units produce geodesic distance origin point three segments. saying bound holds mean able choose hyperplanes divide chain segments angle normal hyperplane following segment big. next bound error terms last section. deﬁne embedding error many monotonic chains divided segments using hyperplanes manifolds point perturbed away manifold coordinates changed magnitude perturbation times small constant factor. example note rather beginning start monotonic chain could begin middle work out. provide orthonormal basis middle segment hidden units represent chain central segment toward either ends chain. reduce total curvature starting point either half. emphasize bound tight. example bound single afﬁne segment since case network encodes orthogonal projection matrix actual error zero. handle non-monotonic chains general piecewise linear manifolds ﬂattened show network divide manifold monotonic chains embed separately stitch embeddings together. suppose wish ﬂatten non-monotonic chain divided monotonic chains ...ml. denote matrices bias used represent hidden units ﬂatten segments. suppose hyperplanes found separate chains. denote matrix rows represent normals hyperplanes oriented point away concatenate vertically letting next −nm×jl m×jl denotes matrix containing ones large constant. note rows. deﬁne matrices concatenated horizontally. lies contain coordinates embedded before. lies different monotonic chain vector small negative numbers. applying relu therefore eliminate numbers. therefore represent module consisting layer network embeds monotonic chain producing zero chains. stitch values together. first must rotate translate embedded chain chain picks previous left off. denote rotation chain denote appropriate translation. then concatenate chains produce ﬁnal network. vertical concatenation respectively block-diagonal concatenation application produce vector entries entries give embedded coordinates rest entries zero. construct third layer network stitch monotonic chains together. denote matrix size m×ml obtained concatenating horizontally identity matrices size describe output network equation note example ﬁrst element ﬁrst coordinates produced module ﬁrst layers. modules produces appropriate coordinates points monotonic chain producing points monotoinic chains. note size network depends many regions required many hyperplanes region needs separate rest manifold worst case quite large. consider example manifold polyline passes every point integer coordinates separate portion polyline rest require regions unbounded however manifolds somewhat pathological. expect many manifolds divided appropriately using many fewer hyperplanes. show example swiss roll real world manifold faces. progressively approximates data using linear subspaces decreasing dimension. ﬁrst divide data segments dimensional subspace whose dimension higher intrinsic dimension data. subdivide segment subsegments lower dimension using similar construction deeper layers network. subsegments represent original data subdivided additional layers ultimately produce subsegments represent data. ﬁrst illustrate hierarchical approach simple example requires extra layer hierarchy. consider monotonic chain m-dimensional linear segments collectively m-dimensional linear subspace d-dimensional space construct ﬁrst hidden layer units active entire monotonic chain gradient directions form orthonormal basis output layer contain coordinates points monotonic chain. form input layers ﬂatten chain described section section already shown ﬂatten manifold layers take input directly input space. accomplish extra layer. however construction using layers also fewer parameters. construction section required parameters. construction require parameters. note increases number parameters used ﬁrst construction increases proportion second construction parameters increase proportion consequently second construction much economical large small. much could represent manifold using hierarchy chains. ﬁrst layers mdimensional chain linear m-dimensional output space. next layers select m-dimensional chain lies m-dimensional space mdimensional space. process repeat indeﬁnitely whether economical depend structure manifold. section provide examples deep networks illustrate potential performance type networks described paper. examples. first synthetically generate points swiss roll. know analytically manifold ﬂattened euclidean space. second make images rendered face model changing figure plots show error ﬂattening swiss roll. relative error constant every segment starting zero monotonic chain increasing segment. absolute error behaves similarly vanishes points segment swiss roll linear approximation coincide. focus paper representational capacity networks attempt learn networks rather construct hand. make prior knowledge intrinsic coordinates image divide images segments. linear subspaces segment constructions paper build corresponding neural network input points euclidean space. swiss roll shown figure hidden units corresponding hyperplanes divide roll three monotonic chains. divide chain segments obtaining total segments. figure shows points input network representation network outputs. points color coded allow reader identify corresponding points. figure plot absolute relative error embedding every point swiss roll linear approximation used network. swiss roll unrolled almost perfectly. fact despite relatively large angular extent monotonic chain relative error exceed mean relative error next construct network ﬂatten images faces. render faces azimuth ranging degrees elevation ranging degrees. figure output network approximates images face using monotonic chain. represents image. coded size indicate elevation color indicate azimuth. four dots display corresponding face images. known viewing parameters divide seven segments construct network. described section begin orthonormal basis middle segment chain attach additional segments ends segment. results shown figure output form perfect grid part elevation azimuth need provide orthonormal basis manifold. however structure variables describe input well-preserved output. direct technical contribution work show deep networks represent data lies lowdimensional manifold great efﬁciency. particular using monotonic chain approximate component data addition single neural unit produce linear segment approximate region data. suggests deep networks effective devices dimensionality reduction. also suggest architectures deep networks encourage type dimensionality reduction. also feel work makes larger point nature deep networks. shown deep network divide input space large number regions network computes piecewise linear functions. indeed number regions exponential number parameters network. suggests source great power also suggests strong constraints regions constructed functions computed. every pair neighboring regions compute arbitrarily different functions. work shows single unit change linear function network computes neighboring regions. demonstrate unit shape function follow manifold contains data. feel suggests interesting directions study deep networks. research based upon work supported ofﬁce director national intelligence intelligence advanced research projects activity iarpa contract views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied odni iarpa u.s. government. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. huang lang nonlinear metric learning deep convolutional neural network face veriﬁyang biometric recognition cation. volume lecture notes computer science springer kuang-chih jeffrey yang ming-hsuan kriegman david. video-based face recognition using probabilistic appearance manifolds. computer vision pattern recognition proceedings. ieee computer society conference volume ieee bianchini scarselli complexity neural network classiﬁers comparison shallow ieee transactions neural netdeep architectures. works learning systems salakhutdinov hinton learning nonlinear embedding preserving class neighbourhood structure. international conference artiﬁcial intelligence statistics", "year": 2016}