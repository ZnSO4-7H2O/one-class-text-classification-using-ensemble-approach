{"title": "Neural Network Based Nonlinear Weighted Finite Automata", "tag": ["cs.FL", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Weighted finite automata (WFA) can expressively model functions defined over strings but are inherently linear models. Given the recent successes of nonlinear models in machine learning, it is natural to wonder whether ex-tending WFA to the nonlinear setting would be beneficial. In this paper, we propose a novel model of neural network based nonlinearWFA model (NL-WFA) along with a learning algorithm. Our learning algorithm is inspired by the spectral learning algorithm for WFAand relies on a nonlinear decomposition of the so-called Hankel matrix, by means of an auto-encoder network. The expressive power of NL-WFA and the proposed learning algorithm are assessed on both synthetic and real-world data, showing that NL-WFA can lead to smaller model sizes and infer complex grammatical structures from data.", "text": "weighted ﬁnite automata expressively model functions deﬁned strings inherently linear models. given recent successes nonlinear models machine learning natural wonder whether extending nonlinear setting would beneﬁcial. paper propose novel model neural network based nonlinear model along learning algorithm. learning algorithm inspired spectral learning algorithm relies nonlinear decomposition so-called hankel matrix means auto-encoder network. expressive power nl-wfa proposed learning algorithm assessed synthetic real world data showing nl-wfa lead smaller model sizes infer complex grammatical structures data. many tasks natural language processing computational biology reinforcement learning rely estimating functions mapping sequences observations real numbers. weighted ﬁnite automata ﬁnite state machines allow succinctly represent functions. widely used many ﬁelds grammatical parsing sequence modeling prediction bioinfomatics probabilistic satisfying constraints computes probability distribution strings; expressively equivalent hidden markov models successfully applied many tasks speech recognition human activity recognition recently so-called spectral method proposed alternative based algorithms learn predictive state representations related models. compared based methods spectral method beneﬁts providing consistent estimators reducing computational complexity. although successfully applied various areas machine learning inherently linear models computation boils composition linear maps. recent positive results machine learning shown models based composing nonlinear functions expressive able capture complex structure data. example leveraging expressive power deep convolutional neural networks context reinforcement learning agents trained outperform humans atari games defeat world-class players deep convolutional networks also recently considerable breakthroughs computer vision showed ability disentangle complex structure data learning representation unfold original complex feature space representation space structure linearized. thus natural wonder extent introducing non-linearity could beneﬁcial. show advantages nonlinear models namely expressiveness ability learn rich representations brought classical computational model. paper propose nonlinear model based neural networks along learning algorithm. contrast computation nl-wfa relies successive compositions nonlinear mappings. model seen extension dynamical recognizers sense nonlinear extension deterministic ﬁnite automata quantitative setting. contrast training recurrent neural networks learning algorithm rely back-propagation time. inspired spectral learning algorithm seen two-step process ﬁrst low-rank factorization called hankel matrix leading natural embedding words low-dimensional vector space perform regression representation space recover transition matrices. similarly learning algorithm ﬁrst ﬁnds nonlinear factorization hankel matrix using auto-encoder network thus learning rich nonlinear representation strings performs nonlinear regression using feed-forward network recover transition operators representation space. related works. nl-wfa closely related computation relies composition nonlinear mappings directed sequence observations. paper explore somehow orthogonal direction recent literature trying connect models back classical computational models formal language theory. connections explored past nonquantitative setting dynamical recognizers whose inference studied e.g. ability learn classes formal languages also investigated e.g. references therein. well know predictive state representations strongly related nonlinear extension proposed deterministic controlled dynamical systems recently building upon reproducing kernel hilbert space embedding non-linearity introduced using recurrent neural networks main diﬀerences approaches learning algorithm rely back-propagation time instead investigate spectral learning method beneﬁcially extended nonlinear setting. letting axax word often shorter notation axα∞. states minimal number states minimal i.e. least states. function recognizable computed wfa. case rank number states minimal computing recognizable rank hankel matrix hankel matrix rς∗×σ∗ associated function bi-inﬁnite matrix entries words spectral learning algorithm relies following fundamental relation rank rank hankel matrix theorem rank rank. practice deals ﬁnite sub-blocks hankel matrix. given basis preﬁxes suﬃxes denote corresponding sub-block hankel matrix rp×s. among possible basis particularly interested ones rank basis complete rank rank rank. arbitrary basis deﬁne pclosure turns hankel matrix p-closed basis partitioned blocks size weighted ﬁnite automaton denote strings ﬁnite alphabet empty word. weighted ﬁnite automaton states tuple α∞{aσ}i initial ﬁnal weight vector respectively rk×k transition matrix symbol comeasy rank hankel matrix upper bounded rank α∞{aσ}i states computing admits rank factorization matrices rς∗×k rk×σ∗ deﬁned avα∞ moreover check paσs spectral learning algorithm relies model assumes transition operators linear. natural wonder whether linear assumption sometimes induces strong model bias moreover even recognizable functions introducing non-linearity could potentially reduce number states needed represent function. consider following example given α∞{aσ}i function recognizable computed denotes kronecker product. check rank rank large intuitively true dimension model using non-linearity. observations motivate introduce nonlinear notation stress function nonlinear. deﬁne nl-wfa states tuple ˜gλ{ ˜gσ}σ∈σi vector initial weights transition function termination function. nl-wfa computes function deﬁned word similarly linear case sometimes shorthand notation ˜gxt ˜gxt− ˜gx. nonlinear model seen generalization dynamical recognizers quantitative setting. easy recovers classical model restricting functions linear. course restrictions nonlinear functions imposed order control applying spectral method componentwise square root hankel matrix would recover rank learning algorithm inspired spectral learning method wfa. order give insights motivate approach ﬁrst show spectral method interpreted representation learning scheme. spectral method summarized twostages process consisting factorization step regression step ﬁrst rank factorization hankel matrix perform regression estimate transition operators {aσ}σ∈σ. first focusing factorization step observe naturally embed preﬁxes vector space mapping preﬁx corresponding hankel matrix however easy check representation highly redundant hankel matrix rank. factorization step spectral learning algorithm rank factorization seen ﬁnding dimensional representation preﬁx original hankel representation recovered using linear formalize encoder-decoder perspective deﬁning maps x>s. easily check ψs)> implies encodes information suﬃcient predict value suﬃx ψp>sv). regression step spectral algorithms consists recovering matrices satisfying paσs. encoder-decoder perspective seen recovering compositional mappings satisfying ψp>aσ follows previous discussion nonlinearity could beneﬁcially brought spectral learning algorithm ways ﬁrst using nonlinear methods perform factorization hankel matrix thus discovering potentially nonlinear embedding hankel representation second allowing compositional feature maps associated symbol nonlinear. nonlinear regression given encoder-decoder maps move recovering transition functions. recall wish compositional feature maps satisfying using encoder obtained factorization step mapping written order learn transition maps thus train neural network symbol minimize following squared error loss function want point hidden units output units network nonlinear. since network trained latent representations computed factorization network output units transition network units corresponding latent representation factorization network nature facilitate optimization process. introducing non-linearity factorization step boils ﬁnding mappings preﬁx brieﬂy going back linear case check hus+s preﬁx implying encoder-decoder maps satisfy hus+ x>s. thus factorization step essentially interpreted ﬁnding auto-encoder able project hankel representation dimensional space preserving relevant information captured extend factorization step nonlinear setting appear clearly training autoencoder learn low-dimensional representation hankel representations potentially unravel rich representation preﬁxes nl-wfa recovered. encoder decoder maps respectively. train autoencoder shown figure achieve precisely rm×n model trained original hankel representation preﬁx latent representation vector vector back original representation achieved minimizing reconstruction error instead linearly factorizing hankel matrix auto-encoder framework consisting networks whose hidden layer activation functions nonlinear. precisely denote nonlinear activation function weights matrices left right neural shown figure function computed auto-encoder written ua)>b)>c)>d also belong ﬁrst step consists building estimate rm×n hankel matrix training data rows indexed preﬁxes columns suﬃxes learning algorithm nl-wfa consists steps deﬁnitions initial vector termination function given seem ad-hoc show learning algorithm derived corresponds minimizing error loss function estimated value preﬁxes intuitively means learning algorithm aims minimizing empirical squared error loss training formally show following theorem factorization network transition networks trained optimality resulting nl-wfa exactly recovers values given ﬁrst column estimate hankel matrix. theorem preﬁx preﬁx-closed equality holds nlwfa ˜gλ{ ˜gσ}σ∈σi estimated value target function word nl-wfa function encoder-decoder maps transition maps described section even though theorem seems suggest learning algorithm prone over-ﬁtting case. indeed akin linear spectral learning algorithm restriction number states nl-wfa induces regularization enforces learning process discriminate signal noise shown non-linearity introduced steps learning algorithm. thus consider three variants algorithm either apply non-linearity factorization step only regression step only steps. easy check three diﬀerent settings correspond three diﬀerent nl-wfa models depending whether termination function nonlinear transition functions nonlinear termination transition functions nonlinear. indeed recall nl-wfa deﬁned tuple ˜gλ{ ˜gσ}σ∈σi. non-linearity intro˜ duced factorization network termination function form linear. similarly non-linearity used transition networks resulting maps linear. argue applying non-linearity termination function would lead expressive enough model. however worth noting case nonlinear factorization step even though transition functions linear operating nonlinear feature space. similar spirit kernel trick linear model learned feature space resulting nonlinear transformation initial input space. moreover back example squared function states even though rank easily build nl-wfa states computing termination function nonlinear. compare classical spectral learning algorithm three conﬁgurations neural-net based nl-wfa learning algorithms applying non-linearity factorization step regression step phases perform experiments grammatical inference task synthetic real data normalized probability assigned learned model normalized true probability since models returned method spectral learning algorithm ensured outputs positive values logarithm negative value deﬁned take absolute values negative outputs. synthetic data experiment generate data probabilistic dyck language. consider language generated following probabilistic context free grammar repeated symbol left. check distribution generate balanced strings brackets. well known distribution cannot computed however compute distribution ﬁnite support model restriction distribution word length less threshold using distribution synthetic experiments want showcase fact nl-wfa lead models better predictive accuracy number states limited better capture complex structure distribution. experiments empirical frequencies training data estimate hankel matrix p-closed basis obtained selecting frequent preﬁxes suﬃxes training data. ﬁrst assess ability nl-wfa better capture structure data number states limited. compared models diﬀerent model sizes ranging number states learned nl-wfa. latter used three hidden layers structure factorization network number hidden units transition networks neural network hidden units. used adamax learning rate respectively train networks. models trained training size pautomac score test size reported figure respectively. metrics nl-wfa gives better results small model sizes. nl-wfa tend perform similarly pautomac score larger model sizes nl-wfa clearly outperforms terms case. shows including non-linearity increase prediction power discovering underlying nonlinear structure beneﬁcial dealing small number states. compared sample complexity learning nl-wfa training diﬀerent models training sizes ranging models rank chosen cross-validation. figure figure show performances four models test size reporting average standard deviation runs experiment. nl-wfa achieve better results small sample sizes pautomac score consistently outperforms linear model sample sizes wer. shows nl-wfa training data eﬃciently expressiveness nl-wfa beneﬁcial learning task. penn treebank well known benchmark dataset natural language processing. consists approximately million words part-ofspeech tagged text million words skeletally parsed text million words text parsed predicate argument structure million words transcribed spoken text annotated speech disﬂuencies. experiment small portion treebank dataset character level english verbs used spice challenge dataset contains sentences alphabet symbols training set. also provides test sets size used test sets validation tested models other. experiment hankel matrix size preﬁxes suﬃxes selected taking frequents training data. used layers factorization network layers size respectively number states nl-wfa. structure transition networks previous experiment. models rank selected using validation set. table report results metrics test set. metrics nl-wfa models outperforms linear spectral learning. individually speaking modeling distribution tran.non gives best performances prediction task fac.non shows signiﬁcant advantage. believe trying combine models formal languages theory models recently several successes machine learning exciting promising line research theoretical practical sides. work ﬁrst step direction proposed novel nonlinear weighted automata model along learning algorithm inspired spectral learning method classical wfa. showed non-linearity introduced ways termination function transition maps directly translates steps learning algorithm. experiment showed synthetic real data nl-wfa lead models better predictive accuracy number states limited nl-wfa able capture complex underlying structure challenging languages nl-wfa exhibit better sample complexity learning data complex grammatical structure. future intend investigate properties nl-wfa theoretical experimental perspectives. former natural question whether could obtain learning guarantees speciﬁc classes nonlinear functions. indeed main advantages spectral learning algorithm provides consistent estimators. diﬃcult obtain guarantees considering functions computed neural networks believe studying case tractable nonlinear functions could insightful. also plan thoroughly investigating connections nl-wfa rnn. practical perspective want ﬁrst tune hyper-parameters nl-wfa extensively current datasets potentially improve results. addition intend experiments real data diﬀerent kinds tasks beside language modeling moreover strong connection interesting nl-wfa context reinforcement learning. worth mentioning spectral learning algorithm cannot straightforwardly used learn functions probability distributions. indeed makes sense probabilistic setting entries corresponding words training data hankel matrix clear entries wants learn function probability distribution e.g. regression task. circumvent issue ﬁrst matrix completion techniques missing entries performing rank decomposition hankel matrix contrast learning algorithm directly applied setting simply adapting loss function factorization network references cyril allauzen mehryar mohri ameet talwalkar. sequence kernels predicting protein essentiality. proceedings international conference machine learning pages raphaël bailly françois denis liva ralaivola. grammatical inference principal component analysis problem. proceedings annual international conference machine learning pages borja balle rémi eyraud franco luque ariadna quattoni sicco verwer. results sequence prediction challenge competition learning next symbol sequence. international conference grammatical inference pages byron boots sajid siddiqi geoﬀrey gordon. closing learning-planning loop predictive state representations. international journal robotics research byron boots arthur gretton geoﬀrey gordon. hilbert space embeddings predictive state representations. proceedings twenty-ninth conference uncertainty artiﬁcial intelligence pages auai press pierre dupont françois denis yann esposito. links probabilistic automata hidden markov models probability distributions learning models induction algorithms. pattern recognition michel fliess. matrices hankel. journal mathématiques pures appliquées mark gales steve young. application hidden markov models speech recognition. foundations trends signal processing daniel sham kakade tong zhang. spectral algorithm learning hidden markov models. proceedings conference learning theory alex krizhevsky ilya sutskever geoﬀrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. mehryar mohri fernando pereira. dynamic compilation weighted context-free grammars. proceedings annual meeting association computational linguistics international conference computational linguisticsvolume pages association computational linguistics cristopher moore. dynamical recognizers real-time language recognition analog computers. foundations computational mathematics pages springer alfredo nazábal antonio artés-rodríguez. discriminative spectral learning hidden markov models human activity recognition. acoustics speech signal processing ieee international conference pages ieee david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature michael thon herbert jaeger. links multiplicity automata observable operator models predictive state representations uniﬁed learning framework. journal machine learning research arun venkatraman nicholas rhinehart lerrel pinto martial hebert byron boots kris kitani andrew bagnell. predictive-state decoders encoding future recurrent networks. arxiv preprint arxiv.", "year": 2017}