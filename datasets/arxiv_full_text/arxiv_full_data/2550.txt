{"title": "Active Algorithms For Preference Learning Problems with Multiple  Populations", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In this paper we model the problem of learning preferences of a population as an active learning problem. We propose an algorithm can adaptively choose pairs of items to show to users coming from a heterogeneous population, and use the obtained reward to decide which pair of items to show next. We provide computationally efficient algorithms with provable sample complexity guarantees for this problem in both the noiseless and noisy cases. In the process of establishing sample complexity guarantees for our algorithms, we establish new results using a Nystr{\\\"o}m-like method which can be of independent interest. We supplement our theoretical results with experimental comparisons.", "text": "paper model problem learning preferences population active learning problem. propose algorithm adaptively choose pairs items show users coming heterogeneous population obtained reward decide pair items show next. provide computationally efﬁcient algorithms provable sample complexity guarantees problem noiseless noisy cases. process establishing sample complexity guarantees algorithms establish results using nystr¨om-like method independent interest. supplement theoretical results experimental comparisons. work interested designing active learning algorithms preference learning problems multiple sub-populations users users sub-population similar preferences large items. propose models algorithm chooses pair items incoming user obtains scalar reward possibly stochastic. reward large either chosen pair items liked user. goal optimal pair items actively eliciting responses users different pairs items. concrete example suppose advertising engine show advertisements incoming user. different appeal less different sub-populations right combination optimizes chance random user like other. paper focuses selecting pair items rather larger subsets sake simplicity. discuss generalizations extensions later paper. preference modeling problems described ubiquitous show internet advertising econometrics marketing informal description preference learning problem consider paper follows given large population users items. applications interested large. suppose user belongs unknown sub-populations original population. interested learning user data pair items among items maximally preferred. contributions paper summarized follows introduce models preference modeling problem sub-populations items. round choose pair items receive reward function pair chosen. reward large either items chosen pair good. models interested designing algorithms discover best pair items using trials possible i.e. algorithms output probability least pair items close best pair items terms expected reward pair. difference models whether reward stochastic deterministic. core idea behind proposed algorithms reduce problem ﬁnding near-optimal pair items problem ﬁnding near-smallest element certain low-rank symmetric positive semi-deﬁnite matrix rank matrix equal number sub-populations. could principle low-rank matrix completion techniques proposed algorithms explicitly exploit spsd structure problem. algorithms propose models based iteratively ﬁnding linearly independent columns spsd matrix. challenge designing column selection step querying elements spsd matrix either expensive noisy hence need selective elements matrix query. algorithms establish sample complexity bounds ﬁnding optimal pair items. establishing sample complexity bounds leads interesting problems matrix approximation maxnorm. contributions make could independent interest low-rank matrix approximation literature. best knowledge formulation preference learning problems bandit problems large number items identifying matrix structure items deriving bounds terms rank underlying matrix makes work novel compared relevant literature. notation. represents dimensional probability simplex. matrices vectors represented bold font. matrix unless otherwise stated notation represents element lijkl submatrix consisting rows columns matrix norms always operator norms. matrix ·max element wise inﬁnity norm. finally column vector. framework closely tied literature pure exploration problems multi-armed bandits. pure exploration problems interested designing algorithms simple regret designing algorithms query complexity. algorithms small simple regret designed past suggested successive elimination median elimination near optimal arms provable sample complexity guarantees. sample complexity guarantees typically scale linearly number arms. principal could naively reduce problem pure exploration problem need good arm. however naive reductions throw away dependency information among arms. algorithms give build algorithm crucially exploits matrix structure problem give much better algorithms naive reduction. consider setup choosing multiple arms reward obtained rewards chosen arms reward chosen reveled algorithm. works focus obtaining guarantees cumulative regret compared best arms hindsight. consider problem context information retrieval multiple bandit arms chosen reward obtained maximum rewards corresponding chosen arms. apart reward information algorithm also gets feedback chosen arms highest reward. also study similar models. major difference mentioned works work feedback reward model fact interested regret guarantees rather ﬁnding good pair arms quickly possible. furthermore linear-algebraic approach problem different previous approaches either based multiplicative weights online greedy submodular maximization also consider similar subset selection problems provide algorithms identify arms. search literature click models proposed model user behaviour bandit analysis models also proposed however models assume users come single population tend richer information formulations interactive collaborative ﬁltering bandit approaches problems also investigated. though goal different goal paper. paper reduce problem preference learning low-rank spsd matrix completion problem noiseless noisy cases. many authors studied problem spsd matrix completion however papers consider passive case i.e. entries matrix revealed control. contrast active setup decide entries matrix reveal. nystr¨om algorithm approximation rank spsd matrices well studied empirically theoretically. nystr¨om methods typically choose random columns approximate original low-rank matrix adaptive schemes columns used nystrom approximation chosen adaptively also considered literature. best knowledge algorithms either need knowledge full matrix provable theoretical guarantees moreover best knowledge analysis nystrom approximation appeared literature assume error free values entries matrix apply problems interested section propose models preference modeling problem. proposed models algorithm chooses pair items receives reward function chosen pair possibly external random factors. models want design algorithms discover using trials possible best pair items. difference models ﬁrst model reward obtained deterministic function pair items chosen whereas second model reward obtained random. figure sketch deterministic stochastic model. multinomial random variable deﬁned probability vector whose output space reward vector indexed playing pair arms round algorithm receives scalar reward deterministic model deterministic given equation whereas stochastic model random variable depends random variable well additional external randomness. however common aspect models expected reward associated pair choices round equal expression given equation concrete example models consider example discussed introduction. here random customer belongs three possible categories probability deﬁned probability vector using stochastic model advertising company information subpopulation customer from makes whenever customer clicks either displayed advertisements otherwise. reward depends sources randomness. sub-population user comes modeled multinomial random variable external randomness modeled bernoulli random variables parameters uzt. case deterministic model reward obtained expected reward stochastic model therefore random. clear figure optimal pair choices satisﬁes equation since interested returning optimal pair choices enough pair returned algorithm attains objective function value optimal value objective function shown equation probability least rank structure models reward matrix rk×k entry probability obtaining reward pair items pulled. equation equation know reward structure deterministic stochastic models form mentioned goal pair items optimal. hence enough entry matrix close smallest entry matrix probability least naive solve problem treat problem best-arm identiﬁcation problem stochastic multi-armed bandits arms corresponding pair items. could successive elimination algorithm median elimination algorithm pairs optimal pair. sample complexity algorithms pairs would roughly typical applications interested large therefore sample complexity naive algorithms large. however simple reductions throw away information different pairs items hence sub-optimal. natural question design algorithms efﬁciently exploit matrix structure problem? turns problem away sample complexity smaller order exploit structural properties matrix equation clear matrix written rank- matrices. hence rank furthermore since rank- matrices positive semi-deﬁnite convex combination such conclude exploit properties algorithm design. proved following proposition proposition matrix shown equation satisﬁes following properties rank approach ﬁnding pair deterministic model considered paper matrix completion. choosing pair items reveals entry matrix seen equation matrix rank standard rank matrix completion ideas relies nuclear norm minimization techniques typical result lrmc literature says need random entries matrix upper bound coherence column space matrix exact recovery matrix provide simple algorithm recovers matrix querying entries matrix algorithm called plans shown figure works matrix plans shown figure iterative algorithm ﬁnds columns matrix independent. plans maintains indices corresponding independent columns matrix initially plans makes single pass columns checks current column independent columns check done line figure importantly requires principal sub-matrix indexed {c}. column passes test elements column whose values queried past queried matrix updated values. test line column selection step plans algorithm justiﬁed proposition finally independent columns chosen impute matrix using nystrom extension. nystrom based methods proposed past handle large scale kernel matrices kernel based learning literature major difference work column selection procedure algorithms deterministic whereas nystrom methods columns chosen random. following proposition simply follows fact principal submatrix spsd matrix also spsd hence admits eigen-decomposition. theorem rk×k spsd matrix rank matrix output plans algorithm satisﬁes moreover number oracle calls made plans sampling algorithm requires samples matrix note sample complexity plans algorithm slightly better typical sample complexity results lrmc. managed avoid factors logarithmic dimension rank well incoherence factors typically found lrmc results also algorithm purely deterministic whereas lrmc uses randomly drawn samples matrix. fact careful deterministic choice entries matrix helps better lrmc. moreover plans algorithm optimal min-max sense. spsd matrix size rank characterized eigen decomposition degrees freedom. hence algorithm completion spsd matrix would need least entries. shown theorem plans algorithm makes queries hence plans min-max optimal. plans algorithm needs rank input. however plans algorithm made work even unknown simply removing condition line plans algorithm. even case sample complexity guarantees theorem hold. finally matrix exactly rank approximated matrix rank plans made robust modifying line σmin σthresh depends stochastic model considered paper shall propose algorithm called r-plans robust version plans. like plans robust version discovers independent columns iteratively uses nystr¨om extension impute matrix. figure provides pseudo-code r-plans algorithm. r-plans like misa algorithm repeatedly performs column selection steps select column matrix linearly independent previously selected columns uses selected columns impute matrix nystrom extension. case deterministic models presence deterministic oracle column selection step pretty straight-forward requires calculating smallest singular-value certain principal sub-matrices. contrast stochastic models availability much weaker stochastic oracle makes column selection step much harder. resort successive elimination algorithm principal sub-matrices repeatedly sampled estimate smallest singular-values matrices. principal sub-matrix largest smallest singular-value determines column selected column selection step. given deﬁne matrix corresponding columns indexed deﬁne respectively repeatedly sampling independent entries index averaging entries. sampling entry matrix sampled least times entry matrix sampled least times sample complexity analysis successive elimination. provide sample complexity analysis need bound spectral norm random matrices mean element sampled possibly different number times. bound plays role correctness successive elimination algorithm. proof bound follows matrix bernstein inequality. relegate proof appendix lack space. lemma random matrix constructed follows. index independent indices ˆpij random variable drawn distribution binomial. then |σmin σmin| lemma successive elimination algorithm shown figure square matrices size size outputs index that probability least matrix largest smallest singular value among input matrices. maxj=...m σmin σmin. number queries stochastic oracle oracle needed order guarantee nystrom extension obtained using matrices accurate theorem consider matrix nystrom extension constructed step r-plans estimated algorithm. note naive lil’ similar performances improve budget outperformed r-plans. naive lil’ samples pair movies. experiments repeated times. experiments demonstrated section provide different algorithms increasing budget measure error algorithm ﬁnding pair choices. algorithms comparison naive lil’ucb lrmc using optspace naive algorithm uniformly distributes equally given budget pairs choices. applies lil’ algorithm treating pair choices stochastic multi-armed bandit game. figure demonstrate results movie lens datasets namely ml-k ml-m. datasets consist incomplete user-movie ratings. task recommend pair movies using user rating data. datasets used experiments generated using dataset performing lrmc missing ratings followed thresholding ratings obtain binary values. users clustered gender figures occupation figure also sub-sampled movies experiments. seen ﬁgures naive lil’ucb algorithms similar performance datasets. ml-k datasets lil’ucb quickly ﬁnds good pair movies fails improve increase budget. observe pairs movies. maximum budget order therefore naive samples pairs average four times. since entries matrix order naive algorithm mostly sees sampling. thing happens lil’ucb algorithm too; samples available pair improve conﬁdence bounds. explains naive lil’ algorithm poor similar performances. contrast r-plans focusses budget select pairs infers value pairs nystrom extensio. experiments r-plans ﬁnds good pair movies quickly ﬁnds even better pairs increasing budget outperforming algorithms. r-plans also better lrmc speciﬁcally exploit spsd structure matrix enables better. would like mention ml-k dataset performance lrmc much inferior. result results synthetic datasets found appendix conclusions paper introduced models preference learning problem showed design active learning algorithms reduction completion spsd matrices. interesting future work design algorithms models choices made time. expect techniques built paper adapted tensor structure useful problems. shall repeat proposition stated main paper sake completeness. proposition spsd matrix size given subset columns matrix indexed independent principal submatrix non-degenerate equivalently λmin would also need classical matrix bernstein inequality borrow work joel tropp theorem independent centered random matrices dimension assume uniformly bounded sample complexity plans algorithm proof theorem theorem rk×k spsd matrix rank matrix output plans algorithm satisﬁes moreover number oracle calls made plans sampling algorithm requires samples matrix proof. plans checks column time starting second column uses test line determine current column independent previous columns. validity test guaranteed proposition test needs additional sample corresponding index column found independent columns rest entries column queried. notice already queried columns rows matrix indexed also queried element line hence need query entries column order entries column combined fact query columns completely worst case diagonal entries might queried total query complexity begin stating lemma. lemma. random matrix constructed follows. index independent indices ˆpij random variable drawn distribution binomial. then −pij)eij matrix entry everywhere independent hence independent random matrices allows apply matrix bernstein type inequalities. order apply matrix bernstein inequality would need upper bound maximum spectral norm summands upper bound variance next bound quantities follows lemma. successive elimination algorithm shown figure square matrices size size outputs index that probability least matrix largest smallest singular value among input matrices. total number queries stochastic oracle round number queries made matrices corresponding column different among them corresponding left submatrix common matrices hence total number queries stochastic oracle obtain estimators matrices question interested good estimate nystrom extension obtained using matrices w.r.t. nystrom extension obtained using matrices construct estimators consider matrix estimate given obtain last inequality used hypothesis lemma obtain inequality used triangle inequality norms obtain inequality used proposition inequlaity follows triangle inequality. proof. proof matrix bernstein inequality. deﬁnition know ˆw−w random matrix entry matrix single bernoulli sample sampled bern. notational convenience denote average independent random matrices whose entry mean random variable variance entry order apply matrix bernstein inequality need upper bound since need total error norm enforce term expression atmost lemma know maxnorm three terms. call three terms r.h.s. lemma repsectively. number copies matrix", "year": 2016}