{"title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.", "text": "introduce model bidirectional retrieval images sentences multi-modal embedding visual natural language data. unlike previous models directly images sentences common embedding space model works ﬁner level embeds fragments images fragments sentences common space. addition ranking objective seen previous work allows fragment alignment objective learns directly associate fragments across modalities. extensive experimental evaluation shows reasoning global level images sentences ﬁner level respective fragments signiﬁcantly improves performance image-sentence retrieval tasks. additionally model provides interpretable predictions since inferred intermodal fragment alignment explicit. signiﬁcant value ability associate natural language descriptions images. describing contents images useful automated image captioning conversely ability retrieve images based natural language queries immediate image search applications. particular work interested training model images associated natural language descriptions later rank ﬁxed withheld sentences given image query vice versa. task challenging requires detailed understanding content images sentences inter-modal correspondence. consider example sentence query tennis ball swimming murky water order successfully retrieve corresponding image must accurately identify entities attributes relationships present sentence ground appropriately complex visual scene. primary contribution formulating structured max-margin objective deep neural network learns embed visual language data common multimodal space. unlike previous work embeds images sentences model breaks embeds fragments images fragments sentences common embedding space explicitly reasons latent inter-modal correspondences. reasoning level fragments allows impose fragment-level loss function complements traditional sentence-image ranking loss. extensive empirical evaluation validates approach. particular report dramatic improvements state methods image-sentence retrieval tasks pascalk flickrk flickrk datasets. plan make code publicly available. related work image annotation image search. growing body work associates images sentences. approaches focus describing contents images formulated either task mapping images ﬁxed sentences written people task automatically generating novel captions closely related approach methods naturally allow bi-drectional mapping modalities. socher fei-fei hodosh kernel canonical correlation analysis align images sentences figure model takes dataset images sentence descriptions learns associate fragments. images fragments correspond object detections scene context. sentences fragments consist typed dependency tree relations. method easily scalable since relies computing kernels quadratic number images sentences. farhadi learn common meaning space method limited representing images sentences single triplet zitnick conditional random field reason complex relationships cartoon scenes natural language descriptions. multimodal representation learning. approach falls general category learning multi-modal data. several probabilistic models representing joint multimodal probability distributions images sentences developed using deep boltzmann machines log-bilinear models topic models ngiam described autoencoder learns audio-video representations shared bottleneck layer. closely related task approach work frome introduced visual semantic embedding model learns images words common semantic embedding ranking cost. adopting similar approach socher described dependency tree recursive neural network puts entire sentences correspondence visual data. however methods reason image global level using single ﬁxed-sized representation layer convolutional neural network description entire image whereas model reasons explicitly objects make complex scene. neural representations images natural language. model neural network connected image pixels side -of-k word representations other. multiple approaches learning neural representations data domains. computer vision convolutional neural networks recently shown learn powerful image representations support state image classiﬁcation object detection language domain several neural network models proposed learn word/n-gram representations sentence representations paragraph/document representations proposed model overview learning inference. task retrieve relevant images given sentence query conversely relevant sentences given image query. train model training images corresponding sentences describe content given correspondences train weights neural network output high score compatible image-sentence pair network score otherwise. training complete training data discarded network evaluated withheld testing images sentences. evaluation score image-sentence pairs sort images/sentences order decreasing score record location ground truth result list. fragment embeddings. core insight images complex structures made multiple interacting entities sentences make explicit references capture intuition directly model breaking images sentences fragments reason alignment. particular propose detect objects image fragments sentence dependency tree relations sentence fragments fragment-level objective. previous related work neural networks embed images sentences common space parameters trained true image-sentence pairs inner product higher false image-sentence pairs margin. approach instead embed image sentence fragments compute imagesentence score ﬁxed function scores fragments. thus addition ranking loss seen previous work second stronger fragment alignment objective. show objectives provide complementary information network. figure computing fragment image-sentence similarities. left representations detected objects mapped fragment embedding space right dependency tree relations sentence embedded model interprets inner products fragments similarity score. alignment latent inferred model image-sentence similarity computed ﬁxed function pairwise fragment scores. ﬁrst describe neural networks compute image sentence fragment embeddings. discuss objective function composed aforementioned objectives. dependency tree relations sentence fragments would like extract represent visually identiﬁable entities described sentence. instance using example figure would like identify entities characterise attributes pairwise interactions inspired previous work observe dependency tree sentence provides rich typed relationships serve purpose effectively individual words bigrams. discard tree structure favor simpler model interpret relation individual sentence fragment thus represent every word using -of-k encoding vector using dictionary words every dependency triplet embedding space follows here matrix encodes -of-k vector d-dimensional word vector representation weights obtained unsupervised objective described huang note every relation weights biases element-wise nonlinearity rectiﬁed linear unit computes max. dimensionality cross-validated. object detections image fragments similar sentences wish extract describe entities images composed inspired prior work modeling assumption observe subject sentence descriptions attributes objects context scene. naturally motivates objects global context fragments image. particular follow girshick detect objects every image region convolutional neural network pre-trained imagenet ﬁnetuned classes imagenet detection challenge detected locations entire image image fragments compute embedding vectors based pixels inside bounding follows takes image inside given bounding returns -dimensional activations fully connected layer immediately classiﬁer. might possible initialize weights parameters classiﬁer layer choose discard weights initial object detection step simplicity. architecture identical described girhsick contains approximately million parameters closely resembles architecture krizhevsky objective function ready formulate objective function. recall given training images corresponding sentences. previous sections described parameterized functions every sentence image fragment vectors respectively. figure objectives batch examples. left rows represent fragments columns every square shows ideal scenario obyij sign visual domain. full objective weighted objectives regularization term shorthand parameters neural network hyperparameters cross-validate. describe objectives detail. fragment alignment objective fragment alignment objective encodes intuition sentence contains fragment least boxes corresponding image high score fragment boxes images mention blue ball score. assumption violated multiple ways triplet refer anything visually identiﬁable image. triplet refers detected rcnn. lastly images contain described visual concept mention omitted associated sentence descriptions. nonetheless assumption still satisﬁed many cases used formulate cost function. consider fragment alignment objective assumes dense alignment every corresponding image sentence fragments here pairs image sentence fragments training set. quantity interpreted alignment score visual fragment sentence fragment incomplete objective deﬁne fragments occur together corresponding image-sentence pair otherwise. constants normalize objective respect number positive negative yij. intuitively encourages scores regions figure less scores along block diagonal multiple instance learning extension. problem objective assumes dense alignment pairs fragments every corresponding image-sentence pair. however hardly ever case. example figure playing triplet refers three detections. describe multiple instance learning extension objective attempts infer latent alignment fragments corresponding image-sentence pairs. concretely every triplet image fragments associated image positive image fragments every image become negative examples. precise formulation inspired mi-svm simple natural extension support vector machine multiple instance learning setting. instead treating constants minimize wrapping equation follows here deﬁne image fragments positive sentence fragment return index image sentence fragments belong note inequality simply states least positive every sentence fragment objective cannot solved efﬁciently commonly used heuristic sj). constraint satisﬁed positive highest-scoring item positive positive label. global ranking objective recall global ranking objective ensures computed image-sentence similarities consistent ground truth annotation. first deﬁne image-sentence alignmnet score average thresholded score pairwise fragment scores here image fragments image sentence fragments sentence range truncate scores zero mi-svm objective scores greater considered correct alignments scores less considered incorrect alignments practice found helpful smoothing term since short sentences otherwise advantage global ranking objective becomes here hyperparameter cross-validate. objective stipulates score true image-sentence pairs higher least margin concludes discussion objective function. note entire model objective functions made exclusively products thresholding zero. optimization stochastic gradient descent mini-batches momentum make epochs training data. learning rate cross-validated annealed fraction last epochs. since multiple instance learning ﬁnetuning beneﬁt good initialization ﬁrst epochs fragment alignment objective weights ﬁxed. epochs switch full objective begin ﬁnetuning cnn. word embedding matrix kept ﬁxed overﬁtting concerns. implementation runs approximately second batch standard workstation. experiments datasets. evaluate image-sentence retrieval performance pascalk flickrk flickrk datasets. datasets contain images respectively image annotated using amazon mechanical turk independent sentences. sentence data preprocessing. explicitly ﬁlter spellcheck normalize sentences simplicity. stanford corenlp parser compute dependency trees every sentence. since many possible relations overﬁtting concerns practical considerations remove relation types occur less time dataset. practice reduces number relations pascalk flickrk flickrk. additionally words found dictionary words discarded. image data preprocessing. caffe implementation imagenet detection rcnn model detect objects images. machine tesla rcnn processes image approximately seconds. discard predictions model random ranking socher devise sdt-rnn fragment alignment objective global ranking objective fragment global images fullframe sentences sentences bigrams model hodosh model imagenet detection classes keep activations fully connect layer immediately classiﬁer detected locations entire image. evaluation protocol bidirectional retrieval. pascalk follow socher images training validation testing. flickr datasets images validation testing rest training compute dense image-sentence similarity every image-sentence pair test rank images sentences order decreasing score. image annotation image search report median rank closest ground truth result list well recallk computes fraction times correct result found among items. comparing hodosh closely follow evaluation protocol keep subset sentences total comparison methods sdt-rnn. socher embed fullframe representation sentence representation semantic dependency tree recursive neural network loss matches global ranking objective. requested source code socher substituted flickrk flickk datasets. better understand beneﬁts using detection fair comparison also train method features. since multiple objects image average representations objects detection conﬁdence threshold. refer exact method socher single fullframe socher method combined features sdtrnn. able evaluate sdt-rnn flickrk training times order multiple days prevent satisfyingly cross-validating hyperparameters. devise. devise source code publicly available approach special case method following modiﬁcations average word vectors sentence fragment average activation objects detection threshold image fragment global ranking objective. quantitative evaluation quantitative results pascalk flickrk flickrk tables respectively. model outperforms previous methods. full method consistently signiﬁcantly outperforms previous methods flickrk flickrk datasets. figure qualitative image annotation results. image show sentences descending conﬁdence. also show triplets sentence connect detections highest compatibility score numbers next triplet indicate matching fragment score. color sentence green correct otherwise. attach many examples supplementary material. pascalk sdt-rnn appears competitive image search. fragment global objectives complementary. seen tables objectives perform well noticeable improvement combined suggesting objectives bring complementary information cost function. note global objective consistently performs slightly better possibly directly minimizes evaluation criterion fragment alignment objective indirectly. extracting object representations important. using global scene-level representation single fragment every image leads consistent drop performance suggesting single fullframe alone inadequate effectively representing images. dependency tree relations outperform bow/bigram representations. compare simpler words baseline understand contribution dependency relations. baseline iterate words instead dependency triplets creating bags sentence fragments seen table leads consistent drop performance. drop could attributed difference using word words time also compare bigram baseline words equation refer consecutive words sentence nodes share edge dependency tree. again observe consistent performance drop suggests dependency relations provide useful structure neural network takes advantage finetuning helps flickrk. end-to-end neural network approach allows backpropagate gradients data particular observed additional improvements flickrk dataset ﬁnetune cnn. able improve validation performance pascalk flickrk datasets suspect insufﬁcient amount training data. qualitative experiments interpretable predictions. show example sentence retrieval results figure alignment model explicitly inferred fragment level allows interpret scores images sentences. instance last image apparent model retrieved sentence erroneously associated mention blue person blue bottom right image. figure triplet retrieve highest scoring image fragments test set. note ball person imagenet detection classes visual properties not. jackets rocky scenes imagenet detection classes. find supplementary material. fragment alignment objective trains attribute detectors. detection trained predict imagenet detection classes clear representation powerful enough support learning complex attributes objects generalize novel classes. whether model successfully learns predict sentence triplets triplet vector search highest scoring boxes test set. qualitative results shown figure suggest model indeed capable generalizing ﬁne-grained subcategories sample classes rocky terrain jacket. limitations. method multiple limitations failure cases. first modeling perspective sentences modeled bags relations. therefore relations belong noun phrase sometimes align different objects. additionally people frequently phrases three children playing model incapable counting. moreover non-maximum suppression rcnn sometimes detect example multiple people inside person. since model take account spatial information associated detections hard tell difference distinct people spurious detections person. language side many dependency relations don’t natural grounding image compound relations attributes also become separated. instance black white parsed relations shown relations give rise powerful representations words bigrams careful treatment sentence fragments might necessary progress. conclusions addressed problem bidirectional retrieval images sentences. neural network model learns multi-modal embedding space fragments images sentences reasons latent inter-modal alignment. reasoning ﬁner level image sentence fragments allowed formulate fragment alignment objective complements traditional global ranking objective term. shown model signiﬁcantly improves retrieval performance image sentence retrieval tasks compared previous work. model also produces interpretable predictions. future work hope extend model support counting reasoning spatial positions objects move beyond bags fragments. references marneffe m.c. maccartney manning c.d. generating typed dependency parses rashtchian young hodosh hockenmaier collecting image annotations using amazon’s mechanical turk. proceedings naacl workshop creating speech language data amazon’s mechanical turk association computational linguistics zitnick c.l. parikh vanderwende learning visual interpretation sentences. iccv srivastava salakhutdinov multimodal learning deep boltzmann machines. nips. kiros zemel r.s. salakhutdinov multimodal neural language models. icml salzmann darrell learning cross-modality similarity multinomial data. iccv. mnih hinton three graphical models statistical language modelling. icml. mikolov sutskever chen corrado g.s. dean distributed representations words", "year": 2014}