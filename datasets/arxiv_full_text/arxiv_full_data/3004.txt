{"title": "Measuring and modeling the perception of natural and unconstrained gaze  in humans and machines", "tag": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Humans are remarkably adept at interpreting the gaze direction of other individuals in their surroundings. This skill is at the core of the ability to engage in joint visual attention, which is essential for establishing social interactions. How accurate are humans in determining the gaze direction of others in lifelike scenes, when they can move their heads and eyes freely, and what are the sources of information for the underlying perceptual processes? These questions pose a challenge from both empirical and computational perspectives, due to the complexity of the visual input in real-life situations. Here we measure empirically human accuracy in perceiving the gaze direction of others in lifelike scenes, and study computationally the sources of information and representations underlying this cognitive capacity. We show that humans perform better in face-to-face conditions compared with recorded conditions, and that this advantage is not due to the availability of input dynamics. We further show that humans are still performing well when only the eyes-region is visible, rather than the whole face. We develop a computational model, which replicates the pattern of human performance, including the finding that the eyes-region contains on its own, the required information for estimating both head orientation and direction of gaze. Consistent with neurophysiological findings on task-specific face regions in the brain, the learned computational representations reproduce perceptual effects such as the Wollaston illusion, when trained to estimate direction of gaze, but not when trained to recognize objects or faces.", "text": "humans remarkably adept interpreting gaze direction individuals surroundings. skill core ability engage joint visual attention essential establishing social interactions. accurate humans determining gaze direction others lifelike scenes move heads eyes freely sources information underlying perceptual processes? questions pose challenge empirical computational perspectives complexity visual input real-life situations. measure empirically human accuracy perceiving gaze direction others lifelike scenes study computationally sources information representations underlying cognitive capacity. show humans perform better face-to-face conditions compared ‘recorded’ conditions advantage availability input dynamics. show humans still performing well eyes-region visible rather whole face. develop computational model replicates pattern human performance including finding eyes-region contains required information estimating head orientation direction gaze. consistent neurophysiological findings task-specific face regions brain learned computational representations reproduce perceptual effects ‘wollaston illusion’ trained estimate direction gaze trained recognize objects faces. keywords gaze perception estimation gaze direction joint attention empirical evaluation computational modeling computational evaluation computer vision machine learning. introduction humans social species remarkably adepts understanding people's mental states based perception actions developmental studies demonstrated even young infants engaged joint attention humans understand mental states observing interpreting non-verbal behavior including looking direction inferring people looking plays major role development communication language opens window mental state serves important towards understanding intentions actions social interactions gaze perception extensively studied human vision scope existing behavioral studies provides limited understanding gaze perception real scenes. main reason many behavioral studies focused subjects’ ability judge whether person’s gaze directed acuity detecting direct eye-contact found high visual angle however brain studies shown direct gaze general gaze directions encoded different brain regions suggesting direct eye-contact involve separate processing mechanism different computational representations compared judging general direction gaze another limitation past studies constraints imposed behavioral studies looker's gaze behavior fixing head certain pose allowing eyes rotation constraints help isolate effects head orientations remains unclear past studies accuracy gaze perception natural unconstrained looking looker move head gaze freely parts face contribute information required perform task. dealing unconstrained gaze direction also challenging computational problem. frontal-view controlled head movement eyes clearly visible observer gaze direction estimated directly relative position iris pupil eyes. contrast unconstrained gaze scenario appearance eyes vary dramatically clear front-view partially occulated eyes eyes invisible complex interaction head pose gaze. finally many studies discriminating gaze perception looker asked look empty space whereas natural realistic situations person's gaze typically oriented object scene. developmental neuropsychological studies shown gazing towards object important source learning estimating direction gaze remains possible experimental conditions ‘empty space’ studies identical object-directed gaze. computationally modeling perceptual cognitive processes involved analysis social interactions including extraction direction gaze interpreting visual joint attention real-life situations pose challenge complexity visual input realistic environment compared restricted configurations studied laboratory conditions. head pose estimation detection gaze studied extensively computer vision applied mathematics communities majority studies addressed either head pose estimation gaze direction estimation disjoint problems studies problems demonstrated impressive performance separate tasks little done addressing problem detecting direction gaze natural unconstrained scenes observed humans look freely different targets. natural unconstrained setting gaze events towards different targets share head poses different poses share gaze towards target. recent study trained deep neural network infer direction gaze natural images using many labeled examples head orientation gaze targets manually annotated. since heads faces training images small sometimes seen back remains unclear face parts contributed gaze estimation. studies suggested estimate direction gaze free-head movements first rectifying eyes images using head pose acquired sensor estimated face image. however eyes rectification procedure complex sets limitation range supported head poses evidence role human perception. eyeregion rectification addressed another study suggested computer graphics method synthesizing realistic close-up images human wide range head poses gaze directions illumination conditions. using synthesized images training phase improved performance state-of-the-are methods gaze estimation including deep neural nets synthesized dataset designed typical laptop-viewing setting limited head pose gaze variations. study first time accuracy gaze perception natural unconstraint looking conditions. settings ‘observer’ interprets gaze direction human ‘looker’ move head eyes freely looking objects around several conditions tested determine source information estimation gaze direction ‘live’ versus ‘recorded’ stimuli ‘dynamic’ versus ‘static’ ‘whole-face’ visibility versus ‘eyes-region’ ‘face-without-eyes’. results show humans perform better ‘live’ condition compared ‘recorded’ condition dynamics visual input. furthermore eyes-region essentially sufficient estimating direction gaze yields equivalent performance ‘whole-face’ condition. better understand human performance constructed compared computational models process head orientation gaze direction images faces. developed model replicates human performance similar ‘recorded’ conditions including finding eyes region contains information interpreting direction gaze head orientation model operates two-stage process head pose estimation followed gaze direction estimation eyes ‘conditioned’ head pose. two-stage processing proved superior end-to-end deep neural networks studied. addition learned representations show effect similar ‘wollaston illusion’ methods study combines empirical testing computational modeling evaluation. goal measure human accuracy unfold sources information underlie perceptual cognitive processes involved. first describe empirical studies conducted different conditions. next describe computational study includes modeling computational processes comparison performance models humans similar conditions. human experiments tasks trial current study involved human participants looker observer. looker sitting facing objects arranged tabletop. trial command generated looker indicating color number target object. looker instructed look target object naturally possible moving head eyes freely. observer either sitting front looker across table watching recorded session looker looker’s gazing action well objects laid table. task observer infer target object looker’s gaze. block trials object selected target random order generated automated script. precision observer’s response primary psychophysical measurement also used evaluation computational study. participants normal corrected-to-normal vision gave informed consent paid participation. experiments procedures approved institutional review board massachusetts institute technology cambridge usa. across experiments varied visual input observers control sources visual information available performing task. described detail below viewing conditions ranged fully unconstraint live observation highly blurred face images tightly constrained patches allowing clear view eyes. design enabled measure overall precision gaze perception also reveal contribution different sources information dynamic static clear view whole face partially blurred views face. started viewing condition contained richest visual information face-toface live viewing four observers opposite side table watching single looker. looker looked target objects target trial following commands showing monitor beginning experiment looker instructed keep looking target beep sound signaled trial seconds looker look back monitor command next trial. observers instructed write name target object interpreting lookers gaze. following blocks observers shift sits clockwise. four positions categorized classes center periphery. make direct comparison results recorded conditions experiment include results data center positions male caucasian african american participants observers part experiment. experiment originally also included sunglasses condition looker wears pair dark sunglasses covers eyes region. observers’ performance condition significantly drops. however wearing sunglasses disrupts automatic face tracking recording system makes data unavailable computational modeling. therefore include data condition results exclude experiment showed observers recorded videos images lookers’ actions computer screen. videos images show clearly whole scene including looker full object array table face looker roughly spans visual angle observer’s field view although live information longer available observers systematically manipulate visual stimuli introducing different viewing conditions exp. dynamic video entire movements looker shown observer starting resting state ending fixed gaze target; whole-face static images images lookers gazing targets shown. image frames extracted recorded videos lookers kept steady head eyes eyesregion static images rectangular image strip around eyes region visible rest face blurred head-only static images eyes-region gaussianblurred rest face visible exp. isolated information available eyes-region introducing conditions tight-eyes face parts boundaries surrounding eyes gaussian-blurred separate-eyes nose bridge eyes also gaussian-blurred whole-face eyes-region conditions repeated expt. baselines. experiments blocks block consisted trials order target objects randomized. trial lasted seconds. observers requested move computer mouse click inferred target. block order trial order within block randomized. observer practice trials beginning experiment using images different looker four lookers actual experiment. array objects laid table concentric configuration columns four rows wooden placed table mark center position concentric array. objects color number column marked side object. distance center array visual angle every adjacent rows point center array column positions angular difference adjacent columns table surface. corresponding visual angle every adjacent columns point average. practice visual angles slightly vary given exact position looker’s head computed trial-by-trial basis microsoft kinect rgb-d sensor also positioned across table facing table object array rgb-d sensor used record video looking trials lookers also provide accurate information recorded scene computational evaluations detailed evaluation kinect’s depth accuracy). live condition observers seated opposite side table away looker. recorded conditions observers seated away display recorded stimuli included video sessions still images looking trials different viewing conditions pixel resolution understand sources information underlying perceptual cognitive processes estimating person’s gaze direction studied computational models recognizing direction gaze images faces. accurate information visual environment extracted rgb-d sensor depth data including position objects faces. microsoft kinect windows provides accurate face tracking reliable head orientation tracked faces. direction gaze defined vector pointing center location eyes location target object. developed model estimating direction gaze image face face parts based state-of-the-art computer vision methods. model works two-stage process head pose first gaze direction eyes ‘conditioned’ head pose. implementation image representations face face parts associated directions head orientation gaze direction using k-nearest neighbors approach similar supplementary material). dataset training evaluation created recorded sessions ‘lookers’ used testing human ‘observers’. dataset consists face images associated head orientation direction gaze collected blocks. evaluation dataset done using leave-one-looker-out crossvalidation approach. evaluated stage model together leading deep neural network models pre-trained object recognition face recognition fine-tuned infer head orientation direction gaze face images models evaluated similar conditions human psychophysics excluding dynamic condition comparison human performance models also tested face images extracted stimuli images human psychophysics test evaluate wollaston illusion face triplets created evaluation dataset. face triplet consisted pair authentic face images artificially ‘synthesized’ face image pair authentic face images depicted face turned left image right image image cloning method used generate synthesized face image eyes first image head pose second image replacing eyes ‘target’ image cropped eyes ‘source’ image. applied models face triplets compared estimated directions gaze. also compared underlying representations eyes extracted deepest ‘pooling’ layer models models trained gaze estimation well models trained object face recognition. comparison used correlations extracted layer responses localized eyes region pairs faces different head poses either different eyes. figure shows overall performance human observers estimating gaze direction lookers tested conditions accuracy measured percentage trials human observers exactly correct target less object correct target ‘exact’ accuracy ‘live’ condition significantly higher p=.) highest ‘exact’ accuracy among tested ‘recorded’ conditions neighborhood accuracies ‘live’ condition significantly higher well p<.; p<.). however difference ‘exact’ accuracy dynamic condition ‘exact’ accuracy static whole-face condition found insignificant p=.) suggesting performance ‘live’ ‘recorded’ conditions input dynamics. ‘exact’ accuracy ‘eyes-region’ condition significantly less ‘exact’ accuracy whole-face condition methods difference -neighborhood accuracies conditions significant p=.; p=.). furthermore ‘exact’ -neighborhood accuracies eyes-region condition much better accuracies head-only condition p<.; p<.; p=.; methods rc). findings suggest ‘eyesstrip’ essentially contains information needed accurate gaze estimation. experiment furthered manipulated images around eyes-region. removing bridge eyes effective ‘exact’ accuracy separate-eye condition significantly lower eyes-region condition p=.). ‘exact’ accuracy tight-eyes condition also lower eyes-region condition marginal significance p=.). computational model evaluation accuracy measured directly angular error estimated direction true direction head orientation gaze target. compare model’s accuracy human accuracy trial also extracted estimated target object nearest object around intersection estimated direction gaze plane object array. applying model test images human experiments well additional evaluation images reproduced performance variations similar conditions although humans performed better computational model. humans accuracy ‘eyes-region’ condition found comparable accuracy whole face condition evaluation estimated head orientation first stage model yielded comparable mean angular errors applied ‘eyes-region’ whole face suggesting ‘eyes-region’ essentially contains full head pose information alternative single-stage models including deep neural networks estimate directly head orientation gaze direction given face image trained evaluated found inferior two-stage model two-stage model inherently supports wollaston illusion perceived gaze identical eyes different head-pose contexts shifted direction head orientation since model eyes conditioned head orientation. quantitative analysis underlying representations eyes model evaluated deep neural network trained gaze estimation reproduced wollaston illusion applied ‘synthesized’ face images dataset mean angular offset m=.º sd=.º gaze direction ‘source’ images identical eyes different head orientation around ‘direct’ gaze towards camera). results suggest eyes representations congruent in-congruent eyes respect head orientation implied face context different. evaluated networks trained object classification face identification yielded significantly lower correlation values network responses eyes region authentic face images correlation values responses eyes region ‘source’ ‘synthesized’ images eyes identical object classification; t=-. face identification). lower correlation values suggest networks similar representations identical eyes ‘source’ ‘synthesized’ images different representations dissimilar eyes ‘source’ ‘target’ images. however network trained gaze estimation difference correlation values pairs images found insignificant p=.) suggesting network representations identical eyes ‘source’ ‘synthesized’ images different representations dissimilar eyes ‘source’ ‘target’ images. related wollaston comment humans identical eyes look different different head-pose contexts. discussion current study aims better understand perception gaze direction natural unconstraint looking conditions single target tens objects. studying unconstrained gaze direction challenging computational problem complex interaction head orientation gaze direction. gaze direction cannot estimated directly relative position pupils eyes front-view case. furthermore appearance eyes varies dramatically fully visible front-view eyes partially occulated eyes eyes region becomes invisible. study measured first time human accuracy perceiving unconstrained natural gaze directions human lookers. experiments tested several viewing conditions including face-to-face recorded conditions. comparison accuracy different conditions indicates performance live condition significantly better recorded conditions. analysis shows dynamics input similar findings worth noting however input dynamics serve strong learning gaze following early infanthood static gaze perception developing later around months performance live recorded conditions part better perception using e.g. stereoscopic vision live condition indicated recent study assumption validated empirically using instance monocular vision covering eye. also body context cues better perceived live condition found influence face gaze perception among static viewing conditions highest accuracy measured whole face visible stimuli expected. however surprisingly small performance viewing whole face compared eyes region only suggests eyes region includes information required accurate perception direction gaze. unexpected limited region contains fraction face features. furthermore computational model replicates findings also extract head orientation human-level accuracy based eyes region only. excluding face boundaries nose bridge eyes-region stimuli results significant performance drop humans model since face parts provide essential information particular head pose estimation compact region relevant information located provide efficient computational representation gaze estimation acquired single saccade entire eyes region falls within foveal vision. interesting test patterns fixations faces performing e.g. task gaze estimation triadic joint attention compare patterns face-related tasks computational model consists processing stages estimation head orientation followed estimation gaze direction eyes ‘conditioned’ estimated head pose. moreover attraction repulsion effects head orientation perceived gaze implicitly included model model proves superior alternative single-stage models including end-to-end deep neural network models learn associate images face facial parts directly gaze direction head orientation. performance evaluation computational model reproduces human performance including differences among different viewing conditions. model also agrees neurophysiological findings organization face processing cells social attention responsive head pose gaze direction. two-stage model also agreement psychophysical findings development face gaze perception infants first able track faces roughly follow gaze using head motion using cues eyes accurate gaze perception later stages developmental trajectory suggest computational processing eyes gaze perception built early acquired capability head pose estimation. intriguing finding reproduction wollaston illusion model trained estimating gaze direction alternative models trained object classification face recognition. finding suggests different face eyes representations learned computational dnn-based models different tasks line neurophysiological findings task-specific cortical regions responsive face facial parts including eyes gaze estimation face object recognition findings suggest wollaston effect depends specifically gazerelated regions. finally computational model accurate estimation gaze direction face images could combined existing methods depth estimation scene segmentation model joint attention social interactions. particular combined scheme able follow direction gaze space identify attended target. artificial intelligence system includes cognitive capability interpreting joint attention able interpret social interactions scenes understanding people scene engage joint attention well learn interact social interactions identifying attended targets humans view -neighborhood also known neumann neighborhood comprises four objects orthogonally surrounding central target object array. mathematically defined points manhattan distance central point. specialized face-processing model inspired organization monkey face patches explains several face-specific phenomena observed humans. scientific reports http//doi.org/./srep reid striano adult gaze influences infant attention object processing implications cognitive neuroscience. european journal neuroscience http//doi.org/./j.-...x tomasello cultural origins human cognition. harvard university press. ullman harari dorfman simple innate biases complex visual concepts. proceedings national academy sciences http//doi.org/./pnas. wang wang kong face pose estimation knowledge-based model. proceedings international conference neural networks signal processing icnnsp’ http//doi.org/./icnnsp.. d.h. t.g. n.k. j.t. s.u. designed research; d.h. t.g. performed research; d.h. t.g. analyzed data; d.h. t.g. n.k. j.t. s.u. wrote paper. d.h. t.g. contributed equally work. figure direction gaze social interactions. fortune teller painting artist georges tour depicting scene young wealth fortune told woman right. young woman’s eyes averted paying careful attention direction gaze right cuts medal worn chain. artist uses gaze direction cues allow common observer full understanding social deception theft scene. figure unconstraint natural looking task. sample faces ‘lookers’ extracted stimuli ‘displayed’ condition illustrating wide range tested gaze directions various appearance combinations head eyes. sample faces looking object object array illustrating variability head pose gaze combinations across ‘lookers’ given target. figure ‘eyes-region’. region around eyes including bridge nose face boundaries near temples sufficient estimating direction gaze essentially equivalent full face. figure wollaston illusion. face perceived gaze left eyes extracted combined head pose image perceived gaze synthesized image humans models front. model suggests perceived gaze shifted direction head pose. eyes representation models synthesized images like different eyes representation authentic image despite fact eyes same. agreement wollaston original comment eyes perceived different different head pose contexts. figure human model accuracy different viewing conditions. accuracy measured percent correct getting exact true target object less surrounding neighbors human experiment results results computational model replicate human results similar conditions figure towards automatic interpretation social interactions. automatic interpretation social interactions images requires fundamental capability interpreting directions gaze detecting attended targets. computational model accurately estimates direction gaze images allows detection attended targets combining gaze direction estimated depth scene segmentation figure apparatus psychophysics experiment. array objects laid table concentric configuration columns rows. wooden marked center position concentric array. objects color number column marked side object. laptop placed behind table facing table object array display instructing commands participating ‘lookers’. microsoft kinect rgb-d sensor positioned across table facing object array used record video looking trials performing ‘lookers’ provide accurate information scene computational evaluations. picture setup taken behind looker’s seat. schematic view. figure sample ‘recorded’ static stimulus. visual stimuli displayed screen depicting ‘looker’ freely looking targets table. trial ‘observer’ asked mark displayed image best guess looker’s target using computer mouse. figure static ‘displayed’ conditions. static ‘displayed’ stimuli consisted viewing conditions whole face visible; ‘eyes-strip’ only; tight ‘eyes-strip’ only; separate eyes only; face without eyes. table informative cues eyes-region. performance computational model trained estimate head orientation direction gaze images eyes-region applied images computational evaluation dataset different occlusion conditions. results indicate information originated eyes also nose bridge face boundaries contribute information mainly head orientation. table performance evaluation computational models. comparison estimated target accuracy mean angular error estimated direction gaze head orientation two-stage computational model alternative single-stage models. results reported computational evaluation dataset. visual stimuli experiments acquired using microsoft kinect rgb-d sensor. still images extracted individual frames recorded videos. pixel resolution video image stimuli different viewing conditions dynamic videos. video clips seconds long trial depicting performing looker. video clip starts looker resting state pose showing full motion trajectory looker looker fixates gaze target table. video clip ends looker returns resting state pose whole-face still images. image frames lookers gazing targets extracted recorded videos lookers keep steady head eyes fixating target. image parts clearly visible including looker’s face object array table faces width pixels height pixels. eyes-region still images. images manipulated follows rectangular image region around eyes kept clearly visible including nose bridge face boundaries temples surrounding image region size pixels including rest face blurred downsampling followed up-sampling image region factor using lanczos resampling method tight-eyes still images. images manipulated follows rectangular image region tightly surrounding eyes kept clearly visible including nose bridge excluding face boundaries surrounding image region size pixels including rest face blurred separate-eyes still images. images manipulated follows rectangular image regions tightly surrounding eyes kept clearly visible surrounding image region size pixels including rest face blurred generating stimuli rc-rc image center locations eyes nose mouth well face bounding automatically detected kinect’s face tracking algorithm. location dimensions rectangular image regions around eyes heuristically determined locations face parts spatial configuration every image. developed model estimating direction gaze images faces using computer vision methods. training model learns associate facial appearances head pose directions associate eye-pairs appearances offset directions head orientation final gaze direction. implementation used image descriptors represent facial appearances dense-sift image descriptors represent eye-pairs appearances inference processing model works stages. first stage model estimates head pose direction input face image extracting familiar faces model similar facial appearances estimated head orientation weight-average associated head pose directions neighboring faces. second stage model extracts familiar faces model similar eye-pairs appearances around estimated head orientation first stage. associated offset directions head orientation final gaze direction extracted neighboring faces weightaveraged yield estimated offset direction estimated head orientation first stage final gaze direction. implementation rotational quaternions represent directions compared performance two-stage model alternative appearance-based models infer directly head orientation final direction gaze input image single processing stage. trained tested additional nearest neighbor models deep neural network models yielded inferior performance compared two-stage model nearest neighbor models included models associating dense-sift appearance descriptors whole face eyes-region directly head orientation gaze direction. deep neural network models based face recognition models fine-tuned minimize error estimated ground-truth head orientation gaze direction given input images whole face eyes-region", "year": 2016}