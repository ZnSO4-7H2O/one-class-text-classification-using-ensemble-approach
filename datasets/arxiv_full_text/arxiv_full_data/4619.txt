{"title": "Asymptotically Optimal Agents", "tag": ["cs.AI", "cs.LG"], "abstract": "Artificial general intelligence aims to create agents capable of learning to solve arbitrary interesting problems. We define two versions of asymptotic optimality and prove that no agent can satisfy the strong version while in some cases, depending on discounting, there does exist a non-computable weak asymptotically optimal agent.", "text": "artiﬁcial general intelligence aims create agents capable learning solve arbitrary interesting problems. deﬁne versions asymptotic optimality prove agent satisfy strong version cases depending discounting exist non-computable weak asymptotically optimal agent. introduction notation deﬁnitions non-existence asymptotically optimal policies existence weak asymptotically optimal policies discussion technical proofs table notation rational agents; sequential decision theory; artiﬁcial general intelligence; reinforcement learning; asymptotic optimality; general discounting. dream artiﬁcial general intelligence create agent that starting knowledge environment eventually learns behave optimally. means able learn chess playing drive lawn task could conceivably interested assigning considering existence universally intelligent agents must precise meant optimality. environment goal known subject computation issues optimal policy easy construct using expectimax search sequential decision theory however true environment unknown agent necessarily spend time exploring cannot immediately play according optimal policy. given class environments suggest deﬁnitions asymptotic optimality agent. diﬀerence strong asymptotically optimal agent must eventually stop exploring weak asymptotically optimal agent explore forever decreasing frequency. paper consider existence weak/strong asymptotically optimal agents class deterministic computable environments. restriction deterministic sake simplicity results case already suﬃciently non-trivial interesting. restriction computable philosophical. church-turing thesis unprovable hypothesis anything intuitively computed also computed turing machine. applying physics leads strong church-turing thesis universe computable made assumptions largest interesting class becomes class computable environments. hutter conjectured universal bayesian agent aixi weakly asymptotically optimal class computable stochastic environments. unfortunately recently shown false proven bayesian agent weakly asymptotically optimal class. idea behind orseau’s proof show aixi eventually stops exploring. somewhat surprising normally assumed bayesian agents solve exploration/exploitation dilemma principled way. result reminiscent bayesian extend work bayesian agents considered show non-computable weak asymptotically optimal agents exist class deterministic computable environments discount functions others. also show asymptotically optimal agent computable reasonable discount functions exist strong asymptotically optimal agent. weak asymptotically optimal agent construct similar aixi exploration component similar ǫ-learning ﬁnite state markov decision processes algorithm bandits. explore suﬃciently often deeply ensure environment used model adequate approximation true environment. time agent must explore infrequently enough actually exploits knowledge. whether possible balance right depends somewhat surprisingly forward looking agent sometimes possible explore enough learn true environment without damaging even weak form asymptotic optimality surprising unexpected. note exploration/exploitation problem well-understood bandit case markov decision processes restrictive settings various satisfactory optimality criteria available. work make assumptions like markov stationary ergodicity else besides computability environment. satisfactory optimality deﬁnition available general case. strings. ﬁnite string alphabet ﬁnite sequence an−an inﬁnite string alphabet inﬁnite sequence sets strings length strings ﬁnite length inﬁnite strings respectively. string substrings denoted xsxs+ xt−xt strings concatenated. length respectively deﬁne xn−xnyy ym−ym xn−xnωωω useful shorthands second ambiguous concatenation wherever yx<t appears assume interleaving deﬁnition intended. example common yx<tyt represents string yxyxyx yt−xt−yt. environments optimality. action observation reward spaces respectively. agent interacts environment illustrated diagram right. first agent takes action upon receives observation/reward pair. agent takes another action receives another observation/reward pair so-on indeﬁnitely. goal agent maximise discounted rewards time. paper consider deterministic environments next observation/reward pair determined function previous actions observations rewards. deﬁnition deterministic environment function observation/reward pair given action taken history yx<t. wherever write implicitly assume refer without deﬁning them. environment computable exists turing machine computes note since environments deterministic next observation need depend previous observations choose leave dependence proofs become clearer action observation sequence visible. need deﬁne value policy environment avoid possibility inﬁnite rewards discounted values. common geometric discounting reasons allow arbitrary timeconsistent discount functions. geometric discounting constant eﬀective horizon feel agents allowed discount function leads growing horizon. seen agents humans generally become less myopic grow older. overview generic discounting. ﬁrst condition natural deﬁnition discount function. second condition often cited purpose discount function economists sometimes non-summable discount functions hyperbolic. second condition also guarantees agent cares inﬁnite future required make asymptotic analysis interesting. consider discount functions satisfying three conditions. following inﬁnite sequence rewards starting time given value normalisation term ensure values scale still compared limit. discount function computable exists turing machine computing well known discount functions geometric ﬁxed horizon hyperbolic computable. note exists represents eﬀective horizon agent. time-steps future starting time agent stands gain/lose assumption combined theorem guarantees existence note normalisation term change policy used ensure values scale appropriately limit. example discounting geometrically have strong asymptotic optimality demands value single policy converges value optimal policy class. means limit strong asymptotically optimal policy obtain maximum value possible environments. weak asymptotic optimality similar requires average value policy converge average value optimal policy. means weak asymptotically optimal policy still make inﬁnitely many mistakes must fraction time converges zero. strong asymptotic optimality implies weak asymptotic optimality. deﬁnition strong asymptotic optimality rather natural deﬁnition weak asymptotic optimality appears somewhat arbitrary. purpose average allow agent make vanishing fraction serious errors life-time. believe necessary condition agent learn true environment. course would possible insist agent make serious errors rather would make stronger version weak asymptotic optimality. choice weakest notion optimality form still makes sense turns already strong discount rates. note versions optimality agent would considered optimal actively undertook policy extremely hell state could escape. since state cannot escaped policy would coincide optimal policy would considered optimal. unfortunately problem seems unavoidable consequence learning algorithms nonergodic environments general including currently fashionable algorithms arbitrary ﬁnite markov decision processes. present negative theorem three parts. ﬁrst shows that least computable discount functions exist strong asymptotically optimal policy. second shows weak asymptotically optimal policy must incomputable third shows exist discount functions even incomputable weak asymptotically optimal policies exist. part theorem says strong asymptotically optimal policy class computable deterministic environments discount function computable. likely exist non-computable discount functions strong asymptotically optimal policies. unfortunately discount functions true likely somewhat pathological realistic. given strong asymptotic optimality strong search weak asymptotically optimal policies. part theorem shows policy necessarily incomputable. result features real ideas relies fact computable policy hand-craft computable environment badly general approach fails incomputable policies hand-crafted environment computable. note rule existence stochastically computable weak asymptotically optimal policy. turns even weak asymptotic optimality strong discount functions. part theorem gives example discount function policy exists. next section introduce weak asymptotically optimal policy geometric example discount function discounting. note also analytically easy work with. follows deﬁnitions value function assumption previous line. follows algebra deﬁnition hti. contradicts therefore strong asymptotically optimal policy exists cannot take sub-optimal action frequently. particular cannot take action large contiguous blocks time. construct environment deﬁned spends time-steps playing receiving reward unlocking reward subsequent plays. hand never unlock reward never plays contiguous block time-steps. deﬁnition optimal policy proof theorem part down} class computable deterministic environments arbitrary discount function. suppose computable consider environment deﬁned shown cannot explore consecutive time-steps starting time-step inﬁnitely often. construct environment similar required. choose larger last time-step policy reminiscant ǫ-exploration ﬁnite state mdps spends time exploiting information already knows still exploring suﬃciently often detect signiﬁcant errors model. idea model-based policy chooses current model ﬁrst environment model class consistent history seen far. increasing probability takes best action according policy still occasionally exploring randomly. explores always bursts increasing length. intuitively time-steps agent explore time-steps. agent exploring time action taken exploring time-step agent explore least interval agent exploring acts according optimal policy ﬁrst consistent environment essentially stochastic policy. technical diﬃculties possible construct equivalent deterministic policy. done choosing -martin-l¨of random sequence sequence martin-l¨of random w.r.t uniform measure. theorem holds deterministic environments. proof somewhat delicate extend nicely stochastic environments. introduction kolmogorov complexity martin-l¨of randomness reason stochastic case easily policy deﬁned deﬁnition computable reasons. first relies stochastic sequences second operation ﬁnding ﬁrst environment consistent history computable. know exists weak asymptotically optimal policy computable given access random number generator proof theorem policy deﬁned deﬁnition true environment. consistent history yxπµ ﬁrst model consistent history yxπµ time used exploring. first claim exists environment facts sequence monotone increasing. sequence bounded claim follows since bounded monotone sequence natural numbers converges ﬁnite time. environment converges note must consistent history yxµπ show contradiction optimal policy weakly asymptotically optimal environment suppose lemma exists inﬁnite sequence intuitively view time-step start exploration phase agent explores time-steps. ⌈log/ importantly independent since assume therefore deﬁnition lemma equation h-diﬀerent history yxπµ means exists plays according optimal policy time-steps inconsistent history yxµπ tk+h contradiction. show indeed play according optimal policy timesteps least formally show following holds probability shown optimal policy similar µ-values optimal policy show acts according suﬃciently often values close optimum policy true environment deﬁnition approximation lemma obtain expect theorem generalise without great diﬃculty discount functions satisfying changes. first extend exploration time function second modify probability exploration ensure lemma remains true. summary. part theorem shows policy strongly asymptotically optimal computable discount function. insight strong asymptotic optimality essentially implies exploration must eventually cease. occurs environment change without agent discovering diﬀerence policy longer optimal. weaker notion asymptotic optimality policy optimal average limit turns interesting. part theorem shows weak asymptotically optimal policy computable. surprised result. computable policy used construct computable environment policy badly. note computable mean deterministic computable. computable stochastic policies weakly asymptotically optimal feel unlikely. part theorem shows even weak asymptotically optimal policies need exist discount function suﬃciently far-sighted. hand theorem shows weak asymptotically optimal policies exist discount rates particular default geometric discounting. non-trivial slightly surprising result shows choice discount function crucial existence weak asymptotically optimal policies. weak asymptotically optimal policies exist must explore inﬁnitely often increasing contiguous bursts exploration length burst dependent discount function. consequences. would appear theorem problematic artiﬁcial general intelligence. cannot construct incomputable policies cannot construct weak asymptotically optimal policies. however problematic seem. number reasonable counter arguments counter-example environment constructed part theorem single environment roughly complex policy itself. certainly world adversarial would problem general appears case. hand environment learning agent itself could result complexity arms race without bound. exist computable weak asymptotically optimal policy extremely large class environments. example algorithm section stochastically computable class environments recursively enumerable contains computable environments. natural class satisfying properties ﬁnite-state markov decision processes }-valued transition functions rational-valued rewards. mathematically pleasant asymptotic behaviour characterise optimal general intelligent behaviour practise usually care immediate behaviour. expect results even formal deﬁnitions intelligence satisfying need challenging worthwhile. relation aixi. policy deﬁned section equivalent aixi also incomputable. however computable environments ordered complexity likely quite similar. diﬀerence policy deﬁned paper continue explore whereas shown aixi eventually ceases exploration environments histories. believe proof hard aixi become weakly asymptotically optimal exploration component added similarly section similar strong asymptotic optimality convergence must histories rather histories actually generated makes self-optimising property substantially stronger form optimality strong asymptotic optimality. proven exists self-optimising policy particular class aixi also self-optimising class limn→∞ proven existence weak self-optimising policy would imply aixi also weakly self-optimising. however policy deﬁned section cannot modiﬁed weak self-optimising property. must allowed choose actions itself. consistent work shows aixi cannot weakly asymptotically optimal cannot weak selfoptimising either. discounting. throughout paper assumed rewards discounted according summable discount function. natural alternative discounting goal agent simply maximise summed rewards. setting easy positive theorem lost negative ones still hold unfortunate discounting presents major philosophical challenge. choose discount function? assumptions/limitations. assumption ensures ﬁnite. negative results countable optimal policy section generalise countable also assumed bounded reward discrete time. ﬁrst seems reasonable second allows substantially easier analysis. additionally considered deterministic computable environments. stochastic case unquestionably interesting. invoked church thesis assert computable stochastic environments essentially largest class interesting environments. prove disprove existence weak asymptotically optimal stochastically computable policy discount function class deterministic computable environments. extend part theorem theorem complete classiﬁcation discount functions according whether admit weak asymptotically optimal policy class computable environments.", "year": 2011}