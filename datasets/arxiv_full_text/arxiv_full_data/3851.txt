{"title": "Natural Language Processing with Small Feed-Forward Networks", "tag": ["cs.CL", "cs.NE", "68T50", "I.2.7"], "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "text": "show small shallow feedforward neural networks achieve near state-of-the-art results range unstructured structured language processing tasks considerably cheaper memory computational requirements deep recurrent models. motivated resource-constrained environments like mobile phones showcase simple techniques obtaining small neural network models investigate different tradeoffs deciding allocate small memory budget. deep recurrent neural networks large network capacity become increasingly accurate challenging language processing tasks. example machine translation models able attain impressive accuracies models hundreds millions billions parameters. models however feasible computational settings. particular models running mobile devices often constrained terms memory computation. long short-term memory models achieved good results small memory footprints using character-based input representations e.g. part-of-speech tagging models gillick roughly parameters. latency however still issue lstms large number matrix multiplications require rush report speeds words/second running two-layer lstm translation system android phone. feed-forward neural networks potential much faster. paper show small feed-forward networks achieve results near state-of-the-art variety natural language processing tasks order magnitude speedup lstm-based approach. begin introducing network model structure character-based representations throughout tasks four tasks address language identiﬁcation part-of-speech tagging word segmentation preordering translation. order feed-forward networks structured prediction tasks transition systems feature embeddings proposed chen manning introduce novel transition systems last tasks. focus budgeted models ablate four techniques improving accuracy given memory budget memory needs dominated embedding vgdg vocabulary sizes dimensions respectively feature group runtime strongly inﬂuenced hidden layer dimensions. hashed character n-grams previous applications network structure used word embeddings represent words however word embeddings effective usually need cover large vocabularies dimensions inspired success character-based representations features deﬁned character n-grams instead relying word embeddings learn embeddings scratch. distinct feature group ngram length control size directly applying random feature mixing deﬁne feature value n-gram string well-behaved hash function. typical values range smaller exponential number unique n-grams. consequence small feature vocabularies also small feature embeddings typically dg=. quantization commonly used strategy compressing neural networks quantization using less precision store parameters compress embedding weights storing scale factors embedding contrast devoting model size higher training objective function combines cross-entropy loss model predictions relative ground truth regularization biases hidden layer weights. optimization mini-batched averaged stochastic gradient descent momentum exponentially decaying learning rates. mini-batch size ﬁxed perform grid search hyperparameters tuning task-speciﬁc evaluation metric held-out data early stopping. full feature templates optimal hyperparameter settings given supplementary material. experiment small feed-forward networks four diverse tasks language identiﬁcation part-of-speech tagging word segmentation preordering statistical machine translation. evaluation metrics addition standard task-speciﬁc quality metrics evaluations also consider model size computational cost. skirt implementation details calculating size number kilobytes needed represent model parameters resources. approximate computational cost number ﬂoating-point operations performed forward pass network given embedding vector cost dominated matrix multiplications compute activation unit values hence metric excludes non-linearities softmax normaltable language identiﬁcation. quantization allows trading numerical precision larger embeddings. models baldwin nearest neighbor nearest prototype approaches. bloom mapped word clusters well known word clusters powerful features linear models variety tasks here show also useful neural network models. however naively introducing word cluster features drastically increases amount memory required word-to-cluster mapping hundreds thousands entries several megabytes own. representing word clusters bloom key-value based generalization bloom ﬁlters reduce space required factor represent clusters word types. shown table best models accuate average across languages monolingual models using fewer parameters fewer flops. cluster features play important role providing relative reduction error vanilla model also increase overall size. halv. language identiﬁcation recent shared tasks code-switching dialects generated renewed interest language identiﬁcation. restrict focus single language identiﬁcation across diverse languages compare work baldwin predicting language wikipedia text languages. task obtain input separately averaging embeddings ngram length summation produce good results. moreover apply quantization embedding matrix without hurting prediction accuracy better less precision dimension dimensions. subsequent models quantization. noticeable variation processing speed performing dequantization on-the-ﬂy inference time. -dim lang-id model runs documents/second preprocessed wikipedia dataset. relationship compact language detector techniques back open-source compact language detector runs google chrome browsers. experimental lang-id model uses overall architecture uses simpler feature less involved preprocessing covers fewer languages. tagging apply model unstructured classiﬁer predict token independently compare performance byteto-span model -layer lstm network maps sequence bytes sequence labeled spans tokens tags. approaches limit feature embedding dimensions still gives reduction error trims overall size back vanilla model staying well total. halved model conﬁguration throughput tokens/second average. potential advantages require tokenized input accurate multilingual version achieving accuracy. memory perspective multilingual model take less space separate models. however runtime perspective pipeline models language identiﬁcation word segmentation tagging would still faster single instance deep lstm model flops estimate. segmentation word segmentation critical processing asian languages words explicitly separated spaces. recently neural networks signiﬁcantly improved segmentation accuracy structured model based transition system table similar proposed zhang clark conduct segmentation experiments chinese treebank recommended data splits. external resources pretrained embeddings used. hashing detrimental quality preliminary experiments hence task. learn embedding unknown characters cast characters occurring training special symbol. selected features using hashing here need careful size input vocabulary. neural network non-linearity theory able learn bigrams conjoining unigrams shown explicitly using character bigram features leads better accuracy zhang suggests embedding manually speciﬁed feature conjunctions improves accuracy -combo’ table however embeddings could easily lead model size explosion thus considered work. results table show spending memory budget small bigram embeddings effective larger character embeddings terms accuracy model size. model featuring bigrams runs text second tokens/second. preordering preordering source-side words target-side word order useful preprocessing task statistical machine translation propose novel transition system task repeatedly apply small network produce permutations. inspired non-projective parsing transition system system uses swap action permute spans. system sound permutations derivation input words permuted order complete permutations reachable training evaluation english-japanese manual word alignments nakagawa pipelines preordering experiment either spending memory budget reordering spending memory budget features predicted tags also requires additional neural network predict tags. full feature templates supplementary material. tagger network uses features based three word window around token another possibility features would affected token reorderer directly. table shows results without using predicted tags preorderer well including features used tagger reorderer directly training downstream task. preorderer includes separate network tagging extracts features predicted tags accurate smaller model includes features contribute reorderer directly. pipeline processes tokens/second taking pretokenized text input tagger accounting computation time. paper shows small feed-forward networks sufﬁcient achieve useful accuracies variety tasks. resource-constrained environments speed memory important metrics optimize well accuracies. large deep recurrent models likely accurate whenever afforded feed-foward networks provide better value terms runtime memory considered strong baseline. timothy baldwin marco lui. language identiﬁcation long short mathuman language technologies ter. annual conference north american chapter association computational linguistics pages association computational linguistics. deng zhao. neural word segmenproceedings tation learning chinese. annual meeting association computational linguistics pages berlin germany. association computational linguistics. danqi chen christopher manning. fast accurate dependency parser using neural netproceedings conference works. empirical methods natural language processing pages kuzman ganchev mark dredze. small statistical models random feature mixing. proceedings acl-hlt workshop mobile language processing pages association computational linguistics. gillick cliff brunk oriol vinyals amarnag subramanya. multilingual language processing bytes. proceedings naacl-hlt pages diego usa. association computational linguistics. song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv.. geoffrey hinton. practical guide training restricted boltzmann machines. neural networks tricks trade lecture notes computer science pages springer. yoon alexander rush. sequencelevel knowledge distillation. proceedings conference empirical methods natural language processing pages austin texas. association computational linguistics. wang ling tiago lu´ıs lu´ıs marujo ram´on fernandez astudillo silvio amir chris dyer alan black isabel trancoso. finding function form compositional character models open vocabulary word representation. proceedings conference empirical methods natural language processing pages association computational linguistics. yijia wanxiang jiang bing ting liu. exploring segment representations neural segmentation models. proceedings twenty-fifth international joint conference artiﬁcial intelligence ijcai york july pages shervin malmasi marcos zampieri nikola ljubeˇsi´c preslav nakov ahmed j¨org tiedemann. discriminating similar languages arabic dialect identiﬁcation report third shared task. proceedings third workshop similar languages varieties dialects pages osaka japan. coling organizing committee. giovanni molina fahad alghamdi mahmoud ghoneim abdelati hawwari nicolas reyvillamizar mona diab thamar solorio. overview second shared task language identiﬁcation code-switched data. proceedings second workshop computational approaches code switching pages austin texas. association computational linguistics. vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pages joakim nivre. non-projective dependency parsproceedings expected linear time. joint conference annual meeting international joint conference natural language processing afnlp pages joakim nivre marie-catherine marneffe filip ginter yoav goldberg hajic christopher manning ryan mcdonald slav petrov sampo pyysalo natalia silveira reut tsarfaty daniel zeman. universal dependencies multilingual treebank collection. proceedings tenth international conference language resources evaluation paris france. european language resources association wenzhe baobao chang. maxmargin tensor neural network chinese word segmentation. proceedings annual meeting association computational linguistics pages baltimore maryland. association computational linguistics. noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. arxiv preprint arxiv.. oscar t¨ackstr¨om ryan mcdonald jakob uszkoreit. cross-lingual word clusters direct transfer linguistic structure. conference north american chapter association computational linguistics human language technologies pages david talbot hideto kazawa hiroshi ichikawa jason katz-brown masakazu seno franz och. lightweight evaluation framework machine translation reordering. proceedings sixth workshop statistical machine translation pages stroudsburg usa. association computational linguistics. ivan titov james henderson. fast robust multilingual dependency parsing generative proceedings latent variable model. joint conference empirical methods natural language processing computational natural language learning pages ivan titov james henderson. latent variable model generative dependency parsing. harry bunt paola merlo joakim nivre editors trends parsing technology dependency parsing domain adaptation deep parsing pages springer netherlands dordrecht. joseph turian ratinov yoshua bengio. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics pages stroudsburg usa. association computational linguistics. david weiss chris alberti michael collins slav petrov. structured training neural network transition-based parsing. proceedings annual meeting association computational linguistics pages yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. corr abs/.. meishan zhang zhang guohong transition-based neural word segmentation. proceedings annual meeting association computational linguistics pages berlin germany. association computational linguistics. zhang stephen clark. chinese segmentation word-based perceptron algorithm. proceedings annual meeting association computational linguistics pages prague czech republic. association computational linguistics. values comprising generic embedding matrix ordinarily stored -bit ﬂoating-point precision implementation. quantization ﬁrst calculate scale factor embedding vector |eij| bias hence number bits required store embedding matrix reduced factor exchange storing additional scale values. inference time embeddings dequantized on-the-ﬂy. flops calculation product rp×q involves flops single relu hidden layer requires performing operation timestep here denotes size embedding vector equals respective models ordered table contrast lstm layer requires eight products timestep model four layers particular sequence-tosequence representation scheme gillick requires least four timesteps produce meaningful output individual input byte start length label predicted span. single timestep therefore relaxed lower bound number flops needed inference. word clusters frequent words large unannotated corpus clustered classes using distributed exchange algorithm procedure described appendix t¨ackstr¨om talbot talbot entry requires bits entropy distribution values number error bits employed. error bits assume uniform distribution values i.e. hence need bits entry entries. language identiﬁcation evaluation -gram embedding vectors dimensions depending experimental setting. hashed vocabulary sizes respectively. hidden layer size ﬁxed preprocess data removing non-alphabetic characters pieces markup text test time results empty string skip markup removal still results empty string process original string. procedure artefact wikipedia dataset documents contain punctuation trivial html code must make predictions render results directly comparable literature. small model comparison uses -grams byte unigrams n-grams embedding sizes byte unigrams dimensions. -dimension setting aforementioned dimensions halved table tagging feature templates. position relative focus token. value j-th byte start/end word. designates unicode character n-grams word. cluster word. table viii word segmentation feature templates. ‘β±i’ denotes starting i-th character left/right front buffer. denote character characterbigram respectively. table preordering feature templates. feature group applies positions given. denotes i-th span stack j-th span front buffer. within span tokens s...sls leftmost token rightmost. table segmentation optimal hyperparameter settings model segmentation experiments reported table columns show learning rate momentum factor step-frequency learning rate scaled number steps training stopped accuracy peaked held-out tuning data. column d.p. shows optimal dropout probability. feature templates used segmentation experiments listed table viii. besides deﬁne length feature number characters between front maximum feature value clipped length feature used segmentation models embedding dimension cutoff character character-bigrams order learn unknown character/bigram embeddings. hidden layer size ﬁxed feature templates preorderer look four spans stack ﬁrst four spans buffer; span feature templates look ﬁrst words last words within span. vanilla variant preorderer includes character n-grams word bytes whether span ever participated swap transition. features predicted tags words positions. table shows full feature templates preorderer. table xiii optimal hyperparameter settings language obtained experiments. columns meanings table ﬁnal column shows test accuracies back averages shown table", "year": 2017}