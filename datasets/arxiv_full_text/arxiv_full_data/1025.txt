{"title": "Sampling Generative Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.", "text": "introduce several techniques sampling visualizing latent spaces generative models. replacing linear interpolation spherical linear interpolation prevents diverging model’s prior distribution produces sharper samples. j-diagrams mine grids introduced visualizations manifolds created analogies nearest neighbors. demonstrate techniques deriving attribute vectors bias-corrected vectors data replication synthetic vectors data augmentation. binary classiﬁcation using attribute vectors presented technique supporting quantitative analysis latent space. techniques intended independent model type examples shown variational autoencoders generative adversarial networks. generative models popular approach unsupervised machine learning. generative neural network models trained produce data samples resemble training set. number model parameters signiﬁcantly smaller training data models forced discover efﬁcient data representations. models sampled latent variables high dimensional space called latent space. latent space sampled generate observable data values. learned latent representations often also allow semantic operations vector space arithmetic figure schematic latent space generative model. general case generative model includes encoder feature space high dimensional latent space. vector space arithmetic used latent space perform semantic operations. model also includes decoder latent space back feature space semantic operations observed. latent space transformation identity function refer encoding decoding reconstruction input model. generative models often applied datasets images. popular generative models image data variational autoencoder generative adversarial network vaes framework probabilistic graphical models objective maximizing lower bound likelihood data. gans instead formalize training process competition generative network separate discriminative network. though frameworks different construct high dimensional latent spaces sampled generate images resembling training data. moreover latent spaces generally highly structured enable complex operations generated images simple vector space arithmetic latent space generative models beginning academia creative applications. paper present techniques improving visual quality generative models generally independent model itself. include spherical linear interpolation visualizing analogies j-diagrams generating local manifolds mine grids. techniques combined generate dimensional embeddings images close trained manifold. used visualization creating realistic interpolations across latent space. also standardizing operations independent model type latent space different generative models directly comparable other exposing strengths weaknesses various approaches. additionally techniques building latent space attribute vectors introduced. labeled datasets correlated labels data replication used create bias-corrected vectors. synthetic attributes vectors also derived data augmentation unlabeled data. quantitative analysis attribute vectors performed using basis attribute binary classiﬁers. generative models often evaluated examining samples latent space. techniques frequently used random sampling linear interpolation. often result sampling latent space locations outside manifold probable locations. work followed useful principles sampling latent space generative model. ﬁrst avoid sampling locations highly unlikely given prior model. technique well established including used original paper adjusted sampling inverse gaussian accommodate gaussian prior second principle recognize dimensionality latent space often artiﬁcially high contains dead zones manifold learned training. demonstrated models implies simply matching model’s prior always sufﬁcient yield samples appear drawn training set. interpolation used traverse known locations latent space. research generative models often uses interpolation demonstrating generative model simply memorized training examples creative applications interpolations used provide smooth transitions decoded images. frequently linear interpolation used easily understood implemented. often inappropriate latent spaces generative models high dimensional gaussian uniform prior. space linear interpolation traverses locations extremely unlikely given prior. concrete example consider dimensional space gaussian prior random vectors generally length close however linearly interpolating usually result \"tent-pole\" effect magnitude vector decreases roughly midpoint standard deviations away expected length. proposed solution spherical linear interpolation slerp instead linear interpolation. formula introduced context great inbetweening rotation animations figure dcgan interpolation pairs identical endpoints uniform prior. pair series linear interpolation bottom spherical. note weak generations produced linear interpolation center present spherical interpolation. analogy shown capture regularities continuous space models. latent space linguistic models king woman results vector close queen technique also used context deep generative models solve visual analogies analogies usually written form generative models include encoder computing latent vectors given samples allow visual analogies. devised visual representation depicting analogies visual generative networks called j-diagram. jdiagram uses interpolation across dimensions expose manifold analogy. also makes symmetric nature analogy clear j-diagram also serves reference visualization across different model settings deterministically generated images held constant. makes useful tool comparing results across epochs training adjusting hyperparameters even across completely different model types figure j-diagram. three corner images inputs system left source analogy targets adjacent reconstruction resulting running image encoder decoder model. bottom right image shows result applying analogy operation images interpolations using slerp operator. generative models produce latent space tightly packed dimensionality latent space often artiﬁcially high. result manifold trained examples subset latent space training resulting dead zones expected prior. model includes encoder simple stay manifold using sample encodings latent space. useful diagnostic overly restrictive creative application context since prevents model suggesting novel samples model. however recover ability also including results operations encoded samples stay close manifold interpolation extrapolation analogy generation. ideally would mechanism discover manifold within latent space. generative models encoder ample out-of-sample data instead precompute locations manifold sufﬁcient density later query nearby points latent space known set. offers navigation mechanism based hopping nearest neighbors across large database encoded samples. combined interpolation call visualization manifold interpolated neighbor embedding mine grid useful visualize local patches latent space figure example local manifold built using celeba validation test images dataset sample features. resulting mine grid represents small contiguous manifold larger latent space. many generative models result latent space highly structured even purely unsupervised datasets combined labeled data attribute vectors computed using simple arithmetic. example vector computed represents smile attribute shorthand call smile vector. following smile vector computed simply subtracting mean vector images without smile attribute mean vector images smile attribute. smile vector applied positive negative direction manipulate visual attribute samples taken latent space approach building attribute vectors means labeled data noted suffer correlated labels many correlations would expected ground truths discovered others appear sampling bias. example male smiling attributes unexpected negative correlations women celeba dataset much likely smiling men. table breakdown celeba smile versus male attributes. total population smile attribute almost balanced separating data shows male attribute smile time without smile time. figure initial attempts build smile vector suffered sampling bias. effect removing smiles reconstructions also added male attributes using replication balance data across attributes computing attribute vectors gender bias removed online service setup automatically remove smiles images discovered gender bias visually evident results. solution replication training data dataset balanced across attributes. effective ultimately vectors simply summed together computing attribute vector balancing technique also applied attributes correlated ground truths. decoupling attributes allows individual effects applied separately. example attributes smiling mouth open highly correlated celeba training surprising physically people photographed smiling would also mouth open. however forcing attributes balanced construct decoupled attribute vectors. allows ﬂexibility applying attribute separately varying degrees figure decoupling attribute vectors smiling mouth open allows ﬂexible latent space transformations. input shown left reconstruction adjacent. noted samples drawn based models tend blurry possible solution would discover attribute vector unblur apply constant offset latent vectors decoding. celeba includes blur label image blur attribute vector computed extrapolated negative direction. found noticeably reduce blur also resulted number unwanted artifacts increased image brightness. concluded result human bias labeling. labelers appear likely label darker images blurry unblur vector found suffer attribute correlation also lightened reconstruction. bias could easily corrected celeba include brightness label rebalancing data. blurring attribute algorithmic solution available. take large images training process gaussian blur ﬁlter original image blurred image encoder subtract means compute attribute vector blur. call synthetic attribute vector label derived algorithmic data augmentation training set. technique removes labeler bias straightforward implement resulted samples closely resembling reconstructions less noticeable blur figure zoomed detail images validation reconstructed applying offset latent space based celeba blur attribute reduce noticeable blur reconstructions introduces visual artifacts including brightening attribute correlation. applying attribute vector instead computed synthetic blur yields images noticeably deblurred reconstructions without unrelated artifacts. results applying attribute vectors visually images often quite striking. would useful able evaluate relative efﬁcacy various attribute vectors across different models hyperparamaters. attribute vectors potential widely applicable outside images domain speciﬁc latent spaces provides additional challenges quantifying performance. example recent work shown ability latent space model continuous representation molecules though straightforward generate vectors attributes solubility spaces evaluation operations existing molecules would appear require speciﬁc domain knowledge. table average classiﬁcation accuracy celeba dataset. models atdot atdot unsupervised training attribute vectors computed training data model training completed. atdot lamb discriminative regularization disabled atvec dumoulin results kumar zhang lecun reported ehrlich figure example using attribute vector classiﬁcation. smile vector ﬁrst constructed latent space labeled training data. taking product smile vector encoded representation face scalar smile score generated score basis binary classiﬁer. histogram shows distribution positive negative scores celeba validation curve smile vector based binary classiﬁer also shown found effective classiﬁers built latent space using product encoded vector computed attribute vector named technique building classiﬁers latent spaces atdot models trained unsupervised data shown produce strong results celeba across attributes importantly binary classiﬁers provide quantitative basis evaluate attribute vectors related surrogate task ability basis simple linear classiﬁer. task established paradigms machine learning provides established tools curves offering insights behavior attribute vectors across different hyperparameters different models software support techniques presented paper included python software library used various generative models. hope continue improve library techniques applicable across broad range generative models. attribute vector based classiﬁers also offer promising evaluate suitability attribute vectors domains outside images. investigating constructing specially constructed prior latent space interpolations could linear. would simplify many latent space operations might enable types operations. given sufﬁcient test data extent encoded dataset deviates expected prior quantiﬁable. developing metric would useful understanding structure different latent spaces including probability random samples fall outside expected manifold encoded data. thankful constructive feedback readers including ehud ben-reuven zachary lipton alex champandard. thank victoria university wellington school design supporting research creative intelligence. also thank vibrant machine learning creative coding communities twitter support encouragement.", "year": 2016}