{"title": "Unitary-Group Invariant Kernels and Features from Transformed Unlabeled  Data", "tag": ["cs.LG", "cs.AI"], "abstract": "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods.", "text": "study representations invariant common transformations data important learning. techniques focused local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. paper study kernels invariant unitary group theoretical guarantees addressing practical issues unavailability transformed versions labelled data observing transformations. present theoretically motivated alternate approach invariant kernel svm. unlike previous approaches invariant proposed formulation solves issues mentioned. also present kernel extension recent technique extract linear unitary-group invariant features addressing issues extend guarantees regarding invariance stability. present experiments datasets illustrate validate methods. becoming increasingly important learn well generalizing representations invariant many common transformations data. transformations give rise many ‘degrees freedom’ even constrained task face recognition fact explicitly factoring leads improvements recognition performance found leibo hinton study invariant features important. anselmi showed features explicitly invariant intra-class transformations allow sample complexity recognition problem reduced. prior invariant kernels. kernel methods machine learning long studied considerable depth. nonetheless study invariant kernels techniques extract invariant features received less attention. invariant kernel allows kernel product remain invariant transformations inputs. work incorporating invariances popular machinery lauer bloch instances incorporating invariances focused local invariances regularization optimization schlkopf decoste sch¨olkopf zhang techniques jittering kernels decoste sch¨olkopf tangent-distance kernels sacriﬁced positive semideﬁnite property kernels computationally expensive. haasdonk burkhardt ﬁrst used group integration arrive invariant kernels however approach address important problems arise practice shortly state problems concretely show invariant kernels proposed fact solve problems. prior invariance dataset augmentation. many approaches past enforced invariance generating transformed training samples form poggio vetter sch¨olkopf smola schlkopf niyogi reisert haasdonk burkhardt assumes knowledge transformation. approach presented paper however unitarity assumption learn transformations unlabelled samples need training dataset augmentation. perhaps popular method incorporating invariances svms virtual support method schlkopf used sequential runs svms order augment support vectors transformed versions themselves. loosli proposed similar algorithm generate prune examples. though methods success still lack explicit theoretical guarantees towards invariance. proposed invariant kernel formulation hand guaranteed invariant. further unlike approaches incorporate invariance proposed invariant kernel solves common important practical problems state shortly. best knowledge ﬁrst formulation prior linear invariant features. recently anselmi proposed linear groupinvariant features explanation multiple characteristics visual cortex. achieve invariance slightly general group integration utilizing measures distribution characterizing orbit sample action group. extend method rkhs using unitary kernels extend properties regarding invariance stability. also show extension solve motivating problems leads practical extracting non-linear invariant features theoretical guarantees. motivating problems. state central problems paper tries address invariant kernels features. common practical problem faces utilizing previous methods involving generating transformed samples computational expense generating processing further many cases transformed labelled samples unavailable.two important problems arise practically applying invariant kernels features problem transformed versions training labelled data available i.e. might access transformed versions unlabelled data outside training e.g. unlabelled transformed images observed. problem members group transformations observed i.e. group partially observed actions e.g. transformations image observed. many practical cases partial invariance fact necessary transformation class another exists. group theory invariance. towards goal study incorporating invariance group integration seems useful. group theory elegant model symmetry. classical invariant theory provides group integration techniques enforce invariance. group integration also used model mean pooling implicit several areas machine learning computer vision. transformations paper modelled unitary collectively form unitarygroup classes learning problems vision often transformations belonging unitary-group would like invariant towards results also extended discrete groups. practice however liao found invariance much general transformations captured model achieved. given explicit access theoretically capitalize properties guaranteed global invariance however controlled local invariance also achieved. local invariance important extreme transformation class overlaps another. unitary property group unitary restriction kernels allow development theoretical motivation existing techniques invariant kernel invariant kernel features theoretically addressing problems contrast many previous studies invariant kernels focus study positive semi-deﬁnite unitary-group invariant kernels features guaranteeing invariance address problem problem using proposed invariant kernel present theoretically motivated alternate approach designing non-linear invariant handle problem problem explicit invariance guarantees. propose kernel unitary-group invariant feature extraction techniques extending theory linear group invariant features presented anselmi show kernel extension addresses problem problem preserves properties global invariance stability. organization. paper broadly organized parts. section present proposed invariant kernels invariant kernel whereas section present proposed invariant features extracted using kernels. section ﬁrst present important known elementary unitary-group integration properties present central result applying group integration rkhs section present theoretically motivated alternate approach designing non-linear invariant present simple albeit important result reduce computation. section continue develop invariant kernel require observe transformed versions input arguments whatsoever. section section propose kernel unitary-group invariant feature extraction techniques extending linear invariant feature extraction method kernel domain. show resultant feature addressing problem preserves important properties global invariance stability. section show simple extension method help solve problems leads practical extracting invariant non-linear features theoretical guarantees. premise consider dataset normalized samples along labels ...n introduce dataset number unitary transformations part locally compact unitary-group augmented normalized dataset becomes {gxi thus assume known accessible completely. mapping high dimensional hilbert space i.e. points mapped problem learning separator space assumed linear. method generating invariant towards group group integration. group integration stemmed classical invariant theory foundational theorem proved haar. theorem every locally compact group exists least left invariant integral. integral unique except strictly positive factor proportionality. case discrete ﬁnite groups group element would scaled compact groups integral converges bounded function group. discrete groups integral replaced sum. group integration shown projection onto g-invariant subspace. subspace deﬁned hilbert space invariant group generated following basic known property based group integration. haar measure exists every locally compact group unique upto positive multiplicative constant similar property holds discrete groups. invariance property results global invariance group property allows generate g-invariant subspace inherent space sample complexity generalization. applying operator dataset points point g-invariant subspace. theoretically would drastically reduce sample complexity preserving linear feasibility trivial observe perfect linear separator learnt would also perfect separator thus theory achieving perfect generalization. prove similar result rkhs case section property theoretically powerful since cardinality large. classiﬁer avoid observe transformed versions {gx} generalize. group integration provides exact invariance domain however requires group structure preserved. context kernels imperative group relation samples preserved kernel hilbert space corresponding kernel restriction unitary possible. present elementary albeit important result allows deﬁning unitary kernels following sense. deﬁnition deﬁne kernel unitary kernel unitary group mapping satisﬁes unitary condition fairly general common class unitary kernels kernel. deﬁne operator unitary. thus mapping within rkhs. unitary following result. theorem unitary kernel sense deﬁnition unitary unitary-group theorem shows unitary-group structure preserved rkhs. provides theoretically motivated approaches achieve invariance rkhs. speciﬁcally theory invariance proposed utilize unsupervised linear ﬁlters also utilize non-linear supervised ‘templates’ discuss section reviewing maximum margin separator found minimizing loss functions hinge loss along regularizer. order invoke invariance utilize group integration kernel space using theorem points mapped given group integration results g-invariant using lemma introducing lagrange multipliers dual formulation becomes subspace within effectively observes samples xg}. known provides exact global invariance testing. further ψhω∗ maximum-margin separator {φ}. shown following result. theorem unitary group unitary kernel ψhω∗ perfect separator {ψhφ} {ψhφ ψhω∗ also perfect separator margin. further max-margin separator {ψhφ} also max-margin separator {φ}. invariant non-linear objective observes samples form obtains max-margin separator ψhω∗. theorem shows margins {ψhφ} deeply related implies max-margin separator datasets. theoretically invariant non-linear able generalize observing utilizing prior information form unitary kernels true practice linear kernels. non-linear kernels practice however invariant still needs observe integrate transformed training inputs. also present following result unitary-group invariant kernels helps saving computation. provide proof supplementary material. thus kernel invariant formulation replaced form ψhφi thereby reducing number transformed training samples required observed order magnitude. also allows kernel invariant orbit i.e. {gx} observing single arbitrary point orbit. nonetheless formulation stands still requires observing entire orbit atleast transformed training samples. however around fundamental problem show next section note general kernel gh-invariant subspace cannot explicitly computed important note testing however formulation invariant transformations test sample regardless linear non-linear kernel. also interestingly might different decision boundary g-invariant kernel form φdghi. preserves positive semi-deﬁnite property kernel guaranteeing global invariance unitary transformations. unlike jittering kernels decoste sch¨olkopf tangent-distance kernels wish include invariance scaling however would lose positive-semi-deﬁniteness nonetheless walder chapelle show conditionally positive deﬁnite kernels still exist transformations including scaling although focus unitary transformations paper. methods section partial invariance gives control degree invariance transformation groups allowing classes transformations another discriminated. relating virtual support vector method consider popular virtual support vector method support vectors augmented small number transformed versions themselves. assumes transformations explicitly known thereby failing address problem augmented training used train another improved invariance. show following section invariant formulation hand address problem group integration framework provides theoretical motivation since minimum suggests transformed versions support vectors. however different different transformed versions whereas group integration would force kernel g-invariant. linear kernels beneﬁts. group integration also suggests building explicit g-invariant subspace projecting training approach increase computation time allowing generalize g-transformed inputs. previous section introduced group integration approach invariant non-linear svm. kernel although formulation addresses problem address problem i.e. φdghi hψhφ ψhφi still requires observing transformed versions labelled input sample namely present approach require observation labelled training sample whatsoever. assume every sample exists vector s.t. arbitrary unlabelled arbitrary templates {ti} assume access transformed versions template i.e. observe following result. theorem unitary group template rd×m {ti} unitary kernel g-invariant kernel hψhφ ψhφi written theorem assumes points lies span allows kernel g-invariant i.e. achieves observing transformed versions unlabelled template useful since theorem solves problem guaranteeing invariance. further practice need explicit knowledge transformations. many cases simply store naturally transforming samples constructed kernel applied dataset directly provided group acts. coefﬁcients required theorem approximated projecting sample onto space spanned rkhs i.e. φ)−φt assumes kernel matrix invertible condition satisﬁed construction. invariant non-linear transformed unlabelled data invariant kernel objective using invariant kernel achieves invariance learning transformation observed unlabelled data. further need multiple runs opposed requires generation transformed labelled examples. theorem allows invariant kernel used directly without computational expense ﬁnding potential support vectors generating transformations processing added samples. further invariance helps reduce sample complexity improve performance given number samples phenomenon observe experiments. studied properties proposed unitary group invariant kernels. shift attention group invariant features. invariant kernels form invariant similarity measure used construct invariant feature maps. anselmi proposed linear invariant features enjoys properties global invariance stability. extend method rkhs using unitary kernels extend invariance stability properties. brieﬂy present theory invariance. orbit sample deﬁned straightforward albeit elegant observation orbit invariant since ogx. measures orbit also provide invariance high dimensional distribution induced group’s action fact anselmi show invariant unique i.e. denotes membership class. thus measures distribution ﬁnite number one-dimensional projections {phxtki}k used similarity measure orbits further measures invariant action unitary group unitary group normalized dot-products arbitrary template empirical estimate -dimensional distribution projection onto template expressed ηndg {...n non-linearity either estimate n-th practice liao n-th moment together deﬁne phxtki found even moments shown sufﬁciently invariant. ﬁnal signature feature vector templates rkhs behave transformed versions owing theorem therefore thus form transformed elements action invariance achieved using form equation anselmi extract non-linear kernel features single sample invariant group without ever needing observe also solves problem listed introduction. recall either estimate moments. case moments ﬁrst moment leads mean pooling inﬁnite moment results pooling. show kernel feature continues satisfy useful properties stability i.e. form stability result anselmi proved using similar analysis. theorem invariant unitary-group non-linearities lipschitz continuous constant maxn s.t. normalized unitary kernel this follows cramer-wold theorem along concentration measures. note even though features extracted non-linear invariance generated purely towards unitary kernel distance hausdorff sense i.e. maxgg′∈g good representation ideally stable distance points feature space bounded. unstable representations skew feature space allow degenerate results. theorem shows lipschitz continuity estimation functions kernel feature distance bounded kernel product. discriminative templates equation instantiated extract discriminative kernel features choosing discriminative instead arbitrary templates. group element train binary classiﬁers template labelled rest recall separator expressed form transformed templates action using partial invariance achieved equation extend notion partial invariance kernel features extracted similar equation following analysis anselmi partial invariance arises partially observing group i.e. observing ﬁnite group practice likely case. however partial invariance obtained observed subset local kernel feature also generalized locally compact groups. partially signiﬁcant changes since group structure preserved theorem summary partial orbits common point identical. invariance theorem anselmi applied modiﬁcation. theorem bijective positive functions locally compact group. further assuming supp further ||bυ −bυ|| thus achieve partial invariance provided limited number transformations unpractice since feature solves motivating problems mentioned section note invariant kernel i.e. objective coupled theorem practice able address problem problem whether kernel invariant features offer advantage linear invariant features refrain using discriminative kernel features since theoretical results assume structure templates. set-up method normalized datasets repository task. form random -fold cross-validation partition /testing dataset order enforce problem introduce number transformations belonging randomly chosen unitary transformations test data thereby multiplying test data size factor thus obtaining however augment training data instead generate random vectors templates augment using unitary transformations test data enforces problem problem inherently enforced large degree since practically difﬁcult generate entire group. transformations introduce subset unitary group i.e. kernel polynomial kernel degree compute inﬁnite moment equivalent max-pooling. evaluation estimate separability data train linear unaugmented data using features linear invariant features baseline) kernel test augmented corresponding fold test data extracting corresponding feature. also report test accuracy testing illustration classiﬁcation difﬁculty introduced transformations added results summarized table second experiment datasets generate random -fold partition. always train untransformed fold test transformed data train standard kernel invariant using kernel described theorem also test standard kernel untransformed data illustration classiﬁcation difﬁculty introduced transformations. results summarized table results ﬁrst observation almost datasets even modestly added transformations signiﬁcantly impaired svm’s performance thus conﬁrm difﬁculty problem learning arises presence inherent transformations relating different orbits data. secondly experiments explicitly generating invariance invariant features invariant kernel helps performance suggesting cases sample complexity lowered. invariant kernel features invariant kernel practice well address problem problem kernel features general modestly outperform linear features datasets since even though features non-linear transformation invariant linear. main handicaps applying invariant kernel methods computational expense generating processing additional transformed form data. further many cases difﬁcult generate samples transformation unknown. however many cases easier obtain transformed unlabelled samples invariant kernels described paper used address issues theoretically guaranteeing invariance. anselmi fabio leibo joel rosasco lorenzo mutch tacchetti andrea poggio tomaso. unsupervised learning invariant representations hierarchical architectures. corr abs/. http//arxiv.org/abs/.. leibo joel liao qianli poggio tomaso. subtasks unconstrained face recognition. international joint conference computer vision imaging computer graphics visigrapp loosli ga¨elle canu st´ephane bottou l´eon. training invariant support vector machines using selective sampling. large scale kernel machines press cambridge poggio vetter recognition structure model view observations prototypes object classes symmetries. laboratory massachusetts institute technology second equality group element since inner-product invariant using argument ψω′i hg′ω ψω′i. true using lemma fact unitary. further ﬁnal equality utilizes fact haar measure normalized. proof. hghφ ghφi since kernel unitary. deﬁne action thus mapping preserves dot-product reciprocating action requirements unitary operator however needs linear. note linearity derived linearity inner product preservation speciﬁcally arbitrary vector scalar ghg′ since therefore deﬁnition. also ghg′ thus closure established. associativity identity inverse properties proved similarly. therefore unitary-group", "year": 2015}