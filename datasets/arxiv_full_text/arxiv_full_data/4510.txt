{"title": "Model Selection Through Sparse Maximum Likelihood Estimation", "tag": ["cs.AI", "cs.LG"], "abstract": "We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l_1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l_1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright & Jordan (2006)), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.", "text": "consider problem estimating parameters gaussian binary distribution resulting undirected graphical model sparse. approach solve maximum likelihood problem added ℓ-norm penalty term. problem formulated convex memory requirements complexity existing interior point methods prohibitive problems tens nodes. present algorithms solving problems least thousand nodes gaussian case. ﬁrst algorithm uses block coordinate descent interpreted recursive ℓ-norm penalized regression. second algorithm based nesterov’s ﬁrst order method yields complexity estimate better dependence problem size existing interior point methods. using determinant relaxation partition function show algorithms used solve approximate sparse maximum likelihood problem binary case. test algorithms synthetic data well gene expression senate voting records data. undirected graphical models oﬀer describe explain relationships among variables central element multivariate data analysis. principle parsimony dictates select simplest graphical model adequately explains data. paper weconsider practical ways implementing following approach ﬁnding model given data solve maximum likelihood problem added ℓ-norm penalty make resulting graph sparse possible. many authors studied variety related ideas. gaussian case model selection involves ﬁnding pattern zeros inverse covariance matrix since zeros correspond conditional independencies among variables. traditionally greedy forward-backward search algorithm used determine zero pattern however computationally infeasible data even moderate number variables. introduce gradient descent algorithm account sparsity inverse covariance matrix deﬁning loss function negative likelihood function. recently huang considered penalized maximum likelihood estimation dahl proposed large scale methods problems sparsity pattern inverse covariance given must estimate nonzero elements matrix. another estimate graphical model neighbors node graph regressing variable remaining variables. vein dobra west employ stochastic algorithm manage tens thousands variables. also great deal interest using ℓ-norm penalties statistical applications. d’aspremont apply norm penalty sparse principle component analysis. directly related problem lasso tibshirani obtain short list neighbors node graph. meinshausen b¨uhlmann study approach detail show resulting estimator consistent even high-dimensional graphs. problem formulation gaussian data therefore simple. diﬃculty lies computation. although problem convex non-smooth unbounded constraint set. shall resulting complexity existing interior point methods number variables distribution. addition interior point methods require step compute store hessian size memory requirements complexity thus prohibitive higher tens. remainder paper organized follows. begin considering gaussian data. section problem derive dual discuss properties solution heavily weight ℓ-norm penalty problem. section present provably convergent block coordinate descent algorithm interpreted recursive ℓ-norm penalized regression. section present second alternative algorithm based nesterov’s recent work non-smooth optimization give rigorous complexity analysis better dependence problem size interior point methods. section show algorithms developed gaussian case also used solve approximate sparse maximum likelihood problem multivariate binary data using determinant relaxation partition function given wainwright jordan section test methods synthetic well gene expression senate voting records data. scalar parameter controls size penalty. penalty term proxy number nonzero elements often used albiet vector matrix variables regression techniques lasso. however number samples small compared number variables second moment matrix invertible. cases estimator performs regularization estimate always invertible matter small ratio samples variables sparse even many conditional independencies among variables distribution. trading maximality likelihood sparsity hope sparse solution still adequately explains data. larger value corresponds sparser solution data less well. smaller corresponds solution data well less sparse. choice therefore important issue examined detail section corresponds seeking estimate maximum worst-case likelihood additive perturbations second moment matrix similar robustness interpretation made number estimation problems support vector machines classiﬁcation. dual problem smooth convex. hundreds problem solved existing software uses interior point method complexity compute ǫ-suboptimal solution using related problem solved dahl compute maximum likelihood estimate covariance matrix sparsity structure inverse known advance. accomplished adding constraints form pairs speciﬁed set. constraint unbounded hope uncover sparsity structure automatically starting dense second moment matrix consider true unknown graphical model given distribution. graph nodes edge nodes missing variables independent conditional rest variables. given node denote connectivity component nodes connected node chain edges. would like choose penalty parameter that ﬁnite samples probability error estimating graphical model controlled. adapt work meinshausen b¨uhlmann follows. denote estimate connectivity component node context optimization problem corresponds entries nonzero. begin detailing algorithm. symmetric matrix a\\j\\k denote matrix produced removing column denote column diagonal element removed. plan optimize column variable matrix time repeatedly sweep columns achieve convergence. means that given second moment matrix chosen condition theorem column sparse maximum likelihood method estimates variable independent variables distribution. particular using work tseng possible show local convergence rate method least linear. practice found small number sweeps columns independent problem size suﬃcient achieve convergence. ﬁxed number sweeps cost method since iteration costs problem penalized least-squares problems known lasso. \\j\\j j-th principal minor sample covariance would equivalent penalized regression variable others. thus approach reminiscent approach explored meinshausen b¨uhlmann diﬀerences. first begin regularization consequence penalized regression problem unique solution. second importantly update problem data regression except ﬁrst update never minor sense coordinate descent method interpreted recursive lasso. section apply recent results nesterov obtain ﬁrst order algorithm solving lower memory requirements rigorous complexity estimate better dependence problem size oﬀered interior point methods. purpose obtain another algorithm found block coordinate descent fairly eﬃcient; rather seek nesterov’s formalism derive rigorous complexity estimate problem improved oﬀered interior-point methods. nesterov’s framework allows obtain algorithm complexity desired accuracy objective problem contrast complexity interior-point methods thus nesterov’s method provides much better dependence problem size lower memory requirements expense degraded dependence accuracy. here bounded closed convex sets diﬀerentiable convex linear operator. challenge write problem appropriate form choose associated functions parameters obtain best possible complexity estimate applying general results obtained nesterov function turns smooth uniform approximation everywhere. diﬀerentiable convex lipschitz-continuous gradient constant computed detailed below. speciﬁc gradient scheme applied smooth approximation convergence rate detail algorithm compute complexity must ﬁrst calculate parameters corresponding deﬁnitions choices above. first strong convexity parameter sense ready estimate complexity algorithm. step gradient smooth approximation computed closed form taking inverse step essentially amounts projecting requires solve eigenvalue problem. true step fact iteration costs number iterations necessary achieve objective absolute accuracy less given thus condition number ﬁxed advance complexity algorithm section consider problem estimating undirected graphical model multivariate binary data. recently wainwright applied ℓ-norm penalty logistic regression problem obtain binary version high-dimensional consistency results meinshausen b¨uhlmann apply determinant relaxation wainwright jordan formulate approximate sparse maximum likelihood problem estimating parameters multivariate binary distribution. show resulting problem gaussian sparse maximum likelihood problem therefore apply previously-developed algorithms sparse model selection binary setting. well-known diﬃculty partition function many terms outer compute. however determinant relaxation partition function developed wainwright jordan obtain approximate sparse maximum likelihood estimate. shall problem next section. particular means reuse algorithms developed sections problems binary variables. relaxation simplest oﬀered wainwright jordan relaxation tightened adding linear constraints variable synthetic experiments require generate underlying sparse inverse covariance matrices. ﬁrst randomly choose diagonal matrix positive diagonal entries. given number nonzeros inserted matrix random locations symmetrically. positive deﬁniteness ensured adding multiple identity matrix needed. multiple chosen large necessary inversion errors. simple approach obtaining sparse estimate inverse covariance matrix would apply threshold inverse empirical covariance matrix however even easily invertible diﬃcult select threshold level. solved synthetic problem size true concentration matrix density drawing samples plot figure sorted absolute value elements left right. clearly easier choose threshold level estimate. applying threshold either would decrease likelihood estimate unknown amount. observe preserve positive deﬁniteness threshold level must satisfy bound begin small experiment test ability method recover sparse structure underlying covariance matrix. figure shows sparse inverse covariance matrix size figure displays corresponding using samples. figure displays solution value penalty parameter chosen arbitrarily solution thresholded. nevertheless still pick features present true underlying inverse covariance matrix. using underlying inverse covariance matrix repeat experiment using smaller sample sizes. solve using arbitrarily chosen penalty parameter value display solutions figure expected ability pick features true inverse covariance matrix diminishes number samples. added reason choose larger value fewer samples figure recovering sparsity pattern small sample size. plot original inverse covariance matrix solution problem solution penalty parameter used figure path following elements solution increases. lines correspond elements zero true inverse covariance matrix; blue lines correspond true nonzeros. vertical lines mark range values using recover sparsity pattern exactly. figure shows path following examples. solve randomly generated problems size samples. lines correspond elements solution zero true underlying inverse covariance matrix. blue lines correspond true nonzeros. vertical lines mark ranges recover correct sparsity pattern exactly. note that theorem values greater shown solution diagonal. related note observe also works well recovering sparsity pattern matrix masked noise. following experiment illustrates observation. generate sparse inverse covariance matrix size described above. then instead using empirical covariance input randomly generated uniform noise size solve various values penalty parameter ﬁgure value shown randomly selected sample covariance matrices size computed number misclassiﬁed zeros nonzero elements solution plot average percentage errors well error bars corresponding standard deviation. shown error rate nearly zero average penalty equal noise level sense practical performance nesterov method block coordinate descent method randomly selected sample covariance matrices problem sizes ranging case number samples chosen third ﬁgure plot average time achieve duality times computed using athlon .ghz processor ram. section numerically examine ability sparse maximum likelihood method correctly classify elements inverse covariance matrix zero nonzero. comparision lasso estimate meinshausen b¨uhlmann shown perform extremely well. lasso regresses variable others time. upon obtaining solution variable estimate sparsity ways either declaring element ˆσij nonzero less conservatively either quantities nonzero noted previously meinshausen b¨uhlmann also derived formula choosing penalty parameter. lasso penalty parameter formulas depend chosen level bound error probability method. experiments following experiments ﬁxed problem size generated sparse underlying inverse covariance matrices described above. varied number samples value shown trials estimated sparsity pattern inverse covariance matrix using lasso-or lasso-and show sets plots. figure corresponds experiments true density plot power positive predictive value density estimated method. figure corresponds experiments true density high plot three quantities. meinshausen b¨uhlmann report that asymptotically lasso-and lasso-or yield estimate sparsity pattern inverse covariance matrix. ﬁnite number samples method seems fall methods terms power positive predictive value density estimate. typically oﬀers average lowest total number errors tied either lasso-and lasso-or. among lasso methods would seem true density slightly better conservative lasso-and. density higher better lasso-or. true density unknown achieve accuracy comparable lasso methods comparable computational complexity. however unlike lasso method parallelizable. parallelization would render lasso computationally attractive choice since variable regressed exchange oﬀer accurate estimate sparsity pattern well well-conditioned estimate covariance matrix itself. applied algorithms rosetta inpharmatics compendium gene expression proﬁles described hughes experiment compendium contains samples variables. view towards obtaining sparse graph replaced resulting penalty parameter figure closes region figure cluster genes unconnected remaining genes estimate. according gene ontology genes associated iron homeostasis. probability gene false included cluster next analyzed subset gene microarray dataset drug treated livers natsoulis study rats treated variety ﬁbrate statin estrogen receptor agonist compounds. taking genes highest variance replaced resulting penalty parameter ﬁrst order neighbors node gaussian graphical model form predictors variable. estimated obtained solving found receptor largest number ﬁrst-order neighbors gaussian graphical model. receptor believed mediators eﬀect statins estrogenic compounds cholesterol. table lists ﬁrst order neighbors receptor. perhaps surprising several genes directly involved either lipid steroid metabolism genes cbp/p known global transcriptional regulators. finally unannotated ests. connection receptor analysis provide clues function. conclude numerical experiments testing approximate sparse maximum likelihood estimation method binary data. data consists senate voting records data congress hundred variables corresponding senators. samples bill vote. votes recorded yes. these. experiment replaced missing votes noes chose penalty parameter according using signiﬁcance level figure shows resulting graphical model rendered using cytoscape. nodes correspond republican senators blue nodes correspond democratic senators. make tentative observations browsing network senators. neighbors democrats democrats republicans republicans. senator chafee democrats neighbors observation supports media statements made chafee years. senator allen unites otherwise separate groups republicans also provides connection large cluster democrats nelson senator lieberman connected democrats kerry running mate presidential election. observations also match media statements made pundits politicians. thus although obtained graphical model relaxation partition function resulting picture largely supported conventional wisdom. figure shows subgraph consisting neighbors degree three lower senator allen. indebted georges natsoulis interpretation iconix dataset analysis gathering senate voting records. also thank martin wainwright peter bartlett michael jordan many enlightening conversations.", "year": 2007}