{"title": "Coupling Adaptive Batch Sizes with Learning Rates", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Mini-batch stochastic gradient descent and variants thereof have become standard for large-scale empirical risk minimization like the training of neural networks. These methods are usually used with a constant batch size chosen by simple empirical inspection. The batch size significantly influences the behavior of the stochastic optimization algorithm, though, since it determines the variance of the gradient estimates. This variance also changes over the optimization process; when using a constant batch size, stability and convergence is thus often enforced by means of a (manually tuned) decreasing learning rate schedule.  We propose a practical method for dynamic batch size adaptation. It estimates the variance of the stochastic gradients and adapts the batch size to decrease the variance proportionally to the value of the objective function, removing the need for the aforementioned learning rate decrease. In contrast to recent related work, our algorithm couples the batch size to the learning rate, directly reflecting the known relationship between the two. On popular image classification benchmarks, our batch size adaptation yields faster optimization convergence, while simultaneously simplifying learning rate tuning. A TensorFlow implementation is available.", "text": "large-scale problems and/or large inefﬁcient impossible evaluate exact gradient typically resorts stochastic gradients randomly drawing mini-batch step optimization algorithm using gradient approximation mini-batch stochastic gradient descent variants thereof become standard large-scale empirical risk minimization like training neural networks. methods usually used constant batch size chosen simple empirical inspection. batch size signiﬁcantly inﬂuences behavior stochastic optimization algorithm though since determines variance gradient estimates. variance also changes optimization process; using constant batch size stability convergence thus often enforced means decreasing learning rate schedule. propose practical method dynamic batch size adaptation. estimates variance stochastic gradients adapts batch size decrease variance proportionally value objective function removing need aforementioned learning rate decrease. contrast recent related work algorithm couples batch size learning rate directly reﬂecting known relationship two. popular image classiﬁcation benchmarks batch size adaptation yields faster optimization convergence simultaneously simplifying learning rate tuning. tensorflow implementation available. step size parameter often called learning rate machine learning context. variants include adagrad adadelta adam restrict considerations paper. prohibitively costly compute estimated sample variance computed minibatch. described below require diagonal elements corresponding variances individual components. estimated following discussion addresses choice batch size single step assuming currently arbitrary ﬁxed point parameter space. notational convenience thus drop notation write cetera. aldynamic recent works. ready attracted attention decreasing friedlander schmidt series bounds gradient variance provably yield fast convergence rates constant learning rate showing increasing batch size replace decreasing learning rate. realize bounds practice propose increase batch size pre-speciﬁed constant factor iteration without adaptation gradient variance. likewise stochastic gradient computed randomly-drawn mini-batch random variable mean assuming composed samples drawn independently replacement covariance matrix practice batch size often ﬁxed value chosen simple empirical tests. actually crucial variable poses intricate trade-off affects optimizer’s performance. hand variance stochastic gradients decreases linearly small batches give vague gradient information thus slow convergence number optimization steps. hand cost step increases linearly thus linearly trade variance cost gradient variance linearly affect performance optimizer; effect depends local structure objective interacts parameter choices optimizer notably learning rate. general thus optimal batch size balances aspects. choosing good batch sizes important aspect design numerical optimizer. standard assumption analysis stochastic optimization algorithms setting overly restrictive bound fast gradient change moving parameter space. consequence change bounded exhibits clear relationship learning rate batch size. small batch sizes require small learning rate larger batch sizes enable larger steps. exploit relationship later explicitly coupling parameters. side note zero variance recover well-known condition guarantees convergence gradient descent deterministic case. obviously larger larger deterministic case optimal ignore computational cost. since cost scales linearly optimal batch size maximizes expected gain cost practical intuitive method descent direction criterion agnostic actual step taken depends learning rate addition direction moreover method introduces additional free parameter work strive alleviate issues resulting batch size adaptation rule stay close form spirit. somewhat related line research aims reduce variance stochastic gradients incorporating gradient information previous iterations current gradient estimate. notable methods svrg saga mini-batch methods since update gradient evaluations individual training examples however recent papers combine variance-reduced methods increasing sample sizes i.e. effective size training increased time. both sample size schedule pre-speciﬁed adapted runtime. note another recent line work non-uniform sampling training samples goal variance reduction orthogonal work since concerned composition batches rather size. cast problem ﬁnding good batch size maximizing lower bound expected gain computational cost individual optimization step. resulting rule similar form provides interpretation introduces explicit interaction learning rate. criterion subsequently simpliﬁed removing unknown quantities free parameters equation. result poses practical problems. first lipschitz constant unknown property objective function. even importantly difﬁcult reliably robustly estimate squared norm true gradient single batch. might tempted replace recovering criterion similar unbiased estimator true gradient norm equation shows. depending noise level intriguingly batch size second term introduce signiﬁcant bias. major advantage cabs rule emphasized name direct coupling learning rate batch size. established large learning rate demands large batches smaller cautious learning rate used smaller batches cabs rule explicitly reﬂects known relationship. using cabs thus seen tailoring noise level learning rate user chosen. show experimentally makes ﬁnding well-performing learning rate easier. apart considerations measure optimization progress. function value used cabs rule deﬁnition measure training progress. norm true gradient conveys similar information simply estimated previously noted. also investigated unbiased estimators correcting bias using variance estimate turned unreliable experiments. additionally equation also shows using denominator leads disadvantageous feedback larger batches cause become smaller expectation which turn leads larger batches according readers rightly worried change unit type replacing gradient norm function value helpful consider units measure quantities denote units parameters objective function respectively. gradient unit variance insight well-chosen learning rate driven quantities unit discussion). case gradient descent update −αk∇f would covariant i.e. independent units measure also evident newton’s method one-dimensional case newton update step ′′−f corresponding learning rate given inverse second derivative unit putting together right-hand side unit lastly cabs realizes bound gradient variance decreasing distance optimality similar theorem friedlander schmidt shown guarantee convergence constant non-decreasing learning rate. outline practical implementation cabs criterion. obviously neither known exactly individual step estimates quantities obtained mini-batch. straightforward objective variance estimate explained equation since estimates diagonal elements covariance matrix ksk. considerations practically compute found realize cabs criterion predictive manner meaning exact batch size satisﬁes single optimization step. achieve exact enforcement criterion byrd increase batch size small increment whenever criterion satisﬁed perform update. incremental computation introduces overhead increment small lead under-utilization computing resources. instead leverage observation gradient variance function value change slowly optimization step next allows current estimates batch size used next optimization step. also allows smoothing quantities multiple optimization steps. estimates fairly noisy especially small batch sizes. exponential moving averages obtain robust estimates. resulting batch size rounded nearest integer clipped minimal maximal batch sizes. minimal batch size avoids under-utilization computational resources small batches provides additional stability algorithm small-batch regime. maximal batch size necessary hardware limitations contemporary deep learning memory limits number samples processed once. implementation limit never reached experiments. note passing algorithmic batch size computational batch size principle independent—a future implementation could split algorithmic batch feasible computational batches necessary freeing algorithm hardware-speciﬁc constraints. algorithm provides pseudo-code. minimize sides respect lefthand side takes optimal value right-hand side gradient respect zero yields minimizer inserting back rearranging yields eliminate equation realizing that under scalar hessian assumption good learning rate corresponds newton step well optimal constant learning rate gradient descent given holds. hence assume well-chosen learning rate simpliﬁes ﬁnally arrive cabs rule drop based assumption non-negative loss holds standard loss functions like least-squares cross-entropy. case i.e. function value non-trivial upper bound distance optimality. optimum close zero good proxy uncommon denominator cabs rule small positive offset compared fundamentally alter implications long come close optimum usually case even modestly complex problems. general form used lieu access tighter lower bound e.g. prior experience similar problems. fact objective function includes additive regularization term suggest unregularized loss proxy form computation gradient variance adds non-negligible manageable computational cost. since duplicates half operations backward pass additional cost pinned roughly primarily implementation issue; cost could reduced implementing special matrix operations compute jointly. note evaluate denotes evaluation function value stochastic gradient variance estimate using mini-batch round clip rounds nearest integer clips provided minimal maximal values. individual gradients mini-batch accessible computed directly adding computational cost squaring summing gradients. unfortunately individual gradients available practical implementations backpropagation algorithm used compute gradients training neural networks. complete discussion technical issue beyond scope paper brieﬂy sketch solution. consider fully-connected layer neural network weight matrix rnl×nl+ forward pass matrix activations rm×nl propagated forward matrix multiplication aggregation individual gradients implicit matrix multiplication. practical implementations rely efﬁciency matrix operations even importantly infeasible store individual gradients memory number parameters high. evaluate proposed batch size adaptation method training convolutional neural networks four popular image classiﬁcation benchmark data sets mnist street view house numbers well cifar- cifar- small medium-scale problems contemporary standards exhibit many typical difﬁculties neural network training. opted benchmarks keep computational cost thorough evaluation method manageable compare constant batch sizes keep plots readable report results batch sizes main text; results batch sizes found supplements. also compare batch size adaptation based criterion used byrd since implementation details differ works combine batch size adaptation measures backtracking line search resort custom implementation said criterion. fair comparison realize similar manner cabs. criterion keeping predictive update mechanism batch size smoothing exponential moving averages rounding clipping exactly cabs implementation described algorithm method simply referred competitor remainder section. optimization process periodically evaluate training loss well classiﬁcation accuracy held-out test set. since method uses different batch size quantities tracked function number accessed training examples instead svhn next train digit classiﬁcation task street view house numbers data set. task similar mnist images larger exhibit real-world views digits house numbers partially clutter misalignment distracting digits sides. train convolutional layers ﬁlters size subsequent max-pooling windows stride followed fullyconnected layers units respectively. activation function relu layers. output layer units softmax activation cross-entropy loss. apply l-regularization perform data augmentation operations training inputs. cifar- cifar- finally train cnns cifar- cifar- data sets task classify pixel images object categories respectively. cifar crop images pixels train convolutional layers ﬁlters size figure results svhn. shared horizontal axis indicates number examples used training. middle panel depict evolution training loss test accuracy respectively color-coded different batch size methods optimal learning rate. bottom panel shows batch size chosen cabs. learning rate batch size method tuned maximum test accuracy given ﬁxed budget accessed training examples. tried candidates relevant range determined exploratory experiments. addition learning rate competitor method free parameter suggest setting highest possible noise tolerance default. experiments found performance method fairly sensitive choice thus tried report results best-performing choice. cabs analogous parameter tune. mnist start experiments well-known mnist image classiﬁcation task identifying handwritten digits pixel gray scale images. network convolutional layers ﬁlters subsequent max-pooling subsequent max-pooling windows stride followed fully-connected layers units respectively. activation function relu layers. output layer units softmax activation cross-entropy loss. perform data augmentation operations training set. svhn cifar- cifar- cabs yields signiﬁcantly faster decrease training loss curve contiuously lying others. also achieves best test accuracy methods three problems. margin secondbest method small svhn amounts noticeable percentage points cifar- even points cifar-. competitor method also chooses small batch sizes throughout. method does however surpass non-adaptive larger batch sizes terms speed contenders reach virtually test accuracy problem. cabs makes rapid progress initially seems choose unnecessarily large batch sizes later resulting high per-iteration cost evidently compensated higher learning rate enables conjecture cabs overestimates gradient variance homogeneous structure mnist data set; distribution gradients closely-centered outliers coordinate directions lead comparably high variance estimates. overall cabs outperforms alternative batch size schemes three four benchmark problems investigated beneﬁts seem increase complexity problem considering cabs batch size schedules depicted bottom panels figures common behavior seems cabs uses minimal batch size considerable portion training process increases approximately linearly afterwards. experiments cabs able speed training neural networks simplify tuning learning rate. contrast existing methods introduce additional free parameters. tensorflow implementation cabs found http//github.com/probabilisticnumerics/cabs. figure learning rate sensitivity svhn. families training loss curves cabs competitor constant batch size individual curve corresponds learning rate there proof identical equation main text. minimize sides inequality. left-hand side minimal value gradient respect right-hand side setting zero minimizer inserting back rearranging yields", "year": 2016}