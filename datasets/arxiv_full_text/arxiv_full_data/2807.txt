{"title": "Bayesian Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems", "text": "​bayesian neural networks christopher bishop ​bayesian training backpropagation networks hybrid monte carlo method radford neal ​weight uncertainty neural networks charles blundell julien cornebise koray kavukcuoglu daan wierstra google deepmind bnns comprised probabilistic model neural network. intent design combine strengths neural networks stochastic modeling. neural networks exhibit approximator universal called capabilities. allow direct specification model known interaction parameters generate data. prediction phase statistical models generate complete posterior distribution produce probabilistic guarantees predictions. thus bnns unique combination neural network stochastic models stochastic model forming core integration. bnns produce probabilistic guarantees it’s predictions also generate distribution parameters learnt observations. means parameter space deduce nature distribution neural network’s learnt parameters. characteristics make highly attractive theoreticians well practitioners. crux probabilistic modeling probability conditioning eminently expressed bayes rule shown above. assumed unobserved parameters define model. figure above represents observed data. probabilistic approach however conditioning assume parameters model distribution according prior belief. parameters data interact likelihood specification. observe data compute posterior parameter distribution likelihood product normalized probability data resulting distribution posterior distribution given observation note inference gives complete probability distribution dealing point estimates. traditionally probabilistic modeling conjugate practitioner likelihood posterior computed analytically form mathematical expression). example beta prior likelihood beta posterior. bernoulli conjugate following sample distributions discrete five powerball numbers random without drawn sixth replacement. powerball number drawn independently first five. various levels wins possible powerball jackpot winning numbers available here https//www.kaggle.com/scotth /powerball-numbers subsection part kaggle competition https//www.kaggle.com/dgaw lik/nyse focused make confident predictions next stock movement. data companies’ daily stock prices provided probabilistic modeling probabilistic modeling directly specifies model prior parameters model likelihood combined yield posterior. modeling assumes knowledge interaction parameters generating observed data. sense generative story model parameters combine specifiable yield likelihood. computed posterior. assume coin fair sees heads show coin toss could bayes rule compute posterior simply generalizing situation observes heads tails posterior calculation follows observation posterior expression analytically simplifiable prior functions fact interesting realistic scenarios closed form expression posterior. further computing expression numerically becomes difficult brute force evaluation choices compounds difficulty case multi-dimensional parameter spaces. cases approximation techniques used either sample estimates variational inference techniques. owever conjugacy requirement prior becomes constraint desirable. cases approximate posterior using sampling variational inference techniques. model specification phase includes specifying prior distribution parameters model exact nature combine yield likelihood function. model completely defined priors concealed hidden. similarly likelihood computable function. case coin toss uncertain coin’s fairness could establish distribution degree feels coin fair prior parameter compute likelihood training data. sees training data one’s prior beliefs updated based made. sake practitioners commercial educational libraries black step simple easy function families approaches consider sampling methods variational inference methods. sampling methods sampling techniques include mostly markov chain monte carlo family algorithms parameter space sampled proportion probabilities yield sample numerous advances made field since late notable ones include metropolis hastings algorithm hamiltonian monte carlo recently no-u-turn sampler metropolis hastings algorithm metropolis algorithm able sample posterior unnormalized measure product likelihood prior probability algorithm takes arbitrarily randomly stance sampling points parameter space could consider markov chain correlated adjacent states then states sampled yield samples desired posterior distribution although markov chain locally correlated possible make ergodic ergodicity requires consider transition probability distribution satisfy detailed-balance condition unnormalized measure probability detailed balance means transition parameter parameter sampling going reverse proof ​ergodicity follows detailed balance deceptively simple however concept quite deep. remains algorithm find transition distribution original authors discovered computationally easy proposal distribution following algorithmic steps transition next state current state ​proof yields ergodic sequence follows detailed balance property. simply candidate state accepted probable current state otherwise accepted part time dictated fraction parenthesis hamiltonian monte carlo hamiltonian monte carlo method exploits geometric properties typical posterior distribution. rather exploring typical means random walk lifts exploration problem nuts adaptive extension retains hmc’s efficiency requires hand tuning. powerful algorithm usefulness limited need tune step size parameter number steps selecting particularly problematic practitioners commonly rely heuristics based autocorrelation statistics preliminary runs core idea behind nuts trajectory begins turn back it’s time stop stop simulation running longer wouldn’t increase distance started phase space consists position momentum. relationship position momentum described hamilton’s equations considered total energy preserving. ability make confident strides parameter space thus converge faster metropolis hastings method also provides stronger guarantees resulting estimator parameters samples generated first sampling standard multivariate gaussian momentum parameter then leapfrog updates made arrive proposal momentum-position negative sign momentum required preserve time-reversibility. leapfrog-integrator constructed volume-preserving. together facts ensure proposal valid metropolis proposal. leapfrog-integrator shown conceptual understanding hamiltonian monte carlo​ betancourt source ​the no-u-turn sampler​ hoffman gelman source ​mcmc using hamiltonian dynamics​ radford neal maximization) optimization problem. variational parameterized introduced attempts posterior closely approximate variational family which chosen needs flexible enough represent wide variety distributions able capture posterior kullback-leibler divergence used measure closeness. applying definition divergence posterior arrive minimizing divergence equivalent maximizing evidence lower bound since unknown constant value equation above. probability data intractable constant value forces play maximization elbo expectation joint distribution entropy first term drives variational distribution place weights posterior distribution high. first term similar maximum posteriori estimate posterior. hand make variational second term attempts distribution diffuse place weights widely around it’s parameter space since intent doubling proceeds choosing direction uniformly random simulating hamiltonian dynamics leapfrog steps direction number previous doublings figures show trajectory dimensions evolves four doublings figures show evolution binary tree. example directions chosen forward backward backward forward talked selecting chose ​for nuts stochastic optimization vanishing adaptation specifically adaptation primal-dual algorithm nesterov​ proposed. maximize elbo could optimization techniques based gradient ascent. important realize elbo maximization objective convex involve computing global optimum. mean field variational inference makes simplifying assumptions model components parameters fully factorized independent parts thus allow simpler modeling regime. boils mathematically variational distribution z​i​ z​i​’s considered independent. despite expressive power variational inference still requires model implementations. automatic differentiation variational inference comes play. automatic inference advi builds automated solutions variational latent inference transforming space variables automating derivatives joint distribution. inputs probability model dataset outputs posterior inferences model’s latent variables. stochastic update techniques applied variational inference allow scale large datasets stochastic updates work using noisy estimators gradient iteration considering single data point. akin familiar popular stochastic gradient descent used optimizing objective functions neural networks. unconstrained real number valued random variables variational optimization problem defined transformed problem minimize latent variables defined real space advi single variational family probabilistic models. ​courtesy blei columbia university ​stochastic variational inference​ hoffman blei wang paisley ​stochastic primal-dual​ anatoli juditsky yuri nesterov figure plot likelihood data y-axis spread data input space given dataset size notice complex model little probability mass model capable representing spread data. contrarily simple model span small region input space thus apportion little probability measure data. model correct complexity span dataset able apportion large part it’s probability mass likelihood. natural expression occam’s razor principle falls line probabilistic modeling models work amount probability measure conventional machine learning computational models notion goodness bolstered regularization mechanism. deep learning models regularization obtained using weight decay parameter dropouts among techniques traditional machine learning models utilize held data sets validation data sets check overfitting. advi re-parameterizes gradient terms gaussian transforming within variational family. transformation allows efficient monte carlo approximations points worth noting advi cannot applied certain models intractability marginalizing discrete variables likelihoods models. include ising model sigmoid belief network nonparametric models. bayesian non-differentiable candidates advi. mathematical formulation objective advi illustrated ​proofs formulas​ section. asymptotic guarantees posterior distribution convergence independent quite advantageous conditions random variables finite probability space. guarantees identically distributed sample space assumption nevertheless perform experimentation various prior distributions especially uninformative priors uniform times even improper priors. flat priors typically posteriors robust changes prior. case highly note weights associated inputs neuron. shown figure input values weighted associated weight summed together. summed scalar value passed nonlinearity arrive output scalar value neuron. notwithstanding absolutely essential. model evidence used this since models higher marginal data likelihoods able account data adequately. ratio model evidences models called bayes factor used compare models. additional generating samples posterior distribution comparing original data. done using posterior predictive check samples closeness original data. usually metrics highest posterior density posterior predictive compared variance mean sample. supervised learning setting input vectors supplied network left. network weights let’s randomly initialized. network computes output neurons first layer propagates results second layer similar computation performed last layer. last layer network predicts particular value. supervised setting know label expected outcome. loss function computes deviation predicted network outcome expected outcome loss value back propagated layers network layer network adjusts weights extent contributed error output. mathematically application chain rule partial differentiation. differentiability order technique work. optimization algorithm stochastic gradient descent used compute loss back propagate weights neural network. activity performed repeatedly minimize loss successive round. termination criteria usually lowest observed cost separate cross-validation data network trained evaluated. loss cross validation dataset tends increase stops training network further. learnt model ready prediction test dataset. claims much output would vary varying particular input vector component input vector component feature). considered elements questions feature relevance important. correlation might inter-feature however without scaffolding inferences easily directly obtainable. importantly neural network deducible model world learnt used practitioner. weight vectors network arrangement direct correlation however individual statistical model. distribution weight parameters parameter space unknown; know point values termination training. nevertheless neural networks proven good image recognition computer vision tasks. sheer size parameter space order hundreds millions easily tell endowed large parameter space capabilities learn highly complex relationships input output. bayesian neural networks order combine best neural networks probabilistic modeling researchers innovated bnn. goal benefit bounds guarantees probabilistic modeling ability neural networks universal function approximators. bnns usually prior used describe parameters utilized input neural network. neural networks’ output utilized compute likelihood specific defined probability distribution. this computes posterior distribution parameters sampling variational inference. experiments attempted three different types problems using bayesian neural networks. used python library pymc experimentation. following description experimentation. code code repository github. powerball dataset powerball dataset consists list ticket numbers. winning powerball lottery ticket numbers first five numbers drawn random without replacement sixth number also called powerball number drawn independently random pool numbers draws twice week wednesdays saturdays evenings. ticket matches numbers jackpot. task wish predict probability number picked given powerball winning lottery ticket number whether possible predict whether number belongs classification problem. additionally interested certainty would assign predictions. bayesian neural networks automatically yield prediction certainties. plot obvious prediction uncertainty ball first number notice uncertainty increase move around uncertainty first matched uncertainty ball originating third reading farther right plot uncertainty ball first reduces curious observation high uncertainty number zero. pays close attention would notice zero valid number within training fact never observed. however force classifier make choice zero comes back extremely high uncertainty validates affirms faith model predictions. classification surface learnt different models various allow perceive functional differences. smoothness functional form classifiers evident boundary plots less sensitive input variations observed. hand weight posteriors wide bases highly sensitive input data encountered thus exhibit high variance. conclusion bayesian neural network exposes powerful techniques insights deep learning. enhances regular neural networks predictive uncertainties posterior network weight distributions handy feature learning model building. backbone probabilistic modeling. ingredient combines neural networks probabilistic modeling posterior approximation technique. sampling traditional approximation technique. contemporary method seen activity variational inference based great methods stochastic blackbox advi operator variational inference areas seen great resurgence research innovation offer great promises. bnns good principled approaches modeling exhibited results support particularly comparison vanilla deep learning models. suffer weaknesses. currently bnns computationally expensive sampling variational inference steps. bnns demonstrated competent moderately sized datasets fully explored vastly large datasets. heorem​ proposal probability distribution satisfies detailed-balance unnormalized probability measure necessarily means markov chain ergodic probability measure proof​ detailed balance implies computing probability successor state proportionally given integrating product probability transitioning remains positive semidefinite re-parameterize covariance matrix using cholesky factorization ll​t non-unique definition cholesky lives unconstrained space lower-triangular matrices real values allows negative diagonal elements. compute gradient inside expectation using automatic differentiation. order calculate expectation standard integration; drawing samples gaussian evaluating empirical mean gradients within expectation. gives noisy unbiased estimators elbo gradient differentiable probability model. stochastic gradients within optimization variational automate inference.", "year": 2018}