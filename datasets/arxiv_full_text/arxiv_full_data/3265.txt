{"title": "Collaborative Multi-sensor Classification via Sparsity-based  Representation", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose a general collaborative sparse representation framework for multi-sensor classification, which takes into account the correlations as well as complementary information between heterogeneous sensors simultaneously while considering joint sparsity within each sensor's observations. We also robustify our models to deal with the presence of sparse noise and low-rank interference signals. Specifically, we demonstrate that incorporating the noise or interference signal as a low-rank component in our models is essential in a multi-sensor classification problem when multiple co-located sources/sensors simultaneously record the same physical event. We further extend our frameworks to kernelized models which rely on sparsely representing a test sample in terms of all the training samples in a feature space induced by a kernel function. A fast and efficient algorithm based on alternative direction method is proposed where its convergence to an optimal solution is guaranteed. Extensive experiments are conducted on several real multi-sensor data sets and results are compared with the conventional classifiers to verify the effectiveness of the proposed methods.", "text": "temporal gait patterns sensor signals inputs classiﬁer. furthermore authors compared difi-fo approaches dataset showed higher discrimination performance fi-fo approach di-do counterpart. paper employ multi-sensor data perform classiﬁcation tasks fi-fo category using appealing sparse signal representation approach. sparse representation mainly based observation signals interest inherently sparse certain bases dictionaries approximately represented signiﬁcant components carrying relevant information proved efﬁcient many discriminative tasks detection classiﬁcation recognition rely crucial observation test samples belonging class usually low-dimensional subspace appropriate dictionaries. furthermore sparse representation allow capture prior known structures present data jungle thus minimize effects noise practical settings. opposed previous approaches problem single sensor used perform classiﬁcation study variety novel sparsityregularized regression methods commonly categorized collaborative multi-sensor sparse representation classiﬁcation effectively incorporates simultaneous structuredsparsity constraints demonstrated row-sparse and/or block-sparse coefﬁcient matrix within sensor across multiple sensors. furthermore robustify models deal different scenarios noises sparse noise low-rank signal-interference/noise. ﬁrst scenario frequently appears sensor data unpredictable uncontrollable nature environment data collection process. second scenario normally observed recorded data superimpositions target signals interferences signals external sources background noises data. interferences normally correlated structure appear low-rank signal-interference/noise. generally model lowrank interference appropriate multi-sensor datasets since sensors spatially co-located data samples temporally recorded thus interference external sources similar effect multiple sensor measurements. another extension collaborative model utilization sparse representation kernel induced feature space. kernel sparse representation well known robust many discriminative tasks since kernel-based methods exploit higher-order non-linear structure testing data linearly separable original space. abstract—in paper propose general collaborative sparse representation framework multi-sensor classiﬁcation takes account correlations well complementary information heterogeneous sensors simultaneously considering joint sparsity within sensor’s observations. also robustify models deal presence sparse noise low-rank interference signals. speciﬁcally demonstrate incorporating noise interference signal low-rank component models essential multi-sensor classiﬁcation problem multiple co-located sources/sensors simultaneously record physical event. extend frameworks kernelized models rely sparsely representing test sample terms training samples feature space induced kernel function. fast efﬁcient algorithm based alternative direction method proposed convergence optimal solution guaranteed. extensive experiments conducted several real multisensor data sets results compared conventional classiﬁers verify effectiveness proposed methods. multi-sensor fusion active research topic within context numerous practical applications medical image analysis remote sensing military target/threat detection applications normally face scenario data sampling performed simultaneously multiple co-located sources/sensors within small spatiotemporal neighborhood recording physical event. multi-sensor data collection scenario allows exploitation complementary features within related signal sources improve resulting signal representation. particular interest multi-sensor fusion classiﬁcation ultimate question take advantage related information different sources achieve improvement classiﬁcation performance. variety approaches proposed literature answer question methods mostly fall categories decision decision feature feature authors investigated di-do method vehicle classiﬁcation problem using data collected acoustic seismic sensors. proposed perform local classiﬁcation sensor signal conventional methods support vector machine local decisions incorporated maximum posterior estimator arrive ﬁnal classiﬁcation decision. fi-fo method studied vehicle classiﬁcation using visual acoustic sensors. method proposed extract paper ﬁrst propose multi-sensor joint sparse representation classiﬁcation imposes row-sparsity constraints within sensor across multiple sensors. model extended deal sparse noise low-rank interference next ms-gjsr+l model group sparse regularization integrated ms-jsr+l concurrently enforce block-sparse row-sparse constraints support coefﬁcients sensors. finally kernel mapping applied implicitly represent proposed models projected nonlinear feature space namely ms-kerjsr ms-kergjsr+l developed ms-jsr ms-gjsr+l respectively. advantages disadvantages methods discussed detail classiﬁcation problems multi-sensor classiﬁcation military projectiles different events using transient acoustic signals; multi-sensor border patrol classiﬁcation goal detect whether event involves footsteps human human leading animals. organized follows. section give brief overview sparse representation classiﬁcation. section introduces various proposed sparsity models based different assumptions structures coefﬁcient vectors sparse low-rank noise/interference. next section provides fast efﬁcient algorithm based alternating direction method multipliers review) solve convex optimization problems arise models. section extends framework non-linear kernel sparse representation. experiments conducted section conclusions drawn section vii. recent years witnessed explosive development sparse representation techniques signal recovery classiﬁcation. classiﬁcation literature well-known sparse representation-based classiﬁcation framework recently proposed based assumption samples belonging class approximately low-dimensional subspace. suppose given dictionary representing distinct classes rn×p feature dimension sample c-th class sub-dictionary columns {dddcp}p=...pc selected training resulting samples dictionary ddd. label test sample often assumed represented subset training samples ddd. mathematically written unknown sparse coefﬁcient vector noise imperfection test sample assumed low-energy. simplicity paper presence omitted model descriptions though still taken account ﬁdelity constraints penalized frobenious norm optimization process. model assumes coefﬁcients non-zeros others insigniﬁcant. particularly entries associated class test sample non-zeros thus sparse vector. classiﬁer seeks sparsest representation solving following problem |ai| employed promote -norm term sparsity structure coefﬁcient vector. sparsitydriven -based optimization extensively investigated literature single-measurement sparse representation shown efﬁcient classiﬁcation tasks provides effective approximate test sample training examples. however many practical applications often given test measurements collected different observations physical event. obvious question simultaneously exploit information various sources come precise classiﬁcation decision rather classifying test sample independently assigning class label simple fusion active line research recently focuses answering question using joint sparse representation mathematically given unlabeled test samples rn×t nearby spatio/temporal observations assume yyyt compactly represented atoms training dictionary rp×t unknown coefﬁcient matrix. joint-sparsity model sparse coefﬁcient vectors {aaat}t share support pattern thus row-sparse matrix non-zero rows. model extension aforementioned model multiple observations shown enjoy better classiﬁcation various practical applications well able reduce sample size needed signal reconstruction applications rowsparsity assumption holds. similarly sample corresponding physical event measured sensor belong class thus approximated training samples dddm different coefﬁcients i.e. dddmaaam aaam row-sparse matrix shares row-sparsity pattern active coefﬁcients within indexes induced class consequently concatenating coefﬁcient matrices combined matrix row-wise sparse hence recovered solving following q-regularized problem ms-jsr framework seen generalization multi-task multivariate sparse representation. fact source information used classiﬁcation inference ms-jsr model returns joint sparse representation presented previous section optimization simpliﬁes hand observation sensor optimization returns conventional multi-task lasso studied extensively literature might exist many different types unpredicted noises impulsive noise wind noise affecting true characteristics signal. unfortunately many cases noise sources uncontrollable arbitrarily large magnitudes sometimes dominate collected signal severely diminish classiﬁcation accuracy. obvious removing noises would possible improve overall performance result. fortunately type noises usually occupy certain frequency bands depending environment hence expect affects coefﬁcients cepstral feature domain case linear model observation -norm along columns. clear regularization encourages shared sparsity patterns across related observations thus solution similar sparse support distributions coefﬁcient vectors. previous section discussed general joint sparse representation framework ﬁtted single-sensor system classiﬁcation. scenario event captured multiple sensors simultaneously seek improve classiﬁcation accuracy exploiting existing correlation well complementary information among homogeneous heterogeneous sources. handle multiple sensors na¨ıve approach employ simple voting scheme sensor aforementioned classiﬁcation algorithms performed class label assigned. ﬁnal decision made selecting label majority-voting process. clear approach cannot exploit relationship different sources except decision level. section propose general framework called multisensor joint sparse representation classiﬁcation enforces joint sparsity prior sparse coefﬁcient vectors obtained different sensors’s data order make collaborative classiﬁcation decision. illustrate model similar notations deﬁne test samples dictionaries. consider multi-sensor system containing sensors used solve c-class classiﬁcation problem. suppose training samples sample different feature modalities. sensor dictionary condenote dddm here class sub-dictionary dddm rn×pc represents training data m-th sensor labeled c-th class. accordingly dddm denotes p-th training sample m-th sensor c-th class label. recall number training given test sample collected sensors sample subset sensor consists rn×t would observations local segment test signal segments obtained simultaneously partitioning test signal sensor segments shown fig. let’s ﬁrst consider representation sensor suppose belongs c-th class observed sensor reconstructed linear combination atoms dictionary ddd. dddaaa row-sparse matrix nonzero rows active merely within supports corresponding class component lllm dddmaaam lllm concatenating interference matrices coefﬁcient matrices becomes low-rank component still exhibits row-wise sparse property. coefﬁcient matrix low-rank interference component recovered jointly solving multi-sensor joint sparse representation low-rank interference optimization problem nuclear matrix norm lll∗ convex-relaxation version rank deﬁned singular values matrix weighting parameter balancing regularization terms. classiﬁer step slightly modiﬁed follows worth mentioning sparsity-based representation sparse noise active research area recent years works explored sparsity dictionary-based approach large dense noise/interference appearing low-rank. recursive projected compressed sensing method proposed recovery sparse representation large correlated dense noise assuming signal interference components changing slowly time relying heavily projecting estimated interference onto null space current signal subspace. mardani proposed learn low-rank compressed sparse matrices simultaneously apply detect anomalies trafﬁc ﬂows. model explore underlying structure among sparse coefﬁcient vectors developed mainly single re-constructive task. ms-jsr+l framework hand able deal lowrank interference also strengthens sparse representation signals different collective structures within across multiple sensors solve classiﬁcation problems. model capability extract low-rank approximation promoting sparsity level concatenated matrix aaa. moreover often applied case existing outliers data samples. words many cases sparse noise -regularization also covered convex low-rank minimization lll∗ resulting ﬂexible model ms-jsr+l. take data collected multi-sensor system presented paper example frequently observed sparse noise component appears nonzero rows nonzero columns essentially extracted lowrank nuclear-norm minimization. speciﬁc small fraction measurements {yyym grossly corrupted measurements {yyym affected certain locations decomposed dddmaaam lllm eeem idea exploiting sparse prior error developed wright context face recognition cand`es robust principal component analysis motivated works propose sparse representation method simultaneously performs classiﬁcation removes clutter noise. prior knowledge errors eeem’s sparse propose solve following optimization retrieve coefﬁcients am’s well errors eeem’s simultaneously matrix formed concatenating matrices eeem’s positive weighting parame eter trades factors cost function determined using -fold cross validation training data experiments. -norm deﬁned |eij| eij’s entries matrix eee. clear minimization encouraging sparsity entry-wise sparsity error eee. previous section discussed multi-sensor joint sparse representation model presence large-but-sparse noise. section study another model capable coping dense large correlated noise so-termed low-rank interference. speciﬁcally low-rank property mean singular values interference zeros close zeros number signiﬁcant singular values much smaller dimensions. scenario often happens external sources interfering recording process sensors. since sensors mounted onto common sensing platform recording physical events simultaneously similar interference sources picked across sensors promoting large low-rank corruption. interference sources include sound vibration passing helicopter hovering nearby interference radio-frequency source underlying background noise. multiple sensor system background portion signals recorded various sensors short span time especially sensors type located within small local area stationary hence raising low-rank background interference. similar ms-jsr+e model measurements collected single sensor composed linear representation dictionary dddm interference general ms-jsr framework joint-sparse constraint advocated sensors without taking care interference noise. note different regularization group constraint cannot case since otherwise optimization erroneously produce section propose fast algorithm solve proposed multi-sensor sparsity-based representation models. discussed aforementioned section ms-gjsr+l general method; hence discuss algorithm solve simplify algorithm generate solutions methods. model convex optimization problem. however presence multiple variables regularization constraints complicates optimization process. common technique tackle problem based variable splitting technique decouples variable variables classical alternating direction method multipliers iteratively solve multiple simpliﬁed subproblems. method shown particularly efﬁcient solving -norm minimization variable splitting technique allows break difﬁcult complex problem multiple sub-problems simpler closed-form solutions hence making complex minimization solvable. however together introducing variables computation iteration step also increases iterations likely required achieve convergence. section introduce alternative efﬁciently optimize without using variable splitting approach. method still relies admm introduces approximation step relieve burden dictionary transform guarantees convergence global optimal solution. augmented lagrangian function deﬁned aaacf lll∗ aaaq lagrangian multiplier positive penalty parameter. suggested admm method optimization consists minimizing respect variable time keeping others ﬁxed updating variables sequentially. algorithm formally presented algorithm contains small number non-zero columns rows. sparse matrix also viewed low-rank component hence summation +eee true low-rank interference outlier component also low-rank. therefore model problem seeking low-rank component ˜lll rowsparse matrix simultaneously. ms-jsr+l capability extract correlated noise/interference simultaneously exploiting inter-correlation multiple sensors coefﬁcient matrix enforcing rowlevel sparsity. moreover ms-jsr+l model extended incorporating group sparsity constraint coefﬁcient matrix aaa. idea adding group structure intensively studied empirically evaluated better represent signals several applications source separation face recognition concept normally beneﬁcial classiﬁcation tasks similar measurements represent signals event also come class hence tentatively select dictionary atoms corresponding class. leads group sparse representation dictionary atoms grouped labeled classes sparse coefﬁcients enforced active groups time. therefore desired classiﬁcation model enforces number active groups small also inside group rows forced active time resulting two-sparsity-level model group-sparse row-sparse combined cost function. tentatively apply concept ms-jsr+l model. model searches group-and-row sparse structure representation among sensors low-rank interference slimultaneously termed ms-gjsr+l concatenation aaac interpreted follows ﬁrst term aaaq encourages row-wise sparsity within among sensors; group regularizer deﬁned second term tends minimize number active groups coefﬁcient matrix aaa; third term accounts interference discussed previous section. consequently model promotes groupsparsity row-sparsity within group time parallel extracting low-rank interference appearing measurements together. solutions coefﬁcient matrix low-rank term recovered class label decided function positive proximal parameter gradient constant aaam. consequently replacing subproblem manipulating last terms component optimization update simpliﬁed derivation second line based separable structure note element-wise separable structure promoting column-separable properties norm perform separable structure separable structure respect rows i.e. aaaq aaacq row-concatenation matrix aaac’s. applying row-separable property ﬁrst third terms simplify solve sub-coefﬁcient matrix class separately algorithm explicitly utilizes approximation step overcome burden dictionary transform utility function eliminates decoupled auxiliary variables. furthermore guaranteed provide global optimum convex program stated following theorem whose proof presented appendix. theorem proximal parameter satisﬁes condition σmax ≤m≤m largest eigenvalue matrix {aaaj lllj} generated algorithm value penalty coefﬁcient converges multi-sensor kernel sparse representation sparse representation widely known efﬁcient method classiﬁcation test sample sparsely represented linear combination training samples original input domain. paper extend linear sparse representation nonlinear kernel domain show empirically kernel methods effective solution algorithm involves main subproblems solve intermediate minimizations respect variables iteration respectively. ﬁrst optimization subproblem updates variable recasted every deﬁne gggm dddmaaam gggm noted elementwise separable structure meaning operation matrix equal summation operations submatrices constitutes therefore second term proximal minimization reference therein proximal-based methods) solved singular value thresholding operator ﬁrst deﬁne singular value decomposition svd. intermediate solution lllj+ determined applying soft-thresholding operator singular values soft-thresholding operator element-wise deﬁned diagonal second subproblem update re-written subproblem convex utility function. unfortunately closed-form solution easily determined. difﬁculties come joint regularization row-sparse group-sparse variable also operation dictionary transformation dddmaaam well engagement multiple modalities fact subproblem alone general form several -based optimization problems. example restrict joint-sparse representation framework. hand exchange ﬁrst regularization still arrive collaborative hierarchical sparse modeling problem normally requires multiple-iteration algorithm achieve converged solution. order tackle difﬁculties solve exact solution instead third term objective function approximated taylor series expansion aaam second derivative order kernel domain experimentally shown section ms-kerjsr method represents signals appropriate nonlinear kernel domain signiﬁcantly improves classiﬁcation accuracy. next natural question robustify kernel model remains effective presence gross noise/interference. answer unfortunately noise models directly extended kernel domain. reason behind collected measurement linear superimposition target signal interference/noise time domain nonlinear kernel transformation completely deform interference/noise structure. example sparse noise component time domain become widespread dense noise non-linear transformation. existing works sparse kernel models therefore focused integrating appearance small dense noise section analytically demonstrated enforcing noise/interference low-rank structure critical beneﬁt multi-sensor problem. moreover low-rank reasonable assumption describe structure noise/interference even nonlinear kernel mapping. fact although kernel transformation distort interferences original forms time domain effects interferences multi-sensor test samples still analogous. dictionary-based description test samples adapted φφφaaam +lllm low-rank corruption proved perform better kernel domain many classiﬁcation tasks reason classes data linearly separable kernel methods used project data onto feature space classes become linearly separable denote kernel function deﬁned inner product implicit mapping maps vector onto higher dimensional space possibly inﬁnite. note general mapping function explicitly deﬁned rather characterized product functions. commonly used kernels include radial basis function gaussian kernel expxxxi xxxj used control width order-d polynomial kernel multi-sensor kernel joint sparse representation section exploit information different sensors kernel sparse representation improve classiﬁcation results. throughout section similar notations deﬁne test samples dictionaries multiple sensors section assumption representations nonlinear kernel-induced space test samples within sensor among different sensors share support sets. rn×t samples test mapping feature training samples feature space aaam row-sparse coefﬁcient matrix kernel sparse signal representation associated signals m-th sensor. recall coefﬁcient matrix seen discriminative feature classiﬁcation. thus incorporating information sensors propose collaboratively solve aaam’s following convex optimization q-norm imposed concatenated coefﬁcient matrix promotes shared sparse pattern across multiple columns aaa. clear information different sensors integrated collaborative classiﬁcation shared sparsity pattern matrices aaam’s. optimization called multi-sensor kernel joint sparse representation optimization implicitly solved feature space using kernel trick. means need explicitly express data feature space; rather evaluate kernel functions training points similar instead directly solving relax constraint left multiplying fig. sparse signal representation low-rank interference decomposition cepstral coefﬁcients measurement presented fig. using ms-gjsr+l algorithm. launch rocket impact. training testing sets generated randomly separating data sets equal sizes. overall test samples data sets respectively. real acoustic signals reported clean. therefore purposely interference various sources engine vent wind rain audio signals acoustic signal sensor. also independently additive white gaussian noise measurement four sensors awgn interference signal power ratio resemble sensor variance. clear synthetic interference component approximated rank- hence low-rank property. feature extraction. acoustic signals pre-processed detect time locations physical event occurs using spectral maximum detection method interference awgn placed acoustic signals extract cepstral features proved effective speech recognition acoustic signal classiﬁcation. power cepstrum signal formulated fourier transform captures rate signal change time respect different frequency band. discard ﬁrst cepstral coefﬁcient next coefﬁcients classiﬁcation. nutshell experiment. comparison methods verify effectiveness proposed approaches compare results several conventional classiﬁcation methods sparse logistic regression linear support vector machine competing methods incorporate information multiple sensors concatenating sensors’ training dictionaries form elongated dictionary rnm×p atoms evaluate performance proposed algorithms multi-sensor classiﬁcation experiments transient acoustic signal classiﬁcation launch/impact munition border patrol control classiﬁcation human/animal footsteps. ﬁrst experiment test samples simulated mixing real acoustic signals recorded four co-located sensors various external interference sources different power levels observe insights models especially models low-rank interference concept. second experiment deals signals collected system nine sensors four different signal types. accordingly demonstrate different practical beneﬁts methods gain fusing information multiple sensors; robustness lowrank interference; advantages learning prior structures class-speciﬁc manner; potential improvement using appropriate kernel representation. multi-sensor classiﬁcation transient acoustic signals experimental setup ﬁrst experiment perform classiﬁcations transient acoustic data collected launch impact types munitions mortar rocket using four-acousticsensor array sampling rate .hz. data sets collected different years namely considered experiment classiﬁcation problem four classes mortar launch mortar impact rocket fig. classiﬁcation results different interference types engine interference vent wind interference engine interference mixed wind-rain interference dictionary considered training samples used train classiﬁers. classiﬁers utilized test concatenated test segments voting scheme ﬁnally employed assign class label test signal. classiﬁcation results section demonstrate performance proposed multi-sensor sparsity-based methods classifying transient acoustic signals. purpose experiment understand algorithms examine effect low-rank interference term. therefore ms-jsr ms-jsr+l msgjsr+l models selected testing. addition classiﬁcation performance evaluated different signal-to-noiseratio levels parameter learning. classiﬁcation performance tied accuracy estimating parameters involving algorithms. experiment fold cross validation training samples learn parameters give best results. moreover plot classiﬁcation accuracy rates data sets respect weighting parameters encode low-rank group-structure information fig. seen algorithms sensitive close correlation data sets performance figs. show intuition example msgjsr+l mortar-impact test sample corrupted interference −db. fig. engine signal demonstrates four acoustic measurements corruptions time domain fig. exhibits sparsesignal representation low-rank interference decomposition performance cepstral feature domain. clear sparse representation term follows closely original clean signal four columns interference component correlated rendering low-rank property. appears low-rank interference well-captured suppressed corrupted observations. fig. shows classiﬁcation results achieved three proposed models averaged different runs experiment well competing methods data sets different interference sources. comparisons reported various interference levels snrs ranging −db. evident cases classiﬁcation results multi-sensor sparsitybased methods constantly outperform slr. furthermore noise/interference large well ms-jsr easily broken msjsr+l ms-gjsr still deliver moderately good results rendering low-rank interference optimization effective. multi-sensor classiﬁcation border patrol control experimental setup data collection. footstep data collection conducted using sets nine sensors consisting four acoustic three seismic passive infrared ultrasonic sensors days test subjects human human leading animals human footsteps include person walking person jogging people walking people running group people walking running; whereas human-animal footsteps include person leading horse people leading horse mule three people leading horse mule donkey group multiple people several dogs. make data practical test test subjects asked carry varying loads backpack metal pipe. addition test participants might include males females both. ideally would like discriminate human wild animal footsteps. however footstep data wild animals difﬁcult collect best data collection setup researchers u.s. army research laboratory performed. test subjects follow path sets nine sensors positioned return starting point. sensor sets placed meters apart path. total round-trip runs conducted days including runs human footsteps another runs human-animal footsteps. increase number test training samples consider trip going forward backward separate run. words total number runs doubled. data sets collected namely corresponding different days december segmentation. accurately perform classiﬁcation necessary extract actual events series. similar ﬁrst experiment identify location strongest signal response using spectral maximum detection method location segments overlap sides signals taken; segment samples corresponding seconds physical signal. process performed sensor data. overall nine signals captured nine sensors; signal divided overlapping segments thus formulations. fig. visually demonstrates sensing signals captured nine sensors segment ground-truth event person walking. observe different sensors characterize different signal behaviors. seismic signal shows cadences test person clearly difﬁcult visualize event sensors. ﬁgure note forth acoustic signal corrupted sensor failure collection process. segmentation extract cepstral features segment keep ﬁrst coefﬁcients classiﬁcation. feature dimension represented number extracted cepstral features comparison methods verify effectiveness proposed approaches compare results kernelized machines namely kerslr kersvm kernel versions kernel bandwidth selected cross validation. another effective method exploit information across sensors heterogeneous model proposed model called heterogeneous feature machine shown efﬁciency solving problems various modalities simultaneously employed. main idea model associate training data training dictionary dddm rn×p appropriate coefﬁcient vector aaam using sparsity joint sparsity regularization together logistic loss taken sensors. coefﬁcient vectors obtained segment test sample assigned class ﬁnal decision made selecting label occurs frequently. method also generalized kernel domain referred kerhfm experiments. aforementioned methods seen combination fi-fo di-do categories feature-level fusion across multiple sensors decision-level fusion observation segments test signal. although methods efﬁcient combining information different sensors clearly suboptimal combining information within sensor. example demonstrate suboptimality event fully exist observation segments fusing observation segments decision level probably result misclassiﬁcation. classiﬁcation results analysis section perform extensive experiments ninesensor data compare aforementioned methods verify effectiveness proposed models. variety different sensor types allows test various sensor combination setups including single sensor sensors type sensors different signal types order provide deeper understanding advantage disadvantages proposed method. presentation purpose number nine sensors sensors correspond four acoustic three seismic ultrasonic sensors respectively. methods combination sets sensors processed compared ﬁrst nine sets conducted separately using single sensor corresponding next sets combine multiple sensors various scenarios listed table noticed experimentation part testing data collected acoustic sensors completely corrupted malfunction sensors. clean acoustic sensors combinations signals types using three seismic sensors using four acoustic sensors respectively. utilizes acoustic seismic signals. evaluate effectiveness using four different types sensors including clean acoustic sensors three seismic sensor well ultrasonic sensors. ﬁnally nine sensors referred ﬁrst experiment data training data testing leads training testing samples. sensor corresponding training dictionary dddm constructed cepstral feature segments extracted training signals. experiments overlapping segments taken individual sensor signal. therefore training dictionary size associated observation size feature dimension. proposed methods based different assumptions structures sparse coefﬁcient vectors noise/interference linearity properties processed sensor sets determine joint coefﬁcient matrix class label determined corresponding minimal error residual classiﬁers. note also cross validation deﬁne regularization parameters. next different methods comparison performed sensor sets. classiﬁcation rates deﬁned ratios total number correctly classiﬁed samples total number testing samples expressed percentages plotted fig. validate efﬁciency proposed methods rerun experiments using data training data testing. again similar phenomenons observed seen fig. tabulate classiﬁcation performance proposed models well competing methods table iii- iii- taking testing samples respectively. second third columns table describe classiﬁcation accuracy using single sensor multiple sensors last column shows overall results averaging sensor sets. furthermore illustrate table detail classiﬁcation performance accommodates nine sensors interesting among sensor combinations. last three columns correspond classiﬁcation accuracy human human-animal footsteps overall accuracy respectively. figs. visualize proposed models solid lines clearly show out-perform competing methods presented dashed lines. especially observe distinct leading performance four frameworks ms-jsr+l ms-gjsr+l ms-kerjsr ms-kergjsr+l. moreover table points ms-gjsr+l exhibits best performance multiple sensors utilized mskergjsr+l achieves highest average classiﬁcation rate individual sensor used kernelized low-rank interference joint method also achieves best classiﬁcation rate averaging results examining sensor sets next take closer look better understanding summarizes main practical beneﬁts models. combining different sensors. obvious plots figs. signiﬁcant performance boost classiﬁcation results going sensor sets sensor sets demonstrating improvement incorporating multi sensors sparsity-based representation methods processing signals within sensor alone. quantitatively average improvements multiple sensors individual sensor range among methods. additionally plots also demonstrate sensor have even sensors different signal types better classiﬁcation performance whole classiﬁcation rates using information allinclusive sensors always among best closed best performance sets proposed models. low-rank interference. discussed section need develop model takes account noise unknown interfered signal low-rank component multi-sensor problem well formulate optimize effectively. empirical classiﬁcation results border patrol control dataset validate lowrank assumption. figs. model msjsr+e sparse noise constraint somewhat improves ms-jsr ms-jsr+l truly brings another layer robustness dataset. clearly reassures discussion noise/interference likely appears low-rank component system co-located sensors; turns critical approach multiple sensory problem. structured sparsity. throughout paper consider row-sparsity broad prior assumption enforce correlation/complementary information among homogeneous/heterogeneous sensors simultaneously accomplish signiﬁcantly enhanced results. moreover ms-gjsr+l model takes group structure coefﬁcient matrices accompanied prior homogeneously slightly better msjsr+l. cements conclusion group information beneﬁcial solving classiﬁcation counterparts. also fact ms-gjsr+l performs best multiple sensors used underlines broad effectiveness incorporating low-rank component interfered noise/signal even sensor corruption carefully choosing structural sparsity priors class-speciﬁc manner. kernelized sparse representation. another observation beneﬁt classifying signal kernel induced domain seen signiﬁcant improvement performance ms-kerjsr ms-jsr. notably kernel model executes well especially cases single sensor utilized. furthermore model ms-kergjsr+l integrates kernel low-rank information yields consistently good classiﬁcation rates single-sensor multi-sensor cases. combining results mskergjsr+l also offers best classiﬁcation rate averaging reduces optimal solution norm proximal operator w.r.t. suggested lemma sssf simple show sufﬁcient condition ˆxˆxˆx otherwise taking frobenious norm sides using simple derivations arrive optimal ˆxˆxˆx expressed proof theorem aaacf suppose ˆaˆaˆa ˆlˆlˆl} aaaq optimal solution optimization theory convex programming exists ˆzˆzˆz following conditions satisﬁed paper propose various novel sparsity models solve multi-sensor classiﬁcation problem exploiting information different signal sources well exploring different assumptions structures coefﬁcient vectors sparse low-rank interference signal non-linearity property. experimental results particular practical real data sets collected u.s. army research laboratory reveal several critical observations complementary information multiple sensors signiﬁcantly improves classiﬁcation results using single sensor; appropriate structured regularizations bring advantage selecting correct classiﬁcation labels hence increasing classiﬁcation rate; low-rank interference/noise critical issue multi-sensor fusion problem; classiﬁcation feature space induced kernel function yields compelling performance improvement. nevertheless techniques applied broader classiﬁcation discrimination problems data usually collected multiple co-located sensors. proof lemma section present brief proof lemma firstly present lemma well-studied closed form solution q-norm proximal operator lemma given matrix monotonically nonincreasing limj→∞ hence sequence {www converges. therefore sequences {aaaj} {zzz converges stationary points. consequently leads convergence {lllj}. suppose {aaaj lllj converges also optimal solution prove taking limitations using convergence sequences aaaj limj→∞ convexity lead ˜zzz furthermore ˜zzz ˜aaa ˜lll related equality dddm imply triple also satisﬁes optimal solution conditions optimization i.e. {aaaj lllj converges optimal solution klausner tengg rinner vehicle classiﬁcation multi-sensor smart cameras using featuredecision-fusion ieee conference distributed smart cameras cand`es j.romberg robust uncertainty principles exact signal reconstruction highly incomplete frequency information ieee trans. information theory vol. park dibazar berger cadence analysis temporal gait patterns seismic discrimination human quadruped footsteps ieee international conference acoustics speech signal processing apr. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. x.-t. yuan visual classiﬁcation multi-task joint sparse representation ieee computer society conference computer vision pattern recognition oct. wright dense error correction minimization ieee trans. information theory vol. jul. vaswani recursive sparse recovery large correlated noise annual allerton conference communication control computing mardani mateos giannakis recovery low-rank plus compressed sparse matrices application unveiling trafﬁc anomalies ieee trans. information theory vol. aug. y.-w. chao y.-r. y.-w. chen y.-j. y.-c. wang locality constrained group sparse representation robust face recognition ieee international conference image processing afonso bioucas-dias figueiredo fast image recovery using variable splitting constrained optimization ieee trans. image processing vol. sep. yang zhang alternating direction algorithms problems compressive sensing siam journal scientiﬁc computing vol. sch¨olkopf smola learning kernels support vector machines regularization optimization beyond. press shawe-taylor cristianini kernel methods pattern analysis.", "year": 2014}