{"title": "Extrapolating Expected Accuracies for Large Multi-Class Problems", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumptions that the classes are sampled identically and independently from a population, and that the classifier is based on independently learned scoring functions, we show that the expected accuracy when the classifier is trained on k classes is the (k-1)st moment of a certain distribution that can be estimated from data. We present an unbiased estimation method based on the theory, and demonstrate its application on a facial recognition example.", "text": "diﬃculty multi-class classiﬁcation generally increases number classes. using data subset classes predict well classiﬁer scale increased number classes? assumptions classes sampled identically independently population classiﬁer based independently learned scoring functions show expected many machine learning tasks interested recognizing identifying individual instance within large possible candidates. problems usually modeled multi-class classiﬁcation problems large possibly complex label set. leading examples include detecting speaker voice patterns identifying author written text labeling object category image examples algorithm observes input uses multiple practical challenges developing classiﬁers large label sets. collecting high quality training data perhaps main obstacle costs scale number classes. aﬀordable ﬁrst collect data small classes even long-term goal generalize larger set. furthermore classiﬁer development accelerated training ﬁrst fewer classes training cycle require substantially less resources. indeed interest small-set performance generalizes larger sets comparisons found literature natural question changing size label aﬀect classiﬁcation accuracy? consider pair classiﬁcation problems ﬁnite label sets source task label size target task larger label size label constructs classiﬁcation rule supposing task test example joint distribution deﬁne generalization accuracy label natural case performance extrapolation would deployment facial recognition system. suppose system developed database individuals. clients would like deploy system larger individuals. performance extrapolation could allow predict well algorithm perform client’s problem accounting diﬀerence label size. uses ﬁxed ﬁnite set. example categories classical caltech- image recognition data assembled aggregating keywords proposed students collecting matching images web. arbitrary nature label even apparent biometric applications labels correspond human individuals cases number labels used deﬁne concrete data therefore experimental choice rather property domain. despite arbitrary nature concretely analyze generalization accuracy although approximate characterizations label selection process often least partially manual. since assume label random generalization accuracy given classiﬁer becomes random variable. performance extrapolation becomes problem estimating average generalization accuracy agak i.i.d. dition i.i.d. sampling labels ensures separation labels obtained. also make assumption classiﬁers train separate model class. convenient property allows characterize accuracy classiﬁer selectively conditioning class time. paper presents main contributions related extrapolation. first present theoretical formula describing average accuracy smaller linked average accuracy label size show accuracy size depends discriminability function determined properties data distribution classiﬁer. second propose estimation procedure allows extrapolation observed average accuracy curve k-class data larger number classes based theoretical formula. certain conditions estimation method property unbiased estimator average accuracy. paper organized follows. rest section discuss related work. framework randomized classiﬁcation introduced section also introduce example revisited throughout paper. section develops theory extrapolation section suggest estimation method. evaluate method using simulations section section demonstrate method facial recognition problem well optical character recognition problem. section discuss modeling choices limitations theory well potential extensions. linking performance diﬀerent related classiﬁcation tasks considered instance transfer learning yang’s terminology setup example multi-task learning source task labeled data used predict performance target task also labeled data. applied examples transfer learning label another include oquab donahue sharif razavian however little theory predicting behavior learned classiﬁer label set. instead research classiﬁcation large label sets deal computational challenges jointly optimizing many parameters required models speciﬁc classiﬁcation algorithms gupta presents method estimating accuracy classiﬁer used improve performance general classiﬁers doesn’t apply diﬀerent sizes. theoretical framework adopt exists family classiﬁcation problems increasing number classes. framework traced back shannon considered error rate random codebook special case randomized classiﬁcation. recently number authors considered problem high-dimensional feature selection multiclass classiﬁcation large number classes works assume speciﬁc distributional models classiﬁcation compared general setup. however deal problem feature selection. perhaps similar method deals extrapolation classiﬁcation error larger number classes found trained classiﬁer identifying observed stimulus functional scan brain activity interested performance larger stimuli sets. proposed extrapolation algorithm heuristic little theoretical discussion. section interpret method within theory discuss cases performs well compared algorithm. example space assume exists prior distribution label space label exists distribution examples words example-label pair conditional distribution given given label sample training test set. training obtained sampling rtrain observations jtrain i.i.d. rtrain test likewise obtained sampling observations classiﬁer allowed depend training data convenient view random. write wish work classiﬁer random function likewise denote score functions whenever considered random. suppose specify random quantities classiﬁcation task. k-class average generalization accuracy classiﬁer expected value generalization accuracy iid∼ assoresulting random labels ciated score functions analysis want classiﬁer rely strongly complicated interactions labels set. therefore propose following property marginal separability classiﬁcation models figure classiﬁcation rule score functions three classes one-dimensional example space. bottom classiﬁcation rule chooses choosing maximal score function. means score function depend labels training samples. therefore considered drawn distribution classes compete selecting highest score constructing score functions. operation marginal classiﬁer illustrated figure estimated bayes classiﬁers primary examples marginal classiﬁers. density estimate example distribution label obtained empirical distribution ˆfy. then estimated density produce score functions notational remark. henceforth shall relax assumption classiﬁer based training set. instead assume exist score functions score functions independent test set. classiﬁer marginal independent tackling extrapolation useful discuss simpler task generalizing accuracy results target larger source set. suppose test data classiﬁcation task classes. label {y}k associated score functions well test observations would predicted accuracy randomly sampled special case estimated bayes classiﬁer obtained multivariate gaussian density mean covariance parameters estimated data. naive bayes estimated bayes classiﬁer obtained product estimated componentwise marginal distributions taking expectations test random labels expected value test accuracy agak. therefore special case provides unbiased estimator agak. next consider case consider label obtained sampling labels uniformly without replacement since unconditionally i.i.d. sample population labels test accuracy unbiased estimator agak. however better unbiased estimate agak averaging possible subsamples deﬁnes average test accuracy subsampled tasks atak. classiﬁer necessary. looking rank correct label allows determine many subsets result correct labels lower score classiﬁcation. speciﬁcally correct label therefore long classes labels labels lower score classiﬁcation correct. implies supposing draw labels classiﬁcation problem assign test instance correct label. test instance would drawn equal probability three conditional distributions illustrated figure bayes rule assigns class highest density illustrated standard normal sorted labels numerically computed iid∼ distributions randomly drawn labels illustrated figure mean distribution k-class average accuracy agak. theory presented figure example left joint distribution bivariate normal correlation right typical classiﬁcation problem instance bivariate normal model classes. figure generalization accuracy example distribution generalization accuracy bivariate normal model circles indicate average generalization accuracy agak; curve theoretically computed average accuracy. section organized follows. begin introducing explicit formula average accuracy agak. formula reveals agak determined moments one-dimensional function using formula estimate using subsampled accuracies. estimates allow extrapolate average generalization accuracy arbitrary number labels. similarly triplets independent identical distributions. speciﬁcally typically note test example therefore true label score function. sures probability score example going maximized particular score function compared random competitor formally write theorem expresses average accuracy weighted integral function essentially theoretical result allows reduce problem estimating agak estimating shall estimate data? propose using non-parametric regression purpose section standard normal cumulative distribution function. figure illustrates level sets function ux∗. highest values near line corresponding conditional mean moves farther line decays. note however large values result larger values since becomes unlikely exceed illuminating consider average accuracy curves functions vary change parameter higher correlations lead higher accuracy seen figure accuracy curves shifted upward increases favorability tends higher average well leads lower values cumulative distribution function–as figure function becomes smaller increases. known basis functions linear coeﬃcients estimated. since proposed method based linearity assumption refer classexreg meaning classiﬁcation extrapolation using regression. accurate extrapolation using classexreg depends good linear model true discriminability function however since function depends unknown joint distribution ideally would like model selection procedure choose obtains best root-mean-squared error extrapolation classes. approximation estimate rmse extrapolation source classes target classes means bootstrap principle. amounts resampling-based model selection approach perform extrapolations classes classes evaluate methods based closely predicted abak matches test accuracy atak. elaborate model selection procedure follows. simulations check proposed extrapolation method classexreg performs diﬀerent settings. results displayed figure varied number classes source data diﬃculty classiﬁcation basis functions. generated data according mixture isotropic multivariate gaussian distributions labels rationale behind radial basis model density mixture gaussian kernels variance control overﬁtting knots separated least distance largest knots size maximum knot absolute value proposed method extrapolating classiﬁcation accuracy larger number classes. method depends repeated kernel-density estimation steps. method brieﬂy motivated original text present notation. observed score comparing feature vector test example i’th class model trained j’th class feature-vector density wrong-class scores estimated smoothing observed scores kernel function bandwidth note method depends non-trivially smoothing bandwidth used density estimation step kernel bandwidth small compared number classes extrapolated accuracy upwardly biased. this consider feature vector correctly classiﬁed original class set. every unsmoothed empirical density therefore acck well. method relies smoothing extrapolation method described page supplement method described one-nearest neighbor classiﬁer setting test observation class taken liberty extending generic multi-class classiﬁcation problem. table maximum rmse across signal-to-noise-levels predicting classes multivariate gaussian simulation. standard errors computed nesting maximum operation within bootstrap properly account variance maximum estimated means. recommended using gaussian kernel bandwidth chosen pseudolikelihood cross-validation simulation tested implementation method using methods cross-validated estimation provided stats package statistical computing environment biased cross-validation unbiased cross-validation figure classexreg methods unbiased biased cross-validation perform comparably gaussian simulations. studied diﬃculty extrapolation relates absolute size number classes extrapolation factor within setting extrapolations times times times times number classes. within problem setting deﬁned number source target classes maximum rmse across signal-to-noise settings quantify overall performance method displayed table kernel-density method produces comparable results seen depend strongly choice bandwidth selection kde-ucv kdebcv show diﬀerent performance proﬁles although diﬀer method used choose bandwidth. also methods show signiﬁcant estimation bias seen figure explained fact method ignores bias introduced exponentiation. even unbiased estimator good estimate since figure correcting bias exponentiation helps greatly reduce overall bias. indeed classexreg shows comparable bias extrapolation bias well-controlled extrapolations. demonstrate extrapolation average accuracy data examples predicting accuracy face recognition large labels system’s accuracy smaller subset extrapolating performance various classiﬁers optical character recognition problem telugu script glyphs. face-recognition example takes data labeled faces wild data selected individuals least face photos. form data consisting photo-label pairs randomly selecting face photos individual. used openface order identify photo embedding feature extraction. obtain feature vector openface network guess telugu optical character recognition example consider three diﬀerent classiﬁers logistic regression linear support-vector machine deep convolutional neural network. full data consists classes training test observations class. create nested hierarchy subsampled data sets consisting subset classes uniformly sampled without replacement classes subset consisting classes uniformly sampled without replacement size- subsample. therefore study three diﬀerent prediction extrapolation problems note unlike case face recognition example assumption marginal classiﬁcation satisﬁed none classiﬁers. compare result model ground truth obtained using full data set. extrapolation results face recognition problem seen figure plots extrapolated accuracy curves method diﬀerent subsamples size seen three methods variances decrease rapidly increases. results diﬀer ranking rmses analagous simulation predicting accuracies around ﬁrst second column figure true accuracy lowest rmse belongs kde-ucv followed closely classexreg kde-bcv highest rmse. discrepancies could explained diﬀerences data distributions simulation face recognition example also fact access -class ground truth real data example. results telugu classiﬁcation displayed table rank three extrapolation methods terms distance ground truth accuracy consistent pattern rankings extrapolation -to- extrapolation. remarked simulation diﬃculty extrapolation appears primarily sensitive extrapolation ratio similar -to- -to- problems. settings classexreg comes closest ground truth deep kde-bcv comes closest ground truth logistic regression. however even logistic regression classexreg better comparably kde-ucv. -to- extrapolation highest extrapolation ratio none three extrapolation methods performs consistently well three classiﬁers. could case variability dominating eﬀect given small training making diﬃcult compare extrapolation methods using -to- extrapolation task. unlike face recognition example resample training classes here would require retraining classiﬁers–which would prohibitively time-consuming deep cnn. thus cannot table telugu extrapolated accuracies extrapolating classes telugu three diﬀerent classiﬁers logistic regression support vector machine deep convolutional network work suggest treating class classiﬁcation task random order extrapolate classiﬁcation performance small task expected performance larger unobserved task. show average moment distribution function. furthermore introduce algorithm estimating underlying distribution allows eﬃcient computation higher order moments. code methods simulations found https//github.com/snarles/classex. random sample homogeneous distribution inappropriate. many natural classiﬁcation problems arise hierarchically partitioning space instances labels. therefore rather modeling also discussed extrapolating classiﬁcation accuracy–or equivalently risk zero-one cost function. however possible extend analysis risk functions arbitrary cost functions subject forthcoming work. third direction exploration impose additional modeling assumptions speciﬁc problems. classexreg adopts non-parametric model discriminability function sense deﬁned spline expansion. however alternative approach assume parametric family deﬁned small number parameters. forthcoming work show certain limiting conditions well-described two-parameter family. substantially increases eﬃciency estimation cases limiting conditions well-approximated. thank jonathan taylor trevor hastie john duchi steve mussmann qingyun robert tibshirani patrick mcclure elidan useful discussion. supported graduate research fellowship would also like thank european research council", "year": 2017}