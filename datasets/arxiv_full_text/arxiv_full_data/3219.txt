{"title": "Analysis of classifiers' robustness to adversarial perturbations", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, namely their instability to adversarial perturbations (Szegedy et. al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on the families of linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured by the distinguishability). Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \\sqrt{d} (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed in the context of neural networks. To the best of our knowledge, our results provide the first theoretical work that addresses the phenomenon of adversarial instability recently observed for deep networks. Our analysis is complemented by experimental results on controlled and real-world data.", "text": "goal paper analyze intriguing phenomenon recently discovered deep networks namely instability adversarial perturbations provide theoretical framework analyzing robustness classiﬁers adversarial perturbations show fundamental upper bounds robustness classiﬁers. speciﬁcally establish general upper bound robustness classiﬁers adversarial perturbations illustrate obtained upper bound practical classes classiﬁers namely linear quadratic classiﬁers. cases upper bound depends distinguishability measure captures notion difﬁculty classiﬁcation task. results classes imply tasks involving small distinguishability classiﬁer considered robust adversarial perturbations even good accuracy achieved. theoretical framework moreover suggests phenomenon adversarial instability ﬂexibility classiﬁers compared difﬁculty classiﬁcation task moreover show existence clear distinction robustness classiﬁer random noise robustness adversarial perturbations. speciﬁcally former shown larger latter factor proportional linear classiﬁers. result gives theoretical explanation discrepancy robustness properties high dimensional problems empirically observed szegedy context neural networks. best knowledge results provide ﬁrst theoretical work addresses phenomenon adversarial instability recently observed deep networks. ﬁnally show experimental results controlled real-world data conﬁrm theoretical analysis extends spirit complex classiﬁcation schemes. state-of-the-art deep networks recently shown surprisingly unstable adversarial perturbations unlike random noise adversarial perturbations minimal perturbations sought switch estimated label classiﬁer. vision tasks results szegedy shown perturbations hardly perceptible human sufﬁcient change decision deep network even classiﬁer performance close human visual system. surprising instability raises interesting theoretical questions initiate paper. causes classiﬁers unstable adversarial perturbations? deep networks classiﬁers unstable behaviour? possible design training algorithms build deep networks robust instability adversarial noise inherent feature deep networks? quantify difference random noise adversarial noise? providing theoretical answers questions crucial order achieve goal building classiﬁers robust adversarial hostile perturbations. paper introduce framework formally study robustness classiﬁers adversarial perturbations binary setting. provide general upper bound robustness classiﬁers adversarial perturbations illustrate specialize obtained upper bound families linear quadratic classiﬁers. cases results show existence fundamental limit robustness adversarial perturbations. limit expressed terms distinguishability measure classes depends considered family classiﬁers. speciﬁcally linear classiﬁers distinguishability deﬁned distance means classes quadratic classiﬁers deﬁned distance matrices second order moments classes. classes functions upper bound robustness valid classiﬁers family independently training procedure fact bound independent training procedure strength. result following important implication difﬁcult classiﬁcation tasks involving small value distinguishability classiﬁer misclassiﬁcation rate vulnerable adversarial perturbations. importantly distinguishability parameter related quadratic classiﬁers much larger linear classiﬁers many datasets interest suggests harder adversarial examples ﬂexible classiﬁers. compare robustness adversarial perturbations linear classiﬁers traditional notion robustness random uniform noise. latter robustness shown larger former factor thereby showing high dimensional classiﬁcation tasks linear classiﬁers robust random noise even small values distinguishability. illustrate newly introduced concepts theoretical results running example used throughout paper. complement theoretical analysis experimental results show intuition obtained theoretical analysis also holds complex classiﬁers. phenomenon adversarial instability recently attracted attention deep network community. following original paper several attempts made make deep networks robust adversarial perturbations moreover distinct related phenomenon explored nguyen closer work authors provided empirical explanation phenomenon adversarial instability designed efﬁcient method adversarial examples. speciﬁcally contrarily original explanation provided szegedy authors argue linear nature deep nets causes adversarial instability. instead paper adopts rigorous mathematical perspective problem adversarial instability shows adversarial instability ﬂexibility classiﬁers compared difﬁculty classiﬁcation task. work confused works security machine learning algorithms adversarial attacks works speciﬁcally study attacks manipulate learning system well defense strategies counter attacks. setting signiﬁcantly differs ours examine robustness ﬁxed classiﬁer adversarial perturbations stability learning algorithms also deﬁned extensively studied again notion stability differs studied here interested robustness ﬁxed classiﬁers learning algorithms. construction learning algorithms achieve robustness classiﬁers data corruption active area research machine learning robust optimization references therein). speciﬁc disturbance model data samples robust optimization approach constructing robust classiﬁers seeks minimize worst possible empirical error disturbances shown that many disturbance models desired objective function written tractable convex optimization problem. work studies robustness classiﬁers different perspective; establish upper bounds robustness classiﬁers independently learning algorithms. using bounds certify instability class classiﬁers adversarial perturbations independently learning mechanism. words algorithmic optimization aspects robust classiﬁers studied works focus fundamental limits adversarial robustness classiﬁers independent learning scheme. paper structured follows sec. introduces problem setting. sec. introduce running example used throughout paper. introduce sec. theoretical framework studying robustness adversarial perturbations. following sections case studies analyzed detail. robustness linear classiﬁers studied sec. sec. study adversarial robustness quadratic classiﬁers. experimental results illustrating theoretical analysis given section proofs additional discussion choice norms measure perturbations ﬁnally deferred appendix. ﬁrst introduce framework notations used analyzing robustness classiﬁers adversarial uniform random noise. restrict analysis binary classiﬁcation task simplicity. expect similar conclusions multi-class case leave future work. denote probability measure data points wish classify label point distribution assumed bounded support. denote distributions class class respectively. arbitrary classiﬁcation function. classiﬁcation rule associated simply obtained taking sign performance classiﬁer usually measured risk deﬁned probability misclassiﬁcation according focus paper study robustness classiﬁers adversarial perturbations ambient space given datapoint sampled denote ∆adv norm smallest perturbation switches here norm quantify perturbation; refer reader appendix discussion norm choice. unlike random noise deﬁnition corresponds minimal noise perturbation sought estimated label justiﬁes adversarial nature perturbation. important note that figure illustration ∆adv ∆unif\u0001. line represents classiﬁer boundary. case quantity ∆adv equal distance line. radius sphere drawn around ∆unif\u0001. assuming observe spherical region line measure means probability random point sampled sphere label deﬁnition words ρadv deﬁned average norm minimal perturbations required estimated labels datapoints. note ρadv property classiﬁer distribution independent true labels datapoints moreover noted ρadv different margin considered svms. fact margins traditionally deﬁned minimal distance boundary training points ρadv deﬁned average distance boundary training points. addition distances case measured input space margin deﬁned feature space kernel svms. denotes uniform measure sphere centered radius words ∆unif\u0001 denotes maximal radius sphere centered perturbed points sampled uniformly random sphere classiﬁed similarly high probability. illustration ∆unif\u0001 ∆adv given fig. similarly adversarial perturbations point outside support general. note moreover ∆unif\u0001 provides upper bound ∆adv \u0001-robustness random uniform noise deﬁned introduce section running example used throughout paper illustrate notion adversarial robustness highlight difference notion risk. consider binary classiﬁcation task square images size images class contain exactly vertical line small constant positive number added pixels images. class images background pixels pixels belonging line equal fig. illustrates classiﬁcation problem number datapoints classify aspect deﬁnition slightly differs proposed szegedy deﬁnes robustness adversarial perturbations average norms minimal perturbations required misclassify datapoints. notion robustness larger upper bounds derived paper also directly apply deﬁnition robustness szegedy figure robustness adversarial noise linear quadratic classiﬁers. original image minimally perturbed image switches estimated label flin fquad. note difference hardly perceptible demonstrates flin robust adversarial noise. hand images clearly different indicates fquad robust adversarial noise clearly relevant concept permits separate classes orientation line bias image also valid concept task separates classes despite much difﬁcult detect visually. class image therefore correctly estimated orientation bias. ﬁrst consider linear classiﬁer deﬁned vector size whose entries equal vectorized image exploits difference bias classes achieves perfect classiﬁcation accuracy indeed simple computation gives important note flin achieves zero risk captures bias fails distinguish images based orientation line. indeed datapoints linearly separable. despite perfect accuracy flin robust small adversarial perturbations small minor perturbation bias switches estimated label. indeed simple computation gives ρadv √da; therefore adversarial robustness flin made arbitrarily small choosing small enough. that among linear classiﬁers satisfy flin maximizes ρadv therefore zero-risk linear classiﬁers robust adversarial perturbations classiﬁcation task. unlike linear classiﬁers ﬂexible classiﬁer correctly captures orientation lines images robust adversarial perturbation unless perturbation signiﬁcantly alters image modiﬁes direction line. illustrate point compare adversarial robustness flin second order polynomial classiﬁer fquad achieves zero risk fig. hardly perceptible change image sufﬁcient switch estimated label linear classiﬁer minimal perturbation fquad modiﬁes direction line great extent. risk adversarial robustness distinct properties classiﬁer. flin deﬁnitely robust small adversarial perturbations. fact flin captures bias images ignores orientation line. capture orientation classiﬁer ﬂexible enough task. unlike class linear classiﬁers class polynomial classiﬁers degree correctly captures line orientation robustness adversarial perturbations provides quantitative measure strength concept. since ρadv ρadv conﬁdently concept captured fquad stronger flin sense essence classiﬁcation task captured fquad flin general classiﬁcation problems quantity ρadv provides natural evaluate compare learned concept; larger values ρadv indicate stronger concepts learned comparable values risk. illustrated example robustness adversarial perturbations assess strength concept. real-world classiﬁcation tasks weak concepts correspond partial information classiﬁcation task strong concepts capture essence classiﬁcation task. introduce theoretical framework analyzing robustness adversarial perturbations. ﬁrst present assumption classiﬁer analysis adversarial robustness. assumption exist that dist max)γ dist max)γ words assumption states datapoint residual max) used bound distance datapoint classiﬁed bounds form established various classes functions since early work lojasiewicz algebraic geometry found applications areas mathematical optimization example lojasiewicz later shown that quite remarkably assumption holds general class analytic functions. shown hold piecewise linear functions. error bounds polynomial systems studied. proving inequality explicit constants different classes functions still active area research sections provide examples function classes holds explicit formulas parameters proof found appendix result provides upper bound adversarial robustness depends risk classiﬁer well measure separation expectations classiﬁer values computed distribution result general assume satisﬁes assumption next sections apply lemma classes classiﬁers derive interpretable upper bounds terms distinguishibality measure quantiﬁes notion difﬁculty classiﬁcation task. studying general result lemma practical classes classiﬁers shows implications fundamental limit adversarial robustness illustrates methodology deriving class-speciﬁc practical upper bounds adversarial robustness general upper bound. goal section two-fold; ﬁrst specialize lemma class linear functions derive interpretable upper bounds robustness classiﬁers adversarial perturbations then derive formal relation robustness linear classiﬁers adversarial robustness robustness random uniform noise figure adversarial robustness ρadv versus risk diagram linear classiﬁers. point plane represents linear classiﬁer illustrative diagram non-achievable zone exact ρadv versus risk achievable curve upper bound estimate running example. using lemma derive interpretable upper bound robustness adversarial perturbations. particular following theorem bounds ρadv terms ﬁrst moments distributions classiﬁer’s risk theorem then guishability classes. note term classiﬁer-independent property classiﬁcation task. dependence upper bound risk thus classiﬁcation tasks means small robustness adversarial perturbations. note upper bound logically increases risk clearly exist robust linear classiﬁers achieve high risk fig. pictorially represents ρadv diagram predicted theorem linear classiﬁer represented point ρadv–r diagram result shows existence region linear classiﬁers cannot attain. intra-class variability therefore even linear classiﬁer achieve good classiﬁcation performance task robust small adversarial perturbations. simple tasks involving distributions signiﬁcantly different averages likely exists linear classiﬁer separate correctly classes large robustness adversarial perturbations. examine robustness linear classiﬁers random uniform noise. following theorem compares robustness linear classiﬁers random uniform noise robustness adversarial perturbations. proof found appendix words ρunif\u0001 behaves √dρadv linear classiﬁers linear classiﬁers therefore robust random noise adversarial perturbations factor typical high dimensional classiﬁcation problems shows linear classiﬁer robust random noise even small. note moreover result tight ρunif ρadv. results perspective empirical results szegedy showed large notions robustness neural networks. analysis provides conﬁrmation high dimensional phenomenon linear classiﬁers. illustrate theoretical results example section case −eµ− √da. using theorem zero-risk linear classiﬁer satisﬁes ρadv √da. choose accurate linear classiﬁers therefore robust adversarial perturbations task. note flin achieves upper bound therefore robust accurate linear classiﬁer easily checked ρadv √da. fig. exact ρadv curve compared theoretical upper bound bias ./√d. besides zero-risk case upper bound tight upper bound reasonably close exact curve values risk computation estimate). results illustrated fig. adversarial noise robustness constant dimension ./√d) robustness random uniform noise increases example value ρunif\u0001 least times larger adversarial robustness ρadv. high dimensions linear classiﬁer therefore much robust random uniform noise adversarial noise. study robustness adversarial perturbations quadratic classiﬁers form symmetric matrix. besides practical quadratic classiﬁers applications represent natural extension linear classiﬁers. study linear quadratic classiﬁers provides insights adversarial robustness depends family considered classiﬁers. similarly linear setting exclude case trivial classiﬁer assigns constant label datapoints. assume satisﬁes lemma assumption holds class quadratic classiﬁers λmin λmax max| proof. goal prove dist assume without loss generality diagonal −p−eµ− pc−p−c− apc−p−c−∗ using generalized cauchy-schwarz inequality denote respectively spectral nuclear matrix norms. iii. max||λmax|)/ max|−/|λmax|−/) words upper bound adversarial robustness depends distinguishability measure deﬁned classiﬁer’s risk. difﬁcult classiﬁcation tasks small quadratic classiﬁer risk satisﬁes assumptions robust adversarial perturbations. noted that distinguishability measured distance means distributions linear case deﬁned difference second order moments matrices c−∗. therefore classiﬁcation tasks involving distributions close means different second order moments zero-risk linear classiﬁer robust adversarial noise zero-risk robust quadratic classiﬁers priori possible according upper bound theorem suggests robustness adversarial perturbations larger ﬂexible classiﬁers comparable values risk. illustrate results running example section case simple computation gives term signiﬁcantly larger difference means therefore hope quadratic classiﬁer accurate robust small adversarial perturbations according theorem fact following quadratic classiﬁer outputs vertical images horizontal images therefore fquad achieves zero risk classiﬁcation task similarly flin. classiﬁers however different robustness properties adversarial perturbations. using straightforward calculations shown ρadv value small values therefore ρadv ρadv. result intuitive fquad differentiates images orientation unlike flin uses bias distinguish them. minimal perturbation required switch estimated label fquad therefore modiﬁes direction line hardly perceptible perturbation modiﬁes bias enough label fquad. explains result originally illustrated fig. section illustrate results practical classiﬁcation examples. speciﬁcally experiments real data seek conﬁrm identiﬁed limit robustness classiﬁers show large adversarial random robustness real data. also study general classiﬁers suggest trends obtained theoretical results limited linear quadratic classiﬁers. given binary classiﬁer datapoint approach close szegedy approximate ∆adv. speciﬁcally perform line search maximum minimizer following problem satisﬁes denote ∆adv obtained solution. empirical robustness adversarial perturbations deﬁned i=∆adv denote training points. evaluate robustness compare represents average norm minimal perturbation required transform training point training point opposite class seen distance measure classes. quantity therefore provides baseline comparing robustness adversarial perturbations robust adversarial perturbations ρadv also compare adversarial robustness classiﬁers robustness random uniform noise. estimate ∆unif\u0001 using line search procedure ﬁnds largest condition compute robustness uniform random noise classiﬁers except rbf-svm classiﬁer often asymmetric assigning classes small pockets ambient space rest space assigned class. cases robustness uniform random noise equal inﬁnity classes given ﬁrst consider classiﬁcation task mnist handwritten digits dataset consider digit digit binary classiﬁcation task randomly chosen images training testing respectively. addition small random translation applied images images normalized unit euclidean norm. table reports accuracy different classiﬁers robustness adversarial random perturbations. despite fact l-svm performs fairly well classiﬁcation task achieves good performance high robustness adversarial perturbations. concerning robustness random uniform noise results table conﬁrm large adversarial random robustness linear classiﬁer predicted theorem moreover results suggest maintained polynomial svm. fig. illustrates robustness different classiﬁers example image. figure original image minimally perturbed images switch estimated label linear quadratic cubic classiﬁers. image corresponds original image perturbed random uniform noise norm ∆unif\u0001 learned linear classiﬁer. linear classiﬁer gives label high probability. norms perturbations reported case. turn natural image classiﬁcation task images taken cifar- database database contains classes images. restrict dataset ﬁrst classes consider subset original data images training testing. moreover images normalized unit euclidean norm. compared ﬁrst dataset task difﬁcult variability images much larger digits. report results table seen classiﬁers robust adversarial perturbations experiment ρadv despite that classiﬁers achieve accuracy around training accuracy robust uniform random noise. fig. illustrates robustness adversarial random noise learned classiﬁers example image dataset. compared digits dataset distinguishability measures task smaller theoretical analysis therefore predicts lower limit adversarial robustness linear quadratic classiﬁers task example). instability classiﬁers adversarial perturbations task suggests essence classiﬁcation task correctly captured classiﬁers even fairly good test accuracy reached. reach better robustness possibilities exist ﬂexible family classiﬁers better training algorithm tested nonlinear classiﬁers. latter solution seems possible theoretical limit quadratic classiﬁers suggests still room improve robustness classiﬁers. since theoretical results suggest ﬂexible classiﬁers achieve better robustness adversarial perturbations binary case explore empirically whether intuitions hold scenarios depart theory different ways consider multiclass classiﬁcation problems consider convolutional neural network architectures. classiﬁers’ ﬂexibility relatively well quantiﬁed polynomial classiﬁers degree polynomials straightforward neural network architectures. section examine effect breadth depth robustness adversarial perturbations classiﬁers. perform experiments multiclass cifar- classiﬁcation task recent method compute adversarial examples multiclass case. focus baseline classiﬁers learn architectures hidden layers. speciﬁcally layer consists successive combination convolutional rectiﬁed linear units pooling operations. convolutional layers consist ﬁlters feature maps layer pooling operations done window size stride parameter build three architectures gradually successively stacking hidden layer previous architecture last hidden layer connected fully connected layer softmax loss used. architectures trained stochastic gradient descent. provide fair comparison different classiﬁers three classiﬁers approximately similar classiﬁcation error ensure similar accuracies perform early stop training procedure necessary. empirical normalized robustness adversarial perturbations three networks compared figure observe ﬁrst increasing depth network leads signiﬁcant increase robustness adversarial perturbations especially layers. depth neural network important impact robustness classiﬁer like degree polynomial classiﬁer important factor robustness. going layers however seems marginal effect robustness. noted that despite increase robustness depth normalized robustness computed classiﬁers relatively small suggests none classiﬁers really robust adversarial perturbations. note also results figure showing increase robustness depth inline recent results showing depth provides robustness adversarial geometric transformations fig. show effect number feature maps estimated normalized robustness adversarial perturbations. unlike effect depth observe number feature maps barely effect robustness adversarial perturbations. finally comparison normalized robustness measures deep networks vgg- vgg- imagenet shows networks behave similarly terms robustness important phenomenon many practical implications. better understand implications limit derived specialized upper bounds families classiﬁers. family linear classiﬁers established limit small problems interest. hence linear classiﬁers usually robust adversarial noise however different nonlinear classiﬁers family quadratic classiﬁers limit adversarial robustness usually larger linear classiﬁers gives hope classiﬁers robust adversarial perturbations. fact using appropriate training procedure might possible closer theoretical bound. general nonlinear classiﬁers designing training procedures speciﬁcally take account robustness learning important future work. also believe application general upper bound lemma derive explicit upper bounds speciﬁc e.g. deep neural networks important future work. that believe important derive explicitly parameters assumption class functions consideration. results algebraic geometry seem suggest establishing results might possible general classes functions addition experimental results suggest that unlike breadth neural network depth plays crucial role adversarial robustness. identifying upper bound adversarial robustness deep neural networks terms depth network would great step towards better understanding systems. thank hamza fawzi goodfellow discussions comments early draft paper guillaume aubrun pointing reference theorem also thank seyed mohsen moosavi help preparing experiments.", "year": 2015}