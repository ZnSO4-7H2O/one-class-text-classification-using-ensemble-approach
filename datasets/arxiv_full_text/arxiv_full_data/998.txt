{"title": "The Power of Depth for Feedforward Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We show that there is a simple (approximately radial) function on $\\reals^d$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.", "text": "show simple function expressible small -layer feedforward neural networks cannot approximated -layer network certain constant accuracy unless width exponential dimension. result holds virtually known activation functions including rectiﬁed linear units sigmoids thresholds formally demonstrates depth even increased exponentially valuable width standard feedforward neural networks. moreover compared related results context boolean functions result requires fewer assumptions proof techniques construction different. learning multi-layered artiﬁcial neural networks a.k.a. deep learning seen dramatic resurgence popularity past years leading impressive performance gains difﬁcult learning problems ﬁelds computer vision speech recognition. despite practical success theoretical understanding properties still partial best. paper consider question expressive power neural networks bounded size. boundedness assumption important here well-known sufﬁciently large depth- neural networks using reasonable activation functions approximate continuous function bounded domain hornik funahashi barron however required size networks exponential dimension renders impractical well highly prone overﬁtting. learning perspective theoretically practice main interest neural networks whose size bounded. network bounded size basic architectural question trade width depth networks narrow deep shallow wide? deep deep learning really important? perhaps always content shallow neural networks? overwhelming empirical evidence well intuition indicates depth neural network indeed important networks tend result complex predictors seem hard capture using shallow architectures often lead better practical performance. however types networks used practice surprisingly formal results work consider fully connected feedforward neural networks using linear output neuron non-linear activation function neurons commonly-used rectiﬁed linear unit max{z well sigmoid threshold informally speaking consider following question functions speciﬁcally consider simplest possible case namely difﬁculty approximating functions computable -layer networks using -layer networks networks feedforward fully connected. following standard convention deﬁne -layer network width inputs clearly prove something separation -layer -layer networks need make assumption activation function identity -layer -layer networks compute linear functions hence difference expressive power). essentially require universal sense sufﬁciently large -layer network approximate univariate lipschitz function non-constant bounded domain. formally following assumption assumption given activation function constant following holds l-lipschitz function constant outside bounded interval exist scalars a{αi γi}w assumption satisﬁed standard activation functions familiar with. first provide appendix constructive proof relu function. threshold sigmoid general sigmoidal functions limz→−∞ proof idea similar implied proof theorem debao note sometimes also adds constant bias parameter output neuron easily simulated constant neuron ﬁrst layer chosen appropriately. also sometimes output neuron deﬁned non-linearity well stick linear output neurons common reasonable assumption networks computing real-valued predictions. essentially single neuron sigmoidal activation express single-step function combination neurons express function steps l-lipschitz function constant outside approximated accuracy function involving steps. addition technical reasons require following mild growth measurability conditions satisﬁed virtually activation functions literature including examples discussed earlier main result following theorem implies -layer networks width polynomial dimension cannot arbitrarily well approximated -layer networks unless width exponential theorem suppose activation function satisﬁes assumption constant well assumption exist universal constants following holds every dimension probability measure function following properties proof sketched sec. formally presented sec. roughly speaking approximates certain radial function depending norm input. layers approximating radial functions arbitrary accuracy straightforward ﬁrst approximating squared norm function approximating univariate function acting norm. however performing approximation layers much difﬁcult proof shows exponentially many neurons required approximate constant accuracy. conjecture much wider family radial functions also satisfy property. make following additional remarks theorem remark theorem places constraints activation function beyond assumptions fact inapproximability result function holds even activation functions different across ﬁrst layer neurons even chosen adaptively long satisfy assumption remark theorem places constraints whatsoever parameters -layer networks take values contrast related depth separation results context threshold circuits require size parameters constrained remark least speciﬁc activation functions relu sigmoid threshold proof construction implies poly-lipschitz -layer network expressing parameters bounded poly. qualitative level question considering similar question boolean circuit lower bounds computational complexity cases consider functions represented combination simple computational units large deep representation needs order compute approximate given function. boolean circuits relatively rich literature strong lower bounds. recent example paper rossman shows explicit depth linear-sized circuit cannot non-trivially approximated depth circuits size polynomial said well-known type computation performed unit circuit crucially affect hardness results lower bounds boolean circuits readily translate neural networks type used practice real-valued express continuous functions. example classical result boolean circuits states parity function cannot computed constant-depth boolean circuits whose size polynomial nevertheless parity function fact easily computed simple -layer owidth real-valued neural network reasonable activation functions. model closer threshold circuit neural network neurons threshold activation function input boolean cube survey). threshold circuits main known result context computing inner products d-dimensional boolean vectors cannot done -layer network poly-sized parameters poly width done small -layer network note unlike neural networks practice result hajnal speciﬁc non-continuous threshold activation function considers hardness exact representation function -layer circuits rather merely approximating following initial publication paper informed proof technique together techniques papers martens possibly used show inner product also hard approximate using -layer neural networks continuous activation functions long network parameters constrained polynomial activation function satisﬁes certain regularity conditions. even result pose constraints parameters regularity conditions beyond assumptions moreover introduce proof technique different demonstrate hardness approximating boolean inner-product-mod- function rather functions simple geometric structure moving networks real-valued outputs related ﬁeld arithmetic circuit complexity survey) focus computing polynomials thought neural networks neuron computes linear combination product inputs. again different standard neural networks used practice results techniques readily translate. recently several works machine learning community attempted address questions similar consider here. pascanu montufar consider number linear regions rumelhart figure reportedly structure even found automatically back-propagation. threshold activation function input network given construction easily generalizes activation functions possibly using remark martens conditions needed constructions relying distributions ﬁnite however since consider continuous distributions require conditions. expressed relu networks given width size bianchini scarselli consider topological complexity networks certain activation functions function depth. although seen measures function’s complexity results translate directly lower bound approximation error thm. delalleau bengio martens medabalimi cohen show strong approximation hardness results certain neural network architectures however fundamentally different standard neural networks considered here. quite recently telgarsky gave simple elegant construction showing k-layer wide relu networks one-dimensional data express sawtooth function oscillates times moreover rapidly oscillating function cannot approximated poly-wide relu networks depth. also implies regimes exponential separation e.g. k-depth networks approximating k-depth network requires width. results demonstrate value depth arbitrarily deep standard relu networks single dimension using functions exponentially large lipschitz parameter. work different techniques show exponential separation results general activation functions even number layers changes using functions whose lipschitz parameter polynomial nutshell -layer network construct approximates radial function bounded support whose norm larger threshold). layers approximating radial functions rather straightforward first using assumption construct linear combination neurons expressing univariate mapping arbitrarily well bounded domain. therefore adding combinations together inside bounded domain next layer compute univariate function resulting approximately radial function. layers less clear approximate radial functions. indeed proof essentially indicates approximating radial functions layers require exponentially large width. formalize this note probability measure well-behaved density function written function approximation guarantee theorem equivalently written particular consider density function equals inverse fourier transform indicator origin-centered unit-volume euclidean ball continuing note formula given explicitly illustration dimensions provided figure also easily veriﬁed indeed density function clearly non-negative isometry fourier figure left ﬁgure represents dimensions. right ﬁgure represents cropped re-scaled version better show oscillations beyond origin-centered bump. density probability measure deﬁned note square-integrable formally speaking fourier transform standard sense function however assuming |fi| grows polynomially fourier transform general sense tempered distribution distribution shown supported lines. convolution-multiplication principle implies equals convolution indicator unit-volume ball since supported property -layer networks derive main theorem. note holds roughly speaking function fivi constant direction perpendicular hence non-zero fourier components directions. dimension seen fact fourier transform constant function dirac delta function equals everywhere except origin. constant distance function supported high dimensionality plays crucial role unless exponentially large dimension domain sparse considers large distances origin sense poly disjoint intervals width values range note strictly speaking cannot take hard-toapproximate function equal since discontinuous therefore cannot expressed -layer neural network continuous activations functions. however since probability distribution shown bounded density support -layer network approximate \u0001igi above hard indicator function replaced lipschitz function differs arbitrarily small probability mass). letting function good approximation -layer network approximate function also cannot approximate -layer approximation explain function deﬁned gives need. large supported thin euclidean shell hence approximately cigi constant \u0001ici ˆgi. since simple indicator function fourier transform difﬁcult compute explicitly involves appropriate bessel function turns sufﬁciently large mass sufﬁciently away origin. knowing summand relatively large mass high frequencies remaining objective choice signs entire property. attained random choice signs easy observation given orthogonal projection hilbert space |vi| signs independent bernoulli variables. using observation projection onto subspace spanned functions supported high frequencies functions follows least choice \u0001i’s sufﬁciently large portion ˜g’s mass high frequencies. preliminaries begin deﬁning standard notation shall use. denote natural real numbers respectively. bold-faced letters denote vectors d-dimensional euclidean space plainfaced letters denote either scalars functions integration denotes euclidean denotes space norm weighted probability measure shorthand function shorthand given sets denotes space squared integrable functions denotes space absolutely integrable functions norm denotes inner product space gdx) dµ). given functions radial functions. radial function dealing radial functions invariant rotations somewhat abuse notation interchangeably vector arguments denote value function scalar arguments denote value function vector norm thus radial function equals euclidean spheres balls. unit euclidean sphere d-dimensional unit euclidean ball radius rdbd volume one. standard results following useful lemma gamma function. although closed form oscillating shape asymptotically large behaves figure illustrates function appendix provide additional results approximations bessel function necessary proofs. discussed sec. theorem rests constructing distribution appropriate function easy approximate small -layer networks difﬁcult approximate using -layer networks. thus begin formally deﬁning use. fourier transform indicator unit-volume euclidean ball rdbd}. note since fourier transform rdbd} hence indeed probability measure. form proof appears appendix deﬁne hard-to-approximate function introduce notation. large numerical constants determined later assumed integer consider intervals proof proposition requires intermediate steps. remainder section assume chosen large enough satisfy assumptions lemma lemma words assume cα/d suitable universal constant begin following lemma suppose above. exists choice proof. informally proof based convolution-multiplication linearity principles fourier well fourier transform roughly speaking case ˆfix shown supported span{vj} convolution must supported span{vi} rdbd. summing gives stated result. unfortunately simple analysis formally true since guaranteed fourier transform function converge). however least functions satisfying claim’s conditions fourier transform still exists generalized function tempered distribution using object attain desired result. turn provide formal proof constructions starting description tempered distributions relevant properties complete survey). start denote space schwartz functions tempered distribution context continuous linear operator particular measurable function satisﬁes polynomial growth condition similar viewed tempered distribution deﬁned fourier transform shown directly generalizes standard notion fourier transforms functions. finally tempered distribution supported subset function vanishes subset. based equation claim ˜fiϕ supported complement span{vi} rdbd. would imply tempered distribution˜fiϕ supported span{vi} rdbd this ˆgdx expx w)dwdx expx gδdx dirac delta function fourier transform constant function. above follows radial; follows cauchy-schwartz; follows unit norm; follows fact term squared expectation uniformly distributed rsd− jensen’s inequality; follows non-increasing; follows fact unit norm. proof proposition relies assumption ensures approximate univariate functions using activation function. discussed thm. also plug weaker versions assumption versions proposition width guarantee worse polynomial dependence parameters would lead versions thm. somewhat worse constants polynomial dependence dimension exactly -layer network except additional constant term however increasing simulate additional neuron σ+z) x)+z) scalar write function -layer network width proof theorem proof straightforward combination propositions ﬁrst choose cα/d constant taken statement proposition invoking proposition obtain signs universal constant function expressed bounded-size -layer network satisﬁes supported part marie curie grant intel icri-ci institute israel science foundation grant thank james martens anonymous colt reviewers several helpful comments.", "year": 2015}