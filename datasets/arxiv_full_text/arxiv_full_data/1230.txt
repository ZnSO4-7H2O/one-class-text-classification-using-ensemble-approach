{"title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "tag": ["cs.LG", "cs.CG", "cs.CV", "cs.NE"], "abstract": "Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.", "text": "markus thom driveu institute measurement control microtechnology university germany günther palm institute neural information processing university germany sparseness useful regularizer learning wide range applications particular neural networks. paper proposes model targeted classiﬁcation tasks sparse activity sparse connectivity used enhance classiﬁcation capabilities. tool achieving sparseness-enforcing projection operator ﬁnds closest vector pre-deﬁned sparseness given vector. theoretical part paper comprehensive theory projection developed. conclusion shown projection differentiable almost everythus implemented smooth neuronal transfer function. entire model hence tuned end-to-end using gradient-based methods. experiments mnist database handwritten digits show classiﬁcation performance boosted sparse activity sparse connectivity. combination both performance signiﬁcantly better compared classical non-sparse approaches. keywords supervised learning sparseness projection sparse activity sparse connectivity sparseness concept efﬁciency neural networks exists variants context sparse activity property means small fraction neurons active time. sparse connectivity property means neuron connected limited number neurons. properties observed mammalian brains inspired variety machine learning algorithms. notable result achieved sparse coding model olshausen field given small patches images natural scenes model able produce gabor-like ﬁlters resembling properties simple cells found mammalian primary visual cortex another example optimal brain damage method lecun used prune synaptic connections neural network making connectivity sparse. although small fraction possible connections remains pruning sufﬁcient achieve equivalent classiﬁcation results. since then numerous approaches measure sparseness proposed hurley rickard overview achieve sparse solutions classical machine learning problems. pseudo-norm natural sparseness measure. computation consists counting number non-vanishing entries vector. using rather sparseness measures shown induce biologically plausible properties however ﬁnding optimal solutions subject pseudo-norm turns np-hard analytical properties counting measure poor non-continuous rendering localization approximate solutions difﬁcult. manhattan norm vector convex relaxation pseudo-norm employed vast range applications. sparseness measure signiﬁcant disadvantage scale-invariant intuitive notion sparseness cannot derived higher values indicate sparse vectors. well-deﬁned holds also scale-invariant. composition differentiable functions differentiable entire domain. sparseness measure fulﬁlls criteria hurley rickard except dalton’s fourth states sparseness vector identical sparseness vector resulting multiple concatenation original vector. property however crucial proper sparseness measure. example sparseness connectivity biological brain increases quickly volume connectivity human brain times sparse brain follows features desirable properties proper sparseness measure. sparseness-enforcing projection operator suitable projected gradient descent algorithms proposed hoyer optimization respect pre-deﬁned target degree sparseness operator ﬁnds closest vector given scale sparseness given arbitrary vector. expressed formally euclidean projection onto parameterizations sets ﬁrst achieving unrestricted projections whereas latter useful situations non-negative solutions feasible example non-negative matrix factorization problems. constants target norms chosen points sets achieve sparseness example unity yielding normalized projections easily derived deﬁnition hoyer’s original algorithm computation projection alternating projection onto hyperplane representing norm constraint hypersphere representing norm constraint non-negative orthant. slightly modiﬁed version algorithm proved correct theis special case exactly negative entry emerges zeroed orthant projection. however still mathematically satisfactory proof general case. paper improves upon previous work following ways. section proposes simple algorithm carrying sparseness-enforcing projections respect hoyer’s sparseness measure. further improved algorithm proposed compared hoyer’s original algorithm. projection differentiable ideal tool achieving sparseness gradient-based learning. exploited section sparseness projection used obtain classiﬁer features sparse activity sparse connectivity natural way. beneﬁt properties demonstrated real-world classiﬁcation problem proving sparseness acts regularizer improves classiﬁcation results. ﬁnal sections give overview related concepts conclude paper. theoretical side ﬁrst rigorous mathematically satisfactory analysis properties sparseness-enforcing projection provided. lengthy technical therefore deferred several appendixes. appendix ﬁxes notation gives introduction general projections. appendix certain symmetries subsets euclidean space effect projections onto sets studied. problem ﬁnding projections onto sets hoyer’s sparseness measure attains constant value addressed appendix ultimately algorithms proposed section proved correct. appendix investigates analytical properties sparseness projection concludes efﬁcient algorithm computes gradient. gradients optimization parameters architecture proposed section collected ﬁnal appendix called euclidean projection onto exactly point projm projm used abbreviation. ﬁnite-dimensional projm nonempty closed projm singleton closed convex literature elements projm also called best approximations projections onto sets fulﬁll certain symmetries special interest paper notable projections onto formalized discussed appendix greater detail. permutation-invariant membership stable upon coordinate permutation order-preserving. proved lemma consequence vector sorted ascending descending order projection onto sorted accordingly. reﬂection-invariant signs arbitrary coordinates swapped without violating membership projection onto orthant-preserving shown lemma means point projection onto located orthant. exploiting property projections onto yielded recording discarding signs coordinates argument projecting onto ﬁnally restoring signs coordinates result using signs argument. formalized lemma example concepts consider vectors exactly non-vanishing entries. clearly permutation-invariant reﬂectioninvariant. therefore projection respect pseudo-norm constraint must order-preserving orthant-preserving. fact projection onto consists simply zeroing entries greatest absolute value trivially fulﬁlls aforementioned properties order-preservation orthant-preservation. permutation-invariance reﬂection-invariance closed intersection union operations. therefore unrestricted target projection permutation-invariant reﬂection-invariant. hence enough handle projections onto ﬁrst place projections onto unrestricted target easily recovered. remainder section problem dimensionality avoid existence trivial solutions. applications sparseness projection paper always unity achieve normalized projections adjusted explained section achieve target degree sparseness related problem ﬁnding best approximation point regardless concrete scaling computing projections onto solved projecting onto rescaling result minimize variation yields method justiﬁed theoretically remark alternating projections first note target written intersection simpler sets. canonical basis n-dimensional euclidean space further vector entries identical unity. denotes target hyperplane coordinates points non-negative orthant equivalent norm constraint. further deﬁne target hypersphere points satisfying norm constraint. yields following factorization computation projections onto intersection ﬁnite number closed convex sets enough perform alternating projections onto members intersection clearly non-convex general approach altered work speciﬁc setup. first consider denotes intersection norm target hyperplane norm target hypersphere. essentially possesses structure hypercircle points also central point real number points squared distance shown appendix λ/n· intersection non-negative orthant norm hyperplane rn≥∩h scaled canonical simplex. barycenter coincides barycenter finally index denote subset points coordinates index vanish. barycenter given λ/d·∑i∈i preparations simple algorithm proposed; computes sparseness-enforcing projection respect constraint induced hoyer’s sparseness measure theorem every algorithm computes element projd. line line iterations projd singleton. already pointed idea algorithm projections onto computed alternating projections onto geometric structures deﬁned. rigorous proof correctness appendix proceeds showing solutions tampered projection onto intermediate structures non-convexity relation sets simplex non-trivial needs long arguments described further especially lemma corollary projection onto hyperplane straightforward discussed section c... essentially hypersphere embedded subspace projections points onto achieved shifting scaling section c... alternating projection onto beginning algorithm make result projection onto invariant positive scaling arbitrary shifting argument shown corollary especially useful practice alleviating need certain pre-processing methods. formula projections onto generalized projections onto index keeping already vanished coordinates zero section projections onto simplex involved discussed length section relevant result rn\\c exists separator projc maximum taken element-wise cases considered paper always shown lemma implies entries less survive projection hence pseudo-norm strictly greater simplex projection therefore enhances sparseness. separator number nonzero entries projection onto computed algorithm adapted version algorithm chen line denotes symmetric group denotes permutation matrix associated permutation algorithm works sorting argument determining mean value largest entries minus target norm number relevant entries computation equal pseudo-norm projection found trying feasible values starting largest ones. computational complexity algorithm dominated sorting input vector thus quasilinear. permutation-invariance sets involved projections enough sort vector projected onto once. guarantees working vector emerges subsequent projections sorted also. additional sorting carried using algorithm projections onto additionally side effect non-vanishing entries working vector always concentrated ﬁrst entries. hence relevant information always stored small unit-stride array access efﬁcient large sparse array. further index non-vanishing entries working vector always form number nonzero entries. algorithm variant algorithm optimizations applied explicit formulas intermediate projections used. following result proved appendix states algorithms always compute result projections onto increase amount vanishing entries working vector ﬁnite dimension hence alternating projections carried algorithm terminates ﬁnite time. further complexity iteration linear pseudo-norm working vector. theoretic overall computational complexity thus quadratic problem dimensionality original algorithm sparseness-enforcing projection operator proposed hoyer hard understand correctness proved theis special case only. simple alternative proposed algorithm paper. based symmetries induced hoyer’s sparseness measure exploiting projection onto simplex improved method given algorithm improved algorithm proposed paper always requires number iterations alternating projections original algorithm. original algorithm uses projection onto non-negative orthant achieve vanishing coordinates working vector. max. improved algorithm simplex operation written projrn≥ projection used purpose expressed formally projc chosen accordingly. theoretical results simplex geometry section application lemma section number always non-negative. therefore least amount entries zero simplex projection compared projection onto non-negative orthant also corollary hence induction number non-vanishing entries working vector number iterations proposed algorithm needs terminate bounded number iterations original method needs terminate given input. experimental determination estimate number iterations required carried follows. random vectors sparseness sampled sparse projections computed using respective algorithms gain best normalized approximations target sparseness degree algorithms vectors used input. run-time algorithms number iterations necessary compute result counted. additionally number nonzero entries working vector figure comparison number iterations original algorithm projection onto improved version proposed paper. sparseness-enforcing projection target sparseness carried input vectors sparseness thick lines indicate mean number iterations required projection thin lines indicate minimum maximum number iterations respectively. even input vectors million entries less iterations required projection. improved algorithm reduces iterations. figure shows statistics number iterations algorithms needed terminate. already observed hoyer number required iterations grows slowly problem dimensionality. iterations needed original algorithm compute result. algorithm improved requiring iterations amounts roughly less iterations. small slope number required iterations conjectured quantity logarithmic problem dimensionality applies complexity algorithm quasilinear. input vector sorted beginning also possible fall complexity class. progress working dimensionality reduction problem dimensionality depicted figure averaged input vectors experiment. ﬁrst iteration projecting onto working dimensionality still matches input dimensionality. starting second iteration dimensions discarded projecting onto original algorithm onto improved variant yields vanishing entries working vectors. original algorithm mean entries nonzero second iteration improved algorithm original dimensions remain mean. trend continues subsequent iterations ﬁnal working dimensionality reached quickly algorithm proposed paper. although using algorithm perform simplex projection expensive setting negative entries zero orthant projection overhead quickly amortizes boost dimensionality reduction. figure comparison number non-vanishing entries working vectors original algorithm improved algorithm run-time. algorithms input vectors dimensionality initial sparseness compute projections sparseness standard deviations always less omitted plot avoid clutter. algorithm proposed paper reduces dimensionality quickly terminates earlier original algorithm. figure ratio computation time original algorithm improved algorithm variety input dimensionality initial vector sparseness. numbers greater indicate parameterizations proposed algorithm efﬁcient original one. large region speedup decent. determination relative speedup incorporated simplex projection access unit-stride arrays permutation-invariance algorithms implemented programs using optimized implementation blas library carrying vector operations. employed processor intel core i-x. range different dimensionalities vectors varying initial sparseness sampled. number vectors every pair dimensionality initial sparseness chosen processing time algorithms several orders magnitudes greater latency time operation system. absolute time needed algorithms compute projections target sparseness measured ratio taken compute relative speedup. results experiment depicted figure evident maximum speedup achieved vectors dimensionality initial sparseness greater initial sparseness achieved randomly sampled vectors speedup achieved broad spectrum dimensionality improvements original algorithm thus theoretical also noticeable practice. speedup especially useful projection used neuronal transfer function classiﬁer proposed section computational complexity prediction class membership unknown samples reduced. clear theorem projection onto unique almost everywhere. therefore |projd| null set. however example projection unique vectors entries identical. words follows projh projl possible solution given projd given stated remark case positive. additionally another solution given projd unequal solution similar argument used show non-uniqueness merely small non-uniqueness issue practical applications. well-deﬁned {±}n given described lemma note computation crucial prerequisite computation unrestricted variant used exclusively section non-negativity necessary application proposed there. employed objective function optimized information whether functions differentiable crucial selecting optimization strategy. example consider projections onto constant. already mentioned section projection onto consists simply zeroing elements smallest absolute value. point permutation identity matrix entries diagonal belonging small absolute values zeroed out. requirement fulﬁlled small distortion sufﬁcient point projection onto differentiable. contrast projection differentiability non-trivial. full-length discussion given appendix concludes differentiable almost everywhere. efﬁcient product gradient arbitrary vector needs computed corollary expression emerges natural application chain rule objective function sparseness-enforcing projection used. practice weaker form thus mostly restriction preferable efﬁciency reasons general complete gradient given theorem derivative obtained exploiting structure algorithm projection onto essentially composition projections onto overall gradient computed using chain rule. gradients intermediate projections simple expressions combined yield matrix iteration alternating projections. since iteration gradients basically sums dyadic products product arbitrary vector computed primitive vector operations. matrix product associativity process repeated efﬁciently compute product gradient arbitrary vector. this sufﬁcient record intermediate quantities execution algorithm major overhead algorithm itself. gradient unrestricted variant deduced straightforward gradient close relationship. sparseness-enforcing projection operator cast almost everywhere vector-valued function differentiable almost everywhere section section proposes hybrid auto-encoder network two-layer neural network sparseness projection employed neuronal transfer function. proposed model called supervised online autoencoder intended classiﬁcation means neural network features sparse activity sparse connectivity. analytical properties sparseness-enforcing projection operator model optimized end-to-end using gradient-based methods. figure depicts data proposed model. module reconstruction capabilities module classiﬁcation capabilities. reconstruction module depicted left figure operates converting input sample internal representation computing approximation original input sample. product input sample matrix bases rd×n computed transfer function applied. sparse activity chosen sparseness-enforcing projection operator projection respect pseudo-norm. guarantees internal representation sparsely populated close reconstruction achieved like linear generative model multiplication matrix bases internal representation. hence matrix used encoding decoding rendering reconstruction module symmetric words tied weights. approach similar principal comfigure architecture data supervised online auto-encoder circle left mapping input sample approximation comprises reconstruction module. classiﬁcation module consists mapping classiﬁcation decision matrix bases shall sparsely populated account sparse connectivity property. transfer function sparseness projection internal representation sparsely populated fulﬁlling sparse activity property. enforcing sparsely populated sparse connectivity property holds well. formally holds target degree connectivity sparseness i-th column condition adopted non-negative matrix factorization sparseness constraints context neural networks synaptic weights individual neurons stored columns weight matrix interpretation formal sparseness constraint neuron allowed sparsely connected input layer. classiﬁcation module shown right-hand side figure computes classiﬁcation decision feeding one-layer neural network. network output yielded computation product matrix weights wout rn×c addition threshold vector θout application transfer function module shares inference internal representation reconstruction module also considered one-layer neural network. therefore entire processing path forms two-layer neural network stores synaptic weights hidden layer wout θout parameters output layer. input sample shall approximated target vector classiﬁcation shall approximated achieved optimization parameters soae quantities wout θout. goodness approximation estimated using differentiable similarity measure approximation assessed shall optimized controls trade-off reconstruction classiﬁcation capabilities. incorporate sparse connectivity feasible solutions restricted fulﬁll soae identical symmetric auto-encoder network sparse activity sparse connectivity. case soae forms two-layer neural network classiﬁcation sparsely connected hidden layer activity hidden layer sparse. parameter also used blend continuously extremes. note depends wout θout depends wout θout. hence wout θout relevant whereas essential choices appropriate choice correlation coefﬁcient normed values interval invariant afﬁne-linear transformations differentiable. model invariant concrete scaling shifting occurring quantities yielded. follows also invariant transformations corollary similarity measure classiﬁcation capabilities chosen cross-entropy error function shown empirically simard induce better classiﬁcation capabilities mean squared error function. softmax transfer function used transfer function output layer. provides natural pairing together cross-entropy error function supports multi-class classiﬁcation. proposed optimization algorithm minimization objective function esoae projected gradient descent here update degrees freedom followed application sparseness projection columns enforce sparse connectivity. theoretical results convergence projected gradient methods projections carried onto convex sets target projection non-convex. nevertheless experiments described show projected gradient descent adequate heuristic situation soae framework tune network parameters. completeness gradients esoae respect network parameters given appendix update steps carried every presentation pair input sample associated target vector. online learning procedure results faster learning improves generalization capabilities batch learning learning samples associated target vectors one-of-ccodes input algorithm. dimensionality internal representation target degree sparseness respect connectivity parameters algorithm. sparseness connectivity increases larger hoyer’s sparseness measure employed deﬁnition feasible solutions. possible choices hidden layer’s transfer function achieve sparse activity discussed paper. possibility carry projection respect pseudonorm. sophisticated method unrestricted sparseness-enforcing projection operator respect hoyer’s sparseness measure carried algorithm cases target degree sparse activity parameter learning algorithm. case projection sparseness degree denoted sparseness increases smaller values projection used larger values indicate sparse activity. initialization columns achieved selecting random subset learning similar initialization radial basis function networks ensures signiﬁcant activity hidden layer start resulting strong gradients therefore reducing training time. parameters output layer wout θout initialized sampling zero-mean gaussian distribution standard deviation every epoch randomly selected subset samples associated target vectors learning used stochastic gradient descent update wout θout. results appendix used efﬁciently compute gradient objective function. there gradient transfer function emerges product vector. gradient projection trivial given example section hoyer’s sparseness-enforcing projection operator possible exploit product gradient vector needed. case efﬁcient compute result multiplication implicitly using corollary thus avoid computation entire gradient every epoch sparseness projection applied columns guarantees holds therefore sparse connectivity property fulﬁlled. trade-off variable controls weight reconstruction classiﬁcation term adjusted according denotes number current epoch. thus starts zero increases slowly asymptotically reaches one. emphasis beginning optimization thus reconstruction capabilities. subsequently classiﬁcation capabilities incorporated slowly ﬁnal phase training classiﬁcation capabilities exclusively optimized. continuous variant unsupervised pre-training leads parameters vicinity good minimizer classiﬁcation capabilities classiﬁcation preferred reconstruction trade-off parameter compared choice strategy helps stabilize trajectory parameter space makes objective function values settle quickly termination criterion satisﬁed earlier. assess classiﬁcation capabilities impact sparse activity sparse connectivity mnist database handwritten digits employed. popular benchmark data classiﬁcation algorithms numerous results respect data reported literature. database consists samples divided learning samples evaluation samples. sample represents digit size pixels class label associated therefore input output dimensionalities respectively. classiﬁcation error given percent evaluation samples hence corresponds single misclassiﬁed digit. generation original data placement digits achieved based barycenter sampling rounding errors localization uncertainty hence assumed less pixel directions. account uncertainty learning augmented jittering sample eight possible directions pixel yielding samples learning total. evaluation left unchanged yield results compared literature. noted hinton learning problem permutation-invariant jittering information neighborhood pixels implicitly incorporated learning set. however classiﬁcation results improve dramatically prior knowledge used. demonstrated schölkopf using virtual support vector method improved support vector machine polynomial kernel degree error jittering support vectors pixel four principal directions. result extended decoste schölkopf support vector machine polynomial kernel degree nine improved error jittering possible eight directions. improvements achieved generating artiﬁcial training samples using elastic distortions reduced error two-layer neural network hidden units compared error yielded training samples created afﬁne distortions. deep neural networks possess large number adaptable weights. conjunction elastic afﬁne distortions neural networks yield errors current record error held approach combines distorted samples committee convolutional neural networks architecture optimized exclusively input data represents images neighborhood pixels hard-wired classiﬁer. allow plain evaluation depend additional parameters creating artiﬁcial samples jittered learning samples used throughout paper. experimental methodology follows. number hidden units chosen experiments described below. increased number compared hidden units employed simard promises yield better results adequate number learning samples used. tested learning algorithms essentially gradient descent methods initial step size chosen. candidate step size runs two-fold cross validation carried learning set. then step size median resulting classiﬁcation errors computed. winning step size determined achieved minimum median classiﬁcation errors. every epoch samples randomly chosen learning presented network. number samples chosen /-th jittered learning set. step size multiplicatively annealed using factor every epoch. optimization terminated relative change objective function became small signiﬁcant progress learning could observed. resulting classiﬁers applied evaluation misclassiﬁcations counted. variants supervised online auto-encoder architecture proposed section trained augmented learning set. variants target degree sparse connectivity choice made samples learning possess sparseness less therefore resulting bases forced truly sparsely connected compared sparseness digits. ﬁrst variant denoted soae-σ. here sparseness-enforcing projection operator used transfer function hidden layer. target degrees sparse activity respect hoyer’s sparseness measure chosen interval steps size variant trained jittered learning using method described section every value resulting sparseness activity measured training using pseudo-norm. this sample learning presented networks number active units hidden layer counted. figure shows resulting mean value standard deviation sparse activity. chosen mean total hidden units active upon presentation sample learning set. hundred units active time eleven active units. standard deviation activity decreases sparseness increases hence mapping resulting number active units becomes accurate. second variant denoted soae-l differs soae-σ projection respect pseudo-norm transfer function used. target sparseness activity given parameter controls exact number units allowed active time. experiments values chosen match mean activities soae-σ experiments. results variants compared based uniﬁed value activity sparseness. results depicted figure usage projection consequently outperforms projection sparseness degrees. even high sparseness activity percent units allowed active time good classiﬁcation capabilities obtained soae-σ. classiﬁcation results soae-l reach optimum. soae-σ robust classiﬁcation capabilities ﬁrst begin collapse sparseness whereas soae-l starts degenerate sparseness falls roughly translating activity equal classiﬁcation performance achieved using soae-σ. thus concluded using sparseness-enforcing projection operator described paper yields better results simple projection used achieve sparse activity. assess beneﬁt precisely investigate effect individual factors several comparative experiments carried out. summary experiments outcome given table variants soae-σ soae-l denote entirety respective experiments sparseness activity lies intervals described above respectively. using intervals soae-σ soae-l achieved median error evaluation respectively. variant soae-σ-conn essentially equal soae-σ except sparse connectivity incorporated. sparseness activity also chosen resulted equal classiﬁcation results entire range. dropping sparse connectivity increases misclassiﬁcations median error soae-σ-conn thereby greater median error soae-σ. approaches included comparison multi-layer perceptrons topology dynamics classiﬁcation module supervised online auto-encoder exceptions. first transfer function hidden layer hyperbolic tangent thus including explicit sparse activity. second experiment sparse connectivity either incorporated achieved means performing projection learning epoch. besides variation sparseness connectivity experiments differ initialization network parameters. variant runs carried resulting classiﬁers applied evaluation compute classiﬁcation error. then best four worst four results discarded included analysis. hence random sample size achieved original data trimmed away. procedure also applied results figure resulting amount nonzero entries internal representation entries depending target degree sparseness activity respect hoyer’s sparseness measure values entries nonzero whereas high sparseness degrees entries vanish. error bars indicate standard deviation distance mean value. standard deviation shrinks increasing sparseness degree making mapping accurate. figure resulting classiﬁcation error mnist evaluation supervised online autoencoder network dependence sparseness activity hidden layer. projection onto pseudo-norm constraint variant soae-l projection onto constraint determined sparseness measure variant soae-σ used transfer functions. error bars indicate standard deviation difference mean. table overview comparative experiments. second third columns indicate whether sparse connectivity sparse activity incorporated respectively. fourth column reports result statistical test normality interpreted section ﬁnal column gives median standard deviation achieved classiﬁcation error mnist evaluation set. results experiment trimmed gain sample size allowing statistical robust estimates. basic variant denoted baseline discussion mlp-random network parameters initialized randomly. achieved median error evaluation considerably worse soae-σ. variant mlp-samples hidden layer initialized replication randomly chosen samples learning set. decrease overall learning time. however median classiﬁcation error slightly worse compared mlp-random. variant mlp-scfc network parameters initialized unsupervised manner using sparse coding fast classiﬁcation algorithm method precursor soae proposed paper. also features sparse connectivity sparse activity differs essential parts. first sparseness activity achieved latent variable stores optimal sparse code words samples simultaneously. using matrix code words activity individual units enforced sparse time entire learning set. soae achieves sparseness space sample pre-deﬁned fraction units allowed active time. second difference sparse activity achieved indirectly approximation latent matrix code words feed-forward representation. soae sparseness activity guaranteed construction. mlp-scfc achieved median classiﬁcation error mnist evaluation rendering slightly worse mlp-random equivalent mlp-samples. ﬁrst experiment incorporates sparse connectivity smlp-scfc. initialization done mlp-scfc training sparseness connectivity yielded application sparseness-enforcing projection operator weights hidden layer every learning epoch. hence sparseness gained unsupervised initialization retained. mlp-scfc features sparse connectivity initialization loses property training proceeds. slight modiﬁcation median error smlp-scfc decreases signiﬁcantly better baseline result. effect better generalization sparse connectivity also observed lecun context convolutional neural networks. explained bias-variance decomposition generalization error effective number degrees freedom constrained overﬁtting less likely hence classiﬁers produce better results average. argument applied soae-σ additional sparse activity improves classiﬁcation results. last variant called mlp-obd. here optimal brain damage algorithm used prune synaptic connections hidden layer irrelevant computation classiﬁcation decision network. parameters network ﬁrst initialized randomly optimized learning set. impact synaptic connection objective function estimated using taylor series objective function diagonal approximation hessian employed terms cubic higher order neglected. using information number connections halved setting weight connections impact zero. network retrained weights removed connections kept zero. procedure repeated target percentage active synaptic connections hidden layer achieved. results reported here chosen reﬂects sparse connectivity approaches best. mlp-obd achieved median classiﬁcation error comparable baseline result. statistical analysis carried assess signiﬁcance differences performance eight algorithms. procedure follows proposals pizarro demšar hypothesis testing concluded effect size estimation proposed grissom acion algorithm sample size available allowing robust analysis results. first results tested normality using test developed shapiro wilk resulting test statistics p-values given table p-values large cannot rejected samples came normally distributed populations. thus normality assumed remainder discussion. next test proposed levene applied determine whether equality variances groups holds. resulted test statistic degrees freedom therefore p-value hence hypothesis group variances equal rejected high signiﬁcance. consequently parametric omnibus post-hoc tests cannot applied require groups equal variance. alternative nonparametric test kruskal wallis based rank information employed test whether algorithms produced classiﬁers equal classiﬁcation errors mean. test statistic degrees freedom p-value less hence statistically signiﬁcant difference mean classiﬁcation results. locate deviation critical difference comparing mean ranks algorithms computed. tukey-kramer type modiﬁcation applied dunn’s procedure yields critical difference less conservative nemenyi’s procedure kruskal-wallis test note approach nevertheless similar post-hoc figure diagram multiple comparison algorithms following demšar algorithm mean rank computed kruskal-wallis test. then critical difference computed signiﬁcance level. algorithms produce classiﬁcation results statistically equal difference mean ranks greater critical difference. induced three groups algorithms produced statistically equivalent results marked black bars. procedure proposed demšar paired observations diagrams proposed adapted case unpaired observations. result depicted figure critical difference statistical signiﬁcance level given. test induces highly signiﬁcant partitioning eight algorithms namely three groups given partition turn induces equivalence relation. statistical equivalence hence unambiguous well-deﬁned moreover p-value partition signiﬁcance level would lower this groups would blend together. assess beneﬁt algorithm group chosen algorithm another group probability superior experiment outcome estimated this classiﬁcation errors pooled respect membership three groups. tested whether pooled results still come normal distributions. group singleton trivially fulﬁlled result table group shapiro-wilk test statistic p-value group achieved test statistic p-value standard signiﬁcance level chosen assumed normally distributed also. random variable modeling classiﬁcation results algorithms group {abc}. assumed normally distributed unknown mean unknown variance clearly normally distributed also groups {abc}. therefore probability algorithm produces better classiﬁer another could computed gaussian error function group means variances known. however using rao-blackwell theory minimum variance unbiased estimator probability computed easily evaluation expression shows estimated estimated estimated therefore effect choosing soae-σ seven algorithms dramatic results interpreted follows. neither sparse activity sparse connectivity incorporated worst classiﬁcation results obtained regardless initialization network parameters. exception mlp-obd incorporates sparse connectivity although name says destructive way. synaptic connection removed cannot recovered measure relevance lecun vanishes synaptic connections zero strength. statistics smlp-scfc shows sparse connectivity obtained using sparseness-enforcing projection operator superior results achieved. nature projected gradient descent possible restore deleted connections helps decrease classiﬁcation error learning. soae-σ-conn sparse activity used classiﬁcation results statistically equivalent smlp-scfc. therefore using either sparse activity sparse connectivity improves classiﬁcation capabilities. used results improve even variant soae-σ shows. hold soae-l however projection used transfer function. hoyer’s sparseness measure according projection possess desirable analytical properties considered smooth approximations pseudo-norm. smoothness seems produce beneﬁt practice. section reviews work related contents paper. first theoretical foundations sparseness-enforcing projection operator discussed. next application neuronal transfer function achieve sparse activity classiﬁcation scenario context alternative approaches possible advantages sparse connectivity described. ﬁrst major part paper dealt improvements work hoyer theis here algorithm sparseness-enforcing projection respect hoyer’s sparseness measure proposed. technical proof correctness given appendix projected onto intersection simplex hypercircle hypersphere lying hyperplane. overall procedure described performing alternating projections onto certain subsets approach common handling projections onto intersections individual sets. example neumann proposed essentially idea investigated sets closed subspaces shown converges solution. similar approach carried intersections closed convex cones generalized translated cones used approximate convex alternating methods necessary know projections onto individual members intersection achieved. although methods exhibit great generality severe drawbacks scenario paper. first target projection must intersection convex sets. scaled canonical simplex clearly convex hypercircle non-convex contains point. condition generates cannot easily weakened achieve convexity. original hypersphere replaced closed ball would convex. changes meaning problem dramatically virtually sparseness original target degree sparseness obtained. target norm ﬁxed sparseness measure decreases whenever target norm decreases. geometric terms method proposed paper performs projection within circle onto boundary increase sparseness working vector. argument given detail figure proof lemma second drawback general methods projecting onto intersections solution achieved asymptotically even convexity requirements fulﬁlled. special structure number alternating projections carried solution using algorithm bounded problem dimensionality. thus exact projection always found ﬁnite time. furthermore solution guaranteed found time quadratic problem dimensionality. crucial point computation projection onto certain subsets nature norm latter straightforward. former efﬁcient algorithms proposed recently independent solutions required projection point onto scaled canonical simplex norm also carried linear time without sort vector projected. achieved showing separator performing simplex projection unique zero monotonically decreasing function zero function found efﬁciently using bisection method exploiting special structure occurring expressions context paper explicit closed-form expression preferable permits additional insight properties projected point. major part proving correctness algorithm interconnection ﬁnal solution zero entries according positions working vector thus chain monotonically decreasing pseudo-norm achieved. result established lemma characterizes projections onto certain faces simplex corollary application lemma analysis theoretical properties sparseness-enforcing projection concluded differentiability appendix idea exploit ﬁniteness projection sequence apply chain rule differential calculus. necessary show projection chain robust neighborhood argument. reduces analysis individual projection steps already studied literature. example projection onto closed convex guaranteed differentiable almost everywhere non-convexity issue critical point barycenter. simplex characterization critical points given lemma lemma shown expression projection onto invariant local changes. explicit expression construction gradient sparseness-enforcing projection operator given theorem corollary shown computation product gradient arbitrary vector achieved efﬁciently exploiting sparseness special structure gradient. similar approaches sparseness projections discussed following. iterative hard thresholding algorithm gradient descent algorithm projection onto pseudo-norm constraint performed application lies compressed sensing linear generative model used infer sparse representation given observation. sparseness acts regularizer necessary observations sampled nyquist rate. spite simplicity method shown achieves good approximation optimal solution np-hard problem norm constraint sparseness measure becomes xp/x. hoyer’s sparseness measure constant normalization obtained. converges decreasingly zero converges point-wise pseudo-norm. hence small values natural sparseness measure obtained. theis tanaka also proposed extension hoyer’s projection algorithm. essentially neumann’s alternating projection method closed subspaces replaced \"spheres\" induced pseudo-norms. note sets non-convex convergence guaranteed. further closedform solution projection onto \"lp-sphere\" known numerical methods employed. problem similar projections employed minimize convex function subject group sparseness context mixed norm balls particular interest matrix rn×g mixed norm deﬁned last problem discussed elastic criterion constraint norm norm. feasible written convex control shape note norms considered whereas non-convex consists intersection different constraints. therefore elastic induces different notion sparseness hoyer’s sparseness measure does. case mixed norm balls projection onto simplex generalized achieve projections onto sparseness-enforcing projection operator respect hoyer’s sparseness measure projection onto pseudo-norm constraint differentiable almost everywhere. thus suitable gradient-based optimization algorithms. section used transfer functions hybrid auto-encoder network two-layer neural network infer sparse internal representation. representation subsequently employed approximate input sample compute classiﬁcation decision. addition matrix bases used compute internal representation enforced sparsely populated application sparseness projection learning epoch. hence supervised online auto-encoder proposed paper features sparse activity sparse connectivity. properties also investigated exploited context autoassociative memories binary inputs. entries training patterns sparsely populated weight matrix memory sparsely populated well training hebbian-like learning rules used assumption sparsely coded inputs also results increased completion capacity noise resistance associative memory input data sparse inherently feature detectors perform sparsiﬁcation prior actual processing memory matrix factorization shown achieve sparse connectivity certain data sets however data sets work although hoyer’s model makes sparseness easily controllable explicit constraints inherently suited classiﬁcation tasks. extension intended incorporate class membership information increase discriminative capabilities proposed heiler schnörr approach additional constraint added ensuring every internal representation close mean internal representations belong class. words method interpreted supervised clustering number clusters equal number classes. however guarantee distribution internal representations exists reproduction error minimized internal representations arranged pattern. unfortunately heiler schnörr used subset small data handwritten digit recognition evaluate approach. precursor supervised online auto-encoder proposed thom there inference sparse internal representations achieved ﬁtting one-layer neural network approximate latent variable optimal sparse representations. transfer function used approximation hyperbolic tangent raised power greater equal three. resulted depression activities small magnitude favoring sparseness result. similar techniques achieve shrinkage-like effect increasing sparseness activity neural network used gregor lecun glorot information processing purely local scalar function evaluated entrywise vector thus information interchanged among individual entries. non-local shrinkage reduce gaussian noise sparse coding already described hyvärinen here maximum likelihood estimate weak assumptions yields shrinkage operation conceived projection onto scaled canonical simplex. case object recognition hard shrinkage also employed de-noise ﬁlter responses whenever best approximation permutationinvariant used shrinkage-like operation must employed. using projection operator neural transfer function hence natural extension ideas. projection sufﬁciently smooth entire model tuned end-to-end using gradient methods achieve auto-encoder classiﬁer. second building block thom incorporated supervised online auto-encoder architectural concept classiﬁcation. well-known layers neural network sufﬁcient approximate continuous function compactum arbitrary precision similar architectures also proposed classiﬁcation combination sparse coding inputs. however sparse connectivity considered context. bradley bagnell used kullback-leibler divergence implicit sparseness penalty term combined backpropagation algorithm yield classiﬁer achieved error rate mnist evaluation set. kullback-leibler divergence chosen replace usual norm penalty term smoother latter therefore sparsely coded internal representations stable subject subtle changes input. related technique supervised dictionary learning mairal objective function additive combination classiﬁcation error term term reproduction error norm constraint. inference sparse internal representations achieved solving optimization problem. procedures time-consuming greatly increase computational complexity classiﬁcation. approach classiﬁcation error mnist evaluation achieved. approaches used original mnist learning without jittering digits thus considered permutationinvariant. augmentation learning virtual samples would contributed improve classiﬁcation performance demonstrated schölkopf finally consider sparse connectivity property mostly neglected literature favor sparse activity. shown paper sparse connectivity helps improve generalization capabilities. practice property also used reduce computational complexity classiﬁcation order magnitude results exploiting sparseness using sparse matrix-vector multiplication algorithms infer internal representation major computational burden class membership prediction. shown paper thom small number nonzero entries weight matrix hidden layer sufﬁcient achieving good classiﬁcation results. furthermore additional savings required storage capacity bandwidth allow using platforms modest computational power practical implementations. sparseness therefore elementary concept efﬁciency artiﬁcial processing systems. without sparseness brains higher mammals probably would developed viable life-forms. important concept efﬁciency discovered neuroscientists practical beneﬁt obtained engineers artiﬁcial information processing systems. paper studied hoyer’s sparseness measure particular projection arbitrary vectors onto sets attains constant value. simple efﬁcient algorithm computing sparsenessenforcing projection operator proposed paper correctness proved. addition demonstrated proposed algorithm superior run-time hoyer’s original algorithm. analysis theoretical properties projection concluded showing differentiable almost everywhere. projections onto constraints well-understood constitute ideal tool building systems beneﬁt sparseness constraints. original case introduced paper. here projection implemented neuronal transfer function yielding differentiable closed-form expression inference sparse code words. besides sparse activity connectivity system also forced sparse performing projection presentation learning examples. smoothness entire system optimized end-to-end gradient-based methods yielding classiﬁcation architecture exhibiting true sparse information processing. supervised online auto-encoder applied benchmark data pattern recognition. sparseness constraints reduce amount feasible solutions clear ﬁrst place whether performance achieved all. however target degree sparseness activity reasonable range classiﬁcation results equivalent superior classical non-sparse approaches. result supported statistical evaluation showing performance increase merely coincidental statistically signiﬁcant. therefore sparseness seen regularizer offers potential improve artiﬁcial systems seems improve biological systems. authors wish thank patrik hoyer xiaojing sharing source code algorithms. authors also grateful anonymous reviewers valuable comments feedback. work supported daimler germany. appendix ﬁxes notation provides prerequisites following appendices. denotes natural numbers including zero real numbers non-negative real numbers. n-dimensional euclidean space canonical basis denotes vector entries identical unity. vectors subscript denotes amount nonzero entries corresponding entry vector vector given pseudo-norm denote manhattan norm euclidean norm respectively. denotes canonical product euclidean space. given vector diag denotes square matrix main diagonal zero entries positions diagb denotes hadamard product entrywise product vectors square matrices diag denotes block diagonal matrix blocks given symmetric group denotes permutation matrix denotes complement universal clear context. power denoted by℘. denotes boundary topological sense. sign function denoted sgn. list symbols frequently used throughout paper given table hence ratio target norm target norm important actual scale. argument generalized projections onto scale-invariant therefore naturally holds also problem dimensionality canonical basis vector entries target manhattan norm target euclidean norm target sparseness projection target non-negative sparseness projection appendix investigates certain symmetries sets effect projections onto sets. great variety sparseness measures fulﬁlls certain symmetries vector entries equally weighted hurley rickard means entry preferred another negative entries usually absolute value squared value taken signs entries ignored. consider following deﬁnition symmetries analyzed deﬁnition called permutation-invariant permutations further called reﬂection-invariant {±}n. words subset euclidean space permutation-invariant membership invariant permutation individual coordinates. reﬂection-invariant single entries negated without violating membership. equivalent x−∑i∈i xiei index sets condition technically easier handle. following observation states symmetries closed common operations remark permutation-invariant reﬂection-invariant proof obvious elementary algebra. consider following general properties functions mapping power sets deﬁnition power function. called order-preserving implies hence function order-preserving relative order entries arguments change upon function evaluation. thus entries sorted ascending descending order entries every vector orthant-preservation denotes fact every vector located orthant. link symmetries projection properties established following result. weaker form statements described duchi special case projection onto simplex. lemma projm. following holds proof projm. assume permutation-invariant. consider single transposition application proposition yields requirement contradicts minimality projection onto hence must hold. projm. deﬁne sgn}. claim holds trivially assume deﬁne ∑i∈i piei. follows reﬂection-invariant. proposition implies clearly ∑i∈i pixi. therefore application proposition yields −∑i∈i pixi. definition obtains pixi {−}. hence would index would contradict minimality therefore claim follows. projection onto permutation-invariant unique equal entries argument cause equal entries projection remark permutation-invariant projm unique follows proof projm assume would hold permutation-invariance follows hence projm uniqueness projection contradicts therefore next result shows solutions projection onto reﬂection-invariant sets turned non-negative solutions vice-versa. second part already observed hoyer special case sparseness-enforcing projection operator duchi connection projections onto simplex onto ball studied. provide proof latter work hint possible proof given. lemma sufﬁces consider non-negative solutions projections onto reﬂection-invariant sets. lemma reﬂection-invariant then proof first note proja lemma hence follows sgn) therefore furthermore reﬂection-invariant proja shown |p|−|x| q−|x|. deﬁne ∑i∈i qiei signs entries ﬂipped. clearly conjunction remark beginning proof follows |p|−|x| obtains hence ˜qi− qi−|xi|. follows hence ˜qi−xi yields thus q−|x| projb. clearly −|xi|. follows therefore proja remark beginning proof yields projb yields p−|x| using result immediately yields condition projections absolutely order-preserving lemma permutation-invariant reﬂection-invariant. function projm absolutely order-preserving. purpose appendix rigorously prove correctness algorithm algorithm compute projections onto projections onto inferred easily explained appendix geometric structures first considerations compute projections onto intersection non-negative orthant target hyperplane target hypersphere section further intersection yields hypercircle intersection yields scaled canonical simplex structure analyzed section section respectively. properties discussed section section results used section prove theorem theorem analysis subsets certain coordinates vanish useful deﬁne following quantities index cardinality |i|. corresponding face denoted barycenter ∑i∈i further denotes hypercircle according vanishing entries deﬁnitions intermediate goal prove projections onto computed alternating projections onto geometric structures deﬁned earlier. idea show solutions tampered alternating projections onto norm constraint—target hyperplane first projection onto target hyperplane considered. lemma elaborated version result theis included completeness. using statements assumed considered point lies without modiﬁcation solution projection onto target lemma following holds follows thus proposition yields hence projd. therefore barycenter projection origin onto next remark gathers additional information norm products point. remark proposition norm constraint—target hypersphere projection onto target hyperplane carried consider joint constraint target hypersphere first note hypercircle hypersphere subspace intrinsic dimensionality reduced therefore product equal euclidean norm additive constant. next consider projections onto note solution respect changed operation. major arguments result taken theis here statements lemma incorporated resulting quadratic equation solved explicitly simplifying original version theis lemma claim follows directly substitution. positive deﬁnite. thus unique projection onto projd projd projd deﬁnition thus similarly lemma hold forms null set. practice however occur input vector algorithm poorly chosen example entries equal. case projl hence point chosen processing. combining properties shown projections onto invariant afﬁne-linear transformations positive scaling corollary projd projd. proof lemma lemma enough show projl) projl). projh projl. hence independent lemma yields proposition yields therefore shifting positive scaling argument algorithm change outcome. overview steps carried given figure consider point projl). already hence projd. therefore situations holds relevant remainder discussion. joint constraint target hyperplane non-negativity yields simplex following deﬁnition likewise deﬁnitions chen michelot deﬁnition called canonical n-simplex. clear scaled canonical simplex. further index face simplex intrinsically possesses structure simplex itself—although reduced intrinsic dimensionality. consider following observation topology embedded subspace proposition metric space figure sketch situation section projected onto target hyperplane projection input point onto projection onto hypercircle squared radius intersection non-negative orthant simplex denoted feasible intersection marked solid black lines. lemma lemma follows projd projd projd hence next steps consist projecting onto ﬁnding projection onto proof simple omitted contribute deeper insight. hence faces subsets topological border using proposition statement inradius made turn used show simplex projection carried proposition squared inradius proof closed convex enough consider distance interior points boundary points. hence insphere radius point computed minimum distance boundary points. proposition points characterized points least entry vanishes. using lagrange multipliers shown minc∈∂cm− ρin. further shown point center larger insphere. achieved constructing projections certain faces discussed detail forthcoming lemma fulﬁlled requirement projection within outside simplex unique must located boundary remark projc proof obvious closed convex. combination proposition remark evident projection within onto yields vanishing entries. ﬁrst projection onto subspace never left throughout arguments presented here remark always applies. shown projection onto possesses zero entries coordinates. reduction problem dimensionality achieved iterative algorithm constructed compute projection onto algorithm guaranteed terminate latest problem dimensionality equals proposition quite methods proposed carrying projections onto canonical simplexes. iterative algorithm developed michelot similar hoyer’s original method computation projection onto simpler effective algorithm developed duchi building upon work chen proposed rigorously proved correctness similar algorithm explicit duchi algorithm adapted better suit needs sparseness-enforcing projection. adapted version given algorithm section following note makes adaptations explicit. proposition projc. following holds proof arguments chen hold projections onto case scaled canonical simplex recovered using projn therefore lines algorithm adapted respectively. correct number nonzero entries follows immediately expression fact sorted descending order termination criterion algorithm remark already sorted descending order sorting needed beginning algorithm projection sorted also permutation-invariant. case nonzero entries located ﬁrst entries projection within onto yields zero entries working vector. still remains shown projection onto possesses zero entries coordinates projection onto holds true dimensionality original problem reduced iterative arguments applied. main building block proof explicit construction projections within simplex onto certain face. next lemma fundamental proving correctness algorithm describes construction result projection onto simplex face poses statement norm turn used prove position vanishing entries change upon projection. proof words constructed setting entry zero adjusting remaining entries ones previously zero norm preserved. generates ﬁnite series points progressively approaching ﬁnal point relevant products vanish process orthogonal projections. hence distance points computed using pythagorean theorem shown unique projection onto entry vanish norm newly constructed point greater original point entries indices must sufﬁciently small non-decreasing norm property hold magnitude entries however strongly connected magnitudes respective entries original point rank preserved point successor. figure gives example cases non-decreasing norm property holds. figure situation lemma projection point within simplex onto faces yields point sufﬁciently close hold point projected onto away. example point located outside dashed square edge length would yield projection smaller euclidean norm. follows using proposition unique euclidean projection exists closed convex. remainder proof assume mini∈i holds. ﬁrst shown fulﬁlled requirement induction deﬁnition denoting indicator function application lemma shows projection point face onto must reside face given original point located within sphere squared radius around shown lemma automatically fulﬁlled projections onto figure sketch proof corollary projection onto must located simplex face assume projection must sufﬁciently close application lemma projection onto located outside hence intersection line convexity farther projection onto therefore cannot projection onto corollary projd. proof projd. assume least showing mini∈i assume permutation-invariant using remark intersection permutationinvariant sets. hence transposition swapping consider hence pτq− violates minimality therefore mini∈i must hold. drawing next arguments given figure lemma consider clearly hence intermediate value theorem exists lies convex. construction hence clearly |β∗|·s− β∗|·v− proj v−t. therefore figure situation lemma lemma point projected onto simplex yielding resides faces point constructed projecting within onto target hypersphere. also hence contradicts hence must hold thus isomorphic simplex lower dimensionality algorithm constructed compute projection onto discussed following. next lemma summarizes previous results analyzes projections onto greater detail. shows solution respect projection onto tampered solutions zeros positions projection onto figure provides orientation quantities discussed lemma proof existence guaranteed proposition remains shown consider index nonnegative entries obtains index hence therefore assume hence using yields projd yields similarly therefore projd. following corollary states similar result theis however proof uses notion simplex projections instead relying pure analytical statements. result presented stronger multiple entries vector zero simultaneously theis entry zeroed single iteration. corollary projd. proof projc. lemma follows claim follows lemma ﬁnal step meet hypersphere constraint again. this simplex projection projected onto target hypersphere simultaneously keeping already vanished entries zero yielding point lemma gives explicit formulation projection shows solution respect projection onto stays same. refer figure sketch construction lemma projc using lemma |i|. more/d therefore hence thus proposition hence projd. claim follows projd. corollary applied follows especially hence claim follows. write consider further clearly thus case membership implies membership respectively. application lemma implies proj˜l). hence follows hence claim follows. converse analogous above membership implies membership respectively. lemma lemma enough show projd projd. like lemma follows well proj proj ˜d). projd projd lemma thus assume proj exists violating minimality hence proj analogously follows proj ˜d). proj proj implies proj proj ˜d). thus projd obtains analogously projd. therefore projd projd. lemma point constructed. already solution projection onto otherwise lemma lemma applied more gaining point lemma states amount nonzero entries must decrease hence process repeated iterations. point non-vanishing entries results guaranteed solution proposition using previous results shown proposed algorithm actually computes correct solution algorithm always terminates ﬁnite time. proof theorem proving partial correctness arbitrary. lemma yields projd projd line lemma follows projd projd line pre-test loop line shown loop-invariant projd projd. beginning loop must hold thus l\\c. line projd projd holds lemma lemma projd projd ensured line hence loop-invariant holds. thus loop projd projd projd line line chosen point respectively example point given remark case projection unique valid representative found. prove total correctness shown loop line terminates. remark applied guarantees number nonzero entries strictly less loop number nonzero entries upon entering loop. hence iterations loop carried solution already proposition thus algorithm terminates ﬁnite time. remains shown optimized variant also correct. proof theorem first note algorithm consists procedure proj_l carrying projections onto in-place main body. function proj_c called obtain information perform projections onto carried algorithm upon entry main body input vector sorted descending order yielding vector algorithm operates sorted vector undoes sorting permutation end. permutation-invariant projections onto respective sets guaranteed remain sorted lemma therefore sorted simplex projection algorithm would require. also note lemma simplex projection smallest elements zero original algorithm continues working non-vanishing entries. order-preservation entries zero relevant information concentrated therefore algorithm continue working ﬁrst entries only index non-vanishing entries always nonzero elements stored contiguously memory access realized small unit-stride array. efﬁcient working large sparsely populated vector. therefore loop starting line corresponds loop starting line algorithm main body sorting permutation inverted entries sorted result vector stored vector entries ignored setting entire vector zero before-hand. appendix studied situations deﬁned section differentiable hence continuous. further explicit expression gradient sought. clear theorem projection point onto written ﬁnite composition projections onto respectively. words points exists ﬁnite sequence index sets denotes iterated composition functions starting decreasing )))··· sequence projlih depends intermediate goal show sequence remains ﬁxed neighborhood projection chain differentiable almost everywhere. implies differentiability except null set. close relationship also differentiable almost everywhere shown appendix. projection onto differentiable everywhere clear explicit formula given lemma considering projection unique cast function unless point projected equal barycenters respectively. considering explicit formulas given lemma lemma clear functions differentiable composition differentiable functions. thus projection onto simplex demands attention. note number proposition equal mean value entries argument survive projection modulo additive constant proposition projc. /|i|· proof follows directly proposition algorithm undoing permutation note similar projection onto lemma next result states condition locally constant hence identiﬁes points projection onto differentiable closed form expression lemma projc given proposition following holds ∑i∈i indicator vectors respectively follows characterization given proposition identity validated directly using proposition follows choosing mini∈{...n}|xi positive requirement validation follows like using projection onto written locally closed form using neighborhood index vanishing entries projected points change. hence projection differentiable composition differentiable functions. clear points projc continuous differentiable example points projected onto vertices structure situation locally equivalent absolute value function. however every point projection onto differentiable subtle change sufﬁcient point projection differentiable lemma consider function projc point differentiable exists point differentiable proof separator proposition projection onto index collisions denoted nonempty lemma consider differentiable deﬁne clearly proposition follows separating projection onto independent entries indices long less equal entries strictly smaller hence differentiable lemma therefore projc differentiable forms null set. next result gathers gradients individual projections involved computation sparseness-enforcing projection operator respect using chain rule gradient derived afterwards multiplication individual gradients. clearly gradients projh projl special cases gradients projc projli respectively. therefore need separate handling computation overall gradient. exploiting special structure matrices involved readily sorted input algorithm gradient computation optimized. remainder appendix oa×b {}a×b ja×b {}a×b denote matrices rows columns entries equal zero unity respectively. theorem sorted descending order differentiable denote number iterations algorithm needs terminate. every iteration algorithm store following values line numbers reference algorithm proof gradient projections onto merely special case projections onto also applies respective projections onto lemma hence ﬁrst iteration special case iterations consider single iteration algorithm computation projli ◦projc write short. input vector sorted requirement intermediate vectors projected sorted well using lemma thus holds. lemma gradient rn×n projection onto form /d·uut −diag e−u. copy padded zeros achieve full dimensionality gradient projection onto general given rn×n using lemma gradient whole iteration given chain rule yielding entries left submatrix dimensionality vanishing according statement applies left submatrix dimensionality holds according entries survive matrix multiplication. therefore sufﬁcient compute left entries gradients individual iterations remaining entries relevant ﬁnal gradient. reﬂected deﬁnition matrices claim. gradient thus computed using matrix-matrix multiplications matrices square edge length number nonzero entries result projection. computation efﬁcient using matrices individual projections. however target degree sparseness thus amount nonzero entries result projection large gradient computation become inefﬁcient. practice often product gradient arbitrary vector required. case procedure sped exploiting special structure gradient corollary sorted descending order differentiable product gradient arbitrary vector computed using vector operations only. proof note associativity matrix product enough consider product gradient rn×n iteration algorithm vector statements theorem sufﬁces consider left entries ﬁrst entries entries vanish. therefore δ/d· jn×n −αsst α/d· jn×n rn×n non-vanishing block given theorem vector ones jn×n denote vector ﬁrst entries using matrix product associativity distributivity multiplication scalar yields although theorem corollary necessary input vector sorted general case easily recovered proposition point sorted descending order differentiable gradient rn×n. also differentiable gradient proof follows likewise gradient unrestricted projection computed gradient proposition point differentiable gradient rn×n. {±}n given s◦π≥ differentiable gradient diaggdiag. proof follows analogously proposition using diagx. summing gradient projection onto computed efﬁciently bookkeeping values discussed theorem applying simple operations recover general case. product gradient vector required computation made efﬁcient stated corollary direct application theorem avoided situation high computational complexity. objective function esoae convex combination similarity measures degrees freedom wout θout soae architecture tuned gradient-based methods minimize functions. appendix reports gradient information needed reproduction experiments. ﬁrst statement addresses reconstruction module. proposition /∂w)t rd×n ∂sr/∂ gradient similarity measure respect ﬁrst argument. additionally /∂wout)t rn×c ∂sr/∂θout r×c. correlation coefﬁcient recommended choice similarity measure reconstruction module normed invariant afﬁne-linear transformations. also differentiable almost everywhere proposition correlation coefﬁcient gradients similarity measure classiﬁcation capabilities essentially equal ordinary two-layer neural network computed using back-propagation algorithm however pairing softmax transfer function crossentropy error function provides particularly simple structure gradient completeness gradients classiﬁcation module soae summarized ment ∂sc/∂y −y)t denotes element-wise quotient diag− esoae convex combination reconstruction error classiﬁcation error overall gradient follows immediately proposition proposition proposition results appendix gradient projection described section used compute explicit gradients procedure proposed paper.", "year": 2016}