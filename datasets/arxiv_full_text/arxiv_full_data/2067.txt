{"title": "Apprenticeship Learning for Model Parameters of Partially Observable  Environments", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider apprenticeship learning, i.e., having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.", "text": "consider apprenticeship learning i.e. agent learn task observing expert demonstrating task partially observable environment model environment uncertain. setting useful applications explicit modeling environment diﬃcult dialogue system. show extract information environment model inferring action selection process behind demonstration assumption expert choosing optimal actions based knowledge true model target environment. proposed algorithms achieve accurate estimates pomdp parameters better policies short demonstration compared methods learns reaction environment. learning demonstration framework learning perform complex task observing demonstration expert particularly useful domains expert knowledge domain limited diﬃcult represent demonstrations much easier designing controller task. assumption expert trying maximize reward. idea that although reinforcement learning produce optimal policy respect given reward function designing reward function captures desired task behavior always obvious requires expert knowledge domain. moreover learning reward function demonstration requires much less amount demonstration compared learning policy directly demonstration reinforcement learning combines reward function environment model optimizing policy future rewards. inverse reinforcement learning successfully applied tasks environment fully observable including aerobatic helicopter ight robot hand control prediction linguistic structures inverse reinforcement learning partially observable environments exact model available also studied however design bottleneck limited reward function. many tasks model environment obvious well requires expert knowledge domain especially environment partially observable. example dialogue system tasks often represented partially observable markov decision process user’s mental state situated hidden state designing model requires considerable amount work domain experts annotating dialogue corpus. thus need estimate uncertain parameters environment model non-annotated demonstration data. since true state hidden construct belief state. denote belief vector denotes probability state current time step. following update formula used calculate belief next time step belief current time step given action current time step observation next time step case pomdp environment reduces problem parameter estimation inputoutput hidden markov model however approach assumes nothing demonstrator applicable cases demonstration generated learning agent even naive random policy. claim demonstration expert contains much richer information environment comes expert’s knowledge extracting information reduce burden designing model suitable task. proposal apply framework apprenticeship learning estimate uncertain parameters environment assuming expert’s behavior based stochastic optimal policy knowledge perfect pomdp model target environment extract expert’s knowledge regarding pomdp parameters demonstration. extracted information expert knowledge combined io-hmm estimation environmental response provide better estimate pomdp parameters. present straightforward estimation algorithms maximum posteriori estimator posterior sampler markov chain monte carlo combined planning algorithms achieve modelparameter apprenticeship learning experiments short demonstrations show algorithms achieve accurate estimates knowledge previous studies benet factor inference parameter consider change reward function. cases rewards uncertain inference relatively easy since value function given policy given linear function reward since true state hidden policy agent action must dened past actions observations. pomdp specied belief suﬃcient statistic past actions observations belief policy induces value function represents expected discounted total reward executing policy starting known value function associated optimal greedy policy approximated arbitrary accuracy convex piecewise-linear function space approximated solution computed polynomial time sarsop approximated pomdp solvers implements elaborated point selection pruning algorithm. algorithm shows sampler posterior model parameters. algorithm similar sampling algorithm model parameters iohmm posterior deals likelihood environmental response diﬀerence lies lines performs metropolis algorithm expert action likelihood term algorithm specied number samples collected excluding burn-ins interval samples. goal achieve model-parameter apprenticeship learning; make optimal policy learned posterior pomdp model parameters. section describe produce optimal policy based sampled results. note that case estimate obtain policy applying existing planning methods pomdp bayesian uncertainty applicable because require uncertainty represented conjugate priors cannot represent posterior distribution parameters observing demonstration. instead employed method develop pomdp policy based sampled parameters. idea extend hidden state pomdp variable index sampled parameters beginning uniformly distributed never changes. extended pomdp solved standard pomdp solver. expect belief sample index converges index likely paramevalues however consider cases transition observation probabilities uncertain inference complex nonlinear dependence parameters value function. maximum posteriori inference maximizes posterior unfortunately easy sophisticated optimization techniques using gradients changes beliefs complicates obtaining gradients either factor major diﬀerence setting inverse reinforcement learning evaluate gradient expert action likelihood observation likelihood constant given obtain optimal action-value function gives soft-max optimal policy pomdp used evaluating expert action likelihood observation likelihood apply standard forward algorithm io-hmm algorithm guarantee parameter based local search. however practice seems reasonably good solution calculation quick compared sampling approach describe next. also employ markov chain monte carlo sampling approach infer posterior distribution unlike inference approximation calculated mcmc arbitrarily accurate suﬃcient computational time. traditional sampling parameters iohmm markov chain monte carlo approach; alternately sampling hidden state sequence given parameter given using conjugate prior parameters easily sample posterior given make sample distribution follow expert action likelihood introduce metropolis algorithm accepts proposed sample expert action likelihood proposed sample previous sample respectively. prob. hear left tiger left prob. hear right tiger right reward seeing tiger rmse root mean squared error estimate values. s.d. samples average standard deviation sampled values. agent interacts environment. case target pomdp episodic want retain belief beyond episodes convert target pomdp non-episodic pomdps extension. note optimal policy extended pomdp becomes good policy target pomdp samples represent target well. need agent learns exploring uncertainty target pomdp need scheduled resampling done fully observable environments boss algorithm paper chose resample purpose evaluate proposed model-parameter apprenticeship learning algorithms performed experiments tasks simple environment based well-known tiger problem task designed dialog system. following experiments used appl toolkit implements sarsop algorithm pomdp solver. used cobyla implementation nlopt library ated demonstrations experts soft-max policy consisting steps actions observations demonstration applied learning algorithms demonstration estimate posterior. estimated posterior optimal greedy policy derived tested simulating steps true environment average reward measured. sampling algorithms mcmc steps performed including -step burn-in parameters sampled every steps generate greedy policy. shown figure accurate estimate leads better results simulation learned policy. results policies based estimated posterior methods much worse expert policy knows true parameter values. hand policies based iohmm estimation occasionally result policies shown \\less\" average rewards gure. considering demonstration short noisy results indicate model-parameter apprenticeship learning methods prevent agents critical failures learning follow demonstrated task. show eﬀectiveness methods realistic scenario developed task dialog management ticket-vending system. user asks agent ticket certain origin destination unreliable voice recognition interface; task agent repeat order correctly issue ticket. expect expert demonstration useful determine parameters represents user’s preferred ticket routes talking. task consists observations voice recognition including three place names agent choose actions consisting uttering words waiting next word user issuing ticket. dialog managed pomdp parameterized -dimensional vector parameters assigned route preferences ways talking voice recognition errors agents required estimate parameters -step demonstration generated expert. experiments didn’t samplers since require much computational resources. generated demonstrations experts softmax policy consisting steps actions observations using learned parameters applied sarsop pomdp solver obtain greedy policy measured average reward testing policy original environment. since calculating exact solution pomdp expensive timeout seconds parameter canods based io-hmm. proposed methods provide better rmse io-hmm methods. estimates io-hmm methods closer prior mean suggesting provided demonstration short obtain accurate estimate. hand estimates proposed methods closer true value indicates proposed methods provide better estimate using length demonstration. also proposed sampler produces narrower posterior distribution io-hmm shown apprenticeship learning approach used estimate parameters unknown pomdp environment. assuming expert knowing perfect pomdp model target environment maximize reward extract expert’s knowledge environment demonstration terms posterior distribution unknown parameters. proposed algorithms simple capable estimating pomdp parameters accurately even demonstration short. also showed extracted knowledge used develop policy reasonably well target environment. approach generalization inverse reinforcement learning sense unknown parameters limited reward function also transition observation functions. approach particularly useful domain applications interact human beings whose model unknown demonstration experts available. direct extension approach estimate parameters discount factor expert demonstration. future work also include development eﬃcient algorithms done context inverse reinforcement learning. research supported aihara innovative mathematical modelling project japan society promotion science \\funding program world-leading innovative science technology initiated council science technology policy jsps grant-in-aid young scientists", "year": 2012}