{"title": "A Combinatorial Algorithm to Compute Regularization Paths", "tag": ["cs.LG", "cs.AI", "cs.CV", "F.2.2; I.5.1"], "abstract": "For a wide variety of regularization methods, algorithms computing the entire solution path have been developed recently. Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier. Most of the currently used algorithms are not robust in the sense that they cannot deal with general or degenerate input. Here we present a new robust, generic method for parametric quadratic programming. Our algorithm directly applies to nearly all machine learning applications, where so far every application required its own different algorithm.  We illustrate the usefulness of our method by applying it to a very low rank problem which could not be solved by existing path tracking methods, namely to compute part-worth values in choice based conjoint analysis, a popular technique from market research to estimate consumers preferences on a class of parameterized options.", "text": "wide variety regularization methods algorithms computing entire solution path developed recently. solution path algorithms compute solution particular value regularization parameter entire path solutions making selection optimal parameter much easier. currently used algorithms robust sense candeal general degenerate input. present robust generic method parametric quadratic programming. algorithm directly applies nearly machine learning applications every application required diﬀerent algorithm. illustrate usefulness method applying rank problem could solved existing path tracking methods namely compute part-worth values choice based conjoint analysis popular technique market research estimate consumers preferences class parameterized options. study combinatorial algorithm solve parameterized quadratic programs i.e. compute whole solution path. unlike methods employed machine learning algorithm deal singular objective function matrices without perturbing input. regularization methods resulting parametrized quadratic programs successfully applied many optimization classiﬁcation regression tasks variety areas example signal processing statistics biology surface reconstruction information retrieval. brieﬂy review applications here also study another application namely choice based conjoint analysis detail. conjoint analysis comprises popular family techniques mostly used market research assess consumers’ preferences options speciﬁed multiple parameters overview recent developments. show regularization approach analysis preference data leads parameterized quadratic program sparse rank positive semi-deﬁnite matrix describing quadratic term objective function. solution path algorithms machine learning. algorithm compute entire regularization path c-svm originally reported hastie gave algorithm lasso later proposed solution path algorithms ν-svm one-class respectively. also receiver operating characteristic curves solved methods support vector regression interesting underlying quadratic program depends parameters regularization parameter tube-width parameter recently gave solution path algorithm). also recent overview. hastie point drawback algorithm two-class work singular kernel matrices requires process algorithm occurring principal minors kernel matrix need invertible. required existing path algorithms mentioned above. however large kernel matrices often numerical rank even cases radial base function kernels used course also case linear svms sparse features application conjoint analysis discussed paper. inability deal singular sub-matrices probably main reasons none mentioned algorithms could eﬀectively applied medium/larger scale problems report algoobserving mentioned algorithms reporting solution paths parametric quadratic programming form point fact necessary diﬀerent algorithms problem variant. generic algorithms known quite words given value want retrieve optimal solution quickly without solve problem scratch. task solving problem possible values parameter called parametric quadratic programming. want variety machine learning methods particular many regularization methods direct instances parametric quadratic programming. examples include support vector machines support vector regression lasso surface reconstruction -regularized least squares compressed sensing shortly describe support vector machine popular example results regularization. later re-discover corresponding context choice based conjoint analysis. support vector machine. support vector machine standard tool two-class classiﬁcation problems. section estimating part-worth values choice based conjoint analysis seen problem geometrically dual binary classiﬁcation. primal soft margin c-svm following next present generic algorithm uses techniques; contrast murty’s method uses extremely simple elegant criss-cross method subroutine resulting believe simplest generic algorithm able deal arbitrary matrices algorithm works principle general continuous functions main idea transform parametric linear complementarity problem criss-cross method quickly update solution varies. also robust sense small errors tracking solution path. also algorithms faster sparse problems linear svms conjoint analysis need matrix inversions. comparison ways deal degeneracies. instead using described generic criss-cross method another obvious avoid degeneracies caused singular sub-matrices objective function small value diagonal entry original matrix subsequently simple methods regular case used. several problems approach. first rank objective function matrix blown artiﬁcially potential using efﬁcient small-rank-qp methods would wasted. secondly solution path perturbed problem diﬀer substantially original problem; particular perturbation lead much higher number bends therefore higher tracking cost computed solutions could real solutions. contrast criss cross method avoids issues since always solves original unperturbed problem. quadratic program problem minimizing convex quadratic function subject linear equality inequality constraints. here interested parameterized quadratic programs form criss-cross method combinatorial method ﬁnding vectors satisfy given ﬁxed method guaranteed terminate given suﬃcient matrix e.g. matrix class contains matrices meaning criss-cross method applicable setting. description special case matrices performance. analysis algorithm calculates entire solution path parametric quadratic program ﬁnite time. also well suited make sparseness solutions property regularization methods. running algorithm relevant size matrices deal bounded number number non-zero entries plus odds ends. solution path computed discontinuous since solution jump move fact general unique solution criss-cross method control optimal solution ﬁnds. however strictly wants continuity simply insert connecting straight-line segments since endpoints solutions intermediate points solutions well. holds x-part convexity optimal region also holds w.r.t. result adler gale work even assume optimal solution throughout method handle general case. start µmin unsolvable unsolvable situation later. order trace situation simgeneral conjoint analysis includes tasks preference data assessment analysis assessed data. choice based conjoint analysis preference data assessed options sequence choice experiments. every choice experiment consumer choose preferred option options presented her/him options assumed carry conjoint since depends linearly linear functions) easily compute largest value also every value still solution right-hand side order able trace solution beyond apply criss-cross method again starting basis righthand side symbolic parameter meant represent arbitrarily small positive value. solve slightly perturbed starting solution practice expect take iterations. theoretical guarantees this though. increasing therefore subdivide interval pieces solution therefore also solution linear ﬁnitely many pieces since basis repeat also valid intermediate value). figure important property duality makes useful application duality non-vertical hyperplanes points preserves relative positions. dual points labeled lowercase letters dual hyperplanes capital letters. hyperplane separates points according labels. several geometric duality transform know hyperplanes points vice versa example principle allow transform problem compute part-worth values standard two-class classiﬁcation problem. duality transform consider maps non-vertical hyperplanes points vice versa figure example since many hyperplanes vertical i.e. parallel m’th coordinate axis augment hyperplane normals coordinate value coordinate leads two-class classiﬁcation problem parameterized formulating population also individuals inconsistent choices. also contradictory information proceed diﬀerence dualizing work soft margin c-svm deal contradictions. leads following called part-worth value level i.e. value contributes overall value option level present. goal choice based conjoint analysis compute/estimate part-worth values attribute levels choice data. regularization approach compute part-worth values. goal review computing part-worth values choice based conjoint analysis naturally leads geometrically dual formulation details. assuming scale linear parttor called feasible satisﬁes constraints. feasible vectors general double cone whose apex origin. among feasible vectors want choose good generalization properties. phrased two-class classiﬁcation problem follows hyperplane closed halfspaces bounded hab. note preferred choice experiment constraint form otherwise preferred choice experiment assign label spectively hyperplane depending outcome choice experiment hyperplane. since label attached hyperplane opposite label attached restrict hyperplanes every pair e.g. ﬁxing j’th choice experiment. slack penalj= assuming information choice experiments standard trade-oﬀ parameter model complexity quality observed data. problem already suggested evgeniou arbitrary order elements considering hyperplanes comes order. given labelled hyperplanes input looking point feasible cone written intersection halfspaces iment linear two-class classiﬁcation situation around given labelled points looking compute part-worth values without giving details well suited task. similar resulting formulation also known ranking representing features present parameter levels option. test criss-cross method provided experimental proof concept implementation. implementation current stage really eﬃcient results number iterations needed criss-cross method along path promising. hope fully exploit behavior state implementation near future. tested criss-cross method choice based conjoint analysis data obtained larger user study measure perceived quality visualization task conjoint study parameters respectively levels. total study comprised levels estimate part-worth value. estimate part-worth values participants study provide answers choice experiments. hence problem leads problem whose choice experiments exemplary paths need iterations criss-cross method ﬁrst three bends c-interval clearly shows even starting point solution path needs time computed described criss-cross method eﬀective continue path bends. presented generic solution path algorithm parameterized quadratic programs works regularization methods result single parametric quadratic program also kernel matrix full rank. since state solution methods machine learning moving away ﬁnding exact solutions faster approximate methods would interesting research topic investigate paths approximate solutions parametrized quadratic programs. also investigated multi-parametric programming approaches help several parameters simultaneously regularization parameter regression tube width also kernel parameters parametric quadratic programming problem. applied mathematics parallel computing festschrift klaus ritter h.fischer b.riedemller s.schﬄer heidelburg physica-verlag pages figueiredo nowak wright. gradient projection sparse reconstruction application compressed sensing inverse problems. selected topics signal processing ieee journal gustafsson herrmann huber. conjoint analysis instrument market research practice. gustafsson herrmann huber editors conjoint measurement. methods programming problems. mathematical programming proceedings international congress mathematical programming. janeiro april richard cottle milton luiz kelmanson bernhard korte pages", "year": 2009}