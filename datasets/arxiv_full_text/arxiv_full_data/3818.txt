{"title": "Investigating gated recurrent neural networks for speech synthesis", "tag": ["cs.CL", "cs.NE"], "abstract": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.", "text": "recently recurrent neural networks powerful sequence models re-emerged potential acoustic model statistical parametric speech synthesis long short-term memory architecture particularly attractive addresses vanishing gradient problem standard rnns making easier train. although recent studies demonstrated lstms achieve signiﬁcantly better performance spss deep feedforward neural networks little known why. attempt answer questions lstms work well sequence model spss; component important. present visual analysis alongside series experiments resulting proposal simpliﬁed architecture. simpliﬁed architecture signiﬁcantly fewer parameters lstm thus reducing generation complexity considerably without degrading quality. statistical parametric speech synthesis quite steadily advanced naturalness past decade witnessed series blizzard challenges however quality synthetic speech produced spss still natural human speech cannot compete best unit selection systems concatenate waveforms suggested acoustic modelling captures complex relationship linguistic acoustic representations limiting factor focus work. relation prior work neural networks re-emerged potential powerful acoustic model spss. feed-forward neural networks employed linguistic representation derived input text directly acoustic features. deep belief network used model relationship linguistic acoustic representations jointly. mixture density networks realvalued neural autoregressive density estimators proposed respectively predict acoustic feature distributions given input linguistic features. various implementations viewed replacement decision tree hmm-based speech synthesis; linguistic features acoustic features frame frame multiple hidden layers. however temporal sequence nature speech explicitly modelled network architectures. include temporal constraints proposed include contextual information stacking low-dimensional bottleneck features multiple consecutive frames still framework minimum trajectory error training sequence error training criterion proposed minimise utterance-level trajectory error rather frame-by-frame error. hand recurrent neural networks directly elegantly include temporal information network architecture making attractive modelling speech parameter trajectories. standard employed predict prosodic information speech synthesis. variants standard rnns elman clockwork investigated speech synthesis. widely used recurrent network speech processing applications long short-term memory architecture. lstm addresses vanishing gradient problem standard easier train. lstm employed model contour. bidirectional lstm employed sequence linguistic features corresponding sequence acoustic features. lstm recurrent output layer proposed perform sequence mapping linguistic acoustic representations. studies formulate spss sequence-to-sequence mapping demonstrate effectiveness lstms. however lstm architecture seems rather ad-hoc obvious various components actually contributing performance. raises least questions answered previous studies exactly lstm architecture model speech parameter sequence; components lstm architecture important could discarded. answers questions suggest better perhaps simpler recurrent network architectures. first give analysis forget gate memory cell lstm architecture. speciﬁcally visualise activation forget gate understand forget gate resets memory cell state forget gate relates speech structure. analyse cell state correlates trajectory predicted. visualisations enable understand lstms model temporal structure speech synthesis. best knowledge ﬁrst attempt visually analyse lstm architecture predicting speech parameter sequence. second analyse importance lstm component speech synthesis propose simpliﬁed architecture. analysis done empirically several variants lstm. removes different component vanilla lstm. analysis inspired studies focus speech synthesis application. based analysis present simpliﬁed architecture forget gate. simpliﬁed architecture signiﬁcantly fewer parameters vanilla lstm reduces computational cost generation considerably without degrading quality synthesised speech. standard rnns hard train well-known vanishing exploding gradient problems address vanishing gradient problem lstm architecture proposed basic idea presented commonly used architecture described formulated formulations input gate forget gate cell state output gate block output time instance respectively; sigmoid tangent activation functions respectively; input time weight matrices applied input recurrent hidden units respectively; peep-hole connections biases respectively; means element-wise product. call vanilla lstm. central idea lstm so-called memory cell maintains state time gating units used regulate information memory cell speciﬁcally input gate allow input signal adjust cell state prevent output gate allow cell state affect neurons block that; forget gate enables cell remember forget previous state. however discussed architecture might optimal tasks relative importance component clear. section present several variants lstm propose simpliﬁed version forget gate; therefore signiﬁcantly fewer parameters lower computational cost. variants share lstm concept memory cell gates call gated recurrent neural networks. four variants lstm assess importance component start four variants lstm architecture. removes component lstm architecture understand much component contributes performance. differences vanilla lstm variant past cell state still contribute current cell state without controlling scaling forget gate. note that removing input forget output gates number parameters reduced. formulae observed architecture similar lstm without separate memory cell. peep-hole connections output activation functions combines input forget gates update gate balance previous activation candidate activation ˜ht. reset gate allows forget previous state. removed without degrading speech synthesis performance signiﬁcantly. hence propose even simpler variant removes output gates peep-hole connections replaces input gate forget gate form forget gate retained. simplest variant written simpliﬁed architecture similar except uses memory cell state. cell state controlled forget gate only trades past cell state current block input. activation forget gate small cell state mainly depend block input otherwise mainly copy past cell state. experimental setup corpus british male speaker employed experiments divided three subsets training development testing sampling rate used straight vocoder extract -dimensional mel-cepstral coefﬁcients band aperiodicities fundamental frequency log-scale frame step. systems used acoustic features. linearly interpolated before modelling binary voiced/unvoiced feature used record voicing information. dynamic features mccs baps also computed. acoustic features mean-variance normalised modelling mean variance restored generation time. generation time maximum likelihood parameter generation algorithm applied smooth parameter trajectories. systems used input linguistic features comprising features. binary features derived linguistic context quin-phone identities partof-speech positional information phoneme syllable word phrase number syllables words phrases etc. remaining numerical features capture frame position information e.g. frame position state phoneme. linguistic features normalised modelling. rnns employed three-layer feed-forward neural network bottom. feed-forward layers used gated recurrent neural networks. bottom feed-forward layers intended feature extraction layers hidden units using tangent activation function layer. implementations used units recurrent layer. hyperparameters system optimised development set. ﬁxed momentum tuned learning rates. analysis lstm ﬁrst visualised forget gate cell state thought important components modelling long-term temporal structure. averaged activations forget gate function frame index presented fig. solid line forget gates averaged activations; blue dashed lines show phoneme boundaries. clear peaks forget gate activation trajectory strong correspondence phoneme boundaries; within phoneme contribution past cell state decays linearly. forget gate capturing important temporal structure speech; surprising since phoneme boundaries explicitly represented input linguistic features. memory cell maintains state time could store trend trajectory predicted. analyse relationship cell states trajectories computed correlation cell states ﬁrst trajectory found cell state highest correlation ﬁrst trajectory. correlation high comparison presented fig. shows cell state tracks shape trajectory. table objective measures. mel-cepstral distortion. distortion band aperiodicities. rmse calculated linear scale. v/uv voiced/unvoiced error. note number parameters listed recurrent layer include bottom three feed-forward layers size across systems. generation time generate utterances development testing sets. roughly predict model performance. objective results table compared lstm achieve similar objective distortion considerably fewer parameters lower generation time input gate output gate peep-hole connections necessary. system increases distortion considerably forget gate important. ﬁnding consistent system achieves similar performance lstm system even though even fewer parameters performs well nph. also consistent studies tasks although s-lstm slightly increases distortion compared lstm achieves similar performance measures. s-lstm half number parameters recurrent layer compared lstm reduces generation time seconds seconds. generation time total time generate utterances development testing sets. system achieves preference score paired lstm preferred time. objective results table conclude forget gate critical component lstm architecture; input gate output gate peep-hole connections omitted. forget gate learn temporal structure speech; activations high correspondence phone boundaries. memory cell maintains state time task forget gate critical component lstm; components omitted reduction naturalness. results propose simpliﬁed lstm architecture uses critical forget gate. simpliﬁed lstm signiﬁcantly fewer parameters vanilla lstm achieves similar performance objective subjective evaluations. acknowledgements research supported epsrc programme grant ep/i/ natural speech technology research data collection accessed http//datashare.is.ed.ac.uk/handle//. heiga andrew senior mike schuster statistical parametric speech synthesis using deep neural networks proc. ieee int. conf. acoustics speech signal processing heng simon king oliver watts combining vector space representation linguistic context deep neural network text-to-speech synthesis proc. isca speech synthesis workshop qian yuchen wenping frank soong training aspects deep neural network parametric synthesis proc. ieee int. conf. acoustics speech signal processing qiong zhizheng korin richmond junichi yamagishi yannis stylianou ranniery maia fusion multiple parameterisations dnn-based sinusoidal speech synthesis multi-task learning proc. interspeech heiga andrew senior deep mixture density networks acoustic modeling statistical parametric speech synthesis proc. ieee int. conf. acoustics speech signal processing benigno uria iain murray steve renals cassia valentini modelling acoustic feature dependencies artiﬁcial neural networks trajectory-rnade proc. ieee int. conf. acoustics speech signal processing zhizheng cassia valentini-botinhao oliver watts simon king deep neural networks employing multi-task learning stacked bottleneck features speech synthesis proc. ieee int. conf. acoustics speech signal processing yuchen qian frank soong sequence generation error minimization based deep neural networks training text-to-speech synthesis proc. interspeech sin-horng chen shaw-hwa hwang yih-ru wang rnn-based prosodic information synthesizer mandarin text-to-speech ieee transactions speech audio processing vol. sivanand achanta tejas godambe suryakanth gangashetty investigation recurrent neural network architectures statistical parametric speech synthesis proc. interspeech raul fernandez asaf rendel bhuvana ramabhadran hoory prosody contour prediction long short-term memory bi-directional deep recurrent neural networks proc. interspeech heiga hasim unidirectional long short-term memory recurrent neural network recurrent output layer low-latency speech synthesis proc. ieee int. conf. acoustics speech signal processing rafal jozefowicz wojciech zaremba ilya sutskever empirical exploration recurrent network architectures proc. ieee int. conf. machine learning kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations using arxiv encoder-decoder statistical machine translation preprint arxiv. alain cheveign´e restructuring speech representations using pitch-adaptive time–frequency smoothing instantaneous-frequency-based extraction possible role repetitive structure sounds speech communication vol. keiichi tokuda takayoshi yoshimura takashi masuko takao kobayashi tadashi kitamura speech parameter generation algorithms hmm-based speech synthesis proc. ieee int. conf. acoustics speech signal processing", "year": 2016}