{"title": "Double-Base Asymmetric AdaBoost", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Based on the use of different exponential bases to define class-dependent error bounds, a new and highly efficient asymmetric boosting scheme, coined as AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical derivation procedure, unlike most of the other approaches in the literature, our algorithm preserves all the formal guarantees and properties of original (cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel derivation scheme enables an extremely efficient conditional search procedure, dramatically improving and simplifying the training phase of the algorithm. Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is able to save over 99% training time with regard to Cost-Sensitive AdaBoost, providing the same cost-sensitive results. This computational advantage of AdaBoostDB can make a difference in problems managing huge pools of weak classifiers in which boosting techniques are commonly used.", "text": "based diﬀerent exponential bases deﬁne class-dependent error bounds highly eﬃcient asymmetric boosting scheme coined adaboostdb proposed. supported fully theoretical derivation procedure unlike approaches literature algorithm preserves formal guarantees properties original adaboost similarly state-of-the-art costsensitive adaboost algorithm. however advantage adaboostdb novel derivation scheme enables extremely eﬃcient conditional search procedure dramatically improving simplifying training phase algorithm. experiments synthetic real datasets reveal adaboostdb able save training time regard cost-sensitive adaboost providing cost-sensitive results. computational advantage adaboostdb make diﬀerence problems managing huge pools weak classiﬁers boosting techniques commonly used. boosting algorithms adaboost epitome active focus research since ﬁrst publication strong theoretical guarantees together promising practical results including plethora diﬀerent applications implicit classiﬁcation tasks well-deﬁned costs depending diﬀerent kinds mistakes possible decision hand many problems imbalanced class priors class extremely frequent easier sample one. deal scenarios classiﬁers must capable focusing attention rare class instead searching hypothesis that trying well data driven prevalent class. several modiﬁcations adaboost proposed literature deal asymmetry well-known viola jones face detector framework validation used modify adaboost strong classiﬁer threshold posteriori order adjust false positive detection rates balance. nevertheless authors stated clear selected weak classiﬁers optimal asymmetric goal modiﬁcations preserve adaboost training generalization errors original guarantees vast majority proposed methods cope asymmetry direct manipulations weight updating rule. proposals full reformulation algorithm asymmetric scenarios analyzed heuristic modiﬁcations adaboost. however recent contributions proposed deal asymmetric boosting problem fully theoretical hand cost-sensitive boosting framework masnadi-shirazi vasconcelos drives algorithm complex computationally demanding original adaboost strong theoretical guarantees. hand class-conditional description adaboost landesa-v´azquez j.l. alba-castro demonstrates asymmetric weight initialization also eﬀective theoretically sound reach boosted cost-sensitive classiﬁers. theoretical alternatives follow diﬀerent asymmetrizing perspectives drive diﬀerent solutions. work follow approach closer cost-sensitive boosting framework though sharing equivalent theoretical roots guarantees cost-sensitive adaboost proposal entails selfcontained analytical framework leading novel asymmetric boosting algorithm call adaboostdb approach based three distinctive premises derivation inspired generalized boosting framework error bound modeled terms class-conditional exponential bases parallel class-conditional weight subdistributions used updated boosting iterations. result diﬀerent perspective following completely diﬀerent derivation path reach algorithm able solution cost-sensitive adaboost much eﬃcient way. indeed approach gives rise tractable mathematical model enables searching scheme dramatically reduces number weak classiﬁers evaluated iteration. paper organized follows next section describe adaboost original algorithm asymmetric variations proposed literature paying special attention cost-sensitive adaboost algorithm section adaboostdb conditional search method derived explained discussed. section empirical framework experiments shown. finally section includes main ideas conclusions future research lines drawn work. given space feature vectors possible class labels adaboost goal learn strong classiﬁer weighted ensemble weak classiﬁers predicting label instance training examples labeled positive negative weight distribution deﬁned learning round weak learner must select best classiﬁer according labels weights. weak classiﬁer selected added ensemble modulated goodness parameter correspondingly updating weight distribution. weak hypothesis search guided maximize goodness equivalent maximize weighted correlation labels predictions procedure many studies focus discrete version adaboost simpler intuitive analysis case weak hypothesis binary parameter rewritten terms weighted error weak hypothesis equivalent ﬁnding classiﬁer smaller seen follow notation operator true otherwise. addition sake simplicity term ‘ok’ refer training examples result weak classiﬁer right ‘nok’ wrong yi}. initially deﬁned exponential error bound direct class-dependent behavior several modiﬁcations adaboost proposed literature enhance seemingly symmetric nature. proposed variations based directly modifying weight update rule asymmetric way. however since update rule consequence error bound minimization process changes really aﬀecting theoretical properties optimality adaboost cannot guaranteed. considering previous variations heuristic masnadi-shirazi vasconcelos proposed theoretically sound approach based statistical view boosting. according interpretation boosting algorithms seen round-by-round estimations building additive logistic regression model exponential error bound modeled minimization next expression means expectation following analytical guidelines proposed schapire singer section present theoretically derive asymmetric generalization adaboost adaboostdb based modifying usual costinsensitive exponential error bound class-dependent bases. based inequality equation original adaboost formulation geared minimize exponential error bound weighted training error minimization purposes speciﬁc exponential base irrelevant whenever simplicity selected base classical formulation adaboost suppose without loss generality training divided meaningful subsets deﬁne exponential bounds diﬀerent bases one. calling bases decomposed exponential bound expressed equation assume without loss generality base-dependent behavior graphically analyzed figure greater base respect other penalized respective errors. therefore associating positive examples subclass negative ones imbalanced behavior directly mapped class imbalanced cost-sensitive approach. rewriting expression terms asymmetric exponents double-base perspective immediately linked costsensitive boosting framework approaches equivalently parameterized class-conditional costs positives negatives respectively) statistical meaning figure misclassiﬁcation adaboost exponential training error bounds diﬀerent bases. ﬁnal score strong classiﬁer represented horizontal axis vertical axis loss related possible score. seen double-base approach shares cost-sensitive adaboost common theoretical root. however change point view along derivation inspired original framework schapire singer allow follow diﬀerent derivation pathway ending much eﬃcient formulation. suppose again ﬁrst examples training positives rest negatives base-dependent behavior results class-dependent one. case also split initial weight distribution class-dependent subdistributions positives negatives respectively error components formally identical original bound allowing directly insert diﬀerent exponential bases them. wanted. original adaboost formulation initial weight subdistributions extrapolated round-by-round ones iteratively updated normalized analogous way. obtained unraveling weight update rule allows decouple class error bound factors depending previous rounds ap/nt− depending performance current round zp/nt convexity exponential functions minimum bound analytically found canceling derivative. deﬁning cost parameters commented previous section log) bearing mind equation goal derivative expressed equation strictly true discrete case weak hypothesis however weak hypothesis real range equation would transform upper bound explained case would minimizing upper bound instead directly behavior original adaboost real-valued weak predictors. point unknown variable minimization equation modeled polynomial making change variable rewriting terms parameters instead hyperbolic model used latter equation independent variable general possible solutions which nature problem interested real positive. easy deﬁnition real values interval. consequence sign change consecutive coeﬃcients polynomial descartes’ rule signs ensure equation real positive solution solution. straightforward solve posed problem calculating zeros polynomial ﬁnally keep real positive root. process repeated possible weak hypothesis order ﬁnally select leading greatest goodness greatest root. direct mechanism requiring scalar search similar proposed cost-sensitive adaboost computational advantage evaluating polynomial instead hyperbolic function. main drawback straightforward solution section still requires search associated root every classiﬁer every boosting round. could expensive computational burden terms example applications needing select hundreds thousands diﬀerent classiﬁers evaluated several thousands training examples computer vision algorithms nevertheless slight change point view serve drastically reduce computational burden. deﬁne functions follows rewrite equation ﬁrst function polynomial whose coeﬃcients parameters depend previous boosting rounds. second coeﬃcients also depending dependence current round well. result minimization procedure given round modeled crossing point static function ﬁxed current round variable function figure crossing point scenario static variable functions modeling minimization problem. graphical representation contribution improvement conditions. descartes’ rule signs ensures existence crossing point solutions satisfying interesting classiﬁcation problem weak hypothesis goodness i.e. really contributing strong classiﬁer. contribution condition formalized follows weak classiﬁer meet requirement directly discarded current round without computation. hand computed valid solution comparatively evaluate candidate need know related root greater already have. using information would eﬀectively calculate speciﬁc root weak classiﬁers greater roots directly rejecting ones. bearing mind increasing functions given possible weak classiﬁers associated functions solution ﬁrst them second classiﬁer better ﬁrst call rule improvement condition. applying conditions weak hypothesis searching process nested average number zeros eﬀectively computed decreases respect straightforward solution consuming time important emphasize improved searching technique coined conditional search huge computational saving brings made possible polynomial double-base modeling proposed framework. show assess performance adaboostdb practical terms conducted series empirical experiments analyze asymmetric behavior algorithm comparing theoretical optimal classiﬁers cost-sensitive adaboost using synthetic real datasets. detection problems costs related good decisions considered null cost matrix dependent errorrelated parameters directly assimilable previous theoretical analysis. bearing mind optimal decision unchanged cost matrix multiplied constant resulting matrix actually degree freedom call asymmetry problem. curves nevertheless alternative representation proposed drummond r.c. holte dual respect traditional curves based expected costs shown appropriate cost-sensitive classiﬁcation problems experimental analysis based representations. following guidelines probability cost function normalized expected cost deﬁned equations prior probabilities example positive negative respectively false negatives false positives rates obtained classiﬁer. ﬁrst step going compare adaboostdb classiﬁers optimal bayes classiﬁers counterparts diﬀerent cost combinations. deﬁned synthetic dataset scenario easily calculate theoretically optimal classiﬁer following bayes risk rule. synthetic scenario illustrated figure bivariate normal point clouds positives negatives priors variances diﬀerent means. customary many boosting works weak learners stumps computed case projection points discrete range angles space diﬀerent random datasets generated training test. nineteen diﬀerent asymmetries evaluate also deﬁned trying sweep wide range cost combinations goal ﬁrst comparative test compute lower envelope classiﬁers cost space. cost space deﬁned relationship probability cost function normalized expected cost framework every classiﬁer though trained speciﬁc asymmetry tested arbitrary cost scenarios thus drawing line passing cost space. result family classiﬁers represented collection lines whose lower envelope deﬁnes minimum cost classiﬁer along operating range comparing three resulting lower envelopes appreciate equivalent behavior slight diﬀerences among them. second comparative test among classiﬁers tested speciﬁc asymmetry trained for. results seen figure adaboostdb performance follows trend bayes optimal classiﬁer describing consistent gradual asymmetric behavior across diﬀerent costs studied parameters moreover figure lower envelope graphic representations three classiﬁers families bayes adaboostdb cost-sensitive adaboost. figure shows three lower envelopes superimposed. figure performance comparison classiﬁers obtained adaboostdb bayes cost-sensitive adaboost speciﬁc asymmetry bayes synthetic test set. false positives false negatives classiﬁcation error normalized expected cost. real datasets selected several datasets asymmetric deﬁnition machine learning repository considered positives valuable classes according original problems. synthetic real cases weak learners stumps. every dataset every cost requirement followed -fold cross-validation strategy evaluate asymmetric performance whole dataset divided three subsets leaving iteratively test forming training set. result every dataset-cost combination obtain performance averages three classiﬁers. obtained results shown table expected positives become costly negatives false negative rates tend decrease false positives rates tend increase. opposite situation roles accordingly exchanged showing progressive consistent asymmetric behavior generalized across datasets cost combinations. information table supplemented global performance measures classiﬁcation error normalized expected cost experiment. explained section adaboostdb cost-sensitive adaboost share common theoretical root diﬀer model derive equivalent starting point. result frameworks give rise diﬀerent algorithms must obtain solution given problem. scenario consequences side though classiﬁers obtained algorithms theoretically identical trained conditions practice numerical errors make diﬀer. side polynomial model conditional search mechanism important highlight cost-sensitive adaboost shown outperform previous asymmetric approaches literature seen consequence order avoid redundant experiments already published works focused eﬀorts comparing method cost-sensitive adaboost demonstrate that thought diﬀerent computational burden algorithms equivalent classiﬁcation performance. encourage reader consult deepen comparison algorithms since classiﬁcation performance diﬀerences cost-sensitive adaboost adaboostdb numerical errors negligible. commented though theoretically equivalent classiﬁers obtained adaboostdb cost-sensitive adaboost tend diﬀer numerical errors related diﬀerent model adopted case. section seen diﬀerences bayes scenario negligible. test relevance diﬀerence used datasets cost combinations -fold crossvalidation strategy used last section applied cost-sensitive adaboost. mean error alternatives tabulated table seen order hundredths worst case. make visual interpretation diﬀerences also computed mean standard deviation across datasets normalized expected cost every trained cost-combination. result seen figure diﬀerences range thousandths. could expect classiﬁcation performance diﬀerences negligible cases. next item empirical comparison quantifying terms time number evaluated zeros accelerating power adaboostdb respect cost-sensitive boosting. task recorded time consumed train classiﬁers used previous tests adaboostdb cost-sensitive adaboost plus variation adaboostdb also computed without conditional search order figure mean error standard deviation normalized expected cost between adaboostdb cost-sensitive boosting across datasets every cost-combination. results shown table seen last table polynomial model even evaluating number zeros gets average training time saving respect hyperbolic model cost-sensitive boosting. hand conditional search method achieves reduction total number evaluated zeros driving full version adaboostdb consume time average used costsensitive boosting. times faster. last experiment trained adaboostdb learning algorithm simple mono-stage face detector using haar-like features kind real-world asymmetric problem boosting commonly used. purpose used balanced subset cbcl training face non-face paper presented derived empirically tested cost-sensitive adaboost scheme adaboostdb based double-base exponential error bounds. sharing equivalent theoretical root costsensitive boosting opposed asymmetric approaches literature adaboostdb supported full theoretical derivation makes possible preserve formal guarantees original adaboost general asymmetric scenario. approach based three basic mainstays double-base perspective derivation scheme based generalized boosting framework polynomial model problem distinctive features whole also enable conditional search method increase compactness ease eﬃciency algorithm. consequence adaboostdb training consumes time average needed cost-sensitive adaboost reach solution. computational advantage make diﬀerence applications coping huge number weak hypothesis object detection computer vision. point next steps research require thorough comparison adaboostdb/cost-sensitive adaboost adaboost asymmetric weight initialization order clarify theoretically practically diﬀerent properties advantages disadvantages asymmetry model. work partially supported european regional development fund galician regional government project consolidation research units atlanttic galician regional funds research contract", "year": 2015}