{"title": "Training Quantized Nets: A Deeper Understanding", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.", "text": "currently deep neural networks deployed low-power portable devices ﬁrst training full-precision model using powerful hardware deriving corresponding lowprecision model efﬁcient inference systems. however training models directly coarsely quantized weights step towards learning embedded platforms limited computing resources memory capacity power consumption. numerous recent publications studied methods training quantized networks studies mostly empirical. work investigate training methods quantized neural networks theoretical viewpoint. ﬁrst explore accuracy guarantees training methods convexity assumptions. look behavior algorithms non-convex problems show training algorithms exploit high-precision representations important greedy search phase purely quantized training methods lack explains difﬁculty training using low-precision arithmetic. deep neural networks integral part state-of-the-art computer vision natural language processing systems. high memory requirements computational complexity networks usually trained using powerful hardware. increasing interest training deploying neural networks directly battery-powered devices cell phones platforms. low-power embedded systems memory power limited cases lack basic support ﬂoating-point arithmetic. make neural nets practical embedded systems many researchers focused training nets coarsely quantized weights. example weights constrained take integer/binary values represented using low-precision ﬁxed-point numbers. quantized nets offer potential superior memory computation efﬁciency achieving performance competitive state-of-the-art high-precision nets. quantized weights dramatically reduce memory size access bandwidth increase power efﬁciency exploit hardware-friendly bitwise operations accelerate inference throughput handling low-precision weights difﬁcult motivates interest training methods. learning rates small stochastic gradient methods make small updates weight parameters. binarization/discretization weights training iteration rounds small updates causes training stagnate thus naïve approach quantizing weights using rounding procedure yields poor results weights represented using small number bits. approaches include classical stochastic rounding methods well schemes combine full-precision ﬂoating-point weights discrete rounding procedures schemes seem work practice results area largely experimental little work devoted explaining excellent performance methods poor performance others important differences behavior methods. contributions paper studies quantized training methods theoretical perspective goal understanding differences behavior reasons success failure various methods. particular present convergence analysis showing classical stochastic rounding methods well newer powerful methods like binaryconnect capable solving convex discrete problems level accuracy depends quantization level. address issue algorithms maintain ﬂoating-point representations like work well fully quantized training methods like stall training complete. show long-term behavior important annealing property needed non-convex optimization classical rounding methods lack property. arithmetic operations deep networks truncated -bit ﬁxed-point without signiﬁcant deterioration inference performance extreme scenario quantization binarization -bit used weight representation previous work obtaining quantized neural network divided categories quantizing pre-trained models without retraining training quantized model scratch focus approaches belong second category used training inference constrained resources. training quantized scratch many authors suggest maintaining high-precision ﬂoating point copy weights feeding quantized weights backprop results good empirical performance. limitations using methods low-power devices however ﬂoating-point arithmetic always available desirable. another widely used solution using low-precision weights stochastic rounding experiments show networks using -bit ﬁxed-point representations stochastic rounding deliver results nearly identical -bit ﬂoating-point computations lowering precision -bit ﬁxed-point often results signiﬁcant performance degradation bayesian learning also applied train binary networks comprehensive review found objective function decomposes many functions neural networks objective functions form non-convex loss function. ﬂoating-point representations available standard method training neural networks stochastic gradient descent iteration selects function randomly computes learning rate paper consider problem training convolutional neural networks convolutions computationally expensive; precision weights used accelerate replacing expensive multiplications efﬁcient addition subtraction operations bitwise operations train networks using low-precision representation weights quantization function needed convert real-valued number quantized/rounded version notation quantizing vectors assume acts dimension vector. different quantized optimization routines deﬁned selecting different quantizers also selecting quantization happens optimization. common options denotes quantization step resolution i.e. smallest positive number representable. exception deﬁnition consider binary weights weights constrained values uniform rounding becomes sign. deterministic rounding maintains quantized weights updates form denotes low-precision weights quantized using immediately applying gradient descent update. gradient updates signiﬁcantly smaller quantization step method loses gradient information weights never modiﬁed starting values. produced uniform random number generator. operator nondeterministic rounds argument probability otherwise. quantizer satisﬁes important property similar deterministic rounding method optimization method also maintains quantized weights updates form binaryconnect binaryconnect algorithm accumulates gradient updates using full-precision buffer quantizes weights gradient computations follows. either stochastic rounding deterministic rounding used quantizing weights practice common choice. original binaryconnect paper constrains low-precision weights generalized recent method binary-weights-net allows different ﬁlters different scales quantization often results better performance large datasets. present convergence guarantees stochastic rounding binaryconnect algorithms updates form respectively. purposes deriving theoretical guarantees assume differentiable convex domain convex dimension consider case µ-strongly convex well weakly convex. also assume gradients bounded results also assume domain problem ﬁnite. case rounding algorithm clips values leave domain. example binary case rounding returns bounded values denotes quantization error t-th iteration. want bound error expectation. present following lemma. lemma stochastic rounding error iteration bounded expectation proofs theoretical results presented appendices. lemma rounding error step decreases learning rate decreases. intuitive since probability entry differing small gradient update small relative using lemma present convergence rate results stochastic rounding strongly-convex case non-strongly convex case. error estimates ergodic i.e. terms theorem assume µ-strongly convex learning rates given consider algorithm updates form then have cases converges reaches accuracy ﬂoor. quantization becomes grained theory predicts accuracy approaches highprecision ﬂoating point rate linear extra term caused discretization unavoidable since method maintains quantized weights. analyzing algorithm assume hessian satisﬁes lipschitz bound slightly non-standard assumption enables gain better insights behavior algorithm. results hold stochastic uniform rounding. case quantization error approach sr-sgd. nonetheless effect rounding error diminishes shrinking multiplies gradient update thus implicitly rounding error well. theorem assume l-lipschitz smooth domain ﬁnite diameter learning rates given consider bc-sgd algorithm updates form then have converge error ﬂoor. looks like convergence guarantees however things change assume strong convexity bounded hessian. theorem assume µ-strongly convex learning rates given consider algorithm updates form have error ﬂoor determined quadratic least-squares problem gradient linear hessian constant. thus following corollary. corollary assume quadratic learning rates given algorithm updates form yields figure method starts location adds perturbation rounds. learning rate gets smaller distribution perturbation gets squished near origin making algorithm less likely move. squishing effect part distribution lying left right effect relative probability moving left right. scale would expect perform fundamentally better seem like restrictive condition evidence even non-convex neural networks become well approximated quadratic later stages optimization within neighborhood local minimum note convergence results instead measures convergence directly comparable. possible bound used values converge usual sense global convergence results presented convex problems show that general algorithms converge within accuracy minimizer however results explain large differences methods applied non-convex neural nets. study long-term behavior differs note section makes convexity assumptions proposed theoretical results directly applicable neural networks. typical methods important exploration-exploitation tradeoff. learning rate large algorithm explores moving quickly states. exploitation happens learning rate small. case noise averaging causes algorithm greedily pursues local minimizers lower loss values. thus distribution iterates produced algorithm becomes increasingly concentrated near minimizers learning rate vanishes maintains property well—indeed corollary class problems iterates concentrate minimizer small section show method lacks important tradeoff stepsize gets small algorithm slows down quality iterates produced algorithm improve algorithm become progressively likely produce low-loss iterates. behavior illustrated figures understand problem conceptually consider simple case one-variable optimization problem starting iteration algorithm computes stochastic approximation gradient sampling distribution call gradient multiplied stepsize probability moving right roughly proportional magnitude note random variable distribution α−p. suppose small enough neglect tails outside interval probability transitioning using stochastic rounding denoted figure effect shrinking learning rate problem. left ﬁgure plots objective function histograms plot distribution quantized weights iterations. plots correspond bottom different learning rates learning rate shrinks distribution concentrates minimizer distribution stagnates. depend words provided small average makes decisions/transitions learning rate learning rate takes times longer make decisions used. situation exploitation beneﬁt decreasing figure shows plot loss function. visualize distribution iterates initialize iterations using quantization resolution figure shows distribution quantized weight parameters iterations optimized different learning rates shift distribution iterates transitions wide/explorative distribution narrow distribution iterates aggressively concentrate minimizer. contrast distribution produced concentrates slightly stagnates; iterates spread widely even learning rate small. figure markov chain example states. right ﬁgure halved transition probability moving states remaining probability self-loop. notice halving transition probabilities would change equilibrium distribution instead would increase mixing time markov chain. state transition probability depends learning rate ﬁxed clearly markov process transition matrix long-term behavior markov process determined stationary distribution show small stationary distribution nearly invariant thus decreasing threshold virtually effect long term behavior method. happens because shrinks relative transition probabilities remain even though absolute probabilities decrease case exploitation beneﬁt decreasing theorem denote probability distribution entry stochastic gradient estimate assume constant long term stationary behavior relatively insensitive convergence speed algorithm not. measure this consider mixing time markov chain. denote stationary distribution markov chain. \u0001-mixing time chain smallest integer show mixing time markov chain gets large small means exploration slows down even though exploitation gain realized. theorem satisfy assumptions theorem choose sufﬁciently small exists proper subset states stationary probability greater denote \u0001-mixing time chain learning rate then explore implications theory above train vgg-like networks residual networks binarized weights image classiﬁcation problems. cifar- train resnet- wide resnet- vgg- high capacity vgg-bc network used original model also train resnet- cifar- resnet- imagenet adam baseline optimizer found frequently give better results well-tuned train three quantized algorithms mentioned section i.e. r-adam sr-adam bc-adam. image pre-processing data augmentation procedures following quantize weights convolutional layers linear layers training initial learning rate decrease learning rate factor epochs cifar- cifar- imagenet experiments train model epochs decrease learning rate epochs appendix additional experiments. results overall results summarized table binary model trained bc-adam comparable performance full-precision model trained adam. sr-adam outperforms r-adam veriﬁes effectiveness stochastic rounding. performance sr-adam bc-adam across models datasets. consistent theoretical results sections predict keeping track real-valued weights bc-adam produce better minimizers. exploration exploitation tradeoffs section discusses exploration/exploitation tradeoff continuous-valued methods predicts fully discrete methods like unable enter greedy phase. test effect plot percentage changed weights function training epochs sr-adam explores aggressively; changes weights conv layers r-adam bc-adam keeps changing weights nearly weights differ starting values method never changes weights indicating stays near local minimizer explores less. interestingly weights conv layers changed r-adam; tails stochastic gradient distribution light method ineffective. section unable exploit local minima because small learning rates shrinking learning rate produce additional bias towards moving downhill. illustrated figure truly cause problem theory predicts improve performance low-precision training increasing batch size. shrinks variance gradient distribution figure without changing mean concentrates gradient distribution towards downhill directions making algorithm greedy. verify this tried different batch sizes including found larger batch size better performance figure illustrates effect batch size methods. method like classical performs best figure effect batch size sr-adam tested resnet- cifar-. test error epoch. test error reported dashed lines train error solid lines. percentage weight changes since initialization. percentage weight changes every epochs. small batch size. however large batch size essential method perform well. figure shows percentage weights changed training. large batch methods change weights less aggressively small batch methods indicating less exploration. figure shows percentage weights changed epochs training. clear small-batch changes weights much frequently using batch. property batch training clearly beneﬁts figure table batch training improved performance sr-adam consistently. addition providing means improving ﬁxed-point training suggests recently proposed methods using batches able exploit lower levels precision accelerate training. training quantized neural networks essential deploying machine learning models portable ubiquitous devices. provide theoretical analysis better understand binaryconnect stochastic rounding methods training quantized networks. proved convergence results methods predict accuracy bound depends coarseness discretization. general non-convex problems proved differs conventional stochastic methods unable exploit greedy local search. experiments conﬁrm ﬁndings show mathematical properties indeed observable practice. goldstein supported part national science foundation grant ofﬁce naval research grant n--- sloan foundation. studer supported part xilinx inc. grants eccs- ccf- career ccf-. samet supported part grant iis--. references courbariaux hubara soudry el-yaniv bengio binarized neural networks training deep neural networks weights activations constrained arxiv preprint arxiv. baldassi ingrosso lucibello saglietti zecchina subdominant dense clusters allow simple learning high computational performance neural networks discrete synapses. physical review letters ˜fwt ˜fwt) ˜fwt ˜fwt αtewt ˜fwt αtewt ˜fwt ˜fwt eαt∇ ˜fwt αtwt w∇fwt αtwt w∇fwt ∇fwt ˜fwt αtwt w∇fwt αtwt w∇fwt ∇fwt αtwt w∇fwt αtlrtwt using ewt+ αtwt w∇fwt αtwt w∇fwt largest scalar stochastic linear operator non-negative entries. non-negative entries column sums equal thus deﬁnes transition operator markov chain. denote stationary distribution markov chain transition matrix claim also stationary distribution verify noting error) corresponding elements since columns diagonal elements well approximated well approaches note perron-frobenius matrix eigenvalue thus multiplicity multiplicity eigenvalues/vectors matrix vary continuously small perturbations matrix follows that small stationary distribution distribution approaches leading eigenvector leading eigenvector follows stationary distribution approaches assumption whenever neighbors differ single coordinate. follows every state markov chain accessible every state traversing path non-zero transition probabilities every state suppose contradiction mixing time chain remains bounded vanishes. integer upper bounds \u0001-mixing time assumption theorem select states starting state distribution note train image classiﬁers using types networks vgg-like cnns residual networks cifar-/ imagenet vgg- cifar- consists convolutional layers fully connected layers. convolutional layers contain ﬁlters respectively. batch normalization relu convolutional layer ﬁrst fully connected layer. details architecture presented table vgg-bc high-capacity network used original method contains convolutional layers linear layers. architecture except using softmax cross-entropy loss instead squared hinge loss respectively. details architecture presented table resnets- convolutional layers linear layer contains three stages residual blocks stage number residual blocks. also create wide resnet- doubles number ﬁlters residual block resnets- imagenet description default minibatch size however big-batch sr-adam method adopts large minibatch size following weight decay training. implement models torch train quantized models nvidia gpus. similar quantize weights convolutional layers linear layers training. binarizing linear layers causes performance drop without much computational speedup. fully connected layers little computation overhead compared conv layers. also state-of-the-art cnns number parameters quite small. number params conv/fc layers cnns table vgg- vgg-bc resnet- wrn-- resnet ./.. vgg-like nets many parameters efﬁcient higher performing resnets almost entirely convolutional. figure training testing errors different training methods vgg- vgg-bc resnet- wideresnet-- resnet-. solid line training error dashed line testing error. experiments sr-adam r-adam weights convolutional layers intitialized random rademacher variables. authors adopt small initial learning rate takes epochs converge. observed large binary weights generate small gradients batch normalization used hence large learning rate necessary faster convergence. experiment larger learning rate converges performance within epochs comparing epochs original paper figure shows effect applying weight decay bc-adam. shown figure bc-adam weight decay yields worse performance compared zero weight decay. applying weight decay bc-adam shrink well increase distance figure shows distance training. weight decay average weight difference approaches indicates close zero. weight decay cannot decay weight binarized networks.", "year": 2017}