{"title": "Sign-Constrained Regularized Loss Minimization", "tag": ["cs.LG", "cs.AI"], "abstract": "In practical analysis, domain knowledge about analysis target has often been accumulated, although, typically, such knowledge has been discarded in the statistical analysis stage, and the statistical tool has been applied as a black box. In this paper, we introduce sign constraints that are a handy and simple representation for non-experts in generic learning problems. We have developed two new optimization algorithms for the sign-constrained regularized loss minimization, called the sign-constrained Pegasos (SC-Pega) and the sign-constrained SDCA (SC-SDCA), by simply inserting the sign correction step into the original Pegasos and SDCA, respectively. We present theoretical analyses that guarantee that insertion of the sign correction step does not degrade the convergence rate for both algorithms. Two applications, where the sign-constrained learning is effective, are presented. The one is exploitation of prior information about correlation between explanatory variables and a target variable. The other is introduction of the sign-constrained to SVM-Pairwise method. Experimental results demonstrate significant improvement of generalization performance by introducing sign constraints in both applications.", "text": "sign constraints introduce prior knowledge directly learning machines. example consider binary classiﬁcation task. case h-th explanatory variable positively correlated binary class label positive weight coeﬃcient expected achieve better generalization performance negative coeﬃcient because without sign constraints entry optimal solution might negative small sample problem. hand case negatively correlated class label negative weight coeﬃcient would yield better prediction. sign constraints explicitly imposed inadequate signs coeﬃcients could avoided. strategy sign constraints generic learning problems rarely discussed although extensive reports non-negative least square regression supported many successful applications including sound source localization tomographic imaging spectral analysis hyperspectral image super-resolution microbial community pattern detection face recognition non-negative image restoration them non-negative least square regression used important ingredient bigger methods nonnegative matrix factorization several eﬃcient algorithms non-negative least square regression developed. active method lawson hanson widely used many years several work accelerated optimization combining active method projected gradient approach. interior point methods proposed alternative algorithm nonnegative least square regression. however cannot applied generic regularized loss minimization problems. paper present algorithms signconstrained regularized loss minimization problem practical analysis domain knowledge analysis target often accumulated although typically knowledge discarded statistical analysis stage statistical tool applied black box. paper introduce sign constraints handy simple representation non-experts generic learning problems. developed optimization algorithms sign-constrained regularized loss minimization called sign-constrained pegasos sign-constrained sdca simply inserting sign correction step original pegasos sdca respectively. present theoretical analyses guarantee insertion sign correction step degrade convergence rate algorithms. applications signconstrained learning eﬀective presented. exploitation prior information correlation explanatory variables target variable. introduction sign-constrained svm-pairwise method. experimental results demonstrate signiﬁcant improvement generalization performance introducing sign constraints applications. widely used loss functions satisfy assumptions. several examples loss functions described table hinge loss chosen learning machine well-known instance called support vector machine. square error loss chosen learning machine called ridge regression. denote optimal lution constraint problem argmin paper. function -smooth function vector subvector containing entries corresponding sub-matrix containing columns corresponding r|a| deﬁned =xi∈a original pegasos algorithm assumed classical hinge loss function iterate consists three steps mini-batch selection step gradient step projection-onto-ball step. mini-batch selection step chooses generic loss functions. surge algorithms unconstrained regularized empirical loss minimization developed svrg prox-svrg saga kaczmarz emgd finito study focuses popular algorithms pegasos sdca prominent characteristic algorithms unnecessity choose step size. optimization algorithms guarantee convergence optimum assumption small step size although step size often small used. meanwhile theorem pegasos developed step size large enough adopted actually. sdca needs step size. algorithms developed study sign-constrained problems simple modiﬁcations pegasos sdca. regularized loss minimization called sc-pega scsdca developed simply inserting sign correction step introduced section original pegasos sdca. learning eﬀective presented. exploitation prior information correlation explanatory variables target variable. introduction sign-constrained svm-pairwise method original sdca framework unconstrained problems sdca dual problem solved instead primal problem. namely dual objective maximized iterative fashion respect dual variables maximizer single example chosen randomly iterate single dual variable optimized variables frozen. denote value dual vector previous iterate dual vector updated ∆αei determined argmax ∆α∈r projection-onto-ball step plays important role getting smaller upper-bound norm gradient regularization term objective eventually reduces number iterates attain ǫ-approximate solution subsections proof theorem theorem suggests convergence rate algorithm deteriorated compared original sdca cases l-lipschitz smooth losses despite insertion sign correction step. rd×m instead vector loss function example m-dimensional vector. here maximal score among here without loss generality class labels given several loss functions used multiclass classiﬁcation follows. soft-max loss rithm designed inserting sign correction step original pegasos iterate algorithm developed simply adding sign correction step sdca iterate. resultant algorithm described algorithm therein deﬁned λnγ/ clip max). subsection derivation found following theorem states required number iterates guaranteeing expected primal objective threshold sign constraints. horizontal concatenation columns rm×n selected minibatch following assumptions convex function; nrloss; rm×n kxik extending algorithm algorithm minimization subject sign constraints developed described algorithm rm×n. iterate columns chosen random instead choosing dual variable update matrix a+∆αe⊤i used iterate number superscript determine value following auxiliary funcition introduced coli prediction ﬁrst task predict coli counts river water. coli count used indicator fecal contamination water environment many parts world experiment data points coli counts probable number assigned positive class others negative. hydrological water quality monitoring data used predicting coli counts positive negative. ensuring microbial safety water usage meaningful predict coli counts real-time basis. concentration coli water measured culturedependent methods used monitor fecal contamination water environment proved eﬀective prevent waterborne infectious diseases varied water usage styles. hand real-time monitoring coli counts achieved. take least hours obtain coli counts culture-dependent methods also least several hours needed measure concentration coli culture-independent methods polymerase chain reaction. since possible measure hydrological water quality data realtime sensors real-time prediction coli counts realized hydrological water quality data available coli count prediction. many training examples required obtain better generalization performance. serious issue however measuring concentration coli time-consuming cost reagents expensive. demonstrate issue relaxed exploiting domain knowledge hoarded ﬁeld water engineering. suppose ﬁrst proteins training positive class rest negative ﬁrst similarities sequence similarities positive examples xn++ similarities negative examples. n-dimensional vectors weight coeﬃcients then prediction score target protein expressed input protein sequence predicted particular cellular function score threshold. preferable ﬁrst weight coeﬃcients non-negative rest weight coefﬁcients wn++ non-positive. svm-pairwise approach ensure requirements. meanwhile approach capable explicitly impose constraints approach applied predict protein functions saccharomyces cerevisiae annotations protein functions provided mips comprehensive yeast genome database dataset contains proteins. smith-waterman similarities available https//noble.gs.washington.edu/proj/ sdp-svm/ used sequence similarities among proteins. number categories proteins multiple cellular functions. indeed proteins dataset function. reason pose independent binary classiﬁcation tasks instead single multi-class classiﬁcation task. proteins randomly splited half datasets. used training testing. classiﬁcation tasks repeated procedure times obtained scores. table reports scores averaged trials standard deviations binary classiﬁcation tasks. sign constraints signiﬁcantly surpassed classical training tasks. surprisingly observed score actually measured concentrations coli times december april obtained data points including positives negatives. chose examples data points random training examples used testing. prediction performance evaluated precision recall break-even point score. compared classical sign-constrained examine eﬀects sign constraints. repeated procedure times obtained prbep scores. sc-svm achieved signiﬁcant improvement compared classical svm. sc-svm achieved prbep score average trials whereas classical respectively. difference classical trial plotted histograms figure positive improvements scores obtained trials trials whereas scores decreased trials. prbep positive improvements obtained trials whereas deteriorations observed trials. protein function prediction ﬁeld molecular biology understanding functions proteins positioned step elucidation cellular mechanisms. sequence similarities major mean predict function unannotated protein. beginning century prediction accuracy improved combining sequence similarities discriminative learning. method named svm-pairwise uses feature vector contains pairwise similarities annotated protein sequences. several literature also provided empirical evidences fact svm-pairwise approach powerful framework. basically proteins training dataset feature vector entries", "year": 2017}