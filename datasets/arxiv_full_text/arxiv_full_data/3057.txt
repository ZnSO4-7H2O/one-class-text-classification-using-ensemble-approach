{"title": "FearNet: Brain-Inspired Model for Incremental Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.", "text": "incremental class learning involves sequentially learning classes bursts examples class. violates assumptions underlie methods training standard deep neural networks cause suffer catastrophic forgetting. arguably best method incremental class learning icarl requires storing training examples class making challenging scale. here propose fearnet incremental class learning. fearnet generative model store previous examples making memory efﬁcient. fearnet uses brain-inspired dual-memory system memories consolidated network recent memories inspired mammalian hippocampal complex network long-term storage inspired medial prefrontal cortex. memory consolidation inspired mechanisms occur sleep. fearnet also uses module inspired basolateral amygdala determining memory system recall. fearnet achieves state-of-the-art performance incremental class learning image audio classiﬁcation benchmarks. incremental classiﬁcation agent must sequentially learn classify training examples without necessarily ability re-study previously seen examples. deep neural networks revolutionized machine perception off-the-shelf dnns canincrementally learn classes catastrophic forgetting. catastrophic forgetting phenomenon completely fails learn data without forgetting much previously learned knowledge methods developed mitigate catastrophic forgetting shown kemker methods sufﬁcient perform poorly larger datasets. paper propose fearnet brain-inspired system incrementally learning categories signiﬁcantly outperforms previous methods. standard dealing catastrophic forgetting dnns avoid altogether mixing training examples ones completely re-training model ofﬂine. large datasets require weeks time scalable solution. ideal incremental learning system would able assimilate information without need store entire training dataset. major application incremental learning includes real-time operation on-board embedded platforms limited computing power storage memory e.g. smart toys smartphone applications robots. example robot need learn recognize objects within local environment interest owner. using cloud computing overcome resource limitations pose privacy risks scalable large number embedded devices. better solution on-device incremental learning requires model less storage computational power. paper propose incremental learning framework called fearnet fearnet three brain-inspired sub-systems recent memory system quick recall memory system long-term storage sub-system determines memory system particular example. fearnet mitigates catastrophic forgetting consolidating recent memories long-term storage using pseudorehearsal pseudorehearsal allows network revisit previous memories incremental training without need store previous training examples memory efﬁcient. problem formulation here incremental class learning consists study-sessions. time learner receives batch data contains labeled training samples i.e. {}nt input feature vector classiﬁed corresponding label. number training samples vary sessions data inside study-session assumed independent identically distributed study session learner access current batch memory store information prior study sessions. refer ﬁrst session model’s base-knowledge contains exemplars classes. batches learned subsequent sessions contain class i.e. identical within sessions. fearnet’s architecture includes three neural networks inspired hippocampal complex recent memories inspired medial prefrontal cortex long-term storage inspired basolateral amygdala determines whether mpfc recall. motivated memory replay sleep fearnet employs generative autoencoder pseudorehearsal mitigates catastrophic forgetting generating previously learned examples replayed alongside novel information consolidation. process involve storing previous training data. catastrophic forgetting dnns occurs plasticity-stability dilemma network plastic older memories quickly overwritten; however network stable unable learn data. problem recognized almost years french methods developed extensively discussed french argued mitigating catastrophic forgetting would require separate memory centers long-term storage older memories another quickly process information comes also theorized type dual-memory system would capable consolidating memories fast learning memory center longterm storage. catastrophic forgetting often occurs system trained non-iid data. strategy reducing phenomenon examples examples simulates conditions. example system learns classes study session needs learn classes later study session solution could examples ﬁrst study session later study session. method known rehearsal earliest methods reducing catastrophic forgetting rehearsal essentially uses external memory strengthen model’s representations examples learned previously overwritten learning data classes. rehearsal reduces forgetting performance still worse ofﬂine models. moreover rehearsal requires storing training data. robins argued storing training examples inefﬁcient little interest introduced pseudorehearsal. rather replaying past training data pseudorehearsal algorithm generates examples given class. robins done creating random input vectors network assign label mixing training data. idea revived draelos generative autoencoder used create pseudo-examples unsupervised incremental learning. method inspired fearnet’s approach memory consolidation. pseudorehearsal related memory replay occurs mammalian brains involves reactivation recently encoded memories integrated long-term storage mpfc recently renewed interest solving catastrophic forgetting supervised learning. many methods designed mitigate catastrophic forgetting study session contains permuted version entire training dataset unlike incremental class learning labels contained study session. pathnet uses evolutionary algorithm optimal path large freezes weights along path assumes classes seen study session capable incremental class learning. elastic weight consolidation employs regularization scheme redirects plasticity weights least important previously learned study sessions learns study session uses training data build fisher matrix determines importance feature classiﬁcation task learned. shown work poorly incremental class learning kemker fixed expansion layer model mitigates catastrophic forgetting using sparse updates uses hidden layers second hidden layer connectivity constraints. layer much larger ﬁrst hidden layer sparsely populated excitatory inhibitory weights updated training. limits learning dense shared representations reduces risk learning interfering memories. requires large number units work well gepperth karaoguz introduced approach incremental learning call geppnet. geppnet uses self-organizing reorganize input onto two-dimensional lattice. serves long-term memory simple linear layer classiﬁcation. initialized updated input sufﬁciently novel. prevents model forgetting older data quickly. geppnet also uses rehearsal using previous training data. variant geppnet geppnet+stm uses ﬁxed-size memory buffer store novel examples. buffer full replaces oldest example. pre-deﬁned intervals buffer used train model. geppnet+stm better retaining base-knowledge since trains consolidation phase stm-free version learns data better updates model every novel labeled input. icarl incremental class learning framework. rather directly using classiﬁcation icarl uses supervised representation learning. study session icarl updates using study session’s data stored examples earlier sessions kind rehearsal. study session examples retained carefully chosen using herding. learning entire dataset icarl retained exemplars class icarl used compute embedding stored example mean embedding class seen computed. classify instance used compute embedding class nearest mean embedding assigned. icarl’s performance heavily inﬂuenced number examples stores shown fig. fearnet heavily inspired dual-memory model mammalian memory considerable experimental support neuroscience theory proposes mpfc operate complementary memory systems responsible recalling recent memories mpfc responsible recalling remote memories. geppnet recent based theory also independently explored french rousset section review evidence dual-memory model. major reasons thought responsible recent memories bilaterally destroyed anterograde amnesia occurs memories semantic information preserved. mechanism facilitate creating memories adult neurogenesis. occurs hc’s dentate gyrus neurons higher initial plasticity reduces time progresses contrast mpfc responsible recall remote memories taupin gage gais showed mpfc plays strong role memory consolidation sleep. mcclelland euston theorized that sleep reactivates recent memories prevent forgetting causes recent memories replay mpfc well dreams possibly caused process. memories transferred mpfc evidence suggests corresponding memory erased recently kitamura performed contextual fear conditioning experiments mice trace formation consolidation recent memories long-term storage. experiments involve shocking mice subjecting various visual stimuli found responsible regulating brain’s fear response would shift retrieved corresponding memory memory consolidated time. fearnet follows memory consolidation theory proposed kitamura fearnet complementary memory centers short-term memory system immediately learns information recent recall storage remote memories fearnet also separate network determines memory center contains associated memory required prediction. sleep phases fearnet uses generative model consolidate data mpfc pseudorehearsal. pseudocode fearnet provided supplemental material. focus work representation learning pre-trained resnet embeddings obtain features fearnet. fearnet’s model variant probabilistic neural network computes class conditional probabilities using stored training examples. formally estimates probability input feature vector belongs class figure mpfc sub-systems fearnet. mpfc responsible long-term storage remote memories. used prediction time determine memory recalled shortlong-term memory. lclass supervised classiﬁcation loss lrecon unsupervised reconstruction loss illustrated fig. lclass standard softmax loss. lrecon weighted mean squared error reconstruction losses layer given number mpfc layers number hidden units layer hencoder hdecoder outputs encoder/decoder layer respectively reconstruction weight layer. mpfc similar ladder network combines classiﬁcation reconstruction improve regularization especially lowshot learning. hyperparameters found empirically largest decreasing deeper layers prioritizes reconstruction task makes generated pseudo-examples realistic. training completed study session data pushed encoder extract dense feature representation original data compute mean feature vector covariance matrix class stored used generate pseudo-examples consolidation study fearnet’s performance function much data stored sec. fearnet’s sleep phase original inputs stored transferred mpfc using pseudo-examples created autoencoder. process known intrinsic replay used draelos unsupervised learning. using class statistics encoder pseudo-examples class generated sampling gaussian mean covariance matrix obtain ˆxrand. then ˆxrand passed decoder generate pseudo-example. create balanced training class mpfc learned generate pseudo-examples average number examples class stored pseudo-examples mixed data mixture used ﬁne-tune mpfc using backpropagation. consolidation units deleted. prediction fearnet uses network determine whether classify input using mpfc. challenging trained class probability mass class whereas mpfc likely less conﬁdent. output given value indicating mpfc used. trained study session using data pseudoexamples generated mpfc using procedure described sec. instead using probability class according weighted conﬁdence associated memory actually stored number layers/units mpfc encoder uses logistic output unit. discuss alternative models supplemental material. evaluating incremental learning performance. evaluate well incrementally trained models perform compared ofﬂine model three metrics proposed kemker study session model learned class compute model’s test accuracy class accuracy base-knowledge accuracy test data seen point study sessions complete model’s ability retain base-knowledge given ωbase line accuracy multi-layer perceptron trained ofﬂine model’s ability immediately recall information measured ωnew αnewt. finally measure well model available test data ωall ωall metric shows well memories integrated model time. metrics higher values indicate superior performance. ωbase ωall relative ofﬂine model value indicates model similar performance ofﬂine baseline. allows results across datasets better compared. note ωbase ωall incremental learning algorithm accurate ofﬂine model occur better regularization strategies employed different models. datasets. evaluate models three benchmark datasets cifar- cub- audioset. cifar- popular image classiﬁcation dataset containing mutually-exclusive object categories used rebufﬁ evaluate icarl. images pixels. cub- ﬁne-grained image classiﬁcation dataset containing high resolution images different bird species version dataset. audioset audio classiﬁcation dataset variant audioset used kemker contains class subset none classes supersub-classes another. also since audioset data samples class chosen samples classes chosen subset. cifar- cub- extract resnet- image embeddings input models resnet- pre-trained imagenet output mean pooling layer normalize features unit length. audioset audio embeddings produced pre-training model youtube-m dataset pre-extracted audioset feature embeddings represent second sound clips comparison models. compare fearnet geppnet geppnet+stm icarl onenearest neighbor geppnet geppnet+stm chosen previously reported efﬁcacy incremental class learning kemker icarl explicitly designed incremental class learning represents state-of-the-art problem. compare similarity model. forget previously observed examples tends worse generalization error parametric methods requires storing training data. experiments models take feature embedding input given dataset. required modifying icarl turning fully connected network. performed hyperparameter search model/dataset combination tune number units layers training parameters. fearnet implemented tensorﬂow. mpfc fully connected layer uses exponential linear unit activation function output encoder also connects softmax output layer. xavier initialization used initialize weight layers biases initialized one. bla’s architecture identical mpfc’s encoder except logistic output unit instead softmax layer. mpfc trained using nadam. train mpfc base-knowledge epochs consolidate mpfc epochs train epochs. mpfc’s decoder vital preserving memories learning rate times lower encoder. performed hyperparameter search dataset model varying model shape depth often sleep across datasets mpfc performed best hidden layers number units layer varied across datasets. speciﬁc values used dataset given supplemental material. preliminary experiments found beneﬁt adding weight decay mpfc likely reconstruction task helps regularize model. unless otherwise noted class seen unique study-session ﬁrst baseknowledge study session contains half classes dataset. perform additional experiments study changing number base-knowledge classes affects performance sec. unless otherwise noted fearnet sleeps every study sessions across datasets. table shows incremental class learning summary results methods. fearnet achieves best ωbase ωall three datasets. fig. shows fearnet closely resembles ofﬂine baseline methods. ωnew measures test accuracy recently trained class. fearnet measures performance bla. ωnew account well class consolidated mpfc happens later sleep phase; however ωall account this. achieves high ωnew score able achieve nearly perfect test accuracy every class learns results forgetting quickly fearnet. similar model; fails generalize well fearnet memory inefﬁcient slow make predictions. ﬁnal mean-class test accuracy ofﬂine used normalize metrics cifar- cub- audioset. novelty detection bla. evaluated performance comparing oracle version fearnet i.e. version knew relevant memory stored either mpfc table shows fearnet’s good predicting network use; however decrease ωnew suggests sometimes using mpfc using model sleep? study frequency memory consolidation affects fearnet’s performance trained fearnet cub- varied sleep frequency study sessions. fearnet increases number classes learns sleeping better able retain base-knowledge reduces ability recall information. humans sleep deprivation known impair learning forgetting occurs sleep time fearnet sleeps mpfc weights perturbed cause gradually forget older memories. sleeping less causes hc’s recall performance deteriorate. multi-modal incremental learning. shown sec. fearnet incrementally learn retain information single dataset perform inputs differ greatly previously learned ones? scenario ﬁrst shown cause catastrophic forgetting mlps. study this trained fearnet incrementally learn cifar- audioset training -way classiﬁcation problem. this audioset’s features zero-padded make length cifar-s. table shows performance fearnet three separate training paradigms fearnet learns cifar- baseknowledge incrementally learns audioset; fearnet learns audioset baseknowledge incrementally learns cifar-; base-knowledge contains split datasets fearnet incrementally learning remaining classes. results suggest fearnet capable incrementally learning multi-modal information model good starting point however model starts lower base-knowledge base-knowledge effect performance. section examine size baseknowledge affects fearnet’s performance cub-. this varied size base-knowledge classes remaining classes learned incrementally. detailed plots provided supplemental material. base-knowledge size increases noticeable increase overall model performance mpfc better learned representation larger quantity data many incremental learning steps remaining dataset base-knowledge performance less perturbed. fearnet’s mpfc trained discriminate examples also generate examples. main mpfc’s generative abilities enable psuedorehearsal ability also help make model robust catastrophic forgetting. gillies observed unsupervised networks robust catastrophic forgetting target outputs forgotten. since pseudoexample generator learned unsupervised reconstruction task could explain fearnet slow forget information. table shows memory requirements model sec. learning cifar- hypothetical extrapolation learning classes. chart accounts ﬁxed model capacity storage data class statistics. fearnet’s memory footprint comparatively small stores class statistics rather training data makes better suited deployment. open question deal storage updating class statistics classes seen study sessions. possibility running update class means covariances better favor data recent study session learning autoencoder. fearnet assumed output mpfc encoder normally distributed class case. would interesting consider modeling classes complex model e.g. gaussian mixture model. robins showed pseudorehearsal worked reasonably well randomly generated vectors associated weights given class. replaying vectors strengthened corresponding weights could happening pseudo-examples generated fearnet’s decoder. largest impact model size stored covariance matrix class. tested variant fearnet used diagonal instead full covariance matrix. table shows performance degrades fearnet still works. fearnet adapted paradigms unsupervised learning regression. unsupervised learning fearnet’s mpfc already form implicitly. regression would require changing mpfc’s loss function require grouping input feature vectors similar collections. fearnet could also adapted perform supervised data permutation experiment performed goodfellow kirkpatrick would likely require storing statistics previous permutations classes. fearnet would sleep learning different permutations; however number classes high recent recall suffer. paper proposed brain-inspired framework capable incrementally learning data different modalities object classes. fearnet outperforms existing methods incremental class learning large image audio classiﬁcation benchmarks demonstrating fearnet capable recalling consolidating recently learned information also retaining information. addition showed fearnet memory efﬁcient making ideal platforms size weight power requirements limited. future work include integrating directly model replacing semi-parametric model; learning feature embedding inputs; replacing pseduorehearsal mechanism generative model require storage class statistics would memory efﬁcient. bernard stphane rousset. avoiding catastrophic forgetting coupling reverberating neural networks. comptes rendus l’acadmie sciences series sciences issn bruno bontempi catherine laurent-demir claude destrade robert jaffard. time-dependent reorganization brain circuitry underlying long-term memory storage. nature robert coop aaron mishtal itamar arel. ensemble learning ﬁxed expansion layer networks mitigating catastrophic forgetting. ieee trans. neural networks learning systems timothy draelos nadine miner christopher lamb jonathan craig vineyard kristofor carlson william severa conrad james james aimone. neurogenesis deep learning extending deep networks accommodate classes. international joint conference neural networks ieee peter eriksson ekaterina perﬁlieva thomas bj¨ork-eriksson ann-marie alborn claes nordborg daniel peterson fred gage. neurogenesis adult human hippocampus. nature medicine chrisantha fernando dylan banarse charles blundell yori zwols david andrei rusu alexander pritzel daan wierstra. pathnet evolution channels gradient descent super neural networks. arxiv. paul frankland bruno bontempi lynn talton leszek kaczmarek alcino silva. involvement anterior cingulate cortex remote contextual fear memory. science steffen gais genevi`eve albouy m´elanie boly thien thanh dang-vu annabelle darsaud martin desseilles g´eraldine rauchs manuel schabus virginie sterpenich gilles vandewalle sleep transforms cerebral trace declarative memories. proceedings national academy sciences jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter. audio ontology human-labeled dataset audio events. icassp orleans xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics goodfellow mehdi mirza xiao aaron courville yoshua bengio. empirical investigation catastrophic forgetting gradient-based neural networks. arxiv. hetherington mark seidenberg. catastrophic interference connectionist networks. proceedings annual conference cognitive science society volume erlbaum hillsdale james kirkpatrick razvan pascanu neil rabinowitz joel veness overcoming catastrophic forgetting neural networks. proc. national academy sciences takashi kitamura sachie ogawa dheeraj teruhiro okuyama mark morrissey lillian smith roger redondo susumu tonegawa. engrams circuits crucial systems consolidation memory. science alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems james mcclelland bruce mcnaughton randall o’reilly. complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory. psychological review bernhard sch¨olkopf john platt john shawe-taylor alex smola robert williamson. estimating support high-dimensional distribution. neural comput. july takashima karl magnus petersson rutters tendolkar jensen zwarts mcnaughton fernandez. declarative memory consolidation humans prospective functional magnetic resonance imaging study. proceedings national academy sciences united states america table shows training parameters fearnet model dataset. also experimented various dropout rates weight decay various activation functions; however weight decay work well fearnet’s mpfc. table shows training parameters icarl framework used paper. adapted code author’s github page experiments. resnet- convolutional neural network replaced fully-connected neural network. experimented various regularization strategies increase initial base-knowledge accuracy weight decay working table shows training parameters geppnet geppnet+stm. parameters listed default parameters deﬁned gepperth karaoguz values given range values hyperparameter search spaces. table provides additional experimental results exemplars class icarl framework. rebufﬁ used original paper; however increased number storing training data helped icarl. although higher increase icarl performance still outperform fearnet. note cub- training samples class icarl storing entire training epc. main results default value model classiﬁer determines whether prediction made using mpfc alternative approach would outlier detection algorithm determines whether data processed sub-network outlier sub-network therefore processed sub-network. explore alternative formulation experimented three outlier detection algorithms one-class support vector machine determining data gaussian distribution using minimum covariance determinant estimation isolation forest three methods rejection criterion test sample exists whereas binary reports probability likely test sample resides table compares individual methods. isolation forest elliptic envelope seem prefer data one-class prefers data mpfc binary worked best choosing correct sub-network use. fig. shows plots multi-modal experiments sec. three base-knowledge experiments cifar- base-knowledge audioset trained incrementally audioset base-knowledge audioset trained incrementally base-knowledge datasets remaining classes trained incrementally. three base-knowledge experiments show mean-class accuracy base-knowledge entire test set. fearnet works well adequately learns base-knowledge however fearnet learns poorly incremental learning deteriorates. fig. shows effect base-knowledge’s size fearnet’s performance. expected ωbase increases many sleep phases overwrite existing base-knowledge. ωnew figure detailed plots multi-modal experiment. base-knowledge cifar- middle base-knowledge audioset bottom base-knowledge datasets. left column represents mean-class accuracy base-knowledge test right column computes mean-class accuracy entire test set. remains relatively even size base-knowledge effect model’s ability immediately recall information; however slight decrease corresponds model erroneously favoring mpfc cases. importantly ωall sees increase performance because; like ωbase many sleep phases perturb older memories mpfc.", "year": 2017}