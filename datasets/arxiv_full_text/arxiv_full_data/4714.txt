{"title": "Efficient Clustering with Limited Distance Information", "tag": ["cs.LG", "cs.AI"], "abstract": "Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s 2 S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. We use our algorithm to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire dataset. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.", "text": "given point unknown metric study problem eﬃciently partitioning clusters querying distances points. model assume access versus queries given point return distances points. show given natural assumption structure instance eﬃciently accurate clustering using distance queries. algorithm cluster proteins sequence similarity. setting nicely model because fast sequence database search program query sequence entire dataset. conduct empirical study shows even though query small fraction distances points produce clusterings close desired clustering given manual classiﬁcation. clustering pairwise distance information important problem analysis exploration data. many variants formulations extensively studied many diﬀerent communities many diﬀerent clustering algorithms proposed. many application domains ranging computer vision biology recently faced explosion data presenting several challenges traditional clustering techniques. particular computing distances pairs points required traditional clustering algorithms become infeasible many application domains. consequence become increasingly important develop eﬀective clustering algorithms operate limited distance information. work initiate study clustering limited distance information; particular consider clustering small number versus queries. imagine least diﬀerent ways query distances points. distances pairs points distances point points. clearly versus query implemented pairwise queries draw distinction former often signiﬁcantly faster practice query implemented database search. main motivating example considering versus distance queries sequence similarity search biology. program blast optimized search single sequence entire database sequences. hand performing pairwise sequence alignments takes several orders magnitude time even pairwise alignment fast. disparity runtime hashing blast uses identify regions similarity input sequence sequences database. program maintains hash table words database linking word locations. query performed blast considers word input sequence runs local sequence alignment locations database. therefore program performs limited number local sequence alignments rather aligning input sequence sequence database. course downside never consider alignments sequences share word. however case alignment relevant anyway assign distance inﬁnity sequences. even though search performed blast heuristic shown protein sequence similarity identiﬁed blast meaningful motivated scenarios paper consider problem clustering dataset unknown distance function given capability versus distance queries. design eﬃcient algorithm clustering accurately small number queries. formally analyze correctness algorithm assume distance function metric clustering problem satisﬁes natural approximation stability property regarding utility k-median objective function clustering points. particular analysis assumes -property balcan objective function -property assumes clustering c-approximation error deﬁne mean error assume exists unknown relevant target clustering error proposed clustering fraction misclassiﬁed points optimal matching clusters ﬁrst main contribution designing algorithm given -property k-median objective ﬁnds clustering close target using versus queries. particular assumption balcan obtain performance guarantees using small number versus queries. addition handling diﬃcult scenario also provide much faster algorithm. algorithm implemented time proposed runs time also algorithm cluster proteins sequence similarity compare results gold standard manual classiﬁcations given pfam scop databases. classiﬁcation databases used ubiquitously biology observe evolutionary relationships proteins close relatives particular proteins. sources obtain clusterings usually closely match given classiﬁcation performance algorithm comparable best known algorithms using full distance matrix. classiﬁcation databases limited coverage completely automated method useful clustering proteins classiﬁed. moreover method cluster large datasets eﬃcient require full distance matrix input infeasible obtain large dataset. related work property related \u0001-separability introduced ostrovsky clustering instance \u0001-separated cost optimal k-clustering times cost optimal clustering using clusters. \u0001-separability properties related case clusters large ostrovsky condition implies balcan condition ostrovsky also present sampling method choosing initial centers followed single lloyd-type descent step gives constant factor approximation k-means objective instance \u0001-separated. however sampling method needs information full distance matrix probability picking points cluster centers proportional squared distance. similar strategy used obtain o-approximation k-means objective arbitrary instances. strategy implemented versus distance queries. however o-approximation good enough purposes. approximate clustering using sampling studied extensively recent years methods proposed papers yield constant factor approximations k-median objective using versus distance queries. however constant factor approximations least proposed sampling methods necessarily yield clusterings close target clustering -property holds small constant interesting case setting. given metric space point unknown distance function satisfying triangle inequality points would like k-clustering partitions points sets using versus distance queries. section present algorithm accurately clusters points assuming clustering instance satisﬁes -property clusters target clustering small. algorithm presented much faster given balcan require pairwise distances input. instead require versus distance queries achieve performance guarantee clustering algorithm described algorithm start using landmark-selection procedure select landmarks. procedure repeatedly chooses uniformly random furthest points ones selected appropriate algorithm uses distances selected landmarks points requires versus distance queries. formalize -property need deﬁne notion distance k-clusterings deﬁne distance fraction points disagree optimal matching clusters clusters dist minσ∈sk bijections clusterings \u0001-close dist assume exists unknown relevant target clustering given proposed clustering deﬁne error respect dist. goal clustering error. -property deﬁned follows. deﬁnition instance satisﬁes -property k-median objective function respect target clustering clustering approximates optφ within factor \u0001-close i.e. optφ dist analysis next section denote center point refer value using k-median objective i.e. deﬁne weight point contribution k-median objective mini similarly denote distance second-closest cluster center among addition avi= erage weight points cardinality possible implement iteration landmarkselection time point store minimum distance landmarks chosen updated constant time landmark. select landmark iteration choose random number linear time selection algorithm select furthest point next landmark. expand-landmarks expands ball around landmark chosen landmark-selection. variable denote radius balls algorithm starts increments balls satisfy property described below. relevant values adding point results |l|n values total. algorithm maintains graph vertices correspond balls least smin points them vertices connected edge corresponding balls overlap point addition maintain points balls clustered list connected components refer components {comp compm}. iteration expand balls single point update components clustered. exactly components |clustered| terminate report points balls part component distinct clusters. condition never satisﬁed report no-cluster. sketch algorithm given below. refer next landmarkpoint pair considered corresponding expanding include algorithm expand-landmarks expand-ball) null using min-heap store landmark-point pairs disjoint-set data structure keep track connected components iteration loop implemented amortized time number iterations bounded |l|n gives worst-case running time last step algorithm takes clustering returned expand-landmarks improves compute contains exactly landmark cluster assign point cluster corresponding closest landmark runtime landmark-selection expandlandmarks implemented last part procedure takes time thus total runtime algorithm moreover algorithm uses distances between selected landmarks points uses versus distance queries. prove theorem introduce notation analysis similar argue structure clustering instance. dist. assumption kmedian clustering satisﬁes -property since cluster target clustering least points optimal k-median clustering diﬀers target clustering points cluster must least points. deﬁne critical distance dcrit call point good dcrit dcrit else called bad. words good points points close cluster center cluster center. addition break good points good sets good points optimal cluster core optimal cluster note distance points satisﬁes dcrit. addition distance points diﬀerent good sets greater dcrit. this consider pair points xj=i. distance cluster center least dcrit. triangle inequality dcrit dcrit dcrit. proved k-median instance satisﬁes -property respect cluster size least intuition many points agree close enough second-closest move center among clusters corresponding centers producing clustering whose objective value close violating property. second part follows fact remainder section prove given structure clustering instance landmarkclustering ﬁnds accurate clustering. ﬁrst show almost surely landmarks returned landmark-selection property cluster cores landmark near argue expand-landmarks ﬁnds partition clusters points core correctly. conclude proof theorem argues clustering returned last step procedure improved clustering close uses landmark-clustering landmark-selection choose landmark points. following lemma proves iter almost surely selected landmarks property landmark closer dcrit point good set. lemma given landmark-selection probability exp) landmark closer dcrit point good set. proof. points iteration uniformly random choose points probability good point added least iteration. using chernoﬀ bound show probability fewer good points added iterations less e−t/ therefore iterations good points added probability e−ω. note good points must distinct cannot choose point twice ﬁrst iterations. possibilities regarding ﬁrst good points added either selected distinct good sets least selected good set. former true statement trivially holds. latter true consider ﬁrst time second point chosen good call points assume chosen distance must less dcrit good set. therefore chosen minl∈l dcrit. moreover chosen {sn−b+ minl∈l minl∈l therefore chosen least points satisfy minl∈l minl∈l dcrit. since good satisﬁes |xi| follows must landmark closer dcrit point good set. proof. indicator random variable deﬁned follows point chosen iteration expectation words number good points chosen iterations algorithm expected value. algorithm uses expand-landmarks procedure k-clustering following lemma states accurate clustering additional property relevant last part algorithm. lemma given landmarks chosen landmark-selection condition lemma satisﬁed expand-landmarks returns k-clustering cluster contains points distinct good bijection mapping good cluster containing points distance satisﬁes dcrit. proof. lemma argues since good sets well-separated dcrit ball radius overlap balls overlap diﬀerent cannot share points. moreover since consider balls points them number points ball must overlap good set. lemma argues since landmark near good value dcrit contained ball around landmark radius facts argue correctness algorithm. first observe exactly components good contained within distinct component. ball overlaps lemma since dcrit know ball overlaps exactly lemma also know balls overlap diﬀerent cannot share points thus connected therefore balls overlap diﬀerent diﬀerent components moreover lemma contained ball radius good designate ball contains points since size good satisﬁes |xi| ball overlaps connected thus component therefore exactly components good contains points since least good points means number points ball least hence condition line expand-landmarks satisﬁed algorithm terminate return k-clustering cluster contains points distinct good suppose start consider ﬁrst value condition line satisﬁed. point exactly components number points components must case dcrit know condition satisﬁed considering relevant values ascending order. before ball must overlap good using lemma argue since dcrit ball overlap balls overlap diﬀerent cannot share points. follows component contains points single moreover since size good satisﬁes |xi| points left component must contain points distinct thus return k-clustering cluster contains points distinct good prove second part statement bijection matching good cluster containing points clearly made points balls radius dcrit overlap consider ball around landmark denote point overlap. triangle inequality distance s∗)+d dcrit+r dcrit. satisﬁes dcrit. radius dcrit around landmark words overlaps good must least points diﬀerent good sets triangle inequality follows dcrit. however know dcrit giving contradiction. prove second part consider balls radius dcrit around landmarks words assume overlap diﬀerent good sets purpose contradiction let’s assume share least point refer point. triangle inequality follows distance point satisﬁes dcrit. since overlaps overlaps follows pair points dcrit contradiction. therefore overlap diﬀerent good sets proof. good choose point landmark satisfy dcrit. distance point satisﬁes dcrit dcrit dcrit. consider maxlimaxx∈xid. clearly contained ball radius dcrit. lemma suppose distance landmark given point dcrit case proof theorem cluster clustering output expand-landmarks contains points distinct good clustering exclude points good. nonetheless means disagree points good points. number points disagree therefore thus least o-close least o-close moreover additional property allows clustering \u0001-close denote bijection mapping good cluster containing points landmark closer dcrit observation points satisfy properties good points points dcrit. call points detectable points. clarify detectable points points much closer cluster center cluster center good points subset detectable points also close cluster center. detectable points using choose point landmark insert cluster argminid. lemma argues detectable point landmark follows agree detectable points. since fewer points agree detectable follows dist dist landmark clustering algorithm cluster proteins using sequence similarity. mentioned introduction versus distance queries particularly relevant setting sequence database search programs blast blast aligns queried sequence sequences database produces score alignment measure quality however blast consider alignments sequences database case assign distances inﬁnity corresponding sequences. observe deﬁne distances manner almost form metric practice draw random triplets sequences check distances triangle inequality almost always satisﬁed. moreover blast successful detecting sequence homology large sequence databases therefore plausible clustering using distances satisﬁes -property relevant clustering perform experiments datasets obtained classiﬁcation databases pfam scop sources classify proteins evolutionary relatedness therefore classiﬁcations ground truth evaluate clusterings produced algorithm methods. pfam classiﬁes proteins using hidden markov models represent multiple sequence alignments. levels pfam classiﬁcation hierarchy family clan. clustering experiments compare classiﬁcation family level because relationships clan level less likely discerned sequence alignment. experiment randomly select several large families pfam-a retrieve sequences proteins families landmark-clustering algorithm cluster dataset. scop groups proteins basis structures classiﬁes proteins whose structure known. thus datasets scop much smaller size. scop classiﬁcation also hierarchical proteins grouped class fold superfamily family. consider classiﬁcation superfamily level seems appropriate given using sequence information. pfam data experiment create dataset randomly choosing several superfamilies retrieve sequences corresponding proteins landmarkclustering algorithm cluster dataset. cluster particular dataset compare clustering manual classiﬁcation using distance measure theoretical part work. fraction misclassiﬁed points optimal matching clusters clusters solve minimum weight bipartite matching problem σ|/n. cost matching landmark-clustering using number clusters ground truth clustering. pfam dataset landmarks/queries scop dataset landmarks/queries. addition algorithm uses three parameters whose value proof based assuming clustering instance satisﬁes -property. practice must choose value parameter. experiments function number points dataset average size ground truth clusters smin .µ/.µ pfam/scop datasets since selection landmarks randomized dataset perform several clusterings compare ground truth report median quality. landmark-clustering sensitive smin parameter report clustering smin small large. recommend trying several reasonable values smin increasing decreasing order clustering none clusters large. clustering clusters large likely means several ground truth clusters merged. happen smin small causing balls outliers connect diﬀerent cluster cores smin large causing balls diﬀerent cluster cores overlap. algorithm less sensitive parameter. however large ground truth clusters merged recommend using smaller value points still clustered last step. again values algorithm output clustering output clustering clusters large. algorithm least sensitive parameter. using landmarks make poor choice figure shows results experiments pfam datasets. datasets clustering almost identical ground truth. datasets large benchmark comparison consider algorithms comparable amount distance information natural choice following algorithm randomly choose landmarks embed point d-dimensional space using distances k-means clustering space. notice procedure uses exactly versus distance queries equal number queries used algorithm. expect procedure work well indeed look figure ﬁnds reasonable clusterings. still clusterings reported procedure match pfam classiﬁcation exactly showing ﬁnding exact pfam partition trivial. figure shows results experiments scop datasets. results good likely scop classiﬁcation superfamily level based biochemical structural evidence addition sequence evidence. contrast pfam classiﬁcation based entirely sequence information. still scop datasets much smaller compare algorithm methods require distances points. particular paccanaro showed spectral clustering using sequence data works well applied proteins scop thus exact method described benchmark comparison scop datasets. moreover clustering randomly generated datasets scop also consider main examples labeled ﬁgure. figure performance landmark-clustering comparable spectral method good considering algorithm used paccanaro signiﬁcantly outperforms clustering algorithms data moreover spectral clustering algorithm requires full distance matrix input takes much longer run. konstantin voevodski supported igert fellowship grant dge- awarded aces training program center computational science. maria florina balcan supported part grant ccf- grant n--- afosr grant fa--. heiko r¨oglin supported veni grant netherlands organisation scientiﬁc research. shang-hua teng supported part grant ccr-.", "year": 2014}