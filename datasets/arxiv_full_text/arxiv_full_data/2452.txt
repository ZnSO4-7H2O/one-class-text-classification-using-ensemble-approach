{"title": "Gaussian Approximation of Collective Graphical Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster.", "text": "li-ping daniel sheldon thomas dietterich school eecs oregon state university corvallis university massachusetts amherst mount holyoke college south hadley collective graphical model models population independent identically distributed individuals collective statistics observed. exact inference cgms intractable previous work explored markov chain monte carlo approximations learning inference. paper studies gaussian approximations cgm. population grows large show distribution converges multivariate gaussian distribution maintains conditional independence properties original cgm. observations exact marginals marginals corrupted gaussian noise inference gcgm approximation computed efﬁciently closed form. observations follow different noise model expectation propagation provides efﬁcient accurate approximate inference. accuracy speed gcgm inference compared mcmc methods simulated bird migration problem. gcgm matches exceeds accuracy method signiﬁcantly faster. consider setting wish model behavior population independent identically distributed individuals observe collective count data. example might wish model relationship education housing income census data. privacy reasons census bureau releases count data number people having given level education number living particular region. another example concerns modeling behavior animals counts individuals observed various locations times. arises modeling migration birds. constructed ﬁrst deﬁning individual model—a graphical model describing single individual. clique separator junction tree constructed individual model. then deﬁne copies individual model create population i.i.d. individuals. permits deﬁne count variables number individuals clique conﬁguration counts sufﬁcient statistics individual model. marginalizing away individuals provides model joint distribution typical applications cgms make noisy observations depends variables seek answer queries distribution conditioned observations. cliques individual graphical model contains counts settings clique require clique individual model. addition usual role graphical models inference distribution also serves estimate parameters individual model sufﬁcient statistics individual model. inference cgms much difﬁcult individual model. unlike individual model many conditional distributions closed form. space possible conﬁgurations large count variable take values original paper sheldon dietterich introduced gibbs sampling algorithm sampling subsequent experiments showed exhibits slow mixing times motivated sheldon kumar dietterich introduce efﬁcient algorithm ters. function log-partition function. given ﬁxed parameters subset marginal distribution vector entries possible |a|. particular interested clique marginals node marginals µ{i}. junction trees. development relies existence junction tree cliques write relevant gcgm distributions closed form. henceforth assume junction tree exists. practice means need ﬁll-in edges original model obtain triangulated graph maximal cliques. clear limitation graphs high tree-width. methods apply directly trees practical treewidth graphs. since properties junction tree directly review essential details review reader lauritzen details. cliques adjacent intersection called separator. separators number times appears separator i.e. number different edges |a|) complete vector counts possible settings variables particular n{u} vector node counts. also combined vector clique separator counts—these sufﬁcient statistics sample size graphical model. distribution vector distribution. computing approximation based minimizing tractable convex approximation distribution. although approximation still scales exponentially domain size individual-model variables fast enough permit ﬁtting cgms modestsized instances however given wish apply problems need method even efﬁcient. paper introduces gaussian approximation cgm. count variables multinomial distribution reasonable apply gaussian approximation. however approach raises three questions. first gaussian approximation asymptotically correct? second maintain sparse dependency structure distribution critical efﬁcient inference? third well work natural observation distributions counts poisson distribution? paper answers questions proving asymptotically correct gaussian approximation cgms. shows approximation done correctly able preserve dependency structure cgm. demonstrates applying expectation propagation non-gaussian observation distributions handled. result inference procedure gives good accuracy achieves signiﬁcant speedups previous methods. beyond cgms main result highlights remarkable property discrete graphical models asymptotic distribution vector sufﬁcient statistics gaussian graphical model conditional independence properties original model. consider graphical model deﬁned graph nodes clique denote random variables assume simplicity variables take values domain size particular conﬁguration variables subvector variables belonging clique non-negative potential function. probability model notation refers matrix whose here entry marginal probability note follows immediately deﬁnition covariance indicator variables easily seen scalar form pr−pr also covers case nonempty. particular recover diag µuµt covariance matrix marginal multinomial distribution preceding arguments becomes clear covariance matrix full vector indicator variables simple block structure. deﬁne vector concatention clique separator indicator variables corresponding vector concatenation marginals. follows covariance matrix matrix whose block marginal distribution µab. model count vector i.i.d. copies result moments obtained scaling moments thus arrive natural moment-matching gaussian approximation cgm. deﬁnition gaussian denoted gcgm multivariate normal distribution vector clique separator marginals graphical model parameters deﬁned equation denote distribution cgm. here notation means adjacent proposition ﬁrst proved nearly form sundberg proposition differs presentations writing terms original parameters instead clique separator marginals including hard constraints base measure hard constraints enforce consistency sufﬁcient statistics cliques adjacent separators treated implicitly prior sheldon dietterich proof equivalence expression expressions prior work given supplementary material. dawid lauritzen refer distribution hyper-multinomial distribution fact follows conditional independence properties analogous original graphical model. proof probability model factors clique separator count vectors factors different count vectors appear together consistency constraints appear together adjacent thus graphical model structure claim follows. section develop gaussian approximation gcgm show asymptotically correct distribution goes inﬁnity. show gcgm conditional independence structure explicitly derive conditional distributions. allow gaussian message passing gcgm practical approximate inference method cgms. part consequence fact conditional independence properties hold also hold limit intuitively clear seems require justiﬁcation provided supplementary material. goal inference gcgm tractable approximate alternative inference method cgms. however difﬁcult compute covariance matrix cliques. particular note block requires joint marginal adjacent hard compute. fortunately sidestep problem completely leveraging graph structure part theorem write distribution product conditional distributions whose parameters easy compute perform inference gaussian message passing resulting model. challenge full rank gcgm distribution written degenerate density. seen noting vector nonzero probability satisﬁes afﬁne consistency constraints —for example vector sums population size n—and afﬁne constraints also hold probability limiting distribution. this instead linear transformation reduced vector reduced covariance matrix invertible. work wainwright proposed minimal representation graphical model corresponding random variable full rank covariance matrix. transformation project indicator variable form. full rank covariance matrix. denote maximal non-maximal cliques triangulated graph. note must subset subset also clique denote every space possible conﬁgurations excluding largest value domain variable means agree setting variables follows whole transformation built blocks follows every choose construct block blocks zero. redundancy might many ways choosing work long proposition deﬁne above deﬁne separated holds za+. proof since follows ˜za+ ˜za+ ˜zb+ deterministic functions respectively. since deterministic function ˜zs+ property holds condition instead proof bijection indicates model representation wainwright deﬁnes model wainwright full rank covariance matrix result gcgm decomposed conditional distributions distribution nondegenerate gaussian distribution. consider observations cliques observations. require subset clique choosing distribution modeler substantial ﬂexibility. example permits closed-form inference. consist independent noisy observations zc). little work describe decompose gcgm special case original graphical model tree. assume counts single nodes observed. case marginalize edge counts z{uv} retain node counts gcgm normal distribution marginalization easy. conditional distribution deﬁned node counts. deﬁnition proposition property conditional independence write arbitrarily-chosen root node directed edges oriented away marginalization edges greatly reduces size inference problem similar technique also applicable general gcgms. specify parameters gaussian conditional densities assume blocks deﬁned marginal vector node without last entry ˜µuv marginal matrix edge minus ﬁnal column. mean covariance martix joint distribution need infer z{uv} distribution ﬁrst calculate distribution time assume blocks t{uv}+{uv} deﬁned mean variance applying linear transformation t{uv}+{uv} mean variance z{uv}. standard gaussian conditioning formulas give ˜zv). conditional distribution remark reasoning gives completely different deriving results wainwright concerning sparsity pattern inverse covariance matrix sufﬁcient statistics discrete graphical model. conditional independence proposition factored gcgm density translates directly sparsity pattern precision matrix ˜σ−. unlike reasoning wainwright derive sparsity directly conditional independence properties asymptotic distribution fact gcgm share covariance matrix. consider problem inference gcgm observations noisy. throughout remainder paper assume individual model—and hence cgm—is tree. case cliques correspond edges separators correspond nodes. also assume nodes observed. notational simplicity assume every node observed instead represent edge clique. finally assume entries dropped vector described previous section factored density described corresponding continuous variable determines amount noise distribution. denote vector observations note missing entry must reconstructed remaining entries computing likelihood. poisson observations longer closed-form solution message passing gcgm. address applying expectation propagation laplace approximation. method previously applied nonlinear dynamical systems ypma heskes projection operator proj computed steps. first joint approximating normal distribution laplace approximation project onto random variables laplace approximation step need mode calculate hessian mode obtain mean variance approximating normal distribution optimization problem solved optimizing ﬁrst optimal value computed closed form terms since normal densities involved. optimal value found gradient methods function concave always global optimum. note decomposition approach depends tree structure model hence work observation distribution. mode mean variance normal distribution approximating distribution edge counts inferred method section projection step distribution projected marginalizing other. computational complexity inference gcgm? inferring node counts must solve optimization problem compute ﬁxed number matrix inverses. matrix inverse takes time laplace approximation step gradient calculation takes time. suppose iterations needed. outer loop suppose must perform passes message passing iteration sweeps whole tree. overall time maximization problem laplace approximation smooth concave relatively easy. experiments usually converges within iterations. task inferring edge counts consider complexity calculating mean needed applications. part solved closed form time-consuming operation matrix inversion. exploiting simple structure covariance matrix obtain inference method time complexity section evaluate performance method compare approximation sheldon kumar dietterich evaluation data generated bird migration model introduced shelet model simulates migration population birds map. entire population initially located bottom left corner map. bird makes independent migration decisions time steps. transition probability cell cell time step determined logistic regression equation employs four features. features encode distance cell cell degree cell falls near path cell destination cell upper right corner degree cell lies direction toward wind blowing factor encourages bird stay cell denote parameter vector logistic regression formula. simulation individual model bird -step markov chain domain consists cells map. variables vectors length containing counts number birds cell time number birds moving cell cell time time refer node counts edge counts time step data generation model generates observation vector length contains noisy counts birds cells time observed counts generated poisson distribution unit intensity. consider inference tasks. ﬁrst task parameters model given task infer expected value posterior distribution time step given observations measure accuracy node counts edge counts separately. important experimental issue cannot compute true estimates node edge counts. course values generated simulation noise introduced observations necessarily expected values posterior. instead estimate expected values running mcmc method burn-in period million gibbs iterations collecting samples million gibbs iterations averaging results. evaluate accuracy approximate methods relative error ||napp nmcmc||/||nmcmc|| napp approximate estimate nmcmc value obtained gibbs sampler. experiment report mean standard deviation relative error computed runs. generates values node counts edge counts observation counts requires separate mcmc baseline run. compare method approximate method introduced sheldon treating counts continuous approximating factorial function method ﬁnds approximate mode posterior distribution solving convex optimization problem. work shows method much efﬁcient gibbs sampler produces inference results parameter estimates similar obtained long mcmc runs. second inference task estimate parameters transition model observations. performed expectation maximization gcgm method applied compute step. compute relative error respect true model parameters. table compares inference accuracy approximate gcgm methods. table ﬁxed logistic regression coefﬁcient vector varied population size smallest population size approximation slightly better although result statistically signiﬁcant. makes sense since gaussian approximation weakest population size small. larger population sizes gcgm gives much accurate results. note approximation exhibits much higher variance well. second inference experiment vary magnitude logistic regression coefﬁcients. large coefﬁcients transition probabilities become extreme gaussian approximation work well. ﬁxed evaluated three different parameter vectors table shows gcgm much accurate approximation gives third inference experiment explores effect varying size map. increases size domain random variables also increases number values must estimated vary scale population size accordingly setting coefﬁcient vector results table show smallest methods give similar results. number cells grows relative error approximation grows rapidly variance result. comparison relative error gcgm method barely changes. turn measuring relative accuracy methods learning. experiment vary population size iteration compute relative error ||wlearn−wtrue||/||wtrue|| wlearn parameter vector estimated learning methods wtrue parameter vector used generate data. figure shows training curves three parameter vectors results consistent previous experiments. small population sizes gcgm well approximation. cases overﬁts data. approximation also exhibits overﬁtting. creates extreme transition probabilities also observe approximation learns faster although gcgm eventually matches performance enough iterations. ﬁnal experiment measures time required experiment varied perform inference. used parameter vector measured time consumed infer node counts edge counts. proximation. true measured terms estimating values latent node edge counts estimating parameters underlying graphical model. gcgm method running factor faster. gcgm approximation particularly good population size large transition probabilities near conversely population size small probabilities extreme approximation gives better answers although differences statistically signiﬁcant based trials. surprising ﬁnding approximation much larger variance answers gcgm method. paper introduced gaussian approximation collective graphical model shown case observations depend separators gcgm limiting distribution population size showed gcgm covariance matrix maintains conditional independence structure presented method efﬁciently inverting covariance matrix. applying expectation propagation developed efﬁcient algorithm message passing gcgm non-gaussian observations. experiments bird migration simulation showed gcgm method least accurate approximation shelet exhibits much lower variance times faster compute. method infers node edge counts jointly whereas gcgm ﬁrst infers node counts computes edge counts them. report time required computing node counts also total time required compute node edge counts. figure shows running time approximation much larger running time gcgm approximation. values except average running time gcgm times faster approximation. plot also reveals computation time gcgm dominated estimating node counts. detailed analysis implementation indicates laplace optimization step time-consuming. overall l|a| entries square matrix. view ˜ia+ matrices indicator function graph conﬁgurations. since trivial linear combination ˜ia+ constant conclusion wainwright ˜ia+ linearly independent columns. therefore must full rank must linearly independent columns. show showing descibe probability ordered sample sufﬁcient statistics indeed suppose exists ordered sample sufﬁcient statistics clear inspection junction tree reparameterization remains show exists whenever exactly shown sheldon dietterich junction trees hard constraints enforce local consistency integer count variables equivalent global consistency property exists ordered sample sufﬁcient statistics equal brieﬂy note interesting corollaries argument. first reasoning reparameterization factors used replace distribution. second base measure exactly number different ordered samples sufﬁcient statistics equal also expression instances replaced note assumed conditional independence property because sequence {nn} converges distribution convergence term corresponding term means", "year": 2014}