{"title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption  Generator?", "tag": ["cs.CL", "cs.CV", "cs.NE"], "abstract": "In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators.", "text": "neural image captioning systems recurrent neural network typically viewed primary ‘generation’ component. view suggests image features ‘injected’ rnn. fact dominant view literature. alternatively instead viewed encoding previously generated words. view suggests used encode linguistic features ﬁnal representation ‘merged’ image features later stage. paper compares architectures. that late merging outperforms injection suggesting rnns better viewed encoders rather generators. image captioning emerged important testbed solutions fundamental challenge grounding symbolic linguistic information perceptual data captioning systems focus hodosh refer concrete conceptual descriptions captions describe strictly within image although recently growing interest moving beyond this research visual question-answering imagegrounded narrative generation among others. systems rely computer vision techniques extract object detections features source image using input stage latter roughly akin microplanning realisation modules well-known pipeline architecture systems frame task retrieval problem caption parts thereof identiﬁed computing proximity/relevance strings training data given image. done exploiting either unimodal multimodal space. many retrieval-based approaches rely neural models handle image features linguistic information systems also rely neural models rather performing partial wholesale caption retrieval generate novel captions using recurrent neural network usually long short-term memory typically models image features extracted pre-trained convolutional neural network bias towards sampling terms vocabulary sequence terms produces caption relevant image suggest ‘the trained generate next word view also expressed lecun similar position also taken work focusing rnns language models generation however alternative view possible whereby role thought primarily encode sequences directly generate them. views associated different architectures neural caption generators discuss illustrated figure class architectures image features directly incorporated sequence encoding process models natural think primary generation component image captioning system making predictions conditioned image. different architecture keeps encoding linguistic perceptual features separate merging later multimodal layer point predictions made type model functioning primarily encoder sequences word embeddings visual features merged linguistic features later multimodal layer. multimodal layer drives generation process since never sees image hence would able direct generation process. architectural alternatives attested literature implications knowledge systematically discussed comparatively evaluated. follows ﬁrst discuss distinction architectures present experiments comparing conclusion grounding language generation image data best conducted architecture ﬁrst encodes modalities separately merging predict captions. neural language model encodes preﬁx either predicts next item sequence help feed forward layer else passes encoding next layer make prediction itself. item added preﬁx next iteration predict another item end-of-sequence symbol reached. typically prediction carried using softmax function sample next item according probability distribution vocabulary items based activation. process illustrated figure condition predict image captions inject visual linguistic features directly depicted figure refer ‘conditioning-by-inject’ different types inject architectures become widely attested among deep learning approaches image captioning non-linguistic information inject architecture hand uses full image features every word preﬁx training effect learning ‘visuo-linguistic’ representation word. effect image features serve specify disambiguate ‘meaning’ words disambiguating tokens word correlated different image features implies inject models learn larger vocabulary training. architectures also differ number parameters need handle. noted above since inject architecture combines image word training effectively handling larger vocabulary merge. assume image vectors concatenated word embedding vectors ﬁnal state then inject architecture number figure rnns work state encodes preﬁx incorporates output word derived previous state. practice neural network output single word probability distribution known words vocabulary. legend feedforward layer; <beg> start-of-sentence token; <end> end-of-sentence token. yang zhou given training pairs consisting image caption component models trained exposure preﬁxes increasing length extracted caption tandem image. alternative architecture refer ‘conditioning-by-merge’ treats exclusively ‘language model’ encode linguistic sequences varying length. linguistic vector resulting encoding subsequently combined image features separate multimodal layer. amounts viewing primarily encoder linguistic information. type architecture also attested literature albeit lesser extent inject architecture limited number approaches also proposed architectures combined weights function caption embedding images whereas merge word embeddings contribute size layer network. size word embedding size vocabulary image vector size state size rnn. inject case number weights whereas merge. smaller number weights handled merge offset larger number weights ﬁnal softmax layer take input state image size systematic comparison architectures would shed light best conceive role rnns neural language generation. apart theoretical implications concerning stage language grounded visual information comparison also practical implications. particular turns merge outperforms inject would imply linguistic representations encoded could pre-trained re-used variety tasks and/or image captioning datasets domain-speciﬁc training required ﬁnal feedforward layer tuning required make perceptually grounded predictions carried out. return point section evaluate performance inject merge architectures thus roles trained evaluated flickrk flickrk datasets image-caption pairs. purposes experiments used version datasets distributed karpathy fei-fei dataset splits identical used karpathy fei-fei flickrk split images training validation testing whilst flickrk split images training images validation images testing. image figure illustration different architectures tested paper. numbers letters bottom refer vector size output layer. arbitrary layer size varied experiments vocabulary size also varied experiments. ‘dense’ means fully connected layer bias. datasets different captions. element image feature vectors extracted pre-trained also available distributed datasets. normalised image vectors unit length preprocessing. tokens frequency lower threshold training replaced ‘unknown’ token. experiments varied threshold order measure performance model vocabulary size changes. thresholds gives vocabulary sizes flickrk flickrk. since purpose compare performance architectures used ‘barest’ models possible fewest number hyperparameters. means complexities usually introduced order reach state-of-the-art performance regularization avoided since difﬁcult determine combination hyperparameters give unfair advantage architecture other. input hidden state inputs all-zeros vector cell state inputs all-zeros vector input gate inputs forget gate inputs output gate inputs input gate inputs modiﬁed input used calculate inputs weight matrix bias vector elementwise vector multiplication operator ‘sig’ refers sigmoid function. hidden state cell state always size. experiments basic neural language model used part different architectures inject architecture image vector concatenated word vectors caption. merge architecture concatenated ﬁnal lstm state. layer sizes embedding lstm state projected image vector also varied experiments order measure effect increasing capacity networks. layer sizes used details architectures used experiments illustrated figure training performed using adam optimisation algorithm default hyperparameters minibatch size captions. cost function used crossentropy. training carried early stopping criterion terminated training soon performance validation data started deteriorate initialization weights done using xavier initialization biases zero. beam width clipped maximum length words. mscoco evaluation code used measure quality captions using standard evaluation metrics bleu meteor cider rouge-l also calculated percentage word types actually used generated captions vocabulary available word types. measure indicates well architecture exploits vocabulary trained table reports means standard deviations three runs mscoco measures vocabulary usage. since point compare effects architectures rather reach state-of-the-art performance include results published systems tables. across experimental variables performance merge architecture generally superior inject architecture measures except rougel bleu follows focus cider measure caption quality speciﬁcally designed captioning systems. although merge outperforms inject rather narrow margin standard deviation three training runs suggests consistent performance advantage across train-and-test runs. case clearly disadvantage merge strategy respect injecting image features. peculiarity results flickrk better flickrk. could mean flickrk captions contain less variation hence easier perform well preliminary results larger dataset mscoco show cider results table results captions generated using inject merge architectures. values means three separately retrained models together standard deviation parentheses. legend layer layer size used vocab. vocabulary size used. inject models tend imquency threshold prove increasing state size datasets relationship performance merge state size shows discernible trend. inject therefore seem overﬁt state size increases even larger dataset. time inject seems able outperform best scores achieved merge much larger layer size. therefore practical terms inject models larger capacity merge. differently merge higher performance model size ratio makes efﬁcient limited resources given layer sizes vocabulary number parameters merge greater inject. difference becomes greater vocabulary size increased. vocabulary size layer size merge parameters inject whilst vocabulary size layer size merge parameters. however foregoing remarks concerning overunder-ﬁtting also apply difference number parameters small. difference performance least part architectural differences differences number parameters. merge models greater proportion training vocabulary test captions. however proportion vocabulary used generally quite small architectures less flickrk less flickrk. overall trend smaller proportions overall training vocabulary used vocabulary grows larger suggesting neural language models harder infrequent words practice means reducing training vocabularies results minimal performance loss. overall evidence suggests delaying merging image features linguistic encodings late stage architecture advantageous least corpus-based evaluation measures concerned. furthermore results suggest merge architecture higher capacity inject architecture generate better quality captions smaller layers. primary role generating captions would need access image order know generate. seem case including image generally beneﬁcial performance caption generator. viewing rnns primary role encoding rather generating makes sense inject architecture generally suffers performance compared merge architecture. plausible explanation handling variation. consider task image captioning task training captions broken preﬁxes increasing length preﬁx compressed ﬁxed-size vector illustrated figure above. inject architecture encoding task made complex inclusion image features. indeed version inject used experiments commonly used solution caption generation literature image features concatenated every word caption. upshot requirement compress caption preﬁxes together image data ﬁxed-size vector substantial growth vocabulary size handle image+word treated single ‘word’. problem alleviated merge encodes linguistic histories only expense parameters softmax layer. practical consequence ﬁndings that merge models handle variety smaller layers increasing state size merge architecture potentially quite proﬁtable entire state used remember greater variety previously generated words. contrast inject architecture increase memory would used better accommodate information distinct combined modalities. referring architectures inject image features parallel word embeddings rnn. literature type architecture used image features might included words changed different words paper presented views role image caption generator. ﬁrst decides word likely generated next given generated before. multimodal generation view encourages architectures image incorporated along words generated order allow make visuallyinformed predictions. second view rnn’s role purely memory-based encode sequence words generated thus far. representation informs caption prediction later layer network function encoding perceptual features. view encourages architectures vision langauge brought together late multimodal layer. caption generation turns perform worse general image features injected rnn. thus role better conceived terms learning linguistic representations used inform later layers neural network predictions made based generated past together image guiding generation. component primarily involved generating caption would need informed image order know needs generated; however line reasoning seems hurt performance applied architecture. suggests case main component caption generator involved generation. short given neural network architecture expected process input sequences multiple modalities arriving joint representation would better separate component encode input bringing together late stage rather pass separate input channels. respect question language grounded perceptual data tentative answer offered experiments link symbolic perceptual established late encoding performed. recurrent networks best viewed learning represenexperiments reported conducted separate datasets. concern results flickrk flickrk entirely consistent though superiority merge inject clear both. currently extending experiments larger mscoco dataset insights discussed paper invite future research generally applicable merge architecture different domains. would like investigate whether similar changes architecture would work sequence-to-sequence tasks machine translation instead conditioning language model image conditioning target language model sentences source language. similar question arises image processing. conditioned sensitive certain types objects saliency differences among regions complex image conditioning vector incorporated beginning thereby conditioning entire would better instead incorporate ﬁnal layer saliency differences would based high-level visual features? also practical advantages merge architectures transfer learning. since merge keeps image separate used captioning conceivably transferred neural language model trained general text. cannot done inject architecture since would need trained combine image text input. future work intend performance caption generator affected weights initialized general neural language model along lines explored neural machine translation references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answerproc. iccv’ pages santiago ing. chile. ieee. satanjeev banerjee alon lavie. meteor automatic metric evaluation improved proce. workcorrelation human judgments. shop intrinsic extrinsic evaluation measures machine translation and/or summarization volume pages raffaella bernardi ruket cakici desmond elliott aykut erdem erkut erdem nazli ikizler-cinbis frank keller adrian muscat barbara plank. automatic description generation images survey models datasets evaluation measures. journal artiﬁcial intelligence research xinlei chen lawrence zitnick. mind’s recurrent visual representation image caption generation. proc. cvpr’. institute electrical electronics engineers jun. jeff donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. ieee conference computer vision pattern recognition institute electrical electronics engineers jun. stevan harnad. symbol grounding problem. lisa anne hendricks subhashini venugopalan marcus rohrbach raymond mooney kate saenko trevor darrell. deep compositional captioning describing novel object categories without paired training data. proc. cvpr’. institute electrical electronics engineers jun. micah hodosh peter young julia hockenmaier. framing image description ranking task journal data models evaluation metrics. artiﬁcial intelligence research may. flickrk. ting-hao huang francis ferraro nasrin mostafazadeh ishan misra aishwarya agrawal jacob devlin ross girshick xiaodong pushmeet kohli dhruv batra lawrence zitnick devi parikh lucy vanderwende michel galley margaret mitchell. visual proc. naacl-hlt’ pages storytelling. andrej karpathy fei-fei. deep visualsemantic alignments generating image descripproc. cvpr’. institute electrical tions. electronics engineers june. diederik kingma jimmy girish kulkarni visruth premraj sagnik dhar siming yejin choi alexander berg tamara berg. baby talk understanding generating image descriptions. proceedings ieee conference computer vision pattern recognition pages colorado springs. ieee. automatic evaluation machine translation quality using longest common subsequence skip-bigram statistics. proc. acl’. association computational linguistics tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr doll´ar lawrence zitnick. microsoft coco common objects context. computer vision eccv pages springer nature. junhua yang jiang wang zhiheng huang alan yuille. learning like child fast novel visual concept learning sentence descriptions images. proc. iccv’ santiago chile institute electrical electronics engineers margaret mitchell jesse dodge amit goyal kota yamaguchi karl stratos xufeng alyssa mensch alex berg xufeng tamara berg daume iii. midge generating image descriptions computer vision detections. proc. eacl’ pages avignon france. association computational linguistics. imtext describing images using million captioned photographs. proceedings conference advances neural information processing systems pages granada spain. curran associates ltd. kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evaluation machine translation. proc. acl’ pages association computational linguistics. prajit ramachandran peter quoc unsupervised pretraining sequence sequence learning. arxiv. mingoo song chang yoo. multimodal representation kneser-ney smoothing/skip-gram based ieee international neural language model. conference image processing institute electrical electronics engineers sep. ilya sutskever james martens geoffrey hinton. generating text recurrent neural networks. procededings international conference machine learning pages bellevue acm. ramakrishna vedantam lawrence zitnick devi parikh. cider consensus-based image ieee conference scription evaluation. computer vision pattern recognition institute electrical electronics engineers jun. oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image caption generator. ieee conference computer vision pattern recognition institute electrical electronics engineers jun. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation viproceedings intersual attention. national conference machine learning volume abs/. page quanzeng hailin zhaowen wang chen fang jiebo luo. image captioning semantic attention. proc. cvpr’. institute electrical electronics engineers jun. peter young alice micah hodosh julia hockenmaier. image descriptions visual denotations similarity metrics semantic inference event descriptions. transactions association computational linguistics", "year": 2017}