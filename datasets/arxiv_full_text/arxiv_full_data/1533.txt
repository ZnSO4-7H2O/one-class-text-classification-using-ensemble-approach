{"title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "tag": ["cs.CL", "cs.AI", "cs.LG", "H.3.1; I.2.6; I.2.7"], "abstract": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, \"buy\" entails \"own\". Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word-context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.", "text": "inference natural language often involves recognizing lexical entailment identifying whether word entails another. example entails own. general strategies proposed strategy manually construct asymmetric similarity measure context vectors another treat problem learning recognize semantic relations using supervised machine learning techniques paper experiment recent state-of-the-art representatives general strategies. ﬁrst approach asymmetric similarity measure designed capture degree contexts word form subset contexts another word second approach represents word pair feature vector concatenation context vectors applies supervised learning training labeled feature vectors. additionally introduce third approach instance relation classiﬁcation strategy. third approach represents word pair feature vector features diﬀerences similarities reference words. three approaches vector space models semantics based word–context matrices. perform extensive evaluation three approaches using three diﬀerent datasets. proposed approach performs signiﬁcantly better approaches datasets dataset signiﬁcantly worse. along address concerns raised past research regarding treatment problem semantic relation classiﬁcation suggest beneﬁcial make connections research lexical entailment research semantic relation classiﬁcation. recognizing textual entailment popular task natural language processing research relevance text summarization information retrieval information extraction question answering machine translation paraphrasing applications involves pairs sentences following objective develop algorithms determine whether text sentence entails hypothesis sentence. gold standard entailment established common sense rather formal logic. text entails hypothesis meaning hypothesis inferred meaning text according typical human interpretations text hypothesis example text entails hypothesis. recent years pairs grown richer challenging. text whole paragraph. vector space models semantics particularly useful lexical semantics hence natural apply rle. paper experiment three algorithms lexical entailment. three word–context matrices word corresponds vector called context vector. given word corresponding context vector represents distribution word various contexts. contexts consist words occur near given word large corpus text. models inspired distributional hypothesis ﬁrst three algorithms balapinc attempts address problem asymmetric similarity measure context vectors idea design measure captures context inclusion hypothesis paraphrase geﬀet dagan call distributional inclusion hypothesis. prefer call context inclusion rather distributional inclusion clear included. text–hypothesis example above intent balapinc take context vectors words calculate numerical score measures degree contextually includes context inclusion hypothesis inspired model theory formal logic assertions formal logic. model theory means entails models true subset models true. models include models second third algorithms approach task recognizing lexical entailment using techniques research semantic relation classiﬁcation. semantic relation classiﬁcation task learning recognize word pair instance given semantic relation class lexical entailment covered hyponymy– hypernymy semantic relation. word pair instance hyponym– hypernym relation relatively large body work semantic relation classiﬁcation general good results hyponym– hypernym relation particular since semantic relation classiﬁcation algorithms worked well important subclass lexical entailment seems plausible approach expanded cover subclasses lexical entailment perhaps subclasses lexical entailment. second three algorithms represents word pair feature vector concatenation context vector context vector example concatenation three-dimensional vectors six-dimensional vector algorithm given name baroni ease reference call convecs context combination hypothesis tendency entail correlated learnable function contexts occurs contexts occurs; conjunctions contexts tend indicate entailment others tend indicate lack entailment. hypothesis implies contexts contexts suitable features feature vector representation word pair hypothesis correct concatenated context vectors appropriate representation word pairs supervised machine learning lexical entailment. hypothesis explicitly stated baroni implicit approach. supervised learning word pairs context concatentation ﬁrst-order feature vector representation word pairs. call ﬁrst-order features directly based elements context vectors. paper introduces algorithm simdiﬀs third three algorithms evaluate. simdiﬀs uses second-order feature vector representation features diﬀerences similarities reference words similarities given cosines ﬁrst-order context vectors reference words similarity diﬀerences hypothesis tendency entail correlated learnable function diﬀerences similarities reference words diﬀerences tend indicate entailment others tend indicate lack entailment. example consider animal versus table animal. suppose life reference words. animal similar respect reference word life; diﬀerence similarities small. hand table animal dissimilar respect life; large diﬀerence similarities. diﬀerences important entailment others usually little eﬀect given labeled training data able learn diﬀerences similarities aﬀect lexical entailment. empirically evaluate three algorithms balapinc convecs simdiﬀs using three diﬀerent datasets. simdiﬀs performs signiﬁcantly better algorithms cases case signiﬁcantly worse. convecs signiﬁcantly worse balapinc simdiﬀs dataset whereas balapinc signiﬁcantly worse convecs dataset signiﬁcantly worse simdiﬀs datasets. section deﬁnes lexical entailment terms semantic relations words. disagreement whether lexical entailment approached semantic relation classiﬁcation task. address issue section past work examined section performance measures algorithms presented section describe three algorithms detail section three algorithms evaluated using three datasets presented section datasets kotlerman baroni jurgens mohammad turney holyoak experimental results reported section discuss implications experiments section limitations work considered section conclude section present diﬀerent deﬁnition lexical entailment here. idea whether word entails another depends semantic relation words. discuss objections idea section words. able entails outside context speciﬁc sentence must case strong semantic relation entailment must follow nature semantic relation. entails following three conditions fulﬁlled typical relation given typical semantic relation comes mind. typical semantic relation between typical semantic relation cannot entail outside speciﬁc context. ﬁrst condition relational deﬁnition typical relation relation naturally comes mind presented together. multiple senses juxtaposition suggest semantic relation also constrain possible senses words. constrained senses words necessarily frequent prototypical senses words. example consider words lion cat. word senses house feline words lion juxtaposed relation naturally comes mind hyponym–hypernym sense constrained feline although house sense frequent prototypical feline sense. context determines sense ambiguous word lexical entailment considers word pairs outside context sentences. since word senses aﬀect entailment approach lexical entailment must decide handle ambiguous words. substitutional deﬁnition lexical entailment invites imagine natural sentence provides missing context constrains possible senses words. relational deﬁnition lexical entailment invites imagine semantic relation connects words constrains possible senses. second condition relational deﬁnition determines whether word entails another based semantic relation. since hyponym implies hypernym lion entails cat. second condition excludes semantic relations imply entailment. example antonymy excluded hyponym–hypernym relation included direction correct ﬁrst condition substitutional deﬁnition lexical entailment asks consider whether sense word implies sense another word. hypothesize implication must depend semantic relation senses words. seems that semantic relation words possible word imply other. words implies another implication must follow nature semantic relation. idea second condition relational deﬁnition lexical entailment make connection semantic relations lexical entailment explicit. third condition relational deﬁnition handles ambiguous cases erring side non-entailment. people might feel lion suggest either hyponym–hypernym relation coordinate relation coordinates words shared hypernym. lion house share hypernym feline. means house lion coordinates. hyponym implies hypernym coordinates imply other. lion implies feline sense house sense. thus relations agree whether lion implies cat. case believe hyponym–hypernym natural lion implies cat. people feel semantic relations natural third condition says entailment; them lion imply cat. third condition could modiﬁed diﬀerent uses. dataset chose side non-entailment ideally choice would made based downstream application. applications better side entailment. possibility give higher weight relations weighting choose entailment nonentailment relations disagree. weighting could based corpus frequency relations contexts words appear. apply relational deﬁnition lexical entailment helpful taxonomy semantic relations provide options paper taxonomy bejar chaﬃn embretson includes seventynine subcategories semantic relations grouped high-level categories. taxonomy given tables section results indeed comparable past work. substitutional deﬁnition relational deﬁnition operational deﬁnitions tests used determine presence entailment. require understanding word implies implies synonym entails; theoretical deﬁnitions entailment. attempt objectively capture underlying notion implication hence compared contrasted terms well capture notion. zhitomirsky-geﬀet dagan’s substitutional deﬁnition lexical entailment intended capture substitutional cases entailment. explicitly excluded non-substitutable lexical entailment. argue conditions yield good inter-annotator agreement result lexical entailment decisions well needs systems recognizing textual entailment. believe trade-oﬀ inter-annotator agreement coverage. substitutional relational deﬁnitions diﬀer regarding trade-oﬀ. substitutional deﬁnition leads higher levels inter-annotator agreement relational deﬁnition substitutional deﬁnition excludes important cases lexical entailment word pair glassfragile typical relation comes mind itemattribute attribute thus ﬁrst condition relational deﬁnition fulﬁlled. item entails attributes; glass entails fragile; thus second condition fulﬁlled. exceptions bulletproof glass bulletproof glass typical glass. typical relation glass fragile third condition fulﬁlled. limitation substitutability deﬁned zhitomirsky-geﬀet dagan allow lexical entailment part speech another. example glass entails fragile glass noun fragile adjective cannot substitute sentence. however spite diﬀerence parts speech seems reasonable glass entails fragile. typical situation involves glass situation also involves something fragile. another example case substitutional deﬁnition excludes lexical entailment relational deﬁnition captures consider bequeathheir instance actrecipient relation address limitation substitutional deﬁnition possibility would relax deﬁnition substitutability cope diﬀerent parts speech. example given noun adjective could allow replaced ‘something perhaps relatively small list substitutional patterns could handle part speech substitution cases. however pursue option here address fundamental limitation substitutional deﬁnition absence semantic relations. believe semantic relations lexical entailment intimately connected idea substitional patterns suggests generalization lexical entailment phrasal entailment. example phrase bequeathed entails phrase heir patterns like learned corpora applied successfully however focus lexical entailment phrasal entailment. believe good algorithm lexical entailment useful component algorithm phrasal entailment. experiments three diﬀerent datasets. three consist word pairs labeled entails entail. dataset labeled using zhitomirsky-geﬀet dagan’s substitutional deﬁnition. preliminary inspection seems semantic relations dataset often part–whole hyponym–hypernym relations word pairs systematically labeled relation categories. another dataset pairs labeled entails instances hyponym–hypernym relation. third dataset pairs generated bejar al.’s taxonomy. dataset includes pairs sampled seventy-nine subcategories taxonomy. pair labeled entails entail based subcategory came from. tables section list subcategories relations entailment labels. semantic relation classiﬁcation literature supervised learning algorithms applied task classifying word pairs. general algorithms capable classifying symmetric asymmetric relations. particular convecs simdiﬀs approach lexical entailment problem supervised relation classiﬁcation capable learning symmetric asymmetric relations. able learn lexical entailment behaves asymmetrically behaves symmetrically balapinc measure designed capture asymmetry likely give approximately equal scores carautomobile automobilecar. seen considering details deﬁnition lexical entailment superset known relations rather designed select sub-cases lexical relations needed applied entailment inference. example lexical entailment cover cases meronyms sub-cases part–whole relationship mentioned herein. addition relations also covered lexical entailment like ocean water murder death seem directly correspond meronymy hyponymy relations. notice also whereas lexical entailment directional relation speciﬁes word pair entails other relation hold directions pair words case synonyms. agree zhitomirsky-geﬀet dagan sub-cases part– whole involve lexical entailment sub-cases not. however issue addressed breaking part–whole category subcategories. high-level categories bejar al.’s taxonomy part–whole subcategories. claim eight subcategories involve entailment involve entailment consistent claim ‘lexical entailment cover cases meronyms’ regarding ‘ocean water murder death’ word pair oceanwater instance bejar al.’s objectstuﬀ subcategory murderdeath instance causeeﬀect subcategory regarding relations lexical entailment directions synonymy readily handled marking entailing directions semantic relation subcategories hypothesis lexical entailment superset high-level categories semantic relations superset lower-level subcategories semantic relations. convecs simdiﬀs treat semantic relation classiﬁcation problem. algorithms require semantic relation subcategories hypothesis possible fruitful ideas research semantic relation classiﬁcation even hypothesis wrong. however semantic relation subcategories hypothesis correct even reason treat semantic relation classiﬁcation problem. semantic relation subcategories hypothesis section generating dataset evaluating algorithms. experiments train algorithms using data based bejar al.’s taxonomy test previous lexical entailment datasets. ﬁrst challenge took place regular event since then. since beginning many systems included module recognizing lexical entailment early modules typically used symmetric similarity measure cosine measure measure measure based wordnet understood entailment inherently asymmetric symmetric measure rough approximation proposed asymmetric similarity measure degree word replaced word sentence without substantially changing meaning sentence. weeds weir introduced asymmetric similarity measure degree speciﬁc term subsumed general term idea developed further speciﬁcally application lexical entailment series papers culminated balapinc measure degree entails describe balapinc detail section glickman dagan shnarch deﬁne lexical reference somewhat similar lexical entailment deﬁned relative speciﬁc text sentence. mirkin dagan shnarch deﬁne entailment lexical elements includes entailment words non-compositional elements. deﬁnition based substitutability; accept many kinds lexical entailment excluded substitutability. deﬁnition involves inferred lexical element context natural text. compared number papers lexical entailment relatively large body literature semantic relation classiﬁcation semantic relation classiﬁcation part several semeval exercises papers apply semantic relation classiﬁcation lexical entailment papers emphasize hyponym–hypernym semantic relation important lexical entailment relation involves entailment. algorithms semantic relation classiﬁcation supervised although objection supervised learning lexical entailment require large quantity labeled training data. baroni oﬀer elegant solution training data issue based observation that adjective–noun phrases adjective–noun pair generally entails head noun. example entails cat. observation allows label large quantity training data relatively little eﬀort. however technique seem applicable many relevant subcategories bejar al.’s taxonomy. solution word pairs labeled bejar al.’s classes using amazon’s mechanical turk dataset covers much wider range semantic relations baroni al.’s dataset. diﬀerence asymmetric similarity measure classiﬁcation model based supervised machine learning former yields real-valued score whereas latter gives binary-valued classiﬁcation however diﬀerence superﬁcial. many supervised learning algorithms able generate real-valued probability score likewise easy generate binary-valued class real-valued score setting threshold score. experiments evaluate three algorithms realvalued asymmetric similarity measures binary-valued classiﬁers. average precision performance measure real-valued scores following kotlerman precision recall f-measure accuracy performance measures binary-valued classiﬁcation following baroni balapinc measure originally designed performance measure information retrieval systems. suppose issued query search engine returned ranked list documents sorted descending order automatically estimated degree relevance query. assume human judges manually labeled documents either relevant irrelevant given query. fraction highest ranked documents label relevant. precision ranked list list r-th document. r-th document labeled relevant otherwise. deﬁned follows deﬁnition reﬂects bias information retrieval. typical query typical document collection documents irrelevant emphasis ﬁnding relevant documents. machine learning classes usually considered equally important. kotlerman emphasize class believe class also important. example scoring challenge gives equal reward recognizing text sentence entails hypothesis sentence not. therefore report variations call deﬁne next paragraph. suppose dataset word pairs manually labeled number word pairs dataset. measure assigns real-valued score word pair sort pairs descending order scores. fraction highest ranked pairs label fraction bottom lowest ranked pairs label r-th document labeled otherwise. r-th document bottom labeled otherwise. total number pairs labeled total number pairs labeled deﬁne follows experiments kotlerman report possible increase system’s performance according cost lower performance. formula sensitive labels list. happens bottom list little impact gives weight labels bottom list. hand formula sensitive labels bottom list. focus ignore prefer algorithms list right even poorly bottom list. therefore important report like precision recall originally designed performance measures information retrieval systems. precision system estimate conditional probability document truly relevant query system says relevant. recall system estimate conditional probability system document relevant query truly relevant. tradeoﬀ precision recall; optimized cost other. f-measure harmonic mean precision recall. designed reward balance precision recall. accuracy natural intuitive performance measure sensitive relative sizes classes. easy interpret accuracy equal-sized classes diﬃcult interpret class much larger other. f-measure better measure classes balanced. variations precision recall f-measure depending whether focus class class confusion matrix number word pairs actually class algorithm predicted class deﬁne precision recall f-measure follows section discuss three approaches describe algorithms approach detail. three approaches based word–context matrices. introduction concepts behind word–context matrices survey paper turney pantel preliminary experiments development datasets tuned three approaches optimize performance. describe generated section algorithm selected matrix matrices accurate development data. balapinc convecs chose word–context matrix turney neuman assaf cohen simdiﬀs chose word–context matrices turney convecs simdiﬀs support vector machines supervised learning. used development datasets select best kernels svms. best kernel convecs second-degree polynomial kernel best kernel simdiﬀs radial basis function kernel. include balapinc experiments kotlerman experimentally compared wide range asymmetric similarity measures found balapinc best performance. balapinc asymmetric similarity measure balanced combination asymmetric apinc measure symmetric measure balance achieved using geometric mean simdiﬀs naturally deﬁned terminology linear algebra. theoretical terminology kotlerman linear algebraic terminology turney pantel reader easily perspectives. leads small amount redundancy believe helpful connect points view. first linear algebraic notation suppose word–context matrix vector corresponds word column vector corresponds context. matrix co-occurrence frequencies. word corresponding i-th vector context corresponding j-th column vector number times occurs context given corpus. matrix result calculating positive pointwise mutual information word context element ppmi takes co-occurrence frequencies transforms weights represent importance given context given word. ppmi matrix typically sparse cells negative. theoretical notation given word corresponding i-th contexts nonzero. corresponds corresponds column think contexts features characterize word |fw| number features corresponds i-th |fw| number nonzero cells i-th vector convecs simdiﬀs fundamentally linear algebraic conception whereas balapinc fundamentally theoretic. cannot readily describe three systems kind notation. feature corresponds ppmi value xij. rank features descending order corresponding ppmi values. r-th feature ranking ranges |fw|. rank rank thus rank want normalize rank ranges higher ppmi values closer lower ppmi values closer function provides normalization recall context inclusion hypothesis word tends occur subset contexts word occurs tends entail suppose test features order rank features contextually included consisting features among ﬁrst features included apinc variation average precision measure originally developed measuring performance information retrieval systems consider ﬁrst term highest-ranking feature included otherwise product reduces importance feature word apinc high score important features also important features apinc asymmetric require important features important features parameters maxf varied control performance balapinc. parameter maxf sets maximum number features word. given word calculate features |fw| maxf remove lowest-ranking features |fw| maxf reduces impact low-ranking features apinc score. parameter threshold classiﬁcation. balapinc word pair classiﬁed otherwise classiﬁed describe parameters tuned section experiments balapinc section ppmi matrix matrix used turney matrix rows columns. rows correspond single multi-word entries wordnet columns correspond unigrams wordnet distinguished according whether appear left right context given n-gram. window size context four words left four words right n-gram. matrix density ppmi matrix based corpus words collected university websites webcrawler. corpus indexed wumpus search engine designed passage retrieval rather document retrieval. suppose element matrix co-occurrence frequencies i-th matrix corresponds n-gram wordnet j-th column matrix corresponds unigram value calculated sending query wumpus counting frequency retrieved passages. matrix described detail section turney common smooth ppmi matrix applying truncated singular value decomposition development datasets experimented smoothing matrix results poor. problem truncated yields matrix density balapinc designed highly sparse matrices. convecs algorithm baroni able match performance balapinc. convecs represent word pair concatentation context vectors apply supervised learning algorithm training word pairs word pair represented concatenated context vectors labeled entails entail. supervised learning algorithm generates classiﬁcation model enables assign labels word pairs present training data. decomposes product three matrices uσvt column orthonormal form diagonal matrix singular values rank also rank diagonal matrix formed singular values matrices produced selecting corresponding columns matrix ukσkvt matrix rank best approximates original matrix minimizes approximation errors. ukσkvt minimizes matrices rank denotes frobenius norm parameter well-known literature less familiar. caron introduced improving performance truncated term–document matrices information retrieval. improve performance word–context matrices recall context combination hypothesis tendency entail correlated learnable function contexts occurs contexts occurs; conjunctions contexts tend indicate entailment others tend indicate lack entailment. given context combination hypothesis vector concatenation natural represent learning lexical entailment. supervised learning algorithm baroni used weka libsvm. used polynomial kernel support vector machine also weka polynomial kernel sequential minimal optimization weka generate real-valued probability estimates well binary-valued classes. probability estimates based ﬁtting outputs logistic regression models tried various kernels convecs development datasets found second-degree polynomial kernel best performance. default settings polynomial kernel weka except disable normalization vectors already normalized length. seems convecs good algorithm generic semantic relation representation takes advantage background knowledge lexical entailment might require less training data. thing know lexical entailment convecs reliably recognize similar word pair appears training data labeled entails. cover broad range possible values must many diﬀerent pairs training data. convecs representation make eﬃcient training data. simdiﬀs uses diﬀerent word–context matrices domain matrix function matrix domain matrix designed measuring domain similarity words example carpenter wood high degree domain similarity; come domain carpentry. function matrix designed measuring function similarity example carpenter mason high degree function similarity; function artisans. matrices diﬀerent types context. domain matrix uses nouns occur near given word context word whereas function matrix uses verbs occur near given word. part-of-speech information generated opennlp tagger. motivation using matrices simdiﬀs generate larger varied features supervised learning algorithm. turney demonstrated domain function matrices work together synergetically applied semantic relations. experiments development datasets tried using domain function matrices balapinc convecs algorithms worked better word–context matrix turney simdiﬀs combination domain function matrices turney best performance development datasets. ppmi section results total four parameters need tuned domain space function space. following experiments simplify search parameter space make domain function matrices based corpus word– context matrix turney wumpus used index corpus search passages described section rows columns. ppmi matrix density rows columns. ppmi matrix density matrices truncated results density rows matrices correspond single multi-word entries wordnet. columns complex; turney provides detailed description columns aspects matrices. matrices diﬀerent numbers rows because applying removed rows entirely zero. function matrix lower density zero-valued rows domain matrix. reference words. recall similarity diﬀerences hypothesis tendency entail correlated learnable function diﬀerences similarities reference words diﬀerences tend indicate entailment others tend indicate lack entailment. simdiﬀs represent word pair feature vector composed four sets features deﬁned follows diﬀerence domain space respect similarities reference words diﬀerence function space. based diﬀerences spaces whereas based diﬀerences diﬀerent spaces. cross-spatial diﬀerences seem counterintuitive. consider example murder death suggested quotation zhitomirsky-geﬀet dagan section murder typically involves people victim aggressor whereas death typically involves person deceased. suggests functional diﬀerence words hence function similarities murder quite diﬀerent function similarities death. however perhaps domain similarities murder somewhat similar function similarities death perhaps function similarities murder somewhat similar domain similarities death include similarities supervised learning algorithm make them. reference words words basic english thus word pair represented features. words basic english selected ogden form core vocabulary suﬃcient represent english words paraphrasing. chose words small enough keep number features manageable broad enough cover wide range concepts. reference words also suitable; topic future work. mentioned section convecs ineﬃcient learning hand consider represented simdiﬀs. looking equations that given word pair every feature value zero. therefore take many examples training data learn supervised learning algorithm weka. based experiments development datasets radial basis function kernel. default settings except disable normalization. generate probability estimates classes. section describes three datasets experiments. ﬁrst datasets used past lexical entailment research. third dataset used semantic relation research; ﬁrst time used lexical entailment. refer dataset initials authors paper ﬁrst reported. kdsz dataset introduced kotlerman evaluate balapinc. dataset contains word pairs labeled entails labeled entail. created taking dataset labeled word pairs zhitomirsky-geﬀet dagan adding labeled pairs. labeling original subset pairs described detail zhitomirskygeﬀet dagan deﬁnition lexical entailment judges used substitutional deﬁnition given section three judges labeled pairs inter-annotator agreement three judges varying dataset properties complicate experiments. first class sizes balanced; pairs labeled entail labeled entails. second although every word pair unique words appear many times many diﬀerent pairs. address points experiments. bbds dataset created baroni applied evaluating balapinc convecs. paper baroni discuss several diﬀerent datasets. dataset call described section dataset contains word pairs labeled entails labeled entail. pairs labeled entails hyponym–hypernym noun–noun pairs pope leader. pairs generated automatically wordnet validated manually. although class sizes balanced entails entail bbds dataset representative variety semantic relations involve entailment section also although every word pair unique words appear many times. word pairs composed unigrams unigrams appear wordnet corresponding vectors matrices. jurgens created semantic relation dataset semeval- task measuring degrees relational similarity. dataset contains word pairs labeled seventy-nine types semantic relations. section describe original dataset consists word pairs labeled using relation classiﬁcation scheme bejar hierarchical classiﬁcation system high-level categories subcategories total seventy-nine distinct subcategories. original semeval- dataset generated phases using amazon’s mechanical turk refer mechanical turk workers turkers. ﬁrst phase seventy-nine subcategories turkers shown paradigmatic examples word pairs given subcategory asked generate word pairs semantic relation type. second phase seventy-nine subcategories turkers shown word pairs generated ﬁrst phase asked rate pairs according degree prototypicality given semantic relation type. cleaning improve quality dataset removed lowestrated word pairs subcategory. since original dataset word pairs average subcategory word pairs. cleaning operation reduced pairs subcategory total word pairs mapping mapped subcategory labels labels mapping given tables assume word pairs within subcategory belong class result mapping word pairs labels. pairs labeled pairs labeled interpret tables given pair anestheticnumbness label instrumentgoal table value label instrumentgoal label given pair numbnessanesthetic labeled instrument goal− table value label instrument goal− label words anesthetic numbness seventy-nine subcategories twenty-ﬁve labeled class twelve labeled class note subcategories labeled class shows transformation handles symmetric asymmetric semantic relations. class-inclusion class-inclusion class-inclusion class-inclusion class-inclusion part-whole part-whole part-whole part-whole part-whole part-whole part-whole part-whole part-whole part-whole similar similar similar similar similar similar similar similar contrast contrast contrast contrast contrast contrast contrast contrast attribute attribute attribute attribute attribute attribute attribute attribute attribute taxonomic functional singularcollective pluralcollective classindividual objectcomponent collectionmember masspotion eventfeature stageactivity itemtopological part objectstuﬀ creaturepossession itemnonpart itemex-part synonymity dimension similarity dimension excessive dimension naughty conversion attribute similarity coordinates change contradictory contrary reverse directional incompatible asymmetric contrary pseudoantonym defective itemattribute attributecondition objectstate attributestate objecttypical attributetypical actact attribute actobject attribute actresultant ﬂowertulip weaponknife cutleryspoon dishessaucers mountaineverest carengine foresttree timemoment banquetfood kickoﬀfootball roomcorner glacierice millionairemoney horsewings prisonerfreedom carauto simmerboil concernedobsessed listeneavesdrop grapewine rakefork sondaughter crescendosound alivedead happysad buysell leftright slowstationary hotcool popularshy limpwalk glassfragile edibleeaten beggarpoverty contentiousconﬂict soldierﬁght viablelive creepslow sterilizeinfectious rainwet non-attribute non-attribute non-attribute non-attribute non-attribute non-attribute non-attribute non-attribute case relations case relations case relations case relations case relations case relations case relations case relations cause-purpose cause-purpose cause-purpose cause-purpose cause-purpose cause-purpose cause-purpose cause-purpose space-time space-time space-time space-time space-time space-time space-time space-time space-time reference reference reference reference reference reference itemnonattribute attr.noncondition objectnonstate attr.nonstate obj.atypical attr.atypical actact nonattr. actobject nonattr. agentobject agentrecipient agentinstrument actobject actrecipient objectrecipient objectinstrument recipientinstr. causeeﬀect causecounteract enablerobject actgoal agentgoal instrumentgoal instrumentuse prevention locationitem locationproduct locationactivity locationinstr. contiguity timeactivity timeassociated sequence attachment signsigniﬁcant expression representation plan knowledge concealment harmonydiscordant brittlemolded laureatedishonor dullcunning reclusesocialize reticenttalk creepfast embellishaustere tailorsuit doctorpatient farmertractor plowearth bequeathheir speechaudience pipewrench graduatediploma enigmapuzzlement hungereat matchcandle ﬂeeescape climberpeak anestheticnumbness abacuscalculate pesticidevermin arsenalweapon bakerybread highwaydriving beachswimsuit coastocean childhoodplay retirementpension prologuenarrative beltwaist sirendanger hugaﬀection personportrait blueprintbuilding psychologyminds disguiseidentity independently created mapping like tables disagreed twelve mappings compared tables discussed arrived consensus. twelve disagreements consensus label tables result consensus. used ﬁrst types information table decide relation classes entailment classes. independently created mapping table agreed approach task follows consider paradigm pairs instances given relational schema. interpret pairs light schema. three paradigmatic pairs entails interpreted annotate given category entails likewise entails three paradigmatic pairs entails pair exception seems unusual make note exceptional pair later discussion. mentioned above assume word pairs within subcategory belong class test assumption randomly selected word pairs labeled entails labeled entail. labels independently manually labeled pairs ﬁrst using relational deﬁnition lexical entailment second time using substitutional deﬁnition lexical entailment table shows percentage agreement manual labels automatic labeling generated semeval- dataset mapping tables relational deﬁnition lexical entailment agreed labels. agreement manual labels labels generated automatically applying mapping tables semeval dataset varied numbers suggest assumption word pairs within subcategory belong class reasonable. assumption yields levels agreement comparable agreement manual labels. mentioned section zhitomirsky-geﬀet dagan inter-annotator agreements range whereas agreement hypothesize substitutability relatively objective test leads higher levels agreement excludes important cases lexical entailment. discussed examples cases missed substitutional deﬁnition section table shows agreement manual labels relational deﬁnition substitutional deﬁnition. supports hypothesis substitutability objective. agreement close levels reported zhitomirsky-geﬀet dagan hand number pairs labeled entails drops relational deﬁnition substitional deﬁnition. supports hypothesis substitutability excludes many cases entailment. relational deﬁnition yields approximately twice number lexical entailments captured substitutional deﬁnition. expected automated labeling using semeval corresponds closely manual labeling relational deﬁnition manual labeling substitional deﬁnition conﬁrms construction dataset accordance intention relational deﬁnition. experiments split dataset three equal parts development sets test splits random except balance class sizes maintained three subsets. contain pairs test contains pairs. category class-inclusion hence pairs classinclusion table pairs hand tables subcategories category class-inclusion surprising pairs class-inclusion table pairs number pairs labeled entails number labeled entail balapinc measure parameters tune maxf maximum number features threshold classiﬁcation. calculated balapinc times using diﬀerent values maxf given value maxf value optimized f-measure dev. gave pairs values maxf tested settings chose setting maximized f-measure maxf balapinc measure robust respect parameter settings. accuracy ranged maxf maxf kept best maxf setting tuned union dev. parameter settings applied balapinc test set. tried tried values increments ﬁfty pairs values weka using training data testing data. maximum f-measure achieved convecs robust respect parameter settings. accuracy ranged high weka time using union training data test testing data. function space reduced parameters setting tried ﬁfty pairs values convecs. maximum f-measure achieved simdiﬀs robust respect parameter settings. accuracy ranged weka time union training data test testing data. table shows performance three algorithms test set. accuracy convecs signiﬁcantly diﬀerent accuracy simdiﬀs according fisher’s exact test however convecs simdiﬀs accurate balapinc conﬁdence level. performance measures follow general pattern accuracy would usually expect balanced dataset. ﬁnal column table shows conﬁdence interval accuracy calculated using wilson method. table shows accuracies three algorithms vary high-level categories test set. convecs simdiﬀs roughly similar proﬁles balapinc substantially diﬀerent two. would expect given convecs simdiﬀs approach lexical entailment semantic relation classiﬁcation problem whereas balapinc approaches problem designing asymmetric similarity measure. approach balapinc near level relation categories substantially others table explore contribution features performance simdiﬀs. columns value indicates included feature vector indicates excluded diﬀerence domain space respect similarities reference words diﬀerence function space. based diﬀerences spaces whereas based diﬀerences diﬀerent spaces. parameters tuned individually table tuned simdiﬀs table results based test set. diﬀerences accuracies table signiﬁcant accuracy features together signiﬁcantly higher accuracy without help according fisher’s exact test conﬁdence level. supports view working diﬀerent spaces synergetic eﬀect since feature based diﬀerent spaces whereas feature based space. refer matrix turney refer domain function matrices turney section mentioned performed experiments development datasets order select matrices algorithm. based experiments chose matrix balapinc convecs chose table vary matrices evaluate performance test whether development datasets reliable guide choosing matrices. matrices chosen based development datasets bold font. balapinc indeed best matrix. convecs seems might better choice diﬀerence accuracy statistically signiﬁcant. simdiﬀs slightly less accurate diﬀerence signiﬁcant. expected matrices signiﬁcantly better test matrices chosen based development datasets. standard evaluation ten-fold cross-validation folds random. evaluation yields relatively high scores because although every pair kdsz dataset unique many pairs share common term. makes supervised learning easier pair testing fold often share term several pairs training folds. folds. pairs share common term fold. large number pairs shared terms possible construct folds absolutely terms shared folds. therefore gave high priority isolating common shared words single folds allowed less common shared words appear fold. thus pair testing fold rarely share term pairs training folds. standard clustered evaluations examples class class balanced dataset takes clustered evaluation step further ﬁrst clustering folds randomly removing pairs labeled class folds equal number pairs classes. diﬀerent evaluation instead cross-validation algorithms trained jmth dataset tested kdsz dataset kdsz dataset balanced randomly removing pairs labeled class balapinc measure parameters maxf maximum number features threshold classiﬁcation. four experimental setups used setting maxf based tuning experiments jmth dataset used training split four experimental setups. standard clustered balanced setups training split nine folds used training step ten-fold cross-validation. diﬀerent setup training split whole jmth dataset. four setups value optimized f-measure training split. table four experimental setups given order increasing challenge increasing realism. four experimental setups believe diﬀerent evaluation challenging realistic. module part commercial system module inevitably encounter word pairs ﬁeld quite diﬀerent pairs training. diﬀerent evaluation comes closest approximating ﬁeld usage. diﬀerent evaluations balapinc achieves accuracy convecs accuracy simdiﬀs reaches statistically signiﬁcant diﬀerence accuracies according fisher’s exact test conﬁdence level. cause convecs simdiﬀs supervised learning thus beneﬁt standard setup training data highly similar testing data. balapinc training data used tune threshold limits beneﬁt training data. note standard performance diﬀerent performance simply question quantity data. diﬀerent setup qualitative diﬀerence training data testing data. increasing size training dataset data type helpful. goal diﬀerent setup test ability algorithms bridge qualitative training testing data. qualitative challenging supervised learning quantitative gap. learning algorithms inevitably face real applications kdsz dataset used previous research past results comparable results. kotlerman reported without trade-oﬀ kotlerman attempt evaluate balapinc classiﬁer report precision recall f-measure accuracy. experimented three diﬀerent ways splitting dataset. table evaluations follow setups table however balanced setup since bbds dataset already balanced. diﬀerent evaluation algorithms trained jmth dataset evaluated bbds. realistic evaluation setup. table diﬀerent evaluations balapinc achieves accuracy convecs accuracy simdiﬀs reaches accuracies signiﬁcantly diﬀerent according fisher’s exact test conﬁdence level. bbds data used baroni compare balapinc convecs. used diﬀerent evaluation setups similar standard diﬀerent setups. balapinc using standard setup obtained accuracy slighly result diﬀerence likely minor diﬀerences word–context matrices used. balapinc using diﬀerent setup accuracy compared used independent dataset tune balapinc whereas used jmth dataset. given word–context matrices training data diﬀerent theirs accuracies closer might expected. convecs using standard setup baroni report accuracy whereas achived using diﬀerent setup obtained whereas accuracy seems likely training data less similar bbds dataset independent dataset made diﬀerent setup challenging theirs. nonetheless accuracies closer might expected given diﬀerences setups. table summarizes accuracy results experiments. kdsz bbds experiments diﬀerent evaluation shown. bold font used mark cases accuracy signiﬁcantly less accuracy simdiﬀs. case accuracy signiﬁcantly greater accuracy simdiﬀs. jmth dataset based seventy-nine types semantic relations. pairs dataset labeled accordance relational deﬁnition lexical entailment explains balapinc designed substitutional deﬁnition mind performs poorly jmth dataset. convecs simdiﬀs designed semantic relation classiﬁcation surprising perform much better balapinc. kdsz dataset labeled using substitutional deﬁnition lexical entailment dataset statistically signiﬁcant diﬀerence algorithms. ideal dataset balapinc dataset designed natural balapinc highest accuracy. hand learning algorithms handle dataset well although trained jmth dataset quite diﬀerent kdsz dataset. good able cope qualitative diﬀerence training data testing data. positive pairs bbds dataset instances hyponym– hypernym semantic relation. instances relation substitutable balapinc designed handle them. convecs also designed speciﬁcally dataset table convecs reaches accuracy training data similar testing data. however convecs trouble bridging qualitative training data testing data diﬀerent setup. hand simdiﬀs able bridge gap. class entails also scarce natural settings. therefore table presents alternative summary results. table reports instead accuracy; puts emphasis entails class. kdsz bbds datasets report clustered setup. closer evaluation setup kotlerman table bold font mark signiﬁcant diﬀerences agreement appropriate statistical test although tables based diﬀerent scores experimental setups support simdiﬀs similarity diﬀerences hypothesis. generally suggest second-order features useful modeling lexical entailment. also suggest beneﬁcial diﬀerent spaces constructing features lexical entailment. manually designing asymmetric similarity measure diﬃcult task equations section believe lexical entailment tractable approached supervised learning problem. eﬀort involved manually designing feature vectors less required designing similarity measures. performance simdiﬀs indicates supervised learning yield better results manually designing measures. evaluated directly applications would module inside larger system. future work needed demonstrate results direct evaluation predict module perform component larger system. although simdiﬀs performs better competition much room improved performance. however simdiﬀs used component larger system words given contexts sentences. support contextual information help modules system simdiﬀs might yield substantial improvements performance. related proposed future work shnarch barak dagan evaluated lexical reference rules derived wikipedia rte- dataset. used component system rules improved rte- score past work based context inclusion hypothesis convecs simdiﬀs show approaches based novel hypotheses achieve competitive results. believe progress problem come recall semantic relation subcategories hypothesis lexical entailment superset high-level categories semantic relations superset lowerlevel subcategories semantic relations. experiments lend support hypothesis research needed. counterexamples hypothesis could handled revising taxonomy. however required revisions become onerous hypothesis rejected. three algorithms based three diﬀerent hypotheses three achieve degree success task rle. suggests would fruitful combine three approaches. simple combine would average real-valued outputs apply voting binary-valued outputs. could useful direction future research. looking tables section high density class-inclusion part-whole. strong connection categories lexical entailment explain morris hirst call hypernymy meronymy classical relations whereas relation chapelfuneral non-classical instance wordnet contains information hypernymy meronymy space-time relations. particular relations might considered classical particularly useful making inferences. connection another topic future work. paper evaluated three diﬀerent algorithms three different datasets. algorithm relies diﬀerent hypothesis lexical entailment. simdiﬀs best performance three datasets. third dataset signiﬁcant diﬀerence three algorithms. performance simdiﬀs suggests similarity diﬀerences make useful features learning recognize lexical entailment. approached lexical entailment supervised learning problem semantic relation classiﬁcation. results indicate promising approach lexical entailment. builds bridge research lexical entailment research semantic relation classiﬁcation. hope connection strengthen research ﬁelds. thanks lili kotlerman dagan idan szpektor maayan zhitomirskygeﬀet providing copy kdsz dataset answering questions. thanks marco baroni raﬀaella bernardi ngoc-quynh chung-chieh", "year": 2014}