{"title": "Predicting Alzheimer's disease: a neuroimaging study with 3D  convolutional neural networks", "tag": ["cs.CV", "cs.LG", "stat.AP", "stat.ML"], "abstract": "Pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease have been the subject of extensive research in recent years. In this paper, we use deep learning methods, and in particular sparse autoencoders and 3D convolutional neural networks, to build an algorithm that can predict the disease status of a patient, based on an MRI scan of the brain. We report on experiments using the ADNI data set involving 2,265 historical scans. We demonstrate that 3D convolutional neural networks outperform several other classifiers reported in the literature and produce state-of-art results.", "text": "pattern recognition methods using neuroimaging data diagnosis alzheimer’s disease subject extensive research recent years. paper deep learning methods particular sparse autoencoders convolutional neural networks build algorithm predict disease status patient based scan brain. report experiments using adni data involving historical scans. demonstrate convolutional neural networks outperform several classiﬁers reported literature produce state-of-art results. alzheimer’s disease common type dementia. dementia refers diseases characterized loss memory cognitive impairments caused damage nerve cells brain. united states estimated million people ages mild cognitive impairment condition individual mild noticeable changes thinking abilities. individuals likely develop inviduals without early detection disease achieved magnetic resonance imaging technique uses magnetic ﬁeld radio waves create detailed image brain. multitude machine learning methods tried task recent years including support vector machines independent component analysis penalized regression. methods shown effective diagnosing neuroimages sometimes even effective human radiologists. instance recent studies shown machine learning algorithms able predict accurately experienced clinicians therefore great interest develop improve prediction methods. paper build learning algorithm that using images input able discriminate healthy brains diseased brains. investigate class deep artiﬁcial neural networks speciﬁcally combination sparse autoencoders convolutional neural networks. main novelty approach convolutions whole image yield better performance convolutions slices experiments. report classiﬁcation results obtained using -way classiﬁer three binary classiﬁers ∗corresponding author giovanni.montanakcl.ac.uk *data used preparation article obtained alzheimer’s disease neuroimaging initiative database such investigators within adni contributed design implementation adni and/or provided data participate analysis writing report. complete listing adni investigators found http//adni.loni.usc.edu/wp-content/uploads/how apply/ adni acknowledgement list.pdf article organised follows. describe data section introduce deep learning approach section section offers brief review different classiﬁcation methods reported literature problem. finally section provide experimental results discussion. experiments data made available part alzheimer’s disease neuroimaging initiative adni ongoing multicenter study designed develop clinical imaging genetic biochemical biomarkers early detection tracking alzheimer’s disease. adni study began third phase. dataset used originally prepared analysed consists patients three classes total scans. statistical parametric mapping used normalize image data international consortium brain mapping template. conﬁguration includes positron density template weighting image th-order b-spline interpolation whilst remaining parameters default. also normalised data subtracting mean dividing standard deviation. dimension image results voxels. figure shows example three two-dimensional slices extracted scan. take two-stage approach whereby initially sparse autoencoder learn ﬁlters convolution operations build convolutional neural network whose ﬁrst layer uses ﬁlters learned autoencoder. paper particularly interested comparing performance convolutional networks. section present architecture sparse autoencoder section describe convolutional network. network detailed section autoencoder -layer neural network used extract features input image autoencoder input layer hidden layer output layer; layer contains several units. input output layers number units. network encoder function maps given input hidden representation decoder function maps representation output problem inputs patches extracted scans. purpose decoder function reconstruct input hidden representation ℜp×n matrix weights vector biases encoder function respectively; analogously ℜn×p weights biases decoder sigmoid function identity function. identity function decoder inputs real-valued whereas sigmoid function would constrain output units interval reconstruction would difﬁcult. moreover impose tied weights real-valued inputs pixel intensities appropriate choice cost function mean squared error i.e. total number inputs. autoencoder used obtain representation input data hidden layer. decided using autoencoder overcomplete hidden layer i.e. autoencoder equal larger number hidden units input units. autoencoders overcomplete hidden layers useful feature extractors. potential issue overcomplete autoencoders minimize reconstruction error hidden layer potentially learn identity function therefore need impose additional constraints. experiments autoencoders sparsity constraints investigate whether sparse autoencoder i.e. autoencoder obtained enforcing hidden units close zero used learn useful ﬁlters convolution operations. sparsity constraint expected advantageous context encourages representations disentangle underlying factors controlling variability images. mean activation hidden unit averaged training set. impose constraint sparsity hyper-parameter typically small value close zero satisfy constraint penalty term cost function. choose penalty based concept kullback-leibler divergence quantiﬁes divergence bernoulli distribution mean mean number units hidden layer index summing hidden units. penalty property otherwise increases moves away thus minimizing term effect causing close cost function deﬁned previously hyper-parameter controls weight penalty term. also added third term cost function called weight decay used reduce overﬁtting; hyper-parameter controls amount weight decay. approach train autoencoder randomly selected patches size extracted scans. purpose autoencoder training learn ﬁlters convolution operations explain section convolution covers series spatially localised regions input. total extract patches scans training total patches. train sparse overcomplete autoencoder hidden units patches. patches training patches validation patches test set. patch unrolled vector size cost function minimised using gradient descent mini batches training divided several mini batches iteration mini batches function minimize; algorithm expected converge faster full batches. also deﬁne basis weights linking unit hidden layer units input layer. basis extract spatially localised features input. bases used next section convolutional neural networks. training sparse autoencoder build convolutional network takes input scan. convolutional networks found useful image classiﬁcation problems several domains handwritten digit recognition object recognition ﬁrst describe idea local connectivity. hidden layer unit connected units previous layer small number units spatially localised region. property beneﬁcial number ways. hand reduces number parameters thus making architecture less prone overﬁtting whilst also alleviating memory computational issues. hand modelling portions image hidden units able detect local patterns features important discrimination. part image hidden unit connected referred receptive ﬁeld. possible receptive ﬁeld ﬁxed size associated speciﬁc hidden unit. hidden units corresponds single feature hidden units altogether cover whole image. hidden layer several feature maps hidden units within feature share parameters. parameter sharing feature useful reduces number parameters hidden units within feature extract features every position input. -dimensional array feature hidden layer -dimensional array input. also -dimensional ﬁlter connecting input feature scalar bias term feature map. computation feature given sigmoid activation function denotes convolution operation. computation scalar term added every entry array assume size deﬁne convolution input ﬁlter layer made several feature maps obtained called convolutional layer. every basis sparse autoencoder trained previously learned weights basis ﬁlter convolutional layer. applying convolutions bases obtain convolutional layer feature maps. since patches size convolution image basis produces feature size also bias term associated basis apply sigmoid activation function every unit feature map. convolutional layer likely discover local patterns structures input image allows algorithm exploit topology/spatial information image. convolutional layers followed pooling layers. max-pooling consists segmenting feature several non-overlapping adjacent neighbourhoods hidden units. within every neighbourhood hidden unit largest activation retained. pooling operation reduces number units hidden layer useful noted above. pooling also builds robustness small distortions image translations. approach apply max-pooling operation reduce size feature maps convolutional layer. feature therefore becomes max-pooled feature size round nearest integer ignore borders. outputs every max-pooled feature stacked. feature maps size total outputs. outputs used inputs -layer fully-connected neural network choose hidden layer units sigmoid activation function output layer units softmax activation function. units output layer represent conditional probabilities input belongs classes figure provides illustration network architecture. number scans summing classes function computed network input label scan respectively. weight decay early experiments addition term found beneﬁcial. -layer network trained mini batch gradient descent. weights hidden layer randomly initialised weights softmax layer initialised zero. important note include convolutional layer ﬁnal training; convolutional layer pre-trained autoencoder. also momentum method speed training -layer network. brieﬂy method consists adding weighted average past gradients gradient descent updates remove noise particularly directions high curvature cost function experiments also test approach using convolutions comparative purposes. initial hypothesis approach would provide boost performance compared traditional convolutions capturing local patterns structures image useful discrimination. approach consists training sparse autoencoder patches extracted slices scans. case extract patches size hidden units autoencoder exactly approach. apply convolutions slices scan order obtain feature since slice size convolution slice autoencoder basis gives output size previous architecture also bias term sigmoid activation function. max-pooling operation case consists patches reduces size slice outputs max-pooled feature maps stacked. since slices feature obtain total outputs comparable size previous architecture. deliberately choose similar sizes outputs approaches order make comparison accurate possible. outputs used inputs -layer fully connected network. hyper-parameters kept before. several pattern classiﬁers tried discrimination subjects using structural modalities among many approaches support vector machines used extensively area. table contains selection related studies along sample sizes reported performance. although direct comparison studies difﬁcult study uses different datasets preprocessing protocols tables gives indication typical accuracy measures achieved classiﬁcation images. used svms linear kernels classiﬁcation grey matter signatures benchmarked results performance achieved expert radiologists surprisingly found less accurate algorithm. another approach used independent component analysis feature extractor coupled algorithm describe approach combines penalised regression data resampling feature extraction prior classiﬁcation using svms gaussian kernels. report best performance achieved using classiﬁer bagging method logistic regression model boosting algorithm extract highly discriminative patches classiﬁed using graph kernels; using methods t-tests sparse coding patch assigned probability quantiﬁes discriminative ability. recently deep learning methods also explored data classiﬁcation. autoencoder ﬁrst used learn features patches extracted either scans natural images. parameters autoencoder used ﬁlters convolutional layer. study classiﬁcation achieved using -layer neural network softmax function. difference network architecture convolutions. report deep fully-connected network pre-trained stacked autoencoders ﬁne-tuned; however approach convolution operations contrary ours. convolutional neural networks using convolutions trained used training examples. validation examples used determine early-stopping time explained above. finally test examples used evaluate performance model unseen examples compute performance ﬁgures reported below. table review selected methods classiﬁcation. provide sample size accuracy every method. accuracy refers proportion correct predictions test set. alzheimer’s disease; mild cognitive impairment; healthy control. example means trying classify scans table gives accuracy architectures. expected approach superior performance -way comparison well comparisons. comparison noticeable differences. results compare favourably table reported studies although deﬁnite claims made superiority approach differences datasets sample sizes preprocessing steps. interpretation results made difﬁcult nature deep neural network architectures. figure shows convolutions scans classes fourth basis sparse autoencoder. table results models test set. accuracy refers proportion correct predictions. -way means trying classify scan three lines refer binary classiﬁcations. second column corresponds results convolutions third column corresponds results convolutions. paper designed tested pattern classiﬁcation system combines sparse autoencoders convolutional neural networks. primarily interested assessing accuracy approach relatively large patient population also wanted compare performance convolutions convolutional neural network architecture. experiments indicate approach potential capture local patterns boost classiﬁcation performance albeit small margin. investigations could improved future studies carrying exhaustive searches optimal hyper-parameters architectures. moreover overall performance systems could improved. instance convolutional layer used experiments pre-trained autoencoder ﬁne-tuned. evidence ﬁnetuning improve performance cost much increased computational complexity training stage.", "year": 2015}