{"title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.", "text": "understanding language goes hand hand ability integrate complex contextual information obtained perception. work present novel task grounded language understanding disambiguating sentence given visual scene depicts possible interpretations sentence. introduce multimodal corpus containing ambiguous sentences representing wide range syntactic semantic discourse ambiguities coupled videos visualize different interpretations sentence. address task extending vision model determines sentence depicted video. demonstrate model adjusted recognize different interpretations underlying sentence allowing disambiguate sentences uniﬁed fashion across different ambiguity types. ambiguity deﬁning characteristics human languages language understanding crucially relies ability obtain unambiguous representations linguistic content. ambiguities resolved using intra-linguistic contextual cues disambiguation many linguistic constructions requires integration world knowledge perceptual information obtained modalities. work focus problem grounding language visual modality introduce novel task language understanding requires resolving linguistic ambiguities utilizing visual context linguistic content expressed. type inference frequently called human communication occurs visual environment crucial language acquisition much linguistic content refers visual surroundings child task also fundamental problem grounding vision language focusing phenomena linguistic ambiguity prevalent language typically overlooked using language medium expressing understanding visual content. ambiguities superﬁcially appropriate description visual scene fact sufﬁcient demonstrating correct understanding relevant visual content. task addresses issue introducing deep validation protocol visual understanding requiring providing surface description visual activity also demonstrating structural understanding levels syntax semantics discourse. enable systematic study visually grounded processing ambiguous language create corpus lava corpus contains sentences linguistic ambiguities resolved using external information. sentences paired short videos visualize different interpretations sentence. sentences encompass wide range syntactic semantic discourse ambiguities including ambiguous prepositional verb phrase attachments conjunctions logical forms anaphora ellipsis. overall corpus contains sentences interpretations sentence average videos depict visual variations sentence interpretation corresponding total videos. using corpus address problem selecting interpretation ambiguous sentence matches content given video. approach tackling task extends sentence tracker introduced sentence tracker produces score determines sentence depicted video. earlier work concept ambiguities; assumed every sentence single interpretation. extend approach represent multiple interpretations sentence enabling pick interpretation compatible video. summarize contributions paper threefold. first introduce task visually grounded language understanding ambiguous sentence disambiguated using visual depiction sentence’s content. second release multimodal corpus sentences coupled videos covers wide range linguistic ambiguities enables systematic study linguistic ambiguities visual contexts. finally present computational model disambiguates sentences corpus accuracy previous language vision studies focused development multimodal word sentence representations well methods describing images videos natural language studies handle important challenges multimodal processing language vision provide explicit modeling linguistic ambiguities. sense disambiguation however work limited context independent interpretation individual words consider structure-related ambiguities. discourse ambiguities previously studied work multimodal coreference resolution work expands line research addresses discourse ambiguities interpretation ellipsis. importantly best knowledge study ﬁrst present systematic treatment syntactic semantic sentence level ambiguities context language vision. interactions linguistic visual information human sentence processing extensively studied psycholinguistics cognitive psychology considerable fraction work focused processing ambiguous language providing evidence importance visual information linguistic ambiguity resolution humans. information also vital language acquisition much linguistic content perceived child refers immediate visual environment time children develop mechanisms grounded disambiguation language manifested among others usage iconic gestures communicating ambiguous linguistic content study leverages insights develop complementary framework enables addressing challenge visually grounded disambiguation language realm artiﬁcial intelligence. work provide concrete framework study language understanding visual context introducing task grounded language disambiguation. task requires choose correct linguistic representation sentence given visual context depicted video. speciﬁcally provided sentence candidate interpretations sentence video depicts content sentence needs choose interpretation corresponds content video. figure example visually grounded language disambiguation task. given sentence approached chair potential parses correspond different semantic interpretations. ﬁrst interpretation second reading chair. task select correct interpretation given visual context terpretation corresponds parse bag. second interpretation associated parse chair rather sam. given visual context ﬁgure task choose interpretation appropriate sentence. address grounded language disambiguation task compositional approach determining speciﬁc interpretation sentence depicted video. framework described detail section sentence accompanying interpretation encoded ﬁrst order logic give rise grounded model matches video provided sentence interpretation. model comprised hidden markov models encode semantics words trackers locate objects video frames. represent interpretation sentence word models combined trackers cross-product respects semantic representation sentence create single model recognizes interpretation. figure linguistic visual interpretations sentence bill held green chair bag. ﬁrst interpretation chair green second interpretation chair green different color. forestation sentence candidate object locations combined form model determine given interpretation depicted video. test interpretation report interpretation highest likelihood. enable systematic study linguistic ambiguities grounded vision compiled corpus ambiguous sentences describing visual actions. sentences formulated correct linguistic interpretation sentence determined using external non-linguistic information depicted activity. example sentence bill held green chair correct scope green determined integrating additional information color bag. information provided accompanying videos visualize possible interpretations sentence. figure presents syntactic parses example along frames respective videos. although videos contain visual uncertainty ambiguous respect linguistic interpretation presenting hence video always corresponds single candidate representation sentence. corpus covers wide range well table templates generating sentences corpus. rightmost column represents number sentences category. sentences produced replacing tags visually applicable assignments lexical items corpus lexicon shown table sentences candidate interpretations sentences interpretations. table presents corpus templates ambiguity class along number sentences generated template. corpus videos ﬁlmed indoor environment containing background objects pedestrians. account manner performing actions videos shot twice different actors. whenever applicable also ﬁlmed actions different directions finally videos shot cameras different view points. taking variations account resulting video corpus contains videos sentence videos sentence interpretation corresponding total videos. average video length seconds overall hours footage custom corpus required task because existing corpus containing either videos images systematically covers multimodal ambiguities. datasets sports youtube hmdb come activity recognition community accompanied action labels sentences control content videos aside principal action performed. datasets image video captioning mscoco tacos known syntactic semantic discourse ambiguity classes. ambiguities associated various types different sentence interpretations always represent distinct sentence meanings hence encoded semantically using ﬁrst order logic. syntactic discourse ambiguities also provide additional ambiguity type speciﬁc encoding described below. syntax syntactic ambiguities include prepositional phrase attachments verb phrase attachments ambiguities interpretation conjunctions. addition logical forms sentences syntactic ambiguities also accompanied context free grammar parses candidate interpretations generated deterministic parser. semantics corpus addresses several classes semantic quantiﬁcation ambiguities syntactically unambiguous sentence correspond different logical forms. sentence provide respective logical forms. discourse corpus contains types discourse ambiguities pronoun anaphora ellipsis offering examples comprising sentences. anaphora ambiguity cases ambiguous pronoun second sentence given candidate antecedents ﬁrst sentence well corresponding logical form meaning second sentence. ellipsis cases part second sentence constitute either subject verb verb object omitted. provide interpretations omission form single unambiguous sentence logical form combines meanings ﬁrst second sentences. corpus generated using part speech sequence templates. template tags replaced lexical items corpus lexicon described table using visually applicable assignments. generation process yields overall sentences table overview different ambiguity types along examples ambiguous sentences linguistic visual interpretations. note similarly semantic ambiguities syntactic discourse ambiguities also provided ﬁrst order logic formulas resulting sentence interpretations. table shows additional examples ambiguity type frames sample videos corresponding different interpretations sentence. control aspects videos main action performed provide range ambiguities discussed here. closest dataset siddharth controls object appearance color action direction motion making likely suitable evaluating disambiguation tasks. unfortunately dataset designed avoid ambiguities therefore suitable evaluating work described here. perform disambiguation task extend sentence recognition model siddharth represents sentences compositions words. given sentence ﬁrst order logic interpretation video model produces score determines sentence depicted video. simultaneously tracks participants events described sentence recognizing events themselves. allows ﬂexible presence noise integrating top-down information sentence bottom-up information object property detectors. word query sentence represented recognizes tracks satisfy semantics given word. essence model described layers object tracking occurs words observe tracks ﬁlter tracks satisfy word constraints. given sentence interpretation construct sentence-speciﬁc model recognizes video depicts sentence follows. predicate ﬁrst order logic formula corresponding recognize predicate true video given arguments. variable corresponding tracker attempts physically locate bounding corresponding variable frame table examples ambiguity classes described table example sentences least interpretations depicted different videos. three frames video shown left right sentence. figure tracker lattices every sentence participant combined predicate hmms. estimate resulting cross-product lattice simultaneously ﬁnds best tracks best state sequences every predicate. interpretations sentence claire bill moved chair different ﬁrst order logic formulas. interpretation corresponds bill claire moving chair bottom describes moving different chairs. predicates highlighted blue variables highlighted bottom. predicate corresponding recognizes presence video. variable corresponding tracker locates video. lines connect predicates variables argument slots. predicates move take multiple arguments. predicates move applied multiple times different pairs variables. scored probability observing particular detection given state probability transitioning states structure formula fact multiple predicates often refer variables recorded mapping predicates arguments. model computes estimate sentences words refer tracks trivially extended arbitrary arities. figure provides visual overview model cross-product tracker models word models. model extends approach siddharth several ways. first depart dependency based representation used work recast model encode ﬁrst order logic formulas. note complex ﬁrst order logic formulas cannot directly encoded model require additional inference steps. extension enables represent ambiguities given sentence multiple logical interpretations syntactic parse. video. creates bipartite graph hmms represent predicates connected trackers represent variables. trackers similar hmms comprise lattice potential bounding boxes every frame. construct joint model sentence interpretation take cross product hmms trackers taking cross products dictated structure formula corresponding desired interpretation. given video employ object detector generate candidate detections frame construct trackers select detections frame ﬁnally construct overall model hmms trackers. provided interpretation corresponding formula composed predicates variables along collection object detections bframe detection index frame video length model computes score videosentence pair ﬁnding optimal detection participant every frame. essence viterbi algorithm algorithm hmms applied ﬁnding optimal object detections jframe variable participant optimal state kframe predicate predicate every frame. detection scored conﬁdence object detector object track scored motion coherence metric determines motion track agrees underlying optical ﬂow. predicate second introduce several model components speciﬁc disambiguation required encode linguistic constructions present corpus could handled model siddharth components predicate equal disjunction conjunction. addition among components support predicate equal enforces tracks i.e. objects distinct other. example sentence claire bill moved chair would want ensure movers distinct entities. earlier work required sentences tested work designed distinguish objects based constraints rather identity. words might different people distinguished sentence actions appearance. faithfully recognize actors moving chair earlier example must ensure disjoint other. order create predicate assigns probability tracks heavily overlap forcing model different actors previous example. combining ﬁrst order logic based semantic representation lieu syntactic representation expressive model encode sentence interpretations required perform disambiguation task. figure shows example different interpretations discussed sentence claire bill moved chair. object trackers correspond variables ﬁrst order logic representation sentence interpretation shown red. predicates constrain possible bindings trackers corresponding predicates representation sentence shown blue. links represent argument structure ﬁrst order logic formula determine cross products taken predicate hmms tracker lattices order form joint model recognizes entire interpretation video. resulting model provides single uniﬁed formalism representing ambiguities table moreover approach tuned different levels speciﬁcity. create models speciﬁc interpretation sentence generic accept multiple interpretations eliding constraints common different interpretations. allows model like humans defer deciding particular interpretation infer multiple interpretation sentence plausible. tested performance model described previous section lava dataset presented section video dataset pre-processed object detectors humans bags chairs telescopes. employed mixture detectors trained held sections corpus. object class generated proposals detectors trained scoring function results space. scoring function consisted sigmoid conﬁdence detectors trained held portion training set. none disambiguation examples discussed rely speciﬁc identity actors detect identity. instead sentence contains names automatically converted contains arbitrary person labels. sentences corpus either three interpretations. interpretation associated videos scene shot different angle carried either different actors different objects different directions motion. sentence-video pair performed -out-of- -out-of- classiﬁcation task determine interpretations corresponding sentence best video. overall chance performance dataset slightly lower out-of- classiﬁcation examples. model presented achieved accuracy entire corpus averaged across error categories. demonstrates model largely capable capturing underlying task similar compositional crossmodal models same. major ambiguity classes accuracy syntactic ambiguities semantic ambiguities discourse ambiguities. signiﬁcant source model failures poor object detections. objects often rotated presented angles difﬁcult recognize. certain object classes like telescope investigation focuses structural inference intend extend line work learning scenarios agent deduce meaning words sentences structurally ambiguous input. furthermore framework beneﬁcial image video retrieval applications query expressed natural language. given ambiguous query approach enable matching clustering retrieved results according different query interpretations. much difﬁcult recognize small size fact hands tend largely occlude them. accounts degraded performance semantic ambiguities relative syntactic ambiguities many semantic ambiguities involved telescope. object detector performance similarly responsible lower performance discourse ambiguities relied much accuracy person detector many sentences involve people interacting without additional objects. degrades performance removing helpful constraint inference according people tend close objects manipulating. addition sentences introduced visual uncertainty often involved three actors. remaining errors event models. hmms ﬁxate short sequences events seem part action fact noise preﬁx another action. ideally would want event model global view action object went beginning video person holding it’s likely object picked event models used cannot enforce constraint merely assert object moving number frames; event happen noise object detectors. enforcing local constraints instead global constraint motion object video makes joint tracking event recognition tractable framework presented lead errors. finding models strike better balance local information global constraints maintaining tractable inference remains area future work. present novel framework studying ambiguous utterances expressed visual context. particular formulate task resolving structural ambiguities using visual signal. fundamental task humans involving complex cognitive processing challenge language acquisition childhood. release multimodal corpus enables address task well support investigation ambiguity related phenomena visually grounded language processing. finally", "year": 2016}