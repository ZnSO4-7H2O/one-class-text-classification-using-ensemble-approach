{"title": "Discrete and fuzzy dynamical genetic programming in the XCSF learning  classifier system", "tag": ["cs.AI", "cs.LG", "cs.NE", "cs.SY", "math.OC"], "abstract": "A number of representation schemes have been presented for use within learning classifier systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the XCSF learning classifier system. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous fuzzy logic networks in the continuous-valued case. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such dynamical systems within XCSF to solve a number of well-known test problems.", "text": "abstract number representation schemes presented within learning classiﬁer systems ranging binary encodings neural networks. paper presents results investigation using discrete fuzzy dynamical system representations within xcsf learning classiﬁer system. particular asynchronous random boolean networks used represent traditional condition-action production system rules discrete case asynchronous fuzzy logic networks continuous-valued case. shown possible self-adaptive openended evolution design ensemble dynamical systems within xcsf solve number wellknown test problems. traditionally learning classiﬁer systems ternary encoding generalize environmental inputs associate appropriate actions. number representations previously presented beyond scheme however including real numbers fuzzy logic artiﬁcial neural networks temporally dynamic representation schemes within represent potentially important approach since temporal behaviour kinds viewed signiﬁcant richard preen larry bull department computer science creative technologies university west england bristol e-mail richard.preenuwe.ac.uk larry.bulluwe.ac.uk paper explore examples dynamical system representation within xcsf learning classiﬁer system —termed dynamical genetic programming traditional tree-based genetic programming used within calculate action represent condition uses graph-based representation node constantly updated asynchronous parallelism evolved using openended self-adaptive scheme. discrete case node boolean function therefore representation form random boolean network continuous case node performs fuzzy logical function representation form fuzzy logic network recently introduced within extend work recent form continuous-valued domain exploring potential harness collective emergent behaviour computation. show xcsf able solve number well-known immediate delayed reward tasks using temporally dynamic knowledge representation scheme competitive performance representations. moreover exploit memory inherent discrete case. remainder paper organised follows. section provides brief discussion related work. section introduces giving detailed description form temporal behaviour. section describes xcsf learning framework. section details asynchronous evolved used computation within xcsf. section discusses presents results maze experiments requiring exploitation inherent memory within discrete dynamical ensemble. section introduces continuous-valued extension discusses dynamical behaviour. section describes used continuous production system rules within xcsf. section presents results experimentation continuous-input discrete-action maze environment fully continuous reinforcement learning problem. section provides ﬁnal conclusions. common form discrete dynamical system cellular automaton consists array cells cells exist states ﬁnite update states synchronous parallelism discrete time. extensively used model genetic regulatory networks however continuous network models exist extension boolean networks nodes still represent genes connections regulate inﬂuence gene expression. indeed growing body work exploring evolution diﬀerent forms continuous-valued grn. diﬀerential equations wherein gene interactions incorporated logical functions typical approach reiter investigated aﬀect fuzzy background dynamics various fuzzy logic sets found choice logic used leads signiﬁcantly diﬀerent behaviours. example applying various logical functions create fuzzy versions game life noted certain sets logics generated fuzzy-ca tended toward homogeneous fuzzy behaviour whereas others consistent chaotic complex behaviour. relevant form used herein relatively small amount prior work graphbased representations. particular neural programming uses directed graph connected nodes performing arbitrary function. potentially selectable functions include read write if-then-else along standard arithmetic zero-arity functions. additionally complex user deﬁned functions used. signiﬁcantly recursive connections permitted node executed synchronous parallelism number cycles output node’s value taken. linear-graph graph structured program evolution schmidt lipson recently demonstrated number beneﬁts graph encodings traditional trees reduced bloat increased computational eﬃciency signiﬁcant beneﬁt symbolic representations expressive power represent relationships sensory inputs landau used purely evolution-based form rules represented directed graphs genotypes tokens stack-based language whose execution builds labeled graph. bit-strings used represent language tokens applied nonmarkov problems. genotype translated sequence tokens interpreted similarly program stack-based language instructions create graph’s nodes connections labels. subsequently unused conditions actions stack added structure popped stack. tokens used specify matching conditions executable actions well instructions construct graph manipulate stack. bit-strings later replaced integer tokens applied non-markov problems bull o’hara presented form fuzzy representation within using radial basis function neural networks embody condition-action rule. simple class neural-fuzzy hybrid system. furthermore explored similar representation based within lcs. however contribution rule determined strength well extent antecedent matches environment. furthermore contrast bull o’hara condition-action rule corresponds hidden node instead fully-connected network rules added incrementally instead evolved date explored neuro-fuzzy hybrid representation within lcs. originally introduced kauﬀman explore aspects biological genetic regulatory networks. since used tool wide range areas self-organisation computation robotics typically consists network nodes performing boolean function inputs nodes network updating synchronously such viewed generalization binary unorganized machines since ﬁnite number possible states deterministic dynamics eventually fall basin attraction. wellestablished value aﬀects emergent behaviour wherein attractors typically contain increasing number states increasing three phases behaviour suggested ordered attractors consisting states; chaotic large number states attractor; critical regime around similar states trajectories tend neither diverge converge nodes change state attractor cycle analytical methods presented determine typical time taken reach basin attraction number states within basins given degree connectivity closely akin work described here kauﬀman describes simulated evolution design must play matching game wherein mutation used change connectivity boolean functions reports typical emergence high ﬁtness solutions together increase initialised size. sipper ruppin extended sipper’s heterogeneous approach enable heterogeneity node connectivity along node function; evolved form rbn. broeck kawai explored simulated annealing-type approach design feedforward four-bit parity problem lemke evolved ﬁxed match arbitrary attractor. figure shows aﬀect node rbn; results average runs value seen higher value greater number states networks cycle through shown higher rate change node states. further initial rapid decline rate change value stabilises states fall respective attractors. synchronous case number nodes changing state converges around thus ordered regime occurs approximately less nodes changing state cycle chaotic regime occurring larger rates change. noted above traditional consist nodes updating synchronously discrete time steps asynchronous versions also presented harvey bossomaier leading classiﬁcation space possible forms asynchronous forms also explored wherein often suggested asynchrony realistic underlying assumption many natural artiﬁcial systems since discrete time synchronously updating networks certainly biologically defensible development interactions regulatory elements occur lock-step fashion asynchronous logic devices known potential consume less power dissipate less heat exploitable eﬀorts towards hardware implementations systems. asynchronous logic also known potential improved fault tolerance particularly delay insensitive schemes also prove beneﬁcial hardware implementations. harvey bossomaier showed asynchronous exhibit either point attractors seen asynchronous loose attractors network passes indeﬁnitely subset possible states thus asynchrony represents another feature potential signiﬁcantly alter underlying dynamics thereby oﬀering another mechanism simulated evolutionary design process given task. paulo showed wheel random selection) exploiting multistep problems biased selection strategy often employed wherein exploration conducted probability pexplr otherwise exploitation occurs action built composed classiﬁers advocating selected action. next action executed environment feedback received form payoﬀ single-step problem updated using current reward. average time since last invocation greater threshold value θga. parent classiﬁers chosen based ﬁtness. oﬀspring produced parents usually recombination mutation. typically oﬀspring payoﬀ error ﬁtness average parents’. subsumption enabled oﬀspring subpossible evolve asynchronous exhibit rhythmic behaviour equilibrium. asynchronous also evolved figure shows percentage nodes changing state cycle various values node asynchronous rbn. seen that similar synchronous case higher value greater number states networks cycle attractor. values signiﬁcantly lower synchronous case however. example approximately nodes change synchronously updated cycle compared updated asynchronously. difference expected because asynchronous case lack synchronicity increases complexity enhancing number possible states interactions. complexity changes attractor basins transforming enlarging them. reduces number attractors states attractors previously mentioned asynchronous case cycle attractors point loose attractors. rule traditionally takes form environment string consisting ternary alphabet binary action string subsequent information including classiﬁer’s expected payoﬀ error rate ﬁtness symbol environment condition provides mechanism generalise inputs received matching logical bit. phase learning cycle match generated population composed classiﬁers whose environment condition matches current environmental input. event number actions present less threshold value θmna covering used produce classiﬁer matches current environment state along action assigned randomly present typically θmna maximum number possible actions must least classiﬁer representing action present. subsequently system prediction made action based upon ﬁtness-weighted average predictions classiﬁers proposing action. classiﬁers advocating potential system actions covering invoked generate classiﬁers match current environment state advocate relevant action. action selected using system predictions typically alternating exploring instead parents’ numerosity incremented. multistep problem previous action updated using q-learning type algorithm described opposed single-step problems. sequence loops terminated predetermined number problem instances. xcsf classiﬁer also maintains vector series weights many weights inputs environment plus extra classiﬁer maintains prediction calculated product environmental input classiﬁer weight vector input weights initially zero subsequently adapted accurately reﬂect prediction using modiﬁed rule rule modiﬁed correction step proportional diﬀerence current correct prediction controlled correction rate modiﬁed rule reinforcement update thus enables accurate piecewise-linear approximation payoﬀ opposed piecewise-constant approximation also applied binary problems boolean multiplexer maze environments resulting faster convergence optimality well compact rulebase wilson details. asynchronous rules within xcsf following scheme adopted. initial randomly created rule’s nodes randomly assigned connections initially many nodes input ﬁelds given task outputs plus other described i.e. ﬁrst connection input node corresponding locus input message. connections assigned random within usual. current input state always considered along current state network update cycle nodes. nodes initialised randomly time network determine etc. population initially empty covering applied generate rules standard xcsf approach. matching consists executing rule cycles based current input. value chosen value typically within basin attraction rbn. asynchrony implemented randomly chosen node updated given cycle many updates overall network update cycle nodes network equivalent cycle synchronous case said occurred. gershenson alternative schemes. study well-known maze problems explored eight possible actions accordingly three required output nodes. extra matching node also required enable rbns match speciﬁc sets inputs. given logical match node regardless output node’s state rule join scheme also exploited within neural windowed approach utilised output decided common state last steps example last states node updating prior cycle ending nodes state would covering necessitated randomly constructed created executed cycles determine status match output nodes. procedure repeated created matches environment state. self-adaptive mutation ﬁrst applied within bull rule maintains mutation rate similar approach used evolution strategies mutation rate locally evolving entity itself adapts search process. self-adaptive mutation reduces number hand-tunable parameters evolutionary algorithm also shown improve performance. following bull hurst mutation used here. node’s truth table represented binary string connectivity list integers range since node given ﬁxed value node maintains binary string length forms entries look-up table possible input states node aforementioned work evolving example. strings subjected mutation reproduction self-adapting rate rule. hence within representation evolution deﬁne diﬀerent boolean functions node within given network rule along connectivity map. speciﬁcally rule mutation rate stored real number initially seeded uniform randomly range parameter passed oﬀspring. oﬀspring applies mutation rate using gaussian distribution i.e. mutating rest rule resulting rate. need possible diﬀerent number nodes within rules given task scheme also variable length. truth table connections mutated randomly connected node either added last added node removed probability latter case occurs network currently consists initial number nodes. addition rule maintains value initially seeded randomly thereafter oﬀspring potentially increment decrement probability evolved similar fashion however initially seeded cannot greater thus temporally dynamic search process representation scheme. simplest form short-term memory ﬁxed length buﬀer containing recent inputs; common extension apply kernel function buﬀer enable non-uniform sampling past values e.g. exponential decay older inputs however clear biological systems make shift registers. registers require interface environment buﬀers input presented simultaneously. impose rigid limit duration patterns deﬁning longest possible pattern requiring input vectors length. furthermore approaches struggle distinguish relative temporal position absolute temporal position whereas many systems expression based also utilised form memory state. example linear indexed memory work evolving data structures maintain internal state addition systems used data structures manipulate internal state recently poli explored soft assignment soft return operations forms memory within linear tree-based explore extend hypothesis inherent content-addressable memory existing within synchronous diﬀerent possible routes basin attraction asynchronous case maintaining node states across inputupdate-output cycle. signiﬁcant advantage approach rule/network’s short-term memory variable-length adaptive networks adjust memory parameters selecting within limits capacity memory aspects input sequence available computing predictions addition open-ended evolution maximum size short-term memory also open-ended increasing number nodes within network grows. here nodes initialised random initial random placing maze thereafter reset subsequent matching cycle. consequently network processes environmental input ﬁnal node states become starting point next processing cycle whereupon network receives environmental input places network trajectory toward diﬀerent locally stable limit point. therefore network given environmental input diﬀerent initial node states fall diﬀerent basin attraction thus rules’ dynamics constantly aﬀected inputs system executes. cell maze environments encoded binary bits white space represented obstacles food ‘f’. furthermore actions encoded binary shown figure task simply shortest path food given random start point. obstacles represent cells cannot occupied. woods optimal number steps food maze optimal steps woods woods teletransportation mechanism employed whereby trial reset agent reached goal state within discrete movements. woods maze non-markov environment containing communicating aliasing states positions border non-aliasing state identically sensed require diﬀerent optimal actions. thus solve maze optimally form memory must utilised optimal performance previously achieved woods addition memory register mechanism corporate using rule-linkage neural using recurrent links furthermore proof concept experiment cyclical directed graph shown capable representing rules memory solve woods however found twice ﬁfty experiments performance using -bit memory register number macro-classiﬁers population converges around furthermore average number nodes networks increases almost number connections declines fractionally mutation rate declines rapidly approx lowest point around trial moment optimal performance also observed. lastly figure conveys ﬁrst thousand trials sees rapid increase number cycles rapid decrease value subsequently continues increase along average number nodes networks; remains stable fewer woods maze non-markov environment containing aliasing conglomerates adjacent aliasing states. introduction aliasing conglomerates increases complexity learning task facing agent signiﬁcantly. would appear three memory-register bits required resolve perceptual aliasing. however since situations occur separate parts environment possibility optimal policy could evolve certain register bits used situation thus requiring fewer bits all. therefore clear large bit-register strictly necessary however practice register redundancy found important memory register required within solve maze optimally -bit registers achieving steps respectively figure shows performance ddgpxcsf woods parameters used prior experiment however pexpl although population size seem disproportionate population classiﬁers required woods representing scale compared increase required memory register potential number internal actions required rises thus resources clearly increasing quickly search space. optimality observed approximately trials slower explicit -bit memory register however size memory need predetermined inherent within networks action selection policy remains constant constant activity unlike lanzi wilson number macroclassiﬁers population converges around furthermore average number nodes networks increases fractionally number connections declines average mutation rate declines rapidly ﬁrst trials reaches lowest point trials. lastly figure seen average increases boolean functions replaced fuzzy logical functions fuzzy theory. thus generalize continuous representation generalize fuzzy-ca less restricted graph topology. wang explored -gene regulation networks using found able represent varying degrees gene expression also dynamics networks able mimic cell’s irreversible changes invariant state progress periodic cycle. randomly chosen fuzzy logical function. total number choices fuzzy logical functions decided number inputs. node inputs diﬀerent fuzzy logical functions. deﬁnition node inputs membership function deﬁned function degree membership work thus nodes updated simultaneously synchronously. number diﬀerent fuzzy logic sets introduced since original max/min method proposed. commonly used fuzzy logics include cfmqvs probabilistic gcd/lcm previously mentioned choice fuzzy result signiﬁcantly diﬀerent behaviour. therefore paper range commonly used logics potentially selectable leaving evolution identify appropriate combinations given problem. previously mentioned typically updated synchronously however asynchronous schemes fuzzy-ca shown provide number beneﬁts modeling dynamics realistically. figure shows aﬀect node updated asynchronously figure updated synchronously; results average experiments value contrast larger results increased percentage nodes changing state update cycle seen greater value less number states networks cycle within attractor. tendency fuzzy logic functions gravitate extremes increased inputs. after initial rapid decline rate change networks begin stabilise states fall respective attractors. however similar seen asynchronous updating scheme results lower percent nodes changing state compared synchronous case. asynchronous case number nodes changing state converges around compared synchronous nodes approximately compared nodes synchronous case. whereas used previously model aspects prior studies explored evolution networks computation. furthermore prior studies considered synchronous updating scheme. asynchronous rules within xcsf following scheme adopted. initial randomly created rule’s nodes randomly assigned connections node thus retains constant node state. initially many nodes input ﬁelds given task outputs plus other matching i.e. ﬁrst connection input node corresponding locus input message. connections continuous gridworld environment dimensional environment wherein current state real valued coordinate agent initially randomly placed within grid attempts shortest path goal located upper right corner; speciﬁcally paper goal found point agent given ﬁxed reward otherwise given. action would take system outside environment moves system nearest boundary. teletransportation mechanism employed whereby trial reset agent reached goal state within movements. actions agent choose four possible movements step size optimal number steps thus continuous state space combined long sequence actions required reach goal make continuous gridworld challenging multistep problems hitherto considered figure shows performance fdgp-xcsf continuous gridworld environment using parameters used lanzi however ninit figure seen optimal solution learnt around trials slower xcsf interval-conditions however similar performance mlp-based neural-xcsf average mutation rate within networks declines rapidly trials declines slower rate reaching bottom around trials. number macro-classiﬁers initially grows rapidly reaching peak declining around furthermore figure seen average number nodes flns increases used). further node’s connectivity represented list integers range represents input received connection. integer list subjected mutation reproduction self-adapting rate rule. hence within representation evolution select diﬀerent fuzzy logic functions node within given network rule along connectivity map. assigned random within fln. node states initialised random ﬁrst step trial thereafter reset subsequent matching cycle. population initially empty covering applied generate rules standard xcsf approach. given value fewer match node regardless state outputs rule join scheme also exploited within neural output nodes discretised similar fashion state fewer translates binary otherwise furthermore windowed approach utilised whereby ﬁnal state node calculated average last cycles average number connections within networks remains near static around additionally average value remains static around value increases slightly average frog problem single-step problem non-linear continuous-valued payoﬀ function continuous one-dimensional space. frog given learning task jumping catch distance frog frog receives sensory input jumping chosen distance receiving reward based distance given wilson presented form xcsf action computed directly linear combination input state vector action weights conducted experimentation continuous-action frog problem selecting classiﬁer highest prediction exploitation. tran subsequently extended adapting action weights problem moreover exploration action selection policy modiﬁed purely random selecting action highest prediction. reinforcement updates running invoked using combination mixed crossover mutation. reported greater performance averaged number trials superior performance reported wilson recently ramirez ruiz applied fuzzy-lcs continuous vector actions evolved action parts fuzzy systems continuous-action frog problem achieved lower error q-learning trials accommodate continuous-actions following modiﬁcations made fdgp-xcsf. firstly output nodes longer discretized instead providing real numbered output range building standard built selecting single classiﬁer adding matching classiﬁers whose actions within predetermined range rule’s proposed action parameters discrete case equivalent form rbn. shown xcsf able design ensembles asynchronous whose emergent behaviour collectively solve discrete-valued computational tasks reinforcement learning scheme. particular shown possible evolve retrieve content-addressable memory existing locally stable limit points within asynchronously updated networks ﬁnal node states previous match processing cycle become starting states next environmental input. furthermore shown parameters controlling system sampling networks’ dynamical behaviour made self-adapt updated executed usual exploitation functions selecting single best rule following experiments compare performance achieved using various criteria select best rule match set. parameters used used wilson tran i.e. θdel output node required thus ninit figure illustrates performance fdgp-xcsf continuous-action frog problem. figure seen greater performance achieved fewer trials faster previously reported results minimal changes resulting none drawbacks; exploration conducted roulette wheel prediction instead deterministically selecting highest predicting rule approach suitable online learning. furthermore tran action weights update component includes evaluation oﬀspring last input/payoﬀ discarded mutant oﬀspring accurate parent; therefore additional evaluations performed reﬂected number trials reported. figure seen average number macro-classiﬁers rapidly increases approximately trials converging around compact xcsf interval conditions showing fdgp-xcsf provide strong generalisation. addition networks grow average nodes average connectivity remains static around average mutation rate declines ﬁrst trials converging around average value increases paper explored examples temporally dynamic graph-based representation updated asynchronous parallelism syntax presented consists node receiving arbitrary number inputs unrestricted topology performing arbitrary function. representation evolved self-adaptive open-ended scheme allowing topology grow size meet demands problem space. temporal complexities target environment. introduced system thus need prior knowledge dynamics solution networks necessary represent environment. particular representation scheme exploited solve woods nonmarkov maze maze previously solved using explicit -bit memory register. rule network’s short-term memory variable-length adaptive networks adjust memory parameters selecting within limits capacity memory aspects input sequence available computing predictions. addition topology variable length maximum size short term memory openended increasing number nodes within network grows. thus maximum size contentaddressable memory need predetermined. subsequently generality scheme explored replacing selectable boolean functions fuzzy logical functions permitting application continuous-valued domains. speciﬁcally collective emergent behaviour ensembles asynchronous shown exploitable solving continuous-valued input-output reinforcement learning problems similar performance mlp-based neuralxcsf continuous-valued multistep grid environment superior performance reported previously frog problem. current research exploring possibilities general representation scheme solve complex problems lcs. particular problems wherein additional expressiveness beneﬁcial requiring memory temporal prediction.", "year": 2012}