{"title": "Granger-causal Attentive Mixtures of Experts", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Several methods have recently been proposed to detect salient input features for outputs of neural networks. Those methods offer a qualitative glimpse at feature importance, but they fall short of providing quantifiable attributions that can be compared across decisions and measures of the expected quality of their explanations. To address these shortcomings, we present an attentive mixture of experts (AME) that couples attentive gating with a Granger-causal objective to jointly produce accurate predictions as well as measures of feature importance. We demonstrate the utility of AMEs by determining factors driving demand for medical prescriptions, comparing predictive features for Parkinson's disease and pinpointing discriminatory genes across cancer types.", "text": "figure overview attentive mixtures experts attentive gating networks attend combined hidden state ame. expert’s assigns attentive factor opportunistically control it’s contribution ames ﬁnal prediction algorithmic decisions might biased discriminating uncover basis decisions legal ethical circumstances call explanations moreover information source decision especially important wrong decision could potentially severe consequences model’s decisions merely hints suggest thorough follow-up assessments model discovers patterns could advance scientiﬁc understanding underlying phenomena estimating relative contribution input features towards individual outputs deep neural network hard input features undergo multiple hierarchical interdependent non-linear transformations pass network propose approach importance attribution optimises jointly predictive performance accurate attribution feature importance end-to-end trained neural network. approach builds intuitive idea distributing feature groups interest among experts mixture experts model uses attentive gating assign weights individual experts. optimising attentive gating networks granger-causal objective ensure weights given individual experts correlate strongly measurably ability contribute decision hand. demonstrate approach able combine strength neural networks scalable feature extractors ability quantitatively measure feature importance enabling cases neural networks knowledge discovery observational data allowing measurably accurate attribution existing ones. several methods recently proposed detect salient input features outputs neural networks. methods offer qualitative glimpse feature importance fall short providing quantiﬁable attributions compared across decisions measures expected quality explanations. address shortcomings present attentive mixture experts couples attentive gating granger-causal objective jointly produce accurate predictions well measures feature importance. demonstrate utility ames determining factors driving demand medical prescriptions comparing predictive features parkinson’s disease pinpointing discriminatory genes across cancer types. introduction neural networks often criticised black-box models. researchers attempted address criticism developing tools provide visualisations qualitative explanations inner workings neural networks however comparatively little progress made towards providing quantiﬁable measures relative feature importance expected accuracy. quantiﬁable measures desirable many machinelearning cases predictive performance interpretability paramount importance measures facilitate communicating adjusting arguing decisions machine-learning models enable tell introduce granger-causal objective ensures attention weights assigned attentive gates ames correlate respective expert’s ability contribute individual decision. demonstrate utility ames variety settings determining factors driving demand medical prescriptions comparing predictive features parkinson’s disease pinpointing discriminatory genes across multiple types cancer. perturbation-based approaches. perturbation-based approaches sensitivity individual inputs neurons neural network explained modelling impact local perturbations gradient-based sensitivity analysis. gradient-based approaches based idea following gradient output nodes input nodes obtain features output sensitive several improvements technique since proposed perturbation-based gradient-based approaches share limitations neither provide measures relative feature importance input features whose magnitude meaningfully compared across decisions expected accuracy explanations limiting qualitative analyses singular decisions. above-mentioned categories aporthogonal proaches provide attribution pointing inﬂuential training data given decision similarly ames outside abovementioned categories optimise jointly predictive performance accurate attribution single endto-end trained model individual decision. mixtures experts. conceptually ames compared hierarchical mixtures experts gating networks hmes trained gating network that based respective input sample stochastically activates exactly many specialised expert networks. goal approach achieve decoupled competing experts decompose given learning problem subtasks ideally easier solve. recently proposed end-to-end optimised mixture experts layer lowers computational cost machine translation conditional computation. aforementioned mixture models gating increase either computational predictive performance require expert receives input structure thus making unsuitable heterogenous inputs. gating used neural networks long time. example gating used enable learning longterm dependencies long short-term memory layers facilitate training deep neural architectures attentive models. attentive models used various domains improve interpretability performance computer vision related works used attention convolutional neural networks selectively focus input data internal convolutional ﬁlters however fundamentally na¨ıve soft attention mechanisms provide incentive neural network yield attention factors correlate feature importance. soft attention mechanisms therefore interact components neural networks opaque ways furthermore possible measure expected quality explanations provided na¨ıve soft attention mechanisms. therefore follow approach attribution deep neural networks models optimises explicitly predictive performance well attribution accuracy. approach advantages existing ones provides measure expected quality network’s attributions ability compare relative magnitude attributions across individual decisions enabling quantitative analysis model errors biases identiﬁcation novel patterns multiple independent decisions observational data. attentive mixtures experts ames consist experts corresponding attentive gating networks prediction time attentive gating networks output attention factor expert control respective contribution ames ﬁnal prediction experts attentive gating network neural networks parameters architectures. ames impose restrictions expert networks need expose topmost feature representation figure graphical illustration attentive gating network placed expert within model. gating network associated mlpi weight bias parameters. experts’ mlpi transforms combined hidden state hidden representation measure similarity hidden representation informative hidden context representation obtain attention factor local prediction expert conceptually formulation equation reminiscent gradient boosting estimators iteratively added reduce residual error previous estimators generalised additive models aside lending end-to-end optimisation note characteristics ames rich introspective ability given gating network well human observers attention mechanism ability attentive gating network dynamically activate experts depending combined hidden state whole rather inputs. line reports related works rely gating networks early training mini-batch stochastic gradient descent proposed attentive gating mechanism commonly collapses either consistently assign attention weight single expert others assign amount attention experts. stuck local minima unlikely recover. fundamental problem na¨ıvely-trained incentive learn feature representations yield accurate attributions. combat undesired tendency towards attributions correspond experts’ respective ability contribute introduce secondary granger-causal objective function lgranger. granger-causality follows humean deﬁnition causality declares causal relationship random variables better able predict using available information information apart local prediction given input subset input gating networks hidden states local predictions expert concatenated form combined hidden state whole ame. important property note ames require every expert makes predictions based subset input data expert therefore operate distinct subset input data. experiments make property able quantify impact different feature groups input data ame’s decisions. figure shows conceptual overview ame’s components interactions. ames attentive gating networks control contributions expert outputs output attentive gating network attention factor expert. attention factors dynamically modulate contribution expert ﬁnal prediction based current combined hidden state ame. calculate attention factors using corresponds single-hidden-layer multi-layer perceptron activation function weight matrix bias yields hidden representation equation calculate attention factors expert computing softmax dot-product similarity expert’s context vector uei. corresponds hidden representation informative combined hidden state expert jointly optimise parameters training. comparing current hidden representation informative hidden representation expert attentive gating network learns activate experts hidden states contribute ﬁnal prediction ame. individual attention mechanism expert soft attention mechanism described equations differs signiﬁcantly used related works. related works typically train shared informative state common figure quantify degree expert able contribute individual decisions comparing prediction error using available information prediction error using information apart expert desired possible express a-priori preference accurate explanations accurate predictions weighting components accordingly. terms implementation estimate εx\\{i} using ground truth labels ytrue main loss lmain single-layer auxiliary models faux fauxi receive input combined hidden state combined hidden state minus hidden state local prediction expert \\{hi respectively. faux reused across experts error estimation requires additional auxiliary output expert. additionally clip ∆εxi positive consider cases εx\\{i} might happen different degrees intermittent convergence auxiliary outputs. furthermore note important prevent gradient towards ∆εxi e.g. using tensorflow’s tf.stop gradient prevent network optimising error match desired attention values rather around. theoretically training provide attributions predictions time adverse impact predictive performance neural network objectives share common feature space i.e. able accurately attribute helpful producing predictions vice-versa. essence ames make attribution implicitly performed neural network explicit. experiments order demonstrate utility ames performed experiments variety tasks distinctly different characteristics. experiments include settings high numbers samples high numbers features high correlations features heterogenous input data. speciﬁcally ames determine drivers medical prescription demand scale compare predictive features monitoring parkinson’s disease pinpoint discriminatory genes across multiple types cancers. experiments showcase breadth tasks neural networks scalable non-linear feature extractors used equipped ability quantitatively measure feature importance across decisions. addition wish evaluate degree attentive gating decisions correlate expert’s ability contribute whether providing attributions adverse used spite name empirical settings granger-causality akin correlation causation therefore strongly caution reader mistake granger-causality standard deﬁnition causality nonetheless theory granger-causality allows naturally deﬁne simple efﬁcient loss function ensures relative magnitudes attention factors correlate respective expert’s ability contribute ame’s ﬁnal output given heterogenous input data denote ame’s prediction error without including information expert ame’s prediction error considering available information deﬁne degree expert able contribute ﬁnal output decrease error associated adding expert’s information available information sources note using ∆εxi deﬁne degree attribution intuitively resolves even cases combinations features enable improvements ame’s prediction error experts would attributed equally decrease. using deﬁne desired attribution corresponding experts’ attention weights given input equation normalises attributions across experts ensure scale within across decisions. arrive granger-causal objective function lgranger computing mean kullbackleibler divergence samples target distribution actual distribution attention values granger-causal loss measures probabilistic distance actual attributions desired grangercausal attributions valid proxy expected quality explanations. granger-causal loss indicates perfect match granger-causal attributions satisﬁes axioms outlined therefore apply familiar framework cross-validation held-out test data estimate expected accuracy attributions provided attention factors finally total loss main loss arbitrarily chosen depending dataset hand granger-causal loss accurate demand forecasts medical prescriptions play important role preventing stock outages keeping cost supply operations check. however demand forecasting prescriptions extremely challenging task large prescribed items poorly understood characteristics inﬂuence factors. gain deeper understanding factors drive demand individual prescription items trained model predict next month’s demand individual prescription items level practices. inspection assigned attention factors enables analyse factors predictive speciﬁc prescription items. regression setting challenging highly heterogenous input data different time scales need scale tens millions available training samples. collected multiple heterogenous data streams covering whole country england united kingdom during time frame january december used data streams corresponding feature groups. data stream listed represented expert model demand history. primary data stream used monthly data revenues generated reimbursed prescription items general practices england released british national health service ensure full history past demand prefigure illustration process converting existing neural network architecture experts operating predeﬁned feature subsets expert limited make local prediction based subset data post-hoc analysis attention factors output attentive gating networks enables quantify contribution individual experts output dictions removed practices dataset remained closed point prediction time frame. remove noise introduced different prescription formulations packagings aggregated prescription items prescription item class deﬁned pharmaceutical code british national formulary reported nhs. context added monthly total revenue region current practice belongs practice’s distance region’s centre additional input features. used territory level regions deﬁned geographically subdivide england regions. online search interest. used google trends retrieve relative monthly online search interest time frame regional code total queried online search interest medical terms comprehensive medical subject headings ontology many medical terms mesh ontology common vernacular ultimately medical terms search activity signiﬁcant enough return result google trends. applied principal component analysis transform explains variance search interest data reduces feature vector dimensions. regional weather. weather impacts consumption across wide variety industries therefore added monthly weather data including rainfall days frost hours sunshine average temperature data ofﬁce stations dataset. regional demographic economic labor data. regional demographic economic labor data proximity practice could affect future prescription demand groups different population-level proﬁles potentially require different types care different amounts. analyse predictive potential population-level factors added yearly indicators demographic proﬁle yearly indicators economic proﬁle yearly indicators labor proﬁle region dataset. represented three population-level feature groups indicators distinct expert model. analogous online search interest data applied transforms explain variance three feature groups. baselines trained autoregressive integrated moving average models recurrent neural networks feedforward neural networks architectures models derived trained model main difference evaluated former receives input data collapsed time rather sequentially. univariate arima models commonly used forecasting practice comparison served baseline make information apart revenue history. preprocessing. prior ﬁtting models standardised prescription revenue history data time series range preserved minimum maximum time series value separate input features. additionally normalised numeric features except prescription revenue data mean value standard deviation using data training set. compared models beneﬁted preprocessing. hyperparameters. ensure fair comparison took systematic approach towards hyperparameter search. models based neural networks performed hyperparameter search hyperparameters chosen random predeﬁned ranges runs. performance comparison selected models hyperparameter search achieved best performance validation data. used batch normalisation dropout neural network models. methodologically optimised neural networks’ mean squared error early stopping validation set. arima used iterative parameter selection algorithm dataset split. applied random split general practice separate available data training validation test report main properties different folds table metrics. compared predictive accuracy different models computing symmetric mean absolute percentage error rate anomalous events held-out test table properties dataset folds used demand forecasting experiment. report number time series number general practices number distinct prescription item classes average monthly prescription item revenue pound sterling practice month corresponding percentage whole dataset. table comparison symmetric mean absolute percentage error rate anomalous events held test practices also report average± standard deviation hours required training evaluation across runs. performance. results showed extra information collected various sources indeed improved prediction performance univariate arima baseline especially anomalous events ﬁnding suggests healthcare providers suppliers consider incorporating open data sources forecasting models order reduce supply costs better manage risk out-of-stock events. addition found providing attributions adverse impact performance comparing corresponding model time train models comparable rnns. attribution. factors driving demand prescription items plotted mean contribution per|aiyi| centage mean average amount attention given experts speciﬁc prescription items. results showed signiﬁcant differences sensitivity proﬁles across different prescription items determine degree unmodulated modulated expert predictions aiyi correlate ground truth demand ytrue performed hypothesis tests using kendall’s tested correlation prescription items held-out test set. applied benjamini-hochbergyekutieli procedure control false discovery rate multiple testing. large differences exception detable ratio prescription items local predictions contributions aiyi considered signiﬁcantly correlated ground truth prescription demand ytrue average absolute effect size contributions’ correlations prescription items held-out test set. figure inﬂuence experts specialised demand history online search interest regional weather regional demographic data regional economic data regional labor data predictions made prescription item classes held-out test set. item report average attention mean contribution percentage attributed expert. dashed lines indicate mean average attention prescription item classes. symbols point positions selected well-known prescription items signiﬁcant correlations ground truth demand ytrue respective experts. available data samples training sammand history expert modulated unmodulated expert outputs indicate attentive gating decisions ples validation testing indeed highly effective dynamically activating experts according ability contribute reason rate signiﬁcant correlations increase demand expert already highly correlated ground truth modulation. hyperparameters. features used single hidden layer batch normalisation dropout rate neurons expert networks ame. trained learning rate epochs early stopping patience validation set. chose hyperparameters based grid search validation set. parkinson’s disease slowly progressing neurodegenerative disease currently exists cure. however diagnosed symptoms managed effectively using surgical pharmacological interventions. early diagnosis therefore signiﬁcantly improve quality life patients manifests speech impairments microphones could used task train classify audio features whether produced person without analysing attention weights assigned enables compare suitability various features task. setting goal evaluate behavior ames presence highly correlated inputs. dataset. used oxford parkinson’s tele-monitoring dataset consisting sustained vowel phonations study participants reported highly correlated features determined several subsets features achieve similar performances dataset. comparison results baseline. applied stratiﬁed random split separate attribution. found almost uniform distribution attention factors across features held-out test uniform distribution attention factors indicates many features redundant. indeed reported subset features sufﬁces reach performance available features. therefore correctly identiﬁed features largely equivalent importance. however differences between feature importances within range estimation error attributions possible determine best subset features attention factors. hypothesise larger training might raise attribution accuracy level enables distinguishing highly correlated features. widespread adoption next generation sequencing exists wealth genomic data could infer genes predictive speciﬁc phenotypes. knowledge genomic factors related phenotypes help guide development treatments well increase understanding biological processes involved complex diseases. pinpoint genes differentiate different types cancer trained model classify gene expression data being either breast invasive carcinoma kidney renal clear cell carcinoma colon adenocarcinoma lung adenocarcinoma prostate adenocarcinoma setting characterised large number input genes relevant. dataset. used subset data cancer genomic atlas pan-cancer rnaseq dataset includes gene expression data multiple cancer types individuals. keep visualisations succinct used subset genes input data. applied stratiﬁed random split separate available data samples training samples validation testing hyperparameters. genes used single hidden layer batch normalisation single neuron expert networks. trained learning rate epochs early stopping patience validation set. chose hyperparameters based grid search validation set. weighted main granger-causal loss ratio. additionally trained figure importance speciﬁc genes distinguishing multiple cancer types measured average assignment attention factors report average attention factors samples per-cancer subsets test set. grey bars spanning subsets highlight discriminatory genes average attention all. bolded names genes whose associations substantiated literature evidence. performance. across runs best models achieved classiﬁcation errors test reinforcing hypothesis providing explicit attributions neural networks adverse impact performance. attribution. inspecting ames assigned attributions found based decisions primarily small number highly predictive genes different types cancer genes highest average attributions found prior works report links respective cancer type gene locus identiﬁed limitations expert convergence. possible expert weighted correspondingly feature group’s ability contribute either overunderﬁt. similarly expert network simply enough capacity best architecture right choice hyperparameters accurately represent value underlying feature group. however monitor performance convergence experts local predictions confounding. granger-causal association imply causation confounders could inﬂuenced attribution. verifying causality observational data impossible reason nonetheless demonstrated experiments ame’s ability quantify predictive associations inputs outputs across decisions provide valuable explanations help generate insights observational data. conclusion present novel approach towards attribution neural networks built intuitive idea distributing feature groups interest among experts mixture experts model uses attentive gating assign weights individual experts. show assigned attributions contain valuable information providing attributions adversely impact predictive performance. approach enables exciting cases neural networks knowledge discovery observational data providing measurably accurate attributions compared across decisions. acknowledgments work partially funded swiss national science foundation project within national research program data snsf project contains public sector information licensed open government licence references adler philip falk casey friedler sorelle rybeck gabriel scheidegger carlos smith brandon venkatasubramanian suresh. auditing black-box models indirect inﬂuence. data mining ieee international conference ieee baehrens david schroeter timon harmeling stefan kawanabe motoaki hansen katja m¨uller klausrobert. explain individual classiﬁcation decisions. journal machine learning research bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. international conference learning representations choi edward bahadori mohammad taha jimeng kulas joshua schuetz andy stewart walter. retain interpretable predictive model healthcare using reverse time attention mechanism. advances neural information processing systems ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning kindermans pieter-jan hooker sara adebayo julius alber maximilian sch¨utt kristof d¨ahne sven erhan dumitru been. reliability saliency methods. arxiv preprint arxiv. little mcsharry patrick hunter eric spielman jennifer ramig lorraine suitability dysphonia measurements telemonitoring parkinson’s disease. ieee transactions biomedical engineering montavon gr´egoire lapuschkin sebastian binder alexander samek wojciech m¨uller klaus-robert. explaining nonlinear classiﬁcation decisions deep taylor decomposition. pattern recognition ribeiro marco tulio singh sameer guestrin carlos. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining riemer matthew vempaty aditya calmon flavio heath fenno hull richard khabiri elham. correcting forecasts multifactor neural attention. international conference machine learning schwab patrick scebba gaetano zhang delai marco karlen walter. beat beat classifying cardiac arrhythmias recurrent neural networks. computing cardiology selvaraju ramprasaath abhishek vedantam ramakrishna cogswell michael parikh devi batra dhruv. grad-cam that? visual explanations deep networks gradient-based localization. arxiv preprint arxiv. shazeer noam mirhoseini azalia maziarz krzysztof davis andy quoc hinton geoffrey dean jeff. sparsely-gated mixture-of-experts layer. arxiv preprint arxiv. simonyan karen vedaldi andrea zisserman andrew. deep inside convolutional networks visualising image classiﬁcation models saliency maps. international conference learning representations stollenga marijn masci jonathan gomez faustino schmidhuber j¨urgen. deep networks internal selective attention feedback connections. advances neural information processing systems weinstein john collisson eric mills gordon shaw kenna mills ozenberger brad ellrott kyle shmulevich ilya sander chris stuart joshua network cancer genome atlas research cancer genome atlas pan-cancer analysis project. nature genetics kelvin jimmy kiros ryan kyunghyun courville aaron salakhudinov ruslan zemel rich bengio yoshua. show attend tell neural image caption generation visual attention. international conference machine learning yang zichao yang diyi dyer chris xiaodong smola alexander hovy eduard hierarchical attention networks document classiﬁcation. proceedings conference north american chapter association computational linguistics human language technologies zhang zizhao yuanpu xing fuyong mcgough mason yang lin. mdnet semantically visually interpretable medical image diagnosis network. international conference computer vision pattern recognition", "year": 2018}