{"title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users  with Limited Communication Skills", "tag": ["cs.AI", "cs.CV", "cs.HC", "cs.LG", "cs.RO", "I.2.4; I.2.6; I.2.8; I.2.9; I.2.10; I.4.8; I.5.1"], "abstract": "As autonomous service robots become more affordable and thus available also for the general public, there is a growing need for user friendly interfaces to control the robotic system. Currently available control modalities typically expect users to be able to express their desire through either touch, speech or gesture commands. While this requirement is fulfilled for the majority of users, paralyzed users may not be able to use such systems. In this paper, we present a novel framework, that allows these users to interact with a robotic service assistant in a closed-loop fashion, using only thoughts. The brain-computer interface (BCI) system is composed of several interacting components, i.e., non-invasive neuronal signal recording and decoding, high-level task planning, motion and manipulation planning as well as environment perception. In various experiments, we demonstrate its applicability and robustness in real world scenarios, considering fetch-and-carry tasks and tasks involving human-robot interaction. As our results demonstrate, our system is capable of adapting to frequent changes in the environment and reliably completing given tasks within a reasonable amount of time. Combined with high-level planning and autonomous robotic systems, interesting new perspectives open up for non-invasive BCI-based human-robot interactions.", "text": "fig. framework unifying decoding neuronal signals high-level task planning low-level motion manipulation planning scene perception centralized knowledge base core. intuitive goal selection provided adaptive graphical user interface. robotic service assistant. record neuronal activity elicited human brain common origin types communication electroencephalography system. furthermore adopt convolutional neural network approach online decoding neuronal activity order allow users navigate graphical user interface provided high-level task planner. feasible actions displayed depends turn current state world stored central knowledge base continuously updated information provided robot camera perception system. task selected decomposed sequence atomic actions high-level planner. subsequently action resolved motion mobile manipulator using low-level motion manipulation planning techniques. following individual components shown fig. described detail presenting quantitative evaluation overall system regarding performance. abstract— autonomous service robots become affordable thus available also general public growing need user friendly interfaces control robotic system. currently available control modalities typically expect users able express desire either touch speech gesture commands. requirement fulﬁlled majority users paralyzed users able systems. paper present novel framework allows users interact robotic service assistant closed-loop fashion using thoughts. brain-computer interface system composed several interacting components i.e. non-invasive neuronal signal recording decoding high-level task planning motion manipulation planning well environment perception. various experiments demonstrate applicability robustness real world scenarios considering fetch-and-carry tasks tasks involving human-robot interaction. results demonstrate system capable adapting frequent changes environment reliably completing given tasks within reasonable amount time. combined high-level planning autonomous robotic systems interesting perspectives open non-invasive bci-based humanrobot interactions. patients heavily impaired communication capabilities severly paralyzed patients condition forces constantly rely help human caretakers. robotic service assistants re-establish autonomy patients offer adequate interfaces possess sufﬁcient level intelligence. generally intelligent system requires adaptive task motion planning modules determine appropriate task plans motion trajectories robot implement task real world. moreover requires perception component e.g. detect objects interest avoid accidental collisions obstacles. typically used interfaces haptic audio visual interfaces command robotic system intuitive easy options healthy users difﬁcult impossible paralyzed individuals. authors contributed equally work. authors department computer science faculty medicine university freiburg germany. {burgetf kuhnerd aldinger jboedeck nebel burgard}informatik.uni-freiburg.de {lukas.ﬁederer martin.voelker tonio.ball}uniklinik-freiburg.de. research robin.schirrmeister supported german research foundation grant bmi-bot baden-w¨urttemberg stiftung. multiple previous studies focused robotic systems assisting people disabilities. example park implemented system autonomous feeding yogurt person. chung focused autonomous drinking involved locating drink picking bringing person’s mouth. using hybrid head movement control achic studied setup moving wheelchair attached robotic arm. none systems used pure control. contrast wang used motor imagery three classes achieve low-level control robotic arm. relevant schr¨oer developed robotic system receives command user autonomously assists user drinking cup. however approach considers single object ﬁxed-base manipulator. recently muelling presented shared-control approach assistive robotics albeit focused invasive bcis. nonetheless approach could combined high-level planning approach presented work. applications robust decoding brain signals required. inspired successes deep convolutional neural networks computer vision speech recognition deep convnets recently applied frequently brain-signal decoding. deep convnets already applied decoding tasks useful building brain-computer interfaces. lawhern used deep convnet decode oddball signals feedback error-related negativity movement-related tasks. evaluated cross-subject i.e. trained subjects evaluated others convnet yields competitive accuracies compared widely-used traditional brain-signal decoding algorithms. tabar halici used convnet combined convolutional stacked autoencoder decode motor imagery within-subject yielding better accuracies several non-convnet decoding algorithms. schirrmeister used shallow deep convnet decode motor imagery motor execution within-subject reaching slightly surpassing accuracies widely used motor-decoding algorithm ﬁlter bank common spatial patterns bashivan used convnet trained fourier-transformed inputs estimate mental workload. addition work evaluating convnet decoding accuracies convnet visualization methods allow sense brain-signal features network using taken together advances make deep convnets viable alternative brainsignal decoding brain-computer interfaces. still knowledge online control deep convnets reported eeg-based brain-computer interface. system hand developed control complex scenarios ones considered previous work. particularly consider scenarios involving manipulation objects well human-robot interaction. feasible goals determined controlled directional commands. reliable classiﬁcation brain signals navigation directions cannot achieved directly non-invasive bcis used deep convnet approach decoding multiple mental tasks approach introduces hybrid network combining deep convnet shallower convnet architecture. deep part consists convolution-pooling blocks using exponential linear units pooling whereas shallow part uses single convolutionpooling block squaring non-linearities mean pooling. parts ﬁnal convolution produce output features. features concatenated ﬁnal classiﬁcation layer. trained convnet decode mental tasks right hand ﬁnger feet movements object rotation word generation rest. mental tasks evoke discernible brain patterns used surrogate signals control gui. ofﬂine training done cropped training strategy using shifted time windows within trials input data train decoder subjects environment close possible real application environment avoid pronounced performance drops. therefore designed gradual training paradigm within high-level planner displayed environment timing actions identical task. training paradigm proceeds follows ﬁrst train subject ofﬂine using simulated feedback. subjects aware control gui. mental tasks cued using grayscale images presented center display. times ﬁxation circle displayed center subject instructed ﬁxate minimize movements. random time interval ﬁxation circle switched disk indicates mental task. time action corresponding cued mental task performed update gui. keep training realistic include error rate i.e. average every ﬁfth action erroneous. instruct subjects count error occurrences assert vigilancy. ofﬂine data used train individual deep convnets. then subjects online training performing decoded mental tasks gui. finally stop cueing mental tasks. evaluate performance control subjects create instructed high-level plans gui. tasks executed simulated robot real mobile manipulator available. provide control mobile manipulator enhance feeling agency subjects conﬁrm execution every planned action interrupt chain actions moment execution. decoding accuracies label-less instructed tasks assessed manually rating decoding based instructed task steps. statistical signiﬁcance decoding accuracies tested using conventional permutation test random permutations labels complex task. user formulate high-level goal without knowledge internal representation objects planning system exact capabilities robot. achieved intuitive graphical user interface object parameters goal speciﬁed incrementally reﬁning objects referring type e.g. attributes e.g. content apple-juice. domain independent planning identiﬁes sequence actions transforms current world state state satisfying goal condition. planning task consists planning domain describing static components object type hierarchy available actions problem instance describing objects present world current state well goal description. current state objects extracted knowledge base goal chosen gui. restricted vocabulary shared user planning system. objects sets objects identiﬁed creating referring expressions composed shared references built vocabluary brieﬂy describe relevant aspects previous work area general referring expression logical formula single free variable. refers object valid. e.g. reference contains refers cups containing water. restrict references simple conjunctions facts preferable computational reasons also allows incrementally reﬁne references adding constraints. example adding contains restricts cups cups containing water. distinguish three types fundamental object references individual references typename references relational references. individual references identiﬁed name omnirob robot. typename references identiﬁed name type. cannot refer cups scenario directly refer unspeciﬁc cup. relational references encountered objects referred predicates occur argument. relations scenario object attributes. example content used clarify meant. object references used create references goals. start deﬁning goals action achieves found natural user e.g. put∧ cup∧ shelf initial selection goal type necessary determine objects parameters goal predicate action. parameters reﬁned constraining previous choice argument either determined uniquely user declares remaining option acceptable. exclude unreachable goals allow goals achieved sequence preceding actions goal determined selection process passed custom domain independent planner. generating paths mobile base apply sampling-based planning framework birrt* given pair terminal conﬁgurations performs bidirectional search using uniform sampling conﬁguration space initial sub-optimal solution path found. path subsequently reﬁned remaining planning time adopting informed sampling strategy yields higher rate convergence towards optimal solution. execution paths implemented closed-loop joint trajectory tracking algorithm using robot localization feedback. realize pick place pour drink motions efﬁciently adopt probabilistic roadmap planner approach planner uses graph randomly sampled task poses connected edges. plan poses planner connects poses roadmap graph uses algorithm optimal path start goal pose. execution plan maps task space velocity commands joint velocity commands employing task space motion controller. sample random poses around object determine grasp motions. dropping objects extract horizontal planes camera’s point cloud sample poses planes suitable drop location. implementation framework real world requires several components neuronal signal decoding scene perception knowledge base operations well symbolic motion planning parallel. therefore distributed computation across network computers communicating among ros. decoding neuronal signals four components. measurements performed using waveguard caps electrodes neurone ampliﬁer mode. additionally vertical horizontal eogs emgs four extremities ecg’s recorded. recording online-preprocessing used matlab. transferred data server deep convnet classiﬁed data classes. high-level planner consists backfront-end. backend uses fast downward planner iteratively build goal references symbolic plans selected goal. planning time crucial performance system used fast downward basic conﬁguration experiments. central knowledge base implemented node able store objects arbitrary attribute information. changes knowledge base automatically trigger updates front-end unexpected ones interrupt current motion trajectory execution. finally used simtrack object pose detection tracking. fig. experimental environment shelves table considered robot performing manipulation actions. five rgbd sensors observe environment. human operator selects goal using control high-level planner gui. user sits wheelchair front screen displaying graphical interface high-level planner. robot used experiments omnirob omni-directional mobile manipulator platform kuka robotics composed degrees freedom i.e. mobile base manipulator. additionally dexterous hand schunk attached manipulator’s ﬂange used perform grasping manipulation actions. tasks considered experiments required robotic system autonomously perform following actions drive location another pick object drop object pour liquid bottle supply user drink. moreover perception system composed rgbd cameras. three statically mounted shelves table order observe scene report captured information knowledge base. cameras carried robot on-board. ﬁrst located mobile base used perform collision checks manipulation planning. second camera mounted robot’s end-effector used tasks involving physical human-robot interaction. demonstration framework found accompanying video http//www.informatik.uni-freiburg.de/~burgetf/ecmr/. evaluated control setup four healthy subjects progress validation mobile manipulator performed. total runs recorded subjects executed various instructed high-level plans. runs used simulated feedback order generate signiﬁcant amount data evaluation. performance decoding runs assessed using video recordings interactions gui. rated actions correct correspond instructed path incorrect otherwise. actions necessary remediate previous error interpreted correct correction intentionally clear. finally rated rest actions correct robot executions incorrect next robotic action initialized ignored high-level plan creation. evaluation metrics extracted video recordings accuracy control time took subjects execute high-level plan number steps used execute high-level plan path optimality i.e. ratio steps used minimally possible number steps average time step. summarized results table total correct control achieved required step. selecting plan using took average required user perform average steps highlevel planner. path formed steps average away optimal path. decoding accuracy every subject signiﬁcantly chance subject-averaged data used train hybrid convnets decoding results train/test transfer visualized fig. fig. show signal-tonoise ratio classes labeled datasets. deﬁne given frequency time electrode corresponds values position i-th task |mi| number repetitions. median median interquartile range respectively. upper part describes variance class medians i.e. large variance means distinguishable class clusters higher snr. denominator describes variance values class i.e. lower variance values results higher snr. channels shows subjects move tasks. decoding accuracies achieved test data initial training convnets visualized fig. support neural origin control signals fig. shows physiologically plausible input-perturbation network-prediction correlation results methods). might differ reasons. number executed calls lower scheduled ones indicates previous action step failed succeed plan recovery possible. hand higher number executed calls indicates user able achieve plan recovery commanding repetition failed action. moreover recorded largest standard deviation approach action attributed diverse complexity planning problem mobile base distance travel selected grasp drop location. total system achieved success rate entire task. planning execution required average errors mainly caused object detection issues i.e. system able detect object detection precise enough able successfully grasp drop object. last experiment evaluates direct interaction between user robot. therefore implemented autonomous robotic drinking assistant. approach enables robot liquid move robot user ﬁnally provide drink user execution corresponding drinking motion front user’s mouth. addition techniques described above successful pouring drinking using robot requires detection liquid level reliable detection localization user’s mouth. detect liquid level pouring follow vision-based approach introduced given camera’s viewing angle liquid’s index refraction liquid height determined depth measurement using relationship based snell’s details). using knowledge ﬁrst detect extract depth values liquid ﬁnally estimate real liquid height. type liquid hence index refraction assumed given beforehand. viewing data decoding results. ﬁrst data fig. used train hybrid convnet. highest observed alpha lower beta bands. frequency bands robust markers task related mental tasks. note non-eeg channels withheld convnets time displayed negative control. channels displayed space constraints. confusion matrix decoding accuracies train/test transfer. accuracies well theoretical chance level topographically plausible input-perturbation network-prediction correlation maps alpha frequency band. details visualization technique refer reader ﬁrst experiment considering real robot evaluates complete system fetch-and-carry tasks. goal transfer object location another e.g. shelf table using robot. fulﬁll tasks robot typically needs execute four subtasks approach object location grasp object approach location drop object. user instructed select predeﬁned goal using eeg-controlled high-level planner. moreover selected random initial placement objects order cover different environment states. experiment repeated times user. table shows averaged results experiment. second column indicates overall number desired action calls scheduled high-level planner well number calls actually performed. third ﬁfth columns represent success rate mean standard deviation runtime actions respectively. note number scheduled actually executed actions angle determined depth data. kalman ﬁlter used track liquid level compensate noise. detected liquid level exceeded user deﬁned value stop signal sent terminate pouring motion. detection localizing user’s mouth adopt step approach. ﬁrst step segment image based output face detection algorithm order extract image region containing user’s mouth eyes. afterwards detect position mouth user considering obtained image patch. regarding mouth orientation additionally consider position eyes order obtain robust estimation face orientation hence compensating slightly changing angles head. face mouth detectors implemented opencv applying algorithm uses haar cascades table shows averaged results experiment. here scheduled actions repeated order complete task successfully. plan recovery possible leading abortion task. thus system achieved total success rate drinking task. planning execution required average evaluation liquid level detection approach speciﬁed desired level executed runs pour action. resulting mean error standard deviation instances bottle obstructed camera view resulting poor liquid level detection higher error. paper presented thought-controlled mobile robotic service assistant capable successfully performing complex tasks including close range interaction user continuously changing environment increase independence severely paralyzed patients. high-level planner intermediate layer user autonomous mobile robotic service assistant overcome curse dimensionality typically encountered non-invasive control schemes thus opening perspectives human-robot interaction scenarios. c.-s. chung wang cooper autonomous function wheelchair-mounted robotic manipulators perform daily activities rehabilitation robotics ieee international conference achic montero penaloza cuellar hybrid system operate electric wheelchair robotic navigation manipulation tasks advanced robotics social impacts ieee workshop schr¨oer killmann frank v¨olker fiederer ball burgard autonomous robotic assistant drinking robotics automation ieee international conference muelling venkatraman j.-s. valois downey weiss javdani hebert schwartz collinger bagnell autonomy infused teleoperation application brain computer interface controlled manipulation autonomous robots sainath kingsbury saon soltau a.-r. mohamed dahl ramabhadran deep convolutional neural networks large-scale speech tasks neural networks vol. sercu puhrsch kingsbury lecun very deep multilingual convolutional neural networks lvcsr ieee international conference acoustics speech signal processing schirrmeister springenberg fiederer glasstetter eggensperger tangermann hutter burgard ball deep learning convolutional neural networks brain mapping decoding movement-related information human arxiv. burget bennewitz burgard birrt* efﬁcient sampling-based path planning framework task-constrained mobile manipulation ieee/rsj international conference intelligent robots systems daejeon korea pauwels kragic simtrack simulation-based framework scalable real-time object pose detection tracking ieee/rsj international conference intelligent robots systems ieee schubert burgard probabilistic approach liquid level detection cups using rgb-d camera ieee/rsj international conference intelligent robots systems daejeon korea viola jones rapid object detection using boosted cascade simple features computer vision pattern recognition cvpr proceedings ieee computer society conference vol.", "year": 2017}