{"title": "The Riemannian Geometry of Deep Generative Models", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Deep generative models learn a mapping from a low dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold. However, further investigation into this phenomenon is warranted, to identify if there are other architectures or datasets where curvature plays a more prominent role. We believe that exploring the Riemannian geometry of deep generative models, using the tools developed in this paper, will be an important step in understanding the high-dimensional, nonlinear spaces these models learn.", "text": "deep generative models learn mapping lowdimensional latent space high-dimensional data space. certain regularity conditions models parameterize nonlinear manifolds data space. paper investigate riemannian geometry generated manifolds. first develop efﬁcient algorithms computing geodesic curves provide intrinsic notion distance points manifold. second develop algorithm parallel translation tangent vector along path manifold. show parallel translation used generate analogies i.e. transport change data point semantically similar change another data point. experiments real image data show manifolds learned deep generative models nonlinear surprisingly close zero curvature. practical implication linear paths latent space closely approximate geodesics generated manifold. however investigation phenomenon warranted identify architectures datasets curvature plays prominent role. believe exploring riemannian geometry deep generative models using tools developed paper important step understanding high-dimensional nonlinear spaces models learn. learning unlabeled sensory observations often high-dimensional problem signiﬁcant importance machine learning. inﬂuential notion line research manifold hypothesis states high-dimensional observations concentrated around manifold much lower dimensionality indeed manifold hypothesis basis much prior work problems unsupervised semi-supervised learning following recent success deep generative models modeling observed data higher ﬁdelity earlier possible. particularly true visual observations deep generative models variational autoencoders generative adversarial networks pixelcnn variants shown generate good quality images. models involve learning mapping lower-dimensional latent space high-dimensional space observed data. allows generating novel data samples ancestral sampling seeded samples latent space. learned generator models able generate high-ﬁdelity data samples generator mapping argued approximate data manifold reasonably well. explored context semi-supervised learning obtain smooth invariances classiﬁcation estimating tangent directions data manifold learned generator however metric properties generated manifolds still remain unexplored. work investigate riemannian geometry manifolds learned deep generative models. contributions summarized follows propose algorithm computing geodesic paths points generated manifold. used interpolate generated data points manifold using least amount change necessary enforcing points along path remain manifold. arclength minimal geodesic path distance metric points manifold natural measure similarity data points. continuous geodesic equation requires expensive second derivatives matrix inversions formulate efﬁcient numerical strategy computing discretized geodesic curves avoids computations. addition point-to-point geodesics paths show shoot geodesic initial starting position initial velocity next develop algorithm parallel translation tangent vectors along path generated maniconditions image smooth manifold depicted figure maps linear coordinates onto curvilinear coordinates construct composition multiple layers i.e. g◦g◦···◦g superscripts denote layer index. layer afﬁne mapping followed nonlinear activation function used subscripts denote component output denote weight matrix image smooth d-dimensional immersed manifold every point jacobian rank straightforward application chain rule true following conditions note condition enforced modeling phase selecting appropriate activation function. condition must checked training. also note condition sufﬁcient necessary could potentially less-than-maximal rank weight matrices middle layers long ﬁnal rank jacobian however checking general condition would require checking jacobian rank-d every possible input feasible. finally emphasize guaranteed immersed manifold. means locally diffeomorphic d-dimensional euclidean space globally self intersections. jacobian matrix provides tangent vectors latent space tangent vectors manifold. point jacobian matrix linear mapping tangent space tangent space practice computed partial derivative matrix backpropagation. riemannian metric provides inner product structure tangent vectors tangent space txm. induced metric ambient data space words thinking vectors living linear subspace euclidean product compute riemannian metric intuitively speaking curvature riemannian manifold measures extent metric deviates euclidean. precise mathematical explanation curvature refer standard texts riemannian geometry e.g. emphasize important distinction because manifold i.e. zero curvature fold. parallel translation moves tangent vector continuously along path using minimal amount change needed keep tangent manifold. operation provides means computing analogies i.e. taking change points manifold applying change third point experiments show tools used explore riemannian geometry manifolds learned deep generative models particular investigate curvature manifolds. demonstrate least architecture used experiments generated manifolds learned real images used experiments svhn surprisingly little curvature. result straight lines latent space generator curves manifold quite similar geodesics. help explain latent coordinates interpolations them tend give plausible changes generated images. geodesic curves sense smoothest possible transitions move constant speed minimize amount distance needed travel point another. conclusion latent coordinates approximate geodesics desirable property have checked interrogating riemannian geometry trained deep generative model. section illustrate connection deep generative models manifolds. deep generative model represents mapping lowdimensional latent space high-dimensional data space certain inverse gil. geodesic paths computed using numerical integration ordinary differential equation however notice computation christoffel symbols requires taking derivatives also matrix inverse show next subsection expensive calculations avoided start discrete counterpart geodesic energy efﬁcient discrete geodesic computation begin discretized curve sequence coordinates think approximating continuous curve thus time steps discrete time interval also corresponds discrete curve manifold using forward ﬁnite differences approximate velocity curve g)/δt. discrete analog gives energy curve fixing endpoints target start points geodesic path minimize discrete geodesic energy taking gradient descent remaining points curve zt−. gradient respect ∇zie notice gradient ﬁnite-difference second derivative space followed jacobian coming chain rule. second ﬁnite difference space component normal tangent space tgm. however project normal component gradient gradient tziz. finally geodesic path ﬁnding proceeds optimizing curve coordinates using gradient descent gradient gradient descent algorithm computing discretized geodesics avoids expensive christoffel symbol calculations still require computation jacobian generator deep generative models jacobian expensive. however make additional speed models corresponding encoder function i.e. mapping models e.g. vaes encoder mean isn’t nonlinear. example take sheet paper draw straight line bend sheet paper shape without creasing surface metrically equivalent euclidean space straight line drew geodesic curve length. words surface zero curvature example rolling paper famous swiss roll results surface highly nonlinear nonetheless zero curvature. section develop three algorithms riemannian computations manifold represented deep generative network geodesic interpolation between points manifold parallel translation tangent vector along path manifold geodesic shooting initial point velocity manifold. begin general discussion geodesic equation riemannian manifold. consider objects deﬁned coordinate space however point objects corresponding unique counterpart manifold mapping represent riemannian metric symmetric positive deﬁnite matrix ﬁeld deﬁned point latent coordinate space given formula jacobian signiﬁcantly faster. imagine moving discrete curve points negative gradient direction along mapping direction jacobian produce equivalent direction coordinates. results following modiﬁed faster-to-compute gradient replaces although modiﬁed gradient longer gradient discrete curve energy move initial direction. also descent modiﬁed gradient direction ﬁxed point gradient descent. ﬁnal geodesic path algorithm given algorithm given geodesic path point point transfer change change third point type analogy performed three steps compute initial velocity geodesic parallel translate velocity along geodesic velocity shoot geodesic segment. euclidean space operations would take difference consider vector based shoot geodesic adding parallel translation non-ﬂat manifolds moves tangent vector along manifold little change possible still enforcing vector stay tangent. operation preserves inner product tangent vectors such preserves length translated tangent vector. concrete example imagine sphere tangent vector north pole. rotate sphere tangent vector parallel translation along path swept rotation. assume already discrete path coordinates tangent vector initial point manifold. small step parallel translation approximately equivalent euclidean translation vector however vector position slightly tangent space. corrected applying minimal rotation bring vector tangent space. note using singular value decomposition jacobian left singular vectors give orthonormal basis tangent space. rotation onto basis equivalent projection followed rescaling vector back it’s original length. repeating process time step along curve gives parallel translation routine summarized algorithm given starting point starting velocity unique geodesic initial conditions euclidean space intuitively says given starting point velocity straight line initial conditions. compute geodesic shooting geodesic path initial conditions connection geodesic equation parallel translation previous subsection. geodesic equation says velocity geodesic moves parallel translation along geodesic. therefore compute discrete geodesic step taking small step current velocity direction followed updating velocity point parallel translation. process detailed algorithm section conduct extensive empirical study proposed algorithms various riemannian geometry computations context deep generative models. work variational autoencoder generative model choice however proposed algorithms equally applicable popular generative models generative adversarial network pixelcnn figure comparing linear interpolation geodesic interpolation pair points manifold induced generator trained data hyperbolic paraboloid. left true surface hyperbolic paraboloid middle surface overlaid curves linear geodesic interpolation right linear geodesic interpolation curves table architectural details model used celeba svhn datasets. architecture generator reverse encoder conv layers replaced transposed convolutions additional ﬁnal deconv layer size since difﬁcult visualize high dimensional real data manifolds illustrate geodesic traversal using simple analytically deﬁned manifold. particular hyperbolic paraboloid surface three dimensions deﬁned sample data sample points manifold activations. encoder outputs mean variance approximate posterior. decoder reverse architecture encoder maps dimensional latents three dimensional points manifold. exponential linear units resulting generator mapping differentiable although elus result mapping ensure generate manifold. also proposed algorithms valid require ﬁrst derivatives generator. train using minibatch stochastic gradient descent batch size learning rate minibatch iterations. pick points reasonably away analytically deﬁned hyperbolic paraboloid latent space trained using encoder context represents mean approximate posterior. corresponding points manifold obtained algorithm estimate geodesic connecting points compare curve traced generator’s manifold linear interpolation fig. visualizes true shape analytically deﬁned hyperbolic paraboloid along shape manifold learned vae’s generator also visualize geodesic linear interpolation curves points learned manifold curves dimensional latent generated manifold pick real image dataset corresponding point generated manifold. show images linear geodesic interpolation curves along arclengths celeba svhn respectively. although geodesic curve manifold gives shorter arclength linear interpolation space difference pronounced observed earlier experiment synthetic manifold. suggests generated manifolds learned architecture celeba svhn although nonlinear little curvature. take step look fr´echet mean chosen points generated manifold comparing linear mean space. fr´echet mean point manifold minimizes total sum-of-squared geodesic distance points set. setting input data points fr´echet mean deﬁned solution optimization problem real images celeba constructed randomly selecting images dataset value chosen pair attributes. construct four sets consisting images corresponding attributes clearly brings differences linear geodesic interpolation paths shorter geodesic curve manifold traced longer curve latent space. real manifolds section investigate riemannian geometry generated manifolds learned real images carrying computations geodesic interpolation geodesic mean comparing corresponding linear counterparts space. real image datasets experiments celeba. consists face images celebrities. center-cropped images shape used several earlier works using training vae. svhn consists house numbers obtained google street view images. cropped digits shape training provided part dataset. implementation details. architecture encoder celeba svhn shown table architecture generator reverse encoder architecture additional transposed convolution layer outputs image. latent dimension kept datasets. model trained minibatch iterations using adam learning rate trices visualized fig. celeba. embedding based geodesic distances visually seem give slightly tighter concentration around groups compared embedding based linear distances. also calculate eigenvalues matrices plot fig. eigenvalues explain whether data isometrically embedded euclidean space eigenvalues non-negative euclidean embedding possible dimension euclidean space number nonzero eigenvalues. presence negative eigenvalues demonstrate space nonzero curvature exact euclidean embedding impossible. magnitude negative eigenvalues measure manifold distances deviating euclidean i.e. measure much curvature manifold has. expected linear distance matrix resulted exactly positive eigenvalues exactly zero eigenvalues geodesic distance matrix negative eigenvalues small magnitude compared positive eigenvalues. strongly indicates generated manifold curvature close zero. analogy problem deﬁned context images want image related related reuse four celeba groups constructed earlier experiments. take geodesic mean group geodesic mean group take randomly selected test image attributes geodesic analogy ﬁrst compute geodesic initial velocity vector parallel translated along geodesic connecting using algorithm algorithm shoot geodesic length geodesic along parallel translated vector. point geodesic expected similar semantic relation related closed) respectively. corresponding points vae’s generated manifold applying function images. fig. visualizes fr´echet means linear means four groups images. fr´echet means similar appearance linear means latent space. again indicates limited curvature manifold. however certainly subtle differences indicates curvature playing least role. section analyze well geodesic distances aligned groupings images based ground truth attributes. reuse four groups images constructed earlier section celeba experiment. addition also construct groups images svhn group consisting randomly sampled images digit. apply points corresponding points generated manifold compute linear geodesic distances pair points. gives linear geodesic distance matrices size celeba svhn. calculate scores distance matrix attribute label total number data points. score essentially measures ratio intra-group squared distances total squared distances higher value indicating better agreement attribute based grouping distances. score already normalized squared distances directly comparable across linear geodesic distance matrices. shown table obtain slightly higher scores geodesic distances compared linear distances indicating geodesic distances group similar images slightly closer together linear distances. figure eigenvalues matrices four groups images celeba dataset left eigenvalues. right zooming lowest eigenvalues. note vertical axis scale much smaller right plot. generative models. experiments show models represent real image data manifolds surprisingly little curvature. consequently straight lines latent space relatively close geodesic curves manifold. fact explain traversal latent space results visually plausible changes generated data curvilinear distances original data metric roughly preserved. however experiments limited single type deep network real image data sets investigation phenomenon warranted identify architectures datasets curvature plays prominent role. also even results presented here role curvature completely discounted still differences latent distances geodesic distances nuanced effects certain applications. believe exploring riemannian geometry deep generative models using tools developed paper important step understanding high-dimensional nonlinear spaces models learn. figure linear mean vector analogy geodesic parallel translated vector analogy first rows change black hair blond last rows change closed mouth open. responding test image answer linear analogy problem taken image fig. shows results geodesic linear analogies different attribute combinations. linear analogy visually quite close geodesic analogy suggests generated manifold curvature.", "year": 2017}