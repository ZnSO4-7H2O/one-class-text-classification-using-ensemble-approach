{"title": "A Unified Framework for Probabilistic Component Analysis", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present a unifying framework which reduces the construction of probabilistic component analysis techniques to a mere selection of the latent neighbourhood, thus providing an elegant and principled framework for creating novel component analysis models as well as constructing probabilistic equivalents of deterministic component analysis methods. Under our framework, we unify many very popular and well-studied component analysis algorithms, such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Locality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of which have no probabilistic equivalents in literature thus far. We firstly define the Markov Random Fields (MRFs) which encapsulate the latent connectivity of the aforementioned component analysis techniques; subsequently, we show that the projection directions produced by all PCA, LDA, LPP and SFA are also produced by the Maximum Likelihood (ML) solution of a single joint probability density function, composed by selecting one of the defined MRF priors while utilising a simple observation model. Furthermore, we propose novel Expectation Maximization (EM) algorithms, exploiting the proposed joint PDF, while we generalize the proposed methodologies to arbitrary connectivities via parameterizable MRF products. Theoretical analysis and experiments on both simulated and real world data show the usefulness of the proposed framework, by deriving methods which well outperform state-of-the-art equivalents.", "text": "abstract. present unifying framework reduces construction probabilistic component analysis techniques mere selection latent neighbourhood thus providing elegant principled framework creating novel component analysis models well constructing probabilistic equivalents deterministic component analysis methods. framework unify many popular well-studied component analysis algorithms principal component analysis linear discriminant analysis locality preserving projections slow feature analysis probabilistic equivalents literature thus far. ﬁrstly deﬁne markov random fields encapsulate latent connectivity aforementioned component analysis techniques; subsequently show projection directions produced also produced maximum likelihood solution single joint probability density function composed selecting deﬁned priors utilising simple observation model. furthermore propose novel expectation maximization algorithms exploiting proposed joint generalize proposed methodologies arbitrary connectivities parametrizable products. theoretical analysis experiments simulated real world data show usefulness proposed framework deriving methods well outperform state-of-the-art equivalents. uniﬁcation frameworks machine learning provide valuable material towards deeper understanding various methodologies also form ﬂexible basis upon extensions easily built. ﬁrst attempts unify methodologies made seminal work models factor analysis principal component analysis mixtures gaussian clusters linear dynamic systems hidden markov models independent component analysis uniﬁed variations unsupervised learning single basic generative model. deterministic component analysis uniﬁcation frameworks proposed previous works provide signiﬁcant insights methods principal component analysis linear discriminant analysis laplacian eigenmaps others jointly formulated e.g. least squares problems mild conditions general trace optimisation problems. nevertheless several probabilistic equivalents e.g. formulated date uniﬁcation framework proposed probabilistic component analysis. motivated latter paper propose ﬁrst uniﬁed framework probabilistic component analysis. based markov random fields framework uniﬁes component analysis techniques whose corresponding deterministic problem solved trace optimisation problem without domain constraints parameters principal component analysis linear discriminant analysis locality preserving projections slow feature analysis framework provides insight component analysis methods probabilistic perspective. entails providing probabilistic explanations data hand explicit variance modelling well reduced complexity compared deterministic equivalents. features especially useful case methods probabilistic equivalent exists literature lpp. furthermore framework generate novel component analysis techniques merely combining products mrfs arbitrary connectivity. rest paper organised follows. initially introduce previous work highlighting properties proposed framework subsequently formulate joint complete-data probability density function observations latent variables. show maximum likelihood solution joint co-directional solutions obtained deterministic changing prior latent distribution which show models latent dependencies thus determines resulting technique. using fully connected obtain pca. choosing product fully connected connected within-class data derive lda. derived choosing locally connected ﬁnally produced joint prior linear markov-chain. based aforementioned subsequently propose expectation maximization algorithms finally sec. utilising synthetic real data demonstrate usefulness advantages family probabilistic component analysis methods. important contribution paper lies proposed uniﬁcation probabilistic component techniques giving rise ﬁrst framework reduces construction probabilistic component analysis models design appropriate prior thus deﬁning latent neighbourhood. nevertheless novelties arise methods generated framework. section review state-of-the-art deterministic probabilistic sfa. highlight novelties advantages proposed framework entails wrt. alternative formulation. throughout paper consider zero mean -dimensional observations length represented matrix methods discover -dimensional latent space preserves certain properties total scatter matrix identity matrix. optimal projection basis recovered probabilistic approaches independently proposed probabilistic generative model adopted rf×n matrix relates latent variable observed samples noise assumed isotropic gaussian model. motivation that latent variables oﬀer parsimonious explanation dependencies arising observations. several probabilistic latent variable models exploit class information recently proposed another related attempts made formulate plda. considering i-th sample c-th class generative model described adopt formulation instead equivalent maximizing trace between-class scatter matrix since facilitates following discussion probabilistic alternatives. note models become equivalent choosing common classes also disregarding matrix case solution given obtaining eigenvectors corresponding largest eigenvalues hence solution vastly diﬀerent obtained deterministic resembling solution problems retain maximum variance. fact learning diﬀerent class model reduces applying ppca class. best knowledge probabilistic model solution closely related deterministic probalocality preserving projections linear alternative laplacian eigenmaps obtain projections latent space preserves locality original samples. first deﬁne weights represent locality. common choices weights heat kernel constant weights ﬁnds projection basis matrix solving following problem diag diagonal matrix main diagonal vector vector ones). objective function chosen weights results heavy penalty neighbouring points mapped apart. therefore minimization ensures near projected features near well. best knowledge probabilistic models exist lpps. following show probabilistic version lpps arises choosing appropriate prior latent space consider case columns samples time series length slow feature analysis given sequential observation vectors output signal representation features change slowest time assuming linear mapping output representation minimizes slowness values deﬁned variance ﬁrst derivative formally computed summarizing following sections formulate uniﬁed probabilistic framework component analysis which incorporates special case produces probabilistic solution loading matrix similar direction deterministic make assumptions regarding number samples class provides ﬁrst best knowledge probabilistic model explains naturally incorporates special case provides variance estimates observations also latent dimension provides straightforward framework producing novel component analysis techniques. section present proposed maximum likelihood framework probabilistic component analysis show generated within framework also proving equivalence known deterministic models. firstly ease computation assume generative model i-th observation deﬁned order fully deﬁne likelihood need deﬁne prior distribution latent variables prove choosing priors deﬁned subsequently taking solution wrt. parameters explored. piece puzzle added recent work linear dynamical system prior used order provide derivation framework. formulating appropriate priors models unify subspace methods single probabilistic framework linear generative model along prior form diﬀerentiation amongst models lies neighbourhood potentials deﬁned. fact varying neighbouring system translated matrices functional form potentials essentially encapsulating latent covariance connectivity. e.g. ﬁnally following show estimation using potentials equivalent deterministic formulations lpp. special case already shown potential form framework produces projection direction following show substituting priors maximising likelihood obtain loadings co-directional deterministic lpps sfa. firstly substituting general prior likelihood obtain easy prove since diagonal matrices satisﬁes simultaneously diagonalises xbxt xbxt substituting matrices deﬁned consider cases separately. utilising rehence given eigenvectors total scatter matrix reformulated λwxmxt λwxxt thus given directions simultaneously diagonalise yields λwxlxt λwxdt therefore given directions simultaneously diagonalise x˜lxt ˜dxt finally utilising becomes λwxkxt λwxxt given directions simultaneously diagonalise xkxt shows solution following framework equivalent deterministic models sfa. direction depend estimated optimizing regards parameters. work provide update rules using framework. observe loading depend exact setting long diﬀerent. larger values correspond expressive discriminant local slower latents corresponds directly ordering solutions sfa. recover exact equivalence another limit required corrects scales. several choices natural choice case ﬁxes prior covariance latent variables forces xdxw case lpp. choice also discussed sfa. note case analogous corresponding eigenvalue covariance matrix since otherwise method result minor component analysis. following propose uniﬁed framework component analysis. framework treat priors undirected links prior contains directed links loops thus solved similarly linear dynamical system treat links undirected autoregressive component analysis order perform prior adopt simple elegant mean ﬁeld approximation theory essentially allows computationally favourable factorizations within framework. consider since diﬀerent models diﬀerent latent connectivities mean-ﬁeld inﬂuence latent point function afdepends model-speciﬁc connectivity calculating normalising integral priors given easily shown follows gaussian distribution mean-ﬁeld well distribution latent variable previously proposed probabilistic approaches variation mean case shifted mean ﬁeld addition method models per-dimension variance note order fully identify ppca proposed sfa. prior allows interpretations graphical model undirected directed dynamic bayesian network based undirected interpretation trivially framework described section leading autoregressive model able learn bi-directional latent dependencies. considering prior directed markov chain resort exact inference techniques applied dbns. fact straightforwardly reduced solving standard linear dynamical system also enforcing diagonal transition matrices setting complexity. proposed algorithm iteratively recovers latent space preserving characteristics enforced selected latent neighbourprobabilistic classiﬁcation. exploit probabilistic nature proposed em-lda order probabilistically infer likely class assignment unseen data. instead using inferred projection essentially utilise log-likelihood model. detail estimate marginal log-likelihood test point assigned class estimated utilising inferred model parameters along class model. note since posterior mean given depends observations excluding need store class mean estimated weighted average training data training data class proof concept provide experiments synthetic real-world data. experimentally validate equivalence proposed probabilistic models models belonging class experimentally evaluate performance models others class. synthetic data. demonstrate application proposed probabilistic techniques synthetic data generated utilising dimensionality reduction toolbox. detail compare corresponding deterministic formulations proposed probabilistic models. mainly qualitatively illustrate equivalence proposed methods furthermore variance modelling latent dimension em-lda clear prove beneﬁcial prediction-wise show following section. real data face recognition em-lda. common applications face recognition. therefore utilise various databases order verify performance proposed em-lda. detail utilise popular extended yale database well databases experiments span wide range variability various facial expressions illumination changes well pose changes. detail database used total images near frontal images subject. training randomly selected subset consisting images subject testing remaining images used. extended yale database utilised subset near frontal images subject random selection images subject used training rest images used testing. regarding focus facial expressions. ﬁrstly randomly select subjects. subsequently images portray varying facial expressions session using corresponding images session testing. related experiments compared em-lda deterministic fukunaga-koontz variant plda presence gaussian noise. used gradients image pixel features since experimentally veriﬁed improved results compared methods. errors compared method database accompanied increasing gaussian noise input shown fig. although plda oﬀers substantial improvement wrt. deterministic performs better fk-lda clear proposed em-lda outperforms compared variants. attributed explicit variance modelling models appears enable robust classiﬁcation. real data face visualisation em-lpp. typical applications neighbour embedding methods visualisation usually highdimensional data hand. particular lpps often used visualising faces providing intuitive understanding variance structural properties data order evaluate proposed em-lpp best knowledge ﬁrst probabilistic equivalent experiment frey faces database contains images captured sequential frames video sequence. apply similar experiment ﬁrstly perturbed images random gaussian noise subsequently apply em-lpp lpp. resulting space illustrated fig. clear deterministic unable cope added gaussian noise failing capture meaningful data clustering. note proposed em-lpp able well capture structure input data modelling pose expression within inferred latent space. paper introduced novel unifying probabilistic component analysis framework reducing construction probabilistic component analysis models selecting proper latent neighbourhood design latent connectivity. framework thus used introduce novel probabilistic component analysis techniques formulating latent priors products mrfs. shown speciﬁc priors used generate probabilistic models corresponding introduced ﬁrst favourable complexity-wise probabilistic equivalent lpp. finally means theoretical analysis experiments demonstrated various advantages proposed methods pose existing probabilistic deterministic techniques.", "year": 2013}