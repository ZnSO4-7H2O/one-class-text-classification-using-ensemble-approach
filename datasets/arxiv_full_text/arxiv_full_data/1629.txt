{"title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.", "text": "generate discrete sequences popular adopt autoregressive models teacher forcing which nevertheless causes exposure bias problem existing approach trains auto-regressive models maximize conditional probabilities next tokens based ground-truth histories. words training auto-regressive generative models exposed ground truths data distribution rather model distribution i.e. predictions. prohibits trained model take advantage learning context previous generated words make next prediction resulting bias difﬁculty approaching true underlying distribution another limitation teacher forcing inapplicable auto-regressive models latent random variables performed better autoregressive recurrent neural networks multiple tasks alternative attractive solution training autoregressive models using generative adversarial networks discussed problem prevented generative models able visit predictions training overall view generated sequences. suggest facilitate training autoregressive models additional discriminator setting. discriminator trained separate real versus generated sequences generative model able make knowledge discriminator improve itself. since discriminator trained entire sequence principle provide training signal avoid problem exposure bias. however nontrivial apply gans discrete data difﬁcult optimize generator using signal provided discriminator. fact usually hard push generated distribution real data distribution impossible moving generated sequence towards true high dimensional discrete state space. standard back-propagation fails discrete settings generator optimized using discriminator’s output reward reinforcement learning. unfortunately despite successes capturing continuous distributions application generative adversarial networks discrete settings like natural language tasks rather restricted. fundamental reason difﬁculty backpropagation discrete random variables combined inherent instability training objective. address problems propose maximum-likelihood augmented discrete generative adversarial networks. instead directly optimizing objective derive novel low-variance objective using discriminator’s output follows corresponds log-likelihood. compared original objective proved consistent theory beneﬁcial practice. experimental results various discrete datasets demonstrate effectiveness proposed approach. generative models appealing provide ways obtain insights underlying data distribution statistics. particular models play pivot role many natural language processing tasks language modeling machine translation dialogue generation. however generated sentences often unsatisfactory example often lack consistency long-term semantics less coherence high-level topics syntactics montreal institute learning algorithms universit´e montr´eal montr´eal canada department computing hong kong polytechnic university hong kong hong kong university science technology ivado. correspondence tong <tong.cheumontreal.ca>. formally train generator together discriminator original form discriminator trained distinguish generating distribution real data distribution generator trained maximize ex∼pθ namely objective generator optimize follows work related viewpoint casting training reinforcement learning problem moving reward signal monotone deﬁne normalized probability distribution bounded region guarantee integrability also maximum-entropy regularizer encourage diversity yielding regularized loss constant depending hence optimizing traditional basically equivalent optimizing kl-divergence kl). major problem approach always moves undesirable stability convergence. samples want change order adjust likelihood samples improve quality generator. however since initially generates sequences little chance generating good sequences order positive rewards. though dedicated pre-training variance reduction mechanisms help algorithm based moving reward signal still seems unstable work large scale datasets. therefore propose utilize information discriminator additional source training signals maximum-likelihood objective. employ importance sampling make objective trainable. novel training objective much less variance vanilla reinforcement learning approaches directly adopt reward signals. analysis discussions presented detail section even careful pre-training found policy difﬁculties positive stable reward signals discriminator. tackle limitations propose maximumlikelihood augmented discrete generative adversarial networks core model novel training objective sidesteps stability issue happening using discriminator output direct reinforcement learning reward. alternatively develop normalized maximum likelihood optimization target inspired importance sampling several variance reduction techniques order successfully optimize objective. procedure discovered independently hjelm context image generation. target brings several attractive properties proposed maligan. first theoretically consistent easier optimize second allows model maximize likelihood good behaviors also minimize likelihood behaviors help discriminator. equipped strengths model focuses improving itself gaining beneﬁcial knowledge well acquired excluding probable harmful behaviors. combined several proposed variance reduction techniques proposed maligan successfully stably models discrete data sequences basic framework discrete sequence generation data {xi}n coming underlying generating distribution training parameterized autoregressive probabilistic model work generate discrete data especially discrete sequential data setting deﬁnes framework training generative models posing minimax game discriminative model. goal generator match distribution real data distribution achieve this generator transforms noise sampled data sample following this discriminator trained distinguish samples coming used provide training signal generator. applying framework discrete data discontinuity prohibits update generator parameters standard back-propagation. tackle this employ typical reinforcement learning strategy directly uses discriminator’s output reward. practice problem usually solved reinforce-like algorithms baseline reinforcement learning order reduce variance. practice increase slowly combined objective discriminator ordinary proposed maligan algorithm shown algorithm proposed objective also theoretically guaranteed sound. following theorem show training objective approximately optimizes divergence kl||pθ) close optimal. what’s more objective still makes sense well trained optimal. theorem following theoretical guarantees training objective. addition attractiveness theory demonstrate gradient estimator practically produce better training signal generator original objective. similar discussions jective signiﬁcantly reduces variance training including theoretical practical analysis objective’s equivalence attractive properties. also show core algorithm combined several variance reduction techniques form full maligan algorithm discrete sequence generation. propose maximum-likelihood augmented discrete generative adversarial networks generate discrete data. maligan train discriminator standard objective employs. different gans novel objective generator optimize using importance sampling makes training procedure closer maximum likelihood training auto-regressive models thus stable less variance gradients. keep delayed copy generator whose parameters updated less often order stabilize training. basic property gans know optimal property pd+p therefore case target distribution maximum likelihood training deﬁne augmented target distribution objective attractive property ﬁxed distribution training i.e. sufﬁciently trained always approximately data generating disdeﬁning gradient tribution following importance sampling formula assume delayed generator step behind current update experiments. importance sampling procedure discovered independently propose optimize generator using following novel gradient estimator training auto-regressive models teacher forcing serious problem exposure bias namely model trained demonstrated behaviors also want trained free-running behaviors. positive baseline model ﬁrst generates samples tries adjust probabilities generated samples trying reinforce best behaviors exclude worse behaviors relatively mini-batch. proposed renormalized objective maligan supports much stable training behavior objective standard gan. nevertheless long sequence generation procedure consists multiple steps random sampling better integrate following advanced variance reduction techniques. size mini-batch. using monte carlo tree search brings several beneﬁts. first allows different steps generated sample adjusted different weights. second gives stable estimator partition function properties dramatically reduce variance proposed estimator. dealing long sequences model result accumulated variance. alleviate issue signiﬁcantly reduce variance clamping input using training data time steps switch free running mode remaining time steps. training procedure inspired ranzato slowly move towards original setting reinforcement learning perspective e.g. inclusive free running auto-regressive model viewed agent exploring state space getting reward exploration. model tries adjust probability exploration paths according reward. however gradient estimator would drastically inefﬁcient almost generated paths small discriminator output. unfortunately common training cannot even solved carefully selected baseline. maligan objective however partition function estimated using samples minibatch helps dealing dilemma. choose example baseline weights generated paths zero probability path adjusted according absolute value discriminator output relative quality minibatch. ensures model always learn something long exist generations better others mini-batch. furthermore previous theorem ensures consistency mini-batch level normalization procedure. theoretical point view normalization procedure also helps. although ﬁrst glance optimal prove estimating seems introduce additional variance model. however using estimator fact reduces variance following reason actually function singularity region data space even careful pre-training region making ratio blow since almost target impossible samples reasonable size mini-batch actual distribution sampling regularized distribution importance sampling estimate training objective small mini-batches actually normalizedweights importance sampling based ep\\ω pθ]/ep\\ω since monte carlo estimator much variance estimate ep∇θ ep\\ω practical mini-batch training settings view importance sampling distribution objective much less variance compared importance sampling inﬁnite singularity. estimating ep\\ω important order reduce variance mini-batch training setting. much larger effects changing low-level variables. motivated observation mini-batch ﬁrst draw mini-batch samples highlevel latent variables high level value draw number level data samples re-estimate partition function lowlevel samples generated high-level samples. lower-level sampling much smaller variance model receive better gradient signals weights provided discriminator. sampling principle corresponding applying mixed mle-mali training discussed autoregressive settings. case ﬁrst sample data samples ﬁrst words network generate samples next mini-batch. refer full algorithm sequential maligan mixed training summarized algorithm sample minibatch sequences {yi}m keeping ﬁrst steps {yi}m sample minibatch sequences {xi}m time step update discriminator taking gradient ascend discriminator loss. sample minibatch sequences {xi}m sample length larger minibatch clamp generator ﬁrst words freely model generate samples till sequence. update generator applying mixed mle-mali gradient update reason single real sample based renormalization beneﬁcial summarized around elements. first consider sample training set. ﬁrst words completed >n|s≤n model. conditional distribution much simpler full distribution namely many generative models multiple layers randomnesses. example auto-regressive models samples generated multiple sampling steps. examples include hierarchical generative models like deep boltzmann machines deep belief networks second normalization scheme makes model robust mode missing common failure pattern training gans single sample based renormalization ensures every real sample model receive moderately strong training signal perform better generating conditioned however batch-wise renormalization basic maligan possible might completions large training samples mini-batch receives little gradient signals. examine effectiveness proposed algorithms conduct experiments three discrete sequence generation tasks. achieve promising results three tasks including standard challenging language modeling task. empirical results following analysis demonstrate soundness maligan show robustness overﬁtting. ﬁrst evaluate maligan binarized image generation task mnist hand-written digits dataset similar hjelm original datasets samples training testing sets respectively. split training randomly selected samples validation. adopted generator deep convolutional neural network based dcgan architecture generate discrete samples sample generator’s output binomial distribution. adopt algorithm maligan training single latent variable renormalization technique variance reduction. compare proposed maligan models trained using discriminator’s output direct reward also train generator network architecture output discriminator weight generated samples. denote reinforce-like model. comparison results shown figure figure ﬁgures ﬁrst line training losses generator discriminator proposed maligan. training process maligan variance reduction techniques stable loss curve meaningful. bottom ﬁgures figure samples generated reinforce-like model maliexamine effectiveness model chinese poem generation task. typically genres chinese poems. refer poem- poem- consisting chinese characters short sentence respectively. dataset provided split standard generator one-layer lstm hidden units poem- poem-. discriminators two-layer bilstms hidden neurons. denote models trained algorithm algorithm maliganbasic maligan-full. choose compared models auto-regressive model architecture trained maximum likelihood seqgan following report bleu- scores table maligan-full obtained best bleu- scores tasks maligan-basic next best. clearly lagged behind despite architecture attributed inherent defect teacher-forcing training framework. pointed previous researchers wiseman rush bleu might proper evaluation metric also calculate perplexity four models obtaining qualitatively ﬁgures models perform training procedure. although oscillations maligan-basic maligan-full achieved lower perplexity. especially poem- figure proposed models prevent overﬁtting ended that. comparison training curve maligan-basic maligan-full latter less variance. demonstrates effectiveness advanced variance reduction techniques full model. peak curve poem- figure however unlikely result overﬁtting recovered fast continued convergence till end. fact harder train stable model poem- poem-. conjecture resulted intricate mutual inﬂuence improper evaluation small training data size. also examine proposed algorithm challenging task sentence-level language modeling considered fundamental task applications various discrete sequence generation tasks. explore possibilities limitations algorithm conduct extensive experiments standard penn treebank dataset parameter searching model ablations. evaluation report sentence-level perplexity averaged perplexity sentences test set. simplicity efﬁciency adopt -layer generator setting baseline model trained standard teacher forcing. bi-directional network discriminator. stabilize training provide good initialization generator ﬁrst pre-train generator training using teacher forcing train models maligan-basic maliganfull. maligan-basic trained algorithm without mcts. maligan-full trained algorithm variance reduction techniques included. note computational cost mcts large remove sentences longer words training set. beginning training pre-train discriminator make reliable enough provide informative correct signals generator. perplexity shown table achieved best performing model hidden neurons dimensions word embeddings. table simplest model trained maligan reduced perplexity baseline effectively. basic full model i.e. maliganbasic maligan-full obtained notably lower perplexity compared model. although dataset much difﬁcult obtain results consistent table encouraging model robust overﬁtting consideration relative small size data. results strengthen belief realize algorithm even larger datasets leave future work. positive result demonstrates effectiveness maligan whose primary component novel optimization objective propose besides also gain insights model ablation tests advanced variance reduction techniques provided section combined perplexity curve figure advanced techniques maliganfull performed stable training extent achieve lower perplexity scores maligan-basic. believe fruitful techniques beneﬁcial similar problem settings. improve performance discrete auto-regressive models researchers tackle exposure bias problem discussed detailed problem occurs training algorithm prohibits models exposed predictions training. second issue discrepancy objective training evaluation metric testing analyzed ranzato summarized loss-evaluation mismatch wiseman rush typically objectives training auto-regressive models maximize word-level probabilities test-time often evaluate models using sequencelevel metrics bleu alleviate issues straightforward evaluation metrics objective training phase. metrics often discrete cannot utilized standard back-propagation researchers generally seek help reinforcement learning. ranzato exploits reinforce algorithm proposes several model variants well situate algorithm text generation applications. shares similar idea directly optimizes image caption metrics policy gradient methods exists third issue namely label bias especially sequence-to-sequence learning framework obstacles trained models optimized globally addresses abovementioned issues training autoregressive models propose formulate problem setting generative adversarial networks. initially proposed goodfellow generative adversarial network attracted attention provides powerful framework generate promising samples min-max game. researchers successfully applied generate promising images conditionally unconditionally realize image manipulation super-resolution produce video sequences despite successes feasibility advantage applying text generation restrictedly explored noteworthy. appealing generate discrete sequences using discussed above. generative models able utilize discriminator’s output make information distribution inaccessible trained teacher forcing however nontrivial train discrete data discontinuity nature. instability inherent training makes things even worse lamb exploits adversarial domain adaption regularize training recurrent neural networks. applies discrete sequence generation directly optimizing discrete discriminator’s rewards. adopt monte carlo tree search technique similar technique employed improves response generation using adversarial learning. bornschein bengio inspired authors propose mini-batch reweighting training latent variable models discrete variables. however make inference network infeasible setting. work also closely related norouzi norouzi propose work objective conditional generation setting. case situation similar rewards bleu scores available. however conditional generation metrics bleu scores decomposable time steps property make able directly sample augmented distributions possible sequence-level gans e.g. language modeling. importance sampling train model. spite great popularity continuous datasets images gans haven’t achieved equivalent success discrete domains natural language processing. observed main cause discriminator almost perfectly discriminate good samples ones notoriously difﬁcult pass information generator difﬁculty credit assignment discrete computation inherent instability algorithms applied dynamic environments sparse reward. work take different approach. start ﬁrst maximum likelihood training objective importance sampling combined discriminator output derive novel training objective. argue although objective looks similar objective used reinforcement learning normalization fact reduce variance estimator ignoring region data space around singularity generator almost zero probability samples from. namely estimating partition function using samples approximately normalized importance sampling another distribution much lower variance c.f. section practically single real sample normalization process combined mixed training successfully avoided missing mode problem providing equivalent training signal mode. ingly robust overﬁtting. teacher forcing prone overﬁt maximizing likelihood training data model easily regularities also noise data. however model generator tries much noise data generated sample look good hopefully discriminator able capture differences generated real samples easily. references andor daniel alberti chris weiss david severyn aliaksei presta alessandro ganchev kuzman petrov slav collins michael. globally normalized transition-based neural networks. corr abs/. bengio samy vinyals oriol jaitly navdeep shazeer noam. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems chelba ciprian mikolov tomas schuster mike brants thorsten koehn phillipp. billion word benchmark measuring progress statistical language modeling. corr abs/. kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems lamb alex goyal anirudh zhang ying zhang saizheng courville aaron bengio yoshua. professor forcing algorithm training recurrent networks. advances neural information processing systems ledig christian theis lucas husz´ar ferenc caballero jose aitken andrew tejani alykhan totz johannes wang zehan wenzhe. photo-realistic single image super-resolution using generative adversarial network. arxiv preprint arxiv. mathieu michael couprie camille lecun yann. deep multi-scale video prediction beyond mean square error. proceedings international conference learning representations nguyen yosinski jason bengio yoshua dosovitskiy alexey clune jeff. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. norouzi mohammad bengio samy chen zhifeng jaitly navdeep schuster mike yonghui schuurmans dale. reward augmented maximum likelihood neural structured prediction. nips norouzi mohammad bengio samy jaitly navdeep schuster mike yonghui schuurmans dale reward augmented maximum likelihood neural structured prediction. advances neural information processing systems papineni kishore roukos salim ward todd wei-jing. bleu method automatic evaluation proceedings anmachine translation. nual meeting association computational linguistics association computational linguistics radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. ranzato marc’aurelio chopra sumit auli michael zaremba wojciech. sequence level training recurrent neural networks. proceedings international conference learning representations reed scott akata zeynep xinchen logeswaran lajanugen schiele bernt honglak. generproceedative adversarial text-to-image synthesis. ings international conference machine learning serban iulian sordoni alessandro bengio yoshua courville aaron pineau joelle. building end-toend dialogue systems using generative hierarchical neural network models. aaai- serban iulian sordoni alessandro lowe ryan charlin laurent pineau joelle courville aaron bengio yoshua. hierarchical latent variable encoderdecoder model generating dialogues. thirty-first aaai conference artiﬁcial intelligence silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc dieleman sander grewe dominik nham john kalchbrenner sutskever ilya lillicrap timothy leach madeleine kavukcuoglu koray graepel thore hassabis demis. mastering game deep neural networks tree search. nature sønderby casper kaae caballero jose theis lucas wenzhe husz´ar ferenc. amortised inference image super-resolution. proceedings international conference learning representations sordoni alessandro galley michel auli michael brockett chris yangfeng mitchell margaret jian-yun jianfeng dolan william neural network approach context-sensitive generation conversational responses. hlt-naacl wiseman rush alexander sequence-tosequence learning beam-search optimization. proceeddings conference empirical methods natural language processing lantao zhang weinan wang yong. seqgan sequence generative adversarial nets policy gradient. thirty-first aaai conference artiﬁcial intelligence zhang hongsheng zhang shaoting huang xiaolei wang xiaogang metaxas dimitris. photo-realistic image synthesis stacked generative adversarial networks. arxiv. jun-yan kr¨ahenb¨uhl philipp shechtman efros alexei generative visual manipulation proceedings european natural image manifold. conference computer vision", "year": 2017}