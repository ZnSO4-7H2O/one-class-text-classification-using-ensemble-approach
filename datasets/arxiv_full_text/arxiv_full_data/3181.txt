{"title": "Revisiting Bayesian Blind Deconvolution", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that couples the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements.", "text": "blind deconvolution involves estimation sharp signal image given blurry observation. problem fundamentally ill-posed strong priors sharp image blur kernel required regularize solution space. naturally leads standard estimation framework performance compromised unknown trade-oﬀ parameter settings optimization heuristics convergence issues stemming non-convexity and/or poor prior selections. mitigate problems number authors recently proposed substituting variational bayesian strategy marginalizes high-dimensional image space leading better estimates blur kernel. however underlying cost function involves integrals closed-form solution complex function-valued arguments thus losing transparency map. beyond standard bayesian-inspired intuitions thus remains unclear exactly mechanism methods able operate rendering understanding improvements extensions diﬃcult. elucidate issues demonstrate methodology recast unconventional problem particular penalty/prior couples image blur kernel noise level principled way. unique penalty number useful characteristics pertaining relative concavity local minima avoidance scale-invariance allow rigorously explain success including existing implementational heuristics approximations. also provides strict criteria choosing optimal image prior that perhaps counter-intuitively need reﬂect statistics natural scenes. challenge prevailing notion successful blind deconvolution providing transparent platform introducing enhancements extensions. experimental results using derived modiﬁcations corroborate theoretical conclusions. blind deconvolution problems involve estimation latent sharp signal interest given observation compromised unknown ﬁltering process. although relevant algorithms analysis apply general setting paper focus particular case blind image deblurring unknown convolution blur operator well additive noise corrupt image capture underlying natural scene. blurring undesirable consequence often accompanies image formation process arise example camera-shake acquisition. blind image deconvolution deblurring strategies recover sharp image blurry compromised observation long-standing problem remains active research topic moreover applications extend widely beyond standard photography astronomical bioimaging signal processing data eliciting particular interest operator noise term assumed zero-mean gaussian covariance task blind deconvolution estimate sharp image blur kernel given blurry observation mostly assuming represent ﬁltered versions original pixel-domain images. non-invertible high frequency information lost observation process thus even known nonblind estimation ill-posed. however blind case also unknown diﬃculty exacerbated considerably many possible image/kernel pairs explaining observed data equally well. alleviate problem prior assumptions must adopted constrain space candidate solutions naturally suggests bayesian framework. section brieﬂy review common classes bayesian algorithms blind deconvolution used literature maximum posteriori estimation variational bayes later detail fundamental limitations include heuristic implementational requirements complex cost functions diﬃcult disentangle. section uses ideas convex analysis reformulate bayesian methods promoting greater understanding suggesting useful enhancements rigorous criteria choosing appropriate image priors. section situates theoretical analysis within context existing analytic studies blind deconvolution notably seminal work discusses relevance natural image statistics. learning noise variances later addressed section experiments carried section provide corroborating empirical evidence theoretical claims. finally concluding remarks contained section space. recently natural image statistics invoked design prior models e.g. estimation using priors proposed blind deconvolution e.g. speciﬁcations diﬀer basic idea mode although straightforward many problems existing approaches including ineﬀective global minima e.g. poor priors lead degenerate solutions like delta kernel many local minima subsequent convergence issues. therefore generation useful solutions requires delicate balancing various factors dynamic noise levels trade-oﬀ parameter values heuristic regularizers salient structure selection statistics literature. estimated obtained conventional non-blind deconvolution techniques. motivation type strategy based inherent asymmetry dimensionality image relative kernel integrating high-dimensional image estimation process accurately estimate remaining lowdimensional parameters ization computationally intractable integral given realistic image priors. consequently variational bayesian strategy used approximate troublesome marginalization similar idea also proposed number authors represents zero mean gaussian variance prior distribution role decomposition become apparent below. also abuse notation characterize discrete distribution case integral reduced summation. note prior distributions expressible supergaussian therefore varying degrees favor sparse contribution theory show restrict form structural assumptions updates actually computed albeit approximately. purpose common constraint must factorized namely sometimes called mean-ﬁeld approximation approximation eﬀectively utilizing revised upper bound compared original type problem minimizing bound considerably simpliﬁed problematic marginalization eﬀectively decoupled however solving iteration shown full covariance matrix conditioned denoted must computed. possible closed form requires operations number pixels image. computationally impractical reasonably-sized images diagonal approximation must adopted assumption equivalent incorporating additional factorization process requisite update rules shown algorithm numerous methods fall within category implementational diﬀerences. note also full distributions generally needed; certain suﬃcient statistics required analogous standard eﬃciently computed using techniques produced algorithm suﬃcient statistic computed using alternative methodology applies ﬁnite gaussian scale mixtures. however resulting updates nonetheless equivalent algorithm shown proof theorem presented later. possibly well-motivated principle type approach relies rather severe factorial assumptions compromise original high-level justiﬁcations. fact denotes standard divergence distributions posterior generally highly coupled divergence typically quite high indicating associated approximation potentially poor. therefore reason believe anywhere near maximizer ultimate goal motivation type begin outstanding issues persist well. example free energy cost function involves integration function-valued arguments nearly transparent standard estimation moreover practical depends simplicity ignored image boundary eﬀects presenting computation algorithm fact complete expression described appendix proof theorem additionally algorithm present form includes modest diﬀerentiability assumption updating suﬃcient statistics appropriate schedule reducing noise variance iteration implements form coarse-to-ﬁne multiresolution estimation scheme potentially improving convergence rate therefore becomes diﬃcult rigorously explain exactly often empirically successful practice comparisons) decide image priors operate best framework. levin suggested high level marginalization latent sharp image using natural-image-statistic-based priors good idea overcome problems faced estimation argument directly motivates substituting rather providing explicit rationalization thus intend meticulously investigate exact mechanism operates explicitly accounting approximations assumptions note that discussed later certain algorithms perform reasonably well carefully balanced additional penalty factors tuning paramters added however direct comparisons using basic probabilistic model perform substantially better even achieving state-ofthe-art performance without additional tuning. involved drawing convex analysis sparse estimation concepts endeavor naturally motivates extensions framework simple prescription choosing appropriate image prior overall hope demystify providing entry point broader improvements robust non-uniform blur estimation. several surprising possibly counterintuitive conclusions emerge investigation strongly challenge much prevailing wisdom regarding bayesian algorithms advantageous blind deconvolution. include conventional beneﬁts marginalization latter. fact prove section underlying cost functions formally equivalent ideal noiseless environments given factorial assumptions required practical algorithms. instead intrinsic mechanism built allows locally minimizing solutions largely avoided even using highly nonconvex discriminative priors needed distinguish blurry sharp images. represents completely perspective relative advantages technically conclusions apply uniform blur model described many underlying principles nonetheless applied general models. fact already obtained success complex non-uniform multi-image models using similar principles e.g. drawing ideas section completely reformulate methodology elucidate behavior. profoundly demonstrate actually equivalent using unconventional estimationlike cost function particular penalty couples image blur kernel noise well-motivated fashion. procedure removes ambiguity introduced approximation subsequent diagonal covariance approximation reduction heuristic contribute still somewhat mysteriously eﬀectiveness allow pinpoint exact reasons represents improvement conventional estimations form provides speciﬁc criteria choosing image prior following work derivative domain images ease modeling better performance meaning denote lexicographically ordered image derivatives sharp blurry images respectively obtained particular derivative ﬁlter. given convolution commutative operator blur kernel unaltered. extension multiple ﬁlters typically image dimension follows naturally. simplicity notation however omit explicit referencing multiple ﬁlters throughout paper although related analysis follow straightforward manner. rn×m rn×l convolution matrices constructed blur kernel sharp image respectively. introduce matrix rl×m j-th binary vector indicating j-th element appears equivalent norm i-th column also viewed eﬀective norm accounting boundary eﬀects. element-wise magnitude given technically k¯kk depends index image pixels makes diﬀerence near image boundaries. prefer avoid explicit notational dependency keep presentation concise although proofs appendix consider i-dependency relevant. subsequent analysis also omit dependency although results carry general case. true quantities depend k¯kk e.g. parameter deﬁned later erence zero relatively little distinction nonzero values) notion relative concavity induces ordering many common sparsity promoting functions. aggressive promoting sparsity concave relative exammentioned previously algorithm eﬃciently implemented using image prior expressible form however purposes require alternative representation roots convex analysis. based shown prior given also represented maximization scaled gaussians diﬀerent variances leading alternative representation non-negative energy function; associated exponentiated factor sometimes treated hyperprior although generally integrate one. determines form ultimately play central role penalizes images explored results section. proofs deferred appendix reformulation closely resembles quadratic data ﬁdelity term combined additive image kernel penalties. penalty unlike incorporated standard schemes meaning however quite unlike penalty image pixels dependent noise level kernel parameter ratio noise level squared kernel norm. moreover general easily shown function non-separable meaning possible functions remainder section explore consequences crucial previously unexamined distinction typical formulations. therefore underlying cost function eﬀectively diﬀerent regular noiseless setting surprising conclusion seems counter much prevailing understanding deconvolution algorithms. simple example described next serve motivate perhaps paradoxically still outperform even ideal noiseless scenarios. brief diﬀerent solutions still possible decreasing sequence used optimize techniques lead radically diﬀerent optimization trajectory terminating diﬀerent locally minimizing solutions. later sections provide rigorous analysis including aﬀect convergence paths leading several insights regarding performance. finally section address issue choosing optimal image prior equivalent choosing optimal brieﬂy illustrate distinction confounding factors conveniently removed. purposed consider simpliﬁed noiseless situation optimal value zero. based corollary know whenever therefore goes zero cost functions become exactly equivalent given prior selection thus share globally optimal solution mentioned section shown algorithm however essentially deconvolution algorithms begin large value gradually reduce towards minimal value iterations proceed part multi-resolution approach designed good solutions. underlying cost functions equivalent behave diﬀerently argue extensively ultimate advantage lies. although details deferred later sections embedding leads powerful adjustment image penalty curvature iteration smoothing local minima local solutions largely avoided. contrast employs static image penalty easily lured suboptimal basins attraction. globally optimal solution remaining penalty reduces norm represents count nonzero elements sometimes considered canonical metric quantifying sparsity. thus eﬀectively reduces course attempt solve directly diﬃcult combinatorial problem nature. instead begin large gradually reduce towards zero described algorithm updating becomes small cost functions behave diﬀerently since based coupled example generate signal composed multiple spikes convolve diﬀerent blur kernels uniform random creating diﬀerent blurry observations. refer figure ground-truth spike signal associated blur kernels. apply blind deconvolution algorithms prior reduction schedule blurry test signals compare quality reconstructed blur kernels signals. recovery results shown figure readily apparent produces superior estimation quality kernel image. additionally signal recovered considerably sparse indicating done better optimizing consistent subsequent theoretical analysis conducted sections .-.. cannot potentially eﬀective careful tuning initialization much robust suboptimal experimental settings etc. present form. note demonstrable advantage entirely based improved convergence path since possess identical constellation local minima moreover unrelated putative advantage solving revisit latter point section algorithm certain attendant analyses nevertheless carried even closed-form solutions possible. importantly assess properties potentially aﬀect sparsity quality resulting solutions highly sparse prior therefore penalty function generally eﬀective diﬀerentiating sharp images structures blurry ones recall concavity respect coeﬃcient magnitudes signature property sparse penalties potential advantage straightforward characterize associated image penalty; namely highly concave nondecreasing function |xi| expect sparse image gradients heavily favored. candidate image penalties expect former promote even sparser solution latter section argue theorem penalty concave non-decreasing function |xi| concave non-decreasing function moreover least elements equal zero locally minimizing solution domain given appropriate explain precisely often produces superior results map. fact associated penalty actually promote sparse solutions much weaker conditions follows role modulating eﬀective penalty first deﬁne function respect origin conveniently examine concavity properties considering positive half real line. result implies regardless penalizes large magnitudes nearly equivalently. contrast small magnitudes penalized much less becomes smaller. theorem loosely suggests sparse solutions heavily favored smaller. however would ideally like make rigorous statements relative concavity various penalty functions involved allowing make stronger claims sparsity-promotion. perhaps simplest choice satisﬁes conditions theorems choice advocated sparse estimation literature diﬀerent contexts assume constant value turn implies jeﬀreys noninformative prior coeﬃcient magnitudes |xi| solving maximization attractive part embedded hyperparameters selection leads particularly interesting closed-form penalty follows worth spending time examine particular selection detail since elucidates many mechanisms whereby attendant approximations heuristics eﬀective. non-convex norm. contrast becomes large shown terms combined approach scaled versions convex norm. additionally assume ﬁxed kernel ignore boundary eﬀects scaling turns optimal particular bayesian sense described thus noise level increased increases penalty behaves like convex function becomes less prone local minima. contrast k¯kk increased meaning reduced penalty actually becomes concave respect |xi|. phenomena ways similar certain homotopy sparse estimation schemes heuristic hyperparameters introduced gradually introduce greater non-convexity canonical compressive sensing problems without dependence noise factors. diﬀerence however penalty shape modulation explicitly dictated noise level kernel entirely integrated fashion. summarize then ratio viewed modulating smooth transition penalty function shape something akin non-convex norm properly-scaled norm. contrast conventional map-based penalties independent thus retain ﬁxed shape. crucial ramiﬁcations coupling ρ-controlled shape modiﬁcation/augmentation exclusive framework addressed following subsections. choices exhibit partially muted form coupling considered section also address desirable form invariance exists constant. success practical blind deconvolution algorithms heavily dependent form stagewise coarse-to-ﬁne approach whereby kernel repeatedly re-estimated successively higher resolutions. stage lower resolution version used figure example coupled penalty diﬀerent values assuming constant. norm included comparison. example surface plot coupled penalty function gvb; constant. initialize estimate next higher resolution. implement approach initially large values dominant primarily low-frequency image structures dictate optimization subsequent iterations blur kernel begins reﬂect correct coarse shape gradually reduced allow recovery detailed structures. highly sparse prior ultimately eﬀective diﬀerentiating sharp images structures convex one. detailed supported evidence claim found well section below. however prior applied initial stages estimation iterations likely become trapped suboptimal local minima always combinatorial number. moreover early stages eﬀective noise level actually high errors contained estimated blur kernel exceedingly sparse image penalties likely produce unstable solutions. given reformulation outlined above argue implicitly avoids problems beginning large penalty function need resolved penalty function becomes less convex reduced risk local minima instability ameliorated fact likely already neighborhood desirable basin attraction. additionally implicit noise level substantially less. kind automatic ‘resolution’ adaptive penalty shaping arguably superior conventional approaches based concavity/shape induced separable penalty function kept ﬁxed regardless variation noise level scale i.e. diﬀerent resolutions across coarse-to-ﬁne hierarchy. general would seem unreasonable penalty shape would optimal across vastly diﬀerent noise scales. advantage easily illustrated simple head-to-head comparisons underlying prior distributions identical reasonable easiest kernel type handle sparsest smallest elements equal challenging case corresponding broad diﬀuse image blur many local minima. situation penalty function convex conservative. general highly concave prior needed disambiguate highly blurred image relatively sharp one. additionally beginning learning process large penalty naturally favor blurry diﬀuse kernel absence additional information. help ensure relatively convex less aggressive initial iterations. however algorithm proceeds reduced elements pushed towards zero penalty embedded dependency gradually become less convex lower bound drop arbitrarily described concavity modulation). minimized becomes relatively sparse reﬁned explored stage extent pushed towards greater sparsity well again desirable eﬀect occurs relatively limited risk local minima gradual intrinsically-calibrated introduction increased concavity. also consider ideas context existing algorithms adopt various structure selection heuristics implicitly explicitly achieve satisfactory performance viewed adding additional image penalty terms trade-oﬀ parameters example incorporates extra local penalty latent image gradients small-scale structures recovered image close blurry image. thus actually contribute less subsequent kernel estimation step allowing larger structures captured ﬁrst. similarly bilateral ﬁltering step used pruning small scale structures finally develop empirical structure selection metric designed small scale structures pruned away thresholding corresponding response allowing subsequent kernel estimation dominated large-scale structures. generally speaking existing strategies face trade-oﬀ either must adopt highly sparse image prior needed properly resolving structures deal attendant constellation problematic local minima rely smooth image prior augmented compensatory structure-selection measures described avoid global solutions. contrast interpret coupled penalty function intrinsic principled alternative transparent integrated functionality estimation diﬀerent resolutions without additional penalty factors complexity. essentially sparse prior expressed using alternative variational form choosing prior tantamount choosing determines gvb. results theorems suggest concave non-decreasing useful favoring sparsity moreover theorem subsequent analyses suggest simplifying choice possesses several attractive properties regarding relative concavity resulting gvb. selections therefore gvb? becomes clear sparsity intimated related. concretely assuming concave non-decreasing actually one-to-one correspondence whenever optimal equals zero well vice versa. therefore instead examine relative concavity diﬀerent values directly determine sparsity turn sparsity motivates following result term ignored optimal need minimize concave non-decreasing whenever therefore optimal trivially zero. conversely eﬀectively inﬁnite penalty optimal must also zero. draw similar conclusions detailed sections whenever general aﬃne adopted. perhaps importantly also suggests deviates aﬃne function begin lose desirable eﬀects regarding described penalty shape modulation. previously closely scrutinized special aﬃne case still remains examine general aﬃne form fact diﬃcult show increased resulting penalty increasingly resembles norm lesser dependency thus severely muting eﬀect shape modulation appears eﬀective currently seem advantage choosing left multitude potential image priors conveniently simple choice value inconsequential. experimental results support conclusion namely increased zero performance gradually degrades unlike myriad estimation techniques choices exact calibration constraint fundamentally alter form optimal solution beyond mere rescaling. moreover constraint omitted altogether methods must carefully tune associated trade-oﬀ parameters another lack invariance require additional tuning. interestingly babacan experiment variety algorithms using diﬀerent underlying image priors empirically constant works best; however rigorous explanation given case. thus results provide powerful theoretical conﬁrmation selection along number useful attendant intuitions. summarize section shown shape eﬀective image penalty explicitly controlled ratio noise variance squared kernel norm many circumstances leads desired mechanism controlling relative concavity balancing sparsity largely mitigating issues local minima compromise convergence traditional estimators. demonstrated unique choice image prior mechanism sense optimal scale-invariant. course readily concede diﬀerent choices image prior could still useful factors taken account. also based strong simplifying assumption covariance algorithm constant babacan provide preliminary discussion regarding possibly advantageous map. however material mostly exists sparse estimation literature related references) therefore behavior blind deconvolution remains open question including constant might optimal. emphasize none meant suggest real imaging data follows jeﬀreys prior distribution return topic section below. overall perspective provides much clearer picture able operate eﬀectively might expect optimize performance. space precludes detailed treatment many natural extensions suggested developments. example original formation given clear best incorporate alternative noise models required integrations longer tractable. however viewed alternatively using becomes obvious diﬀerent data-ﬁdelity terms easily substituted place quadratic likelihood factor. likewise given additional prior knowledge blur kernel diﬃculty substituting ℓ-norm uniform convolutional observation model reﬂect additional domain knowledge. thus proposed reformulation allows inherit transparent extensibility previously reserved map. levin represents primary inspiration work presents compelling highly inﬂuential case joint estimation generally favors degenerate no-blur solution meaning delta function even assumed image prior reﬂects true underlying distribution meaning ptrue assumed ﬂat. turn presented primary argument inferior line reasoning considerably diﬀerent given section take closer look orthogonal perspectives hopes providing clarifying resolution. note levin frequently mapxk refer joint estimation using mapk estimation alone marginalized terminology mapk represents inference ideal purports approximate equivalent herein. either ground-truth kernel delta kernel sharp blurred signals. bottom cost function value function observed ﬁgures strong edges cost function value using sparse ℓp-norm already favor original sharp image. smaller more-reﬁned structures smaller required. real world image composition large small scale structures cost function values original blurred signals similar value large. smaller cost function value favors original sharp image desired. results presented herein suggest sort paradox section argued actually equivalent unconventional form estimation intrinsic mechanism avoiding local minima increasing chances good global near-global minima found. moreover least noiseless case virtue corollary however based noiseless analysis levin above global solution unlikely involve true sharp image true image statistics used meaning performance poor well global solution. thus reconcile positive performance actually observed solutions equipped appropriate non-ﬂat kernel prior. likewise naturally favors blurry explanations since delta kernel maximize norm. moreover introduces prior convenient form devoid additional tuning parameters whereas traditional estimator would generally require form cross-validation. secondly perhaps importantly question whether heavily favored true image prior ptrue really relevant issue begin with. pertinent question whether becomes suﬃciently small assuming appropriately. crucial understand exponent need correspond true distribution ptrue long reasonably close sparse solution. point small regardless ptrue maximally sparse solutions favored unlikely involve no-blur solution. therefore exactly sparse nonetheless locate sparse approximation suﬃciently reasonable unknown still estimated accurately. strictly enforce compare sharp blurred images using relaxed criterion figure displays results optimal value computed varied using delta kernel true blur kernel strong edges suﬃcient avoiding delta solution gradient image. bottom shows result coupled penalty emerging analysis section clearly percentage local patches favoring true solution increases decreasing suﬃciently small value pattern favored patches similar coupled penalty function. reﬂect results conclusions panel tells diﬀerent story. basically whereas shows delta kernel favored even small values relaxed condition strongly prefers true blur kernel wide range generalizing real images ensure practical success capture high-resolution details lower values deﬁnitely preferred limiting undesirable eﬀects variance reduction mentioned above. visualize claim present revised version depicts regions real image sharp image preferred undesirable blurred solution relative preference include lower values well analogous ranking using near zero. ﬁgure readily apparent smaller values eﬀective penalty function indeed behave like norm small. course practical standpoint solving represents diﬃcult combinatorial optimization problem numerous local minima small speculate many tried direct minimization concluded smaller values inadequate. however penalty function shape modulation intrinsic ultimately provides unique surrogate circumventing problem hence strong performance. thus brieﬂy summarize largely superior allows near-optimal image penalty maximally discriminative blurry sharp images reduced risk getting stuck local minima optimization process. overall conclusions provide complete picture essential diﬀerences proceeding next section emphasize none arguments presented herein discredit natural image statistics directly solving fact levin prove ptrue limit image grows large estimate marginalizing equal justiﬁcation latter cannot directly transferred justiﬁcation former. highlights importance properly diﬀerentiating various forms bayesian inference. natural image statistics ideal cases grow large able integrate unknown beneﬁtting central limit arguments estimating alone. however jointly compute estimates enjoy asymptotic welfare since number unknowns increases proportionally sample size. insights paper show that least regard exactly equal footing type thus must look theoretical justiﬁcation elsewhere leading analysis relative concavity local minima invariance maximal sparsity etc. presented herein. existing blind deconvolution algorithms typically utilize preassigned decreasing sequence described section noted algorithm preferable learned automatically data common applications alternative strategy also conceptual appeal integrated cost function universally reduced even updated unlike algorithm reduction step fact increase overall cost unlike updates. however current deblurring papers either mention seemingly obvious alternative explicitly mention learning problematic without concrete details. example observed noise level learning used represents source problems optimization diverges estimated noise level decreases much. mention might decrease much details analyses absent. interestingly perspective presented herein provides clear picture learning diﬃcult suggests potential ﬁxes. problem stems degenerate global minimum therefore occurring explanation follows consider minimization combined dimensionality larger inﬁnite number candidate solutions therefore term cost function minimized exactly zero even limit moreover theorem observe becomes increasingly small around solutions small near zero. fact actually shown non-decreasing represent globally degenerate minimizing solution broad class even choices slightly subdued form degeneracy still exist since vb-speciﬁc regularization fundamentally favors small essentially factor always favor therefore small. weighting suﬃcient counteracting eﬀect given multitude feasible solutions thus framework makes clear never expect optimize expect achieve satisfactory results explaining empirical observation mentioned above. contrast point considerably obfuscated examine previous free energy-based cost function directly. fortunately view naturally oﬀers potential ﬁxes noise level estimation avoiding types undesirable degenerate solutions. perhaps simplest approach include additional penalty factor user-speciﬁed constant. justiﬁcation inclusion note added factor proportional /λky acts barrier preventing ever going zero even fact easily shown minimizing cost function augmented penalty must satisfy viewed lower-bound /nky wide range images testing scenarios including reported results section numerous real-world experiments shown regardless single ﬁxed value likely less burdensome producing entire reduction schedule also requires user-speciﬁed minimal value anyway. moreover update rules require slight modiﬁcation account additional term retaining existing convergence properties estimating noise level together image kernel make deblurring algorithm noise-aware mostly parameter-free. profoundly initializing large value allowing iterations learn optimal reduction schedule oﬀers natural coarse-to-ﬁne process blind deblurring found crucial factors blind deblurring algorithms discussed above. experimental results section support conclusion. emphasize primary purpose paper formal analysis existing stateof-the-art blind deconvolution methodology development practical system empirical support recent algorithms complementary theoretical presentation already exist nonetheless motivated results herein brieﬂy evaluate simple reﬁnements algorithm help corroborate analytical ﬁndings demonstrating extremely simpliﬁed version albeit theoretically sound underpinnings perform well published state-of-the-art algorithms considerably complexity and/or manual parameters. hope motivate optimal usage sophisticated realistic blind deblurring problems. image prior obtained motivated section instead prior based natural image statistics learn parameter automatically discussion section revised estimation steps summarized algorithm obtained adopting procedure algorithm special case updates derived appendix refer algorithm vb-jeﬀreys note estimation performed gradient domain; however recovered kernel applied non-blind deconvolution step obtain ﬁnal latent image estimate. ﬁnal nonblind step taken standardized across algorithms compared section. given variant reproduce experiments using useful benchmark test data consists base images size diﬀerent blurring eﬀects leading total blurry images. ground truth blur kernels estimated recording trace focal reference points boundaries sharp images related text details experimental setup data collection). kernel sizes range metric deﬁned quantiﬁes error estimated ground-truth images. normalize fact harder kernels give larger image reconstruction error even true kernel known ratio image deconvolved estimated kernel image deconvolved ground-truth kernel used ﬁnal evaluation measure. ational bayesian methods fergus levin labeled vb-fergus vb-levin respectively accompany dataset. eﬀective practice optimized respect considerations provided herein speciﬁc prior selections rigorously motivated. instead priors loosely based statistics natural scenes argued sections optimal. cumulative histogram error ratios shown figure height indicates percentage images error ratio level. high bars indicate better performance. mentioned levin results error ratios already visually implausible regions vb-jeﬀreys algorithm achieve close success error ratio signiﬁcantly higher others. regardless algorithms still exhibit reasonable performance especially given beneﬁt additional prior information regularization heuristics facilitate blur-adaptive structure selection boost typical algorithms discussed previously). however curious phenomenon vb-fergus vb-levin experience relatively large drop-oﬀ performance error ratio reduces diﬃcult absolutely certain plausible explanation decline relates prior selection employed algorithms. cases prior based ﬁnite mixture zero mean gaussians diﬀerent variances roughly matched natural image statistics. prior heavily favor approximately sparse signals never produce exactly sparse estimates resolution course-to-ﬁne hierarchy hence especially high resolutions penalty shape modulation eﬀect highly muted beneﬁcial sparsity/variance trade-oﬀ accompanies strongly sparse priors. thus algorithms optimal resolving extremely details required reliably producing image estimates error ratios. contrast achieve high error ratios lower resolution features need resolved regime vb-levin closest algorithmically vb-jeﬀreys performs nearly well vb-jeﬀreys. again reinforces notion natural image statistics optimal basis image priors within framework. next compare vb-jeﬀreys several state-of-the-art algorithms adopts additional local smoothness prior designed reduce ringing artifacts. includes phases kernel estimation incorporates explicit scheme edge structure selection. finally also carefully-engineered approach coupled structure selection sharp edge prediction schemes help algorithm avoid degenerate delta solution. recall previously argued standard algorithms suﬀer problems either pixel-wise image prior highly sparse convergence sub-optimal local solutions becomes problem prior less sparse global solutions suﬃciently distinguish blurry sharp images. algorithms tested viewed addressing conundrum including additional regularization schemes global near global minima favor sharp images even basic pixel-wise image prior convex diﬀerent strategy adopts simpler underlying model additional regularizers beyond canonical pixel-wise sparse prior. figure reveals simple strategy properly implemented still outperform specially tuned estimates. note results map-cho dataset accompanying directly results map-shan map-xu produced using software provided figure evaluation restoration results cumulative histogram deconvolution error ratio across test examples. height indicates percentage images error ratio level. high bars indicate better performance. comparison several algorithms. comparison several state-of-the-art algorithms. authors adjust parameters carefully. algorithms every test image parameters similar overall vb-jeﬀreys obtains highest reported result existing algorithm important benchmark. paper presents insightful reformulation subsequent analysis blind deconvolution algorithms revealing practical success possible suggesting valuable improvements latter. summarize contributions perspective follows separated implementation heuristics meticulously examined interplay relevant underlying algorithmic details employed practical systems. consequently initially appear plausible rationale achieving high performance limited applicability given assumptions required implement scalable versions underlying cost function requisite approximations accounted for. direct contrast conventional assumptions explaining presumed performance advantages leading principled criteria choosing optimal image prior. crucial emphasize image prior need generally reﬂect accurate statistics real imaging data. instead preferred distribution likely guide iterations high quality global solutions strongly diﬀerentiating blurry sharp images. context motivated unique selection inﬁnite possible sparse image priors simultanescale invariance leads intrinsic coupling blur kernel noise level image penalty local minima largely avoided. best knowledge represents completely viewpoint understanding algorithms. image statistics problem often global near-global solutions properly diﬀerentiate blurry sharp images. contrast highly sparse global solutions optimally selective sharp images convergence local solutions more-or-less inevitable. latter oﬀers compelling advantage. form demonstrate longer diﬃcult enhance performance generality inheriting additional penalty functions noise models commonly reserved map. moreover anticipate contributions lead wider range principled applications non-uniform deconvolution multi-frame video deblurring preliminary results show tremendous promise additionally analysis conducted blind deconvolution well relevant related problems like robust dictionary learning presence noise. overall hope observations ensure under-utilized blind deconvolution related tasks. conclude mentioning that given perspective provided herein possible derive blind deblurring algorithms penalty functions deviate script nonetheless adopt attractive properties. direction ongoing research. obtained starting simply removing minimization deﬁnition plugging value simplifying. basic strategy majorization-minimization approach akin concave-convex procedure derive coordinate-wise γi-independent terms omitted. closed-form solution available instead basic principles convex analysis form strict upper bound facilitate subsequent optimization. particular squared norm reincorporating i-dependent image boundary conditions become somewhat relevant comprehensive version proof. plugging obtain revised problem covers vast majority practical sparse priors secondly reason diﬀerentiable point still solved numerically optimization problem perhaps analytically leveraging structure example non-decreasing function diﬀerentiable zero. however since γopt whenever review would originally like minimize latent variables simplify optimization introduce additional latent variables that combining terms equivalently minimizing algorithms. importantly least purposes updates one-to-one correspondence algorithm albeit inconsequential diﬀerences notation statistical interpretation. speciﬁcally update equivalent update algorithm update equivalent update update becomes equivalent computing diagonal ﬁnally update algorithm requisite boundary conditions explicitly incorporated note update algorithm appears somewhat diﬀerent originally presented considers special case assumed image prior ﬁnite gaussian scale mixture given omit pixel-wise subscript simplicity. likewise later proofs approx +log+f priate. deﬁnition know minγ≥ transformation ignoring constant terms minγ≥ large regardless value regime suﬃciently large diﬀerence terms must converge zero. follows diﬀerence corresponding minimizing values therefore cost function diﬀerence converges zero. last term monotonically increasing zero implies always extra monotonically increasing penalty since dealing continuous functions here minimizing therefore necessarily smaller. using results convex analysis conjugate duality shown minimizing respect know gradient always positive non-increasing function. therefore also infer proof corollary assuming want show suﬃcient show increasing function represents equivalent condition relatively concavity given deﬁnition assuming requisite derivatives exist function consider case denominator always non-negative ignored. numerator allow become arbitrarily large keeping ﬁxed. quadratic term dominate violating assumption therefore must λopt lower bound d/n. thus reﬂect expectation regarding minimal noise modeling error. practice updates merged algorithm without disrupting convergence properties", "year": 2013}