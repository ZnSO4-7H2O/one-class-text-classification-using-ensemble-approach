{"title": "On Characterizing the Capacity of Neural Networks using Algebraic  Topology", "tag": ["cs.LG", "cs.CG", "cs.NE", "math.AT", "stat.ML"], "abstract": "The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks.", "text": "learnability different neural architectures characterized directly computable measures data complexity. paper reframe problem architecture selection understanding data determines expressive generalizable architectures suited data beyond inductive bias. suggesting algebraic topology measure data complexity show power network express topological complexity dataset decision region strictly limiting factor ability generalize. provide ﬁrst empirical characterization topological capacity neural networks. empirical analysis shows every level dataset complexity neural networks exhibit topological phase transitions. observation allowed connect existing theory empirically driven conjectures choice architectures fully-connected neural networks. deep learning rapidly become pervasively applied techniques machine learning. computer vision reinforcement learning natural language processing speech recognition core principles hierarchical representation optimization central deep learning revolutionized state art; goodfellow domain major difﬁculty lies selecting architectures models optimally take advantage structure data. computer vision example large body work etc.) focuses improving initial architectural choices krizhevsky developing novel network topologies optimization schemes speciﬁc vision tasks. despite success approach still general principles choosing architectures arbitrary settings order deep learning scale efﬁciently problems domains without expert architecture designers problem architecture selection must better understood. theoretically substantial analysis explored various properties neural networks relate expressivity generalization capability however foregoing theory used determine architecture practice understood expressive model need order solve problem. hand neural architecture search views architecture selection compositional hyperparameter search result ideally yields expressive powerful architectures often difﬁcult interpret resulting architectures beyond justifying empirical optimality. propose third alternative foregoing data-ﬁrst architecture selection. practice experts design architectures inductive bias data generally like hyperparameter selection problem expressive neural architectures learning particular dataset solely determined nature true data distribution. therefore architecture selection rephrased follows given learning problem architectures suitably regularized expressive enough learn generalize problem? natural approach question develop objective measure data complexity characterize neural architectures ability learn subject complexity. given dataset problem architecture selection distilled computing data complexity choosing appropriate architecture. example take datasets given figure figure respectively. ﬁrst dataset consists positive examples sampled disks negative examples compliment. right dataset consists positive points sampled disks rings hollow centers. geometric measure complexity appears ’complicated’ contains holes clusters. trains single layer neural networks increasing hidden dimension datasets minimum number hidden units required achieve zero testing error ordered according geometric complexity. visually figure regardless initialization single hidden layer neural network units denoted express holes clusters whereas simpler express decision boundary perfectly. returning architecture selection wonders characterization extrapolated; true datasets ’similar’ geometric complexity architecture hidden learns perfectly likewise datasets similar complexity architectures hidden units never learn completion? paper formalize notion geometric complexity language algebraic topology. show questions architecture selection answered understanding ’topological capacity’ different neural networks. particular geometric complexity measure called persistent homology characterizes capacity neural architectures direct relation ability generalize data. using persistent homology develop method gives ﬁrst empirical insight learnability different architectures data complexity increases. addition method allows generate conjectures tighten known theoretical bounds expressivity neural networks. finally show topological characterizations architectures areuseful practice presenting method topological architecture selection applying several openml datasets. order formally describe notions geometric complexity datasets turn language topology. broadly speaking topology branch mathematics deals characterizing shapes spaces sets connectivity. context characterizing neural networks work towards deﬁning topological complexity dataset terms dataset ’connected’ group neural networks capacity produce decision regions connectivity. topology understands relationships different spaces points continuous maps them. informally topological spaces equivalent continuous function inverse also continuous. figure positive label outptus single hidden layer neural networks inputs hidden units respectively training datasets positive examples red. highlighted regions output constitute positive decision region. exists homeomorphic homeomorphism; detailed treatment general topology bredon informal figure since homeomorphism least clusters would need split discontinuously order produce four different regions power topology lies capacity differentiate sets meaningful geometric discards certain irrelevant properties rotation translation curvature etc. purposes deﬁning geometric complexity non-topological properties like curvature would ﬁne-tune architecture selection–say regions squigly boundaries certain architectures might converge–but show grouping neural networks ’topological capacity’ provides powerful minimality condition. show certain architecture incapable expressing decision region equivalent topology training data hope ever generalizing true data. algebraic topology provides tools necessary build foregoing notion topological equivalence measure geometric complexity also compute measure real data core algebraic topology takes topological spaces assigns algebraic objects groups chains exotic constructs. spaces shown topologically equivalent algebraic objects assigned isomorphic thus algebraic topology allow compare complexity decision boundaries datasets objects assigned. topological property invariant preserved homeomorphism. example number holes regions disjoint another topological properties whereas curvature not. although many ﬂavors algebraic topology powerful computationally realizable tool homology. deﬁnition topological space called homology group power number ’holes’ dimension note number separate connected components. call betti number finally homology deﬁned {hn}∞ immediately homology brings closer deﬁning complexity assume actually collection datapoints really union solid balls likewise union solid balls rings compute homology directly. case since connected components; since circles clearly performing computation second case seperate clusters rings/holes. respect reasonable ordering homology complex measure yields non-trivial differentiation spaces higher dimension. example homology hollow donut surprisingly homology space contains great deal information topological complexity. following theorem suggests absolute power homology group topologically similar spaces therefore neural networks topologically similar decision regions. theorem topological spaces. order compute homology needed assume actually geometric shapes sampled. without assumptions dataset number data points. because small enough scales data point isolated connected component; sets pair different positive points disjoint. properly utilize homological complexity better understanding architecture selection need able compute homology data directly still capture meaningful topological information. persistent homology introduced zomorodian carlsson avoids trivialization computation dataset homology providing algorithm calculate homology ﬁltration space. speciﬁcally ﬁltration topological space equipped sequence subspaces figure particular ﬁltration given growing balls size centered point letting resulting subspace ﬁltration. deﬁne betti number homology example every ball disjoint. connected components merge finally union balls forms hole towards center dataset together change homology therefore betti numbers changes summarized succinctly persistence barcode diagram given figure section denotes ’hole’ dimension left endpoint point homology detects particular component right endpoint component becomes indistinguishable ﬁltration. calculating persistent homology datasets frequently diagrams. architectures. understand homological complexity powerful measure differentiating architectures present following principle. suppose dataset drawn joint distribution continuous topological space denote support distribution points positive labels denote points negative labels. denote support homology function essentially homology binary classiﬁer roughly characterization many ’holes’ positive decision region sometimes denote betti number support homology. finally family binary classiﬁers theorem exists misclassiﬁes every essentially theorem says architecture incapable producing certain homological complexity model using architecture always true datapoints model fail. note principle holds regardless attained learned otherwise. principle implies matter well learns correctly classify always counter examples true data. context architecture selection foregoing minimality condition signiﬁcantly reduces size search space eliminating smaller architectures cannot even express ’holes’ data allows return original question ﬁnding suitably expressive generalizable architectures computable language homological complexity neural networks ’architecture’ given dataset architectures exist neural network resurface contemporary theoretical view question thereafter make ﬁrst steps towards empirical characterization capacity neural architectures view topology. theoretically homological complexity neural network framed terms number holes expressible certain architectures. particular bianchini gives analysis maximum betti numbers grows changes. results show width activation fully connected architecture effect topological expressivity varying polynomial exponential degrees. unclear analysis bounds describe expressivity learnability terms individual betti numbers. theoretical perspective bianchini stipulated characterization individual homology groups require solution deeper unsolved problems algebraic topology. however topological complexity effective architecture selection understanding betti number essential grants direct inference architectural properties persistent homology data. therefore turn empirical characterization. understand homology data determines expressive architectures characterize capacities architectures increasing number layers hidden units learn express homology datasets varying homological complexity. restricting analysis case inputs generate binary datasets increasing homological complexity sampling points mixtures unif unif uniform random distributions solid empty circles known support homologies. homologies chosen range contiguously sampled distribution geometrically balanced i.e. topological feature occupies order magnitude. additionally margins induced classes learning. examples shown figure although chose study dimensional setting because allows compute persistent homology decision region directly convergence analysis extends number dimensions. figure topological phase transitions dimensional neural networks homological complexity data increases. upper right corner plot dataset neural networks increasing ﬁrst layer hidden dimension trained. plot gives minimum error architecture versus number minibatches seen. characterize difﬁculty learning homologically complex data terms depth width consider fully connected architectures relu activation functions depth width denote individual architectures pair vary number hidden units ﬁrst layer form half-space basis decision region. weights architecture initialized samples normal distribution variance respecting scale synthetic dataset. homology take several datasets sampled foregoing procedure optimize initializations architecture standard cross-entropy loss. minimize objective adam optimizer ﬁxed learning rate increasing batch size schedule compare architectures average best performance measuring misclassiﬁcation error course training homological expressivity training. latter quantity given measures capacity model exhibit true homology data. compute homology individual decision regions constructing ﬁltration heaviside step function difference outputs yielding persistence diagram exact homological components decision regions. results summarized figures networks exhibit statistically signiﬁcant topological phase transition learning depends directly homological complexity data. dataset random homeomorphism applied thereto best error architectures layers hidden units strictly limited magnitude convergence time hphase. example figure layer neural networks fail converge hphase datasets homology generally homological complexity directly effects efﬁcacy optimization neural networks. shown figure taking increasing progression homologies average convergence time class architectures yields approximately monotonic relationship; case convergence time hphase increases increasing convergence time hphase decreases ﬁxed broader analysis varying number layers given appendix. returning initial question architecture selection provides analysis empirical estimation ﬁrst complete probabilistic picture homological expressivity neural architectures. architectures figure displays estimated probability expresses homology decision region training. speciﬁcally figure indicates that hidden layer neural networks clearly examining figure conjecture increases figure scatter plot number iterations required single-layer architectures varying hidden dimension converge misclassiﬁcation error. colors point denote topological complexity data networks trained. note emergence monotonic bands. multilayer plots given appendix look similar. likewise figure horizontal matrix gives probability expressing layer. result indicates higher order homology extremely difﬁcult learn single-layer case maxf∈fa input dimension. importance foregoing empirical characterization architecture selection two-fold. first analyzing effects homological complexity optimization different architectures able conjecture probabilistic bounds learnable homological capacity neural networks. thus predictions minimal architectures using bounds sufﬁcient enough learn data homology homeomorphism. second analysis individual betti numbers enables data-ﬁrst architecture selection using persistent homology. thus demonstrated discriminatory power homological complexity determining expressivity architectures. however homological complexity practical architecture selection must computable real data generally real data must non-trivial homology. following section present method relating persistent homology dataset minimally expressive architecture predicted foregoing empirical characterization experimentally validate method several datasets. figure table estimated probabilities different neural architectures express certain homological features data training. probabilities express homologies increasing function layers neurons. bottom probabilities expressing function layers neurons. determine appropriate scale accept topological features pertinent learning problem; infer lower-bound hphase topological features decided scale. extraction static homology persistence homology aesthetically valid ill-posed many cases. purposes architecture selection however exact reconstruction original homology necessary. given persistence diagram containing births deaths features given consider assume topological component real space. resulting homologies form chosen ﬁltration certain topologically noisy features included estimate hphase worst architecture overparameterized still learns. estimated hphase underrepresentitive figure topological architecture selection applied four different datasets. persistent homology histogram topological lifespans predicted ˆhphase indicated. test error plot best performance three four layer neural networks given terms number hidden units ﬁrst layer. topological features space worst architecture underparameterized potentially close hphase. either case yields solution plausibly useful seed algorithm selection algorithms adopt static instantiation persistence homology. hidden unit single layer neural networks synthetic datasets known homology previous experiments. resultant discretization model gives lower-bound estimate applying bound order validate approach applied topological architecture selection several binary datasets openml dataset repository fri_c balance-scale banana phoneme delta_ailerons. compute persistent homology labeled classes therein accept topological features lifespans greater standard deviations mean homological dimension. estimated lowerbound hphase single hidden layer neural networks using finally trained neural networks architecture training preliminary ﬁndings indicate empirically derived estimate hphase provides strong starting point architecture selection minimum error ˆhphase near zero every training instance. although estimate given terms dimensional homology data still performed well higher dimensional datasets phoneme fri_c*. failure cases choice greatly affected predicted hphase thus imperative adaptive topological selection schemes investigated. analysis characterization given decision regions individual classes dataset plausible true decision boundary topologically simple despite complexity classes. although directly characterize neural network topology decision boundaries recent work varshney ramamurthy provides exact method computing persistent homology decision boundary number classes dataset. subject future work provide statistical foundation reanalyze homological capacity neural networks context. addition testing topological measures complexity setting architecture selection verify common machine learning benchmark datasets non-trivial homology computation thereof tractable. cifar-. compute persistent homology several classes cifar- using python library dionysus. reduction used. figure persistence barcode exhibits separate signiﬁcant loops well major connected components persistence diagrams relegated appendix. place work context deep learning theory relates expressivity. since seminal work cybenko established standard universal approximation results neural networks many researchers attempted understand expressivity certain neural architectures. pascanu mackay provided ﬁrst analysis relating depth width architectures complexity sublevel sets express. motivated therefrom bianchini expressed theme language pfefferian functions thereby bounding betti numbers expressed sublevel sets. finally guss gave account topological assumptions input data lead optimally expressive architectures. parallel eldan shamir presented ﬁrst analytical minimality result expressivity theory; authors show simple functions cannot expressed layer neural networks without exponential dependence input dimension. work spurred work poole raghu reframed expressivity differential geometric lens. work presents ﬁrst method derive expressivity results empirically. topological viewpoint sits dually differential geometric counterpart conjunction work duality implies topological expression possible exponential differential expressivity allows networks bypass homological constraints cost adversarial sets. furthermore work opens practical connection foregoing theory neural expressivity architecture selection potential substantially improve neural architecture search directly computing capacities different architectures. architectural power closely related algebraic topology decision regions. work distilled neural network expressivity empirical question generalization capabilities architectures respect homological complexity learning problems. view allowed provide empirical method developing tighter characterizations capacity different architectures addition principled approach guiding figure persistent homology barcodes classes cifar- datasets; barcode dimensions ’cars’ class along side different samples. note different orientations shown. current algorithms persistent homology deal well high dimensional data embed entire dataset using local linear embedding neighbors. note embedding dataset lower dimensional subspace small enough error roughly preserves static homology support data distribution distribution theorem embedding dataset take sample points example class ’car’ build persistent ﬁltration constructing vietoris-rips complex data. resulting complex simplices took min. generate. finally computation persistence diagram shown figure took min. locked single thread intel core processor. one-time cost computing persistent homology could easily augment neural architecture search. although give analysis dimension topological features–and certainly higher dimensional homological information cifar-–the persistence barcode diagram rich different components intuitively cifar contains pictures cars rotated across range different orientations exhibited homology. particular several holes born range large loop datasets. compute homology three dimensional datasets attempt assert non-trivial hphase. speciﬁcally compute persistent homology majority classes yeast protein localization sites ecoli protein localization sites htru datasets. datasets dimensionalseveral potential avenues future research using homological complexity better understand neural architectures. first full characterization neural networks convolutional linearities state-of-the-art topologies crucial next step. empirical results suggest exact formulas describing power neural networks express decision boundaries certain properties. future theoretical work determining forms would signiﬁcantly increase efﬁciency power neural architecture search constraining search space persistent homology data. additionally intend studying topological complexity data changes propagated deeper architectures. daniely amit frostig singer yoram. toward deeper understanding neural networks power adinitialization dual view expressivity. vances neural information processing systems edelsbrunner guha computational topology invited paper advances discrete computational geometry eds. chazelle goodmann pollack. contemporary mathematics fernando chrisantha banarse dylan blundell charles zwols yori david rusu andrei pritzel alexander wierstra daan. pathnet evolution channels gradient descent super neural networks. corr abs/. http//arxiv.org/ abs/.. hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pascanu razvan montufar guido bengio yoshua. number response regions deep feed forward networks piece-wise linear activations. arxiv preprint arxiv. poole lahiri subhaneil raghu maithreyi sohldickstein jascha ganguli surya. exponential expressivity deep neural networks transient chaos. advances neural information processing systems szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich ancorr drew. abs/. http//arxiv.org/ abs/.. varshney kush ramamurthy karthikeyan natesan. persistent topology decision boundaries. acoustics speech signal processing ieee international conference ieee homology naturally described using language category theory. denote category topological spaces category abelian groups. deﬁnition homology theory function assigning pair spaces graded group {hp} homomorphisms together natural transformation functors called connecting homomorphism denote following axioms satisﬁed. figure scatter plot number iterations required architectures varying hidden dimension number layers converge misclassiﬁcation error. colors point denote topological complexity data networks trained. note emergence convergence bands. figure topological persistence diagrams several datasets. plot barcode diagram given dimensional features along -dimensional embedding dataset. note compute homologies datasets embeeding exception cifar. figure additional topological phase transitions dimensional neural networks homological complexity data increases. upper right corner plot dataset neural networks increasing ﬁrst layer hidden dimension trained.", "year": 2018}