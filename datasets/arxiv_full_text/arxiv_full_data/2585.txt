{"title": "A Learning Algorithm for Relational Logistic Regression: Preliminary  Results", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set.", "text": "relational logistic regression recently proposed aggregation model represent much complex functions previous aggregators. uses weighted formulae deﬁne conditional probability. learning model data consists structure learning parameter learning phase. former corresponds learning features included model latter corresponds learning weight feature. parents observed poole observed model similar semantics markov logic network therefore discriminative learning algorithm mlns learn model. huynh mooney proposed bottom-up algorithm discriminative learning mlns. logic program learner learn structure l-regularized logistic regression learn weights enable automatic feature selection. problem approach aleph different semantics features marked useful aleph necessarily useful features features marked useless aleph necessarily useless features mln. former happens aleph generates features logical accuracy rough estimate value probabilistic model. latter happens representational power aleph mlns leverage counts aleph based existential quantiﬁer. former issue resolved using l-regularization straightforward resolve latter issue. paper develop test algorithm learning models relational data addresses aforementioned issues current learning algorithms. learning algorithm follows hierarchical assumption formula useful feature proven useful. learning algorithm spirit similar reﬁnement graphs popescul ungar algorithm relational logistic regression representation conditional probability terms weighted formulae modelling multi-relational data. paper develop learning algorithm models. learning model data consists steps learning formulae used model learning weight formula structure learning deploy schmidt murphy’s hierarchical assumption ﬁrst learn model simple formulae complex formulae added iteratively sub-formulae proven effective previous learned models. parameter learning convert problem non-relational learning problem offthe-shelf logistic regression learning algorithm weka open-source machine learning tool learn weights. also indicate hidden features individuals incorporated boost learning performance. compare learning algorithm structure parameter learning algorithms literature compare performance models standard logistic regression rdn-boost modiﬁed version movielens data-set. statistical relational learning aims unifying logic probability provide models learn complex multi-relational data. relational probability models core systems. extend bayesian networks markov networks adding concepts objects properties relations allowing probabilistic dependencies among relations individuals. unlike bayesian networks rpms random variable depend unbounded number parents grounding. cases conditional probability canrepresented table. address issue many existing relational models simple aggregation models existential quantiﬁers noisy-or models. models much compact tabular conjoined features hierarchical assumption given input random variables logistic regression considers features bias feature random variable. generate features conjoining input random variables. instance continuous random variables generate feature given random variables conjoining variables allows generating features. weights represent arbitrary conditional probabilities known canonical representation refer buchman koller friedman large however generating features practically possible also makes model overﬁt training data. order avoid generating features schmidt murphy make hierarchical assumption either useful features neither assumption schmidt murphy ﬁrst learn model considering features conjunctions. regularize loss function hierarchical regularization function weights features contributing prediction zero. learning stops keep features non-zero weights conjoined features whose subsets non-zero weights previous step. learning again. continue process features added. relational logistic regression relational logistic regression analogue relational models. also considered directed analogue markov logic networks order describe ﬁrst need introduce deﬁnitions terminologies used relational domains. population refers individuals corresponds domain logic. population size population non-negative number indicating cardinality. example population planets solar system mars individual population size logical variables start lower-case letters constants start upper-case letters. associated logical variable population |pop| size population. lower-case upper-case letter written bold refer logical variables individuals respectively. parametrized random variable form k-ary function symbol logical variable constant. constants random variable. omit parentheses. predicate symbol range {true false} otherwise however feature generation done using hierarchical assumption instead query reﬁnement feature selection done level hierarchy using hierarchical regularization instead sequential feature selection thus reducing number times model learned data enabling previously selected features removed search proceeds. also incorporate hidden features individual model observe affect performance. test learning algorithm modiﬁed version movielens data-set taken schulte khosravi predicting users’ gender age. compare model standard logistic regression models relational features well rdn-boost state-of-the-art relational learning algorithms. obtained results show learn accurate models compared standard logistic regression rdn-boost. results also show adding hidden features increase accuracy predictions make model over-conﬁdent predictions. regularizing predictions towards mean data helps avoid over-conﬁdency. logistic regression logistic regression popular classiﬁcation method within machine learning community. describe used classiﬁcation following cessie houwelingen mitchell examples labeled composed features xixi binary variable whose value predicted. binary multi-valued continuous. throughout paper assume binary variables take values logistic regression learns weights intercept weight feature simplicity assume dimension added data avoid treating differently deﬁnes probability given follows aforementioned learning algorithm directly applicable number potential weights unbounded. develop learning algorithm handle unbounded number potential weights. range range function. example eexistson predicate function eexistson logical variable planet true life exists given planet. literal assignment value prv. represent true alse formula made literals connected conjunction disjunction. weighted formula logical variables tuple boolean formula parents weight. +exp sigmoid function represents assigned values parents represents assignment individuals logical variables fπz→z formula logical variable replaced according evaluated example consider relational model fig. taken suppose want model \"someone happy least friends kind\". following used represent model handling continuous variables initially designed boolean multi-valued parents. true associated false associated substitute allow continuous prvs wfs. example true false evaluates r∗¬sw evaluates s∗tw evaluates parameter learning suppose given deﬁning conditional probability want learn weight convert learning problem learning problem generating data-set single-matrix form. assignment individuals logical variables generate data number times formula j-th true logical variables replaced individuals conversion singlematrix data learn model. weight learned j-th input gives weight j-th note conversion complete speciﬁes values relations. example consider relational model fig. suppose given following case generate matrix data individual consists four values ﬁrst number serving intercept second number people friends third number kind people friends fourth represents whether happy not. sufﬁcient statistics learning weights. example generated matrices follows structure learning learning structure model refers selecting used. conjoining different relations adding attributes generate inﬁnite number wfs. example suppose want predict gender users movie rating system given occupation movies users rated well many much conjoined relations. useful though. example whose formula action useful feature predicting evaluates constant number users. avoid generating features learning model. systematic need deﬁnitions deﬁnition denote deﬁning conditional probability logical variable formula target connector least relations attributed exists least hanging none deﬁnitions deﬁnition chain literals ordered list literal shares least logical variable preceding literals chain targeted least target logical variable. k-bl contains binary literals r-ul contains unary literals. example suppose rated∗ ratedm)∗ ratedm)∗ comedyw belonging deﬁning conditional probability target connectors also attributed hanging logical variable. chain second literal shares ﬁrst literal third shares second fourth shares ﬁrst second literal. chain targeted contains target logical variable. contains binary unary literals. young∗ comedyw nonchain drama∗ comedyw chain targeted. avoid generating types targeted chains contain hanging logical variables. non-targeted chains drama∗ comedyw predicting always evaluates constant number hanging logical variables rated∗ actedw predicting replaced informative wfs. rest paper consider targeted chains hanging logical variables. useless l-regularized logistic regression assigns zero weight exists useless example rated∗ drama∗ comedyw rated∗ dramaw useless lregularization sets order learn structure model select value generate allowed k-bl-ul wfs. best value cross-validation. unary literals making hierarchical assumption using similar search strategy following algorithm below curw k-bl removedw while){ algorithm starts k-bl wfs. initially labeled removed. initially indicate current maximum number unary literals. stopping criteria weights curw learned using l-regularized logistic regression. weight zero removedw increment update curw k-bl r-ul obeying hierarchical assumption respect removedw function returns wfs). stopping criteria generated increases. adding hidden features exploit observed features objects making predictions object contain really useful information observed. example predicting gender users given movies liked movies appealing males females. might features movies know about contribute predicting gender users. order incorporate hidden features model continuous unary prvs random values dataset. generate k-bl learn weights well values hidden features using stochastic gradient descent lregularization. learn values hidden features treat normal features aforementioned structure parameter learning algorithms learn model. women. means predicting gender users feature male rated action useful feature mlns counts number action movies. many current relational learning algorithms/models however rely mostly existential quantiﬁer aggregator relying existential quantiﬁer models either many complex rules imitate effect rules lose great amounts relational information available terms counts. particular example consider discriminative structure learning huynh mooney mlns. first learn large features using aleph learn weights features l-regularization enable automatic feature selection. cases everyone rated action movie male rated∧ action useful feature aleph distinguish males females. therefore rule included ﬁnal mln. relational learners based existential quantiﬁer potentially imitate effect counts using many complex rules. example male rated action∧ rated∧ action∧ used assign maleness probability people rating action movies. approach requires different rule count rules become complex count grows require pairwise inequalities. practice experiments aleph synthesized data observed that even though could learn rules enhance predictions failed ﬁnding them. based observations argue relational learning algorithms/models need allow richer predeﬁned aggregators enable non-predeﬁned aggregators learned data similar does. also argue structure learning algorithm potential explore features also good candidate discriminative structure learning mlns. relational logistic regression learn complex models multi-relational data-sets. paper developed tested structure parameter learning models based hierarchical assumption. compared model standard logistic regression model tion horror drama) movies rated user containing user-movie pairs actual rating user given movie. experiments ignored actual ratings considered movie rated user not. predicting users considered instead three classes learning logistic regression models l-regularization used open source codes learned ﬁnal logistic regression model weka software compared proposed method baseline model always predicting mean standard logistic regression using relational information rdn-boost. performance learning algorithms obtained -folds cross-validation. fold divided users movielens data-set randomly training test set. learned model train measured accuracy average conditional log-likelihood test averaged folds. acll computed follows obtained results represented table show utilizes relational features improve predictions compared logistic regression model relational information rdb-boost. obtained results also represent adding hidden features models increase accuracy reduce mae. however observed adding hidden features makes model over-conﬁdent pushing prediction probabilities towards zero thus requiring regularization towards mean. rdn-boost represented that movielens data-set achieves higher accuracies. also represented hidden features boost performance models. results presented work preliminary results. future direction includes testing learning algorithm complex data-sets much relational information comparing model relational learning aggregation models literature making learning algorithm extrapolate properly un-seen population sizes testing performance structure learning algorithm discriminative learning markov logic networks.", "year": 2016}