{"title": "Incorporating Feedback into Tree-based Anomaly Detection", "tag": ["cs.LG", "cs.AI", "stat.ML", "I.2.6; I.5.5"], "abstract": "Anomaly detectors are often used to produce a ranked list of statistical anomalies, which are examined by human analysts in order to extract the actual anomalies of interest. Unfortunately, in realworld applications, this process can be exceedingly difficult for the analyst since a large fraction of high-ranking anomalies are false positives and not interesting from the application perspective. In this paper, we aim to make the analyst's job easier by allowing for analyst feedback during the investigation process. Ideally, the feedback influences the ranking of the anomaly detector in a way that reduces the number of false positives that must be examined before discovering the anomalies of interest. In particular, we introduce a novel technique for incorporating simple binary feedback into tree-based anomaly detectors. We focus on the Isolation Forest algorithm as a representative tree-based anomaly detector, and show that we can significantly improve its performance by incorporating feedback, when compared with the baseline algorithm that does not incorporate feedback. Our technique is simple and scales well as the size of the data increases, which makes it suitable for interactive discovery of anomalies in large datasets.", "text": "abstract anomaly detectors often used produce ranked list statistical anomalies examined human analysts order extract actual anomalies interest. unfortunately realworld applications process exceedingly difficult analyst since large fraction high-ranking anomalies false positives interesting application perspective. paper make analyst’s easier allowing analyst feedback investigation process. ideally feedback influences ranking anomaly detector reduces number false positives must examined discovering anomalies interest. particular introduce novel technique incorporating simple binary feedback tree-based anomaly detectors. focus isolation forest algorithm representative tree-based anomaly detector show significantly improve performance incorporating feedback compared baseline algorithm incorporate feedback. technique simple scales well size data increases makes suitable interactive discovery anomalies large datasets. keywords anomaly detection active learning user feedback semi-supervised learning optimization reference format shubhomoy weng-keen wong alan fern thomas dietterich amran siddiqui. incorporating feedback tree-based anomaly detection. proceedings halifax nova scotia canada august pages. https//doi.org/ permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights third-party components work must honored. uses contact owner/author. workshop interactive data exploration analytics august halifax nova scotia canada copyright held owner/author. isbn https//doi.org/ introduction define anomaly data instance generated different process process generating nominal data. hand define outlier data instance likelihood according model. anomaly detectors general good detecting outliers. however outliers anomalies. outliers statistical noise others might interest end-users. class state-of-the-art anomaly detectors dependent unsupervised tree-based methods naturally immune problem. detectors usually partition feature space multiple regions assign scores region individually. scores computed regions reflect true relevance user’s notion anomaly creates semantic mismatch user considers anomaly algorithm considers outlier. order avoid mismatch need expert-feedback make outliers line expert’s idea anomaly. active anomaly discovery recent methods incorporating analyst-feedback ensemble anomaly detectors. paper show tree-based anomaly detectors also treated ensembles incorporate feedback employing aad. present implementation concept specific context tree-based anomaly detector isolation forest competitive state-of-the-art anomaly detectors advantage proposed approach allows incorporating feedback finer level simply combining outputs multiple detectors linearly. section present view general structure tree-based anomaly detectors illustrate view isolation forest example. section presents overview extends incorporate feedback isolation forest. refer algorithm if-aad. section presents quantitative empirical results eight benchmark datasets provides visualization feedback process order gain insight feedback affects instances queried. finally summarize contributions results section tree-based anomaly detectors consider anomaly detection setting anomaly detector used assign anomaly scores data instances assumed feature vectors instances presented analyst ranked order starting anomalous instance. work motivated observation number state-of-the-art anomaly detectors based decision-tree ensembles forests. internal nodes tree correspond threshold tests selected features. thus given instance follow unique path root leaf tree. tree node tree-based anomaly detector stores realvalued weight used calculate anomaly scores. anomaly score instance simply equal average weights tree nodes instance follows forest. note node forest viewed defining distinct volume thus total score combination weights overlapping volumes. despite simplicity anomaly detection structure number state-of-the-art algorithms represented particular choice weight values methods construct trees table illustrates weight values correspond number algorithms. example described detail below isolation forest algorithm assigns constant weight tree nodes. anomaly score evaluates average path length traversed instance across trees forest. another example hs-trees algorithm assigns weight leaf node number training instances node node’s depth. addition hs-trees assigns weight non-leaf nodes. thus case anomaly score instance average weights leaves reaches. practice uniformly best anomaly detector across possible applications. rather best performing detector given application depend well detector’s notion outlier matches analyst’s notion interesting anomaly. difficult predict given application. further unlikely weight settings corresponding state-of-the-art detectors optimal given application considering entire range possible weight settings. motivates incorporating user feedback anomaly detector attempt tune weights toward ideal application-specific detector. section show approach often increases number true anomalies discovered within particular budget instances examined analyst. paper treat isolation forest representative tree-based anomaly detector explain method incorporating feedback detector initialized isolation forest weights. describe detail isolation forest algorithm concreteness illustrate easily captured tree-based anomaly detection framework. isolation forest isolation forest comprises trees denoted ...tt constructed randomized manner outlined algorithm illustrated figure tree constructed root leaves randomly partitioning data node selecting feature threshold uniformly random. trees grown instance isolated leaf. based idea anomalous instances well-separated clusters nominal instances feature space. this anomalous instances quickly reach leaf nodes random partitioning. hand nominal instances form dense clusters require many splits finally reach leaf nodes. therefore length path traversed instance root node leaf also known isolation depth shorter anomalous instances nominal instances. anomaly score assigned instance simply average isolation depth across forest. straightforward describe particular setting weights tree-based anomaly detector. particular weight node given instance easy anomaly score assigned tree-based detector simply negative average number nodes paths traversed forest i.e. negative average isolation depth. note that main purpose make scores negative ensure higher scores indicate anomalous lower scores indicate nominal. order describe algorithm feedback convenient view score assigned detector linear score function. this tree node define indicator feature anomaly score simply product feature weight vectors figure illustrates anomaly score contours single tree synthetic data. anomaly score contours figure show single isolation tree informative. however increase number trees ensemble combined scores fairly accurate even without feedback. illustrated figure number trees figure random trees isolation forest synthetic data. points true anomalies; points gray true nominals. figure shows leaf node regions single tree generated random splits. figure shows contours anomaly scores assigned nodes tree. deeper means anomalous; deeper blue means nominal. circles true anomalies among ranked instances. green circles true nominals among ranked instances. left sidebar figure shows ranking true anomalies ideally true anomalies near bar. feature sampled random fmin min. value across instances fmax max. value across instances value sampled unif. random partition parts basis recurse partitions active anomaly discovery algorithm tries maximize number true anomalies presented analyst interactive feedback loop. assigns anomaly score instance higher score means anomalous. instances internally ranked descending order scores. feedback iteration presents anomalous instance analyst asks true label either anomalous nominal. prior work algorithm developed learn weighting among ensemble anomaly detectors particular ensembles produced loda anomaly detector. show approach used re-weight nodes within trees forest. assume dataset instance note think instances represented vector indicator features corresponding tree nodes. label known instance denote label {anomaly nominal}. instances analyst already provided feedback labeled anomalies labeled nominals. anomaly score instance score goal learn weights likely rank true anomalies near top. algorithm takes quantile parameter input instance τ-th ranked score denoted corresponding score denoted weight vector must ensure scores labeled anomalies higher while time scores labeled nominals lower additionally adds soft pairwise constraints encourage every labeled anomaly higher score every labeled nominal weights learned. instead introducing pairwise constraints anomalies nominals constraints relative current τ-th ranked instance. found change degrade accuracy detecting anomalies makes computation significantly faster. since pairwise constraints ‘soft’ violated constraint multiplied slack penalty term re-formulate objective adding additional terms loss function correspond constraints. allows optimization gradient descent helpful number features high case proposed algorithm. where wu∥wu constant weight hyper-parameters. value larger typically case causes hinge loss anomalies higher associated nominals. encourages scores anomalies higher τ-th ranked instance previous iteration scores nominals lower τ-th ranked instance previous iteration. re-weighting partitions experiments consider starting tuning weights based feedback. simply done initializing weights constant values. algorithm employeed regularization term encourages weights depart initial values. refer figure incorporating feedback isolation forest synthetic data figures show anomaly score contours explained figure green circles instances presented labeling. x-axis figure represents number instances presented analyst y-axis represents number true anomalies discovered. curve figure shows number true anomalies discovered incorporate feedback; blue curve figure shows number true anomalies discovered feedback incorporated. algorithm if-aad. assume forest constructed exactly original algorithm trees kept fixed throughout entire interaction analyst. feedback employed re-weight tree-partitions; partitions never modified. figure shows result incorporating feedback synthetic data. algorithm receives feedback alters contours anomaly scores focuses relevant regions feature space. experiments number trees parameters recommended experiments. large makes algorithm focus regions anomalies already found previously discourages exploration. experiments experiments used mammography dataset well seven datasets repository abalone cardiotocography thyroid forest cover kdd-cup- shuttle yeast. dataset classes divided sets representing nominal instances smaller representing anomlous instances. cardiotocography dataset retained instances nominal class original dataset down-sampled anomaly instances represent around total data. rest datasets used entirety. number true anomalies true nominals dataset along division classes nominals anomalies shown table evaluate anomaly detector based rate simulated analyst able find true anomalies. particular iteration anomaly detection involves giving analyst ranked instance receiving feedback anomalous nominal. compare proposed algorithm if-aad following baselines iforest baseline baseline present instances decreasing order anomaly score computed algorithm uniform weights. algorithm ignores analyst feedback thus ranking constant across iteration. baseline captures performance unsupervised anomaly detector incorporate expert feedback. trees constructed original implementation available part python scikit-learn library. loda-aad corresponds original approach applied ensemble anomaly detectors created loda anomaly detector anomaly detector ensemble corresponds random figure shows quantitative results data sets. graph plots number discovered anomalies versus number iterations. best possible result line slope indicating anomaly discovered iteration. curves averaged independent runs algorithm confidence intervals shown. overall if-aad never hurts performance cases significantly increases number anomalies discovered time compared loda-aad. order gain insight feedback influences algorithm real-world datasets computed two-dimensional representations datasets t-sne visualization. figure shows t-sne plots representative datasets abalone ann-thyroid-v. marked points algorithm focused queries first feedback iterations. observe ways feedback influenced queries. first reduced focus regions queried outliers labeled nominal abalone ann-thyroid-v). second increased focus regions contained previously labeled true anomalies abalone ann-thyroid-v). time taken if-aad feedback iteration depends particular data increases linearly number labeled instances. example ann-thyroid-v if-aad took less second first feedback involved labeled instance took approx. seconds incorporate labeled instances. finally note number tree-based anomaly detectors based non-zero weights leaves order evaluate importance non-zero weights internal nodes evaluated version if-aad keeps weights equal zero except leaf nodes updated aad. algorithm called if-aad-leaf implemented including indicator features weights leaf nodes formulation. figure shows comparison if-aad if-aad-leaf three data sets representative results across data sets. observed if-aad-leaf slightly worse performance if-aad showing utility weighting internal nodes majority impact feedback achieved focusing leaf nodes. summary presented anomaly detection algorithm if-aad fine-tunes output isolation forest feedback loop. treats regions defined nodes isolation trees components ensemble re-weights basis feedback received analyst. if-aad consistently performers experiments real-world data. sometimes detects twice number true anomalies baseline isolation forest algorithm. future work intend extend approach tree-based anomaly detectors. acknowledgments funding provided defense advanced research projects agency contracts wnf--c- fa--c-. content information document necessarily reflect position policy government official endorsement inferred. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation paper based upon work weng-keen wong serving national science foundation. opinion findings conclusions recommendations expressed material authors necessarily reflect views national science foundation. figure total number true anomalies seen number queries datasets. total number queries smaller datasets total number queries larger datasets results averaged runs. error-bars represent confidence intervals. shubhomoy weng-keen wong thomas dietterich alan fern andrew emmott. incorporating expert feedback active anomaly discovery. proceedings ieee international conference data mining. andrew emmott shubhomoy thomas dietterich alan fern wengkeen wong. systematic construction anomaly detection benchmarks real data. corr abs/. http//arxiv.org/abs/. figure low-dimensional visualization abalone ann-thyroid-v using t-sne. plus signs anomalies circles nominals. coloring indicates true anomaly point queried. green indicates nominal point queried. grey circles correspond unqueried nominals. make unqueried anomalies stand visually indicate blue plus signs. amran siddiqui alan fern thomas dietterich shubhomoy das. finite sample complexity rare pattern anomaly detection. conference uncertainty artificial intelligence swee chuan ming ting tony liu. fast anomaly detection streaming data. proceedings twenty-second international joint conference artificial intelligence volume two. kevin woods christopher doss kevin bowyer jeffrey solka carey priebe philip kegelmeyer. comparative evaluation pattern recognition techniques detection microcalcifications mammography. international journal pattern recognition artificial intelligence figure comparison assigning weights leaf nodes assigning weights leaf intermediate nodes curves show total number true anomalies seen number queries. weight leaf node if-aad leaf negative path length root intermediate nodes ignored. weight node if-aad", "year": 2017}