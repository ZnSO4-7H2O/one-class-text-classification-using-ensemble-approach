{"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "tag": ["cs.LG", "cs.AI"], "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.", "text": "training generative models generative adversarial uses discriminative model guide training generative model enjoyed considerable success generating real-valued data. however limitations goal generating sequences discrete tokens. major reason lies discrete outputs generative model make difﬁcult pass gradient update discriminative model generative model. also discriminative model assess complete sequence partially generated sequence nontrivial balance current score future entire sequence generated. paper propose sequence generation framework called seqgan solve problems. modeling data generator stochastic policy reinforcement learning seqgan bypasses generator differentiation problem directly performing gradient policy update. reward signal comes discriminator judged complete sequence passed back intermediate state-action steps using monte carlo search. extensive experiments synthetic data real-world tasks demonstrate signiﬁcant improvements strong baselines. generating sequential synthetic data mimics real important problem unsupervised learning. recently recurrent neural networks long shortterm memory cells shown excellent performance ranging natural language generation handwriting generation common approach training maximize predictive likelihood true token training sequence given previous observed tokens however argued maximum likelihood approaches suffer so-called exposure bias inference stage model generates sequence iteratively predicts next token conditioned previously predicted ones never observed training data. discrepancy training inference incur accumulatively along sequence become prominent length sequence increases. address problem proposed training strategy called scheduled sampling generative model partially synthetic data preﬁx rather true data deciding next token training stage. nevertheless showed inconsistent training strategy fails address problem fundamentally. another possible solution training/inference discrepancy problem build loss function entire generated sequence instead transition. instance application machine translation task speciﬁc sequence score/loss bilingual evaluation understudy adopted guide sequence generation. however many practical applications poem generation chatbot task speciﬁc loss directly available score generated sequence accurately. general adversarial proposed promising framework alleviating problem. speciﬁcally discriminative learns distinguish whether given data instance real generative learns confuse generating high quality data. approach successful mostly applied computer vision tasks generating samples natural images unfortunately applying generating sequences problems. firstly designed generating real-valued continuous data difﬁculties directly generating sequences discrete tokens texts reason gans generator starts random sampling ﬁrst determistic transform govermented model parameters. such gradient loss w.r.t. outputs used guide generative model slightly change generated value make realistic. generated data based discrete tokens slight change guidance discriminative makes little sense probably corresponding token slight change limited dictionary space secondly give score/loss entire sequence generated; partially generated sequence non-trivial balance good future score entire sequence. paper address issues follow consider sequence generation procedure sequential decision making process. generative model treated agent reinforcement learning state generated tokens action next token generated. unlike work requires task-speciﬁc sequence score bleu machine translation give reward employ discriminator evaluate sequence feedback evaluation guide learning generative model. solve problem gradient cannot pass back generative model output discrete regard generative model stochastic parametrized policy. policy gradient employ monte carlo search approximate state-action value. directly train policy policy gradient naturally avoids differentiation difﬁculty discrete data conventional gan. extensive experiments based synthetic real data conducted investigate efﬁcacy properties proposed seqgan. synthetic data environment seqgan signiﬁcantly outperforms maximum likelihood methods scheduled sampling pg-bleu. three realworld tasks i.e. poem generation speech language generation music generation seqgan signiﬁcantly outperforms compared baselines various metrics including human expert judgement. deep generative models recently drawn signiﬁcant attention ability learning large data endows potential vitality ﬁrst proposed contrastive divergence algorithm efﬁciently training deep belief nets proposed denoising autoencoder learns data distribution supervised learning fashion. learn dimensional representation data instance generate decoding network. recently variational autoencoder combines deep learning statistical inference intended represent data instance latent hidden space still utilizing neural networks non-linear mapping. inference done variational methods. generative models trained maximizing training data likelihood which mentioned suffers difﬁculty approximating intractable probabilistic computations. proposed alternative training methodology generative models i.e. gans training procedure minimax game generative model discriminative model. framework bypasses difﬁculty maximum likelihood learning gained striking successes natural image generation however little progress made applying gans sequence discrete data generation problems e.g. natural language generation popular training rnns maximize likelihood token training data whereas pointed discrepancy training generating makes maximum likelihood estimation suboptimal proposed scheduled sampling strategy later theorized objective function underneath improper explained reason gans tend generate natural-looking samples theory. consequently gans great potential practically feasible discrete probabilistic models currently. pointed sequence data generation formulated sequential decision making process potentially solved reinforcement learning techniques. modeling sequence generator policy picking next token policy gradient methods adopted optimize generator reward function guide policy. practical sequence generation tasks e.g. machine translation reward signal meaningful entire sequence instance game reward signal game. cases state-action evaluation methods monte carlo search adopted contract proposed seqgan extends gans rl-based generator solve sequence generation problem reward signal provided discriminator episode monte carlo approach generator picks action learns policy using estimated overall rewards. sequence generation problem denoted follows. given dataset real-world structured sequences train θ-parameterized generative model produce sequence vocabulary candidate tokens. interpret problem based reinforcement learning. timestep state current produced tokens action next token select. thus policy model stochastic whereas state transition deterministic action chosen i.e. next state current state action next states additionally also train φ-parameterized discriminative model provide guidance improving generator probability indicating likely sequence real sequence data not. illustrated figure disn sampled based roll-out policy current state. experiment generator simpliﬁed version speed priority reduce variance accurate assessment action value roll-out policy starting current state till sequence times batch output samples. thus have beneﬁt using discriminator reward function dynamically updated improve generative model iteratively. realistic generated sequences shall re-train discriminator model follows time discriminator model obtained ready update generator. proposed policy based method relies upon optimizing parametrized policy directly maximize long-term reward. following gradient objective function w.r.t. generator’s parameters derived form deterministic state transition zero intermediate rewards. detailed derivation provided appendix. using likelihood ratios build unbiased estimation figure illustration seqgan. left trained real data generated data right trained policy gradient ﬁnal reward signal provided passed back intermediate action value monte carlo search. criminative model trained providing positive examples real sequence data negative examples synthetic sequences generated generative model time generative model updated employing policy gradient search basis expected reward received discriminative model reward estimated likelihood would fool discriminative model speciﬁc formulation given next subsection. reward complete sequence. note reward discriminator discuss later. action-value function sedφ quence i.e. expected accumulative reward starting state taking action following policy rational objective function sequence starting given initial state goal generator generate sequence would make discriminator consider real. next question estimate action-value function. paper reinforce algorithm consider estimated probability real discriminator reward. formally have however discriminator provides reward value ﬁnished sequence. since actually care longterm reward every timestep consider ﬁtness previous tokens also resulted future outcome. similar playing games chess players sometimes would give immediate interests long-term victory thus evaluate action-value intermediate state apply monte carlo search roll-out policy sample unknown last tokens. represent n-time monte carlo search pre-training generator discriminator trained alternatively. generator gets progressed training g-steps updates discriminator needs retrained periodically keeps good pace generator. training discriminator positive examples given dataset whereas negative examples generated generator. order keep balance number negative examples generate d-step positive examples. reduce variability estimation different sets negative samples combined positive ones similar bootstrapping generative model sequences recurrent neural networks generative model. maps input embedding representations sequence sequence hidden states using update function recursively. parameters bias vector weight matrix deal common vanishing exploding gradient problem backpropagation time leverage long short-term memory cells implement update function worth noticing variants discriminative model sequences deep discriminative models deep neural network convolutional neural network recurrent convolutional neural network shown high performance complicated sequence classiﬁcation tasks. paper choose discriminator recently shown great effectiveness text classiﬁcation discriminative models perform classiﬁcation well entire sequence rather unﬁnished one. paper also focus situation discriminator predicts probability ﬁnished sequence real. k-dimensional token embedding concatenation operator build matrix rt×k. kernel rl×k applies convolutional operation window size words produce feature operator summation elementwise production bias term non-linear function. various numbers kernels different window sizes extract different features. finally apply max-over-time pooling operation feature maps max{c ct−l+}. enhance performance also highway architecture based pooled feature maps. finally fully connected layer sigmoid activation used output probability input sequence real. optimization target minimize cross entropy ground truth label predicted probability formulated test efﬁcacy understanding seqgan conduct simulated test synthetic data. simulate real-world structured sequences consider language model capture dependency tokens. randomly initialized lstm true model oracle generate real data distribution following experiments. evaluation metric beneﬁt oracle ﬁrstly provides training dataset secondly evaluates exact performance generative models possible real data. know trying minimize cross-entropy true data distribution approximation i.e. −ex∼p however accurate evaluating generative models draw samples human observers review based prior knowledge. assume human observer learned accurate model natural distribution phuman. order increase chance passing turing test actually need minimize exact opposite average negative log-likelihood −ex∼q phuman role exchanged. synthetic data experiments consider oracle human observer real-world problems thus perfect evaluation metric goracle test stage generate sequence samples calculate nlloracle sample goracle average score. also signiﬁcance tests performed compare statistical properties generation performance baselines seqgan. training setting synthetic data experiments ﬁrst initialize parameters lstm network following normal distribution oracle describing real data distribution goracle. generate sequences length training generative models. seqgan algorithm training discriminator comprised generated examples label instances label different tasks design speciﬁc structure convolutional layer synthetic data experiments kernel size number kernel size between dropout regularization used avoid over-ﬁtting. four generative models compared seqgan. ﬁrst model random token generation. second trained lstm third scheduled sampling fourth policy gradient bleu scheduled sampling training process gradually changes fully guided scheme feeding true previous tokens lstm towards less guided scheme mostly feeds lstm generated tokens. curriculum rate used control probability replacing true tokens generated ones. good stable performance decrease every training epoch. pg-bleu algorithm bleu metric measuring similarity between generated sequence references score ﬁnished samples monte carlo search. results nlloracle performance generating sequences compared policies provided table since evaluation metric fundamentally instructive impact seqgan outperforms baselines significantly. signiﬁcance t-test nlloracle score distribution generated sequences compared models also performed demonstrates signiﬁcant improvement seqgan compared models. learning curves shown figure illustrate superiority seqgan explicitly. training epochs maximum likelihood estimation schedule sampling methods converge relatively high nlloracle score whereas seqgan improve limit generator structure baselines signiﬁcantly. indicates prospect applying adversarial training strategies discrete sequence generative models breakthrough limitations mle. additionally seqgan outperforms pg-bleu means discriminative signal general effective predeﬁned score guide generative policy capture underlying distribution sequence data. discussion synthetic data experiments stability seqgan depends training strategy. speciﬁcally g-steps d-steps parameters algorithm large effect convergence performance seqgan. figure shows effect parameters. figure g-steps much larger d-steps epoch number means train generator many times update discriminator. strategy leads fast convergence generator improves quickly discriminator cannot fully trained thus provide misleading signal gradually. figure discriminator training epochs unstable training process alleviated. figure train generator epoch discriminator gets figure negative log-likelihood convergence performance seqgan different training strategies. vertical dashed line represents beginning adversarial training. d-steps three training strategies described means generate negative examples number given dataset train discriminator various epochs. actually utilize potentially unlimited number negative examples improve discriminator. trick considered type bootstrapping combine ﬁxed positive examples different negative examples obtain multiple training sets. figure shows technique improve overall performance good stability since discriminator shown negative examples time positive examples emphasized lead comprehensive guidance training generator. line theorem analyzing convergence generative adversarial nets important assumption discriminator allowed reach optimum given discriminator capable differentiating real data unnatural data consistently supervised signal meaningful whole adversarial training process stable effective. text generation text generation scenarios apply proposed seqgan generate chinese poems barack obama political speeches. poem composition task corpus chinese quatrains containing four lines twenty characters total. focus fully automatic solution stay general prior knowledge special structure rules chinese poems speciﬁc phonological rules. obama political speech generation task corpus collection paragraphs obama’s political speeches. bleu score evaluation metric measure similarity degree generated texts human-created texts. bleu originally designed automatically judge machine translation quality point compare similarity results created machine references provided human. speciﬁcally poem evaluation n-gram since words classical chinese poems consist characters similar reason bleu- bleu- evaluate obama speech generation performance. work whole test references instead trying references following line given previous line reason generation tasks provide positive examples model catch patterns generate ones. addition bleu also choose poem generation case human judgement since poem creative text construction human evaluation ideal. speciﬁcally real poems generated seqgan mle. experts chinese poems invited judge whether poem created human machines. regarded real gets score otherwise finally average score algorithm calculated. experiment results shown tables signiﬁcant advantage seqgan text generation. particularly poem composition seqgan performs comparably real human data. music generation music composition nottingham dataset training data collection music folk tunes midi format. study solo track music. work numbers represent pitches model ﬁtness discrete piano patterns bleu used evaluation metric. model ﬁtness continuous pitch data patterns mean squared error used evaluation. paper proposed sequence generation method seqgan effectively train generative adversarial nets structured sequences generation policy gradient. best knowledge ﬁrst work extending gans generate sequences discrete tokens. synthetic data experiments used oracle evaluation mechanism explicitly illustrate superiority seqgan strong baselines. three real-world scenarios i.e. poems speech language music generation seqgan showed excellent performance generating creative sequences. also performed experiments investigate robustness stability training seqgan. future work plan build monte carlo tree search value network improve action decision making large scale data case longer-term planning.", "year": 2016}