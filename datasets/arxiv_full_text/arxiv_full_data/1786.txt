{"title": "End-to-End Training Approaches for Discriminative Segmental Models", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses.  We investigate a model class based on recent successful approaches, consisting of a linear model that combines segmental features based on an LSTM frame classifier. Similarly to hybrid HMM-neural network models, segmental models of this class can be trained in two stages (frame classifier training followed by linear segmental model weight training), end to end (joint training of both frame classifier and linear weights), or with end-to-end fine-tuning after two-stage training.  We study segmental models trained end to end with hinge loss, log loss, latent hinge loss, and marginal log loss. We consider several losses for the case where training alignments are available as well as where they are not.  We find that in general, marginal log loss provides the most consistent strong performance without requiring ground-truth alignments. We also find that training with dropout is very important in obtaining good performance with end-to-end training. Finally, the best results are typically obtained by a combination of two-stage training and fine-tuning.", "text": "recent work discriminative segmental models shown achieve competitive speech recognition performance using features based deep neural frame classiﬁers. however segmental models challenging train standard frame-based approaches. segmental models successfully trained lack understanding training different settings different losses. investigate model class based recent successful approaches consisting linear model combines segmental features based lstm frame classiﬁer. similarly hybrid hmm-neural network models segmental models class trained stages end-to-end ﬁne-tuning two-stage training. study segmental models trained hinge loss loss latent hinge loss marginal loss. consider several losses case training alignments available well not. loss provides consistent strong performance without requiring ground-truth alignments. also training dropout important obtaining good performance end-to-end training. finally best results typically obtained combination two-stage training ﬁne-tuning. end-to-end training proved successful example connectionist temporal classiﬁcation encoderdecoders hidden markov model based hybrid systems deep segmental neural networks segmental recurrent neural networks models feature encoder output model generating label sequences. feature encoder recurrent feedforward neural network output model recurrent neural decoder long short-term memory network probabilistic graphical model conditional random ﬁeld semimarkov crf. actual deﬁnition end-to-end training rarely made explicit literature. work deﬁne end-to-end training optimizing encoder parameters output model parameters jointly. alternative refer two-stage training optimizes feature encoder output model parameters separately stages. families training approaches differ terms annotation requirements computational learning efﬁciency loss functions customarily used each. two-stage training typically requires frame-level labels ﬁrst stage therefore require fewer samples learn end-to-end training avoids cascading errors pipelines results hard-to-optimize objectives sensitive initialization. also possible perform end-to-end ﬁne-tuning two-stage training found useful past work work study training approaches segmental models. segmental models shown successful trained scratch focus particular class segmental models lstms encoders linear segmental models output models. models trained stages often extra restriction representation encoded features. example probabilities triphone states hybrid systems systems trained constrained. enable fair comparison model architectures seamlessly permit kinds training without requiring change model parameterization. difference two-stage training leads interpretable encoded features functional architectures identical. order thoroughly compare two-stage end-to-end training consider variety loss functions training settings. end-to-end systems ﬁrst proposed ctc-lstms encoder-decoders srnns tied speciﬁc loss functions per-output cross note though model class suitable studying end-to-end systems various aspects using better encoders srnns might lead better absolute performance. learning segmental model amounts ﬁnding parameters minimize speciﬁed loss function. learning divided cases access ground truth segmentations without. ground truth segmentations receive dataset learning aims solve since segmental models fall structured prediction general loss function structured prediction applicable segmental models. particular investigate hinge loss loss case ground truth segmentations latent hinge loss marginal loss case without ground truth segmentations. loss deﬁnitions given terms single training sample entropy marginal loss. however systems trained different loss functions; e.g. encoder-decoder systems trained hinge loss thus important isolate effect training loss functions models. model class deﬁnition encoder output model completely independent deﬁnition loss functions. allows compare training losses keeping everything else ﬁxed. two-stage training typically uses ﬁne-grained labels training ﬁrst stage segmentations. datasets timit luxury manually annotated segmentations datasets not. needed segmentations typically inferred force aligning labels frames. model class system trained without segmentations depending choice loss function. following sections explicitly deﬁne model class loss functions particular hinge loss loss cases ground truth segmentations latent hinge loss marginal loss not. perform experiments studying two-stage end-toend training different settings different losses. phoneme recognition task show end-to-end training scratch marginal loss achieves best result setting without ground truth segmentations two-stage training followed end-to-end ﬁne-tuning loss achieves best result setting ground truth segmentations.we also dropout crucial combating overﬁtting. speech recognition sequence prediction general formulated search problem. search space paths composed segments. segment associated weight turn path associated weight. prediction becomes ﬁnding highest weighted path search space. formalize below. input space sequences frames e.g. mfccs ﬁlter bank outputs. label e.g. phone phoneme recognition. segment tuple start time time label. segments connected time start time path sequence connected segments. path also seen label sequence segmentation simply possible segments. segmental model tuple parameter vector feature function uses feature encoder parameterized parameters give deﬁnitions feature encoders feature functions conduct phonetic recognition experiments timit hour phonetically transcribed dataset. follow conventional setting training models -utterance training evaluate -utterance core test set. rest utterances test development set. following convention collapse phones training collapse phones evaluation. feature encoder -layer bidirectional lstm cells layer. outputs third layer projected dimensions pass log-softmax layer ﬁnal output probabilities. inputs encoder -dimensional mfccs normalized dimension subtracting mean dividing standard deviation calculated training set. cost function loss not. fact prediction hinge loss always upper bound cost function. hinge loss non-smooth operation loss smooth. hinge loss loss convex non-convex neural network used. latent hinge loss marginal loss require ground truth segmentations. prediction latent hinge loss also upper bound cost function. latent hinge loss non-smooth marginal loss smooth. latent hinge loss marginal loss non-convex hinge loss training segmental models ﬁrst appeared loss marginal loss training ﬁrst-pass segmental models ﬁrst hinge loss ﬁrst loss ﬁrst marginal loss. training ﬁrst-pass segmental models ﬁrst marginal loss. loss functions empirical bayes risk structured ramp loss used training segmental models. loss functions optimized stochastic gradient descent variants. propagate gradients back feature function allowing parameters updated jointly. assume feature encoder example lstm produces given input project |l|-dimensional vector pass resulting vector log-softmax layer words logpℓ projection matrix. case parameters includes projection matrix parameters lstm. frame classiﬁers cross entropy loss frame. lstm serve feature encoder later training lstm corresponds ﬁrst stage two-stage learning. lstm parameters initialized uniformly range biases forget gates initialized biases initialized zero. dropout lstms applied input layers last output layer dropout rate compare adagrad step sizes rmsprop step size decay mini-batch size always utterance. optimizers epochs. choose best performing model according frame error rate development also known early stopping. gradient clipping used training. comparison following train convolutional neural network consisting layer convolution followed fully-connected layers. frame classiﬁcation results shown table observe best performing lstm achieves comparable frame error rate cnn. dropout frame error rate lowered. obtaining lstm frame classiﬁers proceed second stage training segmental models features based lstm probabilities. segmental models trained four loss functions epochs early stopping. overlap cost used hinge loss latent hinge loss. maximum duration frames imposed. feature functions described section regularizer used except early stopping. compare adagrad step sizes rmsprop step size decay phonetic recognition results hinge loss shown table observe lstms perform better frame classiﬁcation give little improvement cnns phonetic recognition. recognition results rest losses table note even though latent hinge loss marginal loss require segmentations during training ground truth segmentations training frame classiﬁer. common setting done purely comparison purposes. observe that except latent hinge losses perform equally well loss slight edge others. four losses compared without dropout. dropout used dropout rate chosen match rate frame classiﬁer training. input layers output layer scaled dropout used. first initialize models trained hinge loss above. adagrad step size epochs early stopping. results shown table observe healthy reductions phone error rates tuning two-stage system across loss functions. also ﬁne-tuning without dropout tends better dropout. though ﬁne-tuning hinge loss leads error reduction note two-stage system trained hinge loss. least centain two-stage system trained hinge loss descent initialization losses. minimizing losses model trained hinge loss less ideal. repeat experiments warm-starting model trained loss function going minimize. results shown table observe signiﬁcant gains loss marginal loss initialized matching loss function. similarly gains dropout cases smaller without dropout. next train architecture scratch. make sure models initialized identically two-stage systems. four losses used training dropout rates ground truth segmentations used training hinge loss loss disregarded training latent hinge loss marginal loss. optimizers step sizes momentum gradient clipping norm adagrad step sizes clipping rmsprop step size decay clipping. earch optimizer epochs early stopping. results shown table first optimizers fail minimize latent hinge loss. stuck local optima fail produce reasonable forced alignments. even though loss functions end-to-end training nonconvex latent hinge loss sensitive initialization losses. second observation adding dropout improves performance. however using dropout rate two-stage system results worse performance. finally though behind best ﬁne-tuned model marginal loss dropout slightly edges losses. seen end-to-end training initialized twostage system leads best results. since end-to-end training meaning intermediate representations enforced anymore unclear intermediate representations deviate learned ones. answer this measure per-frame cross entropy lstm frame classiﬁer end-to-end training. results shown table first per-frame cross entropy best performing lstm training shows -layer bidirectional lstm cells layer able essentially memorize entire timit dataset. however severely overﬁtting. early stopping dropout help balance cross entropies training development set. addition cross entropies sets drop end-to-end training. shows meaning intermediate representations still maintained lstms end-to-end training. next since system trained marginal loss ground truth segmentations since evaluation measure consider segmentations know system able discover reasonble phone boundaries without supervision. approach question aligning label sequences acoustics compare resulting segmentations manually annotated segmentations. alignment quality different tolerance values shown table though results behind models trained speciﬁcally align segmentable forced alignment quality test percentage correctly positioned phone boundaries within predeﬁned tolerance measured best-performing segmental model trained marginal loss. since speech datasets manually annotated segmentations desirable train without manual alignments. know alignments produced system trained marginal loss good quality. therefore forced alignments train two-stage system followed end-to-end ﬁne-tuning. follow exact procedure previous two-stage experiments training lstm frame classiﬁer forced alignments followed training segmental model hinge loss. frame error rate development lstm classiﬁer forced alignments ground-truth segmentations. though frame error rate signiﬁcantly worse training ground-truth segmentations two-stage system achieves phone error rate development set. ﬁne-tune entire system hinge loss. ﬁnal system achieves phone error rate development test signiﬁcant improvement model trained end-to-end marginal loss relying ground truth segmentations. terms efﬁciency training four losses require forward-backward-like algorithms computing gradients. hinge loss requires pass entire search space loss requires passes entire search space latent hinge requires pass entire search space segmentation space marginal loss requires passes entire search space passes segmentation space. average number hours epoch spent computing gradients excluding lstm computations shown table context feeding forward backpropagation lstms takes hours epoch. timing done single .ghz four-core cpu. number hours consistent number passes required compute gradients. note time spent lstms halved without incurring performance loss applying frame skipping shown segmental models work study end-to-end training context segmental models. model class choice includes layer bidirectional lstm feature encoder segmental model using features produce label sequences. model class suitable studying end-to-end training ﬂexibility trained either stage manner end. hypothesis training systems stages easier end-to-end training scratch. hand end-to-end training better optimize loss function might sensitive initialization. model deﬁnition separated deﬁnition loss functions giving ﬂexibility choose loss functions based training settings. consider common training settings ground truth segmentations without. hinge loss loss require segmentations deﬁnition latent hinge loss marginal loss not. show case ground truth segmentations two-stage training followed end-to-end training signiﬁcantly better two-stage training alone end-to-end training scratch. addition end-to-end training marginal loss scratch achieves competitive results. byproduct system able generate high-quality forced alignments. remove dependency ground truth segmentations train another model forced alignments stages followed end-to-end ﬁne-tuning improving upon end-to-end training scratch relative. ﬁnal product strong system trained without requiring ground truth segmentations. research supported google faculty research award grant iis-. opinions expressed work authors necessarily reﬂect views funding agency. gpus used research donated nvidia. daniel povey vijayaditya peddinti daniel galvez pegah ghahrmani vimal manohar xingyu yiming wang sanjeev khudanpur purely sequencetrained neural networks based lattice-free annual conference international speech communication association ossama abdel-hamid deng dong deep segmental neural networks speech jiang recognition. annual conference international speech communication association liang lingpeng kong chris dyer noah smith steve renals segmental recurrent neural networks end-to-end speech recognition annual conference international speech communication association karel vesel`y arnab ghoshal luk´as burget daniel povey sequence-discriminative training deep neural networks. annual conference international speech communication association alex graves navdeep jaitly abdel-rahman mohamed hybrid speech recognition deep bidirectional lstm ieee workshop automatic speech recognition understanding geoffrey zweig patrick nguyen segmental approach large vocabulary continuous speech recognition ieee workshop automatic speech recognition understanding tang weiran wang kevin gimpel karen discriminative segmental cascades livescu feature-rich phone recognition ieee workshop automatic speech recognition understanding tang kevin gimpel karen livescu comparison training approaches discriminative segmental models annual conference international speech communication association yanzhang eric fosler-lussier efﬁcient segmental conditional random ﬁelds phone recognition annual conference international speech communication association joseph keshet shai shalev-shwartz yoram singer chazan large margin algorithm speech-tophoneme music-to-score alignment ieee transactions audio speech language processing vol. yajie miao jinyu yongqian wang shixiong zhang yifan gong simplifying long short-term memory acoustic models fast training decoding ieee international conference acoustics speech signal processing tang weiran wang kevin gimpel karen livescu efﬁcient segmental cascades speech recognition annual conference international speech communication association", "year": 2016}