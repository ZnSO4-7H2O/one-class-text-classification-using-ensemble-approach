{"title": "Alternating Back-Propagation for Generator Network", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.", "text": "generator network fundamental representation knowledge following properties analysis model disentangles variations observed signals independent variations latent factors. synthesis model synthesize signals sampling factors known prior distribution transforming factors signal. embedding model embeds high-dimensional non-euclidean manifold formed observed signals low-dimensional euclidean space latent factors linear interpolation lowdimensional factor space results non-linear interpolation data space. alternating back-propagation factor analysis model learned rubin-thayer algorithm e-step m-step based multivariate linear regression. inspired algorithm propose alternating back-propagation algorithm learning generator network iterates following two-steps langevin dynamics stochastic sampling counterpart gradient descent. gradient computations steps powered back-propagation. convnet structure gradient computation step actually by-product gradient computation step terms coding. given factors learning convnet supervised learning problem accomplished learning backpropagation. factors unknown learning becomes unsupervised problem solved adding inferential back-propagation inner loop learning paper proposes alternating back-propagation algorithm learning generator network model. model nonlinear generalization factor analysis. model mapping continuous latent factors observed signal parametrized convolutional neural network. alternating back-propagation algorithm iterates following steps inferential back-propagation infers latent factors langevin dynamics gradient descent. learning back-propagation updates parameters given inferred latent factors gradient descent. gradient computations steps powered back-propagation share code common. show alternating back-propagation algorithm learn realistic generator models natural images video sequences sounds. moreover also used learn incomplete indirect training data. paper studies fundamental problem learning inference generator network generative model become popular recently. speciﬁcally propose alternating back-propagation algorithm learning inference model. non-linear factor analysis generator network non-linear generalization factor analysis. factor analysis prototype model unsupervised learning distributed representations. directions pursue order generalize factor analysis model. direction generalize prior model prior assumption latent factors. methods independent component analysis sparse coding non-negative matrix factorization matrix factorization completion recommender systems etc. direction generalize factor analysis model generalize mapping continuous latent factors observed signal. generator network example direction. generalizes linear mapping factor analysis non-linear mapping deﬁned alternating back-propagation algorithm follows tradition alternating operations unsupervised learning alternating linear regression algorithm factor analysis alternating least squares algorithm matrix factorization alternating gradient descent algorithm sparse coding unsupervised learning algorithms alternate inference step learning step case alternating back-propagation. explaining-away inference inferential back-propagation solves inverse problem explaining-away process latent factors compete explain training example. following advantages explaining-away inference latent factors latent factors follow sophisticated prior models. instance textured motions dynamic textures latent factors follow dynamic model vector auto-regression. inferring latent factors explain observed examples learn prior model. observed data incomplete indirect. instance training images contain occluded objects. case latent factors still obtained explaining incomplete indirect observations model still learned before. learning incomplete indirect data venture propose main advantage generative model learn incomplete indirect data uncommon practice. generative model evaluated based well recovers unobserved original data still learning model generate data. learning generator network incomplete data considered non-linear generalization matrix completion. contribution related work main contribution paper propose alternating back-propagation algorithm training generator network. another contribution evaluate generative models learning incomplete indirect training data. existing training methods generator network avoid explain-away inference latent factors. methods recently devised accomplish this. methods involve assisting network separate parameters addition original network generates signals. method variational auto-encoder assisting network inferential recognition network seeks approximate posterior distribution latent factors. method generative adversarial network assisting network discriminator network plays adversarial role generator network. unlike alternating back-propagation perform explicit explain-away inference avoids inferring latent factors altogether. comparison alternating back-propagation algorithm simpler basic without resorting extra network. difﬁcult compare methods directly illustrate strength alternating back-propagation learning incomplete indirect data need explain whatever data given. prove difﬁcult less convenient gan. meanwhile alternating back-propagation complementary training. initialize inferential back-propagation result improve inference vae. inferential back-propagation help infer latent factors observed examples thus providing method test explain entire training set. factor analysis beyond d-dimensional observed data vector image. d-dimensional vector continuous latent factors traditional factor analysis model matrix d-dimensional error vector observational noise. assume stands d-dimensional identity matrix. also assume i.e. observational errors gaussian white noises. three perspectives view basis vectors. write zkwk i.e. basis vectors coefﬁcients. loading matrix. write j-th j-th components respectively. loading factors vector loading weights indicating factors important determining called loading matrix. matrix factorization. suppose observe whose factors factor analysis model prototype many subsequent models generalize prior model independent component analysis assumed follow independent heavy tailed distributions. sparse coding assumed redundant sparse vector i.e. small number non-zero signiﬁcantly different zero. non-negative matrix factorization assumed recommender system vector customer’s desires different aspects vector product desirabilities aspects. convnet mapping addition generalizing prior model latent factors also generalize mapping paper consider generator network model retains assumptions traditional factor analysis generalizes linear mapping non-linear mapping convnet collects connection weights bias terms convnet. model becomes reconstruction error assume sophisticated models colored noise non-gaussian texture. binary emit probability sigmoid transformation bernoulli sampling carried pixel-wise. multi-level assume multinomial logistic emission model ordinal emission model. although non-linear mapping convnet parameterization makes particularly close original factor analysis. speciﬁcally write top-down convnet follows element-wise non-linearity layer matrix connection weights vector bias terms layer top-down convnet considered recursion original factor analysis model factors layer obtained linear superposition basis vectors basis functions column vectors factors layer serving coefﬁcients linear superposition. case convnet basis functions shift-invariant versions another like wavelets. appendix in-depth understanding model. observe training data vectors corresponding share convnet intuitively infer {zi} ||yi plus regularization term corresponds prior formally model written adopting language complete-data logi= assume given. learning inference accomplished maximizing complete-data log-likelihood obtained alternating gradient descent algorithm iterates following steps inference step update running steps gradient descent. learning step update step gradient descent. denotes time step langevin sampling step size denotes random vector follows langevin dynamics explain-away process latent factors compete explain away current residual explain langevin dynamics continuous time version sampling exp] xt+∆t ∆te/ ∆tut. dynamics stationary distribution shown wellbehaved testing function xt+∆t alternatively given suppose xt+∆t k]/k] stochastic gradient algorithm used learning iteration single copy sampled running ﬁnite number steps langevin dynamics starting current value i.e. warm start. {zi} sampled close prior small posterior multi-modal evolving energy landscape help alleviate trapping local modes. practice tune value instead estimating langevin dynamics extended hamiltonian monte carlo sophisticated versions code experiments based matconvnet package training images sounds scaled intensities within range adopt structure generator network topnetwork consists multiple layers deconvolution linear superposition relu non-linearity up-sampling tanh non-linearity bottom-layer make signals fall within also adopt batch normalization standard deviation noise vector steps langevin dynamics within learning iteration langevin step size learning iterations learning rate momentum learning algorithm produces learned network parameters inferred latent factors signal end. synthesized signals obtained sampled prior distribution experiment modeling texture patterns. learn separate model texture image. images collected internet resized synthesized images figures shows four examples. inferential back-propagation steps langevin dynamics sample warm start i.e. starting current step follows equation learning back-propagation update computed according equation learning rate gaussian noise langevin dynamics removed algorithm becomes alternating gradient descent algorithm. possible update {zi} simultaneously joint gradient descent. inferential back-propagation learning back-propagation guided residual inferential back-propagation based whereas learning back-propagation based gradients efﬁciently computed back-propagation. computations gradients share steps. speciﬁcally top-down convnet deﬁned share code chain rule computation thus code part code algorithm langevin dynamics samples gradually changing posterior distribution because keeps changing. updating collaborate reduce reconstruction error parameter plays role annealing tempering langevin sampling. large posterior image transformed top-down convnet. learning stage texture experiments. order obtain synthesized image randomly sample expand learned network generate synthesized image training network follows. starting image network layers deconvolution kernels up-sampling factor layer number channels ﬁrst layer decreased factor layer. langevin steps step size experiment modeling sound patterns. sound signal treated one-dimensional texture image sound data collected internet. training signal second clip sampling rate hertz represented vector. learn separate model sound signal. latent factors form sequence follows top-down network consists layers deconvolution kernels size up-sampling factor number channels ﬁrst layer decreases factor layer. synthesis start longer gaussian white noise sequence generate synthesized sound expanding learned network. figure shows waveforms observed sound signal ﬁrst synthesized sound signal second row. experiment modeling object patterns. model object patterns using network structure essentially network texture model except include fully connected layer latent factors d-dimensional vector. images relu leaking factor langevin steps step size figure modeling object patterns. left synthesized images generated method. generated learned discretized values. right synthesized images generated using deep convolutional generative adversarial discretized values within figure modeling object patterns. left image generated method obtained ﬁrst sampling generating image learned middle interpolation. images four corners reconstructed inferred vectors four images randomly selected training set. image middle obtained ﬁrst interpolating vectors four corner images generating image right synthesized images generated dcgan dimension vector sampled uniform distribution. second experiment learn model face images randomly selected celeba dataset left panel figure displays images generated learned model. middle panel displays interpolation results. images four corners generated vectors four images randomly selected training set. images middle obtained ﬁrst interpolating four corner images using sphere interpolation generating images learned convnet. lion-tiger dataset using -dimensional right panel figure displays generated results trained aligned faces celeba dataset code https//github.com/carpedm/ dcgan-tensorflow tuning parameters iterations method. experiment modeling dynamic patterns. model textured motion dynamic texture non-linear dynamic system assume latent factors follow vector auto-regressive model matrix innovation. model direct generalization linear dynamic system reduced principal component analysis singular value decomposition learn model steps. treat {yt} independent examples learn infer {zt} before. treat {zt} training data learn that synthesize dynamic texture. start generate sequence according learned model figure shows experiments ﬁrst segment sequence generated model second generated method dimensionality possible generalize auto-regressive model recurrent network. also treat video sequences images learn generator networks spatial-temporal ﬁlters basis functions. puted summing pixels. partially observed image compute summing observed pixels. continue alternating backpropagation algorithm infer learn inferred learned image automatically recovered able accomplish following tasks recover occluded pixels training images. synthesize images learned model. recover occluded pixels testing images using learned model. figure learning incomplete data. columns belong experiments respectively. original images observed learning. training images. recovered images learning. want emphasize experiments training images partially occluded. experiments different de-noising auto-encoder training images fully observed noises added matter regularization in-painting de-noising prior model regularization already learned given. task mentioned above tasks evaluate method images randomly selected celeba dataset. design experiments types occlusions experiments salt pepper occlusion randomly place masks image domain cover roughly pixels respectively. experiments denoted respectively experiments single region mask occlusion randomly place mask image domain. experiments denoted respectively table displays recovery errors experiments error deﬁned pixel difference original image recovered image occluded pixels. emphasize recovery errors training errors intensities occluded experiment learning incomplete data. method learn images occluded pixels. task inspired fact images contain occluded objects. considered non-linear generalization matrix completion recommender system. experiment learning indirect data. learn model compressively sensed data generate white noise images random projections. project training images white noise images. learn model random projections instead original images. need replace given white noise sensing matrix observation. treat fully connected layer known ﬁlters continue alternating back-propagation infer learn thus recovering image able recover original images projections learning. synthesize images learned model. recover testing images projections based learned model. experiments different traditional compressed sensing task tasks moreover image recovery work based non-linear dimension reduction instead linear sparsity. evaluate method face images randomly selected celeba dataset. images projected onto white noise images pixel randomly sampled random projection image size becomes k-dimensional vector. show recovery errors different latent dimensions table recovery error deﬁned pixel difference original image recovered image. figure shows recovery results. experiment model evaluation reconstruction error testing data. learning model training images evaluate model reconstruction error testing images. randomly select face images training images testing celeba dataset. learning infer latent factors testing image using inferential back-propagation reconstruct testing image figure comparison method pca. original testing images. reconstructions eigenvectors learned training images. reconstructions generator learned training images. methods. using inferred learned inferential back-propagation inferring initialize langevin steps step size table shows reconstruction errors alternating backpropagation learning compared learning different latent dimensions figure shows reconstructed testing images. learn eigenvectors training images project testing images learned eigenvectors reconstruction. paper proposes alternating back-propagation algorithm training generator network. recognize generator network non-linear generalization factor analysis model develop alternating backpropagation algorithm non-linear generalization alternating regression scheme rubin-thayer algorithm ﬁtting factor analysis model. alternating back-propagation algorithm iterates inferential backpropagation inferring latent factors learning back-propagation updating parameters. backpropagation steps share computing steps chain rule calculations. learning algorithm perhaps canonical algorithm training generator network. based maximum likelihood theoretically accurate estimator. maximum likelihood learning seeks explain charge whole dataset uniformly little concern under-ﬁtting biased ﬁtting. unsupervised learning algorithm alternating back-propagation algorithm natural generalization original back-propagation algorithm supervised learning. adds inferential back-propagation step learning back-propagation step minimal overhead coding affordable overhead computing. inferential back-propagation seeks perform accurate explaining-away inference latent factors. worthwhile tasks learning incomplete indirect data learning models latent factors follow sophisticated prior models unknown parameters. inferential back-propagation also used evaluate generators learned methods tasks reconstructing completing testing data. relu piecewise factor analysis generator network element-wise non-linearity modern convnet usually two-piece linearity rectiﬁed linear unit leaky relu relu unit corresponds binary switch. case non-leaky relu following analysis write diag diagonal matrix element-wise indicator function. case leaky relu values diagonal replaced leaking factor forms classiﬁcation according network speciﬁcally factor space divided large number pieces hyperplanes piece indexed instantiation write make explicit dependence piece indexed assuming simplicity δw...δlwl. thus piece deﬁned corresponds linear factor analysis whose basis multiplicative recomposition basis functions multiple layers recomposition controlled binary switches multiple layers hence top-down convnet amounts reconﬁgurable basis representing model piecewise linear factor analysis. retain bias term overall bias term depends distribution essentially piecewise gaussian. generator model considered explicit implementation local linear embedding embedding local linear embedding mapping implicit. generator model mapping explicit. relu convnet mapping piecewise linear consistent local linear embedding except partition linear pieces generator model learned automatically. inferential back-propagation langevin dynamics energy function belongs piece deﬁned inferential backpropagation seeks approximate basis ridge regression. keeps changing langevin dynamics also changing algorithm searches optimal reconﬁgurable basis approximate solve second-order methods iterated ridge regression computationally expensive simple gradient descent. density mapping density shifting suppose training data come data distribution pdata. understand alternating back-propagation algorithm idealization maps prior distribution latent factors data distribution pdata learned deﬁne right hand side minimized maximum likelihood estimate hence data prior pdata especially close true prior words posteriors data points pdata tend pave true prior rubin’s multiple imputation point view algorithm e-step infers number multiple imputations multiple guesses multiple guesses account uncertainty inferring m-step maximizes obtain wt+. data point seeks reconstruct inferred latent factors words m-step seeks pooling pdata hence m-step seeks pdata data distribution pdata. course mapping cannot exact. fact maps d-dimensional patch around d-dimensional local patches {yi∀i} patch d-dimensional manifold form d-dimensional observed examples interpolations. algorithm process density shifting pdata shifts towards thus maps pdata. factor analysis alternating regression alternating back-propagation algorithm inspired rubin-thayer algorithm factor analysis observed data model posterior distribution available closed form. algorithm factor analysis interpreted alternating linear regression factor analysis model joint distribution e-step based multivariate linear regression given m-step updates multivariate linear regression steps accomplished sweep operator. notation gram matrices highlight analogy steps. algorithm considered alternating linear regression alternating sweep operation serves prototype alternating back-propagation.", "year": 2016}