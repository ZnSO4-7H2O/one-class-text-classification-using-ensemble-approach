{"title": "StackSeq2Seq: Dual Encoder Seq2Seq Recurrent Networks", "tag": ["cs.LG", "cs.DM", "cs.NE", "stat.ML"], "abstract": "A widely studied non-deterministic polynomial time (NP) hard problem lies in finding a route between the two nodes of a graph. Often meta-heuristics algorithms such as $A^{*}$ are employed on graphs with a large number of nodes. Here, we propose a deep recurrent neural network architecture based on the Sequence-2-Sequence (Seq2Seq) model, widely used, for instance in text translation. Particularly, we illustrate that utilising a context vector that has been learned from two different recurrent networks enables increased accuracies in learning the shortest route of a graph. Additionally, we show that one can boost the performance of the Seq2Seq network by smoothing the loss function using a homotopy continuation of the decoder's loss function.", "text": "primary problem surrounding recurrent neural network’s approximation shortest route problem diculty network encode longer sequences. problem partly alleviated network architectures long shortterm memory gated recurrent units eorts also towards neural turing machines dierentiable neural computer augmented erentiable) external memory selectively read wrien paper shortest path problem empirical example understand information longer sequences retained rnn. testbed nding shortest routes points graph allow control length dependencies also requisite computational complexity inference problem. path-nding algorithm formulate novel recurrent network based sequence-to-sequence architecture. specically show using context vectors generated dierent recurrent networks facilitate decoder increased accuracy approximating shortest route estimated algorithm. moreover method diers pointer network dierent encoders ptr-net uses aention decoder pointer select member input sequence encoder. methods section describe data-sets procedure generating routes training/test datasets architecture dual encoder seqseq network forms novel contribution paper. calculations performed .ghz workstation single nvidia geforce graphics card. datasets graph based road network minnesota. node represents intersections roads edges represent road connects points intersection. specically graph considered nodes edges constrained coordinates nodes range longitude latitude instead full extent graph i.e. longitude latitude total number nodes. abstract widely studied non-deterministic polynomial time hard problem lies nding route nodes graph. meta-heuristics algorithms employed graphs large number nodes. here propose deep recurrent neural network architecture based sequence--sequence model widely used instance text translation. particularly illustrate utilising context vector learned dierent recurrent networks enables increased accuracies learning shortest route graph. additionally show boost performance seqseq network smoothing loss function using homotopy continuation decoder’s loss function. keywords recurrent neural networks sequence-to-sequence models shortest paths graphs meta-heuristic homotopy continuation reference format alessandro biswa sengupta. stackseqseq dual encoder seqseq recurrent networks. proceedings international workshop heterogeneous networks analysis mining angeles california february pages. ./nnnnnnn.nnnnnnn introduction intersection discrete optimization graph theory lies age-old problem nding shortest routes nodes graph. many theoretical properties shortest path algorithms understood posing graph graphs inventory delivery algorithm posed road network graph clustering similar images videos traditionally discrete non-deterministic polynomial hard optimisation problems studied using metaheuristics algorithms algorithm. algorithms notable mention dantzig-fulkerson-johnson algorithm branch-and-cut algorithms neural networks etc. recent work proposed recurrent neural network learn adjacency matrix graph using training paths generated using vanilla algorithm. permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. heteronam’ angeles california acm. -x-xxxx-xxxx-x/yy/mm.... ./nnnnnnn.nnnnnnn meta-heuristics. algorithm best-rst search algorithm wherein searches amongst possible paths yield smallest cost. cost function made parts particularly iteration algorithm consists evaluating distance travelled time expended start node current node. second part cost function heuristic estimates cost cheapest path current node goal. without heuristic part algorithm operationalises dijkstra’s algorithm many variants experiments vanilla heuristic based euclidean distance. variants anytime repairing shown give superior performance recurrent deep networks. operationalise network follows start feeding source node embedded matrix learned training phase encoder. embedded destination node second time step encoder. resulting hidden states encoders stacked build context vector maintains memory trace source destination. used initial hidden state decoder. subsequently decoder takes input embedding node training sequence. during test phase instead input node predicted previous time step. order predict following node output layer somax non-linear function; gives probability distribution allows rank probable node among remaining nodes corresponding subsequent hop. finally training phase negative log-likelihood cost function dual context seqseq model dierent latent representations learned using dierent encoders context vector takes form stacked latent encoding. figure show context vectors stacked matrix path training set. encoders respective matrices full rank; stacked context vector also full rank. means lstm encode dierent context vectors worth considering accurate encoding. dual context seqseq model dierent latent representations learned using dierent encoders decoder represented vanilla trained homotopy continuation done convolving loss function gaussian kernel details please refer novel contribution lies extending framework obtaining analytic approximation log-somax function. table illustrates diused forms popular activation functions. table list diused forms report popular non-linear activation functions along diused form. obtained convolving function heat kernel table extends work analytic approximation somax function. details please refer networks shown figure input represented tuple encoded context vector subsequently decoded sequence obtain shortest path connecting source destination. moreover test phase compute paths source destination node destination source node forms intersection result shortest path. results graph minnesota nodes edges generated shortest routes randomly picked nodes using algorithm. used routes training seqseq algorithms using training-test splits. figure dual-context sequence-to-sequence architecture approximating meta-heuristics. networks modules encoder last four represent decoded output representing shortest route holborn bank. network trained using shortest route snippets generated using algorithm. represents context vector. epochs updating parameters adam optimisation scheme parameters starting learning rate equal hand diused loss function smooth cost function using gaussian kernel standard deviation training iterates converged epochs value prediction accuracy test data-set reported table doubling hidden state dimension marginally increases percentage shortest paths successful paths necessarily shortest proposed dual encoder composed context vectors dimension total components achieves improvement shortest paths moreover trained diusion turns best performing algorithm accuracy shortest paths successful cases. means proposed encoder signicantly increases number retrieved shortest paths thanks dual dynamics. important highlight dimension latent space proportional task complexity wherein experiments demonstrate doubling dimension context vector lstm encoders considered separately bring marginal improvements. however encodings learned dierent recurrent encoders exibility. table results minnesota graph. percentage shortest path successful paths shown wide-variety seqseq models context vector dimension equal either scores relative algorithm achieves shortest path score discussion prime motivation behind current work understand temporal congruency recurrent neural network replace algorithm recurrent neural network. shortest route problem approximated algorithm gives exible control temporal length sequence simply sampling paths longer. additionally another handle computational complexity problem achieved increasing decreasing size graph. experiments illustrate recurrent networks delity memorize varying lengths sequences learning adjacency matrix given graph required latent dimension embedding learned recurrent network dependent task complexity. however remains unclear increasing memory capacity recurrent network creates/destroys minimizers neural network i.e. performance high capacity invariably suer high quality minimizers dicult obtain non-convexity loss function. clear using context vectors instead improves decoder’s accuracy approximating algorithm. proposed paper akin feature stacking wherein dierent sets features stacked increase classication accuracy. experiments control embedding dimension latent context vector show increased number successful routes produced neural network encoding dynamics encoding dimension. indeed homotopy continuation induced diusion increases accuracy still falls short improving temporal memory encoder. kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabska-barwi´nska sergio g´omez colmenarejo edward grefenstee tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature oriol vinyals meire fortunato navdeep jaitly. pointer networks. advances neural information processing systems cortes lawrence sugiyama garne curran associates inc.", "year": 2017}