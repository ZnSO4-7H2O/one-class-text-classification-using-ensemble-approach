{"title": "A review of mean-shift algorithms for clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "A natural way to characterize the cluster structure of a dataset is by finding regions containing a high density of data. This can be done in a nonparametric way with a kernel density estimate, whose modes and hence clusters can be found using mean-shift algorithms. We describe the theory and practice behind clustering based on kernel density estimates and mean-shift algorithms. We discuss the blurring and non-blurring versions of mean-shift; theoretical results about mean-shift algorithms and Gaussian mixtures; relations with scale-space theory, spectral clustering and other algorithms; extensions to tracking, to manifold and graph data, and to manifold denoising; K-modes and Laplacian K-modes algorithms; acceleration strategies for large datasets; and applications to image segmentation, manifold denoising and multivalued regression.", "text": "natural characterize cluster structure dataset ﬁnding regions containing high density data. done nonparametric kernel density estimate whose modes hence clusters found using mean-shift algorithms. describe theory practice behind clustering based kernel density estimates mean-shift algorithms. discuss blurring nonblurring versions mean-shift; theoretical results mean-shift algorithms gaussian mixtures; relations scale-space theory spectral clustering algorithms; extensions tracking manifold graph data manifold denoising; k-modes laplacian k-modes algorithms; acceleration strategies large datasets; applications image segmentation manifold denoising multivalued regression. intuitive deﬁning clusters assume data points sample probability density function deﬁne clusters density. example shows dataset density estimate whose contours clearly suggest clusters complex shape. ﬁrst step then learn estimate density data points. done parametric model gaussian mixture typically trained algorithm maximize likelihood approach often computationally eﬃcient give good results clusters elliptical shape several disadvantages. likelihood function typically local optima ﬁnding global optimum general diﬃcult; thus result dependent initialization practice user diﬀerent initializations selection model left user well number clusters ﬁnd. clusters complex shapes example image segmentation many components required approximate well increasing training time number local optima. focus nonparametric kernel density estimates generalization histograms deﬁne density estimates dimension smooth. simplify mathematical computational treatment densities crucially enable continuous optimization maxima density. kernel gaussian kernel requires single user parameter bandwidth given bandwidth uniquely determined seen below clusters take complex nonconvex shapes. hence user need select number clusters random restarts. focus clusters deﬁned modes mode local maximum density. natural algorithm modes mean-shift iteration essentially local average described section basic idea mean-shift clustering mean-shift iteration initialized every data point mode deﬁne cluster points converged mode belonging cluster. section reviews theoretical results regarding number location modes figure illustration complex-shaped clusters kernel density estimate meanshift results. dataset. circle radius equal bandwidth used kde. contour plot gaussian bandwidth modes located center blue ellipses. ellipse indicates eigenvectors jacobian mode dotted-line polygon convex hull data points. paths followed gaussian various starting points shown. resulting clustering using mean-shift point color-coded according cluster belongs resulting clustering contours using smaller bandwidth deﬁnes three clusters. convergence mean-shift algorithms character cluster domains. section discusses relations mean-shift algorithms spectral clustering algorithms. sections describe extensions mean-shift clustering manifold denoising respectively. disadvantage mean-shift algorithms computational cost section describes several accelerations. section describes another family kde-based clustering algorithms hybrid k-means mean-shift k-modes laplacian k-modes algorithms exactly clusters mode each work better high-dimensional data. section shows applications image segmentation inverse problems denoising areas. assume multivariate continuous data throughout. weight bandwidth isotropic diagonal full covariance matrix. simplify presentation focus case points same scalar bandwidth weight unless otherwise noted. case found commonly practice. also mostly focus gaussian kernels easier analyze give rise simpler formulas. dk/dt vector mean shift since averages individual shifts weights above. gaussian kernel simpliﬁes following elegant form pp/p posterior probability component centered given point discussed below mild conditions scheme converges modes nearly initial intuitively step moves iterate local average data data points closer larger weight increases density. called mean-shift iteration used distinct ways modes ﬁlter dataset. gives rise diﬀerent clustering algorithms follows. refer mean shift blurring mean shift clustering mean-shift modes here declare mode representative cluster assign data point mode converges mean-shift iteration algorithm given gaussian kernel. also estimate error bars mode local hessian given related local covariance. practical problems need solved. firstly points converge modes. unlikely happen ﬁnite sample points detected examining hessian postprocessing step checks small clusters. second mean-shift iteration stopped ﬁnite number steps example relative change value smaller tolerance means data points theory would converge mode actually stop numerically slightly diﬀerent points. postprocessing step necessary merge unique mode. done ﬁnding connected components graph vertices every convergence point edge pair vertices lying within small distance graph need explicitly constructed. user small enough converge modes good accuracy limiting computational cost incurred; quite larger smaller distance diﬀerent true modes. clustering blurring mean-shift smooth data here point dataset actually moves point given given dataset obtain point applying step mean-shift algorithm thus iteration blurring mean-shift results dataset blurred version iterating process obtain sequence datasets original dataset obtained blurring mean-shift step shown below gaussian seen iterated ﬁltering eventually leads dataset points coincident starting dataset bandwidth. however happens dataset quickly collapses meaningful tight clusters depend point-like clusters continue move towards relatively slowly. stopping criterion detects situation quite reliably based whether entropy dataset changes clustering connected-components postprocessing step merges points actual clusters. algorithm given similarities diﬀerences although based mean-shift iteration diﬀerent algorithms produce diﬀerent clustering results. speciﬁcally given value bandwidth number clusters resulting usually diﬀerent. however collection clusterings produced range bandwidths quite similar. quite faster number iterations runtime particularly using accelerated algorithm introduces essentially approximation error. however considerably accelerated small clustering error tolerated number iterations required varies considerably among points synchronous scheme would largest number iterations. conversely possible asynchronous iterations example moving points soon update computed. however makes result dependent order points picked unlikely faster accelerated algorithm described below. choice bandwidth fundamental parameter mean-shift algorithms bandwidth determines number clusters. statistics literature developed various ways estimate bandwidth mostly setting example based minimizing suitable loss function heuristic rules bandwidth estimators useful give reasonable results used caution bandwidth gives best density estimate need give best clustering—clustering density estimation diﬀerent problems. besides clustering nature exploratory best explore range bandwidths. computationally particularly easy also possible diﬀerent bandwidth point help areas points sparse example. good entropic aﬃnities user sets global number neighbors then data point bandwidth computed point distribution neighbors perplexity i.e. point sets bandwidth eﬀective neighbors. could vary achieve diﬀerent clusterings. ways construct adaptive kdes proposed mean-shift update adaptive bandwidths form mean-shift also used track moving objects sequence images since blobs change size using ﬁxed bandwidth becomes problematic. collins used ideas scale-space theory select scale adapts blob size. image feature deﬁned point scale-space certain diﬀerential operator achieves local maximum respect space scale. mode tracked two-step mean-shift procedure convolves image space ﬁlterbank spatial diﬀerence-of-gaussian ﬁlters convolves scale space epanechnikov kernel eﬃcient way. choice kernel diﬀerent kernels give rise diﬀerent versions mean-shift blurring meanshift algorithms. much previous work uses epanechnikov kernel computational eﬃciency since kernel evaluations involve pairs neighboring points rather pairs points convergence occurs ﬁnite number iterations. however practice gaussian kernel produces better results epanechnikov kernel generates kdes piecewise diﬀerentiable contain spurious modes hierarchical mean-shift scale-space clustering behavior modes critical points function bandwidth basis scale-space theory computer vision here studies evolution image gaussian convolution represent image delta functions centred pixel locations value equal grayscale convolving isotropic gaussian kernel scale gives scale increases image blurs structures image edges objects lose detail large scales image tends uniformly gray. important structures objects associated modes lifetime mode—deﬁned scale interval ideal convolution kernel blur structure image never create structure result would tree modes modes modes merge increases single mode. unfortunately true gaussian kernel dimension dimension i.e. images modes created scale decreases. however practice creation events seem rare short-lived mode created usually merges another mode critical point slightly larger scale. thus generally ignored. computationally starts mode pixel tracks location modes numerical continuation method running mean-shift larger scale using initial points modes previous scale. construction results tree—although unlike agglomerative clustering resulting clusterings scale need nested. another notion lifetime topological persistence explored computational geometry used deﬁne hierarchical mean-shift image segmentation tree modes also proposed statistical literature tool visualization kdes practical small datasets mode tree sensitive small changes data gives diﬀerentiate important modes caused example outliers help combine several trees constructed jittering resampling original dataset write gaussian iteration matrix form terms random-walk matrix stochastic matrix elements here k/σknm gaussian matrix data points exp− aﬃnity matrix deﬁnes weighted graph vertex diag data inhomogeneous ﬁlter ﬁlter depends data updated iteration well hence resulting nonlinear ﬁltering. turn suggests could ﬁlters constructed function scalar function modiﬁes spectrum carreira-perpi˜n´an studied several ﬁlters including explicit implicit power exponential ones depending step size parameter resulting generalized gaussian blurring meanshift algorithms. gave convergence conditions found diﬀerent ﬁlters tend similar clusters i.e. range bandwidths obtain clustering however runtime varies widely. implicit ﬁlters power ﬁlters strong clustering eﬀect iteration iterations costly. considers number iterations cost iteration method found fastest slightly overrelaxed explicit function form mapping points clusters dataset deal data points original dataset? purist option clustering algorithm scratch entire dataset computationally costly. faster option original simply points assigning mode converge reﬂects fact clusters points dataset whole space. however point view implies clusters created points arrive. advantages disadvantages mean-shift algorithms advantages mean-shift algorithms stem nonparametric nature makes model assumptions unlike gaussian mixture models k-means example. able model complex clusters nonconvex shape unlike k-means. user need parameter bandwidth intuitive physical meaning local scale determines automatically number clusters. often convenient select number clusters explicitly. local minima thus clustering deﬁnes uniquely determined bandwidth without need algorithm diﬀerent initializations. outliers problematic gaussian mixtures k-means overly aﬀect using kdes equating modes clusters mean-shift also disadvantages. important kdes break high dimensions number clusters changes abruptly large many minute decrease indeed successful applications mean-shift low-dimensional problems particular image segmentation using modes high-dimensional spaces k-modes algorithm described section applications user seek speciﬁc number clusters. however mean-shift clustering direct control number clusters obtain clusters search computationally costly diﬀerentiate meaningful non-meaningful modes. example outliers typically create mode; density cluster genuinely contain multiple modes problems partially corrected postprocessing results mean-shift k-modes algorithm. finally mean-shift slow computationally addressed section mean-shift algorithm simple probably discovered many times. fukunaga hostetler perhaps ﬁrst propose idea also introduced term mean shift. derived blurring version algorithm epanechnikov kernel gradient ascent variable step size without proving convergence giving stopping criterion. observed could used clustering dimensionality reduction since points converge locally cluster centroids medial lines appropriate values bandwidth. since algorithm also independently known statistics literature mean update algorithm references therein). term blurring process cheng discussed convergence blurring non-blurring versions mean-shift. carreira-perpi˜n´an motivated problem ﬁnding modes gaussian mixture multivalued regression independently rediscovered algorithm gaussian kernel proved convergence arbitrary covariance matrices since early non-blurring mean-shift received much attention thanks work comaniciu meer demonstrated success image ﬁltering image segmentation later tracking followed work many researchers theoretical computational application issues. algorithms similar mean-shift appeared scale-space clustering clustering deterministic annealing pre-image ﬁnding kernel-based methods although deﬁned simple iterative schemes exhibit remarkable somewhat counterintuitive properties regarding whether converge fast converge character convergence domains. geometry modes also surprising. relevant literature scattered diﬀerent areas including computer vision statistics machine learning. give summary results particular attention gaussian kernels without proof. details found references cited. geometry modes gaussian mixture intuitively might expect unimodal kernels bandwidth would modes number modes decrease monotonically increases zero diﬀerent components coalesce. general true gaussian kernel dimension one. motivated scale-space theory several papers showed that dimension gaussian kernel kernel create modes scale increases. easy that kdes non-gaussian kernels modes appear increases creation modes need occur often though kernels likely others create modes. less easy nonetheless true modes also appear gaussian kernel dimension i.e. images thus larger dimension shown example scale-space theory results restricted single bandwidth also creation modes necessarily imply mixture components modes. results general case gaussian mixtures follows. again qualitative diﬀerence more. modes modes above even components isotropic components diagonal full covariance matrices easy construct examples modes components harder achieve components isotropic equal bandwidth still possible. ﬁrst shown construction suggested duistermaat studied carreira-perpi˜n´an williams consisting gaussians vertices equilateral triangle. narrow interval scales extremely shallow fourth mode appears triangle barycenter. generalized construction dimension regular simplex gaussian vertex i.e. components vertex simplex showed number modes either interestingly lifetime mode created barycenter peaks slowly decreases towards zero increases. small perturbations construction prevent creation extra mode suggests unlikely isotropic modes components. however apart isolated studies geometry modes high dimensions poorly understood. components diagonal full covariance matrices modes outside convex hull work mean shift scale space theory tacitly assumes modes ﬁnite number least isolated. although sometimes claimed consequence morse theory proof seems exist arbitrary dimension. indeed kernels uniform triangular kernel create continuous ridges modes even convergence attractive feature deﬁned without step sizes makes deterministic given dataset bandwidth. however useful converges whether case depends kernel used. kernels give rise convergent updates even valid kernels example diverges appears kernels diverges course possible devise optimization algorithms converge modes e.g. introducing line search lose simplicity iteration. kernels converges convergence mode initial points general stationary point here review convergence results focus common kernels dimension simple prove assuming inﬁnite diﬀerentiabilty setting gradient mixture following result. inﬁnitely diﬀerentiable real function either identically zero zero crossings isolated. indeed zero nonempty interval limh→ repeating argument etc. obtain derivatives zero hence taylor’s theorem general mixtures gaussians kernels gaussian expectation-maximization algorithm non-gaussian generalized algorithm seen deﬁning certain dataset certain probabilistic model hidden variables deriving corresponding algorithm maximize loglikelihood verifying coincides algorithm. case model consists constrained mixture gaussians component given weight mean covariance matrix whole mixture freely translated. dataset consists solely point located origin. thus maxima likelihood occur whenever translation vector mode mixture coincides origin. missing data index mixture component generated origin. resulting log-likelihood closed form giving update. non-gaussian kernels step cannot solved closed form update corresponds single iteration solve step. viewing algorithm several consequences. convergence gaussians assured convergence theorems iterate increases leaves unchanged density. general results algorithms also indicate convergence order typically linear finally seen bound optimization local covariance matrix since deﬁnition local mean itself. eigenvalues convergence rate given largest iterates approaching along eigenvector associated largest eigenvalue shows convergence order depends bandwidth nearly always merges. practically useful cases intermediate value rate linear convergence close thus convergence slow. iterates smoothly approach mode along principal component local covariance matrix data points within convex hull data points properties focus gaussian i.e. isotropic covariances bandwidth seen initialized points mode ﬁrst steps often large make considerable progress towards mode. advatageous property generally observed alternating optimization algorithms that steps become small agreement linear convergence rate. path followed iterates following properties near convergence path follows direction principal eigenvector local covariance mode iterate convex linear combination data points path lies interior convex hull data points also true non-gaussian kernels satisfy path smooth sense consecutive steps always make angle mean shift vector proportional gradient k-means centroids partition space voronoi cells convex polytopes. harder characterize regions modes partition space into i.e. domains convergence fact surprising properties general curved nonconvex disconnected allows represent complexshaped clusters advantage k-means. less desirable aspect domain boundaries show fractal behavior although seems conﬁned cluster boundaries could removed necessary postprocessing clusters. fractal behavior iterated mapping would occur deﬁned clusters purely based lines gradient. convergence cheng proved convergence blurring mean-shift follows. kernels broad enough cover dataset convergence dataset points coincident regardless value seen noting diameter data decreases least geometrically. ﬁnite-support kernels small enough convergence several clusters points coincident them; clusters depend value another proof obtained matrix formulation since iteration dataset multiplied times stochastic matrix perron-frobenius theorem broad kernels single eigenvalue equal eigenvalues magnitude less extremely fast convergence that since kept constant dataset shrinks eﬀectively decrease. note smaller initial increases. thus iteration faster convergence direction largest variance collapses much slowly directions. explains practical behavior shown gaussian clusters collapse extremely fast iterations local principal component survives resulting temporary linearly-shaped clusters behaviors make useful clustering denoising respectively. form gaussian dataset iteration produces gaussian standard deviation |φ)| allows complete characterization conditions order convergence terms real function instead matrix function. convergence occurs depending convergence spectral clustering noted earlier putting gaussian matrix form terms gaussian aﬃnity matrix uncovers intimate relation spectral clustering. iteration product data times stochastic matrix random walk graph represents posterior probabilities point kernel density estimate closely related matrix commonly used spectral clustering e.g. normalized eigenvalue/eigenvector pairs satisfy spectral clustering given computes eigenvectors associated eigenvalues spectral space clustering structure data considerably enhanced simple algorithm k-means often clusters. iterate product kept constant would power method column would converge leading left eigenvector rate convergence given second eigenvalue however dynamics complex also changes iteration. practice quickly reach quasistable state points collapsed clusters slowly approach remains almost constant thus seen reﬁning original aﬃnities matrix consisting blocks constant value extracting piecewise-constant eigenvectors cluster power method. generalized algorithm uses instead matrix eigenvectors eigenvalues however manipulation spectrum performed implicitly without actually compute eigenvectors spectral clustering. spectral clustering rely random-walk matrix diﬀer several respects. give clustering results. spectral clustering user sets desired number clusters bandwidth user sets determined this. computationally spectral clustering solves eigenproblem performs small number matrix products thus considerably faster. bilateral ﬁltering nonlinear diﬀusion many image processing algorithms operate range variables deﬁned space variables includes bilateral ﬁltering nonlinear diﬀusion others. mean-shift basically iterated local averaging operates jointly range space bears similarities diﬀerences algorithms described example bilateral ﬁltering spatial component ﬁxed iterations range variables updated stopping criterion necessary prevent excessive smoothing. mean-shift-like algorithms mean-shift algorithms appear whenever expressions form data points function squared distances parameter equating gradient expression zero solve obtain ﬁxed-point iteration form weighted average data points weights depend example mentioned riemannian centers mass. another example coordinates data points low-dimensional space aﬃnity matrix graph laplacian. alternating optimization done mean-shift algorithm. laplacian objectives appear algorithms dimensionality reduction clustering laplacian eigenmaps elastic embedding spectral clustering however noted section resulting mean-shift iteration need converge general. case example weights kernels take negative values elastic embedding. case line search diﬀerent optimization algorithm altogether. tracking mode ﬁnding time applications distribution data changes time want track clusters location order predict location future time example tracking nonrigid object sequence images video moving car. robust represent object color histogram pixels within object. simplest form region ﬁxed shape size variable location initialized user ﬁrst image. then likely location object next image deﬁned region histogram closest current histogram. comaniciu noted diﬀerentiable instead histogram diﬀerentiable similarity function kdes also form weighted ﬁrst order hence mean-shift iterations maximize similarity location thus ﬁnding location pixels look like previous image’s region. resulting algorithm fast enough track several objects real-time video robust partial occlusion clutter distractors camera motion. kernel spatial variables typically gaussian kernel temporal variable considers past observations gives importance recent ones. value ﬁnds modes starting modes previous time value manifold data original mean shift algorithm deﬁned euclidean space sometimes data clustered lies low-dimensional manifold mean-shift iterates modes also manifold. applications known riemannian manifold rotation matrices grassmann manifolds symmetric positive deﬁnite matrices subbarao meer extended meanshift riemannian manifolds deﬁning squared distances appropriately respect manifold however times manifold known priori. shamir extended mean-shift point clouds computer graphics applications constraining mean shift steps surfaces triangulated mesh data. laplacian k-modes described later uses diﬀerent approach modes valid patterns data lying nonconvex manifolds. graph data mean-shift algorithm operates data data point deﬁned feature vector applications data best represented weighted graph vertex data point edge represents neighborhood relation pair data points real-valued weight representing distance. allows work data need live euclidean space used represent distances along manifold algorithms proposed graph data based kde. work assigning data point parent graph another data point neighborhood higher density results forest directed trees spans graph whose roots modes sense neighboring point higher density. tree corresponds cluster. parent data point deﬁned data point optimizes criterion based distances data points. crucial aspect optimization deﬁned data points rather space alternates computing riemannian center mass updating weights. recover mean-shift using squared euclidean distance optimizing unlike mean-shift seem maximize global objective function rather maximizes local objective iteration data point. algorithm gives better clusterings faster blurring version every iteration entire dataset updated. matrix form pairwise distances updating points synchronously) takes form minx...xn minimization columnwise. hence medoidshift seen discrete ﬁlter iteration continuous ﬁlter. accelerated iteration contracts graph reducing number points. possible deﬁne variations using diﬀerent types distances local tukey median medianshift algorithm extensions mean-shift update originally proposed results estimating gradient density steps ﬁrst estimates density diﬀerentiates this. sasaki directly estimate gradient log-density using score matching derive mean-shift algorithm based score matching least-squares true log-density gradient using model. model linear combination basis functions obtains update identical mean-shift update weighted kernels faster parametric method obtained using fewer basis functions data points. observe better results original mean-shift highdimensional data. note that seen using adaptive also gives weighted mean-shift update weights obey diﬀerent criterion. ranking data consists permutations given items discrete. meil˘a extended blurring mean-shift ranking data using kendall distance rather euclidean distance rounding continuous average nearest permutation iteration functional data collection curves surfaces live inﬁnite-dimensional space. meanshift clustering extended functional data corresponds form adaptive gradient ascent estimated surrogate density mean-shift iteration essentially local smoothing provides smooth denoised version data point weighted average nearby data points. clustering speciﬁc problem make smoothing. describe work based mean-shift manifold denoising. already pointed consider data lying low-dimensional manifold noise added mean-shift iteration data point parallel replace point denoised version obtain denoised dataset points moved towards manifold. repeating eventually compresses dataset clusters basis algorithm although course destroys manifold structure. however stop early usually iterations obtain algorithm remove noise dataset manifold structure. denoising ability local smoothing noted independently computer graphics literature problem surface fairing smoothing. earlier laplacian also used smooth ﬁnite element meshes surface smoothing point clouds representing surface object recorded using lidar usually contain noise. noise eliminated laplacian smoothing replaces location point average neighbors neighbors point obtained triangulated graph cloud usually available scanning pattern lidar. typically laplacian constructed without notion random-walk matrix values entries connectivity pattern kept constant iterations. important problem points lying near boundary manifold surface move orthogonally tangentially along manifold away boundary thus shrinking manifold. good clustering surface smoothing mbms bandwidth gaussian kernel. mbms denoises preserving spiral structure ignoring outliers. locally collapses points onto clusters destroying spiral. distorts object shape manifold learning distorts manifold structure. example mnist dataset handwritten digit images motion along manifold changes style digit various approaches rescaling preserve object volume proposed computer graphics machine learning manifold blurring mean-shift algorithm extension preserve local manifold structure. rather applying point mean-shift vector directly vector corrected eliminate tangential motion. mean-shift vector ﬁrst projected onto local tangent space manifold point projection removed motion. hence motion constrained locally orthogonal manifold. comparing local eigenvalues orthogonal tangent spaces gives criterion stop iterating. anti-aliased reduce pixelation) easier read indeed classiﬁer trained denoised data performs better smoothing homogenizes digits somewhat preserves distinctive style aspects digit. mbms performs sophisticated denoising intelligently closing loops removing shortening spurious strokes enlarging holes removing speckle noise general subtly reshaping digits respecting orientation slant thickness. mbms also applied identiﬁcation structure tectonic faults seismic data denoising used preprocessing stage several tasks leading robust models dimensionality reduction classiﬁcation density estimation matrix completion algorithms establish continuum clustering dimensionality reduction clustering extreme case dimensionality reduction manifold dimensionality zero. case mbms tangent space dimension zero recovers bms. another case spectral clustering spectral dimensionality reduction e.g. normalized laplacian eigenmaps operate extracting eigenvectors graph laplacian spectral clustering number eigenvectors number clusters spectral dimensionality reduction cluster assumed eigenvectors correspond degrees freedom along manifold deﬁned cluster. computationally mean-shift algorithms slow complexity quadratic number points many iterations required converge signiﬁcant work focused faster approximate mean-shift algorithms using variety techniques subsampling discretization search data structures numerical optimization others review below. specialized image segmentation others apply data. practical implementation fast mean-shift algorithm could combine several techniques. addition although seem studied detail mean-shift algorithms beneﬁt embarrasing parallelism since iterations point proceed independently rest points. accelerating mean-shift epanechnikov kernel mean-shift converges ﬁnite number iterations. neighborhood structure data known priori case image segmentation eﬃcient without approximations since iteration point involves neighbors. gaussian kernel arbitrary data neighborhood structure known varies iterations runtime large particularly large datasets much work tried accelerate gaussian computationally bottlenecks accurately converging mode require many iterations since convergence typically linear approach sublinearity regions modes close. iteration linear number points thousands millions acceleration algorithms attack either bottlenecks keeping approximation error i.e. producing clustering close naive ms—otherwise would really running diﬀerent clustering algorithm. accelerating convergence obvious answer using newton’s method quadratic convergence computing hessian gaussian mixture simple newton iterations typically much costlier reason that low-dimensional problems best suited computing newton direction themselves. however newton’s method problems introduces user parameters step sizes damping coeﬃcients; need eﬀective mode hessian undeﬁned; since newton’s method diﬀerent make point converge diﬀerent mode solution ms-newton method starts iterations switches newton’s method. average iteration small subset data namely points closest current iterate. done simply ignoring faraway points average introduces approximation error modes updating faraway points infrequently sparse algorithm guarantees convergence true mode. unfortunately either requires ﬁnding nearest neighbors ﬁnal approach consists eliminating many iterations many points altogether eﬀect predicting mode given point converge cannot simply done running small subset points assigning remaining points mode closest point subset mean-shift clusters complex shapes signiﬁcantly diﬀer nearest-neighbor clustering clustering error would large. eﬀective approach image segmentation spatial discretization approximation based fact trajectories followed diﬀerent points converge towards given mode collect close together thus naive subset points keeping track trajectories followed. point soon detect iterate close existing trajectory stop assign trajectory’s mode. image segmentation trajectories coded discretizing image plane subpixel cells marking cell ﬁrst time iterate visits mode converges since nearly cells empty memory required small. reduces average number iterations point nearly small clustering error error controlled subpixel size. actually converge mode however many fast mean-shift algorithm results running mean-shift iterations points assigning mode hence discretization approach above data points actually mean-shift iterations. approximation paris durand combine several techniques accelerate mean-shift image segmentation including spatial discretization convolutions construct hierarchy clusterings based notion topological persistence computational geometry algorithm fast enough segment large images video although clear good approximation true mean-shift. dimension data points increases searching nearest neighbors truncated kernel) becomes bottleneck. approximate nearest-neighbor algorithms. example locality-sensitive hashing used accelerating blurring mean-shift simple eﬀective acceleration essentially zero approximation error given essentially consists interleaving connected-components blurring steps. fact collapses clusters single point suggests soon cluster collapses could replace single point weight porportional cluster’s number points. particularly eﬀective clusters collapse diﬀerent speeds happens diﬀerent sizes predicted section e.g. total number iterations remains original iteration uses dataset fewer points thus faster. speciﬁcally reduction step coincident points replaced single point approximated connected-components step points closer considered coincident takes value ﬁnal connected-components step thus resolution method applies stopped iterating accelerated version applies iteration. hence accelerated algorithm alternates connected-components graph contraction step. experimentally image segmentation remarkable result arises runtime accelerated almost constant bandwidth values unlike corresponding iterations dominated ﬁrst iterations almost merging occurs. means speedup factor discussed earlier equating modes clusters mean-shift implies lose direct control number clusters diﬀerentiate meaningful non-meaningful modes. recently kmodes algorithm proposed addresses problems. k-modes algorithm takes user parameters maximizes objective function gaussian kernel. given assignment seen separately cluster. thus good clustering must move centroids local modes also deﬁne separate kdes. naturally combines idea clustering binary assignment variables idea that suitable bandwidth values high-density points representative cluster function bandwidth k-modes objective function becomes k-means version k-medoids training algorithm alternates assignment step k-means data point assigned closest centroid mode-ﬁnding step mean-shift centroid deﬁned current cluster robust homotopy-based algorithm results starting algorithm k-means iteration) although requires iterations thus much faster mean-shift iteration). k-modes obtains exactly modes even data fewer modes splits data kdes. using homotopy-based algorithm tends track major mode cluster avoid outliers. thus k-modes work well even high dimensions unlike mean-shift. fundamental disadvantage k-modes clusters deﬁnes convex k-means since voronoi cells deﬁned modes. solved laplacian k-modes algorithm minimizes objective function clusters diﬀerent proportions) adds term objective weight encourages neighboring points similar soft assignments. term equivalently written trzt terms graph laplacian constructed aﬃnity matrix diag degree matrix. training algorithm k-modes assignment step becomes convex quadratic program solved eﬃciently variety solvers. laplacian-based algorithms spectral clustering. however solve quadratic program rather spectral problem obtain centroids soft assignments points clusters out-of-sample mapping predict soft assignments test point original training obtained augmenting solving keeping centroids soft assignments training points ﬁxed. result equals projection simplex average terms average assignments neighbors term dependent distances centroids cluster hence laplacian k-modes seen incorporating nonparametric posterior probabilities mean-shift usual obtaining posterior probabilities clustering using mixture model algorithm maximizes likelihood derived. contrast parametric approach laplacian k-modes assignments optimize objective function designed speciﬁcally clustering unlike likelihood; clusters obliged follow model consequently optimization algorithm depend particular type clusters soft assignments posterior probabilities helpful estimate uncertainty clustering uses. example image segmentation used smooth pixel mask image matting conditional random ﬁelds saliency maps. cluster representatives means modes medoids centroid-based clustering algorithms cluster associated centroid cluster mean k-means exemplar k-medoids mode mean-shift k-modes laplacian k-modes. desirable property centroids makes interpretable valid patterns representative cluster well-known disadvantage k-means mean nonconvex cluster need valid pattern since low-density area. often case clusters manifold structure. example given sequence rotated digit- images k-medoids exemplars valid patterns deﬁnition although data noisy individual exemplars look somewhat atypical modes strike optimal tradeoﬀ typical valid denoised representatives follows. first modes deﬁnition high-density areas sense representative cluster. second mode characterized indeed mode maximum density scale-dependent local average data. gradient zero implies equals weighted average data sense seen section function scale mode spans continuum equaling regular mean whole data equaling individual data point intermediate bandwidth local averages data remove noise aﬀects individual exemplar thus even prototypical actual data points. sense k-modes achieves form intelligent denoising similar mbms table compares several clustering algorithms terms whether centroids valid patterns whether model nonconvex clusters whether estimate density whether cluster assignments hard soft. illustrate clustering application eﬀective point clustering applications eﬀective well applications beyond clustering used. matlab code algorithms described obtained author. image segmentation eﬀective low-dimensional data successful application image segmentation applied follows. given image consider pixel data point space consisting types features spatial features range features obtained gaussian cameraman image bandwidth resulting clusters. show projection spatial domain every data point colored cluster note points quickly move towards centroid collapse cluster-to-be local direction maximum variance collapses much slowly lower-variance directions producing linearly shaped clusters straighten shorten. stopping criterion stopped iteration dataset consists clusters coincident points. stopping clusters would keep moving eventually merge single cluster. pixel). range features scaled span approximately range spatial features. features bandwidth pixel units. example image rescale original intensity values range feature vector would correspond pixel located coordinates intensity equal maximum intensity precise scaling aﬀect clustering done carefully. using spatial features beneﬁcial introduce spatial coherence although sometimes range features used. perceptually uniform color space rather space euclidean distances approximately match perceptual diﬀerences color figure left clustering spirals using laplacian k-modes modes denoted contours cluster. right subset centroids obtained k-means laplacian k-modes mean shift mnist data. image size although explore range bandwidths. computer vision applications sometimes used small bandwidth order oversegment image uniform patches processed. cases mean-shift clustering work well fig. illustrates situations produce good clusterings. ﬁrst data manifold structure spirals space shown left panel. either many modes spread data manifolds modes manifolds bandwidth value cluster spirals correctly. laplacian k-modes algorithm able cluster spirals correctly estimating reasonable mode spiral tends produce either single mode uninformative many modes often small groups points outliers. results bandwidth tuned produce clusters k-means laplacian k-modes since data forms nonconvex clusters k-means produces centroids average distant data points possibly diﬀerent digit classes thus look like digits. again laplacian k-modes partition data exactly clusters ﬁnding mode looks like valid digit representative class assigns unique vector possible input estimate given training pairs usual regression setting. multivalued regression mapping assigns possibly multiple vectors input classical example learning inverse mapping example inverse outputs depending value hence case apply mean-shift conditional distribution advantage approach number inverses modes found also estimate error bars local hessian figure left panels illustration robot using conditional modes represent multiple inverses. plots correspond joints’ angle space end-eﬀector workspace want reach point near point reached joint angle conﬁgurations elbow up/down mean θ-space correspond valid inverse. right panel proﬁle view palate several tongue shapes corresponding modes mean conditional distribution speech sound utterance rag. fig. illustrates examples. left panels learn inverse kinematics mapping robot forward kinematics mapping univalued gives position workspace arm’s end-eﬀector given angles joints. inverse mapping multivalued learned using particle ﬁlter. right panel learn speech sounds tongue shapes american english sound produced tongue shapes bunched retroﬂex. ﬁgure shows speciﬁc sound modes tongue shape space inverse kinematics articulatory inversion using conditional mean equivalently univalued least-squares regression leads invalid inverses. applications algorithms based mean-shift also applied video segmentation image denoising discontinuity preserving smoothing object tracking among problems well manifold surface denoising mean-shift algorithms based general idea locally averaging data results moving higherdensity therefore typical regions. iterating done distinct ways depending whether dataset updated mode ﬁnding smoothing used clustering also manifold denoising multivalued regression tasks. practice mean-shift algorithms attractive because—being based nonparametric kernel density estimates—they make assumptions data model nonconvex clusters. user need specify desired scale clustering number clusters itself. although computationally slow mean-shift algorithms accelerated scale large datasets. popular low-dimensional data nonconvex clusters image segmentation. k-modes algorithms remain nonparametric still perform well high-dimensional data outliers. mean-shift intimately related kernel density estimates gaussian mixtures neighborhood graphs. graph laplacian random-walk matrix arises fundamental object clustering dimensionality reduction encapsulates geometric structure dataset. although usually seen directions future research follows. understanding geometry gaussian mixtures modes high dimensions. finding meaningful extension mean-shift directed graphs. designing learning random-walk laplacian matrices optimal clustering sense. finding meaningful deﬁnition clusters based bumps kernel density estimate i.e. distinct regions high probability rather modes. consider undirected graph vertex edge connected component maximal subset every pair vertices connected path using edges hence vertex partitioned connected components. connected component connected-components clustering algorithm right. provides matrix distances every pair vertices threshold deﬁnes graph vertices data points edges connected components graph give clusters. connected-components gives poor clustering results unless clusters tight clearly separated. however reliably used postprocessing step mean-shift algorithms described main text merge points ideally would identical. example ﬁnal iterates points limit would converge mode numerically diﬀerent hence iterates form tight cluster around mode widely separated tight clusters corresponding modes. naively implemented connected-components would time construct graph threshold pairwise distances costs however case tight clearly separated clusters need construct graph explicitly runs connected components. data satisfy following tight clusters assumption exists larger diameter component smaller distance components connected components found incrementally connecting point representative component representative component point component. done process data point given ﬁnal iterates bms. distance dimension typically exceed image segmentation scan pixels raster order pixels component previous pixel then computing distance always ﬁrst last pixels’s component. average cost entire connected-components runtime value depends problem usually chosen wide interval. example image segmentation features pixel units since need subpixel accuracy locate mode modes must least pixel apart meaningful. safely smaller value since cubic convergence rate produces extremely tight clusters quickly. figure pseudocode connected-components implemented naively tight clusters assumption number components. cases input dataset distance function applicable pair points threshold", "year": 2015}