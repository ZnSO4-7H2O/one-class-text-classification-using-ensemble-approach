{"title": "Accelerating Very Deep Convolutional Networks for Classification and  Detection", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., >=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector.", "text": "abstract—this paper aims accelerate test-time computation convolutional neural networks especially deep cnns substantially impacted computer vision community. unlike previous methods designed approximating linear ﬁlters linear responses method takes nonlinear units account. develop effective solution resulting nonlinear optimization problem without need stochastic gradient descent importantly previous methods mainly focus optimizing layers nonlinear method enables asymmetric reconstruction reduces rapidly accumulated error multiple layers approximated. widely used deep vgg- model method achieves whole-model speedup merely increase top- error imagenet classiﬁcation. accelerated vgg- model also shows graceful accuracy degradation object detection plugged fast r-cnn detector accuracy convolutional neural networks continuously improving computational cost networks also increases signiﬁcantly. example deep models witnessed great success wide range recognition tasks substantially slower earlier models real-world systems suffer speed networks. example cloud service needs process thousands requests seconds; portable devices phones tablets afford slow models; recognition tasks like object detection semantic segmentation need apply models higher-resolution images. thus practical importance accelerate test-time performance cnns. series studies accelerating deep cnns common focus methods decomposition layers. methods shown promising speedup ratios accuracy layers whole models. however results available accelerating deep models experiments complex datasets imagenet also limited e.g. results accelerating single layer shallower alexnet moreover performance correspondence author. zhang xi’an jiaotong university xi’an china. work done zhang intern microsoft research. microsoft research beijing china. e-mail {kahejiansun}microsoft.com nontrivial speed whole deep models complex tasks like imagenet classiﬁcation. acceleration algorithms involve decomposition layers also optimization solutions decomposition. data reconstruction solvers based stochastic gradient descent backpropagation work well simpler tasks character classiﬁcation less effective complex imagenet models sgd-based solvers sensitive initialization learning rates might trapped poorer local optima regressing responses. moreover even solver manages accelerate single layer accumulated error approximating multiple layers grow rapidly especially deep models. besides layers deep model exhibit great diversity ﬁlter numbers feature sizes sparsity redundancy. beneﬁcial uniformly accelerate layers. paper present accelerating method effective deep models. ﬁrst propose response reconstruction method takes account nonlinear neurons low-rank constraint. solution based generalized singular value decomposition developed nonlinear problem without need sgd. explicit treatment nonlinearity better models nonlinear layer importantly enables asymmetric reconstruction accounts error previous approximated layers. method effectively reduces accumulated error multiple layers approximated sequentially. also present rank selection method adaptively determining acceleration layer whole experiments demonstrate effects nonlinear solution asymmetric reconstruction whole-model acceleration controlled experiments -layer model imagenet classiﬁcation furthermore apply method publicly available vgg- model achieve speedup merely increase top- centerview error. impact imagenet dataset merely speciﬁc -class classiﬁcation task; deep models pre-trained imagenet actively used replace hand-engineered features showcased excellent accuracy challenging tasks object detection semantic segmentation exploit method accelerate deep vgg- model fast r-cnn object detection. speedup convolutions method graceful degradation pascal detection benchmark preliminary version manuscript presented conference manuscript extends initial version several aspects strengthen method. demonstrate compelling acceleration results deep models among ﬁrst works accelerating deep models. investigate accelerated models transfer-learning-based object detection important applications imagenet pre-trained networks. provide evidence showing model trained scratch sharing structure accelerated model inferior. discovery suggests deep model accelerated simply decomposed network architecture powerful acceleration optimization algorithm able digest information. related work methods accelerating testtime computation cnns general components layer decomposition design reduces time complexity optimization scheme decomposition design. although former attracts attention because directly addresses time complexity latter also essential decompositions similarly easy good local optima. method denton ﬁrst exploit low-rank decompositions ﬁlters. several decomposition designs along different dimensions investigated. method explicitly minimize error activations nonlinearity inﬂuential accuracy show. method presents experiments accelerating single layer overfeat network whole-model results available. jaderberg present efﬁcient decompositions separating ﬁlters ﬁlters earlier developed accelerating generic image ﬁlters channel-wise dimension reduction also considered. optimization schemes proposed ﬁlter reconstruction minimizes error ﬁlter weights data reconstruction minimizes error responses. conjugate gradient descent used solve ﬁlter reconstruction backpropagation used solve data reconstruction. data reconstruction demonstrates excellent performance character classiﬁcation task using -layer network. imagenet classiﬁcation paper evaluates single layer overfeat network ﬁlter reconstruction. performance whole deep models imagenet remains unclear. concurrent work lebedev adopt cp-decomposition decompose layer layers lower complexity. imagenet classiﬁcation single-layer acceleration alexnet reported moreover lebedev report failed good learning rate ﬁne-tuning suggesting nontrivial optimize factorization even single layer imagenet models. besides research decomposing layers streams improving train/test-time performance cnns. fft-based algorithms applicable training testing particularly effective large spatial kernels. hand also proposed train thin deep networks good trade-off speed accuracy. besides reducing running time related issue involving memory conservation also attracted attention. approaches method exploits low-rank assumption decomposition following stream show decomposition closed-form solution linear neurons slightly complicated solution nonlinear neurons. simplicity solver enables asymmetric reconstruction method reducing accumulated error deep models. layers matrix actually ﬁlters whose sizes ﬁlters produce d-dimensional feature map. feature d-by-d matrix implemented ﬁlters whose sizes corresponds convolutional layer spatial support maps d-dimensional feature ddimensional one. note decomposition arbitrary. impact value computed eqn.. simple decomposition singular value decomposition udsdvd d-by-d column-orthogonal matrices d-by-d diagonal matrix. obtain uds/ response sampled feature maps training set. problem solved actually principal component analysis d-by-n matrix concatenating responses mean subtracted compute eigen-decomposition covariance matrix orthogonal matrix diagonal udud ﬁrst eigenvectors. matrix computed good low-rank assumption? sample responses model trained imagenet. responses layer compute eigenvalues covariance matrix plot largest eigenvalues substantial energy small portion largest eigenvectors. example conv layer ﬁrst eigenvectors contribute energy; conv layer ﬁrst eigenvectors contribute energy. indicates fraction ﬁlters precisely approximate original ﬁlters. low-rank behavior responses low-rank behaviors ﬁlter weights inputs although low-rank assumptions ﬁlter weights adopted recent work adopt low-rank assumptions ﬁlter inputs local volumes correlations. responses lower rank approximation precise. optimization directly address low-rank subspace resulting low-rank decomposition reduces time complexity. approximate low-rank subspace minimize reconstruction error responses. formally consider convolutional layer ﬁlter size k×k×c spatial size ﬁlter number input channels layer. compute response ﬁlter applied volume layer input. rkc+ denote vector reshapes volume append last entry sake bias. response position layer computed d-by- matrix number ﬁlters. denotes reshaped form ﬁlter bias appended. assumption vector lowrank subspace write d-by-d matrix rank mean vector responses. expanding equation compute response figure accumulative energy responses layer presented largest eigenvalues ﬁlter number conv conv conv- ﬁgures obtained randomly sampled training images. frobenius norm. problem form known reduced rank regression problem belongs broader category procrustes problems adopted various data reconstruction problems solution follows zy)−. gsvd applied d-by-d orthogonal matrix satisfying d-by-d identity matrix d-by-d matrix satisfying vyyv solution given udsdvd ﬁrst columns largest singular values. show becomes i.e. eigen) gsvd solution becomes decomposition subproblem {zi}. case ﬁxed. subproblem element vector independent other. solve -dimensional optimization problem follows {zi} auxiliary variables size {yi}. penalty parameter. solution converge solution adopt alternating solver ﬁxing {zi} solving vice versa. subproblem case {zi} ﬁxed. easy show solved mean vector {zi}. substituting objective function obtain problem involving figure accumulative energy accuracy rates accuracy evaluated using linear solution layer evaluated independently layers approximated. accuracy shown difference approximation. empirically observe energy approximations roughly related classiﬁcation accuracy. verify observation fig. show classiﬁcation accuracy energy. point ﬁgure empirically evaluated using reduced rank energy means approximation thus degradation classiﬁcation accuracy. fig. shows classiﬁcation accuracy roughly linear energy. simultaneously determine reduced ranks layers assume whole-model classiﬁcation accuracy roughly related product energy layers. formally consider objective function a-th largest eigenvalue layer energy largest layers approximated. objective assumed related accuracy approximated whole network. optimize problem alternatively solve initialization given solution linear case warm solver setting penalty parameter iterations. increase value theory gradually increased inﬁnity difﬁcult iterative solver make progress large. increase iterations resulting solution. before obtain asymmetric reconstruction multi-layer layer approximated independently error shallower layers rapidly accumulated affect deeper layers. propose asymmetric reconstruction method alleviate problem. apply method sequentially layer shallower layers deeper ones. consider layer whose input feature precise approximation previous layer/layers. denote approximate input current layer training data still compute non-approximate responses optimize asymmetric version ﬁrst term non-approximate output layer. second term approximated input layer approximated output layer. contrast using terms asymmetric formulation faithfully incorporates actual terms before/after approximation layer. optimization problem solved using algorithm rank selection whole-model acceleration above optimization based target layer. parameter determines complexity accelerated layer. given desired speedup ratio whole model need determine proper rank used layer. adopt uniform speedup ratio layer. optimal solution layers equally redundant. table architecture spp- model conv layers layers. layer followed relu. ﬁnal conv layer followed spatial pyramid pooling layer levels resulting layer followed another layer -way softmax layer. column complexity theoretical time complexity shown relative numbers total convolutional complexity. column zeros relative portion zero responses shows sparsity layer. problem combinatorial problem adopt greedy strategy solve consider {σla}. initialize step remove eigenvalue chosen certain layer relative reduction reduction complexity deﬁne measure smallest value measure removed. intuitively measure favors small reduction large reduction complexity step greedily iterated constraint total complexity achieved. higher-dimensional decomposition formulation focus reducing channels algorithmic advantages operating channel dimension. firstly dimension easily controlled rank constraint rank constraint enables closed-form solutions e.g. gsvd. secondly optimized low-rank projection exactly decomposed low-dimensional ﬁlters simple closed-form solutions produce good results using small subset training images hand compared decomposition methods operate multiple dimensions method smaller approach given speedup ratio might limit accuracy method. avoid small propose combine solver jaderberg al.’s spatial decomposition. thanks determined decomposed architecture ﬁrst method decompose conv layers model. involves rank selection layers. apply jaderberg al.’s method decompose resulting layers ﬁlters. ﬁrst layer output channels depending speedup ratio. original layer decomposed three layers speedup ratio method contribute speedup decomposed architecture determined solve weights decomposed layers. given order above ﬁrst optimize layers using ﬁlter reconstruction adopt solution layer optimize layers. asymmetric reconstruction eqn.. term approximated input layer term still true response original layer without decomposition. approximation error spatial decomposition also addressed asymmetric reconstruction important alleviate accumulated error. term asymmetric following. approximated whole model ﬁnetune model end-to-end imagenet training data. process similar training classiﬁcation network approximated model initialization. figure linear nonlinear spp- single-layer performance accelerating conv conv. speedup ratios computed theoretical complexity layer. error rates top- single-view shown increase error rates compared approximation however empirically ﬁne-tuning sensitive initialization learning rate. initialization poor learning rate small ﬁnetuning easily trapped poor local optimum makes little progress. learning rate large ﬁne-tuning process behaves similar training decomposed architecture from scratch large learning rate jump initialized local optimum initialization appears forgotten. fortunately method achieved good accuracy even without ﬁne-tuning show experiments. approximated model initialization ﬁne-tuning sufﬁciently small learning rate able improve results. experiments learning rate mini-batch size ﬁne-tune models epochs imagenet training data. experiments comprehensively evaluate method models. ﬁrst model -layer model sppnet denote spp-. model similar architecture overfeat model deeper. conv layers layers. second model publicly available vgg- model conv layers layers. spp- place vgg- place ilsvrc evaluate top- error using single-view testing. view center region cropped resized image whose shorter side single-view error rate spp- imagenet validation vgg- testing numbers serve references increased error rates approximated models. subsection evaluate single-layer performance. evaluating single approximated layer remaining layers unchanged approximated. speedup ratio shown theoretical ratio computed complexity. fig. compare performance linear solution nonlinear solution performance displayed increase error rates speedup ratio layer. fig. shows nonlinear solution consistently performs better linear solution. table figure symmetric asymmetric spp- cases -layer -layer approximation. speedup computed complexity layers approximated. approximation conv approximation conv approximation conv show sparsity layer. zero activation truncation relu. sparsity conv- indicating relu takes effect substantial portion activations. explains discrepancy linear nonlinear solutions. especially conv layer sparsity advantage nonlinear solution obvious. fig. also shows accelerating single layer increased error rates solutions rather marginal negligible. conv layer error rate increased conv- layers error rate increased also notice conv degradation negligible near speedup explained fig. energy little loss degradation grow quickly larger speedup ratios layer channel number small needs reduced drastically achieve speedup ratio. following whole-model experiments conv. next evaluate performance asymmetric reconstruction problem demonstrate approximating layers layers. case layers show results approximating conv case layers show results approximating conv- conv-. comparisons consistently observed cases multi-layer. sequentially approximate layers involved shallower deeper one. asymmetric version output previous approximated layer output previous non-approximate layer. fig. shows comparisons symmetric asymmetric versions. asymmetric solution signiﬁcant improvement symmetric solution. example layers approximated simultaneously improvement speedup indicates accumulative error rate multi-layer approximation effectively reduced asymmetric version. table show results whole-model acceleration. solver asymmetric version. conv layers rank selection used adopt speedup ratio layer determine desired rank accordingly. rank selection used apply select conv-. table shows rank selection consistently outperforms counterpart without rank selection. advantage rank selection observed linear nonlinear solutions. table notice rank selection often chooses higher rank conv-. example speedup rank selection assigns conv layer requires achieve singlelayer speedup itself. explained fig. energy conv- less concentrated layers require higher ranks achieve good approximations. table whole-model acceleration with/without rank selection spp-. solver asymmetric version. speedup ratios shown involve convolutional layers conv. case rank selection speedup ratio layer same. column conv- shows rank used number ﬁlters approximation. error rates top- single-view shown increase error rates compared approximation. compare jaderberg al.’s method recent state-of-the-art solution efﬁcient evaluation. although decomposition shares high-level motivations point optimization strategy different important accuracy especially deep models previous acceleration methods rarely addressed. jaderberg al.’s method decomposes spatial support cascade spatial supports. channel-dimension reduction figure comparisons jaderberg al.’s spatial decomposition method spp-. speedup ratios theoretical speedups whole model. error rates top- single-view shown increase error rates compared approximation comparisons based implementation scheme decomposition ﬁlter reconstruction version used imagenet reproduction ﬁlter reconstruction gives single-layer speedup conv spp- increase error. reference reports increase error conv single-layer speedup evaluated another overfeat network similar spp-. worth discussing implementation jaderberg al.’s data reconstruction scheme suggested backpropagation optimization. reproduction data reconstruction works well character classiﬁcation task studied however nontrivial make data reconstruction work large models trained imagenet. observe learning rate needs carefully chosen sgd-based data reconstruction converge another decomposition) training starts converge results still sensitive initialization conjecture imagenet dataset models complicated using regress single layer sensitive multiple local optima. fact jaderberg al.’s report ﬁlter reconstruction results single layer imagenet. reasons implementation jaderberg al.’s method imagenet models based ﬁlter reconstruction. believe issues settled table comparisons absolute performance spp-. top- error absolute value. running time single view gpu. accelerated models theoretical speedup brackets actual speedup ratios. fig. compare method jaderberg al.’s whole-model speedup. whole-model speedup implement method sequentially conv- using speedup ratio. speedup ratios theoretical complexity ratios involving convolutional layers. method asymmetric version rank selection. fig. shows speedup ratios large method outperforms jaderberg al.’s method signiﬁcantly. example speedup ratio increased error rate method jaderberg al.’s jaderberg al.’s result degrades quickly speedup ratio getting large degrades slowly. suggests effects method reducing accumulative error. compare asymmetric version using decomposition fig. show results asymmetric fig. shows strategy leads signiﬁcantly smaller increase error. example speedup error increased asymmetric solver effectively controls accumulative error even multiple layers decomposed extensively decomposition easier achieve certain speedup ratio. completeness also evaluate approximation method character classiﬁcation model released asymmetric solution achieves speedup drop classiﬁcation accuracy better drop speedup reported apply jaderberg al.’s method conv layer small number input channels ﬁrst decomposed layer small number ﬁlters approach speedup ratio also note speedup ratio conv layers conv accelerated layers slightly larger speedup. also trained from scratch imagenet dataset. hypothesis underlying architecture sufﬁciently powerful acceleration algorithm might necessary. show hypothesis premature. directly train model architecture decomposed model. decomposed model much deeper original model adopt initialization method otherwise easy converge. train model epochs. follow common practice training imagenet models. comparisons table accuracy model trained scratch worse accelerated model considerable margin results indicate accelerating algorithms effectively digest information trained models. also suggest models trained scratch much redundancy. table shows comparisons absolute performance accelerated models. also evaluate alexnet similarly fast accelerated models. comparison based re-implementation alexnet. alexnet except splitting ignored. re-implementation model top- singleview error rate better reported models accelerated asymmetric version top- error without table architecture vgg- model conv layers layers. column complexity theoretical time complexity shown relative numbers total convolutional complexity. column zeros relative portion zero responses shows sparsity layer. table whole-model acceleration with/without rank selection vgg-. solver asymmetric version. speedup ratios shown involve convolutional layers. accelerate conv. case rank selection speedup ratio layer same. column shows rank used number ﬁlters approximation. error rates top- single-view shown increase error rates compared approximation. table also shows actual running time view implementation intel nvidia gpu. version method actual speedup ratios close theoretical speedup ratios overhead mainly comes layers. version actual speedup ratio accelerated model less easy parallelism actual ratio lower. including object detection semantic segmentation image captioning video/action recognition image question answering texture recognition etc. considering impact slow speed model believe practical signiﬁcance accelerate model. firstly discover whole-model rank selection particularly important accelerating vgg. table show results without/with rank selection. decomposition used comparison. speedup rank selection reduces increased error greater diversity layers vggtable absolute performance accelerating vgg- model top- error absolute value. running time single view gpu. accelerated models theoretical speedup brackets actual speedup ratios. unlike spp- repeatedly applies ﬁlters feature size vgg- model applies evenly feature sizes besides ﬁlter numbers conv- increased time complexity conv- smaller others. selected ranks table show adaptivity e.g. layers conv conv keep ﬁlters small time complexity good tradecompactly reduce them. whole-model rank selection maintain high accuracy accelerating vgg-. table evaluate method vgg- imagenet classiﬁcation. evaluate asymmetric version evaluate challenging speedup ratios ratios theoretical speedups conv layers. somewhat surprisingly method demonstrated compelling results deep model even without ﬁne-tuning. no-ﬁne-tuning model increase -view top- error speedup ratio contrary previous method suffers greatly increased depth rapidly accumulated error multiple approximated layers. ﬁne-tuning model increase -view top- error speedup. degradation even lower shallower model spp-. suggests information deep vgg- model highly redundant method able effectively digest ratios. implementation based standard caffe library exhibits actual theoretical ratios speedup ratios sensitive specialized implementation generic caffe kernels optimized layers believe specially engineered implementation increase actual speedup ratio. figurnov al.’s work existing works present results accelerating whole model vgg-. report increased top- view error rates actual speedups thus method substantially accurate theirs. note results ﬁne-tuning. suggests ﬁne-tuning sufﬁcient wholemodel acceleration; good optimization solver decomposition needed. current state-of-the-art object detection results mostly rely vgg- model. evaluate accelerated vgg- models object detection. method based recent fast rcnn evaluate pascal object detection benchmark dataset contains trainval images test images. follow default setting fast r-cnn using publicly released code. train fast r-cnn trainval evaluate test set. accuracy evaluated mean average precision experiments ﬁrst approximate model imagenet classiﬁcation task. approximated model pre-trained model fast r-cnn. asymmetric version ﬁne-tuning. note unlike image classiﬁcation conv layers dominate running time fast r-cnn detection conv layers consume actual running time reported speedup ratios theoretical speedups conv layers only. table shows results accelerated models pascal detection. method convolution speedup graceful degradation map. believe trade-off accuracy speed practical importance even recent advance fast object detection feature extraction running time still considerable. conclusion presented acceleration method deep networks. method evaluated whole-model speedup ratios. effectively reduce accumulated error multiple layers thanks nonlinear asymmetric reconstruction. competitive references simonyan zisserman very deep convolutional networks large-scale image recognition international conference learning representations lecun boser denker henderson howard hubbard jackel backpropagation applied handwritten code recognition neural computation krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks international conference learning representations girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation ieee conference computer vision pattern recognition girshick faster r-cnn towards real-time object detection region proposal networks advances neural information processing systems long shelhamer darrell fully convolutional networks semantic segmentation ieee conference computer vision pattern recognition hariharan arbel´aez girshick malik hypercolumns object segmentation ﬁne-grained localization ieee conference computer vision pattern recognition denton zaremba bruna lecun fergus exploiting linear structure within convolutional networks efﬁcient evaluation advances neural information processing systems lebedev ganin rakhuba oseledets lempitsky speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition international conference learning representations deng shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding arxiv. figurnov vetrov kohli perforatedcnns acceleration elimination redundant convolutions arxiv. vasilache johnson mathieu chintala piantino lecun fast convolutional nets fbfft performance evaluation international conference learning representations gong lazebnik iterative quantization procrustean approach learning binary codes ieee conference computer vision pattern recognition optimized product quantization ieee transactions pattern analysis machine intelligence fang gupta iandola srivastava deng doll´ar mitchell platt from captions visual concepts back ieee conference computer vision pattern recognition", "year": 2015}