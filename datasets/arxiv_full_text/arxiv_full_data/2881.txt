{"title": "Linearized Additive Classifiers", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "We revisit the additive model learning literature and adapt a penalized spline formulation due to Eilers and Marx, to train additive classifiers efficiently. We also propose two new embeddings based two classes of orthogonal basis with orthogonal derivatives, which can also be used to efficiently learn additive classifiers. This paper follows the popular theme in the current literature where kernel SVMs are learned much more efficiently using a approximate embedding and linear machine. In this paper we show that spline basis are especially well suited for learning additive models because of their sparsity structure and the ease of computing the embedding which enables one to train these models in an online manner, without incurring the memory overhead of precomputing the storing the embeddings. We show interesting connections between B-Spline basis and histogram intersection kernel and show that for a particular choice of regularization and degree of the B-Splines, our proposed learning algorithm closely approximates the histogram intersection kernel SVM. This enables one to learn additive models with almost no memory overhead compared to fast a linear solver, such as LIBLINEAR, while being only 5-6X slower on average. On two large scale image classification datasets, MNIST and Daimler Chrysler pedestrians, the proposed additive classifiers are as accurate as the kernel SVM, while being two orders of magnitude faster to train.", "text": "revisit additive model learning literature adapt penalized spline formulation eilers marx train additive classiﬁers efﬁciently. also propose embeddings based classes orthogonal basis orthogonal derivatives also used efﬁciently learn additive classiﬁers. paper follows popular theme current literature kernel svms learned much efﬁciently using approximate embedding linear machine. paper show spline basis especially well suited learning additive models sparsity structure ease computing embedding enables train models online manner without incurring memory overhead precomputing storing embeddings. show interesting connections b-spline basis histogram intersection kernel show particular choice regularization degree b-splines proposed learning algorithm closely approximates histogram intersection kernel svm. enables learn additive models almost memory overhead compared fast linear solver liblinear slower average. large scale image classiﬁcation datasets mnist daimler chrysler pedestrians proposed additive classiﬁers accurate kernel orders magnitude faster train. parametric models classiﬁcation become attractive since introduction kernel methods like support vector machines complexity learned models scale data gives desirable asymptotic properties. however estimation point view parametric models offer signiﬁcant statistical computational advantages. recent years seen shift focus non-parametric models semi-parametric learning classiﬁers. includes work rahimi recht compute approximate feature shift invariant kernels solve kernel problem approximately using linear problem. line work become extremely attractive advent several algorithms training linear classiﬁers efﬁciently pegasos including online variants memory overhead. useful kernels used computer vision based comparing distributions level features images additive e.g. histogram intersection kernel dimensional decomposition allows compute approximate feature maps independently dimension leading compact feature maps making estimation efﬁcient. line work explored maji berg construct approximate feature maps kernel learn piecewise linear functions dimension. γ-homogenous additive kernels vedaldi smoothing splines another estimating additive models well known statistical community. ever since generalized additive models introduced hastie tibshirani many practical approaches training models regression emerged example p-spline formulation eilers marx however algorithms scale extremely large datasets high-dimensional features typical image text classiﬁcation datasets. work show spline framework used derive embeddings train additive classiﬁers efﬁciently well. propose families embeddings property underlying additive classiﬁer learned directly estimating linear classiﬁer embedded space. ﬁrst family embeddings based penalized spline formulation additive models function dimension represented using uniformly spaced spline basis regularization penalizes difference adjacent spline coefﬁcients. second class embeddings based generalized fourier expansion function dimension. work ties literature additive model regression linear svms develop algorithms train additive models classiﬁcation setting. discuss additive embeddings related additive kernels section particular representations include special case arising particular choice b-spline basis regularization. advantage representations allows explicit control smoothness functions choice basis functions desirable certain situations. moreover sparsity representations lead efﬁcient training algorithms smooth functions. summarize previous work next section. history learning additive models goes back proposed backﬁtting algorithm estimate additive models. since many practical approaches emerged prominent penalized spline formulation proposed consists modeling dimensional functions using large number uniformly spaced b-spline basis. smoothness ensured penalizing differences adjacent spline coefﬁcients. describe formulation detail section advantage formulation whole problem could solved using linear system. given data discriminative training functions often involve optimization various kernel svms regularization penalizes norm function implicit reproducing kernel hilbert space kernel. approximating rkhs additive kernels provides training additive kernel classiﬁers efﬁciently. shift invariant kernels rahimi recht derive features based boshner’s theorem. vedaldi zisserman propose closed form features hein bousquet train additive kernel svms efﬁciently many commonly used additive kernels γ-homogenous. kernel maji berg propose approximation efﬁcient learning algorithm work closely related this. additive modeling setting typical regularization penalize norm order deriva. features based encodings enable efﬁcient evaluation computation regularization. discussion assume features dimensional. embeddings derived eilers marx proposed practical modeling approach gams. idea based representing functions dimension using relatively large number uniformly spaced b-spline basis. smoothness functions ensured penalizing ﬁrst second order differences adjacent spline coefﬁcients. denote vector entries projection basis function. p-spline optimization problem classiﬁcation setting hinge loss function consists minimizing ﬁrst difference vector elements αi+. higher order difference matrices computed repeating differencing. dimensional basis difference matrix matrix dii+ zero everywhere else. matrices follows enable reduction linear case propose slightly different difference matrix matrix dii− ﬁrst order difference matrix proposed eilers marx added top. resulting difference matrices ﬁrst effect penalizing norm ﬁrst coefﬁcient spline basis plays role regularization linear setting alternatively think additional basis left point coefﬁcient zero. advantage matrix invertible particularly simple form allows linearize whole system. also show section derived embeddings also approximate learning problem kernel classiﬁer using kernel particular choice spline basis. figure local basis functions linear quadratic cubic various regularizations degrees ﬁgure refers dense features function shown local basis b-splines. degree b-spline refer readers excellent review additive modeling using splines. figure shows various choices regularization degree splines basis linear quadratic cubic. thus overall regularized additive classiﬁer learned learning linear classiﬁer embedded space practice approximate scheme using small number basis function. propose practical ones closed form embeddings fourier basis. classic fourier basis functions orthogonal wrto. weight function derivatives also family hence also orthonormal. normalized feature embeddings shown table property nhn− obtain closed form features shown table also known family polynomial basis functions orthogonal whose derivatives orthogonal belong three families jacobi laguerre hermite extended support weight function hermite basis makes well suited additive modeling. although basis complete practical purposes ﬁrst basis. quality approximation depends well underlying function approximated chosen basis example degree polynomials better represented hermite basis. begin showing close resemblance spline embeddings kernel. this features represented uniformly spaced linear spline basis centered features given features matrix given corresponding linear quadratic cubic b-spline basis. figure shows recent paper maji berg propose linear spline basis regularization train approximate intersection kernel svms turn approximate arbitrary additive classiﬁers. features seen generalization work allows arbitrary spline basis regularizations. kmin figure spline kernels. kmin along corresponding linear quadratic cubic b-spline basis. using uniformly spaced basis separated kernels closely approximate kernel seen difference image. approximation exact however features numerically stable b-spline basis experimental comparison). truncated polynomials degree corresponds b-spline basis degree regularization kernel knots uniformly spaced. b-splines derived truncated polynomial basis repeated application difference matrix noted authors advantages p-spline formulation decouples order regularization spline basis. typically regularization provides sufﬁcient smoothness experiments. b-spline basis exploit sparsity speed-up linear solvers. classiﬁcation function based evaluating methods training linear methods based evaluating classiﬁer updating classiﬁer classiﬁcation incorrect. since number evaluations larger number updates much efﬁcient maintain sparse vector multiplication. updates weight vector various gradient descent algorithms look like step unlike matrix d′ddd matrix dense hence updates change entries however compute steps instead steps exploiting simple form initialize repeat step times followed step times compute ldφ. often large datasets consisting high dimensional features avoid memory bottleneck compute encodings inner loop training algorithm. refer online method. solver based liblinear easily used solver. custom solver allows exploit sparsity embeddings practical regularization b-spline embeddings identity matrix leads sparse features. makes difﬁcult estimate weights basis functions data higher order b-spline basis somewhat mitigate problem. present image classiﬁcation experiments image datasets mnist daimler chrysler pedestrians datasets classiﬁers based histogram intersection kernel outperforms linear classiﬁer used features based spatial pyramid histogram oriented gradients obtain features author’s website experiments. mnist dataset instances features dimensional dense leading non-zero entries. dataset three training sets test sets. training instances features dimensional dense leading entries. sizes typical image datasets training kernel classiﬁers often take several hours single machine. figure data generated according b-spline along various degrees regularizations uniformly spaced bins fourier along using features various frequencies regularizations. hermite along various degrees regularizations. table b-spline parameter choices dataset. training times accuracies various b-spline parameters ﬁrst split dataset. comparison training linear model using liblinear takes achieves accuracy. regularization higher degree splines accurate regularization linear spline basis accurate quadratic/cubic number bins large. training using regularization faster higher order regularization seems unnecessary. note implementation computes encodings hence memory overhead liblinear. dataset. points sampled uniformly grid points satisfying positive class others negative. figure shows data along dimension using uniformly spaced b-spline basis various degrees regularizations. quadratic cubic splines offer smoother data. figure shows learned functions using fourier hermite embeddings various degrees respectively. effect b-spline parameter choices. table shows accuracy training times function number bins regularization degree b-spline basis degree ﬁrst split pedestrian dataset. bias term training models. dataset accurate signiﬁcantly faster. experiments include results addition setting regularization zero leads sparse features used directly linear solver exploit sparsity. training time b-splines scales sub-linearly number bins hence better functions obtained without much loss efﬁciency. effect fourier embedding parameter choices. table shows accuracy training times various fourier embeddings dataset. computing generalized fourier features ﬁrst normalize data dimension using table fourier embeddings dataset. accuracies training times various fourier embeddings ﬁrst split pedestrian dataset. note methods trained ﬁrst encoding features training linear model using liblinear. table training times test accuracies various additive classiﬁers dataset. online training method compute encodings batch method computes encoding uses liblinear train linear classiﬁer. b-splines online method faster hence omit training times batch method. additive classiﬁers outperform linear faster svm. precompute features liblinear train various models since relatively expensive compute features online. case training times similar bspline models. however precomputing storing features possible large scale datasets. comparison various additive models. table shows accuracy training times various additive models compared linear expensive kernel training test combinations dataset. optimal parameters found ﬁrst training test set. additive models faster train accurate kernel svm. b-spline additive models signiﬁcantly outperform linear dataset expense small additional training time. table shows accuracies training times various additive models mnist dataset. train one-vs-all classiﬁers digit classiﬁcation scores normalized passing logistic. testing example given label classiﬁer highest response. optimal parameters training found using -fold cross validation training set. additive models signiﬁcantly outperform linear classiﬁer closely matches accuracy kernel faster. table test error mean training times digit various additive classiﬁers mnist. b-splines online method faster hence omit training times batch method. additive classiﬁers outperform linear faster svm. proposed family embeddings enable efﬁcient learning additive classiﬁers. advocate b-splines based embeddings efﬁcient compute sparse enables train models small memory overhead computing embeddings even number basis large seen generalization generalized fourier features dimensional expensive compute suitable projected features precomputed stored. proposed classiﬁers outperform linear classiﬁers match signiﬁcantly expensive kernel classiﬁers fraction training time. mnist datasets linear b-spline regularization works best closely approximates learning problem kernel svm. higher degree splines useful used regularization even faster training times worse accuracies regularization. code training various spline models proposed paper packaged library libspline released upon publication paper.", "year": 2011}