{"title": "Learning Efficient Convolutional Networks through Network Slimming", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.", "text": "deployment deep convolutional neural networks many real world applications largely hindered high computational cost. paper propose novel learning scheme cnns simultaneously reduce model size; decrease run-time memory footprint; lower number computing operations without compromising accuracy. achieved enforcing channel-level sparsity network simple effective way. different many existing approaches proposed method directly applies modern architectures introduces minimum overhead training process requires special software/hardware accelerators resulting models. call approach network slimming takes wide large networks input models training insigniﬁcant channels automatically identiﬁed pruned afterwards yielding thin compact models comparable accuracy. empirically demonstrate effectiveness approach several state-of-the-art models including vggnet resnet densenet various image classiﬁcation datasets. vggnet multi-pass version network slimming gives reduction model size reduction computing operations. recent years convolutional neural networks become dominant approach variety computer vision tasks e.g. image classiﬁcation object detection semantic segmentation large-scale datasets high-end modern gpus network architectures allow development unprecedented large models. instance alexnet vggnet googlenet resnets imagenet classiﬁcation challenge winner models evolved layers layers. however larger cnns although stronger representation power resource-hungry. instance -layer resnet million parameters requires giga ﬂoat-point-operations inferencing image resolution unlikely affordable resource constrained platforms mobile devices wearables internet things devices. deployment cnns real world applications mostly constrained model size cnns’ strong representation power comes millions trainable parameters. parameters along network structure information need stored disk loaded memory inference time. example storing typical trained imagenet consumes space resource burden embedded devices. run-time memory inference time intermediate activations/responses cnns could even take memory space storing model parameters even batch size problem high-end gpus unaffordable many applications computational power. number computing operations convolution operations computationally intensive high resolution images. large take several minutes process single image mobile device making unrealistic adopted real applications. many works proposed compress large cnns directly learn efﬁcient models fast inference. include low-rank approximation network quantization binarization weight pruning dynamic inference etc. however methods address challenges mentioned above. moreover techniques require specially designed software/hardware accelerators execution speedup another direction reduce resource consumption large cnns sparsify network. sparsity imposed different level structures yields considerable model-size compression inference speedup. however approaches generally refigure associate scaling factor channel convolutional layers. sparsity regularization imposed scaling factors training automatically identify unimportant channels. channels small scaling factor values pruned pruning obtain compact models ﬁne-tuned achieve comparable accuracy normally trained full network. paper propose network slimming simple effective network training scheme addresses aforementioned challenges deploying large cnns limited resources. approach imposes regularization scaling factors batch normalization layers thus easy implement without introducing change existing architectures. pushing values scaling factors towards zero regularization enables identify insigniﬁcant channels scaling factor corresponds speciﬁc convolutional channel facilitates channel-level pruning followed step. additional regularization term rarely hurt performance. fact cases leads higher generalization accuracy. pruning unimportant channels sometimes temporarily degrade performance effect compensated followed ﬁne-tuning pruned network. pruning resulting narrower network much compact terms model size runtime memory computing operations compared initial wide network. process repeated several times yielding multi-pass network slimming scheme leads even compact network. experiments several benchmark datasets different network architectures show obtain models mode-size compression reduction computing operations original ones achieving even higher accuracy. moreover method achieves model compression inference speedup conventional hardware deep learning software packages since resulting narrower model free sparse storing format computing operations. low-rank decomposition approximates weight matrix neural networks low-rank matrix using techniques like singular value decomposition method works especially well fully-connected layers yielding model-size compression however without notable speed acceleration since computing operations mainly come convolutional layers. weight quantization. hashnet proposes quantize network weights. training network weights hashed different groups within group weight value shared. shared weights hash indices need stored thus large amount storage space could saved. uses improved quantization technique deep compression pipeline achieves compression rates alexnet vggnet. however techniques neither save run-time memory inference time since inference shared weights need restored original positions. quantize real-valued weights binary/ternary weights yields large amount model-size saving signiﬁcant speedup could also obtained given bitwise operation libraries. however aggressive low-bit approximation method usually comes moderate accuracy loss. weight pruning sparsifying. proposes prune unimportant connections small weights trained neural networks. resulting network’s weights mostly zeros thus storage space reduced storing model sparse format. however methods achieve speedup dedicated sparse matrix operation libraries and/or hardware. run-time memory saving also limited since memory space consumed activation maps instead weights. guidance sparsity training. overcomes limitation explicitly imposing sparse constraint weight additional gate variables achieve high compression rates pruning connections zero gate values. method achieves better comadvantages channel-level sparsity. discussed prior works sparsity realized different levels e.g. weight-level kernel-level channel-level layer-level. fine-grained level sparsity gives highest ﬂexibility generality leads higher compression rate usually requires special software hardware accelerators fast inference sparsiﬁed model contrary coarsest layer-level sparsity require special packages harvest inference speedup less ﬂexible whole layers need pruned. fact removing layers effective depth sufﬁciently large e.g. layers comparison channel-level sparsity provides nice tradeoff ﬂexibility ease implementation. applied typical cnns fullyconnected networks resulting network essentially thinned version unpruned network efﬁciently inferenced conventional platforms. challenges. achieving channel-level sparsity requires pruning incoming outgoing connections associated channel. renders method directly pruning weights pre-trained model ineffective unlikely weights input output channel happen near zero values. reported pruning channels pre-trained resnets lead reduction number parameters without suffering accuracy loss. addresses problem enforcing sparsity regularization training objective. speciﬁcally adopt group lasso push ﬁlter weights corresponds channel towards zero simultaneously training. however approach requires computing gradients additional regularization term respect ﬁlter weights nontrivial. introduce simple idea address challenges details presented below. scaling factors sparsity-induced penalty. idea introducing scaling factor channel multiplied output channel. jointly train network weights scaling factors sparsity regularization imposed latter. finally prune channels small factors ﬁne-tune pruned network. speciﬁcally training objective approach given denote train input target denotes trainable weights ﬁrst sum-term corresponds normal training loss sparsity-induced penalty scaling factors balances terms. experiment choose known pression rate suffers drawback. structured pruning sparsifying. recently proposes prune channels small incoming weights trained cnns ﬁne-tune network regain accuracy. introduces sparsity random deactivating input-output channel-wise connections convolutional layers training also yields smaller networks moderate accuracy loss. compared works explicitly impose channel-wise sparsity optimization objective training leading smoother channel pruning process little accuracy loss. imposes neuron-level sparsity training thus neurons could pruned obtain compact networks. proposes structured sparsity learning method sparsify different level structures cnns. methods utilize group sparsity regualarization training obtain structured sparsity. instead resorting group sparsity convolutional weights approach imposes simple sparsity channel-wise scaling factors thus optimization objective much simpler. since methods prune sparsify part network structures instead individual weights usually require less specialized libraries achieve inference speedup run-time memory saving. network slimming also falls category absolutely special libraries needed obtain beneﬁts. neural architecture learning. state-of-the-art cnns typically designed experts also explorations automatically learning network architectures. introduces sub-modular/supermodular optimization network architecture search given resource budget. recent works propose learn neural architecture automatically reinforcement learning. searching space methods extremely large thus needs train hundreds models distinguish good ones. network slimming also treated approach architecture learning despite choices limited width layer. however contrast aforementioned methods network slimming learns network architecture single training process line goal efﬁciency. provide simple scheme achieve channellevel sparsity deep cnns. section ﬁrst discuss advantages challenges channel-level sparsity introduce leverage scaling layers batch normalization effectively identify prune unimportant channels network. l-norm widely used achieve sparsity. subgradient descent adopted optimization method nonsmooth penalty term. alternative option replace penalty smooth-l penalty avoid using sub-gradient non-smooth point. pruning channel essentially corresponds removing incoming outgoing connections channel directly obtain narrow network without resorting special sparse computation packages. scaling factors agents channel selection. jointly optimized network weights network automatically identity insigniﬁcant channels safely removed without greatly affecting generalization performance. leveraging scaling factors layers. batch normalization adopted modern cnns standard approach achieve fast convergence better generalization performance. normalizes activations motivates design simple efﬁcient method incorporates channel-wise scaling factors. particularly layer normalizes internal activations using mini-batch statistics. zout input output layer denotes current minibatch layer performs following transformation mean standard deviation values input activations trainable afﬁne transformation parameters provides possibility linearly transforming normalized activations back scales. common practice insert layer convolutional layer channel-wise scaling/shifting parameters. therefore directly leverage parameters layers scaling factors need network slimming. great advantage introducing overhead network. fact perhaps also effective learn meaningful scaling factors channel pruning. scaling layers without layer value scaling factors meaningful evaluating importance channel convolution layers scaling layers linear transformations. obtain results decreasing scaling factor values amplifying weights convolution layers. insert scaling layer layer scaling effect scaling layer completely canceled normalization process insert scaling layer layer consecutive scaling factors channel. channel pruning fine-tuning. training channel-level sparsity-induced regularization obtain model many scaling factors near zero prune channels near-zero scaling factors removing incoming outgoing connections corresponding weights. prune channels global threshold across layers deﬁned certain percentile scaling factor values. instance prune channels lower scaling factors choosing percentile threshold obtain compact network less parameters run-time memory well less computing operations. pruning temporarily lead accuracy loss pruning ratio high. largely compensated followed ﬁne-tuning process pruned network. experiments ﬁne-tuned narrow network even achieve higher accuracy original unpruned network many cases. multi-pass scheme. also extend proposed method single-pass learning scheme multipass scheme. speciﬁcally network slimming procedure results narrow network could apply whole training procedure learn even compact model. illustrated dotted-line figure experimental results show multi-pass scheme lead even better results terms compression rate. handling cross layer connections pre-activation structure. network slimming process introduced directly applied plain architectures alexnet vggnet adaptations required applied modern networks cross layer connections pre-activation design resnet densenet networks output layer treated input multiple subsequent layers layer placed convolutional layer. case sparsity achieved incoming layer i.e. layer selectively uses subset channels received. harvest parameter computation savings test time need place channel selection layer mask insigniﬁcant channels identiﬁed. table results cifar svhn datasets. baseline denotes normal training without sparsity regularization. column- pruned denotes ﬁne-tuned model channels pruned model trained sparsity etc. pruned ratio parameters flops also shown column-&. pruning moderate amount channels mostly lower test errors. accuracy could typically maintained channels pruned. cifar. cifar datasets consist natural images resolution cifar- drawn cifar- classes. train test sets contain images respectively. cifar validation images split training search model. report ﬁnal test errors training ﬁne-tuning training images. standard data augmentation scheme adopted. input data normalized using channel means standard deviations. also compare method cifar datasets. svhn. street view house number dataset consists colored digit images. following common practice training images split validation images model selection training. test contains images. training select model lowest validation error model pruned also report test errors models lowest validation errors ﬁne-tuning. imagenet. imagenet dataset contains million training images validation images classes. adopt data augmentation scheme report single-center-crop validation error ﬁnal model. mnist. mnist handwritten digit dataset containing training images test images. test effectiveness method fully-connected network compare method dataset. cifar svhn dataset evaluate method three popular network architectures vggnet resnet densenet vggnet originally designed imagenet classiﬁcation. experiment variation original vggnet cifar dataset taken resnet -layer pre-activation resnet bottleneck structure used. densenet -layer densenet growth rate imagenet dataset adopt -layer vgg-a network model batch normalization remove dropout layers since relatively heavy data augmentation. prune neurons fully-connected layers treat convolutional channels spatial size. training pruning fine-tuning normal training. train networks normally scratch baselines. networks trained using sgd. cifar svhn datasets train using minibatch size epochs respectively. initial learning rate divided total number training epochs. imagenet mnist datasets train models epochs respectively batch size initial learning rate divided fraction training epochs. weight decay nesterov momentum without dampening. weight initialization introduced adopted. optimization settings closely follow original implementation experiments initialize channel scaling factors since gives higher accuracy baseline models compared default setting training sparsity. cifar svhn datasets training channel sparse regularization hyperparameteer controls tradeoff empirical loss sparsity determined grid search cifar- validation set. vggnet choose resnet densenet λ=−. vgg-a imagenet λ=−. settings kept normal training. pruning. prune channels models trained sparsity pruning threshold scaling factors needs determined. unlike different layers pruned different ratios global pruning threshold simplicity. pruning threshold determined percentile among scaling factors e.g. channels pruned. pruning process implemented building narrower model copying corresponding weights model trained sparsity. fine-tuning. pruning obtain narrower compact model ﬁne-tuned. cifar svhn mnist datasets ﬁne-tuning uses optimization setting training. imagenet dataset time constraint ﬁne-tune pruned vgg-a learning rate epochs. results cifar svhn results cifar svhn shown table mark lowest test errors model boldface. parameter flop reductions. purpose network slimming reduce amount computing resources needed. last model channels pruned still maintaining similar accuracy baseline. parameter saving flop reductions typically around highlight network slimming’s efﬁciency plot resource savings figure observed vggnet large amount redundant parameters pruned. resnet- parameter flop savings relatively insigniﬁcant conjecture bottleneck structure already functioned selecting channels. also cifar- reduction rate typically slightly lower cifar- svhn possibly fact cifar- contains classes. regularization effect. table observe that resnet densenet typically channels pruned ﬁne-tuned network achieve lower test error original models. example densenet- channels pruned achieve test error cifar- almost lower original model. hypothesize regularization effect sparsity channels naturally provides feature selection intermediate layers network. analyze effect next section. imagenet. results imagenet dataset summarized table channels pruned parameter saving flop saving fact channels computation-intensive convolutional layers pruned neurons parameter-intensive fully-connected layers pruned. worth noting method achieve savings accuracy loss -class imagenet dataset methods efﬁcient cnns mostly report accuracy loss. mnist. mnist dataset compare method structured sparsity learning method table despite method mainly designed prune channels convolutional layers also works well pruning neurons fully-connected layers. experiment observe pruning global threshold sometimes completely removes layer thus prune neurons intermediate layers. method slightly outperforms slightly lower test error achieved pruning parameters. provide additional experimental results supplementary materials including detailed structure compact vggnet cifar-; wall-clock time run-time memory savings practice. comparison previous channel pruning method employ multi-pass scheme cifar datasets using vggnet. since skip-connections pruning away whole layer completely destroy models. thus besides setting percentile threshold also constraint layer channels pruned. table results multi-pass scheme cifar- cifar datasets using vggnet. baseline model test errors trained fine-tuned columns denote test errors model trained sparsity ﬁnetuned model channel pruning respectively. parameter flop pruned ratios correspond ﬁne-tuned model trained model next row. compact models. cifar- trained model achieves lowest test error iteration model achieves parameter reduction flop reduction still achieving lower test error. cifar- iteration test error begins increase. possibly contains classes cifar- pruning channels agressively inevitably hurt performance. however still prune near parameters near flops without notable accuracy loss. crucial hyper-parameters network slimming pruned percentage coefﬁcient sparsity regularization term section analyze effects detail. effect pruned percentage. obtain model trained sparsity regularization need decide percentage channels prune model. prune channels resource saving limited. however could destructive model prune many channels possible recover accuracy ﬁne-tuning. train densenet model cifar- show effect pruning varying percentage channels. results summarized figure tuning process typically compensate possible accuracy loss caused pruning. threshold goes beyond test error ﬁne-tuned model falls behind baseline model. notably trained sparsity even without ﬁne-tuning model performs better original model. possibly regularization effect sparsity channel scaling factors. channel sparsity regularization. purpose sparsity term force many scaling factors near zero. parameter equation controls significance compared normal training loss. figure plot distributions scaling factors whole network different values. experiment vggnet trained cifar- dataset. observed increase scaling factors concentrated near zero. i.e. there’s sparsity regularization distribution relatively ﬂat. almost scaling factors fall small region near zero. process seen feature selection happening intermediate layers deep networks channels non-negligible scaling factors chosen. visualize process heatmap. figure shows magnitude scaling factors layer vggnet along training process. channel starts equal weights; training figure visulization channel scaling factors’ change scale along training process taken conv-layer vggnet trained cifar-. brighter color corresponds larger value. bright lines indicate selected channels dark lines indicate channels pruned. proposed network slimming technique learn compact cnns. directly imposes sparsity-induced regularization scaling factors batch normalization layers unimportant channels thus automatically identiﬁed training pruned. multiple datasets shown proposed method able signiﬁcantly decrease computational cost state-of-the-art networks accuracy loss. importantly proposed method simultaneously reduces model size run-time memory computing operations introducing minimum overhead training process resulting models require special libraries/hardware efﬁcient inference. acknowledgements. huang supported international postdoctoral exchange fellowship program china postdoctoral council changshui zhang supported nsfc joint project nsfc /dfg trr-.", "year": 2017}