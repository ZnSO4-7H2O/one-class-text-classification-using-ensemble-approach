{"title": "C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework", "tag": ["stat.ML", "cs.CV"], "abstract": "Sparse modeling is a powerful framework for data analysis and processing. Traditionally, encoding in this framework is performed by solving an L1-regularized linear regression problem, commonly referred to as Lasso or Basis Pursuit. In this work we combine the sparsity-inducing property of the Lasso model at the individual feature level, with the block-sparsity property of the Group Lasso model, where sparse groups of features are jointly encoded, obtaining a sparsity pattern hierarchically structured. This results in the Hierarchical Lasso (HiLasso), which shows important practical modeling advantages. We then extend this approach to the collaborative case, where a set of simultaneously coded signals share the same sparsity pattern at the higher (group) level, but not necessarily at the lower (inside the group) level, obtaining the collaborative HiLasso model (C-HiLasso). Such signals then share the same active groups, or classes, but not necessarily the same active set. This model is very well suited for applications such as source identification and separation. An efficient optimization procedure, which guarantees convergence to the global optimum, is developed for these new models. The underlying presentation of the new framework and optimization approach is complemented with experimental examples and theoretical results regarding recovery guarantees for the proposed models.", "text": "sparse modeling powerful framework data analysis processing. traditionally encoding framework performed solving -regularized linear regression problem commonly referred lasso basis pursuit. work combine sparsity-inducing property lasso individual feature level block-sparsity property group lasso sparse groups features jointly encoded obtaining sparsity pattern hierarchically structured. results hierarchical lasso shows important practical advantages. extend approach collaborative case simultaneously coded signals share sparsity pattern higher level necessarily lower level obtaining collaborative hilasso model signals share active groups classes necessarily active set. model well suited applications source identiﬁcation separation. efﬁcient optimization procedure guarantees convergence global optimum developed models. underlying presentation framework optimization approach complemented experimental examples theoretical results regarding recovery guarantees. work extend approaches number directions. first present hierarchical sparse model groups atoms active time also group enjoys internal sparsity. assume data samples dictionary atoms assembled matrix rm×p sample written linear combination atoms dictionary plus perturbation satisfying basic underlying assumption sparse modeling that optimal nonzero elements. formally deﬁne cost pseudo-norm counting number nonzero elements expect seeking sparsest representation known np-hard. determine practice multitude efﬁcient algorithms proposed achieve high correct recovery rates. -minimization method extensively studied recovery technique. approach non-convex norm replaced convex norm leading appropriate parameter value usually found cross-validation based statistical principles fact regularizer induces sparsity solution desirable regularization point view also model selection perspective wants identify relevant factors conform sample many situations however goal represent relevant factors singletons groups atoms. dictionary atoms deﬁne groups atoms indices given group indexes denote sub-dictionary columns indexed corresponding reconstruction coefﬁcients deﬁne partition order perform model selection group level group lasso problem introduced fig. sparsity patterns induced hilasso c-hilasso model selection programs. notice c-hilasso imposes group-sparsity pattern samples whereas in-group sparsity patterns vary samples always consider noiseless sparse coding problem mina∈rp generic regularizer limit lagrangian sparse coding problem mina∈rp remainder section well section present corresponding lagrangian formulations. models lasso group lasso optimal parameters application data dependent. speciﬁc cases closed form solutions exist parameters. example signal restoration presence noise using lasso gsure method provides simple compute optimal extending methods hilasso beyond scope work rely cross-validation choice parameters. selection important inﬂuence sparsity obtained solution. intuitively increases group constraint becomes dominant solution tends sparse group level less sparse within groups regularized regression problem gives rise called collaborative sparse coding problem considering coefﬁcients matrix rp×n associated reconstruction samples rm×n model given fig. effect different combinations solutions hilasso coding problem. three cases given want recover sparse signal means solution hilasso problem example active groups possible estimate closest norm shown left. ratio increases level sets regularizer become rounder thus encouraging denser solutions. depicted rightmost ﬁgure simple case groups. increasing increases sparsity although ﬁnal effect strong non-zero coefﬁcients detected. collaborative framework introduced here theoretical guarantees. recovery mixed signals optimization addressed model include block sparsity collaboration special case c-hilasso c-glasso investigated theoretical analysis signal recovery properties model developed. collaborative coding structured sparsity also smooth convex function needs ﬁnite convex formulation particular case proximal method framework developed nesterov includes important particular cases lasso group-lasso hilasso problems setting reconstruction error choosing corresponding regularizers regularizer group separable optimization subdivided smaller problems group. framework becomes powerful sub-problems equality follows. since terms positive hold long gives vector thresholding condition solution terms easy show sufﬁcient condition thus obtain experiments. note that expected solution sub-problem cases corresponds respectively scalar soft thresholding vector soft thresholding. particular proposed optimization reduces iterative soft thresholding algorithm multi-signal case equivalent one-dimensional case signal concatenation columns dictionary nm×np block-diagonal matrix blocks copy original dictionary however practice needed build dictionary operate directly matrices deﬁne matrix rm×n whose i-th column given collaborative case section) assume measurement noise perturbation without loss generality assume cardinality |gr| groups size. goal recover code observed solving noise-free hilasso problem probability error recovering correct groups using special case c-hilasso falls exponentially number collaborating samples grows. finally recent theoretical section extend group-wise indexing notation refer subsets rows columns arbitrary matrices {wij identity matrices column spaces respectively. deﬁne sets denote complement indices either respect depending context. difference denoted represents empty denotes cardinality deﬁne clearly columns orthonormal group assuming columns unit norm easily shown range addition easily prove setting block sparse internal structure sub-vector also sparse. order quantify ability recover signals expect internal sparsity well. denote gram matrix then standard block coherence deﬁned terms largest singular value off-diagonal sub-block similar fashion deﬁne sparse block coherence measures terms sparse singular values. different main recovery result stated follows. suppose block k-sparse vector blocks length block sparsity exactly rearrange columns coefﬁcients ﬁrst groups correspond non-zero blocks. within block ﬁrst indices represented correspond nonzero coefﬁcients block contains indices active blocks whereas contains inactive ones. similarly contains indices active coefﬁcients/atoms respectively indexes inactive coefﬁcients/atoms within fig. left indexing conventions shown shaded regions correspond active elements/atoms. active blocks light-colored active elements/coefﬁcients dark colored. represents alternate representation blocks atoms part true solution marked red. right partitioning matrix performed measure |ei| |fj| important assumption rely throughout columns must linearly invertible deﬁne reasons become clear later also need second orthogonal projection onto hilasso able recover situation lasso guaranteed idea that hilasso trades minimization terms tightening term improve group recovery loosening term also although clear conditions theorem ﬁnal data independent bounds also relaxation ones sufﬁcient conditions depend therefore nonzero blocks nonzero locations within blocks which course known advance. nonetheless theorem provides sufﬁcient conditions ensuring hold independent unknown signals proof prove recovers correct vector alternative solution satisfying show λψg) contain indices elements active blocks assumptions block exactly nonzero values. contain conclude guarantee recovery every choice long satisﬁed. note expected reduce lasso recovery condition. also meaning must tighten constrains related part cost function order relax part. hilasso conditions relaxation lasso conditions thus allowing signals correctly recovered. independent bound derived here depends coherence measures dictionary image projection since depends signal dependent. thus need maximize also possible sets deﬁned follows bound first note that since block-diagonal λzλ. choose diagonal matrix choice diagonal elements equal off-diagonal elements bounded using gerˇsgorin theorem norm. randomly chose groups active time sets normalized testing signals generated active group linear combinations elements mixtures created summing signals adding active dictionaries gaussian noise standard deviation generated testing signals hierarchical sparsity structure dictionary concatenating sub-dictionaries used solve lasso group lasso hilasso c-hilasso problems. table summarizes mean squared error hamming distance recovered coefﬁcient vectors observe model able exploit hierarchical structure data well collaborative structure. group lasso selects general correct blocks obtained. scale parameter account different number signals. situation analogous change size dictionary thus proportional square root number signals code. experimented usps digits dataset shown well represented sparse modeling framework signals vectors containing unwrapped gray intensities images obtained samples testing data mixture deviation peak value. digits case results average runs single digit case result average possible situations. amse hamming distance shown bold blue indicating best. without noise c-glasso c-hilasso yield good results. however noisy case c-hilasso clearly superior showing advantage adding example used c-hilasso analyze mixtures data contains different number types sources/classes. fig. used containing mixtures digit images. ﬁrst images obtained sum/mixture number number last images mixture three numbers ﬁgure shows active sets recovered coefﬁcients matrix binary matrix size black dots indicate nonzero coefﬁcients. c-hilasso managed identify active blocks sub-dictionary corresponding mostly active last images. accuracy result depends relationship sub-dictionaries corresponding digit. also compared performance c-hilasso lasso glasso c-glasso task separating mixed textures image. case signals corresponds patches image analyzed. chose textures brodatz dataset trained dictionary fig. example recovered digits mixture missing components. left right noiseless mixture observed mixture missing pixels highlighted recovered digits active recovered samples using c-hilasso lasso respectively. last ﬁgures active sets represented figure coefﬁcients blocks digits marked pink bands. notice c-hilasso exploits efﬁciently hypothesis collaborative group-sparsity succeeding recovering correct active groups samples. lasso lacks prior knowledge clearly capable active sets spread groups. fig. texture separation results. left right sample mixture corresponding c-hilasso separated textures comparison active diagrams obtained lasso lasso shown groups wrongly active c-hilasso bottom showing correct groups selected. c-hilasso best glasso c-glasso best respectively table conclude c-hilasso signiﬁcantly better competing algorithms recovered signals results lower triangle shows hamming error group-wise active recovery. within cell results shown lasso group lasso collaborative group lasso collaborative hierarchical lasso best results blue bold. note that amse hamming distance cases model outperforms previous ones. fig. speaker identiﬁcation results. column corresponds sources identiﬁed speciﬁc time frame true ones marked yellow dots. vertical axis indicates estimated activity different sources darker colors indicate higher energy. possible combination speakers frames evaluated. acknowledgments work partially supported nsseff aro. thank tristan nguyen presented model motivated think hierarchical fashion look particular case fully hierarchical sparse coding framework. thank prof. gonzalo mateos invaluable help optimization methods. thank prof. larry carin guoshen alexey castrodad stimulating conversations fact work also motivated part example missing information. anonymous reviewers prompted early mistake proof theorem that together additional comments improving bounds theorem well overall presentation paper. also want thank reviewer closed form inner loop proposed optimization method simpliﬁed resulted signiﬁcant practical improvements. jenatton audibert bach structured variable selection sparsity-inducing norms arxiv.v eldar mishali robust recovery signals structured union subspaces ieee trans. inform. theory vol. xing tree-guided group lasso multi-task regression structured sparsity icml june jenatton mairal obozinski bach proximal methods sparse hierarchical dictionary learning icml june starck elad donoho image decomposition combination sparse representations variational approach tibshirani regression shrinkage selection lasso royal stat. society series vol. chen donoho saunders atomic decomposition basis pursuit siam scientiﬁc computing vol. donoho compressed sensing ieee trans. inf. theory vol. giryes elad eldar projected gsure automatic parameter tuning iterative shrinkage methods submitted turlach venables wright simultaneous variable selection technometrics vol. sprechmann ramirez sapiro collaborative hierarchical sparse modeling ciss mar. boufounos kutyniok rauhut sparse recovery combined fusion frame measurements arxiv.v eldar kuppinger b¨olcskei block-sparse signals uncertainty relations efﬁcient recovery ieee trans. vol. stojnic block-length dependent thresholds block-sparse compressed sensing arxiv. july d’aspremont ghaoui jordan lanckriet direct formulation sparse using semideﬁnite programming rosenblum zelnik-manor eldar sensing matrix optimization block-sparse decoding arxiv. sep. shoham elad alternating ksvd-denoising texture separation ieee convention electrical electronics", "year": 2010}