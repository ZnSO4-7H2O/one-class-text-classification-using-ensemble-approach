{"title": "Attention-Based Models for Speech Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "text": "recurrent sequence generators conditioned input data attention mechanism recently shown good performance range tasks including machine translation handwriting synthesis image caption generation extend attention-mechanism features needed speech recognition. show adaptation model used machine translation reaches competitive phoneme error rate timit phoneme recognition task applied utterances roughly long ones trained offer qualitative explanation failure propose novel generic method adding location-awareness attention mechanism alleviate issue. method yields model robust long inputs achieves single utterances -times longer utterances. finally propose change attention mechanism prevents concentrating much single frames reduces level. recently attention-based recurrent networks successfully applied wide variety tasks handwriting synthesis machine translation image caption generation visual object classiﬁcation models iteratively process input selecting relevant content every step. basic idea signiﬁcantly extends applicability range end-to-end training methods instance making possible construct networks external memory introduce extensions attention-based recurrent networks make applicable speech recognition. learning recognize speech viewed learning generate sequence given another sequence perspective similar machine translation handwriting synthesis tasks attention-based methods found suitable however compared machine translation speech recognition principally differs requesting much longer input sequences introduces challenge distinguishing similar speech fragments single utterance. also different handwriting synthesis since input sequence much noisier clear structure. reasons speech recognition interesting testbed developing attention-based architectures capable processing long noisy inputs. application attention-based models speech recognition also important step toward building fully end-to-end trainable speech recognition systems active area research. dominant approach still based hybrid systems consisting deep neural acoustic model triphone model n-gram language model requires dictionaries hand-crafted pronunciation phoneme lexicons multi-stage training procedure make components work together. excellent results hmm-less recognizer recently reported system consisting ctc-trained neural network language model still language model added last stage work thus leaving open question much acoustic model beneﬁt aware language model training. paper evaluate attention-based models phoneme recognition task using widelyused timit dataset. time step generating output sequence attention mechanism selects weighs signals produced trained feature extraction mechanism potentially time steps input sequence weighted feature vector helps condition generation next element output sequence. since utterances dataset rather short measure ability considered models recognizing much longer utterances created artiﬁcially concatenating existing utterances. start model proposed machine translation task baseline. model seems entirely vulnerable issue similar speech fragments despite expectations competitive original test reaching phoneme error rate however performance degraded quickly longer concatenated utterances. provide evidence model adapted track absolute location input sequence content recognizing strategy feasible short utterances original test inherently unscalable. order circumvent undesired behavior paper propose modify attention mechanism explicitly takes account location focus previous step features input sequence achieved adding inputs attention mechanism auxiliary convolutional features extracted convolving attention weights previous step trainable ﬁlters. show model convolutional features performs signiﬁcantly better considered task importantly model convolutional features robustly recognized utterances many times longer ones training always staying per. therefore contribution work three-fold. present novel purely neural speech recognition architecture based attention mechanism whose performance comparable conventional approaches timit dataset. moreover propose generic method adding location awareness attention mechanism. finally introduce modiﬁcation attention mechanism avoid concentrating attention single frame thus avoid obtaining less effective training examples bringing attention-based recurrent sequence generator recurrent neural network stochastically generates output sequence input practice often processed encoder outputs sequential input representation suitable attention mechanism work with. context work output sequence phonemes input sequence feature vectors. feature vector extracted small overlapping window audio frames. encoder implemented deep bidirectional recurrent network form sequential representation length i-th step arsg generates output focusing relevant elements figure steps proposed attention-based recurrent sequence generator hybrid attention mechanism based content location information. dotted lines correspond thick solid lines dashed lines eqs. state recurrent neural network refer generator vector attention weights also often called alignment using terminology call glimpse. step completed computing generator state long short-term memory units gated recurrent units typically used recurrent activation refer recurrency. process graphically illustrated fig. inspired distinguish location-based content-based hybrid attention mechanisms. attend describes generic hybrid attention. term dropped attend arguments i.e. attend call content-based case attend often implemented scoring element separately normalizing scores main limitation scheme identical similar elements scored equally regardless position sequence. issue similar speech fragments raised above. often issue partially alleviated encoder e.g. birnn deep convolutional network encode contextual information every element however capacity elements always limited thus disambiguation context possible limited extent. alternatively location-based attention mechanism computes alignment generator state previous alignment attend. instance graves used location-based attention mechanism using gaussian mixture model handwriting synthesis model. case speech recognition type location-based attention mechanism would predict distance consequent phonemes using only expect hard large variance quantity. limitations associated content-based location-based mechanisms argue hybrid attention mechanism natural candidate speech recognition. informally would like attention model uses previous alignment select short list elements content-based attention eqs. select relevant ones without confusion. extend content-based attention mechanism original model location-aware making take account alignment produced previous step. first extract vectors every position previous alignment convolving matrix rk×r three potential issues normalization first input sequence long glimpse likely contain noisy information many irrelevant feature vectors normalized scores positive makes difﬁcult proposed arsg focus clearly relevant frames time second attention mechanism required consider frames time decodes single output decoding output length leading computational complexity easily become prohibitively expensive input utterances long side coin softmax normalization prefers mostly focus single feature vector prevents model aggregating multiple top-scored frames form glimpse sharpening straightforward address ﬁrst issue noisy glimpse sharpening scores αij. sharpen weights introduce inverse temperature softmax function keep top-k frames according scores re-normalize them. sharpening methods however still requires compute score every frame time worsen second issue overly narrow focus. also propose investigate windowing technique. time attention mechanism considers subsequence whole sequence predeﬁned window width median alignment αi−. scores computed resulting lower complexity windowing technique similar taking top-k frames similarly effect sharpening. smoothing observed proposed sharpening methods indeed helped long utterances. however them especially selecting frame highest score negatively affected model’s performance standard development mostly consists short utterances. observations hypothesize helpful model aggregate selections multiple top-scored frames. sense brings diversity i.e. effective training examples output part model input locations considered. facilitate effect replace unbounded exponential function softmax function bounded logistic sigmoid figure decoding performance w.r.t. beam size. rigorous comparison decoding failed generate considered wrongly recognized without retrying larger beams size. models especially smooth focus perform well even beam width small related work speech recognizers based connectionist temporal classiﬁcation extension transducer closest arsg model considered paper. follow earlier work end-to-end trainable deep learning sequences gradient signals ﬂowing alignment process shown perform well phoneme recognition task furthermore recently found able directly transcribe text speech without intermediate phonetic representation considered arsg different transducer ways. first whereas attention mechanism deterministically aligns input output sequences transducer treat alignment latent random variable inference performed. deterministic nature arsg’s alignment mechanism allows beam search procedure simpler. furthermore empirically observe much smaller beam width used deterministic mechanism allows faster decoding second alignment mechanism transducer constrained monotonic keep marginalization alignment tractable. hand proposed attention mechanism result non-monotonic alignment makes suitable larger variety tasks speech recognition. hybrid attention model using convolution operation also proposed neural turing machines time step computes content-based attention weights convolved predicted shifting distribution. unlike ntm’s approach hybrid mechanism proposed lets learning ﬁgure content-based location-based addressing combined deep parametric function sukhbaatar describes similar hybrid attention mechanism location embeddings used input attention model. approach important disadvantage model cannot work input sequence longer seen training. approach hand works well sequences many times longer seen training closely followed procedure experiments performed timit corpus used train-dev-test split kaldi timit recipe. trained standard speaker utterances removed used speaker early stopping. tested speaker core test set. networks trained mel-scale ﬁlterbank features together energy frame ﬁrst second temporal differences yielding total features frame. feature rescaled zero mean unit variance training set. networks trained full -phone extended extra end-of-sequence token appended target sequence. similarly appended all-zero frame input sequence indicate utterance. decoding performed using phoneme scoring done phoneme set. figure alignments produced baseline model. vertical bars indicate ground truth phone location timit. upper image indicates frames selected attention mechanism emit phone symbol. network clearly learned produce left-to-right alignment tendency look slightly ahead confuse repeated kclk phrase. best viewed color. parameters arsg. makes scales derivatives w.r.t. parameters vary signiﬁcantly handle using adaptive learning rate algorithm adadelta hyperparameters weight matrices initialized normal gaussian distribution standard deviation recurrent weights furthermore orthogonalized. timit relatively small dataset proper regularization crucial. used adaptive weight noise main regularizer ﬁrst trained models column norm constraint maximum norm lowest development negative log-likelihood achieved. time respectively. point began using adaptive weight noise scaled model complexity cost factor disabling column norm constraints. lowest development log-likelihood reached ﬁne-tuned model smaller observe improvement development phoneme error rate weight updates. batch size used throughout training. evaluated arsgs different attention mechanisms. encoder -layer birnn units direction activations top-layer units used representation generator single recurrent layer units. generate hidden layer maxout units. initial states encoder generator treated additional parameters. decoding procedure left-to-right beam search phoneme sequences used decoding beam search stopped end-of-sequence token emitted. started beam width increasing network failed produce narrower beam. shown fig. decoding wider beam gives little-to-none beneﬁt. surprise baseline model learned align properly. alignment produced baseline model sequence repeated phonemes presented fig. demonstrates baseline model confused short-range repetitions. also ﬁgure prefers select frames near beginning table phoneme error rates bold-faced corresponds best error rate attention-based recurrent sequence generator incorporating convolutional attention features smooth focus. figure results force-aligning concatenated utterances. represents single utterance created either concatenating multiple copies utterance different randomly chosen utterances. clearly highest robustness achieved hybrid attention mechanism combined proposed sharpening technique good performance baseline model question distinguishes repetitions similar phoneme sequences reliably decodes longer sequences repetitions. created datasets long utterances; repeating test utterance concatenating randomly chosen utterances. cases waveforms cross-faded silence inserted phone. concatenated utterances. first checked forced alignment longer utterances forcing generator emit correct phonemes. alignment considered correct alignment weight lies inside ground-truth phoneme window extended frames side. deﬁnition phones shown fig. properly aligned. ﬁrst column fig. shows number correctly aligned frames w.r.t. utterance length considered models. baseline model able decode sequences phones single utterance repeated phones different utterances concatenated. even failed correctly aligned phones. hand model hybrid attention mechanism convolutional features able align sequences phones long. however began fail model able align almost phones. model smoothing behaved similarly convolutional features only. found baseline model properly aligns ﬁrst phones makes jump recording cycles last phones. behavior suggests learned track approximate location source sequence. however tracking capability limited lengths observed training. tracker saturates jumps recording. figure phoneme error rates obtained decoding long sequences. network decoded alignment sharpening techniques produced proper forced alignments. proposed arsg’s clearly robust length utterances baseline contrast location-aware network failed stopped aligning particular frames selected phone. attribute behavior issue noisy glimpse discussed sec. long utterance many irrelevant frames negatively affecting weight assigned correct frames. line conjecture location-aware network works slightly better repetition utterance frames somehow relevant concatenation different utterances misaligned frame irrelevant. gain insight applied alignment sharpening schemes described sec. remaining columns fig. sharpening methods help location-aware network proper alignments show little effect baseline network. windowing technique helps baseline location-aware networks location-aware network properly aligning nearly sequences. visual inspection noticed middle long utterances baseline model confused repetitions similar content within window confusions happen beginning. supports conjecture above. evaluated models long sequences. model decoded using alignment sharpening techniques helped obtain proper forced alignments. results presented fig. baseline model fails decode long utterances even narrow window used constrain alignments produces. location-aware networks able decode utterances formed concatenating test utterances. better results obtained wider window presumably resembles training conditions step attention mechanism seeing whole input sequence. wide window networks scored long utterances indicating proposed location-aware attention mechanism scale sequences much longer training minor modiﬁcations required decoding stage. proposed evaluated novel end-to-end trainable speech recognition architecture based hybrid attention mechanism combines content location information order select next position input sequence decoding. desirable property proposed model recognize utterances much longer ones trained future expect model used directly recognize text speech case become important incorporate monolingual language model arsg architecture work contributed novel ideas attention mechanisms better normalization approach yielding smoother alignments generic principle extracting using features previous alignments. potentially applied beyond speech recognition. instance proposed attention used without modiﬁcation neural turing machines using convolution instead improving image caption generation authors would like acknowledge support following agencies research funding computing support national science center nserc calcul qu´ebec compute canada canada research chairs cifar. bahdanau also thanks planet intelligent systems gmbh yandex.", "year": 2015}