{"title": "Efficient forward propagation of time-sequences in convolutional neural  networks using Deep Shifting", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "When a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames.", "text": "convolutional neural network used on-the-ﬂy evaluation continuously updating time-sequences many redundant convolution operations performed. propose method deep shifting remembers previously calculated results convolution operations order minimize number calculations. reduction complexity least constant best case quadratic. demonstrate method indeed save signiﬁcant computation time practical implementation especially networks receives large number time-frames. convolutional neural networks proven extremely succesful highdimensional data including time-sequences audio video examples promising real-world applications speechvideo recognition automatic translation challenges software development include training networks using large clusters gathering huge labeled datasets practical user-side evaluation faces completely diﬀerent challenges including eﬃcient fast performance resource consumption responsiveness software responds recognized events quickly possible earlier work focusing achieving challenges include using less-parameter convolution ﬁlters pruning obsolete weights using spiking networks paper deals optimizing convolution timeseries used example convolutional neural networks applied human action recognition observe forward propagating continuously updating time-sequences neural network applies convolution time-dimension many redundant calculations made. order avoid calculations save resources potentially battery life mobile devices propose deep shifting copies results convolution operations earlier time steps rather re-calculating over. save substantial calculation time especially looks large number timeframes. paper organized follows section shows deep shifting performs operations time-sequences without performing redundant calculations. sections examine theoretical practical beneﬁts deep shifting. section investigates possibilities training network using minimal number neurons operations paper ﬁnishes discussion conclusion. input matrix shape timeaxis context axis. context axis holds called channels image convolutions. convolutional auto-encoder label layers respectively applies following encoding operation convolution time axis here size convolution window weight matrices denotes time step input sequence labels time axis weights. denotes activation function choose tanh computational part. schematic view given ﬁgure input consists time-evolving data typically want perform encoding every time information becomes available e.g. every time step. however nature convolution many redundant calculations made. actual time perform encoding denoted denote neuronal activation speciﬁc time step moreover indices deﬁned relative ‘leftmost’ input neuron time network ﬁgure received input time steps initially calculated much exactly prune original network? convolution operation calculation time-frame hidden layer e.g. full vector layers would normally span time steps don’t store recall practical applications convolutional auto-encoders stacked hourglass shape shallow layers span many time steps whilst deeper layers increasingly smaller time axes. assume start deepest hidden layer time-axis size stack number shallower layers. previous layers layer’s neurons calculated convolution. clearly increasing size time-axes number computations regular time-encoding scales quadratically number layers assuming ﬁxed case assumes every layer quadratic scaling holds even varying layer long windows sizes least hand deep shifting architecture scales linearly number layers situation slightly diﬀerent consider case time-axis input ﬁxed increasing numbers deeper layers smaller time axes. case total number convolutions formula give network time steps later time calculate formula goes time steps natural time activations network shifted network. therefore could well stored values copied neighbouring neurons without performing convolution calculation again. denote highest time step hidden neuron time reasoning holds arbitrary number layers used save calculations down-sampling easily extended convolution multiple dimensions convolutions often used video data moreover avoid obsolete reconstruction operations layer ﬁgure ﬁxed value could also principle stored shifted graphical comparison given ﬁgure assumptions deep shifting uses number convolution operations still equaling number layers. situation obtain linear gain performance average number convolutions layer compare matlab implementation deep shifting network ‘deep learn toolbox’ rasmus bergpalm original version applies convolution dimensions images specifically modiﬁed code apply convolution dimension representing time. consider networks ﬁrst using conventional architecture second exploiting deep shifting networks receive dataset representing part long time-sequence subsequent input previous sequence shifted ‘neuron’ time dimension. average time required forward propagation using various context axis sizes convolution windows displayed ﬁgure expected computation time regular increases increasing time-window input whereas remains constant using deep shifting. shifting operation implemented however requires higher base computation time whether deep shifting save computation time dependent parameters cnn. realistic situations context size used large-scale speech tasks deep shifting becomes beneﬁcial roughly time-frames given input cnn. moreover advantage deep shifting becomes noticeable size context axis increases. also sparse connectivity structure sufﬁcient deep shifting context learning? section compares training behaviour deep shifting regular convolutional network. train networks convolutional auto-encoder given test involves datesets. ﬁrst spoken digit dataset used ref. consisting number ‘zero’ ‘nine’ pronounced times diﬀerent speakers. second dataset auslan dataset time-series representing hand-movements australian deaf-community sign language obtained ref. test whether networks classify right digit sign respectively. first hidden units trained properly classify training percentage correct classiﬁcations separate test dataset measured. figure shows results spoken digit dataset. network denoted ‘shiftnet’ employed deep shifting shows competitive results compared regular cnn. note networks manage achieve percentage erroneous classiﬁcations indicating networks encode important information dataset small number hidden units. similarly approach ref. used random data points training others testing. figure shows results auslan dataset. parameter ‘len’ indicates number time-frames input scaled ‘nh’ indicates number hidden units. much harder dataset regular able perform much better shiftnet although latter still able achieve reasonable results despite limited connectivity. test -fold cross validation used data taken test data another used validation data limit overﬁtting mlp. general observe deep shifting able achieve results comparable normal cnns tests ‘easy’ example classiﬁcation errors many hidden units used. classiﬁcation errors large number hidden units small compared data size deep shifting generally reach performance regular cnn. theoretical empirical analysis showed deep shifting requires less convolution operations computation time regular convolutional network number input time-frames exceeds threshold. common practical applications found threshold around roughly time-frames dimensional inputs windows size deep shifting relevant time-sequences need continuously evaluated number timeframes considered larger size convolution window. latter always case multiple layers used. analysis consider graphical processing units sure speed copy operations deep shifting compare largescale parallel operations gpu’s capable although wonder used practical userend implementations. also considered training network deep shifting architecture. tests show regular neural network layout preferred hard training tasks. general training deep shifting architecture applications since networks often trained readily stored datasets limiting need on-the-ﬂy evaluation. still deep shifting easily implementable trick optimize deep neural networks working timeevolving data speed power consumption relevant. beneﬁt mobile devices interpret sound video sensor data. bohte eﬃcient spike-coding multiplicative adaptation spike response model advances neural information processing systems palm prediction candidate learning deep verstraeten schrauwen stroobandt isolated word recognition using liquid state machine proceedings european symposium artiﬁcial neural networks lichman machine learning repository waleed kadous sammut classiﬁcation multivariate time series structured data using constructive induction machine learning", "year": 2016}