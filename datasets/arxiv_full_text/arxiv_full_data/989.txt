{"title": "Provable approximation properties for deep neural networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We discuss approximation of functions using deep neural nets. Given a function $f$ on a $d$-dimensional manifold $\\Gamma \\subset \\mathbb{R}^m$, we construct a sparsely-connected depth-4 neural network and bound its error in approximating $f$. The size of the network depends on dimension and curvature of the manifold $\\Gamma$, the complexity of $f$, in terms of its wavelet description, and only weakly on the ambient dimension $m$. Essentially, our network computes wavelet functions, which are computed from Rectified Linear Units (ReLU)", "text": "bound error approximating size network depends dimension curvature manifold complexity terms wavelet description weakly ambient dimension essentially network computes wavelet functions computed rectiﬁed linear units last decade deep learning algorithms achieved unprecedented success state-of-theart results various machine learning artiﬁcial intelligence tasks notably image recognition speech recognition text analysis natural language processing deep neural networks general sense mechanism learning features data. nevertheless numerous cases results obtained dnns outperformed previous state-of-the-art methods often requiring signiﬁcant domain knowledge manifested handcrafted features. despite great success dnns many practical applications theoretical framework dnns still lacking; along decades-old well-known results developing aspects theoretical framework focus much recent academic attention. particular interesting topics speciﬁcation network topology given target function order obtain certain approximation properties estimating amount training data needed order generalize test data high accuracy also development training algorithms performance guarantees. many practical applications. moreover specify exact topology network show depends curvature complexity dimensions lastly classes functions also provide approximation error rates error rate functions sparse wavelet expansion point-wise error rate functions structure manuscript follows section review fundamental theoretical results neural network analysis well recent theoretical developments. section give quick technical review mathematical methods results used construction. section describe main result namely construction deep neural nets approximating functions smooth manifolds. section specify size network needed learn function view construction previous section. section concludes manuscript. planes denoted variants stand function approximated. scaling wavelet functions respectively. wavelet terms indexed scale oﬀset support function denoted supp. huge body theoretical work neural network research. section review classical theoretical results neural network theory discuss several recent theoretical works. well known result proved independently cybenko hornik others states artiﬁcial neural networks single hidden layer sigmoidal functions approximate arbitrary closely compactly supported continuous function. result known universal approximation property. relate however number hidden units approximation accuracy; moreover hidden layer might contain large number units. several works propose extensions universal approximation property regularization perspective also using radial basis activation functions activation functions achieve universal approximation property). proportional note requirement gets restrictive ambient dimension large constant might scale dependence improved particular constant improved polynomial times diﬀerentiable functions mahskar constructs network single hidden layer sigmoid units achieves approximation error rate decade popular direction neural network research construct neural networks hidden units compute wavelets functions works however give speciﬁcation network architecture obtain desired approximation properties. several interesting recent theoretical results consider representation properties neural nets. eldan shamir construct radial function eﬃciently expressible -layer requiring exponentially many units represented accurately shallower nets. montufar show dnns represent complex functions represent shallow network number units complexity deﬁned number linear regions function. tishby zaslavsky propose evaluate representations obtained deep networks information bottleneck principle trade-oﬀ compression input representation predictive ability output function however provide theoretical results. recent work chui mhaskar brought attention constructs network similar functionality network construct manuscript. network layers data local coordinates manifold upper ones approximate target function chart however using b-splines. function hilbert space inner product norm dictionary i.e. family functions unit norm. assume represented linear combination elements absolutely summable coeﬃcients denote absolute values coeﬃcients expansion section show rectiﬁed linear units used obtain wavelet frame construction wavelets rectiﬁers fairly simple refer results section show obtain frame remark construction computed using network rectiﬁer units ﬁrst layer single unit second layer. hence every wavelet term computed using rectiﬁer units ﬁrst layer rectiﬁer units second layer single linear unit third layer. this wavelet terms computed using network rectiﬁers ﬁrst layer rectiﬁers second layer single linear unit third layer. bounds approximation compactly supported functions lemma compactly supported twice diﬀerentiable bounded. every exists combination terms scale every representing function manifold functions compact d-dimensional manifold {}cγ atlas obtained covering section extension unity i.e. family compactly supported functions {ηi}cγ intuitively would like extend wavelet terms remain constant manifold manifold again. lemma therefore suﬃces extend orthogonal directions constant base height deﬁnition gives constant height distance linear decay vanishes distance construction obtain following lemma finally order construction work input network ﬁrst mapped rmcγ linear transformation blocks coordinates gives local coordinates ﬁrst coordinates orthogonal subspace remaining coordinates. maps essentially orthogonal projections ˜φi. theorem d-dimensional manifold atlas size section approximated using -layer rectiﬁer rectiﬁer units third layer single linear unit fourth layer number wavelet terms used approximating i’th chart. proof. section construct functions equation which lemma ﬁrst layer network consist linear units compute last paragraph section i.e. linearly transform input blocks dimension block ﬁrst coordinates respect tangent hyperplane second layer rectiﬁers third layer single unit fourth layer. remark every chart wavelet terms scales shifts extended using rectiﬁers second layer. remark observe dependence dimension ambient space ﬁrst second layers depends curvature manifold. number wavelet terms i’th chart aﬀects number units second layer dimension manifold sizes third fourth layers depend all. finally assuming regularity conditions allows bound number wavelet terms needed approximation ˆfi. particular consider speciﬁc cases bounded second derivative. corollary theorem approximated combination ˆfini wavelet terms remark unit count theorem corollaries overly pessimistic sense assume sets wavelet terms expansion intersect chart indices. tighter bound obtained allow wavelet functions shared across diﬀerent charts case term replaced total number distinct wavelet terms used charts hence decreasing constant particular corollary using terms k’th scale chart. case constant remark linear units ﬁrst layer simulated using relu units large positive biases adjusting biases units second layer. hence ﬁrst layer contain relu units instead linear units. construction presented manuscript divided main parts analytical topological. analytical part constructed wavelet frame wavelets computed rectiﬁed linear units. topological part given training data d-dimensional manifold constructed atlas represented function functions deﬁned charts. used rectiﬁer units extend wavelet approximation functions ambient space construction allows state size depth neural given function approximated manifold show speciﬁed size depends complexity function curvature manifold particular take advantage fact possibly much smaller construct network size depends strongly addition also obtain squared error rate approximation functions sparse wavelet expansion point-wise error rate twice diﬀerentiable functions. network architecture corresponding weights presented manuscript handmade achieves approximation properties stated above. however reasonable assume network unlikely result standard training process. hence importance results presented manuscript describing theoretical approximation capability neural nets describing trained nets used practice. several extensions work considered. first eﬃcient wavelet representation obtained chart allows wavelets non-isotropic necessarily axis aligned rather correspond level sets function approximated. function relatively constant certain directions wavelet terms stretched directions. thing done using curvelets. second conjecture representation obtained output convolutional pooling layers data concentrates near collection dimensional manifolds embedded high dimensional space starting point current manuscript. think result application ﬁlters data points. assuming conjecture true apply construction output convolutional layers obtain network topology similar standard convolutional networks namely fully connected layers convolutional ones. make arguments applicable cases data initial representation concentrate near dimensional manifold hidden representation does. finally remark choice using rectiﬁer units construct wavelet frame convenient however somewhat arbitrary. similar wavelet frames constructed function used construct bump functions i.e. functions localized fast decay. example general sigmoid functions authors thank stefan steinerberger lederman help andrew barron bosch mark tygert yann lecun comments. alexander cloninger supported award dms-.", "year": 2015}