{"title": "Provable Self-Representation Based Outlier Detection in a Union of  Subspaces", "tag": ["cs.CV", "stat.ML"], "abstract": "Many computer vision tasks involve processing large amounts of data contaminated by outliers, which need to be detected and rejected. While outlier detection methods based on robust statistics have existed for decades, only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces. This paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph. By exploiting the property that data points can be expressed as sparse linear combinations of each other, we obtain an asymmetric affinity matrix among data points, which we use to construct a weighted directed graph. By defining a suitable Markov Chain from this graph, we establish a connection between inliers/outliers and essential/inessential states of the Markov chain, which allows us to detect outliers by using random walks. We provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions. Experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods.", "text": "based randomly selecting subset points ﬁtting subspace them counting number points well subspace; process repeated sufﬁciently many trials best chosen. ransac intrinsically combinatorial number trials needed good estimate subspace grows exponentially subspace dimension. consequently methods choice robustly learn subspaces penalizing unsquared distances points closest subspace penalty robust outliers reduces contributions large residuals arising outliers. however optimization problem usually nonconvex good initialization extremely important ﬁnding optimal solution. cand`es using convex optimization techniques solve problem robustness corrupted entries many recent methods robustness outliers example outlier pursuit uses nuclear norm seek low-rank solutions solving problem minl +λl∗ prominent advantage convex optimization techniques guaranteed correctly identify outliers certain conditions. recently several nonconvex outlier detection methods also developed guaranteed correctness nonetheless methods typically model unique inlier subspace e.g. rank matrix outlier pursuit therefore candeal multiple inlier subspaces since union multiple subspaces could high-dimensional. another class methods theoretical guarantees correctness utilizes fact outliers expected similarities data points. multiway similarity introduced deﬁned polar curvature advantage exploiting subspace structure. however number combinations multi-way similarity prohibitively large. recent works explored using inner products data points outlier detection although computationally efﬁcient methods require inliers well distributed densely sampled within subspaces. many computer vision tasks involve processing large amounts data contaminated outliers need detected rejected. outlier detection methods based robust statistics existed decades recently methods based sparse low-rank representation developed along guarantees correct outlier detection inliers lowdimensional subspaces. paper proposes outlier detection method combines tools sparse representation random walks graph. exploiting property data points expressed sparse linear combinations other obtain asymmetric afﬁnity matrix among data points construct weighted directed graph. deﬁning suitable markov chain graph establish connection inliers/outliers essential/inessential states markov chain allows detect outliers using random walks. provide theoretical analysis justiﬁes correctness method geometric connectivity assumptions. experimental results image databases demonstrate superiority respect stateof-the-art sparse low-rank outlier detection methods. many applications computer vision including motion estimation segmentation face recognition high-dimensional datasets well approximated union low-dimensional subspaces. applications motivated research problems learning subspaces data a.k.a. subspace learning subspace clustering respectively. practice datasets often contaminated points subspaces i.e. outliers. situations often essential detect reject outliers subsequent processing/analysis performed. prior work. address problem outlier detection setting inlier data assumed close union unknown low-dimensional subspaces traditional method solving problem ransac figure illustration self-representation matrix presence outliers. ﬁrst columns data matrix correspond images individual different illuminations extended yale database next images randomly chosen individuals; three examples category shown near also show typical representation vector inlier outlier image complete representation matrix white black denote notice inliers inliers representation outliers inliers outliers representations. return outliers since random walk reaches inlier cannot return outliers. therefore design random walk process identify outliers whose probabilities tend zero. work makes following contributions respect state data self-representation allows method handle multiple inlier subspaces. knowledge number subspaces dimensions required subspaces nontrivial intersection. method explore contextual information using random walk i.e. outlierness particular point depends outlierness neighbors. analysis shows method correctly identiﬁes outliers suitable assumptions data distribution connectivity representation graph. outlier detection self-representation. prior work explored using data self-representation tool outlier detection union subspaces. speciﬁcally motivated observation outliers sparse representations declare point outlier threshold. however -thresholding strategy robust outliers close since representation vectors small -norms. solves low-rank self-representation matrix lieu sparse representation penalizes unsquared self-representation errors overview method contributions. work address problem outlier detection using data self-representation. proposed approach builds self-expressiveness property data union lowdimensional subspaces originally introduced states point subspace always expressed linear combination points subspace. particular columns multiple subspaces exists vector nonzero entries correspond points subspace subspace dimensions small taken sparse computed solving minimization problem undirected graph constructed vertex corresponds data point vertices corresponding connected either nonzero. graph used segment data respective subspaces applying spectral clustering graph’s laplacian. consider case contains outliers subspaces. figure illustrates example representation matrix computed data drawn single subspace plus outliers case representation inliers express linear combinations inliers outliers express linear combinations inliers outliers. motivated observation directed graph model data relations directed edge indicates uses representation random walk representation graph initialized outlier construct data self-representation matrix denoted brieﬂy discussed introduction self-representation matrix computed observed different properties inliers outliers. speciﬁcally inliers usually inliers self-representation i.e. inlier representation also inlier entry property expected hold inliers union dimensional subspaces evidenced works intuitive explanation inliers dimensional subspace inlier sparse representation using points subspace. thus representation found using sparsity-inducing regularization seen contrast outliers generally randomly distributed ambient space selfrepresentation usually contains inliers outliers. since representation computed sparse potentially connectivity issues representation graph i.e. inlier well-connected inliers detected outlier outlier well connected detected inlier. address connectivity issue compute data selfrepresentation matrix elastic problem s.t. controls balance sparseness regularization) connectivity regularization). speciﬁcally chosen close still expect computed representation inlier inliers. regularization introduced promote connections data points i.e. expects nonzero entries detailed discussion representation computed connectivity issue provided section representation graph random walk directed graph call representation graph capture behavior inliers outliers representation matrix vertices correspond data points edges given adjacency matrix irn×n absolute value taken elementwise i.e. weight edge given |rji|. representation graph expect vertices corresponding inliers edges lead inliers vertices outliers edges lead inliers outliers. words expect edges lead inlier outlier. makes robust outliers. however requires subspaces independent union subspaces low-dimensional outlier detection maximum consensus. diverse range contexts maximum consensus robust linear regression people studied problems form appears many applications particular used learn linear hyperplane data corrupted outliers. detect outliers general low-dimensional subspace apply recursively basis orthogonal complement subspace however approach limited because inlier subspace dimension subspace must known advance. outlier detection random walk. perhaps well-known random walk based algorithm pagerank originally introduced determine authority website pages graphs pagerank variants used different contexts ranking centrality vertices graph. particular propose outrank ranks outlierness points dataset applying pagerank undirected graph weight edge cosine similarity similarity connected data points. then points centrality regarded outliers. outliers returned outrank similarity data points. therefore outrank work points subspace dense enough. section present data self-representation based outlier detection method. ﬁrst describe data self-representation associated properties inliers outliers. design random walk algorithm representation graph whose limiting behavior allows identify sets inliers outliers. data self-representation algorithm outlier detection representation graph input data iterations threshold solve using compute using initialize compute ¯π/t output indicator outliers outlier figure illustration random walks representation graph. green balls represent inliers balls represent outliers arrows represent edges among nodes. notice edge going inliers outliers. random walk starting point inlier points. bottom plot corresponding entry thresholding probability distribution correctly distinguish outliers inliers. recall motivation method ideally edge going inlier outlier representation graph. motivates assume random walk starting inlier eventually return itself i.e. inliers essential states markov chain outliers chance never coming back itself i.e. outliers inessential states. formally work markov chain state space {··· state corresponds data transition probability given given accessible denoted exists entry positive. intuitively random walk move ﬁnitely many steps. markov chain transition probability given time next time given aij. deﬁnition starting point random walk inlier never escape inliers edge going inlier outlier. contrast random walk starting outlier likely inlier state since enters inlier never return outlier state. thus using different data points initialize random walks outliers identiﬁed observing ﬁnal probability distribution state random walks irn×n transition matrix entries propose perform outlier detection using random walks representation graph initial probability distribution compute t-step transition interpreted initializing random walk data points ﬁnding probability distributions random walks steps. expected random walks—starting either inlier outlier—will eventually high probabilities inlier states probabilities outlier states. average ﬁrst t-step probability distributions sequence beneﬁt always converges limit whenever latter exists. next section give detailed discussion choice properties outlier detection convergence behavior. includes outliers inliers subspaces. condition requires former larger latter margin close zero close overall condition requires points dense around outliers inliers subspaces close ¯δj. even holds representation subspace-preserving cannot automatically establish equivalence inliers/outliers essential/inessential states potential complications related graph’s connectivity. addressed next. context sparse subspace clustering wellknown connectivity issue refers problem points subspace wellconnected representation graph cause oversegmentation true clusters. thus make assumption true cluster connected guarantee correct clustering. outlier detection problem happen inlier inessential thus classiﬁed outlier inliers well-connected; similarly outlier essential thus classiﬁed inlier connected least inlier. fact situation even involved since representation graph directed inliers outliers behave differently. suppose ﬁrst example exists inlier never used express inliers. equivalent saying edge going point inliers. note subspace-preserving property still hold inlier expresses using inliers. since random walk leaving point would never return identiﬁed inlier. avoid cases need following assumption. assumption requires good connectivity points inlier subspace. also need good connectivity outliers inliers. consider example subset outliers outgoing edges lead points within subset. case subset points detected outliers since representation pattern inliers. next assumption rules case. section establish inliers connect themselves i.e. subspace-preserving representation satisﬁes certain connectivity conditions inliers essential states markov chain outliers inessential states. subsequently section show ces`aro mean identiﬁes essential inessential states thus establishing correctness algorithm outlier detection. subspace-preserving representation ﬁrst establish inliers express inliers union dimensional subspaces. property well-studied subspace clustering literature. borrow terminologies results prior work modify current task outlier detection. deﬁnition inlier representation called subspace-preserving nonzero entries correspond points i.e. representation matrix irn×n called subspace-preserving subspace-preserving every inlier representation matrix subspace-preserving inlier uses points subspace representation. given subspace-preserving representation obtained solving certain geometric conditions hold. following result modiﬁed assumes columns normalized unit -norm. theorem inlier. deﬁne oracle point matrix containing points except outline proof given appendix. note oracle point lies deﬁnition depends points ﬁrst term condition captures distribution points near expected large neighborhood well-covered points second term characterizes similarity oracle point data points note random walk ces`aro mean adopted different popular random walk restart adopted pagerank example. beneﬁt pagerank random walk converges unique stationary distribution. however clear whether stationary distribution identiﬁes outliers. fact states random walk pagerank essential outliers converge zero probabilities. contrast random walk method necessarily unique stationary distribution ces`aro mean converge stationary distributions shown used identify outliers. detailed discussion appendix. several image databases evaluate outlier detection method computing representation solver parameter tuned dataset. particular solution nonzero number iterations experimental setup databases. construct outlier detection tasks three publicly available databases. extended yale dataset contains frontal face images individuals different illumination conditions. face images size downsample caltech- database contains images categories images each. also additional clutter category database contains images different varieties used outliers. coil- dataset contains images different objects. object images taken pose intervals degrees images size extended yale coil- datasets pixel intensity feature representation. images caltech- represented -dimensional feature vector extracted last fully connected layer -layer network baselines. compare representative methods designed detecting outliers multiple subspaces outlierpursuit reaper dpcp -thresholding also compare graph based method outrank implement inexact solving optimization outlierpursuit. code available online https//sites.google.com/site/ guangcanliu/. dpcp code provided authors. methods implemented according description respective papers. evaluation metric. outlier detection method generates numerical value data point indicates table results extended yale database. inliers taken images either three randomly chosen subjects outliers randomly chosen subjects r-graph deﬁnition outlierness threshold value required determining inliers outliers. receiver operating characteristic curve plots true positive rate false positive rate threshold values. area under curve metric performance terms roc. always perfect model model guesses randomly approximately second metric provide f-score harmonic mean precision recall. f-score dependent upon threshold report largest fscore across thresholds. f-score means exists threshold gives precision recall equal i.e. perfect separation inliers outliers. suppose given images individuals data also corrupted face images variety individuals. task detect remove outlying face images. known images face different lighting conditions approximately dimensional subspace. thus task modeled problem outlier detection subspace union subspaces. ﬁrst experiment randomly choose single individual subjects images subject inliers. choose images remaining subjects outliers image subject. overall data outliers. average measures trials reported table fair comparison ﬁne-tuned parameters methods. comparing state art. representation graph based method r-graph outperforms methods. besides method reaper outlier pursuit dpcp algorithms perform well. three methods learn single subspace treat subspace outliers thus making well suited data -thresholding methods data selfrepresentation also case method. however give good outlier detection results probably algorithm solving model guaranteed converge global optimum. thresholding also give good results showing magnitude representation vector robust measure classifying outliers. considering connection patterns representation graph method achieves signiﬁcantly better results. performance outrank signiﬁcantly worse methods. poor performance explained coherence-based distance fails capture similarity data points data subspaces. example argued coherence faces illumination condition higher images face different illumination conditions. dealing multiple inlier groups. order test ability methods deal multiple inlier groups designed second experiment inliers taken images randomly chosen subjects outliers randomly drawn subjects before. methods parameters previous experiment test robustness parameter tuning. results experiment reported table outlier pursuit r-graph best methods. although outlier pursuit models single dimensional subspace still deal data since union three subspaces corresponding three subjects inlier still dimensional treated single dimensional subspace. however postulate outlier pursuit eventually fail increase number inlier groups since union dimensional subspaces longer rank. method limitation. similar outlier pursuit reaper dpcp principle handle multiple inlier groups ﬁtting single subspace union. however reaper dpcp require input dimension union inlier subspaces hard estimate practice. indeed table observe performances reaper table results caltech- database. inliers taken images three randomly chosen categories outliers randomly chosen category -clutter. r-graph deﬁnition table results coil- database. inliers taken images four seven randomly chosen categories outliers randomly chosen categories r-graph deﬁnition test ability methods identify several object categories frequently appear images amidst outliers consist objects rarely occur. caltech- images randomly chosen categories used inliers three different experiments. category ﬁrst images category images. randomly pick certain number images clutter category outliers outliers experiment. coil- randomly pick categories inliers pick image remaining categories outliers. results reported table table r-graph method achieves best performance. geometric distance based methods outrank achieve good results inlier category deteriorate number inlier categories increases. performance reaper outlier pursuit dpcp similar worse method. linear subspace data data databases better modeled nonlinear manifold. -thresholding presented outlier detection method combined data self-representation random walks representation graph. unlike many prior methods robust method able deal multiple subspaces require number subspaces dimensions known. analysis showed method guaranteed identify outliers certain geometric conditions satisﬁed connectivity assumptions hold. experiments face image object image databases method achieves state-of-the-art performance. another commonly used geometric quantity characterizing representations subspace-preserving inradius sets points order understand relationship results found works present corollary theorem appendix organized follows. section discuss subspace-preserving representations give outline proof theorem section contains relevant background markov chain theory used section proving lemma lemma well providing in-depth discussion ces`aro mean used outlier detection. section provide additional results experiments extended yale database provide additional insight behavior methods. idea subspace-preserving representation extensively studied literature subspace clustering guarantee correctness clustering concretely data subspace clustering task assumed union dimensional subspaces without outliers outside subspaces. data self-representation matrix called subspace-preserving point uses points subspace representation. theoretical results subspace clustering adapted study subspace-preserving representations presence outliers. here analysis result studied elastic representation subspace clustering prove subspace-preserving representation result presence outliers i.e. theorem also present corollary theorem allows compare result subspace clustering results. proof theorem follows mostly work provide outline proof completeness. solution problem statement theorem notice entries correspond columns data matrix subsequently construct representation vector padding additional zeros entries corresponding points note vector trivially subspace-preserving construction. idea proof show constructed vector subspace-preserving construction solution optimization problem sufﬁcient condition hold computed needs correlation points precisely following lemma. inradius captures distribution columns i.e. large points well spread thus condition easier satisﬁed points dense well covers entire subspace. note requirement stronger theorem requires points dense around oracle point =jxk∈s ¯δj| large). fact established maxk=jxk∈s ¯δj| condition stronger requirement theorem present background material markov chain theory help understand ces`aro mean used outlier detection method. following material organized textbooks website http//www.math.uah.edu/stat. consider markov chain ﬁnite state space transition probabilities t-step transition probabilities deﬁned p{xt decomposition state space p{xt probability chain starting enters ﬁrst time t-th step. hitting probability p{xt probability random walk ever makes transition state started i.e. since shown equivalence relation induces partition state space disjoint equivalence classes known communicating classes. interested closed communicating classes. deﬁnition non-empty called closed theorem state space markov chain composed essential states inessential states essential states decomposed union communicating classes. therefore probability first show inlier point corresponds essential state markov chain. point since representation matrix subspace-preserving know subspace. furthermore assumption points subspace strongly connected implies thus essential state. second show outlier point corresponds inessential state markov chain. consider i.e. points accessible assumption cannot contain outliers. thus exists inlier. however since representation subspacepreserving know therefore essential state i.e. inessential state. according theorem state space markov chain decomposed ∪···∪en contains inessential states closed communicating class containing essential states. assume withloss generality transition probability matrix form using ces`aro mean following limiting behavior number states class fi→e vector hitting probabilities state class positive vector stationary distributions states therefore zero inessential state. ﬁnishes proof. ces`aro mean outlier detection. stationary distributions. vector converges stationary distribution markov chain fact convex combination stationary distribution closed communicating class stationary distribution markov chain particular stationary distribution converges depends choice initial state distribution -step probability distribution pagerank. traditionally pagerank many spectral ranking algorithms limit -step probability distribution rather adopted method. however note ces`aro mean converges tstep transition probability necessarily converge. consider example probability transition matrix oscillating never converges. general converges closed communicating classes aperiodic. outlier detection method representation graph guaranteed correctly identify outliers union subspaces representation subspace-preserving connectivity assumptions satisﬁed. section ﬁrst prove inliers outliers data correspond essential inessential states respectively markov chain associated representation graph then show average ﬁrst t-step probability distributions identiﬁes essential inessential states thus establishing correctness method. sequence converges closed communicating class markov chain aperiodic necessarily satisﬁed many cases. address this pagerank adopts random walk restart algorithm. interpreted random walk transformed markov chain adds small probability transition state states transition probability original markov chain. transformed markov chain contains single communicating class aperiodic. therefore stationary distribution necessarily becomes unique sequence transformed markov chain converges unique stationary distribution regardless initial state distribution. despite advantages random walk used pagerank states markov chain essential outliers converge zero probabilities. therefore less clear whether stationary distribution algorithm converges effectively identify outliers. table reports average running time experiment extended yale database three inlier groups outliers table observe running times outrank much smaller methods. comes fact outrank based computing data pairwise inner products efﬁcient small scale data. contrast methods solve optimization problems. particular reaper outlierpursuit require computing eigendecomposition matrix size iteration time consuming large. experiments observe reaper converges much faster outlierpursuit thus running time reaper typically much smaller. thresholding method r-graph method compute representation matrix solving optimization problem data points data points dictionary. subsequently -thresholding rejects outliers simply computing norms representations r-graph requires random walk graph deﬁned representation. note random walk r-graph computationally efﬁcient because sparsity representation matrix. step random walk computational complexity order number data points average number nonzero entries ﬁrst step method compute data selfrepresentation matrix using optimization problem section illustrate effect parameter performance method. recall numerical experiments solution nonzero experiments extended yale database inlier groups outliers varying range results shown figure r-graph performs well wide range parameter comparison figure also plots performance methods dataset. experiment number inlier groups vary percentage outliers performances different methods reported figure note parameters methods ﬁxed across different percentages outliers. performance method stable respect percentage outliers. moreover method also achieves best performance among methods. supplement measures previously provided also better understand outliers returned outlier detection method conducted additional experiments display outliers detected experiment. inliers taken images ﬁrst subject extended yale database outlier chosen images randomly chosen remaining subjects outliers returned different methods reported figure images boxes outliers images green boxes inliers false positives methods mostly images taken extreme illumination conditions. images large shadows effect removing underlying subspace associated individual thus making likely detected outliers. results show reaper outlier pursuit dpcp rgraph relatively robust. particular r-graph signiﬁcantly better -thresholding even though sparse representation based methods. shows magnitude representation vector adopted", "year": 2017}