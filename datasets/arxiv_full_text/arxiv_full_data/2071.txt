{"title": "Making Early Predictions of the Accuracy of Machine Learning  Applications", "tag": ["cs.LG", "cs.AI", "stat.ML", "I.2.6; I.5.2"], "abstract": "The accuracy of machine learning systems is a widely studied research topic. Established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set. However, they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy. In this paper we investigate techniques for making such early predictions. We note that when a machine learning algorithm is presented with a training set the classifier produced, and hence its error, will depend on the characteristics of the algorithm, on training set's size, and also on its specific composition. In particular we hypothesise that if a number of classifiers are produced, and their observed error is decomposed into bias and variance terms, then although these components may behave differently, their behaviour may be predictable.  We test our hypothesis by building models that, given a measurement taken from the classifier created from a limited number of samples, predict the values that would be measured from the classifier produced when the full data set is presented. We create separate models for bias, variance and total error. Our models are built from the results of applying ten different machine learning algorithms to a range of data sets, and tested with \"unseen\" algorithms and datasets. We analyse the results for various numbers of initial training samples, and total dataset sizes. Results show that our predictions are very highly correlated with the values observed after undertaking the extra training. Finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained, and show how we can accurately estimate an upper bound on the accuracy achievable after further training.", "text": "accuracy machine learning systems widely studied research topic. established techniques cross-validation predict accuracy unseen data classiﬁer produced applying given learning method given training data set. however predict whether incurring cost obtaining data undergoing training lead higher accuracy. paper investigate techniques making early predictions. note machine learning algorithm presented training classiﬁer produced hence error depend characteristics algorithm training set’s size also speciﬁc composition. particular hypothesise number classiﬁers produced observed error decomposed bias variance terms although components behave diﬀerently behaviour predictable. test hypothesis building models that given measurement taken classiﬁer created limited number samples predict values would measured classiﬁer produced full data presented. create separate models bias variance total error. models built results applying diﬀerent machine learning algorithms range data sets tested unseen algorithms datasets. analyse results various numbers initial training samples total dataset sizes. results show predictions highly correlated values observed undertaking extra training. finally consider complex case ensemble heterogeneous classiﬁers trained show accurately estimate upper bound accuracy achievable training. predicting accuracy trained machine learning system presented previously unseen test data widely studied research topic. techniques cross-validation well established understood theoretically empirically e.g. however techniques predict accuracy unseen data given existing training set. example -fold cross validation averages ﬁtness estimated runs using proportion available data train classiﬁer evaluate therefore repeating diﬀerent values give user indication error rate changed training increased current size since lower values eﬀectively equate smaller training sets. however predict accuracy might achievable training. thus current accuracy acceptable obtaining data comes cost similar techniques oﬀer insights whether worth incurring cost training. theoretical interest successful application machine learning techniques real-world problems places various demands collaborators. must management industrial commercial partner must suﬃciently convinced potential beneﬁts prepared invest money equipment time vitally must signiﬁcant investment time commitment end-users order provide training data system learn. poses problem system developed suﬃciently accurate users management view input wasted eﬀort lose faith process. cases eﬀort re-usable example user labelling training examples stored original form come fairly stationary distribution. however frequently case. example many applications practical store physical training examples rather necessary characterise number variables. failure machine learning system cases stems inappropriate inadequate choice descriptors whole process must repeated. user’s input costly waste time eﬀort loss faith process manifest reduced attention consistency classifying samples. give concrete example ﬁeld diagnostic visual inspection frequently turns suﬃcient store relevant image information necessary process variables patients’ history. data captured time recoverable post-hoc eﬀort collecting labelling database examples wasted. signiﬁcant factor would help gaining conﬁdence trust end-users would ability quickly accurately predict whether learning process going successful. perhaps importantly commercial viewpoint would extremely valuable early warning users save eﬀort system designer reﬁnes choice data algorithms etc. paper investigate technique making early predictions future error rates. consider given samples system still learning reﬁning model stage. interested predicting ﬁnal accuracy might achievable users invest time create samples. leads focus questions. first appropriate descriptors system’s behaviour limited number samples later additional samples? second possible useful relationships predicting second quantities ﬁrst? theoretical studies backed empirical results suggested total error rate follows power-law relationship diminishing extra training samples provided. theoretic bounds error rather loose provide motivation investigating practical approaches quickly reliably estimating error rate observed future training. general error complicated function hypothesis paper deal easily decompose number stable functions. therefore paper concentrates well-known bias-variance decomposition source predictors algorithm used build classiﬁcation model dataset. speciﬁcally hypothesis observed error decomposed bias variance terms although components behave diﬀerently behaviour individually predictable. test hypothesis ﬁrst apply range algorithms variety datasets combination periodically estimating error components training samples introduced full dataset used. data arising process merged regression analysis techniques applied produce three sets predictive models bias variance total error. models takes input measurement obtained classiﬁer produced samples dataset presented learning algorithm predicts value samples applied data merged intention models algorithm-dataset independent. examine stability valid range models evaluate predictive power using previously unseen datasets algorithms. moving consider trainable ensembles diﬀerent classiﬁers show similar approach applied obtain estimates upper bound achievable rest paper proceeds follows. section review related work ﬁeld including bias-variance decomposition error use. following that section describes experimental methodology used collect initial statistics test resulting models. sections describe discuss results obtained. section show approach extended predict future accuracy trainable ensembles classiﬁers. finally section draw conclusions suggestions work. machine learning algorithm presented creates classiﬁer view hypothesis underlying mapping subscripts make explicit speciﬁc classiﬁer induced depends learning algorithm training set. speciﬁc learning algorithm classiﬁers induce denoted consider misclassiﬁcation error words error zero correctly predicts true class item otherwise. formally misclassiﬁcation cost single data item speciﬁc classiﬁer practice course possible exactly measure true error approaches bootstrapping hold-out cross-validation used estimate error given ﬁnite sized examples. lower case error denote estimation used true error. cortes presented empirical study characterised behaviour classiﬁcation algorithms using learning curves. suggest predicted error classiﬁer samples presented follow power-law distribution constants depend particular combination classiﬁcation algorithm data usually close less one. suggests given particular classiﬁer-dataset combination possible commence training take periodic estimates error increased regression values data used predicted future error rates. mukerhjee pointed problem approach namely values estimated error rates subject high variability leads signiﬁcant deviations ﬁtting power-law curve. presented extension method uses signiﬁcance permutation test establish signiﬁcance observed classiﬁer error prior curve ﬁtting. results theoretical bounds probably approximately correct theory presented vapnik begin assumption training drawn independently identically distributed data future training test data drawn dataset way. given restriction test error error deﬁned eﬀectively equation makes explicit assumption machine learning algorithms inherently produce classiﬁers overﬁt available training data. vc-dimension thought power machine learning algorithm maximum number points arranged shatter them. equation makes clear powerful algorithms likely overﬁt data used example grounds select algorithms produce training error diﬀerent complexity also makes explicit dependency given training error maximum amount underestimate true error decreases however practice bounds tend rather loose. recent developments statistical learning theory similar approach exploit rademacher complexity provide tighter bounds common approachesas vc-dimension results idea basis available training data algorithm selects classiﬁer class available analyse learning outcomes error observed training data classiﬁed broken bayes optimal error plus amount best current class classiﬁers would bayes optimal plus amount classiﬁer currently estimated algorithm best diﬀerent actual best thus example approaches structural risk minimisation thought principled methods increasing size/complexity current class classiﬁers includes bayes optimal classiﬁer. underlying assumption error estimated using current training almost certainly overﬁts true underlying distribution current estimates error chosen classiﬁer less true error would seen applied whole data distribution. therefore bounds derived describe extent error training underestimates true error. since described terms search problem identifying understandable take account amount information available search algorithm i.e. size training set. valid worthwhile line theoretical research would argue currently useful practioner. consider example user highly skilled domain knows nothing machine learning providing training examples classiﬁer constructed. theory eﬀectively says based told i’ve built classiﬁer seems error rate tell probability true error rate worse positive provided enough labelled data items create appears accurate classiﬁer valuable. however still early process current error rates high gives clues whether drop. instead attempt provide heuristics answer diﬀerent question based told i’ve built classiﬁers although current error rate probably drop speciﬁc estimated error given training size discussed variance predicted error depends strongly prompted examine diﬀerent formulations explicitly decompose error terms arising inherent bias algorithm variability arising choice number recent studies shown decomposition classiﬁer’s error bias variance terms provide considerable insight prediction performance classiﬁer originally proposed regression later decomposition successfully adapted classiﬁcation single deﬁnition bias variance adopted regression considerable debate deﬁnition extended classiﬁcation paper kohavi wolpert’s deﬁnition bias variance basis widely used deﬁnition strictly non-negative variance terms. terms make explicit terms conditional probability distributions since bayes error non-zero classiﬁcation output crisp speciﬁc choice training depends underlying function number samples. alternative perspective analysis bias term reﬂects inherent limit classiﬁer’s accuracy resulting forms decision boundaries. example elliptical class boundary never exactly replicated classiﬁer divides space using axis-parallel decisions. number studies made conﬁrming intuitive idea size variance term drops number training samples increases whereas estimated bias remains stable e.g. therefore treat inherent noise bias terms upper limit achievable accuracy given classiﬁer. noting many prior works assumed inherent noise term zero single classiﬁer possible distinguish inherent noise bias hereafter adopt convention referring collectively bias. hypothesis main part paper values bias variance components estimated training samples used provide accurate predictions values samples hence ﬁnal error rate observed. prediction statistical models built range dataset-algorithm combinations. create model data repeatedly draw training test sets samples estimate total error together bias variance components. raises issue repeated process. variables continuous unbounded integers underlying distribution classiﬁer generalise course inﬁnite. bounded integer categorical variables number potential training sets size drawn underlying distribution size |x|/n practice even non-trivial datasets possible evaluate possible training sets size however success approach proposed paper depends accuracy predict error components particularly training sizes low. immediately raises question ﬁnding appropriate methodology estimating values quantities. give simple example important later result paper partially relies able distinguish data items always going misclassiﬁed given classiﬁer sometimes misclassiﬁed depending choice training set. since well known -fold cross-validation approach classiﬁes data item once permit type decomposition cannot used. preliminary paper examined possible approaches hold-out method proposed kohavi wolpert sub-samples cross validation method proposed webb conilione latter argued hold-out approach proposed fundamentally ﬂawed partly results small training sets leading instability estimates derives. conﬁrmed results showed stability estimates hence accuracy resulting prediction higher sub-sampling method. therefore restrict approach. sscv procedure designed address weaknesses holdbootstrap procedures providing greater degree variability between training sets. essence procedure repeats -fold times thus ensuring sample training size classiﬁed times classiﬁer true biasix arianceix estimated biasixn varianceixn resulting classiﬁcations. ﬁnal bias variance estimated average thus using samples. following sections describe choice experimental methodology algorithms data sets. please note distinction datasets algorithms used provide data statistical models built used evaluation purposes. order obtain data modelling diﬀerent classiﬁcation algorithms selected diﬀerent bias variance characteristics. were naive bayes nearest neighbour bagging adaboost random forest decision table bayes network support vector learning ripple-down rule learner note includes methods creating ensembles adaboost bagging cases since solely interested outputs treat ensemble single entity rather attempt bias-variance-noise-covariance decomposition. evaluation investigate well models extrapolate classiﬁers used. five classiﬁers diﬀerent bias-variance tradeoﬀs used analysis namely cart randomsubspace data collection required build statistical models carried data sets derived four artiﬁcial real-world visual surface inspection problems dynavis project. artiﬁcial problem consists contrast images created tuneable randomised image generator. class labels assigned images using diﬀerent sets rules increasing complexity acting generator. real world data sets came cd-imprint inspection problems. images labelled diﬀerent operators labelled images inspection problem. image processing routines applied segment measure regions interest image. images derived data sets. ﬁrst features describing global characteristics image contains. second augmented maximum value descriptors. adding labels available provides total diﬀerent data sets range dimensionality cardinality. build models used data sets derived ﬁrst three artiﬁcial image sets images labeled ﬁrst three operators data. remaining four data sets derived fourth artiﬁcial image labelled operator reserved evaluation purposes three example datasets selected repository dataset used sscv estimate values error bias variance using samples dataset. note results diﬀerent values diﬀerent datasets. note also separate test set. consider since always making estimates error unseen data consistent relate estimates error diﬀerent points training using estimation methodology. models form a·vn+b {total error bias variance}. models independent variable dependent variable constants estimated linear regression procedure. compute coeﬃcient determination measure well simple linear model explains variability independent variable hence quality resulting predictions closer better prediction. figure shows process single value would like re-iterate sake clarity building models relate error bias variance function number training samples case would certainly true models could combined single linear model welath theoretical work described shows ample evidence sugest simple predictive lnear model exists. instead building combining linear models form future bias/variance/error function current bias/variance/error seeing predictive power models changes result value exist ﬁelds statistics also machine learning. however results show suﬃcient purposes. obvious candidate future work consider approaches give conﬁdence intervals predicted error better concept providing upper bound achievable accuracy. figures show scatter plots values error bias variance measured samples samples. diﬀerent markers indicate diﬀerent total numbers samples. note case range used axes correspondence would diagonal bottom-left top-right plot. case show results linear regression conﬁdence intervals. thus values classiﬁer-dataset pair estimated samples constitutes single point marked plot. combination vertical distance actual point mean regression line shows diﬀerence between value measured samples available value predicted basis samples models built samples data well plots scattered coeﬃcient determination words linear regression shown would account observed variation values ﬁnal variables comparing estimates variance ﬁnal values former much higher. makes apparent small size data sets leading considerable noise introduces error modelling process. bias plot markers sized datasets would fall fairly evenly either side diagonal. thus noise bias plot seem particularly function dataset size. figure scatter plots error bias variance estimated samples descriptors estimated using samples together results linear regression. absolute values individual plots vary case axes scale range figure scatter plots error bias variance estimated samples descriptors estimated using samples together results linear regression. absolute values individual plots vary case axes scale range whatever form variance takes function taylor expansion would give similar values observed whereas larger datasets variance clearly falls away. however distribution actual values diﬀerent datasets size wide overlaps diﬀerent possible single regression line capture diﬀerences. last observation perhaps least expected arguments taylor expansion variance hold true even might diﬀerence distribution variance markers diﬀerent sized datasets even extreme. fact explained hypothesis variance follows inverse power-law suggested intuitively elements variability caused presence absence training samples particular regions data space probability elements present averaged eﬀect inﬂuence fall non-linearly increases. however major point emphasized even using simple model linear regression observed quantities take account future trying predict models capture characteristics observed data closely. results figure thus form strong evidence conﬁrm original hypothesis behaviours bias variance although diﬀerent predictable. show predictive quality models changes built increasing numbers samples figure shows coeﬃcients determination computed regression process function value bias variance total error estimated using sscv before regression models built relating ﬁnal observed values. data point samples obtain predicted ﬁnal error ways either directly using error-error models calculating summing predicted ﬁnal bias ﬁnal variance. clear separate models bias variance provides better estimates predicted error. plot also shows rapidly estimates stabilise cases. correlation actual error predicted error without bias/variance decompostion correlation actual error predicted error using bias/variance decompostion correlation actual bias predicted bias correlation actual variance predicted variance turn attention examining well statistical models built group dataset-algorithm combinations able predict behavior using previously unseen datasets algorithms both. perform regression analysis predicted observed values ﬁnal error function number samples used make predictions comparing eﬀects using single model error decomposed models. figure shows observed errors predicted values withbias-variance decomposition using diﬀerent classiﬁers described section seven previously unseen datasets. ﬁrst four image processing datasets artiﬁcial cd-operator respectively sizes feature space. investigate well models extrapolate initial observed accuracies outside range values used build models datasets come diﬀerent problems described diﬀerent numbers features also used three well known data sets repository satimage segment seen close alignment predicted observed values method correctly indicates cases accuracies low. good example providing early warning mechanism remedial action might needed. general predictions made decomposition accurate made directly using error-error regression models. speciﬁcally predictions bias highly accurate tendency variance terms overestimated leading overestimation combined error. noticeable errors prediction occur dataset features total data items. case noticeable eﬀect variance term underestimated classiﬁers. could variance term known decrease echoed model value dataset lower data used build model. however eﬀect overestimating variance apparent datasets likely explanation dataset shows higher errors used create initial regression models suggests weakness extrapolation. remains future research examine whether adding additional data model building process would create better linear regression models cause need complex models. quantify accuracy predictions given dataset pool results classiﬁers perform regression analysis calculate coeﬃcient determination observed predicted values error bias variance. figure shows progress small number samples noise curves monotonically non-decreasing. however common patterns seen case four values rise fairly steadily cases reach values exception variance satimage consistently low. notably larger datasets correlations observed errors diﬀerent classiﬁers predicted decomposition fact every case except correlation consistently higher decomposed error predictions direct predictions. conclude using samples make predictions classiﬁer accuracy attained using full training figure comparison error observed n+n′ samples with predicted samples using decomposed direct prediction. observed error decomposed predictions stacking within bars shows bias variance components. evaluate predictive performance treated classiﬁerdataset combination potential item pairs measurements bias variance total error. since principle huge number possible datasets used reasonable measurements considered samples underlying normal distribution therefore appropriate paired samples t-tests. conﬁrm conﬁdence level deviations variance directly predictedobserved error signiﬁcant. contrast less probability deviations bias error predicted decomposition signiﬁcant. next investigate well models extrapolate algorithms used build classiﬁers i.e. ones used training. five algorithms used analysis namely cart randomsubspace logistic complement naive bayes implemented weka library default parameters weka used each. figure compares ﬁnal observed error predictions made without bias-variance decomposition algorithms building classiﬁers seven unseen datasets. seen before almost every case separate model bias-variance provides better estimates error simple error model without decomposition. before exception cmc. nevertheless cases predicted error near actual error note particular complement naive bayes satimage segment results. hypothesise relatively poor predictive accuracy dataset arises high bias components inaccurate extrapolation regression model original data high values. nevertheless worth pointing decomposed approach correctly predicts ﬁnal rank order classiﬁers. figure shows coeﬃcient determination predicted observed error function number initial samples. again small number observations value cause noise correlation high stable artif-f artiff satimage segment data sets rises artif-f cd-f sets variable variance models perform well. figure comparison error observed samples predicted samples using decomposed direct prediction. observed error decomposed predictions stacking within bars shows bias variance components. analysing data relative diﬀerences /observation calculated plotted variables error bias variance. figure shows plots values size extra data colours markers distinguish data sets classiﬁers. note logarithmic x-axis diﬀerent scales y-axes somewhat exaggerate deviations. visually appears slight trend towards overestimating bias increases value causes lesser corresponding trend behaviour error predicted decomposition. apparent pattern variance. however inﬂuence trends borne statistical analysis performing linear regression showed near zero correlation variable. concept decomposing error diﬀerent terms also used help explain behaviour ensembles algorithms. algorithms concerned performing regression tasks decomposing error ensemble terms representing mean bias variance individual algorithms covariance fairly straightforward. good recent survey bias-variance-covariance ambiguity decompositions found ﬁrst pages however deﬁning bias variance loss functions non-trivial several versions kohavi wolpert created formulation variance always non-negative extension handle covariance natural also problematic. best knowledge successful model decomposing loss functions ensembles classiﬁers immediately possible simply extend approach took single classiﬁers. however section present initial ﬁndings approach treat entire ensemble single classiﬁer. revisiting deﬁnitions bias section next develop predictors upper limits attainable accuracy based simple observations behaviour individual classiﬁers ensemble. analysis section used general model predicated fact data items could drawn large potentially inﬁnite universe samples corresponding unlimited future classiﬁers. concerned limited case future estimates still drawn ﬁnite size particular consider whether predict values estimates completing training process. order achieve reformulate models slightly follows. start with assume ﬁnite sample data points. consistency note treating ensemble single high-level entity need worry eﬀects boosting bagging approaches creating ensembles repeatedly sampling training sets. therefore assume higher level training sets size created sampling uniformly without replacement. denote training sets created member note conditions lets look means terms estimates bias classiﬁer. course depend methods used estimates. following well-established previous research assume item data predicted exactly times. true -fold cross-validation webb collione approach general although interestingly kohavi approach means data items counterpart term occurs equal probability. note biasx stated composed terms depend choice training sets assuming ﬁxed data points ﬁxed size training sets. therefore reﬁne deﬁnition bias take account average possible training sets. assume sampling /|x| /|d|. turn attention case data item unambiguously associated possible class labels constrain ensemble output crisp decisions partitioning data above note make following conditions performing summation. first contribute bias since predicted class subset items always correct. second means within partition combination exactly values contribute summation. yields reformulation makes explicit considering proportion samples ensemble always misclassiﬁes yield strict underestimate bias provided exist items prediction made dependent training set. furthermore since according variance term always non-negative quantity |a−|/|x| previous sections illustrated successful regression models built variety dataset-classiﬁer combinations predict error rates could attained future training. however decomposing error diﬀerent components straightforward ensembles classiﬁers moreover would require running -fold cross validation number times accurate estimates bias variance components combination dataset algorithm becomes computationally expensive extended heterogeneous ensemble particularly ensemble trainable. section slightly diﬀerent approach. previously pooled results many experiments build regression models relating observations bias variance diﬀerent values training data variables items. treat data independently built regression models characterise ensemble’s learning curve function noted above theoretical well empirical evidence learning curves power-law dependency number training samples i.e. form experiments bound ensemble’s error derived section |a−|/|x| used estimate minimum achievable error. faced dataset-ensemble combination make observations |a−| ensemble error regular intervals feed power-law regression model order ﬁne-tune parameters model data predicts future development ensemble error detailed section elaborating results section provide analysis stability estimation lower bound error using |a−|/|x|. experiments performed machine vision datasets dynavis project used cart decision trees naive bayes nearest neighbour classiﬁers used base classiﬁers decisions combined using discounted dempster-shafer ensemble training method data classiﬁer value samples -fold cross validation repeated times make predictions class item training set. data calculated values |a−|/|x| function data clarity denote values |a−|/|x| hereafter orn. order examine stability predicted bounds increased plotted inal used linear regression model form inal estimate quality model figure shows progression coeﬃcients corresponding values function seen figure models generated increases produce predictions correlate well observed values training. however seen progression coeﬃcients nature regression models changes. values models predict high constant value inal component related observed value essentially system seen enough diﬃcult samples. since major component predicted value inal ﬁxed correlation fairly low. increases representative sample data seen situation changes. thus training sizes predicted value dominated observed value constant component training sizes increases approximately measured diﬀerent constant regression performed values i.e. obtain constant minimises mean square error values across diﬀerent values value forms estimate lower bound achievable error. results procedure illustrated figure base classiﬁers listed combined using discounted dempster-shafer combination ensemble measured samples. constant regression performed model constant value obtained value used asymptote modelling ensemble errors. errors ensemble makes recorded samples regression model built according equation results procedure illustrated figure cd-operator figure artiﬁcial values errors ensemble shown diﬀerent well regression models built them together estimated standard errors. also ﬁnal error evaluating performance ensemble trained entire data indicated show accurately errors predicted ensemble would trained using larger number training samples first cases results show model expected form lower bound error. seen figure secondary robust regression method predict mean standard deviation observed ensemble error artiﬁcial data extrapolates well ﬁnal observed error falls inside values. much smaller print data ﬁgure less clear estimated standard errors predicted asymptote overlap robust regression prediction. nevertheless observed ﬁnal ensemble error lies within standard deviation value predicted robust regression procedure. paper investigated techniques making early predictions error rate achievable interactions. provided several example scenarios ability would great value practical data mining applications. approach based observations although diﬀerent components error progress diﬀerent ways number training samples increased behaviour displayed component appeared qualitatively similar across diﬀerent combinations dataset classiﬁcation algorithm. investigate ﬁnding created large results many diﬀerent combinations dataset algorithm training size applied statistical techniques examine relationship values observed partial training full training. perhaps surprisingly results showed fact simple linear model provided highly accurately predictor subsequent behaviour diﬀerent components. results conﬁrmed hypotheses could combined produce highly accurate predictions total observed error. ﬁndings validated using range datasets algorithms used during creation statistical models. also examined extent models reliably extrapolate observations values outside ranges data used models. results accurate predictions poor performance adaboost algorithms satimage segment datasets) show bias models extrapolate extremely well. however ﬁnal predictions slightly less accurate nature data high variance component observed e.g. data database. suggests complicated model predicted value depends ﬁnal number samples available necessary variance. bias-variance decomposition available loss functions ensembles classiﬁers straightforward apply methodology used accurately predict performance classiﬁers further training ensembles classiﬁers. shown reformulation bias component provide estimate lower bound achievable error easily computed. especially important cost training high example trainable ensembles classiﬁers. bound used asymptote power-law regression model accurately predict progression ensemble’s error independently data set. figure prediction errors ensemble samples using regression model built using training samples. mean values mean errors ensemble shown together regression models error training entire data depicted large future work focus directions. first combine previous theoretical ﬁndings successful results diﬀerent approaches here. taken together suggest even accurate predictions worth combining linear model bias inverse power model variance using current estimates period predict factors. expected prove particularly useful classiﬁers variance forms major part observed error. second work presented paper used kohavi wolpert’s deﬁnition bias variance investigate whether using deﬁnitions bias variance improve predicted accuracy.", "year": 2012}