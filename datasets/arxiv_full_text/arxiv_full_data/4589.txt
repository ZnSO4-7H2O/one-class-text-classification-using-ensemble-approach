{"title": "The Thing That We Tried Didn't Work Very Well : Deictic Representation  in Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Most reinforcement learning methods operate on propositional representations of the world state. Such representations are often intractably large and generalize poorly. Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods. Yet, there are few experiments on learning with deictic representations reported in the literature. In this paper we explore the effectiveness of two forms of deictic representation and a na\\\"{i}ve propositional representation in a simple blocks-world domain. We find, empirically, that the deictic representations actually worsen learning performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects.", "text": "strategy ning world propositionalize relational vector instance serious potential main. fairly representation including give much basis generalization jects. ered grows exponentially world become complicated. full-propositional propositional possibility word deictic telligence building deictic meaning context holding ples deictic primary tion avoids naturally deictic tations allowing first-order solution algorithms. learning reinforcement propositional representations world state. representations intractably using deictic viable alization reinforcement-learning experiments representations paper explore forms deictic naive propositional blocks-world cally deictic ally worsen learning clude discussion results learning real-world tables learning vector learning likely truly relational done generally specifically reinforcement complex lish whether niques break document reinforcement-learning lational experiments ictic deixis marker. relating green table) agent's hand. marker bound block above right focus. deixis tities focused block. action deictic experiments previous empirical important utility representations evaluate tailored representation. second perceptual task instead tures actions might given arbitrary experiments configurations ndp) baseline since common successful learning method reinforcement feature-based large domains hoped improve performance function lectively u-tree algorithm u-tree settled rithm based algorithm algorithm elements predicting internal series reward. estimating tree initialized state distinctions. neath fringe distinction. fringe agent's distinctions permanently differences fringe statistically parent become official leaves created level description tially tween u-tree requires rience tational uses estimate sition dynamics state split. ting choice data. u-tree uses non-parametric tical test conducted block move side. blocks-world shown figure repeat green block uncovered task cases pick green block. picked agent received sequence task penalty optimal tempted tion sequences. attempted green clear step otherwise. exploration initially trouble harder exploration dence pick-up focus-to-marker means easy agent lose place executing focus. actions focus happened effectiveness progress. compare modified sentation agent stumble mean time-to-goal distractor blocks figure much easier modified actions system scales system. modÂ­ ified action agents learn fast full-propositional agent blocks slightly blocks limited. agent control inherently problem stored implicitly tentially move focus random make agent learn useful blocks-world representation proved exploration implications history common wisdom function like neural input attributes degree function subset representation seems reasonable ictic representation servations ones might relevant uation better reveals given ability terms past actions gorithm frequently characterizations lying world state. scribed looked described green block looked down etc. isolation redundant pose much problem-one seem would algorithm grows graph rather tree allowing underlying state tree severe. tree contains state based historical learn different pending requires value function. \"arms race\" adequately icy. complex complex cycle. basic problem grow enough leaves every policy none approaches inherently relational problem seems like successful naive propositionalization number objects environment; severely redundant ment names objects. deictic seemingly servability reinforcement problem this ability necessary learning observations available. investigating attempted containing agent learn optimal leaves possible. instead amount history possible make domain markov. history erwise given beginning looking drastic tional though might well want deictic expressions naming individual representations real relational learning blocks world. important early work done relational relational propriate able environments.", "year": 2012}