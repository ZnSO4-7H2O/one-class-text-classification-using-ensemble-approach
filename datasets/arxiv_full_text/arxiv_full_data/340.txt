{"title": "On Detecting Adversarial Perturbations", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.", "text": "hendrik metzen genewein volker fischer bastian bischoff bosch center artiﬁcial intelligence robert bosch gmbh robert-bosch-campus renningen germany janhendrik.metzende.bosch.com machine learning deep learning particular advanced tremendously perceptual tasks recent years. however remains vulnerable adversarial perturbations input crafted speciﬁcally fool system quasi-imperceptible human. work propose augment deep neural networks small detector subnetwork trained binary classiﬁcation task distinguishing genuine data data containing adversarial perturbations. method orthogonal prior work addressing adversarial perturbations mostly focused making classiﬁcation network robust. show empirically adversarial perturbations detected surprisingly well even though quasi-imperceptible humans. moreover detectors trained detect speciﬁc adversary generalize similar weaker adversaries. addition propose adversarial attack fools classiﬁer detector novel training procedure detector counteracts attack. last years machine learning particular deep learning methods impressive performance various challenging perceptual tasks image classiﬁcation speech recognition despite advances perceptual systems humans machines still differ signiﬁcantly. szegedy shown small carefully directed perturbations images lead incorrect classiﬁcation high conﬁdence artiﬁcial systems. humans perturbations often visually imperceptible stir doubt correct classiﬁcation. fact called adversarial examples crucially characterized requiring minimal perturbations quasi-imperceptible human observer. computer vision tasks multiple techniques create adversarial examples developed recently. perhaps strikingly adversarial examples shown transfer different network architectures networks trained disjoint subsets data adversarial examples also shown translate real world e.g. adversarial images remain adversarial even printed recaptured cell phone camera. moreover papernot shown potential attacker construct adversarial examples network unknown architecture training auxiliary network similar data exploiting transferability adversarial inputs. vulnerability adversarial inputs problematic even prevent application deep learning methods safetysecurity-critical applications. problem particularly severe human safety involved example case perceptual tasks autonomous driving. methods increase robustness adversarial attacks proposed range augmenting training data applying jpeg compression input distilling hardened network original classiﬁer network however recently published attacks effective counter-measures known yet. paper propose train binary detector network obtains inputs intermediate feature representations classiﬁer discriminate samples original data adversarial examples. able detect adversarial perturbations might help safetysecurity-critical semi-autonomous systems would allow disabling autonomous operation requesting human intervention however might intuitively seem difﬁcult train detector since adversarial inputs generated tiny sometimes visually imperceptible perturbations genuine examples. despite intuition results cifar -class subset imagenet show detector network achieves high accuracy detection adversarial inputs trained successfully. moreover train detector network detect perturbations speciﬁc adversary experiments show detectors generalize similar weaker adversaries. obvious attack approach would develop adversaries take account networks classiﬁcation adversarial detection network. present adversary show harden detector adversary using novel training procedure. since discovery szegedy several methods generate adversarial examples proposed. methods generate adversarial examples optimizing image w.r.t. linearized classiﬁcation cost function classiﬁcation network maximizing probability true class minimizing probability true class method introduced moosavi-dezfooli estimates linearization decision boundaries classes image space iteratively shifts image towards closest linearized boundaries. details methods please refer section several approaches exist increase model’s robustness adversarial attacks. goodfellow propose augment training adversarial examples. training time minimize loss real adversarial examples adversarial examples chosen fool current version model. contrast zheng propose append stability term objective function forces model similar outputs samples training perturbed versions. differs data augmentation since encourages smoothness model output original distorted samples instead minimizing original objective adversarial examples directly. another defense-measure certain adversarial attack methods defensive distillation special form network distillation train network becomes almost completely resistant attacks l-bfgs attack fast gradient sign attack however carlini wagner recently introduced novel method constructing adversarial examples manages break many defense methods including defensive distillation. fact authors previous attacks fragile could easily fail adversarial examples even existed. experiment cross-model adversarial portability shown models higher accuracies tend robust adversarial examples examples fool portable less accurate models. even though existence adversarial examples demonstrated several times many different classiﬁcation tasks question adversarial examples exist ﬁrst place whether sufﬁciently regular detectable studied paper remained open. szegedy speculated data-manifold ﬁlled pockets adversarial inputs occur probability thus almost never observed test set. pockets dense adversarial example found virtually near every test case. authors speculated high non-linearity deep networks might cause existence low-probability pockets. later goodfellow introduced linear explanation given input adversarial noise product weight vector adversarial input xadv given wtxadv wtη. adversarial noise causes neuron’s activation grow wtη. max-norm constraint allow large values dimension thus high-dimensional many small changes dimension accumulate large change neuron’s activation. conclusion linear behavior high-dimensional spaces sufﬁcient cause adversarial examples. tanay grifﬁn challenged linear-explanation hypothesis constructing classes images suffer adversarial examples linear classiﬁer. also point change activation grows linearly dimensionality problem activation wtx. instead linear explanation tanay provide different explanation existence adversarial examples including strict condition non-existence adversarial inputs novel measure strength adversarial examples taxonomy different classes adversarial inputs. main argument learned class boundary lies close data manifold boundary tilted respect manifold adversarial examples found perturbing points data manifold towards classiﬁcation boundary perturbed input crosses boundary. boundary slightly tilted distance required perturbation cross decision-boundary small leading strong adversarial examples visually almost imperceptibly close data. tanay argue situations particularly likely occur along directions variance data thus speculate adversarial examples considered effect over-ﬁtting phenomenon could alleviated proper regularization though completely unclear regularize neural networks accordingly. recently moosavi-dezfooli demonstrated even exist universal imageagnostic perturbations which added data points fool deep nets large fraction imagenet validation images. moreover showed universal perturbations certain extent also transferable different network architectures. observation raises interesting questions geometric properties correlations different parts decision boundary deep nets potential regularities adversarial perturbations also help detecting them. however existence universal perturbations necessarily imply adversarial examples generated data-dependent adversaries regular. actually moosavi-dezfooli show universal perturbations unique even exist many different universal perturbations little common. paper studies data-dependent adversarial perturbations nevertheless detected reliably answers question afﬁrmatively. section introduce adversarial attacks used experiments propose approach detecting adversarial perturbations introduce novel adversary aims fooling classiﬁcation network detector propose training method detector aims counteracting novel adversary. generating adversarial examples input image r×width×height ytrue one-hot encoding true class image jcls) cost function classiﬁer brieﬂy introduce different adversarial attacks used remainder paper. fast method simple approach compute adversarial examples described goodfellow applied perturbation direction image space yields highest increase linearized cost function ∞-norm. achieved performing step direction gradient’s sign step-width here hyper-parameter governing distance adversarial original image. suggested kurakin also refer fast method non-iterative hence fast computation. basic iterative method extension kurakin introduced iterative version fast method applying several times smaller step size clipping pixels iteration ensure results stay ε-neighborhood original image following kurakin refer method basic iterative method i.e. change pixel maximally number iterations addition method based ∞-norm propose analogous method based -norm step method moves direction gradient projects adversarial examples back ε-ball around distance distance exceeds deepfool method moosavi-dezfooli introduced deepfool adversary iteratively perturbs image xadv closest class boundary determined. minimal step according distance xadv traverse class boundary determined resulting point used xadv algorithm stops xadv changes class actual classiﬁer. arbitrary p-norms used within deepfool focus ∞-norm. technical details found would like note variant deepfool presented ﬁrst version paper since found stable compared variant reported ﬁnal version. augment classiﬁcation networks subnetworks branch main network layer produce output padv interpreted probability input adversarial. call subnetwork adversary detection network train classify network inputs regular examples examples generated speciﬁc adversary. this ﬁrst train classiﬁcation networks regular dataset usual subsequently generate adversarial examples data point train using methods discussed section thus obtain balanced binary classiﬁcation dataset twice size original dataset consisting original data corresponding adversarial examples thereupon freeze weights classiﬁcation network train detector minimizes cross-entropy padv labels. details adversary detection subnetwork attached classiﬁcation network speciﬁc datasets classiﬁcation networks. thus evaluation discussion various design choices detector network provided respective section experimental results. worst case adversary might access classiﬁcation network gradient also adversary detector gradient. case adversary might potentially generate inputs network fool classiﬁer fool detector principle achieved replacing cost jcls) jcls) σjdet hyperparameter jdet cost detector generated label i.e. adversarial. adversary maximizing cost would thus letting classiﬁer mis-label input making detectors output padv small possible. parameter allows trading objectives. generating propose following extension basic iterative method would like emphasize stronger assumption granting adversary access original classiﬁer’s predictions gradients since classiﬁer’s predictions need often presented user typically true predictions adversary detector used internally. figure resnet used classiﬁcation. numbers arrows denote number feature maps numbers arrows denote spatial resolutions. conv denotes convolutional layer res∗ denotes sequence residual blocks introduced denotes global-average pooling layer dens fully-connected layer. spatial resolutions decreased strided convolution number feature maps residual’s shortcut increased convolutions. convolutional layers receptive ﬁelds followed batch normalization rectiﬁed linear units. topology detector network attached positions. denotes max-pooling optional second pooling layer skipped pooling layers skipped. counteract dynamic adversaries propose dynamic adversary training method hardening detectors dynamic adversaries. based approach proposed goodfellow instead precomputing dataset adversarial examples compute adversarial examples on-the-ﬂy mini-batch adversary modify data point probability note dynamic adversary modify data point differently every time encounters data point since depends detector’s gradient detector changes time. extend approach dynamic adversaries employing dynamic adversary whose parameter selected uniform randomly generating adversarial data points training. training detector implicitly train resist dynamic adversaries various values principle approach bears risk oscillation unlearning since both detector adversary adapt practice however found approach converge stably without requiring careful tuning hyperparameters. section present results detectability adversarial perturbations cifar dataset static dynamic adversaries. moreover investigate whether adversarial perturbations also detectable higher-resolution images based subset imagenet dataset -layer residual network classiﬁer. structure network shown figure network trained epochs stochastic gradient descent momentum data points train set. momentum term initial learning rate reduced epochs reduced epochs. epoch network’s performance validation data determined. network maximal performance validation data used subsequent experiments network’s accuracy non-adversarial test data attach adversary detection subnetwork resnet. detector convolutional neural network using batch normalization rectiﬁed linear units. experiments investigate different positions detector attached figure illustration detectability different adversaries values cifar. x-axis shows predictive accuracy cifar classiﬁer adversarial examples test data different adversaries. y-axis shows corresponding detectability adversarial examples corresponding chance level. corresponds adversary leaves input unchanged. analysis detectability adversarial examples different adversaries different attachment depths detector. subsection investigate static adversary i.e. adversary access classiﬁcation network detector. detector trained epochs data points train corresponding adversarial examples using adam optimizer learning rate remaining data points cifar train used validation data used model selection. detector attached position except deepfool-based adversaries detector attached discussion. fast iterative adversaries parameter section chosen ∞-based methods -based methods; larger values generally result reduced accuracy classiﬁer increased detectability. iterative method -norm used i.e. iteration make step distance please note values based assuming range color channel input. figure compares detectability different adversaries. general points lower left plot correspond stronger adversaries adversarial examples harder detect time fool classiﬁer images. detecting adversarial examples works surprisingly well given differences perceivable humans shown settings detectability adversaries decrease classiﬁcation accuracy adversaries decrease classiﬁcation accuracy comparing different adversaries fast adversary generally considered weak adversary deepfool based methods relatively strong adversaries iterative method somewhere in-between. moreover methods based -norm generally slightly stronger ∞-norm counter-parts. figure compares detectability different adversaries detectors attached different points classiﬁcation network. chosen minimal constraint classiﬁcation accuracy fast iterative adversaries attachment position works best i.e. attaching middle layer abstract features already extracted still full spatial resolution maintained. deepfool methods general pattern similar except works best adversaries. figure illustrates generalizability trained detectors adversary different choices detector trained large generalize well small direction works reasonably well. figure shows generalizability detectors trained adversary tested data adversaries test shufﬂed detector evaluated dataset. original corresponding adversarial example thus processed independently. figure transferability cifar detector trained adversary maximal distortion tested adversary distortion \u0001test. different plots show different adversaries. numbers correspond accuracy detector unseen test data. figure transferability cifar detector trained adversary tested adversaries. maximal distortion adversary chosen minimally predictive accuracy classiﬁer numbers correspond accuracy detector unseen test data. classiﬁcation accuracy detectors generalize well ∞-norm based variants approach. moreover detectors trained stronger iterative adversary generalize well weaker fast adversary vice versa. detectors trained deepfool-based methods generalize well adversaries; however detectors trained iterative adversaries generalize relatively well deepfool adversaries. section evaluate robustness detector networks dynamic adversaries this evaluate detectability dynamic adversaries optimizer detector network section evaluating detectability dynamic adversaries close need take account adversary might choose solely focus fooling detector trivially achieved leaving input unmodiﬁed. thus ignore adversarial examples cause misclassiﬁcation evaluation detector evaluate detector’s accuracy regular data versus successful adversarial examples. figure shows results dynamic adversary static detector trained detect static adversaries dynamic detector explicitly trained resist dynamic adversaries. seen static detector robust dynamic adversaries since certain values namely detectability close figure illustration detectability versus classiﬁcation accuracy dynamic adversary different values static dynamic detector. parameter chosen smaller values corresponding lower predictive accuracy i.e. left. chance level predictive performance classiﬁer severely reduced less accuracy. dynamic detector considerably robust achieves detectability choice section report results static adversaries subset imagenet consisting data randomly selected classes. motivation section investigate whether adversarial perturbations detected higher-resolution images network architectures residual networks. limit experiment classes order keep computational resources required computing adversarial examples small avoid similar classes would oversimplify task adversary. pretrained classiﬁcation network layer softmax selects relevant class entries logits vector. based preliminary experiments attach detector network fourth max-pooling layer. detector network consists sequence convolutions feature maps using batch-normalization rectiﬁed linear units followed convolution maps onto classes global-average pooling softmax layer. additional max-pooling layer added ﬁrst convolution. note tune speciﬁc details detector network; topologies might perform better results reported below. applicable vary ∞-based methods moreover limit changes deepfool adversaries distance since adversary would otherwise sometimes generate distortions clearly perceptible. train detector epochs using adam optimizer learning rate figure compares detectability different static adversaries. adversaries fail decrease predictive accuracy classiﬁer chance level given values nevertheless detectability percent exception iterative -based adversary adversary detector reaches chance level. choices detector’s attachment depth internal structure hyperparameters optimizer might achieve synsets selected classes palace; joystick; bee; dugong dugong dugon; cardigan; modem; confectionery confectionary candy store; valley vale; persian cat; stone wall. classes selected randomly drawing ilsvrc synset-ids using randint function python-package numpy initializing numpy’s random number generator seed results train images validation images test images. figure illustration detectability different adversaries values -class imagenet. x-axis shows predictive accuracy imagenet classiﬁer adversarial examples test data different adversaries. y-axis shows corresponding detectability adversarial examples corresponding chance level. figure transferability -class imagenet detector trained adversary maximal distortion tested adversary distortion \u0001test. different plots show different adversaries. numbers correspond accuracy detector unseen test data. figure illustrates transferability detector different values results roughly analogous results cifar section detectors trained adversary small value work well adversary larger vice versa. note detector trained iterative -based adversary detect changes adversary accuracy; emphasizes adversary principally undetectable rather optimization detector setting difﬁcult. figure shows transferability adversaries transferring detector works well similar adversaries deepfool adversaries fast iterative adversary based distance. moreover detectors trained deepfool adversaries work well adversaries. summary transferability symmetric typically works best similar adversaries stronger weaker adversary. tiny adversarial perturbations detected well? adopting boundary tilting perspective tanay grifﬁn strong adversarial examples occur situations classiﬁcation boundaries tilted data manifold close nearly parallel data manifold. detector could identify adversarial examples detecting inputs slightly data manifold’s center direction nearby class boundary. thus detector focus detecting inputs move away data manifold certain direction namely directions nearby class boundary ∞-based iterative adversary chosen -based adversary. numbers correspond accuracy detector unseen test data. knowledge class boundaries might learn direction implicitly adversarial training data). however training detector captures directions model small capacity generalizes unseen data requires certain regularities adversarial perturbations. results moosavi-dezfooli suggest exist regularities adversarial perturbations since universal perturbations exist. however perturbations unique data-dependent adversaries might potentially choose among many different possible perturbations non-regular would hard detect. positive results detectability suggest case tested adversaries. thus results somewhat complementary moosavi-dezfooli show universal image-agnostic perturbations exist show image-dependent perturbations sufﬁciently regular detectable. whether detector generalizes different adversaries depends mainly whether adversaries choose among many different possible perturbations consistent way. joint classiﬁer/detector system harder fool? static detector might areas adversarial classiﬁer detector; however subset areas adversarial classiﬁer alone. nevertheless results section show static detector fooled along classiﬁer. however dynamic detector considerably harder fool hand might reduce number areas adversarial classiﬁer detector. hand areas adversarial detector might become increasingly non-regular difﬁcult gradient descent-based adversaries. paper shown empirically adversarial examples detected surprisingly well using detector subnetwork attached main classiﬁcation network. directly allow classifying adversarial examples correctly allows mitigating adversarial attacks machine learning systems resorting fallback solutions e.g. face recognition might request human intervention verifying person’s identity detecting potential adversarial attack. moreover able detect adversarial perturbations future enable better understanding adversarial examples applying network introspection detector network. furthermore gradient propagated back detector used source regularization classiﬁer adversarial examples. leave future work. additional future work developing stronger adversaries harder detect adding effective randomization would make selection adversarial perturbations less regular. finally developing methods training detectors explicitly detect many different kinds attacks reliably time would essential safetysecurity-related applications. would like thank michael herman michael pfeiffer helpful discussions feedback drafts article. moreover would like thank developers theano keras seaborn dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos erich elsen jesse engel linxi christopher fougner tony awni hannun billy patrick legresley libby sharan narang andrew sherjil ozair ryan prenger jonathan raiman sanjeev satheesh david seetapun shubho sengupta wang zhiqian wang chong wang xiao dani yogatama zhan zhenyao zhu. deep speech end-to-end speech recognition english mandarin. international conference machine learning york city sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning lille seyed-mohsen moosavi-dezfooli alhussein fawzi pascal frossard. deepfool simple accurate method fool deep neural networks. computer vision pattern recognition nicolas papernot patrick mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks deep learning systems using adversarial examples. arxiv. february nicolas papernot patrick mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. symposium security privacy jose olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. international conference learning representations stephan zheng yang song thomas leung goodfellow. improving robustness deep neural networks stability training. computer vision pattern recognition cvpr", "year": 2017}