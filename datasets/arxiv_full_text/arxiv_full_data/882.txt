{"title": "Learning Structured Sparsity in Deep Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML", "I.2.6; I.5.1"], "abstract": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn", "text": "high demand computation resources severely hinders deployment large-scale deep neural networks resource constrained devices. work propose structured sparsity learning method regularize structures dnns. learn compact structure bigger reduce computation cost; obtain hardware-friendly structured sparsity efﬁciently accelerate dnn’s evaluation. experimental results show achieves average speedups convolutional layer computation alexnet respectively off-the-shelf libraries. speedups twice speedups non-structured sparsity; regularize structure improve classiﬁcation accuracy. results show cifar- regularization layer depth reduce layers deep residual network layers improve accuracy still slightly higher original resnet layers. alexnet structure regularization also reduces error source code found https//github.com/wenwei/caffe/tree/scnn deep neural networks especially deep convolutional neural networks made remarkable success visual tasks leveraging large-scale networks learning huge volume data. deployment models however computation-intensive memory-intensive. reduce computation cost many studies performed compress scale including sparsity regularization connection pruning rank approximation sparsity regularization connection pruning approaches however often produce non-structured random connectivity thus irregular memory access adversely impacts practical acceleration hardware platforms. figure depicts practical speedup layer alexnet non-structurally sparsiﬁed -norm. compared original model accuracy loss sparsiﬁed model controlled within poor data locality associated scattered weight distribution achieved speedups either limited negative even actual sparsity high deﬁne sparsity ratio zeros paper. recently proposed rank approximation approaches trained ﬁrst trained weight tensor decomposed approximated product smaller factors. finally ﬁne-tuning performed restore model accuracy. rank approximation able achieve practical speedups coordinates model parameters dense matrixes avoids locality problem non-structured sparsity regularization. however rank approximation obtain figure evaluation speedups alexnet platforms sparsity. conv refers convolutional layer forth. baseline proﬁled gemm cublas. sparse matrixes stored format compressed sparse accelerated cusparse. compact structure within layer structures layers ﬁxed ﬁne-tuning costly reiterations decomposing ﬁne-tuning required optimal weight approximation performance speedup accuracy retaining. inspired facts redundancy across ﬁlters channels shapes ﬁlters usually ﬁxed cuboid enabling arbitrary shapes potentially eliminate unnecessary computation imposed ﬁxation; depth network critical classiﬁcation deeper layers cannot always guarantee lower error exploding gradients degradation problem propose structured sparsity learning method directly learn compressed structure deep cnns group lasso regularization training. generic regularization adaptively adjust mutiple structures including structures ﬁlters channels ﬁlter shapes within layer structure depth beyond layers. combines structure regularization locality optimization offering well-regularized models improved accuracy greatly accelerated computation related works connection pruning weight sparsifying. reduced number parameters alexnet vgg- using connection pruning. since reduction achieved fully-connected layers authors obtained layer-wise speedup fully-connected layers. however practical speedups convolutional layers observed issue shown figure convolution computational bottleneck many dnns fewer fully-connected layers e.g. parameters resnet- fully-connected layers compression acceleration convolutional layers become essential. achieved sparsity convolutional layers alexnet accuracy loss bypassed issue shown figure hardcoding sparse weights program achieving layer-wise speedup cpu. work also focus convolutional layers. compared techniques method coordinate sparse weights adjacent memory space achieve higher speedups accuracy. note hardware program optimizations boost system performance level covered work. rank approximation. denil predicted parameters exploiting redundancy across ﬁlters channels. inspired jaderberg achieved speedup cpus scene text character recognition denton achieved speedups cpus gpus ﬁrst layers. works used rank approximation accuracy drop. improved extended larger dnns. however network structure compressed ﬁxed; reiterations decomposing training/ﬁne-tuning cross-validating still needed optimal structure accuracy speed trade-off. number hyper-parameters method increases linearly layer depth search space increases linearly even polynomially deep dnns. comparing contributions dynamically optimize compactness structure hyper-parameter reiterations; besides redundancy within layers also exploits necessity deep layers reduce them; ﬁlters regularized lower rank approximation work together efﬁcient model compression. model structure learning. group lasso efﬁcient regularization learn sparse structures. used group lasso regularize structure correlation tree multi-task regression problem reduced prediction errors. utilized group lasso constrain scale figure proposed structured sparsity learning dnns. weights ﬁlters split multiple groups. group lasso regularization compact obtained removing groups. ﬁgure illustrates ﬁlter-wise channel-wise shape-wise depth-wise structured sparsity explored work. structure lra. adapt structure different databases feng learned appropriate number ﬁlters dnn. different prior arts apply group lasso regularize multiple structures source code found https//github.com/wenwei/caffe/tree/scnn. focus mainly structured sparsity learning convolutional layers regularize structure dnns. ﬁrst propose generic method regularize structures section specify method structures ﬁlters channels ﬁlter shapes depth section variants formulations also discussed computational efﬁciency viewpoint section weight tensor along axes ﬁlter channel spatial height spatial width respectively. denotes number convolutional layers. proposed generic optimization target structured sparsity regularization formulated represents collection weights dnn; loss data; non-structured regularization applying every weight e.g. -norm; structured sparsity regularization layer. group lasso effectively zero weights groups adopt ssl. regularization group lasso weights ||w||g group partial weights total number groups. different groups overlap. group lasso ||w||g learned structure decided splitting groups investigate formulate ﬁler-wise channel-wise shape-wise depth-wise structured sparsity figure simplicity term omitted following formulation expressions. penalizing unimportant ﬁlers channels. suppose nl-th ﬁlter cl-th channel ﬁlters l-th layer. optimization target learning ﬁlter-wise indicated approach tends remove less important ﬁlters channels. note zeroing ﬁlter l-th layer results dummy zero output feature turn makes corresponding channel layer useless. hence combine ﬁlter-wise channel-wise structured sparsity learning simultaneously. learning arbitrary shapes ﬁlers. illustrated figure clmlkl denotes vector corresponding weights located spatial position ﬁlters across cl-th channel. thus deﬁne clmlkl shape ﬁber related learning arbitrary ﬁlter shape homogeneous non-cubic ﬁlter shape learned zeroing shape ﬁbers. optimization target learning shapes ﬁlers becomes regularizing layer depth. also explore depth-wise sparsity regularize depth dnns order improve accuracy reduce computation cost. corresponding optimization target ||g. different discussed sparsiﬁcation techniques zeroing ﬁlters layer message propagation output neurons cannot perform classiﬁcation. inspired structure highway networks deep residual networks propose leverage shortcuts across layers solve issue. illustrated figure even removes entire unimportant layers feature maps still forwarded shortcut. proposed schemes section learn compact computation cost reduction. moreover variants formulations schemes directly learn structures efﬁciently computed. d-ﬁlter-wise sparsity convolution. convolution dnns essentially composition convolutions. perform efﬁcient convolution explored ﬁne-grain variant ﬁlter-wise sparsity namely d-ﬁlter-wise sparsity spatially enforce group lasso ﬁlter nlcl. saved convolution proportional percentage removed ﬁlters. ﬁne-grain version ﬁlter-wise sparsity efﬁciently reduce computation associated convolution group sizes much smaller thus weight updating gradients shaper helps group lasso quickly obtain high ratio zero groups large-scale dnn. combination ﬁlter-wise shape-wise sparsity gemm. convolutional computation dnns commonly converted modality general matrix multiplication lowering weight tensors feature tensors matrices example caffe ﬁlter reshaped weight matrix column collection weights clmlkl related shape-wise sparsity. combining ﬁlter-wise shape-wise sparsity directly reduce dimension weight matrix gemm removing zero rows columns. context row-wise column-wise sparsity interchangeable terminology ﬁlter-wise shape-wise sparsity respectively. evaluated effectiveness using published models three databases mnist cifar- imagenet. without explicit explanation starts network whose weights initialized baseline speedups measured matrix-matrix multiplication caffe single-thread intel xeon experiment mnist examined effectiveness types networks lenet implemented caffe multilayer perceptron network. networks trained without data augmentation. lenet applying lenet constrain network ﬁlter-wise channel-wise sparsity convolutional layers penalize unimportant ﬁlters channels. table summarizes remained ﬁlters channels ﬂoating-point operations practical speedups. table lenet baseline others results applying different strengths structured sparsity regularization. results show method achieves similar error much fewer ﬁlters channels saves signiﬁcant flop computation time. demonstrate impact structures ﬁlters present learned conv ﬁlters figure seen ﬁlters lenet entirely zeroed except important detectors stroke patterns sufﬁcient feature extraction. accuracy lenet drops lenet compared random blurry ﬁlter patterns lenet resulted high freedom parameter space ﬁlters lenet regularized converge smoother natural patterns. explains proposed obtains same-level accuracy much less ﬁlters. smoothness ﬁlters also observed deeper layers. effectiveness shape-wise sparsity lenet summarized table baseline lenet conv ﬁlters regular square lenet reduces dimension constrained rectangle shape conv ﬁlters baseline also regularized shape lenet within channel indicating ﬁlter conv needed. fact signiﬁcantly saves flop computation time. besides convolutional layers proposed extended learn structure fully-connected layers. enforce group lasso regularization input connections neuron. neuron whose input connections zeroed degenerate bias neuron next layer; similarly neuron degenerate removable dummy neuron output connections zeroed out. figure summarizes learned structure flop different networks. results show remove hidden neurons also discover sparsity images. example figure depicts number connections input neuron input neurons zero connections concentrate boundary image. distribution consistent intuition implemented convnet deep residual networks cifar-. regularizing ﬁlters channels ﬁlter shapes results observations networks similar mnist experiment. moreover simultaneously learn ﬁlter-wise shape-wise sparsity reduce dimension weight matrix gemm convnet. also learn depth-wise sparsity resnet regularize depth dnns. convnet network alex krizhevsky baseline implement using caffe. conﬁgurations remain original implementation except added dropout layer ratio fully-connected layer avoid over-ﬁtting. convnet trained without data augmentation. table summarizes results three convnet networks. here row/column sparsity weight matrix deﬁned percentage all-zero rows/columns. figure shows learned conv ﬁlters. table reduce size weight matrix convnet convolutional layer achieve good speedups without accuracy drop. surprisingly without four conv ﬁlters baseline actually all-zeros shown figure demonstrating great potential ﬁlter sparsity. applied half conv ﬁlters convnet zeroed without accuracy drop. hand convnet achieves lower error model even smaller baseline. scenario performs structure regularization dynamically learn better network structure reduce error. resnet investigate necessary depth dnns required -layer deep residual networks proposed baseline. network convolutional layers fully-connected layer. identity shortcuts utilized connect feature maps dimension convolutional layers chosen shortcuts feature maps different dimensions. batch normalization adopted convolution activation. data augmentation training hyper-parameters ﬁnal error baseline depth resnet- regularized depth-wise sparsity. group lasso regularization enforced convolutional layers pair shortcut endpoints excluding ﬁrst convolutional layer convolutional shortcuts. converges layers figure error layer number depth regularization ssl. resnet- original resnet layers. ssl-resnet- depth-regularized resnet layers including last fully-connected layer. indicates convolutional layers output size forth. zero weights removed ﬁnally ﬁne-tuned base learning rate lower baseline. figure plots trend error number layers different strengths depth regularizations. compared original resnet learns resnet layers reaching lower error baseline layers ssl-resnet- resnet- achieve error respectively. result implies work depth regularization improve classiﬁcation accuracy. note efﬁciently learn shallower dnns without accuracy loss reduce computation cost; however mean depth network important. trend figure shows test error generally declines layers preserved. slight error rise ssl-resnet- ssl-resnet- shows suboptimal selection depth group alexnet imagenet show generalization method large scale dnns evaluate using alexnet ilsvrc caffenet replication alexnet mirror changes used experiment. training images rescaled size image randomly cropped scaled image mirrored data augmentation center crop used validation. ﬁnal top- validation error alexnet ﬁrst trained structure regularization; converges zero groups removed obtain structure; ﬁnally network ﬁne-tuned without regain accuracy. ﬁrst studied d-ﬁlter-wise shape-wise sparsity exploring trade-offs computation complexity classiﬁcation accuracy. figure shows d-ﬁlter sparsity saved flop convolutions validation error. figure deeper layers generally higher sparsity group size shrinks figure d-ﬁlter-wise sparsity flop reduction top- error. vertical dash line shows error original alexnet; reconstruction error weight tensor dimensionality. principal component analysis utilized perform dimensionality reduction exploit ﬁlter redundancy. eigenvectors corresponding largest eigenvalues selected basis lower-dimensional space. dash lines denote results baselines solid lines indicate ones alexnet table speedups -norm various platforms alexnet alexnet table used testbenches. number ﬁlters grows. d-ﬁlter sparsity regularization reduce total flop without accuracy loss reduce error alexnet retaining original number parameters. shape-wise sparsity also obtains similar results table example alexnet achieves average layer-wise speedup without accuracy loss shape regularization; top- error also reduced parameters retained. figure obtained lowest error sparsity indicating number parameters still important maintain learning capacity. case works regularization restriction smoothness model order avoid over-ﬁtting. figure compares results dimensionality reduction weight tensors baseline ssl-regularized alexnet. results show smoothness restriction enforces parameter searching lower-dimensional space enables lower rank approximation dnns. therefore work together rank approximation achieve even higher model compression. besides analyses computation efﬁciencies structured sparsity non-structured sparsity compared caffe using standard off-the-shelf libraries i.e. intel math kernel library cuda cublas cusparse gpu. learn alexnet high column-wise row-wise sparsity representative structured sparsity method. -norm selected representative non-structured sparsity method instead connection pruning -norm higher sparsity convolutional layers results alexnet alexnet depicted table speedups achieved measured subroutines gemm nonzero rows columns weight matrix concatenated consecutive memory space. note compared gemm overhead concatenation ignored. measure speedups -norm sparse weight matrices stored format compressed sparse computed sparse-dense matrix multiplication subroutines. table compares obtained sparsity speedups -norm approximately errors e.g. acceptable accuracy loss. fair comparison -norm regularization also ﬁnetuned disconnecting zero-weighted connections accuracy recovered alexnet experiments show dnns require high non-structured sparsity achieve reasonable speedup however always achieve positive speedups. acceptable accuracy loss achieves average layer-wise acceleration respectively. instead -norm achieves average layer-wise acceleration respectively. note accuracy average speedup indeed higher adopts heavy hardware customization overcome negative impact non-structured sparsity. figure shows speedups -norm various platforms including achieve average speedup non-structured sparsity obtain speedup platforms. platforms methods achieve good speedups beneﬁt grows processors become weaker. nonetheless always achieve averagely speedup compared non-structured sparsity. work proposed structured sparsity learning method regularize ﬁlter channel ﬁlter shape depth structures deep neural networks method enforce dynamically learn compact structures without accuracy loss. structured compactness achieves signiﬁcant speedups evaluation off-the-shelf libraries. moreover variant performed structure regularization improve classiﬁcation accuracy state-of-the-art dnns.", "year": 2016}