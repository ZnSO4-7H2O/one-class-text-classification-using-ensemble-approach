{"title": "Language Models for Image Captioning: The Quirks and What Works", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.", "text": "ﬁnal hidden layer object detection input recurrent neural network language model referred multimodal recurrent neural network similar spirit log-bilinear kiros paper study relative merits approaches. using identical state-ofthe-art input rnn-based mebased models able empirically compare strengths weaknesses language modeling components. approach directly generating text mrnn outperforms measured bleu coco dataset recurrent model tends reproduce captions training set. fact simple k-nearest neighbor approach common earlier related work performs similarly mrnn. contrast generates novel captions best captioning images close match training data. deep multimodal similarity model incorporated signiﬁcantly outperforms methods according human judgments. contributions paper follows recent approaches achieved state-of-the-art results image captioning. ﬁrst uses pipelined process candidate words generated convolutional neural network trained images maximum entropy language model used arrange words coherent sentence. second uses penultimate activation layer input recurrent neural network generates caption sequence. paper compare merits different language modeling approaches ﬁrst time using state-ofthe-art input. examine issues different approaches including linguistic irregularities caption repetition data overlap. combining aspects methods achieve record performance previously published results benchmark coco dataset. however gains bleu translate human judgments. recent progress automatic image captioning shown image-conditioned language model effective generating captions. leading approaches explored task. ﬁrst decomposes problem initial step uses convolutional neural network predict words likely present caption; second step maximum entropy language model used generate sentence covers minimum number detected words second approach uses activations mrnn-based approach tends reconstruct previously seen captions; stage approach achieves similar better performance generating relatively novel captions. language models compared trained using output state-of-the-art cnn. used -layer variant vggnet initially trained ilsvrc classiﬁcation task ﬁnetuned microsoft coco data detector conditioned models study effect leveraging explicit detection step objects/attributes images generation examining approach reported previous work novel lstm approach introduced here. trained output words indicating words likely appear caption beam search top-scoring sentence contains subset words. words dynamically adjusted remove words mentioned. refer reader fang full description approach whose -best outputs analyze here. also include output leverages scores deep multimodal similarity model n-best re-ranking. brieﬂy dmsm non-generative neural network model projects image pixels caption text comparable vector space scores similarity. mentioned caption under construction. initialized words predicted threshold words already mentioned sentence history removed produce conditioning words {h}. incorporate information within lstm adding additional input encoded represent remaining visual attributes continuous valued auxiliary feature vector encoded respectively continuous-space representations last word detector learned matrix recurrent histories sigmoid transformation. multimodal recurrent neural network section explore model directly conditioned activations rather word detections. implementation similar captioning models described karpathy fei-fei vinyals donahue joint vision-language referred multimodal recurrent neural network model feed image retrieve -dimensional ﬁnal hidden layer denoted vector hidden layer obtain dimensional representation serves initial hidden state gated recurrent neural network grnn trained jointly produce caption word time conditioned previous word previous recurrent state. decoding perform beam search size emit tokens token produced. -dimensional grnn hidden layer dimensional word embeddings. k-nearest neighbor model donahue karpathy feifei present -nearest neighbor baseline. ﬁrst step replicated results using cosine similarity layer test image training image randomly emit caption similar training image caption reported previous results performance quite poor bleu however explore idea able optimal k-nearest neighbor consensus caption. ﬁrst select nearest training images test image above. denote union training captions caption compute n-gram overlap f-score caption deﬁne consensus caption caption highest mean n-gram overlap captions found better compute average among ci’s similar captions rather hyperparameters obtained grid search validation set. visual example consensus caption given figure intuitively choosing single caption describe many different images similar rather caption describes single image similar believe reasonable approach take retrieval-based method captioning helps ensure incorrect information mentioned. details retrieval-based methods available e.g. microsoft coco dataset work microsoft coco dataset training images validation split validation images testval images. images contain multiple objects signiﬁcant contextual information image comes human. metrics quality generated captions measured automatically using bleu meteor bleu roughly measures fraction n-grams common hypothesis references penalizes short hypotheses brevity penalty term. meteor measures unigram precision recall extending exact word matches include similar words based wordnet synonyms stemmed tokens. also report perplexity studied detectionconditioned lms. pplx many ways natural measure statistical loosely correlated bleu model comparison table summarize generation performance different models. discrete detection based models preﬁxed example generated results show table perhaps surprisingly k-nearest neighbor algorithm achieves higher bleu score models. however demonstrate section generated captions perform signiﬁcantly better nearest neighbor captions terms human quality judgements. addition comparing me-based rnnbased independently explore whether combining models results additive improvement. -best list d-me score hypothesis mrnn. re-rank hypotheses using mert previous work model weights optimized maximize bleu score validation set. extend combination approach d-me model dmsm scores included re-ranking automatic metrics always correlate human judgments also performed human evaluations using procedure fang here human judges presented image system generated caption human generated caption asked caption better. condition judgments obtained images testval set. dme+dmsm outperforms mrnn percentage points better equal human judgment despite systems achieving bleu score. k-nearest neighbor system performs percentage points worse mrnn despite achieving slightly higher bleu score. finally combined model outperform d-me+dmsm terms human judgments despite bleu improvement. although cannot pinpoint exact reason mismatch automated scores human evaluation detailed analysis difference systems performed sections table results comparing produced captions written humans judged humans. percent captions judged better than better equal caption written human. examples common mistakes observe testval shown table d-me system difﬁculty anaphora particularly within phrase shown examples likely fact maintains local context window. contrast mrnn approach tends generate anaphoric relationships correctly. however d-me maintains explicit coverage state vector tracking attributes already emitted. mrnn implicitly maintains full state using recurrent layer sometimes results multiple emission mistakes attribute emitted once. particularly evident coordination present repeated captions models produce large number captions seen training repeated different images test shown table lstm-based model). least potential causes repetition. first systems often produce generic captions close plate food applied many publicly available images. suggest deeper issue training evaluation models warrants discussion future work. second although coco dataset evaluation server encouraged rapid progress image captioning lack diversity data. also note although caption duplication issue systems greater issue mrnn d-me+dmsm. believe reason work image captioning able caption compositionally novel images individual components image seen training entire composition often not. order evaluate results compositionally novel images test images based visual overlap training data. test image compute cosine similarity training image mean value closest images. compute bleu least overlapping dme+dmsm outperforms k-nearest neighbor approach bleu least even though performance whole comparable. additionally d-me+dmsm outperforms mrnn bleu least performs bleu worse most set. evidence dme+dmsm generalizes better novel images mrnn; supported relatively percentage captions generates seen training data still achieving reasonable captioning performance. hypothesize main reasons strong human evaluation results dme+dmsm shown section shown gated conditioned directly activations achieves better bleu performance lstm conditioned discrete activations; similar bleu performance combined dmsm. however dmsm method signiﬁcantly outperforms mrnn terms human quality judgments. hypothesize partially lack novelty captions produced mrnn. fact k-nearest neighbor retrieval algorithm introduced paper performs similarly mrnn terms automatic metrics human judgements. mrnn system alongside dmsm provide additional scores mert reranking n-best produced imageconditioned advance bleu points best previously published results coco dataset. unfortunately improvement bleu translate improved human quality judgments. tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr doll´ar lawrence zitnick. microsoft coco arxiv. common objects context. vicente ordonez girish kulkarni tamara berg. imtext describing images using million captioned photogrphs. proc. annu. conf. neural inform. process. syst. kishore papineni salim roukos todd ward weijing zhu. bleu method automatic proc. assoc. evaluation machine translation. computational linguistics pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural improc. conf. comput. viage caption generator. sion pattern recognition references michael auli michel galley chris quirk gejoint language translaoffrey zweig. tion modeling recurrent neural networks. proc. conf. empirical methods natural language process. pages xinlei chen lawrence zitnick. mind’s recurrent visual representation image caption generation. proc. conf. comput. vision pattern recognition kyunghyun bart merrienboer caglar gulcehre fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. corr. michael denkowski alon lavie. meteor universal language speciﬁc translation evaluation target language. proc. eacl workshop statistical machine translation. jeff donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. arxiv. jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. proc. conf. comput. vision pattern recognition fang saurabh gupta forrest iandola rupesh srivastava deng piotr doll´a margaret mitchell john platt lawrence zitnick geoffrey zweig. captionons visual concepts back. proc. conf. comput. vision pattern recognition farhadi mohsen hejrati mohammad amin sadeghi peter young cyrus rashtchian julia hockenmaier david forsyth. every picture tells story generating sentences images. proc. european conf. comput. vision pages micah hodosh peter young julia hockenmaier. framing image description ranking task data models evaluation metrics. artiﬁcial intell. research pages", "year": 2015}