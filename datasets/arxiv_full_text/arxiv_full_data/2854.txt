{"title": "Model-Based Value Estimation for Efficient Model-Free Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.", "text": "recent model-free reinforcement learning algorithms proposed incorporating learned dynamics models source additional data intention reducing sample complexity. methods hold promise incorporating imagined data coupled notion model uncertainty accelerate learning continuous control tasks. unfortunately rely heuristics limit usage dynamics model. present model-based value expansion controls uncertainty model allowing imagination ﬁxed depth. enabling wider learned dynamics models within model-free reinforcement learning algorithm improve value estimation which turn reduces sample complexity learning. recent progress model-free reinforcement learning demonstrated capacity rich value function approximators master complex tasks. however model-free approaches require access impractically large number training interactions real-world problems. contrast model-based methods quickly arrive near-optimal control learned models fairly restricted dynamics classes settings nonlinear dynamics fundamental issues arise approach complex dynamics demand high-capacity models turn prone overﬁtting precisely low-data regimes needed. model inaccuracy exacerbated difﬁculty longterm dynamics predictions achieve good asymptotic performance poor sample complexity methods exhibit efﬁcient learning struggle complex tasks. paper seek reduce sample complexity supporting complex nonlinear dynamics combining learning techniques disciplined model value estimation. present model-based value expansion hybrid algorithm uses dynamics model simulate short-term horizon q-learning estimate long-term value beyond simulation horizon. improves q-learning providing higher-quality target values training. splitting value estimates near-future component distantfuture component offers model-based value estimate creates decoupled interface value estimation model require differentiable dynamics. scheme trust model informs selection horizon believe model make accurate estimates. horizon interpretable metric applicable many dynamics model classes. prior approaches combine model-based model-free model-based acceleration incorporate data model directly model-free algorithm show lead poor results. alternatively imagination-augmented agents ofﬂoads uncertainty estimation model implicit neural network training process inheriting inefﬁciency model-free methods incorporating model q-value target estimation require model able make forward predictions. contrast stochastic value gradients make differentiability assumptions underlying dynamics usually include non-differentiable phenomena contact interactions using approximate few-step simulation rewarddense environment improved value estimate provides enough signal faster learning actor-critic method experimental results show method outperform fully model-free algorithms prior approaches combining real-world model-based rollouts accelerated learning. remainder paper organized along following contributions deterministic markov decision process characterized possible actions states dynamics deterministic described state transition function bounded reward function work consider continuous state action spaces. deterministic policy action-value function γtrt gives deterministic γ-discounted cumulative reward refer estimate finally recall express value function qπ). denote state-value estimates policy implicitly parameterized indicate subscripts necessary. objective maximize sampled initial state distribution. access off-policy states exploration policy summarized empirical distribution consider proxy instead typical silver describe off-policy actor-critic algorithm. deterministic parameterized policy parameterized critic silver prove tractable form policy gradient ∇θjd exists continuously differentiable respect actions taken policy. result deterministic policy gradient theorem expresses policy gradient expectation on-policy data. continuous case conditions offpolicy policy improvement theorem holds distinct discrete case. emphasize conditions provide proof statement silver inspired discrete case theorem off-policy distribution states critically require dpg’s assumptions continuous differentiability reward dynamics functions. allow model-based value expansion theoretically practically compatible arbitrary forward models including discrete event simulators settings non-differentiable physics. improves value estimates policy assuming approximate dynamical model true reward function improved value estimate used training critic faster task mastery rewarddense environments assume model accurate depth ﬁxed policy evaluate future transitions occur taking actions according future transitions estimate value. deﬁnition using imagined rewards reward obtained model deﬁne hstep model value expansion estimate value given state h-step model value expansion deﬁnition decomposes state-value estimate component γtˆrt tail estimated approach extended stochastic policies dynamics integrating monte carlo method assuming generative model stochastic dynamics policy. since derived actions on-policy model even stochastic case would require importance weights opposed case using traces generated off-policy trajectories. useful settings step horizon sparse even sparse reward settings predicting future state improve critic accuracy. finally applied state-action estimates case used estimating state-action critic original critic respect distribution states. emphasize that even assuming ideal model mere combination guarantee improved estimates. further analysis conducted state-value estimation naturally extended state-action-value estimation. denotes pushforward measure resulting playing times starting states informal presentation demonstrates relation underlying critic assuming model nearly ideal. verify informal reasoning sound presence model error. theorem deﬁne states actions rewards resulting following policy using true dynamics however arbitrary sampling distribution generated exploratory policy inequality rarely holds. particular naive choice results poor performance overall counteracts beneﬁt model-based reward estimates even assuming perfect oracle dynamics model thus trained bellman error distribution mismatch eclipses beneﬁt effect model-based approach evaluating critic imaginary states must wary training critic real distribution believe insight incorporated variety works similar value prediction networks propose solution distribution-mismatch problem observing issue disappears i.e. training distribution ﬁxed point practice given arbitrary off-policy distribution state-action ﬁxed point uniform{··· sample state ˆst|t model accuracy assumption dictates accurately simulate states {ˆst +i}h−t arising playing starting simulated states used construct k-step targets accurately adhering assumptions model setting targets used train entire support instead algorithm model-based value expansion enhance critic target values generic actor-critic method abstracted actor critic. parameterize respectively. assume selected class dynamics models space equipped norm. procedure mve-ac initialize targets initialize replay buffer tired sample real transition empirical distribution transitions observed interacting environment according exploratory policy. learned dynamics model generate ˆrt. since changes joint optimization simulated states discarded immediately batch. take stochastic step minimize ν-based bellman error dynamics neural network network layers neurons ﬁxed learning rate trained predict difference real-vector-valued states similar previous work expect accurate carefully tuned models allow large even weak model shared hyperparameters across tasks ﬂexible class sufﬁces training imagined td-k error. td-k trick enables skirt distribution mismatch problem extent approximate ﬁxed point. td-k trick greatly improves task performance relative training critic alone preceding section presented analysis motivates model-based value expansion approach. section present practical implementation approach high-dimensional continuous deep reinforcement learning. demonstrate apply general actor-critic setting improve target q-values intention achieving faster convergence. implementation relies parameterized actor critic note separate parameterized actor removed feasible compute argmaxa assume actor critic method supplies differentiable actor loss actor critic loss critic. losses functions well transitions sampled distribution instance ddpg gradient actor] approximately ascends deterministic continuous policy improvement theorem ddpg critic loss depends target actor critic relies approximate ﬁxed point construction empirical distribution transitions recall approximation previous section relies current policy imagine steps ahead πθ). thus sampling transitions equivalent sampling point imagined steps future starting state sampled mve-augmented method follows usual actor-critic template critic training uses targets transitions sampled imagine rollouts target actor whose parameters exponentially weighted averages previous iterates parity ddpg uses target actor compute target value estimates. taking recover original actor-critic algorithm. implementation uses multi-layer fully-connected neural networks represent q-function policy. actor critic losses described ddpg importantly imagination buffer save simulated states instead generate simulated states onthe-ﬂy sampling perform stratiﬁed sampling dependent samples time {··· line first figure fully-observable analogues typical environments default environments excerpt certain observed dimensions abscissa requisite reward calculation. done supervised imposition policy invariance certain dimensions remove full observability. shows fully observable half cheetah fully observable swimmer fully observable walker agents rewarded forward motion. detail changes appendix experiments tune baseline ddpg report best performance. exploration parameter-space noise every experiment setting uses adaptive parameterspace noise standard deviation target exploration controlled trials. extension evaluate effect td-k evaluate naive approach using h-step estimates bellman error states sampled equivalent using ﬁrst term line alg. i.e. updating critic parameters without td-k trick model still used simulate depth opportunity learn distribution additional support neglected. first evaluate mve-ddpg improves terms reward performance comparing learning curves original ddpg mveddpg without td-k trick imagination buffer approaches mve-ddpg outperforms alternatives result shows incorporating synthetic samples learned model drastically improve performance model-free greatly reducing number samples required attain good performance. illustrated comparison baseline improvement obtained carefully incorporating synthetic experience short horizon td-k trick. discuss next section speciﬁc design decisions critical good results helps explain lack success learned neural network models observed related methods prior work mve-ddpg improves similar approaches maddpg treatment synthetic data obtained dynamics model. alternative approach adds simulated data back separate imagination buffer effectively modifying alg. mixture simulated data problematic policy changes training data mixed distribution real fake data stale relative terms representing actions would taken implementation ma-ddpg reuse imagined states manner mve-ddpg still outperforms ma-ddpg. suspect factors staleness imaginary states approach delicate interaction using imaginary data overtraining actor. order additional synthetic data approach must take gradient steps imaginary batches. hand since mve-ddpg uses gradient averaged real simulated data choice make additional gradient steps becomes independent consideration dependent stability actor-critic method trained. make sure make accurate models improve critic value estimate improved critic performance results faster training compared ddpg baseline cheetah. also replicate density plot ddpg analyze accuracy directly figure learning curves comparing learned dynamics without td-k trick ddpg cheetah swimmer walker. used found dynamics class inadequate walker reducing walker experiments reduces improvement offer ddpg still exhibits greater robustness poor model note ma-ddpg ma-bddpg approach. bootstrap estimation ma-bddpg reduce model cases possible ma-bddpg would improved performance walker learned dynamics poor compared environments. addition verify td-k trick essential training appropriately. conduct ablation analysis cheetah environment hold parameters constant substitute learned dynamics model true dynamics model making model error zero. mseβ mseh estimates must improve exponentially even without td-k trick. however case. td-k trick increasing yields increasing diminishing returns without adjustment distribution mismatch past certain point increasing hurts performance. dynamics model ideal cases difference critic trained distribution states instead empirical distribution resulting replay buffer. since td-k trick increases support training data trained function class critic need sufﬁcient capacity capture distribution issue experiments. number prior works sought incorporate dynamics models model-free value estimation. observe three general approaches direct incorporation dynamics value gradient imagination additional training data imagination context value estimation. ries applying differentiable dynamics model real data only avoids instability planning overﬁtted dynamics model since model used gradients rather forward predictions. major limitation approach dynamics model used retrieve information already present observed data albeit lower variance actual improvement efﬁciency relatively small. applied horizon length given off-policy data observed transition taken according sampling distribution following probabilistic dynamics model here real data used train dynamics model only trpo algorithm learns imagination rollouts alone. me-trpo limits over-training imagined data using ensemble metric policy performance. words me-trpo bootstraps dynamics model prediction opposed critic ma-bddpg. however dynamics model predictions treated equally regardless imagination depth lack real data policy training become problematic dynamics models fail capture phenomena occurring reality. case me-trpo demonstrates success using bootstrap estimating degree uncertainty models believe notion incorporated future work combined mve. me-trpo orthogonal complementary method. imagination context value estimation similar existing literature address model accuracy. particular unlike aforementioned works focus quantiﬁcation model uncertainty limit model end-to-end approach avoids explicit reasoning model inaccuracy supplies imagined rollouts inputs critic actor networks free interpret imaginary data whatever learn related approach proposed value prediction networks expand encoded state predictions perform backups expanded imagined paths compute improved target estimate tested planning problems discrete spaces. however continuous spaces degree dynamics prediction error unavoidable affect stability sample complexity network must learn deal uncertain rollouts. addition crucially relies ability perform stable backups imagined paths respect current future actions. backups amount optimizing action space maximize value function problematic continuous case. however believe stable model-based value expansion achieved td-k trick along careful optimization make vpn-like approach viable continuous contexts. figure plot true observed cumulative discounted returns predicted critic cheetah training reconstructing fig. dotted black line represents unity. ideal critic concentrates line. verify true dynamics model trains critic improved values relative original ddpg algorithm. runs reduced mini-batch size oracle dynamics expensive compute. approximation). heess consider case off-policy data likely importance weight vanish model-based expansion longer lengths. problem on-policy authors’ account less sample-efﬁcient svg. alternatively dyna-like approaches learned model imagination rollouts providing imagined data model-free value estimation algorithms potentially robust potentially erroneous view task’s dynamics compared planning. number follow-up methods expanded idea. instance model-based acceleration imagination rollouts used additional training data parameterized value network original proposal adds replay buffer imagined data. imagined states used starting point imagination later iterations. reuse violates rule trusting model steps simulation. authors approach work well neural network models settle locally linear ones. even modiﬁcation model deactivated middle training heuristically attain best results. analysis comparisons shed light approach ineffective. approach limit model automatically modelassisted bootstrapped ddpg modiﬁcations mba. ﬁrst imaginary states used starting point future imaginary rollouts second change adds estimate approximate critic uncertainty bootstrap. imagined states used training figure learning curves cheetah environment mve-ddpg ideal oracle dynamical model different horizons model prediction. examine performance without td-k trick. implies model use; original ddpg. first exempliﬁes improving value estimation model marked effect performance dense reward environments offers upper bound improvement result learned-dynamics mve. note mentioned fig. batch size oracle dynamics evaluations reduced computational necessity curves comparable fig. diminishing returns increases observe emphasize model improvement captured even short horizon. second demonstrates value td-k trick small distribution mismatch small still shows performance improvement increases lose monotonic improvements observed finally note high-level similarity n-step return methods. short horizon rollouts improve value estimate frequently used target value computing bellman error. usually n-step return methods on-policy estimate target q-values γtrt trace rewards observed h-step trajectory main difference n-step return methods explicit state prediction dynamics modeling essential enables faster learning off-policy data. recent work path consistency learning relieves on-policy requirements trains soft consistency error compatible offpolicy trajectories believe model-based extension imagined rollouts possible following algorithm design lessons recommends. particular using imagination buffer rollouts augment paths chosen consistency learning result stale data requires model tempered stability algorithm taking many gradient steps without gathering additional data. approach could augment paths imagined branches would followed current policy path consistency value would average branches. words complimentary similar off-policy actor-critic methods still improved value estimation imagined data even though already off-policy. paper introduce model-based value expansion method algorithm incorporating predictive models system dynamics model-free value function estimation. approach provides improved sample complexity range continuous action benchmark tasks analysis illuminates design decisions involved choosing combine model-based predictions model-free value function learning. existing approaches following general dyna-like approach using imagination rollouts improvement model-free value estimates either stale data imagination buffer model imagine past horizons prediction accurate. multiple heuristics proposed reduce model usage combat problems techniques generally involve complex combination uncertainty estimation additional hyperparameters always appropriately restrict model usage reasonable horizon lengths. offers single simple adjustable notion model trust fully utilizes model extent. also demonstrates state dynamics prediction enables on-policy imagination td-k trick starting off-policy data. work justiﬁes exploration model model-free sample complexity reduction. particular estimating uncertainty dynamics model explicitly would enable automatic selection deal sparse reward signals also believe important consider explolillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. international conference learning representations nachum norouzi mohammad kelvin schuurmans dale. bridging value policy advances neural based reinforcement learning. information processing systems plappert matthias houthooft rein dhariwal prafulla sidor szymon chen richard chen asfour tamim abbeel pieter andrychowicz marcin. parameter space noise exploration. nips deep reinforcement learning workshop racani`ere s´ebastien weber th´eophane reichert david buesing lars guez arthur rezende danilo jimenez badia adri`a puigdom`enech vinyals oriol heess nicolas yujia imagination-augmented agents deep reinforcement learning. advances neural information processing systems silver david lever heess nicolas degris thomas wierstra daan riedmiller martin. deterministic policy gradient algorithms. international conference machine learning integrated architectures learning planning reacting based approximating dynamic programming. international conference machine learning morgan kaufmann todorov emanuel erez tassa yuval. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference ieee ration model reﬁnement value estimates. finally admits extensions domains probabilistic dynamics models stochastic policies monte carlo integration imagined rollouts. authors thankful valuable feedback roberto calandra gregory kahn anusha nagabandi richard liaw. research supported part award hshqdc--- cise expeditions award ccf- gifts alibaba amazon services financial capitalone ericsson google huawei intel microsoft scotiabank splunk vmware. references brockman greg cheung vicki pettersson ludwig schneider jonas schulman john tang arxiv preprint zaremba wojciech. openai gym. arxiv. shixiang lillicrap timothy sutskever ilya levine sergey. continuous deep q-learning modelbased acceleration. international conference machine learning heess nicolas wayne gregory silver david lillicrap erez tassa yuval. learning continuous control policies stochastic value gradients. advances neural information processing systems kalweit gabriel boedecker joschka. uncertaintydriven imagination continuous deep reinforcement learning. levine sergey vanhoucke vincent goldberg proceedings annual conference robot learning volume proceedings machine learning research pmlr kurutach thanard clavera ignasi duan tamar aviv abbeel pieter. model-ensemble trust-region policy optimization. international conference learning representations simulate behavior ideal oracle dynamics model needed true environment mujoco simulation dynamics predictions. make experiments tractable implemented cython-based interface physics engine based mujoco-py however able usual interface applying learned dynamics. runs wait timesteps collected ddpg training starts. wait timesteps collected model training begins. perform gradient steps dynamics ddpg networks collected timestep though possible. batch sizes always except training oracle dynamics model reduced computational necessity. experiments learned dynamics seeds. make increased variance runs experiments true dynamics model seeds. table final tuned ddpg parameters environments. refers learning rate. networks hidden layers units each. adaptive parameter-space noise ﬁxed decay refers target decay directions exactly directions positive inner product gradient. deterministic policy admitting transition measure open subsets euclidean space. distribution states off-policy objective theorem deﬁne ascent direction requires local improvement everywhere sufﬁciently small step size. consider following one-dimensional hill-climbing agent attempts climb reward function quadratic bump function. theorem. deﬁne states actions rewards come acting according true environment dynamics constructed starting using learned dynamics imagine h-step rollout. model generalization error depth grow linearly independent assume mseh simplicity presentation analogous result holds critic outperforms model. recall induced policy dynamics denote lipschitz constants note left derivative) −θ−θ−θ. consider pathological equal mass split small suppose current agent local derivative information step left improves situation direction actually worsens performance indeed step direction", "year": 2018}