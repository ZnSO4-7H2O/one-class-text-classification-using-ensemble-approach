{"title": "Pairwise Rotation Hashing for High-dimensional Features", "tag": ["cs.CV", "stat.ML"], "abstract": "Binary Hashing is widely used for effective approximate nearest neighbors search. Even though various binary hashing methods have been proposed, very few methods are feasible for extremely high-dimensional features often used in visual tasks today. We propose a novel highly sparse linear hashing method based on pairwise rotations. The encoding cost of the proposed algorithm is $\\mathrm{O}(n \\log n)$ for n-dimensional features, whereas that of the existing state-of-the-art method is typically $\\mathrm{O}(n^2)$. The proposed method is also remarkably faster in the learning phase. Along with the efficiency, the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art. Pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes. Although these hashing criteria are widely used in previous researches, its analytical behavior is rarely studied. All building blocks of our algorithm are based on the analytical solution, and it thus provides a fairly simple and efficient procedure.", "text": "abstract. binary hashing widely used eﬀective approximate nearest neighbors search. even though various binary hashing methods proposed methods feasible extremely high-dimensional features often used visual tasks today. propose novel highly sparse linear hashing method based pairwise rotations. encoding cost proposed algorithm n-dimensional features whereas existing state-of-the-art method typically proposed method also remarkably faster learning phase. along eﬃciency retrieval accuracy comparable slightly outperforming state-of-the-art. pairwise rotations used method formulated analytical study trade-oﬀ relationship quantization error entropy binary codes. although hashing criteria widely used previous researches analytical behavior rarely studied. building blocks algorithm based analytical solution thus provides fairly simple eﬃcient procedure. approximate nearest neighbors search widely used retrieval scale database increasing rapidly recent times. furthermore achieve accurate retrieval results high-dimensional features fisher vectors vlad used computer vision community. achieve feasible retrieval features highly eﬃcient search methods necessarily needed. vector quantization based methods widely used actively studied ann. high-dimensions product quantization family state-of-the-art methods reduces high-dimensional vector space direct product small subspaces. clustering applied subspace obtain representative vectors although product quantization based methods applicable high-dimensional features still easy obtain good quantizer cases random rotation often needed expensive high-dimensions. ﬂoating-point distance calculation needed retrieval also expensive compared binary-based methods methods transforms real-valued feature vectors binary-valued ones. binary-valued vectors highly favorable large-scale high-dimensional tasks provide high memory eﬃciency fast hamming distance calculation. methods proposed. major approaches categorized vector quantization based methods hyperplane based linear methods nonlinear hashing function methods typical nonlinear method spectral hashing whose hashing functions nonlinear eigenfunctions derived distribution data. family locality sensitive hashing uses nonlinear hashing functions. kernelized approaches also proposed spectral hashing ordinary uses uniform distribution deriving analytical solution precision empirically lower compared state-of-the-art methods nonuniformly distributed data. overcome diﬃculty kernel based approaches proposed. diﬃcult apply high-dimensions. recently proposed method called spherical hashing example non-kernelized nonlinear method. since hashing function hyper-sphere based also needs euclidean distance calculation hashing. computational cost getting large high-dimensional data. recently bilinear hashing method called bpbc feasible high-dimensions proposed knowledge ﬁrst binary hashing method treat dimensions higher. however method folds feature vectors bilinearly rotates folded space. unable treat special orthogonal group still linear high dimensional binary hashing method directly treat paper propose highly eﬃcient linear binary hashing method. method inspired isotropic hashing found natural extension. first study meaning isotropic transformation analytically. develop eﬃcient isotropic hashing algorithm extension using tradeoﬀ relationship isotropy entropy. recently proposed sparse isotopic hashing method produces sparse rotational matrices yield isotropic variances; however learning high-dimensional rotational matrices feasible practice. main contributions algorithm takes encoding cost dimensional features. previously known state-of-the-art method bpbc requires cost. show proposed algorithm accurate bpbc. moreover remarkably faster learning phase. main cost consuming point algorithm calculation variance-covariance matrix. need computational cost learning iteration loop whereas bpbc requires iteration step. therefore practically faster bpbc although total computational cost learning order algorithm bpbc training data size. typical criteria measuring hashing performance quantization error variance entropy. author’s knowledge analytical treatment criteria well studied yet. show analytical result algorithms derived naturally result. analytical calculation mainly based gaussian distribution. mean proposed algorithm applicable gaussian-distributed data. non-gaussian distributions expanded around gaussian assuming gaussian distribution means taking lowest order expansion. discussed below lowest order approximation leads enough hashing accuracy yields extremely eﬃcient algorithm. along clustering methods k-means clustering binary hashing algorithms minimizing quantization error binarized codes original feature vectors. study therefore properties quantization error ﬁrst investigated analytically. result used develop binary hashing algorithm propose. binary hashing algorithms orthogonal transformation means purpose cost minimizing point rotational group also consider rotational group paper. rotational group transformation minimizing equivalent maximizing subject const. since constancy conσ) onto circle solution strains dimensional vector quantization-errorminimizing hashing gaussian distribution. isotropy measure quantization error. entropy binary code calculated. here eigenvalues angle representation used instead variance-covariance matrix twodimensional case treated. representation elements variancecovariance matrix described follows; fig. schematic illustration two-dimensional gaussian distribution. ellipse describes shape distribution. left eigenvalue angle parameterization). eigenvalues variance-covariance matrix. right rotation angle isotropic transformation. θiso θpca given fig. plots quantization error entropy respect angle quantization error minimized entropy also minimized vice versa. means quantization error entropy trade-oﬀ relationship. analysis similar recently proposed paper quantization error product quantization discussed. authors showed bounded determinant variance-covariance matrix proposed algorithm minimizing bound rotational transformation. result indicates trade-oﬀ relationship quantization error entropy gaussian distribution. however since entropy gaussian distribution invariant rotational transformation method determines partition entire space small subspaces. rotations subspaces discussion. contrast analysis consider rotational optimality two-dimensional subspaces investigate entropy binary codes directly. another example authors proposed criteria crossing sparse region balanced buckets. ﬁrst criterion interpreted quantization error minimization second means quantizer entropy maximization. think possible interpret many existing methods trade-oﬀ problem quantization error minimization entropy maximization. binary hashing algorithm based above-discussed theory developed follows. going sparse transformation matrices substantially decrease encoding cost. used reducing number dimension. however transformation matrix dense diﬃcult transformation eﬃcient encoding calculation highly dimensional cases. paper dimension reduction treated. thus assumed number dimensions original feature fig. schematic illustration basic isotropic rotations upper structure transformation matrix. non-zero matrix elements ﬁlled gray color. sorting matrix permutation matrix sort variances descending order. rotation done pair largest smallest variance dimensions. basic rotation continuously applied times dimensions. lower behavior variances continuous multiplication basic isotropic rotation. graphs sorted variances sequential multiplication. rightmost graph initial state. basic isotropic rotation makes variances isotropic pairs. exponentially transformed globally isotropic state continuous application basic rotation. two-dimensional space isotropic transformations. corresponds equation symmetry gaussian distribution enough consider two-dimensional variancecovariance matrix rotation matrix makes variances isotropic rotation pairs. processes denoted permutation matrix rotational matrix ﬁll-ins. transformation call basic isotropic rotationin follows make variances pairwise isotropic. apply transformation sequentially. applying transformation times make variances quadruple isotropic three times makes octuple isotropic finally applying transformation permutation determinant transformation matrix case transformation element however always obtain element applying permutation ﬁnal matrix application means permutation bits aﬀect retrieval results. factorized form highly sparse especially high dimensions. possible standard sparse matrix data structure. memory usage computational cost substantially decreased. factorized sparse transformation makes variances completely isotropic obtained described preceding section. however section trade-oﬀ relationship isotropy entropy maximization revealed. since entropy reduction degrades retrieval accuracy balance isotropy entropy kept. accordingly methods balancing proposed hereafter. ﬁrst simpler increase number ﬁll-ins. second using additional sparse rotation matrices. increases number ﬁll-ins better accuracy ﬁrst cases. tilting ﬁrst method pairwise rotation tilted isotropic angle angle corresponds equation entropy increased tilting since angle entropy-maximizing angle. rotation matrix derived tuning parameter ranging zero control degree balance isotropy entropy tuning since tilting lead completely isotropic variances deﬁnite reason stop applying basic rotations times. however necessary increase number basic rotations practically leads enough accuracy times application. number ﬁll-ins transformation matrix therefore need change. random sparse rotation second method applies additional sparse rotations completely isotropic transformation. additional matrices form basic isotropic rotation precise sense completely isotropic variances obtained dimensional case. dimensions needs inﬁnite number basic isotropic rotation. practice however enough sub-isotropic variances obtained rotational pairs randomly chosen rotation applied pair. procedure called basic rotation. although obvious many times basic rotation applied experiments discussed succeeding section show times rotations attains maximal retrieval accuracy. increasing number ﬁll-ins transformation matrix little. proposed algorithm introduces novel strategy transformation matrix expressed factored form pairwise rotational matrices. constructing rotation variance-covariance matrix used. contrast existing linear binary hashing algorithms objective function directly calculated data data-dependent objective functions capture non-gaussian property distribution data. hand arbitrary probability distribution function expansion series lowest order term given gaussian distribution. expansion called edgeworth expansion viewpoint regarded lowest-order approximation taken algorithm whereas data-dependent methods consider higher order non-gaussian terms. omission higher order terms enables analytical treatments provide simple computationally eﬃcient binarization procedure. despite fact higher order terms disregarded proposed method still achieves considerably high accuracy explained below. experiments -dimensional gaussian data -dimensional sift data high-dimensional vlad data various dimensions used. gaussian data used evaluating theoretical behavior proposed algorithm. sift data used comparing existing methods feasible high dimensions. vlad data used evaluating algorithm comparison state-of-the-art high-dimensional method. settings top- recall performance measure binary hashing. euclidean nearest neighbors original feature space used ground truth. gaussian data data points training query database used. sift data siftm dataset obey original protocol creating vlad data ilsvrc dataset -dimensional -dimensional vlad calculated original sift data. points training points queries randomly picked. rest dataset used database. existing methods compared choose counterpart methods follows sparse random rotation random method corresponding sequence sparse matrices scheme. transformation matrix form method whereas sorting rotation angle pair randomly chosen. number basic rotation well-known methods keeps nearly-state-of-the-art performance wide range data. considered reasonable performance counterpart low-dimensional case. isotropic hashing original method generates orthogonal transformation make variances completely isotropic states variance-covariance matrix. isotropic state diﬀerent retrieval performance diﬀers others terms entropy higher-order cumulants considered counterpart measures quality isotropic transformation. lift-and-projection optimization algorithm proposed hashing hashing name suggests uses linear transformation basis. discussed section basis opposite extreme isotropic basis regard trade-oﬀ relationship quantization error entropy. k-means hashing recently proposed state-of-the-art method. uses k-means vector quantization binary code assignment optimization cluster center. thus kind nonlinear method. selected evaluating binary hashing performance compared nonlinear methods. algorithm parameter ndim/b iteration number deﬁned bilinear projection-based binary codes state-of-the-art high-dimensional hashing method using bilinear transformation. considered baseline method. algorithm parameter ndim/d iteration number. -dimensional random variance-covariance matrix created used generate mean-centered gaussian data. create variance-covariance matrix diagonal matrix random positive eigenvalues distributed log-normally generated. diagonal matrix rotated random rotation. consider diﬀerent eigenvalue distributions. uses log-normal distribution variance uses log-normal distribution variance three fig. shows retrieval results. case sphere-like distribution methods little diﬀerence accuracy shape distribution insusceptible rotational transformation. notable point case sharp distribution completely isotropic obviously inferior cases although isotropic hashing also completely isotropic variances achieves reasonable performance. discussed fig. top- retrieval results -dimensional gaussian data. indicates proposed method m-times basic isotropic rotation n-times basic rotation pcat parameter upper data logvariance value one. lower data log-variance three abbreviated legends plot plot section inﬁnite number isotropic states. lift-andprojection isotropic hashing tends entropically favorable isotropic states. despite fact extremely simple sparse sometimes achieves entropically inferior isotropic states. however inferiority reasonably overcome pcat rspca without loss sparsity. pairwise rotation leads good performance sharp gaussian distribution. important distinguish sequential pairwise almost-pca rotation hashing rotation. obtain exact basis necessary account fig. top- retrieval results siftm data dimension reduction. leftbit case. middle case. right behavior prhs across dimensions. meaning shown figure dimensional case siftm case considered next. fig. retrieval results. common gaussian case completely isotropic leads unfavorable accuracy. pcat rspca attain good performance. especially rspca achieves remarkably better retrieval result compared methods ﬁgure relation dimension reduction although sparse dimension reduction scheme devised study examine eﬀect dimension reduction performance proposed algorithm. existing methods basis tentatively used dimension reduction srr. note keep sparsity transformation. shows results siftm. clear proposed algorithm maintains higher performance dimension reduction compared methods. high-dimensional case high-dimensional case main contribution algorithm examined next. fig. retrieval results -dimensional -dimensional vlad features calculated ilsvrc dataset. pcat achieves state-of-the-art retrieval accuracy. veriﬁed attains high performance dimension. experiment contrary lower-dimensional case rspca inferior pcat. think result improper random pairing rspca possible number pairing since implementation using matlab sparse matrix datatype diﬃcult reasonably evaluate encoding cost comparison methods optimized dense matrix operations. tentative environment evaluation. show comparison number product operations speed improvement ratio bpbc encoding phase improvement ratio calculated naive implementation dense/sparse matrix operation compared theoretical one. number operation also method need operation product operations. bpbc needs almost table. learning time comparison. learns fast case. high dimensional case -dimensional learning time shown store data memory. implementation optimized proposed pairwise rotation hashing linear binary hashing algorithm encoding cost. based two-dimensional analytical study trade-oﬀ relationship quantization error entropy. proposed algorithm also fast learning phase needs computations iteration loop. shows high hashing accuracy retrieval tasks high dimensions. especially achieves state-ofthe-art performance high dimensions still room improvement. study dimension reduction scheme compatible pairwise concept excluded. idea dimension reduction done pairwise fashion droping minor components pairwise issue appropriate pairing method pairwise part. even though rspca demonstrated high performance improved components paired selective ways rather random ways. rspca potential ﬁnding sophisticated pairing scheme favorably balances isotropy entropy. however exhaustive search possible pairing substantially degrades learning speed. non-random eﬃcient pairing scheme needed.", "year": 2015}