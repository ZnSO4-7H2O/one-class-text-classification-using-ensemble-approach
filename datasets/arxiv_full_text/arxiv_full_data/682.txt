{"title": "When Do Differences Matter? On-Line Feature Extraction Through Cognitive  Economy", "tag": ["cs.LG", "cs.AI", "cs.NE", "I.2.6; I.2.4; I.2.8"], "abstract": "For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. The algorithm incorporates an active form of Q-learning, and partitions continuous state-spaces by merging and splitting Voronoi regions. The experiments illustrate a new methodology for testing and comparing representations by means of learning curves. Results from the puck-on-a-hill task demonstrate the algorithm's ability to learn effective representations, superior to those produced by some other, well-known, methods.", "text": "intelligent agent truly autonomous must able adapt representation requirements task interacts world. current approaches on-line feature extraction hoc; contrast paper presents algorithm bases judgments state compatibility state-space abstraction principled criteria derived psychological principle cognitive economy. algorithm incorporates active form q-learning partitions continuous statespaces merging splitting voronoi regions. experiments illustrate methodology testing comparing representations means learning curves. results puck-on-a-hill task demonstrate algorithm’s ability learn eﬀective representations superior produced other well-known methods. representation foundation problem-solving provides vocabulary populates world seek understand control. although sometimes specify good representation particular problems understood general learning problem understand representation learned along behaviors lead success task. also important practical reasons studying autonomous representation learning. example success depend agent’s ability learn eﬀective representation scratch task poorly understood many possible scenarios work complete speciﬁcation state-space advance. even would possible design representation beforehand might cost-eﬀective programmer time especially representation later need updated task environment changes. paper presents approach problem autonomous representation learning applying psychological principle cognitive economy domain reinforcement learning hard problems facing theory cognition ﬁnding principled ways specifying states world same must distinguished. distinguishing every possible state world every state makes learning intractable except small discrete state-spaces; agent cannot learn task representation groups together states world require diﬀerent behaviors. ideally agent learn states must distinguished avoiding irrelevant distinctions prevent generalizing learning states kind thing task. agent learn representation without knowing task beforehand? considered incompatible ignoring diﬀerences leads agent make decisions; states must grouped together. paper presents principled criteria deciding diﬀerences matter ignored. typical approach reinforcement learning represents agent’s knowledge world terms action-value function function gives long-term estimate reward results taking action state following greedy policy thereafter original descriptions q-learning assumed discrete representation action-value function assumed stored table separate values distinct state extend approach large continuous state-spaces store values compactly parameterized function learning problem becomes exercise function approximation agent responds experiences world adding features tuning parameters minimizes meansquared error value predictions sutton barto comment ultimate purpose predictions ﬁnding better policy. best predictions purpose necessarily best minimizing mse. however clear useful alternative goal value prediction might paper presents alternative goal value prediction based insight q-value errors eﬀect agent’s ability perform task. agent learns faster generalize similar states states agree preferred action expectation reward. because similar states diﬀer expected values non-preferred actions grouping states increase overall prediction error—even though diﬀerences impact agent’s performance task. contrast states model characterizes much work function approximation example describe partition representation deﬁning feature state-space region states model encompasses discrete representations partition representations tiled representations perceptrons radial-basis function networks depending deﬁnition {fi}. features state-space regions lead complementary ways looking function approximation process. function rule describing states grouped together. deﬁne state-space groupings terms features equivalently deﬁne features terms groupings. features {fi} continuous-valued corresponding state groupings {si} fuzzy sets. think state groupings determining feature detectors think generalized states determine action values stored—just individual states discrete representation. duality features state-space grouping important grouping states concept deciding q-value diﬀerences matter agent’s task. consider function approximation without discerning role played state abstraction becomes diﬃcult determine approach representation continue subdivide state-space resolution adequate distinguish states kind situation. ideally representation make ﬁner distinctions parts space diﬀerences matter simplify representation areas. words representation resolution varies throughout state-space according demands task. function approximation methods simply cluster task inputs cannot provide kind representation blind respect task requirements. although sometimes specify important areas state-space particular tasks hard general way. example assumed states closest initial state required ﬁnest resolution; assumed states closest paths taken agent state-space important; assumed frequently-seen areas state-space important. criteria eﬀective representations tasks studied readily imagine tasks criteria irrelevant. need representational criteria explain why—and when—these strategies identify state-space diﬀerences relevant agent’s task. deﬁne important diﬀerences terms general criteria representational adequacy. cognitive economy generally refers combined simplicity relevance categorization scheme representation. natural intelligences appear adopt categorizations high cognitive economy order make sense stimuli impinging senses without overloading bounded cognitive resources. heading cognitive economy eleanor rosch writes commonsense notion function categorization provide maximum information least cognitive eﬀort conserving ﬁnite resources much possible writes purpose categorization reduce inﬁnite diﬀerences among stimuli behaviorally cognitively usable proportions. organism’s advantage diﬀerentiate stimulus others diﬀerentiation irrelevant purposes hand. cognitive economy results representation makes task-relevant distinctions ignoring irrelevant information. form selective generalization presents agent simpler working environment task. apply principle reinforcement learning must deﬁne criteria relevant distinctions without appealing task-speciﬁc information. paper deﬁnes relevant distinctions terms amount reward agent stands lose ignoring them. resulting criteria characterize state-space distinctions important agent maximize reward task. task’s reward function determines relevance agent’s world. given pair states want know whether agent safely group together share action values—will state generalization cause agent lose reward? consider states separately comparing expectation reward actions taken expectations example consider immediate reward given taking action value resulting state. push dependence value predictions step future bypass q-value error caused inappropriate generalization compare look-ahead values order decide whether representational errors compromising agent’s ability choose actions maximize reward task. also compare look-ahead values order judge whether safely included generalized state. agent reﬁne representation learns action values separately. next section presents vocabulary deﬁnes value functions discussion. speciﬁcally action preference sets used criteria representational adequacy state compatibility. like function represents expected discounted future reward agent chooses action state parameter q-learning discount future rewards deﬁnition holds task rewards state transitions deterministic. preferred action contains action actions appear maximize agent’s expected reward. thus pref gives action value taking makes selection less stringent causes preference contain actions value within many tasks rewards state transitions either stochastic appear stochastic simply imperfect function approximation. case need modify deﬁnition replace immediate reward expected value consider possible resulting states weighted probability occurrence deﬁnition given assume agent always selects action prefδ. representation state-space ǫ-adequate representation every state reachable agent following properties hold meeting ǫ-adequacy criterion guarantees state generalization prevent agent able learn correct policy mislead agent earlier state desirability thus criterion deﬁnes standard representational accuracy individual states guaranteeing harmful eﬀects state generalization kept check agent learn make sound decisions. standard deﬁnes adequate representation makes distinctions needed task remain learnable. characterizes relevant distinctions terms amount reward agent stands lose ignoring them. approach taken introduces incremental regret representation time t—the amount reward agent loses groups current state category perfect representation would incremental regret step representation would allow agent learn distinguish best action every state. non-zero incremental regret arises kinds representational error grouping states diﬀerent policies grouping states diﬀerent values. first representation groups wrong states action appears best group sub-optimal could lead agent take wrong action second value diﬀerent value states agent might recognize arriving special opportunity action values averaged states s—not could cause agent make wrong choice st−. cases agent makes wrong decisions resulting lost reward task. ǫ-adequacy criterion limits amount lost reward comparing policy value predicted function approximation results one-step look-ahead. thus actions appear best value must good actions state value must close predicted information given category bounds incremental regret ǫ-adequacy criterion allows take representation action values known test whether makes state-space distinctions important particular task. action values still learned judgments provisional. useful additional criterion state compatibility agent learn representation along action values. although ǫ-adequacy criterion provides objective standard adequate representation— allows agent learn task—these characteristics representation really outcome particular distinctions representation makes fails make individual states. often useful able evaluate compatibility states compatibility state region poorly-chosen region could incompatible member states. also happen good regions simply action values updated long while. thus need bridge highlevel description adequate representations low-level decisions agent must make states must kept separate. words must representation distinguish states generalize states order ǫ-adequate? criteria consist three rules. ﬁrst rule ensures actions appear desirable state. second rule requires values states close based one-step lookahead. purpose third rule ensure action appears best compatible states fact pretty good action states set. diﬃcult guarantee compatibility criteria written pairs states rather terms whole set. criteria demand equality preference sets instead merely requiring preference sets overlap. looser restriction equation allows states slightly diﬀerent values actions making criteria suitable practical algorithm must account real-world noise value estimates. cut-oﬀ value appears come errors allowed combining equation equation issues worked also oﬀers proof partition representations separating incompatible states according deﬁnition guarantees ǫ-adequacy representation. deﬁnition thus describes criteria suﬃcient produce ǫ-adequate representations. task ﬁnding necessary suﬃcient conditions remains future work. representational criteria thus allow system detect relevant distinctions generalizing similar states. criteria express principle cognitive economy terms representational adequacy state compatibility. since criteria examining values actions available agent task allow feature extraction proceed without depending taskspeciﬁc knowledge. sense approach principled solution general problem representation learning autonomous agents. representational criteria provide basis understanding accurately action values must learned criteria also provide means analyzing improving representations particular task. challenging application ideas learn representation along rest task especially system forced start scratch regarding task environment black box. success depends integration representation-learning rest system. particular learning action values requires adequate representation representational criteria depend accuracy action values. furthermore changes made representation cause additional changes action values. section presents online system meets challenges learns representation along rest task. although system possible implementation ideas success argument utility robustness representational criteria. algorithm combines q-learning active strategy remembering surprising states examining ends trials. system’s current action leads unexpected results pushes current state replacing-stack data structure. ends trials system conducts mini-trials states stack investigating recent surprising states ﬁrst. investigations produce action-value proﬁles system uses update action values also determine whether representation adequately represents surprising states. states compatible prototype states regions system adjusts state-space representation accordingly. diﬀerences q-learning ǫ-adequacy criterion detect surprising states state compatibility criterion decide separate states. addition version algorithm sometimes selects starting states experiments basis stack instead always beginning start state proceeding terminal state. state-abstraction section algorithm built upon nearest-neighbor representation state-space. partition regions voronoi regions series prototype states given representation. regions consist single prototype voronoi region others compound regions consisting merged voronoi regions. compound regions represented primary prototype state; state taken representative state states fall region even though state nearest-neighbor prototype. state classiﬁed lies within simple un-merged region primary prototype nearestneighbor. state judged incompatible prototype state region split region simply adding surprising state prototype; becomes primary prototype voronoi region space. function updates action values calls function select next action. action value update applied state considered reliable function reliable source true updates updates action rather looking number visits criterion determines whether action values region updated certain minimum number times. value updates experiments reported here. checking experience ǫ-adequacy test done detect surprising states simpliﬁcation deﬁned deﬁnition uses simpler non-stochastic deﬁnition compare recently experienced transition action-value proﬁle typically driver process invokes action begin trial start state trial algorithm updates action values q-learning would chooses next action according current policy action values. algorithm continues process state transitions driver also initiate active exploration trials invoking process stack. figure outlines function process stack investigates states appeared surprising. stack last-in ﬁrst-out data structure process stack explores states occur ends episodes explores earlier states. property allows system focus frontier unlearned states states whose action values already grounded reward given environment. actions leading terminal states learned ﬁrst action values states step earlier. system learns states near ends trials stop surprising system focuses attention states precede states. action values learned states backwards beginning states without extra action-value backups internal states whose values learned. stack implemented replacingstack pushing item causes stack remove previous occurrence item adding one. enable system cope continuous state-spaces exact state might never repeated stack regards states region same. therefore pushing state removes states region stack. ensures stack size grow without bound number items stack limited number state-space regions representation particular region explored session. desirable qualities cyclic tasks like puck-on-a-hill task single region might otherwise stack states system investigates state conducting mini-trial possible action state trials last long enough agent’s state enter another region reach terminal state. case action leads back region investigation times certain number steps. single action takes agent region mini-trial stops action. feature extraction performed every trial. investigation system perform feature extraction system applies results investigation updating action values according proﬁle. like q-learning updates done level algorithm updates back values resulting states determined reliable. unlike update representation. timer timer causes function invoked regular intervals allowing time action values settle changes representation function reliable prototype tests whether updated least updates times reliable source guards action value update level algorithm function compatible applies state compatibility criterion given deﬁnition taking value basic idea this investigating surprising states current state appears incompatible classiﬁcation policy value diﬀerence results signiﬁcant lost reward state seed state-space category. occasionally consider whether representation simpliﬁed merging compatible regions whether merged regions ought stay merged. test algorithm on-line feature extraction? want know whether algorithm produces high quality representations agent’s task. common test simply evaluate performance agent uses algorithm construct representation task goes learning task. although technique often seen literature good performance task necessarily indicate high quality representation; evaluating representation several criteria help evaluate quality representation. representation contains small number features taken evidence cognitive economy provided features allow agent make necessary distinctions task. representation reusable agents evidence captures important features task rather artifacts particular training regimen. representation allows good performance variety starting points indicate high level quality throughout relevant parts state-space. therefore evaluation methodology test representation independently system produced exercise representation signiﬁcant portion state-space. quality assessment based number features eﬀectiveness representation task. eﬀectiveness representation shown eﬀectively learning curve plots task performance training time learning action values. learning curves especially useful evaluating representations single measurement either learning speed performance likely mislead. single measurement learning speed tends favor small representations fewer parameters trained single measurement performance tends favor large representations tend learn slowly eventually produce superior performance. addition learning curves show whether representation reliably supports good performance results occasional successes. learning curve averaged multiple produced order minimize system experimental runs initialization eﬀects. experiments reported consisted stages representation generation stage representation testing stage. generation stage learning system applied algorithm described above constructing representation learned task. output stage speciﬁcation state-space representation task. algorithm succeed must learn perform well task must produce representation captures important features problem form reusable agents. testing stage consisted inserting generated representation another reinforcement learning system producing learning curves system. although representation ﬁxed system must still learn action values scratch improves training. since tester separate agent produced representation able evaluate diﬀerent representations fairly allows compare controls aspects reinforcement learning problem. method evaluating learned representations separate system appears unique. produce learning curve system’s action values reset system generated series learning trials. system’s performance evaluated ends trials every trial. important interrupt running trial task studied rewards ends trials. trial deciding whether generate performance measurement depended two-part test system needed completed either pre-set number learning steps pre-set number trials. two-part test insured system generated enough data points early late performance measurement median score batch trials random starting states. random starting states chosen follows. preliminary series experiments yielded extreme values state-space coordinates; tester’s training trials started states central third observed state-space test trials started states slightly smaller zone—the central quarter space. initializing trials random starting states ensures puck-on-a-hill bang-bang control task rewards seen ends episodes normally long include tight cycles. performance task depends adequacy state-space representation. q-learning agent performs poorly seemingly reasonable representations task; analysis task leads simple twocategory representation optimal sense described below. therefore eﬀective demonstration task evaluating algorithms online feature extraction. puck-on-a-hill similar familiar polebalancing task two-dimensional state-space components position velocity only. simulation details follows. positive represents position right side hill. corresponding angle cart hill given positive represents position left side hill. positive pushes puck toward right. puck-on-a-hill task agent controls puck must learn push left right keep puck balanced hill. agent’s reinforcement comes puck falls hill either side hits analysis task help understand makes good representation. puck’s acceleration determined thrusters downward pull gravity. near center hill thrusters dominate force gravity puck push back crest hill long point puck zero velocity hold position hill; place stationary puck farther cannot avoid falling hill. hand puck already moving hill existing velocity suﬃcient carry back controllable region even starting point farther hill. therefore point return lies farther hill higher puck velocities. agent must keep puck within region thrusters eﬀective controlling puck. call states controllable states call states past pointof-no-return doomed states. figure shows controllable states form band falling roughly diagonally middle state-space. ﬁgure produced running puck experiments point grid. remaining states doomed states puck cannot avoid falling hill hitting wall matter actions takes. source reinforcement task best possible return obtained policy keeps puck within controllable zone. policy therefore optimal policy task. pushing right controllable state results doomed state classify must-pushleft state. similarly pushing left results doomed state classify must-pushright state. left right lead controllable states classify don’t care state. critical states must-pushright must-push-left states agent’s next action determines whether succeeds task. states edges controllable zone; states middle controllable zone don’t-care states neither action push puck past boundary zone. figure shows critical states must-push-left states make curve must-push-right states make bottom curve. plots determined testing controllable state found prior velocity large. away center hill’s slope becomes increasingly steep gravity overwhelms contribution puck’s thrusters. therefore puck fallen hill loses ability climb back fails shortly after. simplest possible representations preserves necessary distinction. therefore diagonal-split representation useful benchmark evaluating representations. results compare performance test system diﬀerent state-space representations. representation evaluated inserting test system generating series learning curves averaged. learning curves plot performance number training steps experienced test system. performance score median trial length batch trials conducted learning turned test trials stopped reached steps. steps training diagonal-split representation learned representation attained averaged performance scores steps. starting scratch system generated representation consisting prototype states shown figure although prototypes slightly asymmetric layout placement allows system easily identify points closer must-push-right boundary mustpush-left boundary vice versa. system adds prototypes states visits sometimes later stages failing trial. reason system kept generating points farther center likely learned negative state value states closer failure points. although value diﬀerence turned unimportant puck task exist tasks would critical distinction. another experiment system seeded representation consisting space points points either side controllable zone; although line connecting quite perpendicular line diagonal-split analysis shows simply pushing toward center hill optimal policy. example puck moving fast enough toward right need push left even already left side hill. otherwise unable slow side avoid hitting right wall. policy pushes left must-push-left states pushes right must-push-right states optimal policy. therefore representation separates classes states adequate learning optimal policy. example simply bisect controllable zone line since representation cleanly separates must-push-right states must-push-left states allows system learn optimal policy. addition representation described earlier points thought suﬃcient distinguish mustpush-left points must-push-right points. objective second experiment verify state compatibility criteria lead generation unnecessary states. conﬁrmed resulting representation simply added states usual failure points task. figure shows representation. learning process produced required trials last trial continuing million steps. results compare performance category generated representation performance four representations diagonalsplit representation described above uniform grid partitioning representation inspired variable resolution dynamic programming representation designed maintain controllability variable resolution dynamic programming produces partitioning state-space highest resolution states visited experimental trials. away experimental trajectories resolution falls gradually according constraint neighboring regions. experimental trials mental practice sessions conducted according internal model learned agent. studies reported here representation constructed trials using puck task environment initial trial agent always pushed right successful trial agent succeeded keeping puck center hill steps. vrdp initializes representation single initial trial consisted selecting action repeatedly representation enough allow good performance mental practice sessions would focus states seen successful trial. therefore vrdp would likely visit points mental practice sessions visited experimental trials—and likely additional points representation learned performance still improving. therefore representation probably idealized version application vrdp puck task. highest resolution state-space coordinate found performing binary splits coordinate. taking state-space dimensions resulted smallest distinctions figure illustrates resulting representation. failure point ﬁrst trial. result reward failure must pass long series intermediate boxes reaches critical states agent actually control puck. test explanation made second vrdpinspired representation shown figure although representation entirely observe constraint neighboring regions removes boxes resulting initial failed trial. since representation performed much better original replaces original vrdp representation comparison plots follow. representation shown figure taken representation attempts limit agent’s loss controllability according oﬀ-line analysis computed terms model task first compute worst-case deviations possible trajectories agent start diﬀerent states; divide state-space regions small enough deviation integrated pairs states region resulting controllability error less tolerance. representation part adaptive heuristic visited frequently-seen states necessarily important ones. second irrelevant state-space distinctions hinder learning original vrdp representation. third important areas space agent’s decision makes critical diﬀerence performing task. representations made relevant distinctions simplest resulted eﬃcient learning. cognitive economy approach resulted system able automatically construct good representation scratch. representation constructed small number categories proved eﬀective task. given eﬀective seed representation system made minimal additions indicating ability discern relevant distinctions. critic system learned balance puck steps average trials training steps. since assumed trials always start experiments reported severe test representation original study test system starts trials randomly-chosen starting points. figure plots performance original vrdp representation controllability quantization note performance scores figure shows averaged curves remaining representations. diagonalsplit representation representation generated learning system uniform grid enhanced vrdp representation. number categories representations respectively instances seen. important statespace regions coarse since action values appear stochastic even deterministic task simply really belong diﬀerent kinds states updated together. nearest-neighbor state-abstraction allowed system split regions simply adding prototype states. like strategy adds category current observation best classiﬁcation poor match. compared kdtree approach segmenting space hierarchical boxes nearest-neighbor approach represent higher-dimensional state-spaces eﬃciently prototypes still suﬃce broad areas similar states instead needing populate state-space boxes whose number grows exponentially dimensions statespace. drawback nearest-neighbor approach requires sophisticated implementation work eﬃciently number prototypes grows large. active strategy natural choice generating action-value proﬁles state provides values actions state time. important values changing agent learning task. even actions lead resulting state agent might mistakenly believe diﬀerent values values computed diﬀerent times. assessing state’s preferred action requires knowing values actions state. active state investigation strategy still form q-learning since q-learning specify value backups must distributed among diﬀerent state-action pairs—only continue sampled. continuing push randomlychosen states onto stack algorithm ensures values continue sampled. given static task ﬁxed lookup table representation assume agent’s action chosen discrete actions. therefore preferred action also discrete. tasks continuous-valued action choices require criteria consider range action aﬀects overall reward. active system explores state-space ends trials presupposes agent’s task episodic. non-episodic tasks sometimes made episodic choosing certain states terminal states; alternatively system could simply conduct explorations regular intervals. serious limitation real-world tasks cannot allow controller reset system state discussed below system implemented non-active versions would handle tasks. system also depends task stochastic. otherwise surprising criterion would need sophisticated presented here taking account trends averages many instances. decreasing learning rate used active investigations helps somewhat since learning rate /updates causes updated value average active reinforcement learning learn efﬁciently eliminates action value backups away frontier learned states; subject future research. idea also explored others. example rout generated monte carlo simulations states frontier thus worked backwards terminal states beginning states. signiﬁcant diﬀerence rout required task acyclic states learned could re-investigated. present strategy robust since replacing stack allows focus frontier reinvestigating states needed learn representation investigate paths prove cyclic research explored oracle provides results state transition active strategy presented here system allowed choose particular state-action combinations investigate; oracle simply provides resulting state reward transition. implementing surprising adopt stringent instead states reliable source section immediately state thus selected prototype region instead ﬁrst exploring states active investigations. decide split regions ﬁrst making tentative splits exploring eﬀectiveness. current system assesses compatibility state primary prototype since making hypothetical separation states order region split. splitting basis surprising test would make less tentative initial separation states algoanother promising approach replace oracle active state investigation internal model agent learns along task. save every state transition experiences tuple stored nearestneighbor prototype state. could reorganized regions merged detached tuples could reassigned prototypes added. agent would construct increasingly accurate model world agent could perform investigations querying model instead requesting environment reset system state actual trials world. would also remove episodic task limitation since agent could conduct investigations internally whenever wished. approach much common prioritized sweeping uses priority queue select states re-examination states whose values signiﬁcantly changed well states predicted lead them. replacingstack performs similar function giving priority surprising states order recency. states leading states often next states considered surprising leading investigated well—unless change values make value policy distinction. dyna also conducts internal experiments sort although stateaction pairs either chosen randomly given priority based long last updated. diﬀerence course dyna prioritized sweeping assume ﬁxed representation. state compatibility criteria presented deﬁnition similar developed others. example acknowledges good approximation value function areas needed impact quality controller splitting criteria split cells likely increase accuracy value function transition optimal control. state-splitting rules include local criteria making policy value distinctions; however approach targets slightly diﬀerent research problem. method requires existing model dynamics reinforcement function task instead allowing online learning unknown task experience. presents similar ideas state-space compatibility focusing decision boundaries policy distinctions generating kd-tree representation state-space system learns task. although compatibility criteria similar deﬁnitions given here criteria given paper arise analysis representational adequacy based idea incremental regret. analysis links discussion know cognitive economy provides objective framework evaluating compatibility criteria. criteria representational adequacy indicate relevant training examples ﬂagging surprising states leading agent focus eﬀorts useful. state compatibility criteria allow agent split merge regions according distinctions relevant task hand. developing representations focus relevant distinctions abilities needed reinforcement learning agents learn complex tasks unknown domains. criteria presented hoc; derived deﬁnition learnability speciﬁes maximum amount lost reward accept errors representation. criteria representational adequacy state compatibility apply general reinforcement learning task agent learns predict long-term reward results taking particular actions particular states. experimental results indicate ideas useful applied successfully real problems. system presented able learn better representations puck task supplied other well-known methods. addition ideas presented allow reinforcement learning scale complex tasks simplify task three important ways cognitive economy allows agent generalize state-space appropriate active state investigations allow agent focus frontier avoid useless action-value backups nearestneighbor representation allows volumes state-space smooth action values represented sparsely. promising directions future research.", "year": 2004}