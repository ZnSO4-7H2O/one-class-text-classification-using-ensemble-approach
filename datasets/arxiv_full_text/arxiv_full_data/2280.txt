{"title": "Deep Learning: A Critical Appraisal", "tag": ["cs.AI", "cs.LG", "stat.ML", "97R40", "I.2.0; I.2.6"], "abstract": "Although deep learning has historical roots going back decades, neither the term \"deep learning\" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.", "text": "although deep learning historical roots going back decades neither term deep learning approach popular five years field reignited papers krizhevsky sutskever hinton’s classic deep model imagenet. field discovered five subsequent years? background considerable progress areas speech recognition image recognition game playing considerable enthusiasm popular press present concerns deep learning suggest deep learning must supplemented techniques reach artificial general intelligence. departments psychology neural science york university gary.marcus nyu.edu. thank christina françois chollet ernie davis zack lipton stefano pacifico suchi saria athena vouloumanos although deep learning historical roots going back decades attracted relatively little notice five years ago. virtually everything changed publication series highly influential papers krizhevsky sutskever hinton’s imagenet classification deep convolutional neural networks achieved state-of-the-art results object recognition challenge known imagenet labs already working similar work year deep learning made front page york times rapidly became best known technique artificial intelligence wide margin. general idea training neural networks multiple layers part increases computational power data first time deep learning truly became practical. deep learning since yielded numerous state results domains speech recognition image recognition language translation plays role wide swath current applications. corporations invested billions dollars fighting deep learning talent. prominent deep learning advocate andrew gone suggest typical person mental task less second thought probably automate using either near deep learning well approaching wall much anticipated earlier beginning resurgence leading figures like hinton chollet begun imply recent months. exactly deep learning shown nature intelligence? expect might expect break down? close artificial general intelligence point machines show human-like flexibility solving unfamiliar problems? purpose paper temper irrational exuberance also consider field might need move forward. paper written simultaneously researchers field growing consumers less technical background wish understand field headed. begin brief nontechnical introduction aimed elucidating deep learning systems well turning assessment deep learning’s weaknesses fears arise misunderstandings deep learning’s capabilities closing perspective going forward technical introduction many excellent recent tutorials deep learning including well insightful blogs online resources zachary lipton chris olah many others. applications deep learning beyond classification possible though currently less popular utside scope current article. include using deep learning alternative regression component generative models create synthetic images tool compressing images tool learning probability distributions important technique approximation known variational inference. neural networks deep learning literature typically consist input units stand things like pixels words multiple hidden layers containing hidden units output units connections running nodes. typical application network might trained large sets handwritten digits labels identify categories inputs belong time algorithm called back-propagation allows process called gradient descent adjust connections units using process given input tends produce corresponding output. collectively think relation inputs outputs neural network learns mapping. neural networks particularly multiple hidden layers remarkably good learning input-output mappings systems commonly described neural networks input nodes hidden nodes output nodes thought loosely analogous biological neurons albeit greatly simplified connections nodes thought reflecting connections neurons. longstanding question outside scope current paper concerns degree artificial neural networks biologically plausible. deep learning networks make heavy technique called convolution constrains neural connections network innately capture property known translational invariance. essentially idea object slide around image maintaining identity; circle left presumed even absent direct experience) circle bottom right. deep learning also known ability self-generate intermediate representations internal units respond things like horizontal lines complex elements pictorial structure. principle given infinite data deep learning systems powerful enough represent finite deterministic mapping given inputs corresponding outputs though practice whether learn mapping depends many factors. common concern getting caught local minima systems gets stuck suboptimal solution better solution nearby space solutions searched. practice results large data sets often quite good wide range potential mappings. speech recognition example neural network learns mapping speech sounds labels object recognition neural network learns mapping images labels deepmind’s atari game system neural networks learned mappings pixels joystick positions. deep learning systems often used classification system sense mission typical network decide categories given input belongs enough imagination power classification immense; outputs represent words places board virtually anything else. deep learning’s limitations begin contrapositive live world data never infinite. instead systems rely deep learning frequently generalize beyond specific data seen whether pronunciation word image differs system seen before data less infinite ability formal proofs guarantee high-quality performance limited. discussed later article generalization thought coming flavors interpolation known examples extrapolation requires going beyond space known training examples neural networks generalize well generally must large amount data test data must similar training data allowing answers interpolated ones. krizhevsky al’s paper nine layer convolutional neural network million parameters nodes trained roughly million distinct examples drawn approximately thousand categories. sort brute force approach worked well finite world imagenet stimuli classified comparatively small categories. also works well stable domains like speech recognition exemplars mapped constant onto limited speech sound categories many reasons deep learning cannot considered general solution artificial intelligence. human beings learn abstract relationships trials. told schmister sister perhaps giving single example could immediately infer whether schmisters whether best friend schmister whether children parents schmisters forth. learning schmister case explicit definition rely hundreds thousands millions training examples capacity represent abstract relationships algebra-like variables. humans learn abstractions explicit definition implicit means indeed even -month infants acquiring learned abstract language-like rules small number unlabeled examples using common technique known data augmentation example actually presented along label many different locations original form mirror reversed form. second type data augmentation varied brightness images yielding still examples training order train network recognize images different intensities. part machine learning involves knowing forms data augmentation won’t help within given system. deep learning currently lacks mechanism learning abstractions explicit verbal definition works best thousands millions even billions training examples deepmind’s work board games atari. brenden lake colleagues recently emphasized series papers humans efficient learning complex rules deep learning systems work steven pinker children’s overregularization errors comparison neural networks geoff hinton also worried deep learning’s reliance large numbers labeled examples expressed concern recent work capsule networks coauthors noting convolutional neural networks face exponential inefficiencies lead demise. good candidate difficulty convolutional nets generalizing novel viewpoints ability deal translation built transformation chose replicating feature detectors grid grows exponentially increasing size labelled training similarly exponential way. although deep learning capable amazing things important realize word deep deep learning refers technical architectural property rather conceptual even down-to-earth concepts like ball opponent reach. consider example deepmind’s atari game work deep reinforcement learning combines deep learning reinforcement learning ostensibly results fantastic system meets beats human experts large sample games using single hyperparameters govern properties rate network alters weights advance knowledge specific games even rules. easy wildly overinterpret results show. take example according widely-circulated video system learning play brick-breaking atari game breakout after minutes training realizes digging tunnel thought wall effective technique beat game. system learned thing; doesn’t really understand tunnel wall learned specific contingencies particular scenarios. transfer tests deep reinforcement learning system confronted scenarios differ minor ways ones system trained show deep reinforcement learning’s solutions often extremely superficial. example team researchers vicarious showed efficient successor technique deepmind’s atari system failed variety minor perturbations breakout training moving coordinate paddle inserting wall midscreen. demonstrations make clear misleading credit deep reinforcement learning inducing concept like wall paddle; rather remarks comparative psychology sometimes call overattributions. it’s atari system genuinely learned concept wall robust rather system superficially approximated breaking walls within narrow highly trained circumstances. team researchers startup company called geometric intelligence found similar results well context slalom game team researchers berkeley openai shown difficult construct comparable adversarial examples variety games undermining also several related techniques recent experiments robin percy liang make similar point different domain language. various neural networks trained question answering task known squad goal highlight words particular passage correspond given question. sample instance trained system correctly impressively identified quarterback winning super bowl xxxiii john elway based short paragraph. liang showed mere insertion distractor sentences handle number variations atari game breakout albeit apparently without multi-game generality deepmind’s atari system. linguist like noam chomsky troubles liang documented would unsurprising. fundamentally current deep-learning based language models represent sentences mere sequences words whereas chomsky long argued language hierarchical structure larger structures recursively constructed smaller components. fodor pylyshyn expressed similar concerns respect earlier breed neural networks. likewise conjectured single recurrent neural networks would trouble systematically representing extending recursive structure various kinds unfamiliar sentences earlier year brenden lake marco baroni tested whether pessimistic conjectures continued hold true. title contemporary neural nets still systematic years. rnns could generalize well differences training test small generalization requires systematic compositional skills rnns fail spectacularly. similar issues likely emerge domains planning motor control complex hierarchical structure needed particular system likely encounter novel situations. indirect evidence struggles transfer atari games mentioned above generally field robotics systems generally fail generalize abstract plans well novel environments. here’s full super bowl passage; liang’s distractor sentence confused model end. eyton manning became first quarterback ever lead different teams multiple super bowls. also oldest quarterback ever play super bowl past record held john elway broncos victory super bowl xxxiii currently denver’s executive vice president football operations general manager. quarterback jeff dean jersey number champ bowl xxxiv. core problem least present deep learning learns correlations sets features flat nonhierachical simple unstructured list every feature equal footing. hierarchical structure inherently directly represented systems result deep learning systems forced variety proxies ultimately inadequate sequential position word presented sequences. systems like wordvec represent individuals words vectors modestly successful; number systems used clever tricks represent complete sentences deep-learning compatible vector spaces lake baroni’s experiments make clear. recurrent networks continue limited capacity represent generalize rich structure faithful manner. ..deep learning thus struggled open-ended inference can’t represent nuance like difference john promised mary leave john promised leave mary can’t draw inferences leaving whom likely happen next. current machine reading systems achieved degree success tasks like squad answer given question explicitly contained within text less success tasks inference goes beyond explicit text either combining multiple sentences combining explicit sentences background knowledge stated specific text selection. humans read texts frequently derive wide-ranging inferences novel implicitly licensed they example infer intentions character based indirect dialog. altough bowman colleagues taken important steps direction present deep learning system draw open-ended inferences based realworld knowledge anything like human-level accuracy. ..deep learning thus sufficiently transparent relative opacity black neural networks major focus discussion last years current incarnation deep learning systems millions even billions parameters identifiable developers terms sort human interpretable labels canonical programmers terms geography within complex network although strides visualizing contributions individuals nodes complex networks observers would acknowledge neural networks whole remain something black box. much matters long remains unclear systems robust self-contained enough might matter; important context larger systems could crucial debuggability. transparency issue unsolved potential liability using deep learning problem domains like financial trades medical diagnosis human users might like understand given system made given decision. catherine o’neill pointed opacity also lead serious issues bias. dominant approach deep learning hermeneutic sense selfcontained isolated other potentially usefully knowledge. work deep learning typically consists finding training database sets inputs associated respective outputs learn required problem learning relations inputs outputs using whatever clever architectural variants might devise along techniques cleaning augmenting data set. handful exceptions lecun’s convolutional constraint neural networks wired prior knowledge often deliberately minimized. thus example system like lerer al’s efforts learn physics falling towers prior knowledge physics newton’s laws example explicitly encoded; system instead approximates learning contingencies pixel level data. note forthcoming paper innate researchers deep learning appear strong bias including prior knowledge even prior knowledge well known. also straightforward general integrate prior knowledge deep learning system part knowledge represented deep learning systems pertains mainly correlations features rather abstractions like quantified statements discussion universally-quantified one-to-one-mappings marcus generics began easily-drawn inferences people readily answer without anything like direct training taller prince william baby prince george? make salad polyester shirt? stick carrot make hole carrot pin? know nobody even tried tackle sort thing deep learning. apparently simple problems require humans integrate knowledge across vastly disparate sources long sweet spot deep learning-style perceptual classification. instead perhaps best thought sign entirely different sorts tools needed along deep learning reach human-level cognitive flexibility. truism causation equal correlation distinction also serious concern deep learning. roughly speaking deep learning learns complex correlations input output features inherent representation causality. deep learning system easily learn height vocabulary across population whole correlated less easily represent correlation derives growth development causality central strand approaches perhaps deep learning geared towards challenges relatively little work within deep learning tradition tried address logic deep learning likely work best highly stable worlds like board game unvarying rules less well systems politics economics constantly changing. extent deep learning applied tasks stock prediction good chance eventually face fate google trends initially great predicting epidemological data search trends complete miss things like peak season ever-growing array papers shown vulnerability linguistic examples liang mentioned wide range demonstrations domain vision deep learning systems mistaken yellow-and-black patterns stripes school buses sticker-clad parking signs well-stocked refrigerators context captioning system otherwise seems impressive. example interesting recent work albeit ocused specifically rather unusual sense term causation relates presence absence objects strikes quite different sort causation finds relation disease symptoms causes. deep learning thus difficult engineer another fact follows issues raised simply hard robust engineering deep learning. team authors google title important unanswered essay machine learning high-interest credit card technical debt meaning comparatively easy make systems work limited circumstances quite difficult guarantee work alternative circumstances novel data resemble previous training data important talk icml leon bottou compared machine learning development airplane engine noted airplane design relies building complex systems simpler systems possible create sound guarantees performance machine learning lacks capacity produce comparable guarantees. google’s peter norvig noted machine learning lacks incrementality transparency debuggability classical programming trading kind simplicity deep challenges achieving robustness. discussion neural networks produced bizarre past tense forms subset inputs. verb mail example inflected past tense membled verb tour toureder. children rarely ever make mistakes like these. discussion course deep learning itself mathematics; none problems identified underlying mathematics deep learning somehow flawed. general deep learning perfectly fine optimizing complex system representing mapping inputs outputs given sufficiently large data set. real problem lies misunderstanding deep learning good for. technique excels solving closed-end classification problems wide range potential signals must mapped onto limited number categories given enough data available test closely resembles training set. deep learning systems work less well limited amounts training data available test differs importantly training space examples broad filled novelty. problems cannot given realworld limitations thought classification problems all. open-ended natural language understanding example thought classifier mapping large finite sentences large finite sentences rather mapping potentially infinite range input sentences equally vast array meanings many never previously encountered. problem like that deep learning becomes square slammed round hole crude approximation must solution elsewhere. clear intuitive sense something amiss consider experiments long tested simplified aspects language development class neural networks popular cognitive science. -vintage networks were sure simpler current models used three layers lacked lecun’s powerful convolution technique. driven backpropagation today’s systems beholden training data. language name game generalization hear sentence like john pilked football mary infer also grammatical john pilked mary football eliza pilked ball alec; equally infer word pilk means infer latter sentences would mean even hear before. distilling broad-ranging problems language simple example believe still resonance series experiments trained threelayer perceptrons identity function training examples represented input nodes represented numbers terms binary digits. number example would represented turning input nodes representing test generalization trained network various sets even numbers tested possible inputs even. every time experiment using wide variety parameters results same network would correctly apply identity function even numbers seen even numbers fail odds numbers yielding example general neural nets tested could learn training examples interpolate test examples cloud points around examples ndimensional space could extrapolate beyond training space. numbers outside training space networks could generalize identity outside space. adding hidden units didn’t help adding hidden layers. simple multilayer perceptrons simply couldn’t generalize outside training space text.) seen paper challenges generalizing beyond space training examples persist current deep learning networks nearly decades later. many problems reviewed paper data hungriness vulnerability fooling problems dealing open-ended inference transfer seen extension fundamental problem. contemporary neural networks well challenges remain close core training data start break cases periphery. different context didn’t help. course people contrast readily generalize novel words immediately upon hearing them. likewise experiments seven-month-olds consisted entirely novel words. widely-adopted addition convolution guarantees particular class problems akin identity problem solved so-called translational invariances object retains identity shifted location. solution general example lake’s recent demonstrations show. general solution within deep learning problem generalizing outside training space. reason other need look different kinds solutions want reach artificial general intelligence. biggest risks current overhyping another winter devastated field lighthill report suggested brittle narrow superficial used practice. although vastly practical applications hype still major concern. high-profile figure like andrew writes harvard business review promising degree imminent automation step reality fresh risk seriously dashed expectations. machines cannot fact many things ordinary humans second ranging reliably comprehending world understanding sentences. healthy human would ever mistake turtle rifle parking sign refrigerator. executives investing massively turn disappointed especially given poor state natural language understanding. already major projects largely abandoned like facebook’s project launched august much publicity general purpose personal assistant later downgraded significantly smaller role helping users vastly small range well-defined tasks calendar entry. probably fair chatbots general lived hype received couple years ago. example driverless also disappoint relative early hype proving unsafe rolled scale simply achieving full autonomy many promises whole field could sharp downturn popularity funding. already seeing hints this serious fears apocalyptic variety largest fear field could trapped local minimum dwelling heavily wrong part intellectual space focusing much detailed exploration particular class accessible limited models geared around capturing low-hanging fruit potentially neglecting riskier excursions might ultimately lead robust path. reminded peter thiel’s famous damning often too-narrowly focused tech industry wanted flying cars instead characters. still dream rosie robost full-service domestic robot take home; decades history bots little play music sweep floors advertisements. didn’t make progress would shame. comes risk also great potential rewards. ai’s greatest contributions society believe could ultimately come domains like automated scientific discovery leading among things towards vastly sophisticated versions medicine currently possible. need make sure field whole doesn’t first stuck local minimum. rather need reconceptualize universal solvent simply tool among many power screwdriver world also need hammers wrenches pliers mentions chisels drills voltmeters logic probes oscilloscopes. interviews deep learning pioneers geoff hinton yann lecun recently pointed unsupervised learning beyond supervised datahungry versions deep learning. clear deep learning unsupervised learning logical opposition. deep learning mostly used supervised context labeled data ways using deep learning unsupervised fashion. certainly reasons many domains move away massive demands data supervised deep learning typically requires. unsupervised learning term commonly used tends refer several kinds systems. common type system clusters together inputs share properties even without explicitly labeled. google’s detector model perhaps publicly prominent example sort approach. another approach advocated researchers yann lecun mutually exclusive first replace labeled data sets things like movies change time. intuition systems trained videos pair successive frames kind ersatz teaching signal goal predict next frame; frame becomes predictor frame without need human labeling. view approaches useful neither inherently solve sorts problems outlined section still left data hungry systems lack explicit variables advance towards open-ended inference interpretability debuggability. said different notion unsupervised learning less discussed find deeply interesting kind unsupervised learning human children children often novel task like creating tower lego bricks climbing small aperture daughter recently climbing chair space seat chair back often sort exploratory problem solving involves good deal autonomous goal setting high level problem solving well integration abstract knowledge could build systems could goals reasoning problem-solving abstract level major progress might quickly follow. another place look towards classic symbolic sometimes referred gofai symbolic takes name idea central mathematics logic computer science abstractions represented symbols. equations like allow calculate outputs wide range inputs irrespective whether seen particular values before; lines computer programs thing themselves symbolic systems often proven brittle largely developed vastly less data computational power now. right move today integrate deep learning excels perceptual classification symbolic systems excel inference abstraction. might think potential merger analogy brain; perceptual input systems like primary sensory cortex seem something like deep learning does areas like broca’s area prefrontal cortex seem operate much higher level abstraction. power flexibility brain comes part capacity dynamically integrate many different computations real-time. process scene perception instance seamlessly integrates direct sensory information complex abstractions objects properties lighting sources forth. tentative steps towards integration already exist including neurosymbolic modeling recent trend towards systems differentiable neural computers programming differentiable interpreters neural programming discrete operations none work fully scaled towards anything like full-service artificial general intelligence long argued integrating microprocessorlike operations neural networks could extremely valuable. extent brain might seen consisting broad array reusable computational primitives—elementary units processing akin sets basic instructions microprocessor—perhaps wired together parallel reconfigurable integrated circuit type known field-programmable gate array argued elsewhere steps towards enriching instruction computational systems built good thing. another potential valuable place look human cognition need machines literally replicate human mind deeply error prone perfect. remain many areas natural language understanding commonsense reasoning humans still retain clear advantage; learning mechanisms underlying human strengths could lead advances even goal exact replica human brain. many people learning humans means neuroscience; view premature. don’t know enough neuroscience literally reverse engineer brain several decades possibly gets better. help decipher brain rather around. either meantime certainly possible techniques insights drawn cognitive developmental psychology order build robust comprehensive artificial intelligence building models motivated mathematics also clues strengths human psychology. good starting point might first understand innate machinery humans minds source hypotheses mechanisms might valuable developing artificial intelligences; companion article summarize number possibilities drawn earlier work others elizabeth spelke’s drawn work focus information might represented manipulated symbolic mechanisms representing variables distinctions kinds individuals class; drawn spelke focus infants might represent notions space time object. second focal point might common sense knowledge develops represented integrated line process interactions real world recent work lerer watters colleagues tenenbaum colleagues davis suggest competing approaches think this within domain everyday physical reasoning. ..bolder challenges whether deep learning persists current form morphs something gets replaced altogether might consider variety challenge problems push systems move beyond learned supervised learning paradigms large datasets. drawing part recent special issue magazine devoted moving beyond turing test edited francesca rossi manuelo veloso suggestions would require system watch arbitrary video answer open-ended questions contained therein. specific supervised training cover possible contingencies; infererence real-world knowledge integration necessities. challenge answers many basic science questions simply retrieved searches others require inference beyond explicitly stated integration general knowledge. general game playing transfer games that example learning first-person shooter enhances performance another entirely different images equipment forth. challenge likely sufficient. natural intelligence multi-dimensional given complexity world generalized artificial intelligence necessarily multi-dimensional well. measure progress worth considering somewhat pessimistic piece wrote yorker five years conjecturing deep learning part larger challenge building intelligent machines such techniques lack ways representing causal relationships likely face challenges acquiring abstract ideas like sibling identical obvious ways performing logical inferences also still long integrating abstract knowledge information objects typically used. seen many concerns remain valid despite major advances specific domains like speech recognition machine translation board games despite equally impressive advances infrastructure amount data compute available. intriguingly last year growing array scholars coming impressive range perspectives begun emphasize similar limits. partial list includes brenden lake marco baroni françois chollet robin percy liang dileep george others vicarious pieter abbeel colleagues berkeley perhaps notably geoff hinton courageous enough reconsider beliefs revealing august interview news site axios deeply suspicious back-propagation enabler deep learning helped pioneer concern dependence labeled data sets. marcus innateness alphazero artificial intelligence. marcus comes turing test? yorker. marcus deep learning revolution artificial intelligence? yorker. marcus kluge haphazard construction human mind. boston houghton marcus rethinking eliminative connectionism. cogn psychol marcus connectionism save constructivism? cognition marcus pinker ullman hollander rosen recursive matrix-vector spaces. proceedings proceedings joint conference empirical methods natural language processing computational natural language learning.", "year": 2018}