{"title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge", "tag": ["cs.AI", "cs.LG", "cs.LO", "stat.ML"], "abstract": "This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.", "text": "jingyi zilu zhang friedman yitao liang broeck computer science department university california angeles angeles jixucs.ucla.eduzhangzilupku.edu.cn{talyliangguyvdb}cs.ucla.edu paper develops novel methodology using symbolic knowledge deep learning. ﬁrst principles derive semantic loss function bridges neural output vectors logical constraints. loss function captures close neural network satisfying constraints output. experimental evaluation shows semantic loss function effectively guides learner achieve state-of-the-art results semi-supervised multi-class classiﬁcation. moreover signiﬁcantly increases ability neural network predict structured objects rankings paths. discrete concepts tremendously difﬁcult learn beneﬁt tight integration deep learning symbolic reasoning methods. widespread success representation learning raises question tasks amenable deep learning require classical model-based symbolic reasoning whether beneﬁt integration both. recent years signiﬁcant effort gone towards various ways using representation learning solve tasks previously tackled symbolic methods. efforts include neural computers turing machines differentiable programming reed freitas graves riedel relational embeddings deep learning graph data neural theorem proving neelakantan duvenaud niepert many more. work sought augment deep learning knowledge stewart ermon m´arquez-neila minervini wang paper considers learning tasks symbolic knowledge connecting different outputs neural network. knowledge takes form constraint boolean logic. simple exactly-one constraint one-hot output encodings complex structured output prediction constraint intricate combinatorial objects rankings subgraphs paths. goal augment neural networks ability learn make predictions subject constraints symbolic knowledge improve performance. neuro-symbolic approaches simulate learn symbolic reasoning end-to-end deep neural network capture symbolic knowledge vector-space embedding. choice partly motivated need smooth differentiable models; adding symbolic reasoning code deep learning pipeline destroys property. unfortunately making reasoning differentiable precise logical meaning knowledge often lost. paper take distinctly different approach tackle problem differentiable sound logical reasoning ﬁrst principles. starting intuitive axioms derive differentiable semantic loss function captures well outputs neural network match given constraint. function precisely captures meaning constraint independent syntax. next show semantic loss gives signiﬁcant practical improvements semi-supervised classiﬁcation. semantic loss deﬁned exactly-one constraint setting permits obtain learning signal vast amounts unlabeled data. idea semantic loss helps improve consistently able classify unlabeled data. simple addition figure outputs neural network feed semantic loss functions constraints representing one-hot encoding total ranking preferences paths grid graph. loss function standard deep learning architectures yields state-of-the-art performance semi-supervised classiﬁcation mnist fashion cifar- datasets. ﬁnal experiments study beneﬁts semantic loss function complex structured output learning tasks preference learning path prediction graph scenarios task two-fold learn structure output space actual classiﬁcation function within space. capturing structure output space logical constraints minimizing semantic loss constraint learning able learn networks much likely correctly predict structured objects. formally deﬁne semantic loss make concepts propositional logic. write uppercase letters boolean variables lowercase letters instantiation sets variables written bold uppercase joint instantiation bold lowercase literal variable negation logical sentence constructed usual variables logical connectives also called formula constraint. state world instantiation variables state satisﬁes sentence denoted sentence evaluates true world deﬁned usual way. sentence entails another sentence denoted worlds satisfy also satisfy sentence logically equivalent sentence denoted output vector neural denoted value represents probability output falls softmax sigmoid units output activation functions. notation states used refer assignment logical sentence enforcing assignment binary vector capturing assignment equivalent notions. figure illustrates three different concrete output constraints varying difﬁculty studied experiments. first examine exactly-one one-hot constraint capturing encoding used multi-class classiﬁcation. states indicators exactly indicators must true rest false. enforced logical constraint conjoining sentences form pairs variables single sentence experiments examine valid simple path constraint. states given source-destination pair edge indicators edge indicators true must form valid simple path source destination. finally explore ordering constraint requires indicator variables represent total ordering variables effectively encoding permutation matrix. full description path ordering constraints refer section goal section semantic loss function bridges continuous world neural networks symbolic world propositional logic. ﬁrst postulating intuitive high-level properties seek function illustrate desired behavior. second postulates establish correspondence constraints data. finally uniquely deﬁne semantic loss function used throughout paper. semantic loss function sentence propositional logic deﬁned variables vector probabilities variables element denotes predicted probability variable corresponds single output neural net. example semantic loss one-hot constraint previous section neural output vector intended capture close prediction exactly output true others false regardless output correct. ﬁrst axiom says loss logical constraint always true independent predicted probabilities axiom semantic loss true sentence zero next enforcing constraints disjoint sets variables want ability compute semantic loss constraints separately results joint semantic loss. axiom sentence probabilities sentence disjoint probabilities semantic loss sentence joint probability vector decomposes additively directly follows axioms probabilities variables used constraint affect semantic loss. proposition appendix formalizes intuition. maintain logical meaning postulate semantic loss monotone order implication. axiom semantic loss vector intuitively stricter requirements logical constraint going making harder satisfy semantic loss cannot decrease. example enforces output neural network encode subtree graph tighten requirement path semantic loss cannot decrease. every path also tree solution solution ﬁrst consequence following monotonicity axiom logically equivalent sentences must incur identical semantic loss probability vector hence semantic loss indeed semantic property logical sentence depend syntax sentence. proposition semantic loss vector second consequence semantic loss must non-negative state equivalently represented data vector well logical constraint enforces value every variable constraint predicted vector represent state semantic loss. axiom state zero semantic loss representation sentence representation deterministic vector axioms together imply vector satisfying constraint must incur zero loss. example constraint requires output vector encodes arbitrary total ranking vector correctly represents single speciﬁc total ranking semantic loss. proposition semantic loss special case logical literals constrain single variable take single value thus play role similar labels used supervised learning. constraints require even tighter correspondence semantic loss must like classical loss function axiom semantic loss single literal proportionate cross-entropy loss equivalent data label log. appendix states additional axioms symmetry values symmetry between variables well lemma ties together multiplicative constants mentioned axiom finally allows prove following form semantic loss state lemma falls short full deﬁnition semantic loss arbitrary sentences. deﬁne additional axioms example following axiom highly desirable. axiom ﬁxed semantic loss monotone probability continuous differentiable. appendix makes notion semantic loss precise stating additional axiom. based observation state loss lemma proportionate log-probability. particular corresponds probability obtaining state independently sampling probability derived semantic loss function ﬁrst principles follows. deﬁnition vector probabilities variable sentence semantic loss intuitively semantic loss proportionate negative logarithm probability generating state satisﬁes constraint sampling values according hence selfinformation obtaining assignment satisﬁes constraint straightforward constraint ubiquitous multi-class classiﬁcation mutual exclusion one-hot-encoded outputs. given example exactly binary label must true. machine learning community made great strides machine learning task invention assorted deep learning representations associated regularization terms many models take large amounts fully labeled data granted data indispensable discovering accurate representations sustain progress alleviate need labeled data growing interest utilizing unlabeled data augment predictive power classiﬁers section shows semantic loss naturally qualiﬁes task. illustrative example illustrate beneﬁt semantic loss semi-supervised setting begin discussion small example. consider binary classiﬁcation task depicted figure ignoring unlabeled examples simple linear classiﬁer learns distinguish classes separating labeled examples figure however unlabeled examples also informative must carry properties give particular label. crux semantic loss model must conﬁdently assign consistent class even unlabeled data. encouraging model results accurate decision boundary illustrated figure next explore idea apply real-world image classiﬁcation tasks. proposed method intends generally applicable compatible feedforward neural network. semantic loss simply another regularization term directly plugged existing loss function. speciﬁcally weight overall loss becomes constraint output space simple semantic loss directly computed deﬁnition concretely exactly-one constraint used n-class classiﬁcation semantic loss reduces general arbitrary constraints semantic loss efﬁcient compute using deﬁnition advanced automated reasoning required. section discusses issue detail. section evaluate semantic loss semi-supervised setting comparing several competitive models. semi-supervised learners build supervised learner changing underlying model signiﬁcantly affects semi-supervised learner’s performance. comparison semantic loss base models used ladder nets currently achieve state-of-the-art results semi-supervised mnist cifar- speciﬁcally mnist base model fully-connected multilayer perceptron layers size ------. cifar- -layer convolutional neural network -by- padded ﬁlters. every layers features subject -by- max-pool layer strides furthermore relu batch normalization adam optimization learning rate refer appendix speciﬁcation model additional details hyper-parameter tuning. semi-supervised experiments standard held-out test examples provided original datasets randomly pick standard training examples validation set. values depend experiment retain randomly chosen labeled examples training remove labels rest. balance classes labeled samples ensure particular class over-represented. images preprocessed standardization gaussian noise added every pixel. mnist permutation invariant mnist classiﬁcation task commonly used test-bed general semi-supervised learning algorithms. setting prior information spatial arrangement input pixels. therefore excludes many data augmentation techniques involve geometric distortion images well convolutional neural networks. compares semantic loss baselines state-of-the-art results literature. ﬁrst baseline purely supervised makes unlabeled data. second classic self-training method semi-supervised learning operates follows. every iterations unlabeled examples predicted probability belonging single class assigned psuedo-label become labeled data. given labeled examples semantic loss gains around improvement purely supervised baseline. improvement even larger compared self-training. considering change additional loss term result encouraging. compared state ladder nets slightly outperform semantic loss accuracy. difference artifact excessive tuning architectures hyper-parameters learning rates mnist dataset subject coming experiments extend work challenging datasets order provide clearer comparison ladder nets. first want share thoughts semantic loss works. classical softmax layer interprets output representing categorical distribution. hence normalizing outputs softmax enforces mutual exclusion constraint enforced semantic loss function. however exist natural extend softmax loss unlabeled samples. contrast semantic loss provide learning signal unlabeled samples forcing underlying classiﬁer make decision construct conﬁdent hypothesis data. however fully supervised case semantic loss signiﬁcantly affect accuracy. enough capacity almost perfectly training data constraint always satisﬁed semantic loss almost always zero. direct consequence proposition fashion fashion dataset consists zalando’s article images aiming serve challenging drop-in replacement mnist. arguably overused requires advanced techniques achieve good performance. previous experiment method epochs whereas ladder nets need epochs converge. again experiments repeated times table reports classiﬁcation accuracy standard deviation experiments show utilizing semantic loss results large improvement baseline labels provided. moreover method compares favorably ladder nets except setting degrades fully supervised. note method already nearly reaches maximum accuracy labeled examples training dataset. cifar- show general applicability semantic loss evaluate cifar-. dataset consisting -by- images classes. simple would enough representation power capture huge variance across objects within class. cope spike difﬁculty switch underlying model -layer described earlier. batch size samples half unlabeled. experiments epochs. however limited computational resources report single trial. note make slight modiﬁcations underlying model used ladder nets reproduce similar baseline performance. please refer appendix details experimental setup. shown table method compares favorably ladder nets. however slight difference performance supervised base models direct comparison would methodologically ﬂawed. instead compare improvements baselines. terms measure method scores gain whereas ladder nets gain overall experiments demonstrated competitiveness general applicability proposed method semi-supervised learning tasks. surpassed previous state fashion cifar- close mnist. considering simplicity method results encouraging. indeed advantage semantic loss requires simple additional loss term. without changing network architecture itself incur almost computational overhead. conversely property makes method sensitive underlying model’s performance. without underlying predictive power strong supervised learning model expect beneﬁts observe here. recently became aware miyato extended work cifar- achieved state-of-the-art results surpassing performance future work plan investigate whether applying semantic loss architecture would yield even stronger performance. figure fashion pictures grouped conﬁdently supervised base model classiﬁes correctly. semantic loss ﬁnal semi-supervised model predicts correctly conﬁdently. figure illustrates effect semantic loss fashion pictures whose correct label hidden learner. pictures correctly classiﬁed supervised base model ﬁrst conﬁdent prediction semantic loss rarely diverts model initially correct labels. however bootstraps unlabeled examples achieve higher conﬁdence learned concepts. additional learning signal model changes beliefs pictures previously uncertain about. finally even conﬁdently misclassiﬁed pictures semantic loss able fully correct mistakes base model. much current machine learning research focused problems multi-class classiﬁcation remain multitude difﬁcult problems involving highly constrained output domains. mentioned previous section semantic loss little effect fully-supervised exactly-one classiﬁcation problem. leads seek difﬁcult problems illustrate semantic loss also highly informative supervised case provided output domain sufﬁciently complex space. semantic loss deﬁned boolean formula used output domain fully described manner. here develop framework tractable semantic loss highly complex constraints evaluate difﬁcult examples. goal develop method computing semantic loss gradient tractable manner. examining deﬁnition semantic loss right-hand side wellknown automated reasoning task called weighted model counting property partial derivatives computed terms other slightly modiﬁed wmcs. furthermore know circuit languages compute wmcs amenable backpropagation language circuit compilation techinques described darwiche build boolean circuit representing semantic loss. certain properties circuit form compute values gradients semantic loss time linear size circuit constructed function standard loss function described section ambition evaluating semantic loss’ performance complex constraints achieve state-of-the-art performance particular problem rather highlight effect. evaluate method problems difﬁcult output space model could longer directly data purposefully simple mlps evaluation. details hyper-parameter tuning given appendix begin classic algorithmic problem ﬁnding shortest path graph. speciﬁcally -by- grid uniform edge weights. randomly remove edges example increase difﬁculty. formally input binary vector length ﬁrst variables indicating sources destinations next edges removed. similarly label binary vector length indicating edges shortest path. finally require constraint output form valid simple path desired source destination. compile constraint method nishino encode pairwise simple paths logically merge enforce correct source destination. details constraint data generation process appendix evaluate generated dataset examples train/validation/test split. table compares test accuracy -layer baseline model augmented semantic loss. report three different accuracies illustrate effect semantic loss coherent indicates percentage examples classiﬁer gets entire conﬁguration right incoherent measures percentage individually correct binary labels whole constitute valid path all. finally constraint describes percentage predictions given model satisfy constraint associated problem. case incoherent accuracy semantic loss little effect fact slightly reduces accuracy combats standard sigmoid cross entropy. regard coherent accuracy however semantic loss large effect guiding network jointly learn true paths rather optimizing binary output individually. observing large increase percentage predictions really paths desired nodes graph. next problem examine predicting complete order preferences. given user features would like predict user would rank preference ﬁxed items. encode preference ordering items ﬂattened binary matrix {xij} denotes item position clearly conﬁgurations outputs correspond valid ordering. data preference rankings types sushi individuals taken preflib take ordering types sushi input features predict ordering remaining types splits identical shen split data train/test/split employ layer baseline. table compares baseline augmented semantic loss valid total orderings. again semantic loss marginal effect incoherent accuracy massively improves network’s ability predict valid correct orderings. remarkably without semantic loss network able output valid ordering test examples. incorporating symbolic background knowledge machine learning long-standing challenge received considerable attention structured prediction natural language processing supervised semi-supervised settings. example constrained conditional models extend linear models constraints enforced integer linear programming constraints also studied context probabilistic graphical models kisa utilize circuit language called probabilistic sentential decision diagram induce distributions arbitrary logical formulas. learn generative models satisfy preference path constraints study discriminative setting. various deep learning techniques proposed enforce either arithmetic constraints logical constraints output neural network. common approach reduce logical constraints differentiable arithmetic objectives replacing logical operators fuzzy t-norms logical implications simple inequalities. downside fuzzy relaxation logical sentences lose precise meaning. learning objective becomes function syntax rather semantics. moreover relaxations often applied horn clauses. alternative encode logic factor graph perform loopy belief propagation compute loss function known issues presence complex logical constraints several specialized techniques proposed exploit rich structure real world labels. deng propose hierarchy exclusion graphs allow ﬂexible joint modeling hierarchical categories. method invented address examples whose labels provided speciﬁc level. finally objective semantic loss increase conﬁdence predictions unlabeled data common information-theoretic approaches semi-supervised learning approaches increase robustness output perturbation difference semantic loss information-theoretic losses semantic loss generalizes arbitrary output constraints. reasoning semi-supervised learning often identiﬁed challenges deep learning going forward. paper developed principled combining automated reasoning propositional logic existing deep learning architectures. moreover showed semantic loss function provides signiﬁcant beneﬁts semi-supervised classiﬁcation well deep structured prediction highly complex output spaces. research conducted zilu zhang visiting student peking university. authors thank arthur choi yujia shen helpful discussions. work partially supported grants iis- iis- darpa grant n---. references mikhail bilenko sugato basu raymond mooney. integrating constraints metric learning semi-supervised clustering. proceedings international conference machine learning antoine bordes nicolas usunier alberto garcia-duran jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems arthur choi broeck adnan darwiche. tractable learning structured probability spaces case study learning preference distributions. proceedings international joint conference artiﬁcial intelligence adnan darwiche. canonical representation propositional knowledge bases. ijcai proceedings-international joint conference artiﬁcial intelligence volume thomas demeester rockt¨aschel sebastian riedel. lifted rule injection relation embeddings. proceedings conference empirical methods natural language processing emnlp austin texas november david duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel al´an aspuru-guzik ryan adams. convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems ayse erkan yasemin altun. semi-supervised learning generalized maximum entropy. thirteenth international conference artiﬁcial intelligence statistics volume pmlr alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwi´nska sergio g´omez colmenarejo edward grefenstette tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. ghahramani welling cortes lawrence weinberger advances neural information processing systems curran associates inc. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classifcation deep convolutional neural networks. pereira burges bottou weinberger advances neural information processing systems takeru miyato shin-ichi maeda masanori koyama nakae shin ishii. virtual adversarial training regularization method supervised semi-supervised learning. arxiv e-prints https//arxiv.org/abs/.. vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference international conference machine learning omnipress deepak pathak philipp krahenbuhl trevor darrell. constrained convolutional neural networks weakly supervised segmentation. proceedings ieee international conference computer vision proof proposition monotonicity axiom specializes choosing implies identity axiom therefore proposition bounds loss axiom replaces every variable negation. axiom sentence probabilities permutation variables sentence obtained replacing variables corresponding permuted vector probabilities. then value variable symmetry axioms together imply equality multiplicative constants label-literal duality axiom literals. lemma exists single constant literal proof. value symmetry implies using label-literal correspondence implies log) multiplicative constants left unspeciﬁed axiom. implies constants identical. similar argument based variable symmetry proves equality multiplicative constants different following ﬁnal axiom requires semantic loss proportionate logarithm function additive mutually exclusive sentences. axiom mutually exclusive sentences then exists positive constant proof theorem truth axiom states positive constants ﬁrst kolmogorov axiom probability. second kolmogorov axiom follows additive independence axiom semantic loss. third kolmogorov axiom given exponential additivity axiom semantic loss. hence probability distribution choice implies deﬁnition multiplicative constant. table shows slight architectural difference used ladder nets ours. major difference lies choice relu. note standard padded cropping preprocess images additional fully connected layer model neither used ladder nets. make slight modiﬁcation baseline performance reported rasmus reproduced. validation sets used tuning weight associated semantic loss hyperparameter causes noticeable difference performance method. semi-supervised classiﬁcation experiments perform grid search optimal value. empirically always gives best nearly best results report results experiments. conv. relu conv. leakyrelu conv. relu conv. leakyrelu conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu fashion dataset speciﬁcally mnist fashion share image size structure methods developed mnist able directly perform fashion withheavy modiﬁcations. this hyper-parameters evaluating method. however sake fairness subject ladder nets small-scale parameter tuning case performance volatile. grids experiment hyper parameter needed tuned weight given semantic loss trying selected based validation results. preference learning experiment initially chose semantic loss weight based validation tuned weight grids compile grid constraint ﬁrst nishino generate constraint source destination pair. then conjoin indicators specifying source destination pair must used ﬁnally disjoin together form constraint. generate data begin randomly removing third edges. ﬁlter connected components fewer nodes reduce degenerate cases proceed randomly selecting pairs point create data points. predictive model employ baseline layer hidden sigmoid units layer. trained using adam optimizer full data batches early stopping respect validation loss used regularizer. preference learning split user’s ordering ordering sushis features ordering labels predict. constraint compiled directly logic done straightforward manner n-item ordering. predictive model layer hidden sigmoid units layer. trained using adam optimizer full data batches early stopping respect validation loss used regularizer.", "year": 2017}