{"title": "The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the {\\em tail function} of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any $(\\epsilon,\\delta)$-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We analyze the robustness of the proposed algorithm and in addition, we compare the performance of this algorithm to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance.", "text": "consider k-armed bandit problem learning agent faced several stochastic arms source i.i.d. rewards unknown distribution. time step agent chooses observes reward obtained sample. sample considered separate item reward designating value goal item highest possible value. basic assumption known lower bound tail function reward distributions. framework provide lower bound sample complexity -correct algorithm propose algorithm attains bound logarithmic factors. analyze robustness proposed algorithm addition compare performance algorithm variant arms distinguishable agent chosen randomly stage. interestingly maximal rewards arms happen similar latter approach provide better performance. classic stochastic multi-armed bandit problem learning agent faces stochastic arms wishes maximize cumulative reward highest expected reward model studied extensively consider variant problem called k-armed bandit problem variant objective obtain sample highest possible reward precisely considering setting objective return -correct sample namely sample whose value ǫ-close overall best probability larger addition wish minimize sample complexity namely expected number samples observed learning algorithm terminates. classical problem algorithms best sense presented lower bounds sample complexity presented essential diﬀerence respect work objective correct sample case. scenario considered max-bandit model relevant single best item needs selected among several clustered sets items represented single arm. sets represent parts come diﬀerent manufacturers produced diﬀerent processes candidates referred diﬀerent employment agencies ﬁnding best match certain genetic characteristics diﬀerent populations choosing best channel among diﬀerent frequency bands cognitive radio wireless network. max-bandit problem apparently ﬁrst proposed reward distribution functions speciﬁc family algorithm upper bound sample complexity increases provided case discrete rewards another algorithm presented without performance analysis. later similar model objective maximize expected value largest sampled reward given number samples studied work attained best reward compared expected reward obtained oracle samples best time. algorithm suggested shown secure upper bound order n−b/α) diﬀerence determined properties distribution functions decreases away speciﬁc functions family. recently similar model goal value given quantile largest studied model compared allowing error size given quantile. case bound sample complexity provided increases ln−ln basic assumption present paper known lower bound formally deﬁned section available tail distributions namely probability reward given close maximum. special case lower bound probability densities near maximum. assumption provide algorithm sample complexity increases lnδ) context context therefore proposed algorithm provides improvement factor result obtained speciﬁc model improvement factor result derived similar diﬀerent objective. compare result note considering expected maximal value maximal possible value follows choice algorithm obtain expected deﬁcit largest sample respect maximal reward possible order furthermore provide lower bound sample complexity every -correct algorithm shown coincide logarithmic term upper bound derived proposed algorithm. best knowledge ﬁrst lower bound present problem. addition analyze robustness algorithm choice tail function bound case choice optimistic case choice overly conservative. basic feature max-bandit problem goal quickly focusing best sampling much possible. natural compare obtained results alternative approach ignores distinction between arms simply draws sample random round. interpreted mixing items associated sampling; accordingly refer variant uniﬁed-arm problem. problem actually coincides socalled inﬁnitely-many armed bandit model studied speciﬁc case deterministic arms studied expected uniﬁedarm approach provides best results reward distribution arms identical. however many arms suboptimal multi-armed approach provides superior performance. paper proceeds follows. next section present model. section provide lower bound sample complexity every -correct algorithm. section present -correct algorithm provide upper bound sample complexity. algorithm simple bound order lower bound logarithmic term then section provide analysis algorithm’s performance case assumption hold. section consider comparison uniﬁed-arm approach. section close paper concluding remarks. certain proofs diﬀered appendix space limitations. consider ﬁnite arms denoted stage learning agent chooses real valued reward obtained arm. rewards obtained independent identically distributed distribution function denote maximal possible reward µ∈r{µ|fk assumed ﬁnite maximal reward among arms maxk∈k tail function deﬁned follows. example uniform addition note cdfs nondecreasing functions therefore tail functions nonincreasing. observed reveal maximal value remains unknown. note every stands random variable distribution furthermore noting tail functions non-negative non-increasing assume lower bound moreover convenience shall assume strictly decreasing denote inverse function important special-case assumes probability density function lower bounded certain constant shall often general bound form illustrate results. algorithm max-bandit model samples time step based observed history require algorithm terminate after random number samples ﬁnite probability return reward maximal reward observed entire period. algorithm said -correct turning proposed algorithm provide lower bound sample complexity correct algorithm. bound established assumption additional provision concave. case non-concave turns complicated analysis currently unclear whether lower bound holds case. lower bound interpreted summing minimal number times optimal needs sampled. important observe several optimal arms excluded summation. indeed bound large several optimal arms denominator summand small arms. follows since algorithm needs obtain samples verify given ǫ-optimal. proof theorem proceeds considering given reward distributions obeys assumption showing algorithm samples suboptimal less certain number times cannot -correct related reward distributions optimal. deﬁne intersection event shown every holds history process every denote number rewards given history time every probability choosing next also hypotheses deﬁnition reward probability same unless chosen therefore deﬁnition hypotheses hypothesis true hence algorithm provide reward probability larger denote expectation probability respectively algorithm considered hypothesis every observed comparing bounds equations upper bound theorem dependence logarithmic term. noted though lower bound currently restricted concave tail function bounds algorithm bound restricted case. establish theorem ﬁrst bound probability event upper bound best maximal reward using extreme value bound. then bound largest number samples algorithm terminates assumption upper bound best maximal reward. found algorithm -correct hypothesis then hypothesis algorithm returns sample smaller least maximal possible reward probability more hence algorithm -correct. therefore -correct algorithm must satisfy arms except possibly addition algorithm starts sampling ﬁxed number times arm. then repeatedly calculates index interpreted upper bound maximal reward samples largest index. performance bounds presented algorithm depend directly choice lower bound tail functions. natural question happens choice optimistic assumption violated. opposite direction tight bound choice conservative? address questions turn. section analyze improvement sample complexity obtained utilizing multi framework compared model arms uniﬁed single sample eﬀectively obtained random arm. uniﬁed-arm model agent samples uniﬁed original arms chosen uniformly random reward sampled arm. uniﬁed therefore imal reward maxk remainder section provide lower bound sample complexity -correct algorithm attains order bound uniﬁed-arm model. then discuss approach better diﬀerent model parameters provide examples illustrate cases. algorithm ﬁxed number instances sampled algorithm chooses best among them. following theorem provide bound sample complexity achieved algorithm might diminish many arms close optimal. combining approaches single algorithm excels either case remains challenge future works. multi-armed algorithm useful compare upper bound sample complexity provided theorem algorithm lower bound uniﬁed-arm model theorem consider extreme cases. sz¨or´enyi busa-fekete weng h¨ullermeier. qualitative multi-armed bandits quantile-based approach. proceedings international conference machine learning icml pages", "year": 2015}