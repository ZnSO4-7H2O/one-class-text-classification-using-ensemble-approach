{"title": "Scalable Alignment Kernels via Space-Efficient Feature Maps", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "String kernels are attractive data analysis tools for analyzing string data. Among them, alignment kernels are known for their high prediction accuracies in string classifications when tested in combination with SVMs in various applications. However, alignment kernels have a crucial drawback in that they scale poorly due to their quadratic computation complexity in the number of input strings, which limits large-scale applications in practice. We present the first approximation named ESP+SFM for alignment kernels by leveraging a metric embedding named edit-sensitive parsing (ESP) and space-efficient feature maps (SFM) for random Fourier features (RFF) for large-scale string analyses. Input strings are projected into vectors of RFF by leveraging ESP and SFM. Then, SVMs are trained on the projected vectors, which enables to significantly improve the scalability of alignment kernels while preserving their prediction accuracies. We experimentally test ESP+ SFM on its ability to learn SVMs for large-scale string classifications with various massive string data, and we demonstrate the superior performance of ESP+SFM with respect to prediction accuracy, scalability and computation efficiency.", "text": "massive string data ubiquitous throughout research industry areas biology chemistry natural language processing data science. example e-commerce companies face serious problem analyzing huge datasets user reviews question answers purchasing histories biology homology detection huge collections protein sequences important part functional analyses therefore strong need develop powerful methods make best massive string data large-scale. kernel methods attractive data analysis tools approximate function decision boundary well enough training data. kernel methods kernel matrix a.k.a. gram matrix computed training data non-linear support vector machines trained matrix. although known kernel methods achieve high prediction accuracy various tasks classiﬁcation regression scale poorly quadratic complexity number training data addition calculation classiﬁcation requires worst case linear time number training data limits large-scale applications kernel methods practice. string kernels attractive data analysis tools analyzing string data. among them alignment kernels known high prediction accuracies string classiﬁcations tested combination svms various applications. however alignment kernels crucial drawback scale poorly quadratic computation complexity number input strings limlarge-scale applications practice. present ﬁrst approximation named esp+sfm alignment kernels leveraging metric embedding named edit-sensitive parsing space-eﬃcient feature maps random fourier features large-scale string analyses. input strings projected vectors leveraging sfm. then svms trained projected vectors enables signiﬁcantly improve scalability alignment kernels preserving prediction accuracies. experimentally test esp+sfm ability learn svms large-scale string classiﬁcations various massive string data demonstrate superior performance esp+ respect prediction accuracy scalability computation eﬃciency. using diﬀerent string similarity measures alignment kernels state-of-the-art string kernel known high prediction accuracies string classiﬁcations remote homology detection protein sequences time series classiﬁcations tested combination svms. however alignment kernels crucial drawback scale poorly quadratic computation complexity number training data kernel methods. solve scalability issues kernel methods kernel approximations using feature maps proposed. projects training data lowdimensional vectors kernel value pair training data approximately equal inner product corresponding pair dimensional vectors. then linear svms trained projected vectors signiﬁcantly improves scalability nonlinear svms preserving prediction accuracies. although variety kernel approximations using proposed enhancing scalability kernel methods polynomial kernels min-max kernels random fourier features approximation shift-invariant kernels kernels) previous works approximation alignment kernels. thus important open challenge required large-scale analyses string data develop kernel approximation alignment kernels. several metric embeddings string distance measures proposed large-scale string processing edit-sensitive parsing metric embedding string distance measure called edit distance moves consists ordinal edit operations insertion deletion replacement addition substring move operation. maps strings space integer vectors named characteristic vectors distance space. thus pair input strings approximately preserved corresponding pair characteristic vectors. date applied string processing string compression indexing edit-distance computation however remains high potential application approximation alignment kernels. expected eﬀective approximating alignment kernels approximates strings distance integer vectors. contribution. paper present ﬁrst approximation alignment kernels solve large-scale learning problems string data. ideas behind method project input strings characteristic vectors leveraging characteristic vectors dimensional vectors train linear svms mapped vectors. however applying high dimensional vectors direct requires memory linearly proportional dimensionality input vectors target dimension. need large applications limits applicability largescale. solve problem present space-eﬃcient requires memory. method called esp+sfm following desirable properties experimentally test ability esp+sfm train svms various massive string data demonstrate esp+sfm superior performance terms prediction accuracy scalability computational eﬃciency. satify positive deﬁniteness kernel matrices. thus proposed numberical corrections deﬁciency kernel matrices. global alignment kernel alignment kernel based global alignments using dynamic time warping distance distance measure. deﬁnes kernel summation score possible global alignments time series. computation time number strings length strings space usage proposed local alignment kernel notion smith-waterman algorithm protein remote homology detections. measures similarity pair strings summing scores obtained local alignments gaps strings. computation time number strings length strings space usage although achieves high classiﬁcation accuracies protein sequences combination applicable protein strings because scoring function optimized proteins. despite importance scalable learning alignment kernels previous work able achieve high scalabilities enables online learning alignment kernels preserving high prediction accuracies. present ﬁrst scalable learning alignment kernels meets demands made possible leveraging idea behind input strings approximately preserved hamming distance corresponding pair mapped strings. recently applied problem edit similarity joins also present kernel approximation alignment kernels called cgk+sfm leveraging idea behind sfm. edit-sensitive parsing approximation method eﬃciently computing edit distance moves string-to-string distance measure turn string series string operations substring move included string operation addition typical string operations insertion deletion replacement. string length i-th character formally string deﬁned minimum number edit operations deﬁned transform given string builds parse tree named tree illustrated figure example. tree balanced tree node tree belongs three types node three children node children node without children addition internal nodes tree node label children satisfying conditions numbers children same; node labels children left-to-right order. since tree balanced nodes level considered sequence node labels listed left-to-right order. denote sequence node labels level tree built input string sequence node labels leaves denoted input string i.e. denote node labels internal nodes height tree length input string d-dimension integer vector built tree dimension number node label appearing vectors called characteristic vectors. builds trees many subtrees node labels possible built common substrings strings resulted approximation distance characteristic vectors i.e. norm. precisely upper lower bounds approximation follows basic idea make pairs nodes left right positions preferentially sequence nodes tree make triples remaining three nodes. then builds type nodes pairs nodes type node triple nodes. builds tree bottom-up manner. precisely length sequence ℓ-th level tree even makes pairs builds type nodes pairs. thus level tree sequence type nodes. length sequence level tree makes pairs makes triple builds type nodes pairs nodes type node triple nodes. thus level tree sequence type nodes except last node type node last node. builds tree bottom-up manner i.e. build tree leaves root. example example. crucial drawback build completely diﬀerent trees similar strings. example aabababbab string character inserted ﬁrst position figure although similar strings builds completely diﬀerent trees respectively resulted large difference distance uses engineered strategy using algorithm. classiﬁes string substrings three categories applies diﬀerent parsing strategies according categories. tree input string built gradually applying parsing strategy strings lowest highest level tree. given sequence divides subsequences following three categories substring pairs adjacent node labels diﬀerent substring length least formally substring starting position ending position satisﬁes substring node label length least formally substring starting position ending position satisﬁes none categories classifying sequence subsequences three categories applies diﬀerent parsing methods substring according categories. applies subsequence sequence category builds nodes -level. subsequences category applies special parsing technique named alphabet reduction. cedure converting sequence sequence alphabet size symbol conversion performed follows. left adjacent symbol suppose represented binary integers. index least signiﬁcant diﬀers bit) binary integer p-th index. label label) deﬁned bit) label) computed position conversion applied sequence alphabet alphabet size resulted label sequence addition important property labels adjacent labels label sequence diﬀerent i.e. label) label) thus conversion iteratively applied label sequence label)label)...label) alphabet size reduction alphabet size performed follows. first sequence replaced least element neighbor generates sequence node labels drawn adjacent characters identical. select position local maximum i.e. shall call positions landmarks. addition pick landmark position local minimum i.e. adjacent already chosen landmark. important property landmarks successive landmark positions either hold. sequence adjacent characters alphabet figure illustrates alphabet reduction sequence abghceaf given collection strings ss...sn maximum length strings among strings builds esp-trees tt...tn built ...n build esp-trees satisfying condition nodes label sequences children node labels uses hash table allows unique symbol found pair triple symbols. hash table denoted holds pairs/triples symbols keyed corresponding symbol resulted memory used esp. practice total number pairs/triples symbols much less thus memory used hash table remains small practice shown sec.. computation time builds characteristic vectors high dimension applying high dimensional vectors consumes large amount memory. next section preset building space-eﬃciently. section present using space also proportional independent dimension vectors rff. space-eﬃcient called improves space usage generating proportional abstract point view based constructing random mapping appropriate distribution depends expectation choice purposes section need know vector coordinates independently sampled according marginal distribution ar′zr) bounded variance imply desired approximation zr′zr. indeed zr′zr poor estimator accuracy improved increasing output dimension speciﬁcally uses independent vectors sampled considers order represent function needs store matrix containing vectors uses space insight vectors need independent ensure good approximation. instead small integer parameter compute vector using hash function chosen t-wise independent family every comes distribution then instead storing need store description hash function memory. apriori seems issues approach original analysis stronger kind approximation guarantee considered namely approximation kernel function pairs points bounded region kind result achieved chosing suﬃciently large obtain stronger tail bounds. however experiments suggest type point-wise guarantee provided theorem suﬃcient application kernel approximations. algorithm generation cauchy random number using -wise independent hash functions. array array arrays -bit unsigned integers; maximum value unsigned -bit integer; parameter. applying high dimensional characteristic vectors consumes huge amount memory. thus present using memory applying t-wise independent hash functions introduced sec.. study. algorithm random number generation cauchy distribution using memory. arrays array array initialized -bit random numbers. function implemented using array array returns random number given input. then random number returned converted random number cauchy distribution tan)/β line algorithm implements generating eq.. computation time memory respectively. vector characteristic vector string computed online manner executable online manner. applying online learning linear svms passive aggressive algorithm linear svms trained online manner. length input strings. maps input strings edit distance space strings length hamming space edit distance between pair input strings approximately preserved hamming distance corresponding pair mapped strings. detail cgk. hamming space characteristic vectors distance space follows. view elements ...l locations instead characters. example view vector length code code concatenate vectors vector dimension l|σ| nonzero elements resulted hamming distance ham) built using sfm. shall call approximation alignment kernels using cgk+sfm. cgk+sfm cannot achieve high prediction accuracies practice shown sec.. section evaluated performance esp+sfm massive sting datasets shown table protein datasets consist human enzymes obtained kegg genes database respectively. enzyme coded string consisting four types nucleotides bases similarity enzyme protein coded string consisting types amino acids. enzymes belonging class enzyme commission numbers protein positive labels enzymes negative labels. enzymes positive labels enzymes negative labels protein. music sports datasets consist reviews musical instruments products sports products english amazon respectively. review rating levels. assigned positive labels reviews four levels rating negative labels reviews. numbers positive negative reviews respectively music. numbers positive negative reviews respectively sports. compound dataset consists bioactive compounds obtained pubchem database national center biotechnology information compound coded string representation chemical structures called simpliﬁed molecular input line entry system biological activities compounds human proteins obtained chembl database. study focused biological activity human protein microtubule associated protein label compound corresponds presence absence biological activity mapt. numbers positive negative compounds respectively. implemented methods performed experiments core quad-core intel xeon stopped execution method ﬁnished within hours first evaluated scalability comparison cgk. table shows execution time memory dimension characteristic vectors generated cgk. practically fast enough build characteristic vectors large datasets. executions ﬁnished within seconds compound largest dataset consisting million compounds. memory used smaller dataset memory consumed execution esp. results demonstrates high scalability massive datasets. characteristic vectors high dimensions built dataset. example million dimension vectors built sports dataset. applying high dimension characteristic vectors consumed huge amount memory deteriorating scalability proposed solve scalability problem shown next subsection. figure shows memory consumed characteristic vectors built dataset. huge amount memory consumed high dimension characteristic vectors projected vectors. around memories consumed esp+fm results sports compound respectively prevented building high dimension vectors rff. memory required linear dimension characteristic vectors dataset. consumed esp+sfm results sports compound respectively. results suggest esp+sfm enables dramatical reduction required memory compared esp+fm. figure shows execution time building projected vectors dataset. execution time increased linearly dimension method built dimension vectors compound around hours. evaluated eﬃciency applied characteristic vectors built compared examined combinations characteristic vectors projected vectors esp+fm esp+sfm cgk+fm cgk+sfm. dimension projected vectors examined deﬁned ﬁxed. average error compared dataset. table shows average errors using characteristic vectors built dataset. average errors almost esp+sfm esp+fm datasets dimension accuracies preserved achieving dramatical reduction memory required tendencies observed avesp+sfm cgk+sfm gak. used implementation downloadable addition laplacian kernels characteristic vectors evaluated denoted esp+kernel cgk+kernel respectively. used liblinear training linear svms esp+sfm cgk+sfm. trained non-linear svms esp+kernel cgk+kernel using libsvm performed -fold cross-validation dataset measured prediction accuracy area curve dimension vectors examined selected best parameter achieving highest among combinations kernel’s parameter svm’s parameter table shows execution time building computing kernel matrices addition training linear/non-linear svms method. applied protein scoring function optimized protein sequences. took hours ﬁnish execution time-consuming among methods protein. execution ﬁnished within hours protein music took around hours hours protein music respectively. execution esp+kernel cgk+kernel ﬁnish hours sports compound. results suggest existing alignment kernels suitable applications massive string datasets. executions esp+sfm cgk+sfm ﬁnished hours datasets. esp+sfm cgk+sfm took around hours hours respectively compound consisting million strings setting large relatively small datasets protein music required huge memory relatively large datasets sports compound. example consumed memories sports compound respectively. memories esp+sfm +sfm least order magnitude smaller kernel. esp+sfm cgk+sfm required memories sports compound setting large respectively. results demonstrated high memory-eﬃciency esp+sfm cgk+sfm. cgk+sfm improved larger prediction accuracy esp+sfm higher cgk+sfm datasets competitive esp+kernel cgk+kernel. results suggest esp+sfm achieve high classiﬁcation accuracy much eﬃcient methods terms memory time building training linear svms. presented ﬁrst approximation alignment kernels solving large-scale classiﬁcations string datasets. method called esp+sfm following appealing properties developed novel space-eﬃcient feature maps named applied approximations alignment kernels principle useful scaling kernel approximations using thus important feature work apply kernel approximations extensions thank takaaki nishimoto ninh pham useful discussions kernel approximation methods. research rasmus pagh received funding european research council european union’s framework programme grant agreement", "year": 2018}