{"title": "Value Directed Exploration in Multi-Armed Bandits with Structured Priors", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Multi-armed bandits are a quintessential machine learning problem requiring the balancing of exploration and exploitation. While there has been progress in developing algorithms with strong theoretical guarantees, there has been less focus on practical near-optimal finite-time performance. In this paper, we propose an algorithm for Bayesian multi-armed bandits that utilizes value-function-driven online planning techniques. Building on previous work on UCB and Gittins index, we introduce linearly-separable value functions that take both the expected return and the benefit of exploration into consideration to perform n-step lookahead. The algorithm enjoys a sub-linear performance guarantee and we present simulation results that confirm its strength in problems with structured priors. The simplicity and generality of our approach makes it a strong candidate for analyzing more complex multi-armed bandit problems.", "text": "multi-armed bandits quintessential machine learning problem requiring balancing exploration exploitation. progress developing algorithms strong theoretical guarantees less focus practical near-optimal ﬁnite-time performance. paper propose algorithm bayesian multi-armed bandits utilizes value-function-driven online planning techniques. building previous work gittins index introduce linearly-separable value functions take expected return beneﬁt exploration consideration perform n-step lookahead. algorithm enjoys sub-linear performance guarantee present simulation results conﬁrm strength problems structured priors. simplicity generality approach makes strong candidate analyzing complex multi-armed bandit problems. multi-armed bandit setup decision-maker repeatedly chooses ﬁnite actions. reward generated independently probability distribution associated action. underlying reward distribution action unknown decision-maker action-reward pair inform future choices. strong performance setup critically depends balance exploring less well-understood actions exploiting actions thought provide high reward. sense problem captures quintessence interplay learning decision-making. many approaches multi-armed bandit problem achieved impressive theoretical empirical results however growing recognition wide-spread practical require algorithms better exploit structured prior information example consider bandit problem arms represent diﬀerent levels customer discounts conversion probabilities discounts known promotion starts safely assume customers choose product oﬀered rather discount. prior information ideally used increase eﬃciency exploration particular large number discounts considered. problems prior information captured using parametric models like glm-ucb models make many additional assumptions diﬃcult verify little data available. paper propose bayesian bandit algorithm designed structured prior information order achieve good short-term performance small number pulls. take approach based online planning using lookahead search consider states possible sequence actions might lead. relying lookahead search makes possible easily exploit structured prior information available. state space grows exponentiaith number arms impossible enumerate reachable states problem horizon. thus formulation problem relies crucially value function applied modest depth cut-oﬀ lieu state search. many methods computing approximate value functions developed reinforcement learning have fact used solve multi-armed bandit problems computationally intensive cumbersome use. main contribution propose section method computing linearly-separable value functions that used concert lookahead search performs well state-of-the-art-algorithms. method computes value functions exploiting existing algorithms weakly-coupled property multi-armed bandit problems. also enjoys sublinear regret show section algorithm simple implement demonstrate section performs well bandit problems structured prior information. given fundamental simplicity approach empirical success optimistic provide basis addressing complex problems contextual bandits future. approach also opens door bandit algorithms yield improved performance additional computation time available. begin describing bayesian multi-armed bandit problem detail. focus case bernoulli bandits deferring discussion complex models section brieﬂy review previously proposed algorithms turning method. advance. achieve maximal cumulative return horizon steps decision-maker must balance exploration learn expected returns arms exploitation order learn arms likely provide high rewards. bayesian variant problem decision-maker access prior distribution expected reward represent prior parameters bandit; distributed according beta distribution. machine learning settings bayesian approach advantages disadvantage proper discussion beyond scope paper refer kaufmann russo references therein details. order markov property hold state must represent posterior distribution given history rewards every posterior parameter distributed according beta distribution beta parameters conjugate prior bernoulli distribution state therefore represented represent beta distribution parameters denote parameters prior beta distributions generally assume corresponds uniform prior. parameters beta distribution convenient interpretation observing successes failures beta. thus transition pulling consists merely adding appropriate based observed reward. transition probabilities follow deﬁnition mean beta distribution. reduce clutter omit obvious context. rewards received transitions respectively. policy discounted inﬁnite-horizon version bandit problem method based directly solving mdp. practical problems impossible compute optimal policy number states grows exponentially number arms. unfortunately gittins index optimal provably generalize bandit problems. established performance measure classic bandit algorithms regret sometimes referred pseudo-regret deﬁned particular realization bandit parameters policy minimize bayesian regret particular focus regret ﬁrst steps. guarantees provided small bound bayesian regret somewhat weaker regular regret reasonable measure circumstances. literature bandit problems enormous focus relevant algorithms. family algorithms problem structure derive tight optimistic upper bounds. algorithms simple used various applications success lack ability incorporate structured prior information dependency diﬀerent reward policies without requiring complex diﬃcult re-analysis upper bounds. kaufmann propose bayes-ucb bayesian index policy improves bayesian bandits taking advantage prior distribution. russo describes approach addressing limitations optimistic approach serves basis family algorithms. describe method considers immediate single-period regret also information gain learn partial feedback optimize exploration-exploitation trade online. provide strong bayesian regret bound applies general class models. method based similar principle uses additive value functions estimate information gain. thompson sampling works choosing based probability best arm. concretely method draws sample decision maker’s current belief distribution chooses yielded highest sample. performance thompson sampling proved near optimal simple eﬃcient implement. thompson sampling easily adapted wide range problem structures prior distributions example reject sets samples contradict contextual information. however simplicity method makes also diﬃcult improve performance. gittins indices exploit weak dependence actions compute optimal action time linear number arms gittins indices however guaranteed optimal basic multi-armed bandit problem require discounted inﬁnitehorizon objective provably cannot extended interesting practical problems involve correlations arms additional context turn approach call elsv described above main goal method ﬂexible takes advantage complex problem structure prior knowledge order reduce regret. achieve taking statespace search-based approach leveraging exploration-exploitation trade-oﬀ behavior existing algorithms build good value functions. elsv algorithm described section improve performance existing methods applied bernoulli bandits. section describe extended easily settings prior information signiﬁcantly outperforms state-of-the-art methods. general approach based n-step lookahead guided speciﬁc value function. instance receding horizon control common approach solving online planning reinforcement learning problems state space enumerated depth point value function evaluated frontier states avoid expansion. note that multiple ways reaching state state space forms graph important detect merge duplicate states. values backed current state root best-looking action chosen. outcome pulling observed cycle repeats fresh lookahead. simpliﬁed version algorithm depicted algorithm estimates value action computing expected value next step. since algorithm rely complex conﬁdence bounds would expect easily generalize many diﬀerent problems. choosing longer lookahead horizon also oﬀers promise trading little previous work considered value function-driven approach bandit problems adelman mersereau notable exception. perhaps simple eﬃcient computing value functions approximate dynamic programming requires complex computation unreliable. show however possible eﬃciently construct good value functions directly popular bandit algorithms. surprisingly value functions simple linearly separate individual arms. apparently lower expected mean less certain second order achieve good results particular sub-linear regret guarantee value function must consider expected return also conﬁdence estimates. another value function must model expected return beneﬁt exploration. seek take advantage implicit value exploration encoded existing multi-armed bandit index algorithm. algorithm shows canonical example index-based algorithm. gittins index many methods basic mold. note index computed separately. notation denotes component state corresponds example important property index function completely independent states arms thus computed eﬃciently. classic algorithm uses sub-linear regret fact shown lower values typically lead better empirical performance also make diﬃcult bound regret. unless otherwise speciﬁed. another celebrated example index policy gittins index example asymptotically optimal following gittins index results optimal policy several simple bandit settings. example gittins indices optimal inﬁnitehorizon discounted bernoulli bandit problem. generally discount horizon approximate inﬁnite horizon. unlike gittins index closed-form expression instead needs precomputed. since index computed independently computed used eﬃciently regardless number arms bandit problem zgitt denote value linearly separable value functions attractive simplicity used widely reinforcement learning approximate dynamic programming previously approximating value function bandit problems corresponding exploration bonus function contrast term represents expected value state time pulled. diﬀerence terms change value current state words much learned pulling increase information equal exploration bonus assigned index. defer proof theorem appendix follows comparing value pulling value hypothetical state would resulted pulling arm. argument relies fact policy aﬀected adding subtracting constant value function value function theorem induces policy index still approximation true value state. therefore value function constructed gittins index lead optimal policy optimal value function. algorithm describes dynamic programming method used compute value function satisﬁes following proposition shown readily algebraic manipulation states complexity algorithm. since values computed independently pre-computed ahead time computed needed basis states relevant choosing action. also important note complexity proposition independent number arms complexity -step lookahead algorithm linear number arms. elsv scale large number arms signiﬁcant diﬃculties. state. number pulls vertical axis smaller since pulled every time step. fig. shows value function computed elsv index comparison. noted above constant oﬀset value functions irrelevant quality policy. value functions plots oﬀset satisfy value function notably simpler gittins index. expected value function concave increasing independent expected success probability. indicates exploration really driven immediate reward certainty it—the potential long-term beneﬁts ignored. hand value function gittins index value exhibits curious structure increases toward high probabilities. counterintuitive would expect value function monotonically increase success probability. multiarmed bandit however also valuable learn good reduces need exploration. arms medium success probabilities provide high rewards require signiﬁcant exploration. notice also property much exaggerated section prove elsv value function sublinear regret. sublinear bound regret surprising; theorem shows elsv value function behaves identically enjoys sublinear regret bounds. instead main goal section establish methodology used analyze regret multi-armed bandit algorithms driven value functions. goal particular derive regret bounds depend property value function used elsv. need additional notation describe property concisely. stand expected value pulling action state value compares estimate expected value precise estimate value precise estimate uses unknown parameter could also interpret ﬁnite-horizon form bellman residual used bounds performance loss reinforcement learning establish bound theorem necessary diﬀerence residuals chosen elsv optimal small must also decrease least quickly words suﬃcient errors small also must decrease information returns arms becomes available. section compare performance elsv bayes-ucb thompson sampling gittins indices simulation. ﬁrst analyze section impact lookahead horizon performance plain bernoulli bandit setting. then section describe application problem structured prior information available. unlike thompson sampling gittins index must pre-computed advance. also optimal inﬁnite-horizon discounted problems. index experiments computed discount factor horizon approximate inﬁnite-horizon value. computed index using calibration method step size described example ni˜no-mora approximations computing index proposed recently ﬁrst experiments standard bernoulli bandit setting described section shown theorem elsv’s performance identical index algorithm based upon. experiments indeed conﬁrm fact. main purpose experimental evaluation section understand eﬀect size lookahead performance algorithm. reasonable assumption online planning algorithms performance generally improves increasing horizon size. fig. compares bayesian regret -step -step lookaheads -armed problem value functions computed elsv. value functions computed gittins index another α-ucb results averaged problem instances success probabilities drawn uniform beta distribution shaded areas around curves show conﬁdence intervals. fig. highlights surprising ﬁnding longer lookahead reduce regret. observed virtually improvement regret horizon point search becomes computationally intractable. hypothesize careful focused deeper search would likely yield improvements. constrained bernoulli bandit problem represents challenging case handled well existing algorithms. described introduction problem motivated application trying optimizing level personalized discount oﬀers customers e-commerce setting. studied extensively operations research using customer choice models although choice models combined methods application historical data often problematic. purpose experiments simply assume success probabilities unlike regular bernoulli bandits rewards depend pulling received reward customer decides purchase product otherwise. since discount decreases index rewards satisfy adapting algorithm constrained setting relatively straightforward. linearly separable value function computed elsv modify lookahead respect constraints particular rejection sampling appropriately adjust transition probabilities updating values lookahead. algorithm shows essentially compute conditional probability distribution given observations well observations arms. probability distribution must estimated empirically closed form. algorithm heuristic setting regret bounds yet. constrained bandit problem arms averaged problem instances conﬁdence intervals. elsv-ucb elsv-gittins value function computed gittins index respectively. omit regret plot regret much higher algorithms. stands regular thompson sampling ignores constraints constrained samples constrained posterior distribution using rejection sampling similarly algorithm results show elsv-gittins outperforms algorithms even problem arms. magnitude improvement elsv-gittins gittins index grows number arms problem increases since constrained becomes important informative. elsv-ucb performs worse still represents signiﬁcant improvement plain ucb. proposed approach bandit problems focused good short-term performance problems structured prior information. approach based kind linearly separable value function incorporates value exploration used concert online planning methods. method elsv performs close optimal basic bernoulli bandits signiﬁcantly outperform existing methods problems prior information. results simple bandit problems promising hope extend approach also contextual bandits. also believe elsv good ﬁrst step developing sophisticated value-directed methods future.", "year": 2017}