{"title": "Parallelized Tensor Train Learning of Polynomial Classifiers", "tag": ["cs.LG", "cs.AI"], "abstract": "In pattern classification, polynomial classifiers are well-studied methods as they are capable of generating complex decision surfaces. Unfortunately, the use of multivariate polynomials is limited to kernels as in support vector machines, because polynomials quickly become impractical for high-dimensional problems. In this paper, we effectively overcome the curse of dimensionality by employing the tensor train format to represent a polynomial classifier. Based on the structure of tensor trains, two learning algorithms are proposed which involve solving different optimization problems of low computational complexity. Furthermore, we show how both regularization to prevent overfitting and parallelization, which enables the use of large training sets, are incorporated into these methods. Both the efficiency and efficacy of our tensor-based polynomial classifier are then demonstrated on the two popular datasets USPS and MNIST.", "text": "abstract—in pattern classiﬁcation polynomial classiﬁers well-studied methods capable generating complex decision surfaces. unfortunately multivariate polynomials limited kernels support vector machines polynomials quickly become impractical high-dimensional problems. paper effectively overcome curse dimensionality employing tensor train format represent polynomial classiﬁer. based structure tensor trains learning algorithms proposed involve solving different optimization problems computational complexity. furthermore show regularization prevent overﬁtting parallelization enables large training sets incorporated methods. efﬁciency efﬁcacy tensor-based polynomial classiﬁer demonstrated popular datasets usps mnist. pattern classiﬁcation machine learning task identifying category observation belongs basis training observations whose category membership known. type machine learning algorithm uses known training dataset make predictions called supervised learning extensively studied wide applications ﬁelds bioinformatics computer-aided diagnosis machine vision speech recognition handwriting recognition spam detection many others usually different kinds learning methods different models generalize training examples novel test examples. pointed important invariants applications local structure variables spatially temporally nearby highly correlated. local correlations beneﬁt extracting local features conﬁgurations neighboring variables classiﬁed small number categories instance handwritten character recognition correlations image pixels nearby tend reliable ones distant pixels. learning methods incorporating kind prior knowledge often demonstrate state-of-the-art performance practical applications. popular method handwritten character recognition using convolutional neural networks variations ∗department mathematics school science hangzhou dianzi uni†department electrical electronic engineering university leuven esat stadius. leuven belgium. email multilayer perceptrons designed minimal amounts preprocessing. model unit layer receives inputs units located small neighborhood previous layer mappings share weight vector bias given convolutional layer. important component pooling layers implement nonlinear form down-sampling. amount parameters computational load reduced network. another popular method uses support vector machines original ﬁnite-dimensional feature space mapped much higher-dimensional space inner product easily computed ‘kernel trick’. considering wolfe dual representation maximum-margin hyperplane separate examples different categories space. however worth mentioning models require large amount memory long processing time train parameters. instance thousands nodes weight matrices fully-connected layers order millions. major limitation basic svms high computational complexity least quadratic dataset size. deal large datasets svmframework using ﬁxed-size least squares approximates kernel mapping problem solved primal space. tensors multidimensional generalization matrices higher orders recently gained attention ﬁeld machine learning. classiﬁcation tensors opposed matrices vectors ﬁrst considered extending concept spectral regularization matrices tensors. tensor data assumed satisfy particular lowrank tucker decomposition unfortunately still suffers exponential storage complexity. work focused speeding-up convolution operation cnns approximating operation low-rank polyadic decomposition tensor. weight matrices fully-connected layers neural networks represented tensor trains effectively reducing number parameters. also used represent nonlinear predictors classiﬁers idea always approximate mapping determined exponential number parameters storage complexity parameters. knowledge idea applied polynomial classiﬁers also suffer curse dimensionality. usual approach circumvent exponential number polynomial coefﬁcients would svms polynomial kernel solve problem dual space. article exploit efﬁcient representation multivariate polynomial order avoid curse dimensionality allowing work directly feature space. main contributions paper organized follows. section give brief introduction tensor basics including decomposition important tensor operations properties. framework learning pattern classiﬁcation presented section based different loss functions efﬁcient learning algorithms proposed section together discussion regularization parallelization. section test algorithms popular datasets usps mnist compare performance polynomial classiﬁers trained ls-svms finally conclusions work summarized section vii. throughout paper small letters scalars small bold letters vectors capital letters matrices calligraphic letters tensors. transpose matrix vector denoted respectively. identity matrix dimension denoted list abbreviations used summarized table real dth-order d-way tensor multidimensional array rn×n×···×nd generalizes notions vectors matrices higher orders. entries aii···id determined indices. numbers called dimensions tensor. example tensor dimensions shown fig. give brief introduction required tensor operations properties information found given vectors outer product denoted tensor rn×n×···×nd entry indices equal product corresponding vector elements namely ◦···◦ ⊗···⊗ symbol denotes kronecker product. illustrate represent polynomial using tensors. denote polynomial ring variables coefﬁcients ﬁeld deﬁnition given vector polynomial variables called pure-power-˜n note throughout article need tt-svd algorithm. instead initialize tt-cores randomly iteratively update cores one-by-one alternating fashion. turns tt-ranks bounded storage grows linearly order max{n nd}. proposition tensor rn×n×···×nd exists tt-decomposition tt-ranks numerical stability learning algorithms guaranteed keeping tt-cores left-orthogonal right-orthogonal achieved sequence decompositions explained section deﬁnition core called leftorthogonal well known number tensor elements grows exponentially order even dimensions small storage cost elements prohibitive large decomposition gives efﬁcient overcome so-called curse dimensionality. main idea decomposition re-express entries tensor rn×n×···×nd product matrices matrix index also called tt-core. turn matrix-by-matrix product scalar boundary conditions introduced. quantities {rk}d called tt-ranks. note core third-order tensor dimensions tt-decomposition tensor rn×n×n illustrated fig. common convert given tensor would tt-svd algorithm example tt-svd algorithm using ttsvd algorithm convert tensor example described procedure fast contraction summarized algorithm order simplify analysis computational complexity algorithm assume required steps compute contraction vector. first need construct matrices contracting tt-cores vectors operation equivalent matrix-vector products total computational cost approximately ﬂops. fortunately contraction tt-core completely independent contractions hence done parallel processors reducing computational complexity processor. maximal values experiments respectively contraction tt-core approximately equivalent product matrix vector. ﬁnal step algorithm product matrices total computational complexity ﬁnal step algorithm equivalent product matrix vector. basic operations implemented tt-format tensor addition computing frobenius norm reader referred easy recognize face understand spoken words read handwritten characters identify gender person. machines however make decisions based data measured large number sensors. section present framework learning. like pattern recognition systems learning method consists dividing system three main modules shown fig. ﬁrst module called feature extraction paramount importance pattern classiﬁcation problem. goal module build features transformations input namely original data measured large number sensors. basic reasoning behind transformbased features appropriately chosen transformation exploit remove information redundancies usually exist samples obtained measuring devices. features exhibit high information packaging properties compared original input samples. means classiﬁcation-related information compressed relatively small number features leading reduction necessary feature space dimension. feature extraction beneﬁts training classiﬁer terms memory computation also alleviates problem overﬁtting since redundant information. deal task feature extraction linear nonlinear transformation techniques widely used. example karhunen-lo`eve transform related principal component analysis popular method feature generation dimensionality reduction. nonlinear kernel version classical called kernel extension using kernel methods. discrete fourier transform another good choice fact many practical applications energy lies low-frequency components. compared basis vectors ﬁxed problem-dependent leads computational complexity. second module classiﬁer core learning. purpose module mark observation based features generated previous module. discussed task pattern classiﬁcation divided sequence binary classiﬁcations. particular binary classiﬁcation classiﬁer assigns observation score indicates class belongs order construct good classiﬁer exploit fact know labels sample given dataset. classiﬁer trained optimally respect optimality criterion. ways classiﬁer regarded kind generalized linear classiﬁer linear classiﬁcation higher dimensional space generated items given pure-power polynomial. local information encoded products features. contrast kernel-based classiﬁers work dual space classiﬁer able work directly high dimensional space exploiting tt-format. similar backpropagation algorithm multilayer perceptrons structure allows updating cores alternating way. next section describe training classiﬁers optimization different loss functions. last module fig. decision module decides category observation belongs binary classiﬁcation decisions made according sign score assigned classiﬁer namely decision depends value corresponding discriminant function. m-class problem several strategies decompose sequence binary classiﬁcation problems. straightforward extension one-against-all binary classiﬁcation problems involved. seek design discriminant functions {gi}m belongs class. classiﬁcation achieved according rule consider tensor expressed tensor train cores {gk}d main idea learning algorithms update cores alternating optimizing appropriate loss function. prior updating tt-cores tt-ranks ﬁxed particular initial guess {gk}d made. tt-ranks interpreted tuning parameters higher values result better risk overﬁtting. straightforward extend algorithms means density matrix renormalization group method tt-ranks updated adaptively. core updated order convergence guaranteed certain conditions described turns updating tt-core equivalent minimizing loss function small number variables done efﬁcient manner. following lemma shows inner product generic feature space linear function tt-cores lemma given vector mapping deﬁned cores rrk−×nk×rk example example illustrate advantageous representation pure-power polynomial suppose polynomial degrees coefﬁcients stored -way tensor tensor evaluation particular given ttrepresentation consists tt-cores storage complexity maximal ttrank. demonstrates potential tt-representation avoiding curse dimensionality tt-ranks small. example following quadratic votes. number large also apply technique binary coding. turns classiﬁers needed ceiling operation. case class represented unique binary code word length decision made basis minimal hamming distance. notational convenience deﬁne continue notation remainder article. stated before classiﬁers designed binary classiﬁcation. given training examples form y)}n feature vector example corresponding class label degree vector. feature mapped higher dimensional space generated corresponding pure-power-˜n monomials mapping rn×n×···×nd decision hyperplane separate two-class examples tensor space also called generic feature space. words like inductive learning described tensor rn×n×···×nd thus shown updating core equivalent solving least squares optimization problem rk−nkrk variables. minimizing respect results solving linear system supposing computational complexity solving maximal values experiments implies need solve linear system order takes seconds using matlab desktop computer. since goal hyperplane separate two-class training examples generic feature space care particular value output. indeed sign output makes sense. gives idea decrease number sign differences much possible updating tt-cores i.e. minimize number misclassiﬁed examples. however model discrete difﬁcult combinatorial optimization problem involved. instead suboptimal solution sense minimizing continuous cost function penalizes misclassiﬁed examples. here consider logistic regression cost function. first consider standard sigmoid function follows ﬁrst present learning algorithms based different loss functions. algorithms learn tensor directly tt-representation given dataset. enhancements namely regularization better accuracy parallelization higher speed described last subsections. least squares estimation simplest thus common estimation method. generic feature space attempt design linear classiﬁer desired output exactly however live errors true output always equal desired one. least squares estimator found minimizing following mean square error function thanks structure gradient respect tt-core equivalently rewritten linear transformation vec. words matrix rrk−nkrk×rk−nkrk determined cores {gj}j=k dkvec. appendix details. follows matrix needs reconstructed tt-core execution learning algorithms. fortunately done efﬁciently exploiting structure. particular updating core left-to-right sweep vectors {pk+)}n construct next matrix easily computed make learning algorithms numerically stable techniques orthogonalization also applied. main idea make sure updating core cores left-orthogonal cores right-orthogonal sequence decompositions. condition number constructed matrix upper bounded subproblem well-posed. updating core orthogonalize extra decomposition absorb upper triangular matrix next core details orthogonalization step found another computational challenge potentially large size training dataset. luckily dimension optimization problem updating learning algorithms rk−rk much smaller independent need compute products dack computations easily done parallel. speciﬁcally important note convex though sigmoid function not. guarantees globally optimal solution instead local optimum. equation lemma function also regarded function core since respectively deﬁned denotes all-ones vector although closedform solution update core gradient hessian allows solution efﬁcient iterative methods e.g. newton’s method whose convergence least quadratic neighbourhood solution. quasi-newton method like broyden-fletcher-goldfarb-shanno algorithm another good choice inverse hessian difﬁcult compute. cost functions learning algorithms regularization term result overﬁtting hence generalization properties obtained classiﬁer. next discuss addition regularization term results small modiﬁcation small optimization problem needs solved updating tt-cores dmrg method also used update cores. involves updating cores time tt-ranks adaptively determined means singular value decomposition give better performance cost higher computational complexity. also removes need tt-ranks priori. local linear convergence algorithm established certain conditions. particular tt-ranks correctly estimated convex optimization problems obtained solution guaranteed global optimum. choosing tt-ranks keep upper bounds tt-ranks proposition mind. section test learning algorithms compare performance ls-svms polynomial kernels popular digit recognition datasets usps mnist. algorithms implemented matlab version freely downloaded https//github.com/kbatseli/ttclassiﬁer. compare ttpolynomial classiﬁers polynomial classiﬁer based ls-svms polynomial kernel. ls-svm-polynomial classiﬁer trained matlab ls-svmlab toolbox freely downloaded http//www.esat. kuleuven.be/sista/lssvmlab/. numerical experiments done desktop intel quad-core processor running .ghz ram. postal service databasecontains handwritten digits including training testing. digit grayscale image. known usps test rather difﬁcult human error rate modiﬁed nist database handwritten digits training examples test examples. subset larger available nist. digits size-normalized centered image. description databases summarized table extracting features handwritten digits ﬁrst execute pre-process deskewing process straightening image scanned written crookedly. choosing varying number corresponding feature vectors extracted pre-trained model --p--p-d- represents input images size convolutional layer maps denotes corresponding diagonal block. term summation right-hand side equation computed distributed cores knl) core computational complexity takes approximately ﬂops. decomposition line computed householder transformations computational complexity approximately ﬂops. maximal values experiments amounts solving computing inverse factorization matrix. note based decision strategy m-class problem decomposed sequence two-class problems whose classiﬁers trained parallel. ﬁlters max-pooling layer non-overlapping regions size convolutional layer maps ﬁlters max-pooling layer non-overlapping regions size convolutional layer maps ﬁlters fully connected output layer neurons. variants well-known model lenet- models trained well mnist database. input image feature vector length third hidden layer. note usps database must resize image data size image input layer. mention techniques deskewing feature extraction using pre-trained model widely used literature decision module adopt one-against-all decision strategy classiﬁers trained separate digit others. implementation algorithm normalize initial core frobenius norm equal one. degree vector given tt-ranks upper bounded rmax. values rmax chosen minimize test error rate ensure subproblems update tt-cores could solved reasonable max. time. dimension subproblem example usps case ﬁrst ﬁxed values rmax. ﬁxed value incremented rmax whether resulted better test error rate. optimality criterion updated tensor tensor sweep. maximum number sweeps namely iterations entire training data performed session. simplify notations ttls ttlr denote learning algorithms based minimizing loss functions respectively. models regularization parameter determined technique -fold cross-validation. words randomly assign training data sets equal size. parameter chosen mean test errors minimal. numerical results usps database mnist database reported tables respectively. monotonic decrease always seen training classiﬁers. fig. shows convergence learning algorithms usps data case rmax training classiﬁer character addition also trained polynomial classiﬁer using lssvms polynomial kernels databases. using basic ls-svm scheme training error rate test error rate obtained usps dataset three half hours computation. runtime includes time required tune tuning parameters -fold cross-validation. using kernel ls-svm possible attain test error classiﬁer polynomial anymore. mnist dataset resulted consistent out-of-memory errors expected basic scheme intended large data sets. would also like point paper presents framework learning pattern classiﬁcation. ﬁrst time used represent polynomial classiﬁers enabling learning algorithms work directly high-dimensional feature space. efﬁcient learning algorithms proposed based different loss functions. numerical experiments show classiﬁer trained several minutes competitive test errors. compared polynomial classiﬁers proposed learning algorithms easily parallelized considerable advantage storage computation time. also mention results improved adding virtual examples future improvements implementation on-line learning algorithms together extension binary classiﬁer multi-class case. given degree vector rn×n×···×nd tensor format cores rrk−×nk×rk investigate gradient respect tt-core give small variation element resulting tensor given zhongming chen acknowledges support national natural science foundation china johan suykens acknowledges support a-datadrive-b pfv// gan; iuap dysco. ngai wong acknowledges support hong kong research grants council general research fund project wang yang zhang phillips yang t.-f. yuan identiﬁcation green oolong black teas china wavelet packet entropy fuzzy support vector machine entropy vol. y.-w. chang c.-j. hsieh k.-w. chang ringgaard c.-j. training testing low-degree polynomial data mappings linear journal machine learning research vol. savostyanov oseledets fast adaptive interpolation multidimensional arrays tensor train format international workshop multidimensional systems ieee rohwedder uschmajew local convergence alternating schemes optimization convex problems tensor train format siam journal numerical analysis vol.", "year": 2016}