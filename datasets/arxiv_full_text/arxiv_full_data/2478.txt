{"title": "Multi-Task Policy Search", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.RO"], "abstract": "Learning policies that generalize across multiple tasks is an important and challenging research topic in reinforcement learning and robotics. Training individual policies for every single potential task is often impractical, especially for continuous task variations, requiring more principled approaches to share and transfer knowledge among similar tasks. We present a novel approach for learning a nonlinear feedback policy that generalizes across multiple tasks. The key idea is to define a parametrized policy as a function of both the state and the task, which allows learning a single policy that generalizes across multiple known and unknown tasks. Applications of our novel approach to reinforcement and imitation learning in real-robot experiments are shown.", "text": "quality learned model. principled accounting model errors resulting optimization bias take uncertainty learned model account long-term predictions policy learning besides sample-efﬁcient learning single task generalizing learned concepts situations research topic learned controllers often deal single situation/context e.g. drive system desired state. robotics context solutions multiple related tasks often desired e.g. grasping multiple objects robot games learning hitting movements table tennis generalizing kicking movements robot soccer unlike multi-task scenarios consider setcontinuous tasks objective learn policy capable solving related tasks prescribed class. since often impossible learn individual policies conceivable tasks multi-task learning approach required generalize across tasks. assume training i.e. policy learning robot given small training tasks ηtrain test phase learned policy expected generalize training tasks previously unseen related test tasks ηtest tackle challenge either hierarchically combining local controllers richer policy parametrization. first local policies learned subsequently generalization achieved combining them e.g. means gating network approach successfully applied robotics gating network used generalize motor primitives hitting movements robot-table tennis. limitation approach deal convex combinations local policies implicitly requiring local policies linear policy parameters. proposed share state-action abstract— learning policies generalize across multiple tasks important challenging research topic reinforcement learning robotics. training individual policies every single potential task often impractical especially continuous task variations requiring principled approaches share transfer knowledge among similar tasks. present novel approach learning nonlinear feedback policy generalizes across multiple tasks. idea deﬁne parametrized policy function state task allows learning single policy generalizes across multiple known unknown tasks. applications novel approach reinforcement imitation learning realrobot experiments shown. complex robots often violate common modeling assumptions rigid-body dynamics. typical example tendon-driven robot shown fig. typical assumption violated elasticities springs. therefore learning controllers viable alternative programming robots. learn controllers complex robots reinforcement learning promising generality paradigm however without good initialization speciﬁc expert knowledge often relies dataintensive learning methods fragile robotic system however thousands physical interactions practically infeasible time-consuming experiments well wear tear robot. make practically feasible robotics need speed learning reducing number necessary interactions i.e. robot experiments. purpose modelbased often promising model-free q-learning td-learning model-based data used learn model system. model used policy evaluation improvement reducing interaction time system. however model-based suffers model errors typically assumes learned model closely resembles true underlying dynamics model errors propagate learned policy whose quality inherently depends research leading results received funding european community’s seventh framework programme grant agreement muri grant n--- department computing imperial college london. department computing imperial college london department computer science darmstadt germany department computer science university stuttgart germany planck institute intelligent systems germany department computer science engineering university washvalues across tasks transfer knowledge. approach successfully applied kicking ball robot context robocup. however mapping source target tasks explicitly required. proposed sample number tasks task distribution learn corresponding individual policies generalize problems combining classiﬁers nonlinear regression. proposed learn mappings tasks meta-parameters policy generalize across tasks. task-speciﬁc policies trained independently elementary movements given dynamic movement primitives second instead learning local policies parametrize policy directly task. instance value function-based transfer learning approach proposed generalizes across tasks ﬁnding regression function mapping task-augmented state space expected returns. follow second approach since allows generalizing nonlinear policies training access tasks given single controller learned jointly tasks using policy search. generalization unseen tasks domain achieved deﬁning policy function state task. test time allows generalization unseen tasks without retraining often cannot done real time. learning parameters multi-task policy pilco policy search framework pilco learns ﬂexible gaussian process forward models uses fast deterministic approximate inference long-term predictions achieve data-efﬁcient learning. robotics context policy search methods successfully applied many tasks seem promising value function-based methods learning policies. hence paper addresses problems robotics multi-task data-efﬁcient policy learning. policy search learning multiple tasks consider dynamical systems continuous states controls unknown transition dynamics term zero-mean i.i.d. gaussian noise covariance matrix policy search objective deterministic policy minimizes expected long-term cost trajectory depends policy thus parameters given cost function state time policy parametrized typically cost function incorporates information task e.g. desired target location xtarget trajectory. finding policy minimizes solves task controlling robot toward target. fig. generalization ability multi-task policy cart-pole experiment sec. iii-a. here state ﬁxed change controls solely change task. black line represents corresponding policy augmented task. controls training tasks denoted circles. policy smoothly generalizes across test tasks. assume dynamics stationary transition probabilities control spaces shared tasks. learning single policy sufﬁciently ﬂexible learn training tasks ηtrain obtain good generalization performance related test tasks ηtest reducing danger overﬁtting training tasks common problem current hierarchical approaches. learn single controller multiple tasks propose make policy function state parameters task trained policy potential generalize previously unseen tasks computing different control signals ﬁxed state parameters varying tasks fig. gives intuition kind generalization power expect policy uses state-task pairs inputs assume given policy parametrization ﬁxed state training targets ηtrain pair policy determines corresponding controls denoted circles. differences control signals achieved solely changing ηtrain assumed ﬁxed. parametrization policy implicitly determines generalization power tasks ηtest test time. policy ﬁxed state varying test tasks ηtest represented black curve. good parameters multi-task policy incorporate multi-task learning approach model-based pilco policy search framework high-level steps resulting algorithm summarized fig. assume training tasks ηtrain given. parametrized policy initialized randomly subsequently applied robot line fig. based initial collected data probabilistic forward model underlying robot dynamics learned consistently account model errors deﬁne policy explicit function state task essentially means policy depends task-augmented state going detail consider case function relates state task. paper consider cases linear relationship task state example state task deﬁned camera coordinates target location parametrizes deﬁnes task. task variable state vector directly related case instance task variable could simply index. approximate joint distribution gaussian joint distribution serves input distribution controller function although assume tasks ηtest given deterministically test time introducing task uncertainty training make sense reasons first training deﬁnes task distribution allow better generalization performance compared induces uncertainty planning policy learning. therefore serves regularizer makes policy overﬁtting less likely. number tasks considered training. expected cost corresponds speciﬁc training task ηtrain intuition behind expected long-term cost allow learning single controller multiple tasks jointly. hence controller parameters updated context tasks. resulting controller necessarily optimal single task optimal across tasks average presumably leading good generalization performance. expected long-term cost computed analytically given joint gaussian prior distribution distribution successor state transition probability p|xt predictive distribution iterating moment-matching approximation time steps ﬁnite horizon yields gaussian marginal predictive distributions p|ηtrain deterministic analytic approximation means moment matching allows analytic computation corresponding gradient π/dθ respect policy parameters line fig. given gradients used gradient-based optimization toolbox e.g. bfgs analytic computation gradients π/dθ efﬁcient estimating policy gradients sampling latter variance gradient estimate grows quickly number policy parameters horizon computing derivatives respect policy parameters requires repeated application chain-rule. deﬁning ext|ηtrain yields assume total derivative dp/dθ known computation previous time step. hence need compute partial derivative ∂p/∂θ. note therefore obtain gaussian approximation marginal state distribution ∂p/∂θ {∂µx ∂{µx ∂{µx ∂{µx control signal approximated gaussian mean moments often computed analytically e.g. linear models polynomial gaussian basis functions. augmentation policy task variable requires additional layer gradients computing π/dθ. variable transformation affects partial derivatives often computed analytically. similar combine derivatives gradients chain product-rules yielding analytic gradient π/dθ used gradient-based policy updates lines fig. following analyze approach multi-task policy search three scenarios under-actuated cartpole swing-up benchmark low-cost robotic manipulator system learns block stacking imitation learning ball-hitting task tendon-driven robot. cases system dynamics unknown inferred data using gps. applied proposed multi-task policy search learning model controller cart-pole swingup. system consists cart mass pendulum length mass attached cart. every external force applied cart pendulum. friction cart ground ns/m. state system comprised position cart velocity cart angle pendulum angular velocity pendulum. equations motion refer nonlinear controller parametrized regularized network gaussian basis functions. controller parameters locations basis functions shared width-matrix weights resulting approximately policy parameters. initially system expected state pendulum hangs down; speciﬁcally pushing cart left right objective swing pendulum balance inverted position target location cart speciﬁed test time xtarget cost function chosen −expx−xtarget) penalized euclidean distance pendulum desired inverted position cart target location optimally solving task required cart stop target location balancing pendulum cart offset caused immediate cost considered four experimental setups nearest neighbor nearest-neighbor baseline experiment independently learned controllers desired swing-up locations controller learned using pilco framework trials total experience test tasks ηtest applied controller closest training task ηtrain. re-weighted independent controllers training identical nn-ic. test time combined individual controllers using gating network similar resulting convex combination local policies. gatingnetwork weights viπi. extensive grid search resulted leading best test performance scenario making rw-ic nearly identical nn-ic. multi-task policy search multi-task policy search known tasks training differ location cart pendulum supposed balanced. target locations ηtrain moreover show results trials i.e. total experience only. multi-task policy search multitask policy search training tasks ηtrain training task covariance diag). show results trials i.e. total experience. testing performance algorithms applied learned policies times test-target locations ηtest −.−. every time initial state rollout sampled mtps experiments plugged test tasks compute corresponding control signals. fig. illustrates generalization performance learned controllers. horizontal axes denote locations ηtest target position cart test time. height bars show average cost time step. means training tasks ηtrain location bars. experiment fig. shows distribution used training bell-curves approximately covers range nn-ic controller nearest-neighbor baseline fig. balanced pendulum cart location away incurred cost fig. performances hierarchical rwic controller shown. performance best value gating network similar performance nn-ic controller. however training tasks local controllers test tasks range convex combination local controllers failures nn-ic pendulum could swung successfully convex combinations nonlinear local controllers eventually decreased generalization performance rw-ic. fig. shows performance mtps controller. mtps controller successfully performed swing-up plus balancing task tasks ηtest close training tasks. however performance varied relatively strongly. fig. shows mtps+ controller successfully performed swing-up plus balancing task tasks ηtest test time sufﬁciently covered uncertain training tasks ηtrain indicated bell curves representing relatively constant performance across tasks covered bell curves achieved. test average cost meant pendulum might balanced cart slightly offset. fig. shows learned mtps+ policy test tasks ηtest state ﬁxed. tab. summarizes expected costs across test tasks ηtest. averaged test tasks applications learned policy initial state sampled although nn-ic rw-ic performed swingreliably incurred largest cost test tasks balanced pendulum wrong cart position could generalize training tasks unseen test tasks. mtps experiments average cost lowest indicating multi-task policy search approach fig. multi-task policies given state varying task black policy obtained applying proposed multi-task approach blue dashed policy obtained hierarchically combining local controllers training tasks corresponding controls marked circles green stars respectively. mtps+ generalizes smoothly across tasks whereas hierarchical combination independently trained local policies generalize well. fig. illustrates difference generalization performance mtps+ approach rw-ic approach controls local policies combined means gating network. since local policies trained independently combination local controls makes sense special cases e.g. local policies linear parameters. example however local policies nonlinear. since local policies learned independently overall generalization performance poor. hand mtps+ learns single policy task always light tasks ηj=i well therefore leads overall smooth generalization. proposed multi-task learning method applied block-stacking task using low-cost off-the-shelf robotic manipulator lynxmotion fig. primesense depth camera used visual sensor. controllable degrees freedom base rotate three joints wrist rotate gripper plastic could controlled commanding desired conﬁguration servos duration executing command camera identical kinect sensor providing synchronized depth image image used camera d-tracking block robot’s gripper. goal make robot learn stack tower blocks using multi-task learning. cost function penalized distance block gripper desired drop-off location. speciﬁed camera coordinates blocks training tasks ηtrain fig. thus test time stacking required exploiting generalization dynamics model learned mapped camera coordinates block gripper commanded controls time corresponding coordinates block gripper time control signals changed rate note learned model inverse kinematics model robot’s joint state unknown. used afﬁne policy axxη policy deﬁned mapping four controlled degrees freedom base rotate three joints. report results based training trials length amounts total experience only. test phase consisted trials stacking task supposed stack block currently topmost block. tasks ηtest test time corresponded stacking blocks fig. gripper target position topmost block. here block means task move block gripper block horizontal axis shows times manipulator’s control signal changed vertical axis shows average distances target position meters. blocks distances approached zero time. thus learned multi-task controller able interpolate extrapolate training tasks test tasks without re-training. multi-task imitation learning ball-hitting movements demonstrate mtps approach also applied imitation learning. instead deﬁning cost function teacher provides demonstrations robot imitate. show mtps approach allows generalize demonstrated behavior behaviors observed before. developed method model-based imitation learning based probabilistic trajectory matching single task. idea match distribution predicted robot trajectories directly d-plane using tendon-driven biorobtm compliant light-weight robotic capable achieving high accelerations fig. torques transferred motor joints system pulleys drive cables springs which biomechanically-inspired context represent tendons elasticity. biorob’s design advantages traditional approaches modeling controlling compliant system challenging. imitation-learning experiment considered three joints robot state contained joint positions velocities robot. controls given corresponding motor torques directly determined policy learning controller used network gaussian basis functions policy parameters comprised locations basis functions weights shared diagonal covariance matrix resulting policy parameters. policy learning required minutes computation time. unlike previous examples represented task two-dimensional vector corresponding ball position cartesian coordinates arbitrary reference frame within hitting plane. task representation basically index hence unrelated state robot cross-covariances deﬁned hitting movements three different ball positions fig. training task expert demonstrated hitting movements kinesthetic teaching. goal learn single policy learns imitate three distinct expert demonstrations generalizes demonstrated behaviors tasks demonstrated. particular tests tasks deﬁned hitting balls larger region around training locations indicated blue fig. matrices blue covered well. fig. experimental setup results multi-task block-stacking task. controller learned directly task space using visual feedback primesense depth camera. paper extend imitation learning approach multi-task scenario jointly learning imitate multiple tasks small demonstrations. particular applied multi-task learning approach learning controller hitting movements variable ball positions distance ball position center table-tennis racket. computed error regular grid blue area fig. distances blue cyan areas sufﬁcient successfully ball hence approach successfully generalized given demonstrations tasks library demonstrations. controlling cart-pole system different target location task could solved without task-augmentation controller inputs possible learn controller depends position cart relative target location—the control signals identical cart location target location cart location position target location approach however learns invariances automatically i.e. require intricate knowledge system/controller properties. note linear combination local controllers usually lead success cart-pole system requires nonlinear controller joint task swinging pendulum balancing inverted position. case lynx-arm invariances longer exist optimal control signal depends absolute position relative distance target. since linear controller sufﬁcient learn block stacking convex combination individual controllers able generalize trained blocks targets extrapolation required. presented policy-search approach multi-task learning robots assume stationary dynamics. instead combining local policies using gating network works linear-in-the-parameters policies approach learns single policy jointly tasks. idea explicitly parametrize policy task therefore enable policy generalize training tasks similar unknown tasks test time. generalization phrased optimization problem jointly learning policy parameters. solving optimization problem incorporated approach pilco policy search framework allows dataefﬁcient policy learning. reported promising results multi-task standard benchmark problem robotic manipulator. approach also applies imitation learning generalizes imitated behavior solving tasks library demonstrations. paper considered case re-training policy test allowed. relaxing constraint incorporating experience test trials subsequent iteration learning procedure would improve average quality controller. deisenroth. efﬁcient reinforcement learning using gaussian processes. scientiﬁc publishing isbn ----. deisenroth englert peters fox. multi-task policy qui˜nonero-candela girard larsen rasmussen. propagation uncertainty bayesian kernel models—application multiple-step ahead forecasting. icassp", "year": 2013}