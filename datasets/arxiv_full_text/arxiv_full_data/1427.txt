{"title": "Searching for Activation Functions", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.", "text": "choice activation functions deep networks signiﬁcant effect training dynamics task performance. currently successful widely-used activation function rectiﬁed linear unit although various hand-designed alternatives relu proposed none managed replace inconsistent gains. work propose leverage automatic search techniques discover activation functions. using combination exhaustive reinforcement learning-based search discover multiple novel activation functions. verify effectiveness searches conducting empirical evaluation best discovered activation function. experiments show best discovered activation function sigmoid name swish tends work better relu deeper models across number challenging datasets. example simply replacing relus swish units improves top- classiﬁcation accuracy imagenet mobile nasnet-a inception-resnet-v. simplicity swish similarity relu make easy practitioners replace relus swish units neural network. heart every deep network lies linear transformation followed activation function activation function plays major role success training deep neural networks. currently successful widely-used activation function rectiﬁed linear unit deﬁned max. relus breakthrough enabled fully supervised training state-of-the-art deep networks deep networks relus easily optimized networks sigmoid tanh units gradients able input relu function positive. thanks simplicity effectiveness relu become default activation function used across deep learning community. numerous activation functions proposed replace relu none managed gain widespread adoption relu enjoys. many practitioners favored simplicity reliability relu performance improvements activation functions tend inconsistent across different models datasets. activation functions proposed replace relu hand-designed properties deemed important. however search techniques automate discovery traditionally human-designed components recently shown extremely effective example zoph used reinforcement learningbased search replicable convolutional cell outperforms human-designed architectures imagenet. work automated search techniques discover novel activation functions. focus ﬁnding scalar activation functions take input scalar output scalar scalar activation functions used replace relu function without changing network architecture. using combination exhaustive reinforcement learning-based search number novel activation functions show promising performance. validate effectiveness using searches discover scalar activation functions empirically evaluate best discovered activation function. best discovered activation function call swish sigmoid constant trainable parameter. extensive experiments show swish consistently matches outperforms relu deep networks applied variety challenging domains image classiﬁcation machine translation. imagenet replacing relus swish units improves top- classiﬁcation accuracy mobile nasnet-a inception-resnet-v accuracy gains signiﬁcant given year architectural tuning enlarging yielded accuracy improvement going inception inception-resnet-v order utilize search techniques search space contains promising candidate activation functions must designed. important challenge designing search spaces balancing size expressivity search space. overly constrained search space contain novel activation functions whereas search space large difﬁcult effectively search. balance criteria design simple search space inspired optimizer search space bello composes unary binary functions construct activation function. figure example activation function structure. activation function composed multiple repetitions core unit consists inputs unary functions binary function. unary functions take single scalar input return single scalar output binary functions take scalar inputs return single scalar output exp). shown figure activation function constructed repeatedly composing core unit deﬁned core unit takes scalar inputs passes input independently unary function combines unary outputs binary function outputs scalar. since scalar activation functions transform single scalar input single scalar output inputs unary functions restricted layer preactivation binary function outputs. given search space goal search algorithm effective choices unary binary functions. choice search algorithm depends size search space. search space small using single core unit possible exhaustively enumerate entire search space. core unit repeated multiple times search space extremely large making exhaustive search infeasible. large search spaces controller visualized figure timestep controller predicts single component activation function. prediction back controller next timestep process repeated every component activation function predicted. predicted string used construct activation function. candidate activation function generated search algorithm child network candidate activation function trained task image classiﬁcation cifar-. training validation accuracy child network recorded used figure controller used search large spaces. step predicts single component activation function. prediction back input next timestep autoregressive fashion. controller keeps predicting every component activation function chosen. controller trained reinforcement learning. update search algorithm. case exhaustive search list performing activation functions ordered validation accuracy maintained. case controller controller trained reinforcement learning maximize validation accuracy validation accuracy serves reward. training pushes controller generate activation functions high validation accuracies. since evaluating single activation function requires training child network search computationally expensive. decrease wall clock time required conduct search distributed training scheme used parallelize training child network. scheme search algorithm proposes batch candidate activation functions added queue. worker machines pull activation functions queue train child network report back ﬁnal validation accuracy corresponding activation function. validation accuracies aggregated used update search algorithm. conduct searches resnet- child network architecture train cifar- steps. constrained environment could potentially skew results performing activation functions might perform well small networks. however show experiments section many discovered functions generalize larger models. exhaustive search used small search spaces controller used larger search spaces. controller trained policy proximal optimization using exponential moving average rewards baseline reduce variance. full list unary binary functions considered follows indicates per-channel trainable parameter sigmoid function. different search spaces created varying number core units used construct activation function varying unary binary functions available search algorithm. complicated activation functions consistently underperform simpler activation functions potentially increased difﬁculty optimization. best performing activation functions represented core units. searches discovered activation functions utilize periodic functions cos. common periodic functions addition subtraction preactivation periodic functions activation functions brieﬂy explored prior work discovered functions suggest fruitful route research. functions division tend perform poorly output explodes denominator near division successful functions denominator either bounded away cosh approach numerator also approaches producing output since activation functions found using relatively small child network performance generalize applied bigger models. test robustness performing novel activation functions different architectures additional experiments using preactivation resnet- wide resnet densenet models. implement models tensorflow replace relu function novel activation functions discovered searches. hyperparameters described work optimizing using momentum follow previous works reporting median different runs. results shown tables despite changes model architecture eight activation functions successfully generalize. activation functions match outperform relu resnet-. furthermore discovered activation functions max) consistently match outperform relu three models. results promising still unclear whether discovered activation functions successfully replace relu challenging real world datasets. order validate effectiveness searches rest work focus empirically evaluating activation function call swish. choose extensively evaluate swish instead max) early experimentation showed better generalization swish. following sections analyze properties swish conduct thorough empirical evaluation comparing swish relu candidate baseline activation functions number large models across variety tasks. swish recap swish deﬁned sigmoid function either constant trainable parameter. figure plots graph swish different values swish equivalent sigmoid-weighted linear unit elfwing proposed reinforcement learning. swish becomes scaled linear function swish becomes like relu function. suggests swish loosely viewed smooth function nonlinearly interpolates linear function relu function. degree interpolation controlled model trainable parameter. like relu swish unbounded bounded below. unlike relu swish smooth nonmonotonic. fact non-monotonicity property swish distinguishes common activation functions. derivative swish ﬁrst derivative swish shown figure different values scale controls fast ﬁrst derivative asymptotes derivative magnitude less inputs less around thus success swish implies gradient preserving property relu longer distinct advantage modern architectures. striking difference swish relu non-monotonic bump swish shown figure large percentage preactivations fall inside domain bump indicates non-monotonic bump important aspect swish. shape bump controlled changing parameter. ﬁxing effective practice experiments section shows training improve performance models. figure plots distribution trained values mobile nasnet-a model trained values spread peak suggesting model takes advantage additional ﬂexibility trainable parameters. practically swish implemented single line code change deep learning libraries tensorflow tf.nn.swish using version tensorflow released submission work). cautionary note batchnorm used scale parameter set. high level libraries turn scale parameter default relu function piecewise linear setting incorrect swish. training swish networks found slightly lowering learning rate used train relu networks works well. benchmark swish relu number recently proposed activation functions challenging datasets swish matches exceeds baselines nearly tasks. following sections describe experimental settings results greater detail. summary table shows swish comparison baseline activation function considered results table aggregated comparing performance swish performance different activation functions applied variety models inception resnet-v transformer across multiple datasets cifar imagenet english→german translation. improvement swish activation functions statistically signiﬁcant one-sided paired sign test. compare swish several additional baseline activation functions variety models datasets. since many activation functions proposed choose common activation functions compare against follow guidelines laid work avoid skewing comparison model type compared once. model multiple results represented median results. speciﬁcally models aggregated results resnet- wide resnet densenet across cifar- cifar- results mobile nasnet-a inception-resnet-v across runs transformer model across newstest results. ﬁrst compare swish baseline activation functions cifar- cifar- datasets follow used comparing activation functions discovered search techniques compare median runs preactivation resnet- wide resnet densenet models. results tables show swish swish- consistently matches outperforms relu every model cifar- cifar-. swish also matches exceeds best baseline performance almost every model. importantly best baseline changes different models demonstrates stability swish match varying baselines. softplus smooth approaches zero side similar swish also strong performance. next benchmark swish baseline activation functions imagenet classiﬁcation dataset imagenet widely considered important image classiﬁcation datasets consisting classes million training images. evaluate validation dataset images. compare activation functions variety architectures designed imagenet inception-resnet-v inception-v inception-v mobilenet mobile nasnet-a architectures designed relus. replace relu activation function different activation functions train ﬁxed number steps determined convergence relu baseline. activation function different learning rates rmsprop pick best. networks initialized initialization verify performance differences reproducible inception-resnet-v mobile nasnet-a experiments times best learning rate ﬁrst experiment. plot learning curves mobile nasnet-a figure results tables show strong performance swish. inception-resnet-v swish outperforms relu nontrivial swish performs especially well mobile sized models boost mobile nasnet-a boost mobilenet relu. swish also matches exceeds best performing baseline models again best performing baseline differs depending model. softplus achieves accuracies comparable swish larger models performs worse mobile sized models. inception-v gains switching activation functions limited swish slightly underperforms softplus elu. general results suggest switching swish improves performance little additional tuning. additionally benchmark swish domain machine translation. train machine translation models standard english→german dataset million training sentences evaluate different newstest sets using standard bleu metric. attention based transformer model utilizes relus -layered feedforward network attention layer. train layer base transformer model different learning rates steps otherwise hyperparameters original work using adam optimize. table shows swish outperforms matches baselines machine translation. swish- especially well newstest exceeding next best performing baseline bleu points. worst performing baseline function softplus demonstrating inconsistency performance across differing domains. contrast swish consistently performs well across multiple domains. zoph real zhong optimizers search techniques discover traditionally hand-designed components instance recently revived subﬁeld meta-learning meta-learning used initializations one-shot learning adaptable reinforcement learning generating model parameters meta-learning powerful ﬂexibility derived minimal assumptions encoded leads empirically effective solutions. take advantage property order scalar activation functions swish strong empirical performance. work focuses scalar activation functions transform scalar another scalar many types activation functions used deep networks. many-to-one functions like pooling maxout gating derive power combining multiple sources nonlinear way. one-to-many functions like concatenated relu improve performance applying multiple nonlinear functions single input. finally many-to-many functions batchnorm layernorm induce powerful nonlinear relationships inputs. prior work focused proposing activation functions studies systematically compared different activation functions. best knowledge ﬁrst study compare scalar activation functions across multiple challenging datasets. study shows swish consistently outperforms relu deep models. strong performance swish challenges conventional wisdom relu. hypotheses importance gradient preserving property relu seem unnecessary residual connections enable optimization deep networks. similar insight found fully attentional transformer intricately constructed lstm cell longer necessary constant-length attentional connections used. architectural improvements lessen need individual components preserve gradients. work utilized automatic search techniques discover novel activation functions strong empirical performance. empirically validated best discovered activation function call swish deﬁned sigmoid. experiments used models hyperparameters designed relu replaced relu activation function swish; even simple suboptimal procedure resulted swish consistently outperforming relu activation functions. expect additional gains made models hyperparameters speciﬁcally designed swish mind. simplicity swish similarity relu means replacing relus network simple line code change. thank esteban real geoffrey hinton irwan bello jascha sohl-dickstein shlens kathryn rough mohammad norouzi navdeep jaitly niki parmar smith simon kornblith vijay vasudevan google brain team help project. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorﬂow system large-scale machine learning. usenix symposium operating systems design implementation volume richard hahnloser rahul sarpeshkar misha mahowald rodney douglas sebastian seung. digital selection analogue ampliﬁcation coexist cortex-inspired silicon circuit. nature kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing humanlevel performance imagenet classiﬁcation. proceedings ieee international conference computer vision andrew howard menglong chen dmitry kalenichenko weijun wang tobias weyand marco andreetto hartwig adam. mobilenets efﬁcient convolutional neural networks mobile vision applications. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision wenling shang kihyuk sohn diogo almeida honglak lee. understanding improving convolutional neural networks concatenated rectiﬁed linear units. international conference machine learning christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. ieee conference computer vision pattern recognition june aaron oord kalchbrenner lasse espeholt oriol vinyals alex graves conditional image generation pixelcnn decoders. advances neural information processing systems ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin. attention need. advances neural information processing systems", "year": 2017}