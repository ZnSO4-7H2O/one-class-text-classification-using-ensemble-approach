{"title": "An Efficient Training Algorithm for Kernel Survival Support Vector  Machines", "tag": ["cs.LG", "cs.AI", "stat.ML", "G.1.6; I.5.1; J.3"], "abstract": "Survival analysis is a fundamental tool in medical research to identify predictors of adverse events and develop systems for clinical decision support. In order to leverage large amounts of patient data, efficient optimisation routines are paramount. We propose an efficient training algorithm for the kernel survival support vector machine (SSVM). We directly optimise the primal objective function and employ truncated Newton optimisation and order statistic trees to significantly lower computational costs compared to previous training algorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with $n$ samples and $p$ features. Our results demonstrate that our proposed optimisation scheme allows analysing data of a much larger scale with no loss in prediction performance. Experiments on synthetic and 5 real-world datasets show that our technique outperforms existing kernel SSVM formulations if the amount of right censoring is high ($\\geq85\\%$), and performs comparably otherwise.", "text": "abstract. survival analysis fundamental tool medical research identify predictors adverse events develop systems clinical decision support. order leverage large amounts patient data eﬃcient optimisation routines paramount. propose eﬃcient training algorithm kernel survival support vector machine directly optimise primal objective functionand employ truncated newton optimisation order statistic trees signiﬁcantly lower computational costs compared previous training algorithms require space time datasets samples features. results demonstrate proposed optimisation scheme allows analysing data much larger scale loss prediction performance. experiments synthetic real-world datasets show technique outperforms existing kernel ssvm formulations amount clinical research primary interest often time occurrence adverse event death reaching speciﬁc state disease progression. survival analysis objective establish connection features time event interest. diﬀers traditional machine learning parts training data partially observed. clinical study patients often monitored particular time period events occurring particular period recorded. patient experiences event exact time event recorded time event uncensored. contrast patient remained event-free study period unknown whether event occurred study ended time event right censored study. lead poor predictive performance non-linearities interactions modelled explicitly. depending level complexity researchers might forced many diﬀerent model formulations cumbersome. success kernel methods machine learning motivated researchers propose kernel-based survival models ease analysis presence non-linearities allow analysing complex data form graphs strings means appropriate kernel functions thus instead merely describing patients feature vectors structured expressive forms representation employed gene co-expression networks kernel-based coxph model proposed authors cast survival analysis regression problem adapted support vector regression whereas authors cast learning-to-rank problem adapting rank support vector machine eleuteri formulated model based quantile regression. transformation model minimal lipschitz smoothness survival analysis proposed finally belle proposed hybrid ranking-regression model. paper focus improving optimisation scheme nonlinear ranking-based survival support vector machine existing training algorithms perform optimisation dual require time excluding evaluations kernel function space number features samples. recently eﬃcient training algorithm linear ssvm much lower time complexity linear space complexity proposed extend optimisation scheme non-linear case demonstrate superiority synthetic real-world datasets. implementation proposed training algorithm available online https //github.com/tum-camp/survival-support-vector-machine. ssvm extension rank right censored survival data consequently survival analysis cast learning-to-rank problem patients lower survival time ranked patients longer survival time. absence censoring case traditional rank pairwise comparisons samples used training. however samples right censored pairwise relationships invalid. comparing censored samples unknown whether i-th sample ranked j-th sample vice versa exact time event unknown samples. applies comparing uncensored sample censored sample therefore pairwise comparison valid sample lower observed time uncensored. formally valid comparable pairs given training hence consists quadratic survival times censored size depends amount uncensored records order observed time points censored uncensored. denote percentage uncensored time points least qen/. situation arises censored subjects drop ﬁrst event observed hence uncensored records incomparable censored records. situation reversed ﬁrst censored time point occurs last time point observed event uncensored records compared m-dimensional vector ones lagrangian multipliers irm×n sparse matrix zero otherwise. easy approach quickly becomes intractable constructing matrix axxa requires space solving quadratic problem requires time. belle addressed problem reducing size elements considered pairs largest uncensored sample however approach eﬀectively simpliﬁes objective function usually leads diﬀerent solution. proposed eﬃcient optimisation scheme solving substituting hinge loss squared hinge loss using truncated newton optimisation order statistics trees avoid explicitly constructing pairwise comparisons resulting much reduced time space complexity. however optimisation scheme applicable training linear model. circumvent problem data transformed kernel training eﬀectively results non-linear model original feature space disadvantage approach requires operations construct kernel matrix assuming evaluating kernel function costs perform singular value decomposition. proposed alternative approach rank allows directly optimising primal objective function non-linear case too. natural adapt approach training nonlinear ssvm describe next. objective function non-linear ssvm similar linear model discussed fact diﬀerentiable convex respect allows employing truncated newton optimisation ﬁrstsecond-order partial derivatives form note expression adβa appears eqs. right multiplying diagonal matrix eﬀect rows corresponding support pairs pairs dropped matrix thus adβa simpliﬁed expressing terms matrix }mβn adβa denotes number support pairs thus objective function derivatives gradient hessian non-linear ssvm share properties corresponding functions linear model. therefore adapt eﬃcient training algorithm linear ssvm small modiﬁcations thereby avoiding explicitly constructing matrix would require space. since derivation non-linear case similar linear case present ﬁnal result refer details. obtained logarithmic time ﬁrst sorting according predicted scores subsequently incrementally constructing order statistic tree hold respectively finally risk score experiencing event data point xnew estimated ¯ncg nnewton average number conjugate gradient iterations total number newton updates respectively non-linear model ﬁrst construct kernel matrix cannot stored memory computing product requires evaluations kernel function operations compute product. evaluating kernel function costs overall complexity thus computing hessian-vector product non-linear case consists three steps following complexities clearly shows that contrast training linear model computing comparable pairs longer time consuming task minimising instead computing dominating factor. number samples training data small kernel matrix computed stored memory thereafter results onetime cost reduces costs compute remaining costs remain same. although pre-computing kernel matrix improvement computing conjugate gradient iteration remains bottleneck. overall complexity training non-linear ssvm truncated newton optimisation order statistics trees note direct optimisation non-linear objective function preferred using kernel transform data training avoids operations corresponding singular value decomposition denoting binary variable denoting drawn binomial distribution probability categorical variable equally distributed levels. addition numeric features sampled multicensoring time drawn uniform distribution interval chosen survival times censored training data. survival times test data subject censoring eliminate eﬀect censoring performance estimation. non-linear model deﬁned correspond dummy codes categorical feature three categories continuous features sampled multivariate normal distribution. note numeric feature associated zero coeﬃcient thus aﬀect survival time. generated pairs train test data samples multiplying real data second experiments focused real-world datasets varying size number features amount censoring framingham oﬀspring coronary artery disease data contained missing values imputed using multivariate imputation using chained equations random forest models ease computational resources validation since missing values problem focus multiple imputed dataset randomly picked analysed. finally normalised continuous variables zero mean unit variance. experiments presented section focus comparison predictive performance survival synthetically generated datasets well real-world data sets alternative survival models regularisation parameter ssvm minlip controls weight hinge loss whereas cox’s proportional hazards model controls weight penalty. optimal hyper-parameters determined grid search evaluating conﬁguration using random splits training data. parameters average performed best across partitions ultimately selected model re-trained full training data using optimal hyper-parameters. used harrell’s concordance index generalisation kendall’s right censored survival data estimate performance particular hyper-parameter conﬁguracross-validation experiments real-world data test data subject censoring hence performance measured harrell’s uno’s c-index integrated area time-dependent cumulativedynamic curve latter evaluated time points corresponding percentile observed time points complete dataset. uno’s c-index truncation time percentile observed time points complete dataset. three measures values indicate completely wrong perfectly correct prediction respectively. finally used friedman’s test nemenyi post-hoc test determine whether performance diﬀerence methods statistically signiﬁcant level synthetic data ﬁrst experiments synthetic data served reference kernel-based survival models compare controlled setup. performed experiments using kernel clinical kernel figure summarises results synthetically generated datasets survival times test data uncensored leads unbiased consistent estimates c-index. experiments revealed using clinical kernel advantageous experiments using clinical kernel combination ssvm models resulted signiﬁcant improvement corresponding model kernel linear model respectively. regarding kernel improved performance linear model except simple ssvm perform signiﬁcantly fig. performance proposed ranking-based survival support vector machine compared kernel-based survival models cox’s proportional hazards model synthetically generated datasets. brackets kernel function used. complexity training non-linear ssvm proposed inadequate. although minlip model based comparable pairs change loss function able compensate degree. proposed optimisation scheme minlip model performed comparably real data section present results real-world datasets based -fold cross-validation clinical kernel preferred feature vectors continuous categorical features demonstrated above. table summarises results. general performance measures correlated well results support conclusions experiments synthetic data described above. simpliﬁed ssvm performed poorly ranked last experiments. particular outperformed linear minlip model outperformed proposed ssvm datasets performed better veteran’s lung cancer data comparable remaining experiments. linear ssvm achieved comparable performance ssvm clinical kernel datasets except coronary artery disease data. finally cox’s proportional hazard model often performed well real-world datasets although model non-linearities explicitly. performance diﬀerence ssvm able consider pairwise relationships training data minlip model considers small subset pairs. turned problematic amount censoring high case aids coronary artery disease studies contained less uncensored records setting training minlip model based much smaller comparable pairs available ssvm ultimately leads minlip model generalises badly. therefore proposed eﬃcient optimisation scheme preferred respect runtime predictive performance. considering experiments together statistical analysis suggests predictive performance survival models comparably. proposed eﬃcient method training non-linear ranking-based survival support vector machines. algorithm straightforward extension previously proposed training algorithm linear survival support vector machines. optimisation scheme allows analysing datasets much larger size previous training algorithms mostly reducing space complexity preferred choice learning survival data high amounts right censoring. opens opportunity build acknowledgments. work supported cruk centre institute cancer research royal marsden heather beckwith charitable settlement john beckwith charitable trust leibniz supercomputing centre data provided framingham heart study national heart lung blood institute national institutes health boston university school medicine", "year": 2016}