{"title": "Beyond Finite Layer Neural Networks: Bridging Deep Architectures and  Numerical Differential Equations", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ($>50$\\%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.", "text": "aoxiao zhong quanzheng mgh/bwh center clinical data science department radiology massachusetts general hospital harvard medical school center data science health medicine peking university beijing china zhongaoxiaogmail.com li.quanzhengmgh.harvard.edu dong beijing international center mathematical research peking university center data science peking university beijing institute data research beijing china dongbinmath.pku.edu.cn deep neural networks become state-of-the-art models numerous machine learning tasks. however general guidance network architecture design still missing. work bridge deep neural network design numerical differential equations. show many effective networks resnet polynet fractalnet revnet interpreted different numerical discretizations differential equations. ﬁnding brings brand perspective design effective deep architectures. take advantage rich knowledge numerical analysis guide designing potentially effective deep networks. example propose linear multi-step architecture inspired linear multistep method solving ordinary differential equations. lm-architecture effective structure used resnet-like networks. particular demonstrate lm-resnet lm-resnext achieve noticeably higher accuracy resnet resnext cifar imagenet comparable numbers trainable parameters. particular cifar imagenet lm-resnet/lm-resnext signiﬁcantly compress original networks maintaining similar performance. explained mathematically using concept modiﬁed equation numerical analysis. last least also establish connection stochastic control noise injection training process helps improve generalization networks. furthermore relating stochastic training strategy stochastic dynamic system easily apply stochastic training networks lm-architecture. example introduced stochastic depth lm-resnet achieve signiﬁcant improvement original lm-resnet cifar. deep learning achieved great success machine learning tasks. end-to-end deep architectures ability effectively extract features relevant given labels achieve state-of-the-art accuracy various applications network design central task deep learning. main objective grant networks strong generalization power using parameters possible. ﬁrst ultra deep convolutional network resnet skip connections keep feature maps different layers scale avoid gradient vanishing. structures skip connections resnet also introduced avoid gradient vanishing dense connections fractal path dirac initialization furthermore attempts improve accuracy image classiﬁcations modifying residual blocks resnet. zagoruyko komodakis suggested need double number layers resnet achieve fraction percent improvement accuracy. proposed widened architecture efﬁciently improve accuracy. zhang pointed simply modifying depth width resnet might best architecture design. exploring structural diversity alternative dimension network design lead effective networks. szegedy zhang authors improved accuracy networks carefully designing residual blocks increasing width block changing topology network following certain empirical observations. literature network design mainly empirical.it remains mystery whether general principle guide design effective compact deep networks. observe residual block resnet written step forward euler discretization ordinary differential equation suggests might connection discrete dynamic systems deep networks skip connections. work show many state-of-the-art deep network architectures polynet fractalnet revnet consider different discretizations odes. perspective work success networks mainly ability efﬁciently approximate dynamic systems. side note differential equations powerful tools used low-level computer vision image denoising deblurring registration segmentation also bring insights success deep neural networks low-level computer vision. furthermore connection architectures deep neural networks numerical approximations odes enables design effective deep architectures selecting certain discrete approximations odes. example design network structure called linear multi-step architecture inspired linear multi-step method numerical odes architecture applied resnet-like networks. paper apply lm-architecture resnet resnext achieve noticeable improvements cifar imagenet comparable numbers trainable parameters. also explain performance gain using concept modiﬁed equations numerical analysis. known literature introducing randomness injecting noise forward process improve generalization deep residual networks. includes stochastic drop residual blocks stochastic shakes outputs different branches residual block work show resnet-like network noise injection interpreted discretization stochastic dynamic system. gives relatively uniﬁed explanation stochastic learning process using stochastic control. furthermore relating stochastic training strategy stochastic dynamic system easily apply stochastic training networks proposed lm-architecture. example introduce stochastic depth lm-resnet achieve signiﬁcant improvement original lm-resnet cifar. link resnet odes ﬁrst observed authors formulated continuum limit resnet liao poggio bridged resnet recurrent neural network latter known approximation dynamic systems. sonoda murata also regarded resnet dynamic systems characteristic lines transport equation distribution data set. similar observations also made chang designed reversible architecture grant stability dynamic system. hand many deep network designs inspired optimization algorithms network lista widely used low-level computer vision tasks image restoration. recent attempts combining deep learning pdes various computer vision tasks i.e. balance handcraft modeling data-driven modeling. proposed linear combinations series handcrafted pde-terms used optimal control methods learn coefﬁcients. later fang extended model handle classiﬁcation tasks proposed learned model however classiﬁcation tasks dynamics interpreted characteristic lines distribution data set. means using spatial differential operators network essential classiﬁcation tasks. furthermore discretizations differential operators l-pde trainable signiﬁcantly reduces network’s expressive power stability. chen proposed feed-forward network order learn optimal nonlinear anisotropic diffusion image denoising. unlike previous work network used trainable convolution kernels instead ﬁxed discretizations differential operators used radio basis functions approximate nonlinear diffusivity function. recently long designed network called pde-net learn general evolution pdes sequential data. learned pde-net accurately predict dynamical behavior data potential reveal underlying model drives observed data. work focus different perspective. first require associate optimization problem assume differential structures optimal learned given supervised learning task. secondly draw relatively comprehensive connection architectures popular deep networks discretization schemes odes. importantly demonstrate connection deep networks numerical odes enables design effective deep networks. example introduce lm-architecture resnet resnext improves accuracy original networks. also note that viewpoint enables easily explain resnet achieve good accuracy dropping residual blocks training whereas dropping sub-sampling layers often leads accuracy drop simply residual block step discretized hence dropping residual blocks amounts modifying step size discrete dynamic system sub-sampling layer part model. explanation similar unrolled iterative estimation proposed greff difference believe data-driven unrolled iterative estimation. section show many existing deep neural networks consider different numerical schemes approximating odes form based observation introduce structure called linear multi-step architecture inspired well-known linear multi-step method numerical odes. lm-architecture applied resnet-like networks. example apply resnet resnext demonstrate performance gain modiﬁcation cifar imagenet data sets. polynet proposed zhang introduced polyinception module residual block enhance expressive power network. polyinception model includes polynomial compositions described therefore architecture polynet viewed approximation backward euler scheme solving note that implicit scheme allows larger step size turn allows fewer numbers residual blocks network. explains polynet able reduce depth increasing width residual block achieve state-of-the-art classiﬁcation accuracy. fractalnet designed based self-similarity. designed repeatedly applying simple expansion rule generate deep networks whose structural layouts truncated fractals. observe that macro-structure fractalnet interpreted well-known runge-kutta scheme numerical analysis. recall recursive fractal structure simplicity presentation fractalnet written demonstrate fractalnet order then every block fractalnet expressed resembles runge-kutta scheme order solving revnet) proposed gomez reversible network require store activations forward propagations. revnet expressed following discrete dynamic system note reversibility means simulate dynamic time initial time also important notation dynamic systems. also attempts design reversible scheme dynamic system nguyen mcmechan shown architectures successful deep neural networks interpreted different discrete approximations dynamic systems. section proposed structure called linear multi-step structure based well-known linear multi-step trainable parameter layer schematic lm-architecture presented figure note midpoint leapfrog network structures chang special case ours. lm-architecture -step method approximating therefore applied resnet-like networks including mentioned previous section. example apply lm-architecture resnet resnext. call networks lm-resnet lm-resnext. trained lm-resnet lm-resnext cifar imagenet achieve improvements original resnet resnext. implementation details. data sets cifar cifar train test networks training testing originally given data set. imagenet models trained training million images evaluated validation images. cifar follow simple data augmentation training pixels padded side crop randomly sampled padded image horizontal ﬂip. testing evaluate single view original image. note data augmentation used resnet imagenet follow practice krizhevsky simonyan zisserman images resized shorter side randomly sampled scale augmentation input image randomly cropped resized image using scale aspect ratio augmentation szegedy experiments resnet/lm-resnet cifar adopt original design residual block i.e. using small two-layer neural network residual block bn-relu-conv-bn-reluconv. residual block lm-resnext bottleneck structure used takes form start networks single conv layer followed residual blocks global average pooling fully-connected classiﬁer. parameters lm-architecture initialized random sampling initialize parameters following method introduced cifar mini-batch size imagenet. training apply weight decay lm-resnet lm-resnext momentum cifar. apply weight decay momentum lm-resnet lm-resnext imagenet. lm-resnet cifar start learning rate divide epochs terminate training epochs. lm-resnext cifar start learning rate divide epochs terminate training epochs. results. testing errors proposed lm-resnet/lm-resnext deep networks cifar presented table figure shows overall improvements lm-resnet resnet cifar varied number layers. also observe noticeable improvements lm-resnet lm-resnext cifar. claimed resnext achieve lower testing error without pre-activation however results show lmresnext pre-act achieves lower testing errors even original resnext without pre-act. training testing curves lm-resnext plotted figure. table also present testing errors fractalnet densenet cifar proposed lm-resnext best result. comparisons lm-resnet resnet imagenet presented table lm-resnet shows improvement resnet comparable number trainable parameters. note results resnet imagenet obtained https//github.com/kaiminghe/deep-residual-networks. worth noticing testing error -layer lm-resnet comparable -layer resnet cifar; testing error -layer lm-resnet comparable -layer resnet cifar; testing error -layer lm-resnet comparable -layer resnet imagenet. similar results lm-resnext resnext well. results indicate lm-architecture greatly compress resnet/resnext without losing much performance. justify mathematically section using concept modiﬁed equations numerical analysis. explanation performance boost modiﬁed equations. given numerical scheme approximating differential equation associated modiﬁed equation another differential equation numerical scheme approximates higher order accuracy original equation. modiﬁed equations used describe numerical behaviors numerical schemes. example consider simple -dimensional transport equation cux. lax-friedrichs scheme lax-wendroff scheme approximates transport equation. however associated modiﬁed equations lax-friedrichs lax-wendroff uxxx respectively shows lax-friedrichs scheme behaves diffusive lax-wendroff scheme behaves dispersive. consider forward euler scheme associated resnet un+−un comparing second order term bigger term represents acceleration leads acceleration convergence boyd wilson elliptic operator term introduce dispersion dissipation speeds fact original motivation introducing lm-architecture note dynamic truly gradient i.e. difference equation lm-structure stability condition experiments observe coefﬁcients lying moreover network indeed accelerating dynamic learned parameters {kn} negative close although original resnet dropout several work showed also beneﬁcial inject noise training. section show regard stochastic learning strategy approximation stochastic dynamic system. hope stochastic dynamic system perspective shed lights discovery guiding principle stochastic learning strategies. demonstrate advantage bridging stochastic dynamic system stochastic learning strategy introduce stochastic depth training lm-resnet. results indicate networks proposed lm-architecture also greatly beneﬁt stochastic learning strategies. example show stochastic learning methods introduced huang gastaldi considered weak approximations stochastic dynamic systems. shake-shake regularization. gastaldi introduced stochastic afﬁne combination multiple branches residual block expressed dimensional brownian motion n-dimensional vector whose elements dimension denotes pointwise product vectors. note alternatives original shake-shake regularization choose stochastic depth. huang randomly drops residual blocks training order reduce training time improve robustness learned network. write forward propagation ηn−pn reduces original stochastic drop training variance assume condition appendix satisﬁed small then following discussion appendix network stochastic drop seen weak approximation stochastic dynamic system section extend stochastic depth training strategy networks proposed lmarchitecture. order apply theory itˆo process consider order rewrite order system derivation suggests stochastic learning networks using lm-architecture implemented simply randomly dropping residual block probability implementation details. test lm-resnet stochastic training strategy cifar. experiments hyper-parameters selected exactly probability dropping residual block layer linear function layer i.e. current layer probability dropping current residual block network depth network dropping probability previous layer. experiments select lm-resnet lm-resnet. training initial learning rate divided factor epoch terminated epochs. addition weight decay momentum results. testing errors presented table training testing curves lm-resnet stochastic depth plotted figure. note lm-resnet stochastic depth training strategy achieved testing error cifar even lower resnet reported original paper. beneﬁt stochastic training explained difference perspectives bayesian information theory stochastic brownian motion involved aforementioned stochastic dynamic systems introduces diffusion leads information gain robustness. paper draw relatively comprehensive connection architectures popular deep networks discretizations odes. connection enables design effective deep networks. example introduce lm-architecture resnet resnext improves accuracy original networks proposed networks also outperform fractalnet densenet cifar. addition demonstrate networks stochastic training process interpreted weak approximation stochastic dynamic systems. thus networks stochastic learning strategy casted stochastic control problem hope shed lights discovery guiding principle stochastic training process. future work odes considered continuum limits deep neural networks tools mathematical analysis used study neural networks. apply geometry insights physical laws smart design numerical schemes design effective deep neural networks. hand numerical methods control theory inspire optimization algorithms network training. moreover stochastic control gives perspective analysis noise injections network training. dong supported part nsfc national research development program china yfc. yiping supported elite undergraduate training program school mathematical sciences peking university. quanzheng supported part national institutes health grant grant rag. ascher linda petzold. computer methods ordinary differential equations differential-algebraic equations. siam society industrial applied mathematics gilles aubert pierre kornprobst. mathematical problems image processing partial differential equations calculus variations volume springer science business media chang lili meng eldad haber lars ruthotto david begert elliot holtham. reversible architectures arbitrarily deep residual neural networks. arxiv preprint arxiv. aidan gomez mengye raquel urtasun roger grosse. reversible residual network backpropagation without storing activations. advances neural information processing systems kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. ieee international conference computer vision alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition saining ross girshick piotr dollr zhuowen kaiming aggregated residual transformations deep neural networks. ieee conference computer vision pattern recognition xingcheng zhang zhizhong change chen dahua lin. polynet pursuit structural diversity deep networks. ieee conference computer vision pattern recognition section brieﬂy recall concepts numerical odes used paper. consider takes form interested readers consult ascher petzold comprehensive introduction subject. forward backward euler method simplest approximation discretize time derivative un+−un approximate right hand side leads forward euler scheme here intermediate approximation solution time +cj∆t coefﬁcients {cj} adjusted achieve higher order accuracy. example popular nd-order runge-kutta takes form here following deﬁnition itˆo integral btn+ gaussian random variable variance known forward euler scheme converges strongly itˆo process. note replace random variable non-gaussian distribution forward euler scheme becomes so-called simpliﬁed weak euler scheme. simpliﬁed weak euler scheme converges weakly itˆo process satisﬁes following condition", "year": 2017}