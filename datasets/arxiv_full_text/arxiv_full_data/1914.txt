{"title": "Deep Learning Applied to Image and Text Matching", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images.In this project we focus on the task of bidirectional image retrieval: such asystem is capable of retrieving an image based on a sentence (image search) andretrieve sentence based on an image query (image annotation). We present asystem based on a global ranking objective function which uses a combinationof convolutional neural networks (CNN) and multi layer perceptrons (MLP).It takes a pair of image and sentence and processes them in different channels,finally embedding it into a common multimodal vector space. These embeddingsencode abstract semantic information about the two inputs and can be comparedusing traditional information retrieval approaches. For each such pair, the modelreturns a score which is interpretted as a similarity metric. If this score is high,the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to.  The visual input is modeled via deep convolutional neural network. On theother hand we explore three models for the textual module. The first one isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a combination of trigram & skip-grams) with an MLP. The third is morespecialized deep network specific for modeling variable length sequences (SSE).We report comparable performance to recent work in the field, even though ouroverall model is simpler. We also show that the training time choice of how wecan generate our negative samples has a significant impact on performance, and can be used to specialize the bi-directional system in one particular task.", "text": "deep learning images computer vision deep learning text deep learning image text multimodal models introduction evaluation metrics image text mapping sentence generation images ability describe images natural language sentences hallmark image language understanding. system wide ranging applications annotating images using natural sentences search images. project focus task bidirectional image retrieval system capable retrieving image based sentence retrieve sentence based image query present system based global ranking objective function uses combination convolutional neural networks multi layer perceptrons takes pair image sentence processes diﬀerent channels ﬁnally embedding common multimodal vector space. embeddings encode abstract semantic information inputs compared using traditional information retrieval approaches. pair model returns score interpretted similarity metric. score high image sentence likely convey similar meaning score likely visual input modeled deep convolutional neural network. hand explore three models textual module. ﬁrst words mlp. second uses n-grams mlp. third specialized deep network speciﬁc modeling variable length sequences report comparable performance recent work ﬁeld even though overall model simpler. also show training time choice generate negative samples signiﬁcant impact performance used specialize bi-directional system particular task. would like thank epfl giving opportunity work interesting project. special thanks bing iain melvin eric cosatto igor durdanovic martin renqiang many fruitful discussions. ability describe images natural language sentences hallmark image language understanding. signiﬁcant applications searching images natural sentences automatic captioning images. currently commercially available image searching systems search text adjacent image instead searching content image itself. severe bottleneck performance systems. advances ﬁeld far-reaching eﬀects since images ubiquitous internet always interacting them. humans describe image relative ease. however computers trivial task. diﬃculty arises mainly input modalities diﬀerent statistical properties cannot directly compared. example images spacial dependency sentences temporal dependency even humans task highly subjective nature various forms descriptions exist. diﬀerent descriptions could focus diﬀerent aspects diﬀerent objects image could also diﬀerent level detail describing content could abstract like describing mood concrete like describing objects. could correct simultaneously correct diﬀerent. modeling variance important machine learning systems achieved high-quality data sets multiple humans describe image. recent work ﬁeld focus approaches multimodal retrieval sentence generation. multimodal retrieval deals tasks retrieving sentences image queries retrieving images sentence queries hand image based sentence generation systems focus creating natural ﬂuent sentences directly image describe contents image. design system taking motivation traditional retrieval systems particular supervised semantic indexing system used document retrieval. expand system using separate specialized textual visual models. models extract semantic information input sentence image transform common vector space easily compare modalities using traditional information retrieval approaches. goal project explore understand signiﬁcance deep learning techniques task. deep learning architectures concepts within individual text image modules. visual component experiment using deep convolutional neural networks become synonymous image recognition textual component experiment diﬀerent features network architectures. word approach n-gram models tf-idf features convolutional neural network sequence embedding. contrast previous work model simpler since don’t complicated models like recurrent neural networks recursive neural networks however even simpler approach model achieves comparable performance recent work ﬁeld. rest thesis report organized follows. chapter present overview available scientiﬁc literature ﬁeld. start deep learning historical perspective discuss main ideas signiﬁcance ﬁrst image community text processing. finally discuss recent previous work combined ﬁeld images texts multimodal input. following this chapter brieﬂy describe important resources used project. includes data sets trained tested model publicly available tools used larger model. chapter provide overview research methodology time line experiments conducted. chapter serves optional section. reader skip without loosing necessary information understanding ﬁnal models experiments material covers experience problem inﬂuenced approached task moving forward chapter deals bulk experimentation done model results get. start discussing preprocessing give details evaluation metrics used meta parameters selected. finally conduct several experiments report results. important aspect deep learning end-to-end automatically trainable systems rely human-designed heuristics. traditional machine learning systems divided modules. first eatureextractor module transforms input data dimensional vectors easily matched compared relatively invariant distortions. classif module general-purpose trainable. major problem approach performance largely determined human input feature extractor part task-speciﬁc needs redone every little task. countering traditional approach showed handcrafted feature extraction advantageously replaced automatic learning algorithms operate directly input data. individual modules computer vision especially object recognition long history using deep multi-layered neural networks. comparison hand-written digit recognition mnist data compare performance multi-layered convolutional neural network traditional machine learning approaches like linear classiﬁer principal component analysis nearest neighbour classiﬁer. comparison shows deep cnns outperform traditional algorithms object recognition tasks reach state-of-the-art. convolutional neural networks ﬁrst used deep learning models biologically inspired variants ann. found existence receptive ﬁelds visual cortex brain cnns emulate behaviour. words specialized network architectures speciﬁcally designed recognize dimensional objects invariant exact position pattern distortions. unit takes input local receptive ﬁeld layer forcing extract local features. furthermore units within plane eaturemap constrained share single weights makes operations performed feature shift invariant weight sharing technique also reduce number free parameters thereby reducing memory requirements training complexity networks. complete cnns formed stacking together multiple convolutional layers sub-sampling layers also added improving invariance shift distortions. entire network trainable gradient descent using back-propagation procedure. popularized lenet- architecture performing state-of-the-art mnist hand writing recognition time published. even evident deeper larger networks tendency perform better. however potential overshadowed outrageous time memory took train larger networks possible time. moreover larger networks powerful modeling also easily overﬁt training data eﬃcient techniques combat annoyance. even though near human performance reached simple task like hand written recognition reciprocated objects recognition realistic settings exhibit considerable variability. well understood training complicated data high variability using examples leads severe overﬁtting cnns release large-scale data step object recognition problem. time computer hardware technology also progressed enough train much larger deeper networks reasonable amount time. used graphical processor units fast eﬃcient implementation neurons million parameters entered neural network ilsvrc- competition achieved winning top- test error rate considerably better state besides larger data larger networks also empirically showed importance useful techniques avoiding overﬁtting. easiest illustrated data augmentation increase size data set. take diﬀerent patches original image thus increasing training data factor although additional images close original one. also also alter intensities colour channels data augmentation. techniques useful adding shift inversion illumination colour invariance model. dropout another technique combating overﬁtting reducing complex-co adaptions neurons. therefore single neuron forced learn robust features without relying neighbouring neurons. pretraining neural network greedy layer-by-layer fashion unsupervised objective function another popular technique intuition behind idea unsupervised training give good initialization weights neural network based actual statistical properties data used instead random initializations often stuck poor local minimas. following network ine− tuned supervised task object recognition. mathematically speaking transforms dimensional feature vector representation. good model also good feature extractor images resulting images used complicated tasks. display concept object localization. train classiﬁcation imagenet data localization take replace ﬁnal classiﬁcation layer regressionnetwork. regression network simply hidden layers units connected ﬁnally output layer units predicts coordinates bounding boxes. ﬁnal layer class-speciﬁc versions rest regression network shares weights. localization regression network’s weights updated remaining larger network acts feature extractor. classiﬁer regressor simultaneously since share network bounding class along conﬁdence number based classiﬁcation conﬁdence. seminal paper domain statistical learning applied natural language processing written classical machine learning approaches calculate n-gram conditional probabilities based simply co-occurrence frequencies words document. models construct tables conditional probabilities next given ﬁxed context fundamental problem curse dimensionality meaning possible combinations contexts grows exponentially thus making models quickly intractable. another problem size context increases occurrence sequences gets extremely rare document thus undermining statistical relevance probability distributions. authors paper attempt solve problem using neural network learn distributed representation words. distributed representation sometimes termed word embedding modern literature feature vector conveys semantic syntactic meaning word. words words vocabulary transformed vector representation ﬁxed dimension furthermore joint probability entire sequence expressed function feature vectors. paper propose feed forward neural networks compute probability next word sequence given previous words advantage neural networks trained trained jointly learn word feature vectors parameters probability function using common global objective function advantage using word vectors would escape curse dimensionality. words would represented ﬁxed real-valued low-dimensional vector similar words would similar meanings. example given paper sentences walking bedroom\" would similar representation \"the running room\" since model would learn similarity individual words sequence etc. noteworthy report improvements terms perplexity best n-gram results task predicting next word sequence performed large scale data sets. idea using neural networks language modeling fact dates back however authors paper discussion ﬁrst propose large scale statistical model learns distributed dense representations words sequence using automatically estimate joint probability function. building this propose single uniﬁed convolutional neural network architecture performs well various challenging tasks part-of-speech tagging chunking named entity recognition semantic role labeling. traditional approaches analyze tasks separately using hand-crafted features speciﬁc task makes approach intractable complicated tasks. model propose contrast deep architecture composing many layers trained end-to-end fashion. ﬁrst layer extracts features word second layer extracts features form sentence treating sequence structure. variable length inputs incorporated passing convolutional neural network word features performing max-pooling resulting dimension give length vector. following layers classical layers since tasks related argue would make sense share features tasks order improve generalization network propose multitask learning approach jointly train model tasks. design network architecture share layers closer input deeper network features extracted become complex abstract last layers network task speciﬁc. also pretrain network unlabeled training data since available much vast quantities compared labeled data. train language model unsupervised ranking criterion would predict middle word sequence related context not. positive examples took ﬁxed length phrases wikipedia generated negative examples substituting middle word valid phrase random word. demonstrated approach trains powerful language model showing word vectors close another embedding space also close semantic meaning. example \"france\" \"spain\" \"italy\" close vector representations another well \"scratched\" \"smashed\" \"ripped\". vector representations word. propose skip gram model predicts surrounding words window given center word. directly opposite earlier approaches predict centre word giver context window. given sequence training words objective criterion maximize average probability words surrounding conditional centre word. interesting feature model preserves linear regularities among learned representations. makes possible perform interesting analogical reasoning using simple vector arithmetic. example result vector calculation closest vec. also close vec. furthermore extends previous model include vector representations phrases well. based work insight idiomatic phrases like \"boston globe\" \"air canada\" semantically understood well combining individual words within phrases. therefore treated phrases individual tokens limiting vocabulary phrases appear frequently together infrequently contexts. train vector embeddings dimensionality words phrases release internet public progress multi-label classiﬁcation problem associating images individual words tags. however challenging problem associating images complete natural sentences recently started gain attention. research area focused primarily tasks namely available data sets analysis modeling techniques employed early stages ﬁeld detailed discussion various evaluation metrics used diﬀerent previous related works. conceptual descriptions identify depicted image. concerned concrete descriptions depicted scenes entities attributes relations well events participate argue three conceptual descriptions relevant image understanding tasks. observe using user generated captions uploaded images popular image-sharing websites serve good training data people tend provide information could easily obtained looking image itself. example kind description models require \"three people setting tent\" people tend provide captions like \"our trip olympic peninsula\". hence establish need data collected purposefully speciﬁc task. good quality data sets collected visa crowdsourcing multiple descriptive captions assigned image. pascalk flickrk flickrk ms-coco examples good quality data sets. earliest work ﬁeld used shallow learning techniques ﬁxed image text features. nearest neighbour search image annotation image description respectively. hand kernel canonical correlation analysis kernel conanical correlation analysis kcca technique takes training data consisting pairs corresponding items drawn diﬀerent modalities ﬁnds maximally correlated linear projections items newly induced common space. similarly popular shallow image features include sift descriptors simple words kernel. evaluation metrics since image description subjective task ideal setting evaluating system would averaged human judgement. however since human judgement expensive slow number metrics employed evaluating systems. metrics divided categories metrics text generation systems. metrics ranking systems. bleu rouge scores popular metrics automatics image description generation systems. originally standard metrics machine translation summarization respectively used evaluate multiple caption generation systems given caption image reference captions bleu score proposed image-caption pair based n-gram precision rouge based corresponding n-gram recall. number times word occurs deﬁned compare bleu rouge scores human judgements examine extents agree. based results question metrics’ usefulness evaluating caption generation systems. also argue useful metrics ﬂuency poor measures content quality language generation. however unless suited metric devised score continue used evaluating modern caption generation systems. next touch upon metrics used evaluate quality ranked list information retrieval tasks. recallk percentage test queries model returns correct result among results. especially useful context search user satisﬁed ﬁrst results containing single relevant item. conversely median rank equal value equal typically varies consider strict threshold comparing human judgement view lower bound actual performance. systems queries multiple relevant answers since test image associated multiple relevant captions test caption deem multiple images besides originally written for. r-precision metric choice conditions since single number allows rank models according overall performance r-precision system query known relevant results deﬁned precision rank simpler terms percentage relevant items among responses returned system. ﬁrst ones deep learning ranking images text.they call model devise deep visual-semantic embedding. original task improve performance image classiﬁcation system large number object categories labels visual system trained propose leverage information textual information image improve object classiﬁcation performance. begin pre-training eﬃcient deep convolutional neural network visual object recognition based architecture parallel pretrain simple neural language model wellsuited learning semantically-meaningful vector representations individual words using skip-gram text modeling architecture following this construct devise model chopping layers re-training predict word embedding vector corresponding image label. used hinge margin ranking loss criterion second phase training observe signiﬁcant improvements using loss criterion. although never trained tested system natural image descriptions inﬂuence work ﬁeld. implemented model image sentence mapping compare performance system. modeling images image based sentence retrieval system. ﬁrst embed image sentence common space rank pair. perform comparison using activations image features versus kcca dimension ﬁxed features. reported improvement recall kcca even activations remained ﬁxed ﬁne-tuned image-text data set. unlike previous work ﬁner level fragments images fragments sentences common embedding space. model works bi-directional retrieval image given text query text given image query. interpret image made multiple entities therefore propose break manageable fragments.artiﬁcial neural network used compute vector representation image sentence fragments multimodal embedding space product pair vectors interpretted compatibility score. global image-sentence compatibility score computed ﬁxed function fragments. detects objects images returns bounding boxes. image fragments locations detected rcnn complete image image broken fragments. following another applied fragments return dimensional embedding vector representing image fragment. architecture closely resembles hand sentence fragments considered edges sentence’s dependency tree. therefore sentence fragment consists words joined stage dependency tree. word dictionary represented using -of-k encoding vector embeddings words obtained unsupervised objective ﬁxed throughout training. sentence fragment score calculated using word embeddings well separate embeddings represent type relation words. plot matrix rows represent image fragments columns represent sentence fragments. element matrix shows product score multimodal fragments. deﬁne kinds objective functions train models fragment alignment objective objective explicitly learns representation sentence fragments visual domain. encodes intuition sentence fragment contained image least boxes give high score sentence fragment boxes corresponding images contain sentence fragment score fragment. also favours least high scoring column matrix model improves performance using combination objective functions. also report dividing image fragments performance improvement versus treating entire image single fragment dependency tree sentence fragments perform consistently better words bigram features. finally found ﬁne-tuning image score calculating image-text data improves results. step ahead entire sentence dependency tree opposed edges sentence fragments model sentences using dt-rnn argue important accurately representing complicated sentences visual domain. test model recurisive recurrent neural networks kcca baseline concluding dt-rnn performs task image sentence mapping. dt-rnn also give similar vector representations multiple captions describe image adding weight model. dt-rnn diﬀerent previous recursive neural network models based constituency tree report sentence vectors computed dt-rnn capturing meaning sentence terms \"visual representation\". moreover dt-rnn vector embeddings robust changes syntactic structure word order opposed ct-rnns recurrent neural networks. ﬁnal sentence embedding vector length image side train deep ﬁrst using unsupervised objective reconstruct input keeping neurons sparse followed supervised learning classifying million images imagenet categories. used particularly large billion parameters achieved precision full imagenet data set. following training chop last layer dimensional vector embeddings images. multimodal mapping transform image representation vector size sentence vector. following like similar works take product produce compatibility score back propagate error using margin ranking loss criterion. however dimensional image vector dimensional sentence vector ﬁxed updates phase. report improved results pascalk data compared words ct-rnn recurrent kcca models. build upon prior work image text matching using fragments image sentence features increase modeling capacity model. initial approach translated words directly vector embeddings consider word ordering context. address problem bi-directional recurrent neural network model input text sequence convert complete sentence embedding. report signiﬁcant improvements using approach. taking somewhat diﬀerent approach multimodal deep boltzmann machines task. model learns joint probability density space multimodal inputs. missing modalities ﬁlled sampling conditional distributions them. example learn joint probability where vimg vector representation image vtxt vector representation text sentence. distribution estimated draw samples conditional probabilities missing modalities drawing samples give missing text sentence vice versa. multimodal extension model multimodal inputs. formed stacking together rbms words multilayer usually trained greedy layer-wise strategy. construct independent dbms image-speciﬁc text-speciﬁc dbm. next connected together another layer construct multimodal dbm. words outputs dbms connected another layer binary hidden units them called \"joint representation\". intuition behind model data modality diﬀerent statistical properties make diﬃcult single hidden layer model directly correlations across modalities. diﬃculty overcome putting layers hidden units inputs modalities evaluate model information retrieval multimodal unimodal queries. multimodal query give higher similarity score image text pairs belonging instance false pairings. unimodal query either text image provided model predicts missing modality pool possible options. brieﬂy touch upon related task generating natural sentences images. although focus project still interesting approach taken area especially since growing interest recently. present model convolutional neural network task. four input pathways image remaining three words. idea that given image three previous words model learns predict next word sequence. learn joint \"multimodal language model\". recurrent neural networks quite popular text generation many researchers task albeit diﬀerent settings inﬂuenced modern based machine translation systems employ encoderdecoder type architecture model. speciﬁcally encoder decoder. encodes input image vector space feature embeddings input rnn. takes image encoding input word generated current time step generate complete sentence word time. special words marked tell model starting word predicting ﬁnishing word. also long short term memory inside memory older generated words. train model directly optimize likelihood target sentence given input image. using similar approach also trained multimodal inputs. input image feature vector current generated word learns predict next word sequence employed various data sets resources research experimentation. provide brief description them. high quality data collected crowd sourcing methods labeled human input. although large-scale automatically generated data sets exist however makes convincing case lack pertinence captions objectives task hence flickrk data comprising images collected flickr website image captions along describe contents image. images data focus people animals performing action. images tend contain well known locations manually selected depict variety scenes situations. images captioned human captioners using crowdsourcing amazon’s mechanical turk order avoid grammar mistakes captioners pass brief quality control test based spelling grammar. following this asked write sentences describe depicted scenes situations events entities multiple captions collected model inherent variability diﬀerent humans describing image. consequence captions images often direct paraphrases other entity event worth mentioning accessing data found several images removed flicker could download total images. therefore although judge performance algorithm data cannot comparison previous work. flickrk extension flickrk data comprising images collected similar counterpart image associated captions written humans using amazon’s mechanical turk. images consists everyday activities events scenes. important annotators familiar speciﬁc entities circumstances depicted them avoid give overly speciﬁc annotations. example annotations \"three people setting tent\" relevant task compared \"our trip olympic peninsula\" moreover diﬀerent annotators diﬀerent levels speciﬁcity describing overall situation speciﬁc actions .this variety descriptions associated image necessary induce similarities expressions trivially related syntactic rewrite rules. overfeat publicly available visual feature extractor based convolutional neural network submitted ilsvrc- large-scale object recognition competition. time release ranked classiﬁcation localization detection tasks ilsvrc- data sets. accurate version model million free parameters billion connections reaches performance competition validation set. network architecture similar architecture addition trained images multiple scales improved inference step. http//nlp.cs.illinois.edu/hockenmaiergroup/k-pictures.html http//shannon.cs.illinois.edu/denotationgraph/ http//cilvr.nyu.edu/doku.php?id=softwareoverfeatstart wordvec publicly available tool provides eﬃcient implementation learning continuous word skip-gram based vector representations words. model implementation based work released addition implementation also provide vector representations words phrases learned training model google news dataset -dimensional vectors million words phrases. interesting feature vector representations capture linear regularities language. example result vector calculation closest vec. section provide overview research methodology time line experiments conducted. experiments mentioned chapter directly involved ﬁnal results reader skip chapter wish material deemed necessary understand model reiterated next chapter. reason chapter’s inclusion make reader understand experience problem inﬂuenced approached task arrived ﬁnal model. decided treat problem image descriptions information retrieval task decided follow ranking approach matching images sentences. decision inﬂuenced prior work relating system document retrieval based query sentence. basic idea generate vector embeddings diﬀerent texts contain semantic information original text. embeddings abstraction content text used similarity comparisons diﬀerent texts. quite similar problem diﬀerence inputs multimodal. query result textual nature text image. presence multimodal inputs makes task much challenging therefore would need change architecture cater complexity. interested making model bi-directional meaning would able retrieve image textual sentence query sentence image query following goal make model diﬀerent signiﬁcant ways architecture divide network visual model textual model. visual model images input textual model images. goal model eﬃciently extract semantic features respective input signals. make overall architecture deeper. since inputs belong diﬀerent modalities diﬀer statistical properties transforming common embedding space would challenging would need modeling power deeper networks follow guidelines design model. visual model initialized random weights. textual model simple single hidden layer treat sentence words trained model endto-end fashion margin ranking criterion. objective criterion maximize distance positive negative example negative example generated making random false pairing sentence image training examples iteration give positive example randomly generated negative example. gradient back propagated entire network train end-to-end manner. however model failed perform well trained fickrk data set. training error decreased increasing number epochs error validation error seem remain close random illustrated figure tried analyze results investigate possible reason failure. changing diﬀerent hyper parameters like learning rate hidden layers hidden units seem produce signiﬁcant eﬀect. since similar architecture seems perform well textual input data could think three possible reasons models failure used made convolutional blocks followed fully connected layers. convolution block convolution layer non-linearity layer sub-sampling layer dropout layer normalization layer. decided ﬁrst verify working expected testing figure model fails perform flickrk data set. error percentage samples model gives higher score positive sample negative sample. error speciﬁes random performance. isolation. using much simpler task classiﬁcation handwritten digits mnist data base. model seemed perform well without tweeking much parameters verifying least functioning properly following tried complicated task classiﬁcation objects categories caltech data set. however model failed perform well task illustrated figure concluded reason performance flickrk ranking task caltech classiﬁcation task reason. able model complicated data well enough. words since number images relatively limited amount variation images rather high able generalize model relevant statistical properties well enough. therefore concluded needed pre-train network larger data attempting train problems. advantage pre-training would model would good initialization would able extract good visual features images could used task. instead wasting scarce time pretrain network scratch much larger data decided publicly available overfeat details model refer chapter however plugging overfeat network directly original task decided ﬁrst test classifying caltech data chopped last softmax layer overfeat replaced small neural network parameters ﬁxed ﬁne-tuned data parameters small network updated. model managed classify caltech signiﬁcantly better original randomly initialized also decided small experiment test image text multimodal ranking approach based caltech data set. textual input used class labels corresponding image incorporated easily since simple words model. often label hyphenated like \"baseball-bat\" \"bear-mug\"; cases decided break individual words total vocabulary size words. satisfaction ranking task also performed well reinforcing conﬁdence model. results shown therefore ﬁnally decided experiments overfeat model. figure overfeat performed well ranking task caltech data set. error percentage samples model gives higher score positive sample negative sample. error speciﬁes random performance. model inspired previous work textual retrieval system. extend model multimodal inputs particular images text. focus bidirectional retrieval. words retrieving images based sentence query retrieving sentences based image query sentences complete natural constructed human annotators opposed individual isolated tags. overall abstract-level architecture complete model illustrated figure sub-models identical architecture share weights. pair text image input calculate score measure similarity corresponding text image spos similarity score positive input pair sneg similarity score negative input pair. positive pair means sentence positively describes image negative pair means image sentence related pair generated simply replacing sentence positive pair another random sentence iteration simultaneously present positive pair negative pair respectively. objective criterion penalizes spos greater certain threshold sneg. intuitively means system trained give high score similar pair sentence image score dissimilar pair. therefore network learns score semantic relevance pair multimodal inputs. model natural variance within descriptions. sentences training process image samples times full epoch believe extremely important sentences make model robust variance natural language. figure overall abstract level architecture complete multimodal retrieval model. share weights. generate score based similarity given pair text image input next lower level abstraction discuss inside model makes illustrated figure system textual model visual model replaces given sentence image anns trained transform dimensional high-quality vector embeddings. vtxt signiﬁes vector embedding generated textual input vimg vector embedding visual input. high-quality mean vectors embedded common embedding space encodes high level semantic information contents image text ignoring feature level details advantage multimodal input emerrori denotes global objective function. given margin threshold positive input score sipos least bigger negative input score sipos error gradient propagates throughout architecture. shown equation below details visual textual models following deep learning approach neural networks visual textual models. anns trained end-to-end manner optimizing global objective function require minimal preprocessing. also discussed chapter various architectures achieve state results various machine learning tasks. hope reciprocate results task. visual model convolutional neural network since become ubiquitous image recognition tasks. discussed chapter experimented training scratch data failed achieve good results. experiments convinced need pretraining large image database like imagenet. however scratch achieving state results would take long time decided explore publicly available models. experiments overfeat convolutional neural network competition winner ilsvrc-. chop last softmax layer overfeat leaves -dimensional feature vector. since deep inside network argue features would high-level abstract directly model. therefore don’t update weights parameters overfeat network treat feature extractor. instead build smaller fully connected takes overfeat feature vector input generates multimodal image ﬁrst simplest model fully-connected unigram words features dictionary size size input layer equal vocabulary size index mapping individual word. next optional hidden layers non-linearity greater modeling strength. finally last layer gives feature vector vtxt entire textual input size nemb. also noteworthy ﬁrst layer would encode information individual words thus experiment initializing good publicly available word embeddings called wordvec major problem model looses sense structure sentence many ways preserve syntactic information sentence model simplest approach n-grams. extend textual model include bi-gram trigram tri-grams skip-grams inputs. vocabulary size increased units although model still whole treats n-gram unit inside still syntactic information present inside individual n-gram unit hope gain performance increased syntactic information textual input. also tried experimenting using tf-idf features instead binary word presence features. also tried experimenting supervised semantic embedding architecture textual model. uses specialized architecture deal variable length sentences temporal dependencies within text. successfully applied applications like sentiment analysis customer reviews motivation using architecture consists three parts. embedding layer embeds word m-dimensional vector. kernels convolved sequence word embeddings give sequences ﬁxed length finally averaging layer averages length reducing sequence k-dimensional vector encodes semantic information complete sentence. layers added depending required functionality. explained above generate negative sample taking positive sample replacing sentence another random sentence eﬀect take approaches replacing sentence another sentence replacing image another image turns choice makes diﬀerence evaluation results refer choice training methodology. choice explained mathematical forumalism discuss eﬀects empirical results next chapter. however would brieﬂy mention approach gives higher performance image annotation systems approach gives higher performance image search systems. therefore methodology generating negative samples used specialize system towards particular task deep cnns becoming ubiquitous image recognition feature extraction achieving state many major computer vision tasks. therefore like recent work also stick cnns visual model. however instead training network scratch publicly available model overfeat achieved good performance ilsvrc-. initial reason waste extra time training model scratch large data sets like imagenet. train model various special tricks like multiple-image fragments multiple-scales maximizing image localization introducing color translation invariance data augmentation. model provides excellent feature vector representation images feel good results task partly high modeling strength accuracy visual model. explained above several approaches textual model simple model n-gram model model. however approach much simpler many researchers recently started using recurrent recursive neural networks model sentences also unlike don’t divide images sentences fragments treat whole. results show complicated models also harder train slower necessary achieve good performance task image-sentence bidirectional retrieval. furthermore show important feature bi-directional retrieval models specialized task empirical results show methodology generating negative sample signiﬁcant ﬁnal performance model approach model specializes image annotation specializes image search. note still remains bi-directional gives good performance task well. images scaled constant dimension since needs constant sized input. also input dimension overfeat utilize model. crop image make sure visually signiﬁcant object cropped unintentionally make harder identify convert sentences lower-case remove punctuation non-alphanumeric character them tokenize sequence individual words. ﬁlter articles \"an\" \"the\" since believe contribute semantic value sentence. finally limit word vocabulary number occurrences. unigram vocabulary contains words bi-grams tri-grams vocabulary size. experiment treating n-gram binary features tf-idf features. equation calculating tf-idf below here count occurrence frequency word document count number documents containing word size vocabulary total number documents. tf-idf values advantage storing relevance information example word frequent documents gets tf-idf score. used ﬁlter stop words. sections report results models sets mainly flickrk data occasionally flickrk .recallk widely used metric domain.it deﬁned percentage test queries model returns positive item among results. useful context search user satisﬁed ﬁrst results containing single relevant item. however deﬁnition ambiguous positive results test query. true imageannotation case since image corresponds sentences. researchers consider ﬁrst sentences test cases others include take account report following evaluation metrics avg_txt recallk taking consideration sentences image taking average them. robust metric since depend relative position sentence takes sentences account. hence comparing models. any_txt recallk taking consideration sentences. returns match image matches sentences. valid imagecaption needs multiple positive results query. rprecision percentage relevant items among responses returned system. select maximum number positive responses imageannotation. since image search relevant result metric relevant train model end-to-end fashion using stochastic gradient descent without momentum weight decay terms. experiments parameters overfeat network weights don’t update back propagation. general takes days fully train model flickrk data using cores typically network runs epochs termination. flickrk flickrk data separate random images test remaining examples held-out validation model selection. tried several models selected meta-parameters based performance held-out validation set. select parameters mentioned experiments fair comparison. first would give ﬁgure show error performance training. figure shows model performs well validation comparison ﬁrst model failed generalize validation seen figure also performance validation saturates around epoch save networks point performance saturate validation set. done minimize chance overﬁtting training set. figure training time error flickrk data set. textual model unigram based hidden layer nemb error percentage samples model gives higher score positive sample negative sample. error speciﬁes random performance. explained chapter tried methodologies generating negative samples training time namely notice choice actually impact performance system peculiar way; specializes particular task. apparent table using methodology gives higher performance image annotation tasks methodology gives higher performance image annotation task. compare performance across several models architectures consistently notice phenomenon. hence future results whenever refer image annotation report results image search report results. intuitively reason specialization approach closely matches image annotation image search. images positive negative samples same sentences diﬀerent system learns better recognizing changes sentences hence ultimately performs better image annotation. vice versa holds table comparison diﬀerent training methodologies means unsing unigram word features means using bigram. results given recall avg_txt. hidden layer textual model nemb size embedding dimension. results model first report results model textual model simply unigram word features. information temporal dependency sentence lost dictionary size frequently words also binary features information multiple occurrences words sentence. hidden layer textual model ﬁxed units since wordvec embeddings dimensionality ﬁxed hidden layer trainable visual model units size multimodal embedding vector also ﬁxed table shows results flickrk test table shows results flickrk test model named model comparing results chapter don’t train model large corpus text speculate wordvec would helpful. repeat experiment flickrk using model diﬀerence randomly initialized network. experiment useful suggested using n-gram models second step onto model however aware publicly available n-gram embeddings. results experiment shown table compare table quick comparison performance metrics tables shows although wordvec initialization help improving accuracy system eﬀects extremely drastic. example ravg_txt image annotation increases image search increases results n-gram models replaced textual model n-gram model. n-gram models conserve sentence-order information theoretically better features. experiments tried bigrams trigrams trigrams skip grams cases vocabulary size ﬁxed size input layer textual model mlp. table shows comparison three diﬀerent kind models task. table comparison diﬀerent n-gram models flickrk data. bi-gram trigram combination tri-gram skip-gram features. case size input vocabulary rest model parameters similar. bold ﬁgures show best results speciﬁc row. hidden layer textual model nemb size embedding dimension. features. intuition suggests tf-idf encode information perform better binary features. however empirical evidence suggests otherwise binary features performed better models. hence future models stick binary features. results shown table table comparison diﬀerent input word features binary flickrk data. case size input vocabulary inputs either rest model parameters similar. bold ﬁgures show best results speciﬁc row. hidden layer textual model nemb size mbedding dimension. finally select best performing model unit hidden layers textual trainable visual models also unit multimodal embedding vector. results model flickrk shown figure notice signiﬁcant improvements model. model named model n-gram comparing results chapter deep textual models necessary will investigate deep network really necessary textual model. explored several shallow single layered architectures input n-gram input linearly transformed embedding vector. surprisingly almost networks tried reached performance close ones achieved deeper network non-linearities. addition performing well shallow models approximately faster train contain hidden layer well dimensional overfeat feature vectors linearly transformed embedding dimension. result model table comparison made table performance diﬀerence compared earlier discussed model n-gram. model named model shallow comparing results chapter lastly also explored using module textual model. details architecture given chapter specialized architecture modeling sequence inside variable length sentences similar architectures achieved good performance task however empirical results show another story model performs well previously discussed models. results best model architecture shown table architecture context window size word level phrase level embeddings dimensions. model initialized wordvec important note wordvec initialization performance fell staggeringly half current performance. feel possible reason failure textual module isn’t enough textual training data. data limited sentences makes total approximately million words. contrast million words train language model similar architecture. architecture much deeper parameters textual models tried distinct layers lookup table individual word embeddings ﬁxed window phrase level embeddings averaging layer complete sentence layer embedding ﬁnally non-linearity followed linear layer greater modeling capacity. deep architecture unlikely needs multimodal data train well. secondly fact wordvec boosts performance also means unable learn good word embeddings initialized random. contrast initialized model without wordvec diﬀerence performance much slighter. reinforces opinion data needed. would like compare performance model existing state systems. however noted chapter inherent ambiguity common evaluation metric deﬁned task. addition this researchers often diﬀer present results. sentences evaluating system. comparing image annotation task report avg_txt comparing image search task report any_txt. follow standard report best knowledge. perform results. comparing results interesting note recent research gone complicated modeling textual domain. example sdt-rnn dependency-tree based recursive neural network. also types recurrent recursive neural networks. besides dependency tree edges fragment sentences contrast simple textual model achieve comparable results. bulk model’s strength comes visual model training methodology specializes performance towards particular task boosting performance. experiments notice temporal dependency modeled simple bi-grams tri-grams enough achieve good results. goal project explore deep learning architectures context multimodal image text modeling. task based bi-directional retrieval. means model would support retrieving images based text query also retrieving text sentences based image query. applications system would include automatic image annotation sentence based image retrieval. design system ranking approach common many traditional information retrieval systems. iteration feed image positive pair negative pair training input. positive input matched pair image sentence negative input simply mismatched pair. model assigns score positive input negative input objective function make positive sample score higher negative sample. model learn rank related pair image text higher arbitrary unrelated pair. since images text diﬀerent statistical properties necessary model diﬀerent sub-models them. idea sub-models extract underlying semantic information encode distributed vector embedding ranking objective function could align embeddings common multimodal space. case simply compute distance betwen vectors measure similarity. visual model explored using deep convolutional neural network initial model initialized random weights failed perform well tasks. experiments diﬀerent related task image recognition mnist caltech able conclude lack pretraining reason network failed. following adopted overfeat visual model publicly available achieved high performance ilsvrc change network stats work well task conﬁrms initial suspicion. textual module tried various models features simple complicated. explored using words approach well n-grams tried binary tf-idf features. experimented using shallow linear architectures intermediate architecture like deeper architectures word embeddings convolutional neural network evaluate performance models testing flickrk flickrk training sets. comparison recent work ﬁeld shows even though models simpler achieve performance comparable results. believe major reason good performance performance extracts powerful semantically relevant features. although like approaches overfeat trained imagenet data base trained special tricks like multiple-image fragments multiple-scales maximizing image localization introducing color translation invariance data augmentation. network achieves excellent performance ilsvrc challenge also increases quality feature vectors extracted model. textual module observed even shallow module single linear layer achieves good performance. although using hidden layers increasing dimensionality embedding layer enhance performance performance still good simple linear layer. provides interesting contrast visual module textual module. since well pretrained able train task. however specialized deeper architecture perform well textual side. provides fundamental question essential deep learning task? think reason performs well pretrained data extract semantic features images. order make textual model work well either need large scale image-text data base need pretrain deeper textual model tasks allow extract semantic information sentences. example task could document classiﬁcation. finally show important feature bi-directional retrieval models specialized task empirical results show methodology generating negative sample signiﬁcant ﬁnal performance model. approach model specializes image annotation specializes image search. note still remains bi-directional gives good performance task well. simplest addition model would turn updating training image-text data set. called ﬁnetuning since pretrained image data alone. expect achieve performance improvement ﬁnetuning reported seen table however come cost signiﬁcantly additional training time hence trade noted chapter experimented promising model experienced performance drop surprise. hypothesize complicated deeper model needs larger training data perform well something true flickrk flickrk data sets. interestingly problem similar faced initially visual model explained chapter uninitialized failed tasks miserably convinced pretrained imagenet. similarly feel module trained large relevant data would signiﬁcantly improve results. experienced portion reported initializing ﬁrst layer wordvec caused accuracy model almost double means although wordvec good word embeddings enough task multimodal learning since initially learned unimodal language model. best approach would large image-text data base train models however high quality large scale data base exist task yet. regeradless data sets future. ms-coco data provides high quality captions labeled diﬀerent humans images. although still extremely large data ﬁrst hope models larger data sets exist usability image text mapping systems questioned. flickrm example data contains million images flickr website associated captions added people uploading images. argue captions good association images since humans tend describe aspects images apparent image itself. contrast system wants know apparent inside image. explored using good quality dataset improve performance quality annotated data sets propose would interesting explore another exciting area explore using data augmentation similar used visual models image recognition. words could artiﬁcially augment textual data matching image training set. three ways this. firstly could original human labeled sentences seeds text based information retrieval systems retrieve similar sentences large textual corpus. secondly could design language model generate sentences similar given input. thirdly could image-based sentence generation system generate additional relevant sentences training retrieval system. larger amount text corresponding images better train deeper high-capacity models also better model inherent variance describing image. next step expand system ideas presented above. include ﬁnetuning pretraining entire system weekly annotated large scale data artiﬁcially augmenting textual data. ﬁrst experimentation image text multimodal modeling system keep improving mnist database hand-written digits consisting training examples test examples. digits size-normalized centered ﬁxed-size image pixels. data long history acting benchmark measuring performance visual recognition systems. caltech database object recognition images. consists images belonging categories categories range helicopter airplane elephant spaghetti riﬂe everyday objects. images left-right aligned makes task relatively harder. imagenet large scale image database organized hierarchical manner. consists total million images belonging categories. addition organize yearly competition computer vision called ilsvrc competition consists three individual tasks object recognition localization detection. pascal ﬁrst publicly available data purposefully collected image text ranking task data consists images pascal voc- object recognition challenge. images randomly selected belonging categories. image annotated descriptive captions using amazon’s mechanical turk resulting sentences. although ﬁrst speciﬁc data suﬀers number shortcomings limit usefulness. domain images limited captions relatively simple sometimes containing grammatical spelling errors trained neural network models using proprietary machine leaning environment developed labs america. milde software environment developing prototyping applications based machine learning statistics. table experiment shows training approach signiﬁcantly eﬀects results case approach boosts performance image annotation. experiment performed flickrk data unigram model wordvec initialization. rprecision high good good high good. table experiment shows training approach signiﬁcantly eﬀects results case approach boosts performance image search. experiment performed flickrk data unigram model wordvec initialization. good high good. bach jordan kernel independent component analysis. journal machine learning research weston grangier collobert sadamasa chapelle weinberger supervised semantic proceedings conference information indexing. knowledge management pages acm. bespalov shokoufandeh sentiment classiﬁcation based supervised latent n-gram analysis. proceedings international conference information knowledge management pages acm. collobert weston uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning pages acm. deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee. farhadi hejrati sadeghi young rashtchian hockenmaier forsyth every picture tells story generating sentences images. computer vision–eccv pages springer. gong wang hodosh hockenmaier lazebnik improving image-sentence embeddings using large weakly annotated photo collections. computer vision–eccv pages springer. hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv.. hodosh young hockenmaier framing image description ranking task data models evaluation metrics. journal artiﬁcial intelligence research pages denker henderson howard hubbard jackel handwritten digit recogniadvances neural information tion back-propagation network. processing systems. citeseer. lecun jackel bottou brunot cortes denker drucker guyon muller sackinger comparison learning algorithms handwritten digit recognition. international conference artiﬁcial neural networks volume pages papineni roukos ward w.-j. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics pages association computational linguistics. rashtchian young hodosh hockenmaier collecting image annotations using amazon’s mechanical turk. proceedings naacl workshop creating speech language data amazon’s mechanical turk pages association computational linguistics. sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks. international conference learning representations cbls. socher karpathy manning grounded compositional semantics ﬁnding describing images sentences. transactions association computational linguistics socher manning parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning pages young hodosh hockenmaier image descriptions visual denotations similarity metrics semantic inference event descriptions. transactions association computational linguistics", "year": 2015}