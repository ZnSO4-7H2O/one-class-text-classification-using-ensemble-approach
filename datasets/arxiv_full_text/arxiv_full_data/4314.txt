{"title": "Random Projections on Manifolds of Symmetric Positive Definite Matrices  for Image Classification", "tag": ["cs.CV", "stat.ML", "I.4.7; I.4.10; I.5.1; I.5.4"], "abstract": "Recent advances suggest that encoding images through Symmetric Positive Definite (SPD) matrices and then interpreting such matrices as points on Riemannian manifolds can lead to increased classification performance. Taking into account manifold geometry is typically done via (1) embedding the manifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert Spaces (RKHS). While embedding into tangent spaces allows the use of existing Euclidean-based learning algorithms, manifold shape is only approximated which can cause loss of discriminatory information. The RKHS approach retains more of the manifold structure, but may require non-trivial effort to kernelise Euclidean-based learning algorithms. In contrast to the above approaches, in this paper we offer a novel solution that allows SPD matrices to be used with unmodified Euclidean-based learning algorithms, with the true manifold shape well-preserved. Specifically, we propose to project SPD matrices using a set of random projection hyperplanes over RKHS into a random projection space, which leads to representing each matrix as a vector of projection coefficients. Experiments on face recognition, person re-identification and texture classification show that the proposed approach outperforms several recent methods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian Locality Preserving Projection and Relational Divergence Classification.", "text": "airm induces riemannian structure invariant inversion similarity transforms. despite properties learning methods using approach deal computational challenges employing computationally expensive non-linear operators. address issue lines research proposed embedding manifolds tangent spaces embedding reproducing kernel hilbert spaces induced kernel functions former approaches effect manifold points euclidean spaces thereby enabling existing euclidean-based learning algorithms. comes cost disregarding manifold structure. latter approach addresses implicitly mapping points manifold rkhs considered high dimensional euclidean space. training data used deﬁne space preserves manifold geometry downside existing euclidean-based learning algorithms need kernelised trivial. furthermore resulting methods still high computational load making impractical complex scenarios. contributions. paper offer novel approach analysing matrices combines main advantage tangent space approaches discriminatory power provided kernel space methods. adapt recent idea techniques speciﬁcally designed learning tasks large image datasets domain image representations mapped reduced space wherein similarities still well-preserved proposed approach employ mapping technique create space preserves manifold geometry considered euclidean. speciﬁcally ﬁrst embed manifold points rkhs stein divergence kernel generate random projection hyperplanes rkhs project embedded points method proposed finally underlying space thought euclidean appropriate euclidean-based learning machinery applied. paper study efﬁcacy embedding method classiﬁcation tasks. show recent advances suggest encoding images symmetric positive deﬁnite matrices interpreting matrices points riemannian manifolds lead increased classiﬁcation performance. taking account manifold geometry typically done embedding manifolds tangent spaces embedding reproducing kernel hilbert spaces embedding tangent spaces allows existing euclidean-based learning algorithms manifold shape approximated cause loss discriminatory information. rkhs approach retains manifold structure require non-trivial effort kernelise euclidean-based learning algorithms. contrast approaches paper offer novel solution allows matrices used unmodiﬁed euclidean-based learning algorithms true manifold shape well-preserved. speciﬁcally propose project matrices using random projection hyperplanes rkhs random projection space leads representing matrix vector projection coefﬁcients. experiments face recognition person re-identiﬁcation texture classiﬁcation show proposed approach outperforms several recent methods tensor sparse coding histogram plus epitome riemannian locality preserving projection relational divergence classiﬁcation. covariance matrices recently employed describe images videos known provide compact informative feature description non-singular covariance matrices naturally symmetric positive deﬁnite matrices form connected riemannian manifolds endowed riemannian metric tangent spaces such riemannian geometry needs considered solving learning tasks address classiﬁcation tasks originally formulated manifold embedding random projection space considered euclidean still honouring manifold geometry structure. propose random projection rkhs stein divergence kernel. random projection approximation approach estimating distances pairs points highdimensional space essence projection point done randomly generated hyperplanes rd×k matrix wherein column contains single hyperplane mapping function maps point random projection space space according johnson-lindenstrauss lemma possible high-dimensional points much lower dimensional space wherein pairwise distance points well-preserved lemma johnson-lindenstrauss lemma. points upon projection uniform random k-dimension subspace following property every pair holds probability least projection despite success numerous approaches using lemma accomplish various computer vision tasks restrict distance function norm mahalanobis metric inner product makes incompatible non-euclidean spaces. recently kulis grauman proposed method allows distance function evaluated rkhs. thus possible apply lemma arbitrary kernel unknown embedding maps points hilbert space approach makes possible construct random projection space manifold manifold structure well-preserved. main idea proposed approach denoted random projection manifold image classiﬁcation ﬁrst points manifold rkhs implicit mapping function stein divergence kernel. followed mapping points rkhs random projection space achieve follow kulis-grauman approach randomly generating hyperplanes rkhs restricted approximately gaussian. embedding function space effective completeness training data generating random projection hyperplanes address synthetic data augment training data. experiments several vision tasks show proposed approach outperforms several stateof-the-art methods. continue paper follows. section provides brief overview manifold structure associated kernel function. detail proposed approach section section presents results study random projection space discriminability well comparisons state-of-the-art results various visual classiﬁcation tasks. main ﬁndings possible future directions summarised section nonsingular d×d-sized covariance matrices symmetric positive deﬁnite matrices. matrices belong smooth differentiable topological space known manifolds. work endow manifold airm induce riemannian structure such point manifold mapped tangent space using symd point tangent space located point would like tangent space txim; matrix logarithm. inverse function maps points particular tangent space manifold functions deﬁne shortest distance points manifold. distance called geodesic distance represented minimum length curvature path connects points mapping functions computationally expensive. also recently introduced stein divergence determine similarities points manifold. symmetrised form stein divergence kernel deﬁned condition kernel matrix formed eqn. positive deﬁnite operations projecting resulting kernel vector random hyperplane demands operations ﬁnally applying classiﬁer projection space done versus support vector machine operations number classes number hyperplanes used deﬁning hyperplane hence complexity classiﬁcation single query data equal efﬁcient relational divergence based classication later shown second best approach experiment part. represents riemannian points similarity vectors training points. similarity vectors euclidean space employs linear discriminant analysis classiﬁer. synthetic data later shown experiment section discriminative power random projection space depends heavily training generates random projection hyperplanes. overcome limitation propose generated synthetic matrices symd centred around mean data mean training determined intrinsically karcher mean algorithm need generate matrices located within radius mean trivial generate matrices follow eqn. establishes relation generate matrices original training points. address this apply relationship geodesic distance given riemannian metric tangent space. symd points manifold txim corresponding points tangent space txim. norm vector xixj equivalent therefore possible point along geodesic whose geodesic distance satisﬁes unknown generation process done indirectly weighted subset given training sets. consider data point training vector underlying distribution unknown mean unknown covariance training exemplars chosen i.i.d. deﬁned according central limit theorem sufﬁciently large rant distributed according vector multi-variate gaussian whitening transform applied results follows distribution hilbert space therefore i-th coefﬁcient vector random projection space deﬁned mean covariance need approximated training data. objects chosen form ﬁrst items reference mean implicitly estimated covariance matrix also formed samples. eqn. solved using similar approach kernel requires projecting onto eigenvectors covariance matrix eigendecomposition therefore eqn. rewritten deﬁne kernel matrix randomly selected training points λθλt based fact zero eigenvalues equal non-zero eigenvalues kulis-grauman showed eqn. equivalent terms calculating computational complexity training algorithm according eqns. expensive step single ofﬂine computation takes computational complexk classifying query point depends three factors computing kernel vector requires parameter settings follows. suggested used number samples chosen create hyperplane. number random hyperplanes used validation data choose based empirical observations validation sets number synthetic samples chosen either number samples class. similar manner value eqn. chosen compare proposed method denoted random projection manifold image classiﬁcation several embedding approaches well several state-of-the-art methods. also evaluate effect augmenting training data synthetic data points refer approach rose synthetic data person re-identiﬁcation task used modiﬁed version ethz dataset dataset captured moving camera images pedestrians containing occlusions wide variations appearance. sequence contains pedestrians sequence contains following ﬁrst downsampled images created training using randomly selected images rest used shape test set. random selection training testing data repeated times. image represented covariance matrix feature vectors obtained pixel location deﬁnition point manifold said normalised geodesic distance symd respect symd proposition matrices exists geodesic curve deﬁned symd normalised geodesic distance respect point determined prove proposition symd order given points manifold. normalise geodesic distance respect point tangent space txµm. tangent space considered euclidean space distance tangent pole preserved euclidean vector normalisation applied. finally normalised point mapped back manifold. steps presented consider three computer vision classiﬁcation tasks texture classiﬁcation face recognition person re-identiﬁcation ﬁrst detail experiment application discuss results comprehensive study random projection space discriminability tasks. ﬁrst embed matrices rkhs stein divergence kernel followed mapping embedded data points random projection space. resulting vectors appendix proof symd task texture classiﬁcation brodatz dataset fig. examples. follow test protocol presented accordingly nine test scenarios various number classes generated create matrices follow downsampling image splitting regions. feature vector pixel calculated region described covariance matrix formed vectors. test scenario randomly select covariance matrices class construct training rest used create testing set. random selection repeated times mean results reported. face recognition task subset feret dataset used. image ﬁrst closely cropped include face downsampled tests various pose angles created evaluate performance method. training consists frontal images illumination expression small pose variations. non-frontal images used create test sets. face image represented covariance matrix every pixel following feature vector computed ﬁrst compare performance proposed rose method several embedding methods kernel using stein divergence kernel kernelised locality-sensitive hashing riemannian spectral hashing hashing method speciﬁcally designed smooth manifolds tables report results dataset. rose considerably outperforms embedding methods texture person re-identiﬁcation applications klsh face recognition task. suggests random projection space constructed table recognition accuracy person reidentiﬁcation task seq. ethz dataset; ksvm kernel svm; klsh kernelised locality-sensitive hashing; riemannian spectral hashing. rose proposed method roses rose augmented synthetic data. random hyperplanes rkhs offers sufﬁcient discrimination classiﬁcation tasks. fact linear classiﬁer results presented follow theoretical results suggest margin classiﬁer still well-preserved random projection. apply roses method three tasks order take closer look contribution training data generating random projection hyperplanes space discriminability. shown results notable improvement rose ethz person re-identiﬁcation well brodatz texture classiﬁcation datasets. however using synthetic points gives adverse effect feret face recognition dataset. results suggest training data contributes space discriminability. probably fact random projection hyperplane represented linear combination randomly selected training points. such variations completeness training data signiﬁcant contributions resulting space. performance loss suffered feret face recognition dataset probably caused skewed data distribution particular dataset. hence adding synthetic points would signiﬁcantly alter original data distribution turn affects space discriminability. empirical observation found data points grouped together intrinsic clustering method applied dataset. poor performance dataset supports view. shown fig. signiﬁcant performance difference rose roses methods highlighting importance training data generating random projection hyperplanes. performance difference pronounced classes excluded training data. note training different training train classiﬁer. although exclude classes training constructing random projection space still provided training data train classiﬁer. comparison recent methods table shows feret face recognition proposed rose method obtains considerdataset ably better results several recent methods logeuclidean sparse representation tensor sparse coding locality preserving projection relational divergence classiﬁcation table contrasts performance roses method brodatz texture recognition task methods. note case synthetic data necessary order achieve improved performance. average roses achieves higher performance methods performance obtained tests. finally compared roses method reseveral identiﬁcation ethz dataset histogram plus epitome symmetry-driven accumulation local features rlpp performance evaluated method’s high computational demands would take approximately hours process ethz dataset. report results loge-sr performance datasets. results shown table indicate proposed roses method obtains better performance. previous experiment synthetic data necessary obtain improved performance. table recognition accuracy face recognition task using log-euclidean sparse representation tensor sparse coding riemannian locality preserving projection relational divergence classiﬁcation proposed rose method. figure sensitivity random projection space discriminability number selected data points generating random hyperplanes well effect adding synthetic data points improving space discriminability. graphs compare performance rose roses sets brodatz texture recognition dataset respectively. highlight proposed roses method experiment sequences brodatz dataset. experiment reduce number data required creating mapping function step step. first step provided training data construct random projection space. then progressively discard training data points particular class construct space. repeat process class left. total classes class samples training. experiment every single combination case table performance brodatz texture dataset loge-sr tensor sparse coding riemannian locality preserving projection relational divergence classiﬁcation proposed roses method. table recognition accuracy person reidentiﬁcation task seq. seq. ethz dataset. histogram plus epitome sdalf symmetry-driven accumulation local features rlpp riemannian locality preserving projection relational divergence classiﬁcation advantage representing images forms non-singular covariance matrix groups superior performance achieved underlying structure group considered. shown endowed afﬁne invariant riemannian metric matrices form connected smooth differentiable riemannian manifold. working directly manifold space airm poses many computational challenges. typical ways addressing issue include embedding manifolds tangent spaces embedding reproducing kernel hilbert spaces embedding manifolds tangent spaces considerably simpliﬁes analysis cost disregarding manifold structure. embedding rkhs better preserve manifold structure adds burden extending existing euclidean-based learning algorithms rkhs. work presented novel solution embeds data points random projection space ﬁrst generating random hyperplanes rkhs projecting data rkhs random projection space. presented study space discriminability various computer vision classiﬁcation tasks found space superior discriminative power typical approaches outlined above. addition found space discriminative power depends completeness training data generating random hyperplanes. address issue proposed augment training data synthetic data. reperson identiﬁcation texture classiﬁcation show proposed method outperforms state-of-the-art approaches tensor sparse coding histogram plus epitome riemannian locality preserving projection relational divergence classiﬁcation. knowledge ﬁrst time random projection space applied solve classiﬁcation tasks manifold space. envision proposed method used bring superior discriminative power manifold spaces general vision tasks object tracking.", "year": 2014}