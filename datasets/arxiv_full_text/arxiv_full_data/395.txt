{"title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video  via Context Translation", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "abstract": "Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, and embodiment. We term this kind of imitation learning as imitation-from-observation and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations and actions in the same environment, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show that our approach can perform imitation-from-observation for a variety of real-world robotic tasks modeled on common household chores, acquiring skills such as sweeping from videos of a human demonstrator. Videos can be found at https://sites.google.com/site/imitationfromobservation", "text": "imitation learning effective approach autonomous systems acquire control policies explicit reward function unavailable using supervision provided demonstrations expert typically human operator. however standard imitation learning methods assume agent receives examples observation-action tuples could provided instance supervised learning algorithm. stands contrast humans animals imitate observe another person performing behavior ﬁgure actions realize behavior compensating changes viewpoint surroundings embodiment. term kind imitation learning imitation-fromobservation propose imitation learning method based video prediction context translation deep reinforcement learning. lifts assumption imitation learning demonstration consist observations actions environment enables variety interesting applications including learning robotic skills involve tool simply observing videos human tool use. experimental results show approach perform imitation-from-observation variety real-world robotic tasks modeled common household chores acquiring skills sweeping videos human demonstrator. videos found https//sites.google.com/ site/imitationfromobservation/ learning enable autonomous agents robots learn complex behavioral skills suitable variety unstructured environments. order autonomous agents learn skills must supplied supervision signal indicates goal desired behavior. supervision typically comes sources reward function reinforcement learning speciﬁes states actions desirable expert demonstrations imitation learning provide examples successful behaviors. modalities combined high-capacity models deep neural networks enable learning complex skills sensory observations major advantage reinforcement learning agent acquire skill trial error high-level description goal provided reward function. however reward functions difﬁcult specify hand particularly success task determined complex observations camera images imitation learning bypasses issue using examples successful behavior. popular approaches imitation learning include direct imitation learning behavioral cloning reward function learning inverse reinforcement learning settings typically assume agent receives examples consist sequences observation-action tuples must learn function maps observations actions example sequences generalizing scenarios. however notion imitation quite different kind imitation carried humans animals learn skills observing people receive egocentric observations ground truth actions. observations obtained alternate viewpoint actions known. furthermore humans capable learning live observations demonstrated behavior also video recordings behavior provided settings considerably different own. design imitation learning methods succeed situations? solution problem would considerable practical value robotics since resulting imitation learning algorithm could directly make natural videos people performing desired behaviors obtained instance internet. term problem imitation-from-observation. goal imitation-from-observation learn policies sequence observations desired behavior sequence obtained differences context. differences context might include changes environment changes objects manipulated changes viewpoint observations might consist sequences images. deﬁne problem formally section imitation-from-observation algorithm based learning context translation model convert demonstration context another context training model perform conversion acquire feature representation suitable tracking demonstrated behavior. deep reinforcement learning optimize actions optimally track translated demonstration target context. illustrate experiments method signiﬁcantly robust prior approaches learn invariant feature spaces perform adversarial imitation learning directly track pre-trained visual features translation method able provide interpretable reward functions performs well number simulated real manipulation tasks including tasks require robot emulate human tool use. imitation learning usually thought problem learning expert policy generalizes unseen states given number expert state-action demonstration trajectories imitation learning algorithms largely divided classes approaches behavioral cloning inverse reinforcement learning. behavioral cloning casts problem imitation learning supervised learning policy learned state-action tuples provided expert. imitation-from-observation expert provide actions provides observations state different context direct behavioral cloning cannot used. inverse reinforcement learning methods instead learn reward function expert demonstrations reward function used recover policy running standard reinforcement learning though recent methods alternate steps forward inverse methods principle learn observations practice using directly high-dimensional observations images proven difﬁcult. aside handling high-dimensional observations images method also designed handle differences context. context refers changes observation function different demonstrations demonstrations learner. include changes viewpoint embodiment surroundings. along similar lines stadie directly address learning domain shift. however method number restrictive requirements including access expert non-expert policies directly optimizing invariance contexts performs poorly complex manipulation tasks consider illustrated section sermanet proposes address differences context using pretrained visual features provide mechanism context translation work relying instead inherent invariance visual features learning. follow-up work proposes increase invariance visual features multi-viewpoint training ryoo propose learn robotic skills ﬁrst person videos humans using explicit hand detection carefully engineered vision pipeline. contrast approach trained end-to-end require prior visual features detectors vision systems. duan proposed translate demonstrations policies training paired examples state sequences however method operates observations require actions demonstrations prior method operates low-dimensional state variables deal context shift like method. technical approach relationships work visual domain adaptation image translation. several methods proposed pixel level domain adaptation well translation visual style domains using generative adversarial networks focus instead translating demonstrations context another conditioned ﬁrst observation target context enable agent physically perform task. although gans prior methods complementary ours incorporating loss could improve performance method further. imitation-from-observation setting consider work agent observes demonstrations task variety contexts must execute demonstrated behavior context. term context refer properties environment agent vary across demonstrations include viewpoint agent’s embodiment positions identities objects environment forth. demonstrations consist observations ....dn} dynamics expert’s policy demonstration produced different context here represents unknown markovian state represents action represents context. assume sampled independently demonstration imitation learner ﬁxed distribution. throughout technical section refer observation time context practical real-world imitation-from-observation application might also contend systematic domain shift where e.g. learner’s embodiment differs systematically demonstrator therefore learner’s context cannot treated sample leave challenge prior work instead focus basic problem imitation-from-observation. means context vary demonstrations learner learner’s context still comes distribution. elaborate practical implications assumption section discuss might lifted future work. algorithm imitation-from-observation must contend challenges ﬁrst must able determine information observations track context differ demonstrations second must able determine actions allow track demonstrated observations. reinforcement learning offers tool addressing latter problem measure distance demonstration reward function learn policy takes actions minimize distance. distance use? observations correspond example image pixels euclidean distance measure give wellshaped objective roughly matching pixel intensities necessarily correspond semantically meaningful execution task unless match almost perfect. fortunately solution ﬁrst problem context mismatch naturally lends solution problem choosing distance metric. order address context mismatch train model explicitly translates demonstrations context another using different demonstrations training data. internal representation learned model provides much well-structured space evaluating distances observations since proper context translation requires understanding underlying factors variation scene. empirically illustrate experiments squared euclidean distances features context translation model reward function learn demonstrated task using model translate features demonstration context learner’s context. ﬁrst describe translation model show used create reward function since demonstration generated unknown context learner cannot directly track demonstrations context however since demonstrations multiple unknown different contexts learn context translation model demonstrations without explicit knowledge context variables themselves. assume ﬁrst frame demonstration particular context used implicitly extract information context translation model trained pairs demonstrations comes unknown context comes unknown context model must learn output observations conditioned ﬁrst observation target context thus model looks single observation target context predicts future observations context look like translating demonstration source context. trained model provided demonstration translate learner’s context tracking discussed next section. model assumes demonstrations aligned time though assumption could relaxed future work using iterative time alignment gupta al.. goal learn overall translation function target initial observation encoder encode observations source target features referred translator translates features features context denoted ﬁnally target context decoder decodes features denote feature extractor generates features input observation context image. encoders either different weights tied weights depending diversity demonstration scenes. deal complexities pixel-level reconstruction figure context translation model source obtranslated give prediction servation observation target context trans given context image target context. convolutional encoders deconvolutional decoder decodes features back observations. colors indicate tied weights. details neural network architectures module appendix include skip connections dec. model supervised squared error loss ltrans decz pair expert demonstrations chosen randomly training hyperparameters. examples translated demonstrations shown section appendix model described previous section translate observations features demonstration context learner’s context however order learning agent actually perform demonstrated behavior must able acquire actions track translated features. choose number deep reinforcement learning algorithms learn output actions track translated demonstrations given reward function describe below. ﬁrst component feature tracking reward function penalty deviations translated features. time step translation function used translate demonstration observations learner’s context reward function corresponds minimizing squared euclidean distance demonstrations approximately tracking average resulting schulman simulated experiments. real-world robotic experiments trajectory-centric method used local policy optimization guided policy search levine based ﬁtting locally linear dynamics performing lqr-based updates. compute image features include part state. cost function squared euclidean distance state space omit image tracking cost described simulated striking real robot pushing cost function also weighted quadratic ramp function weighting squared euclidean distances later time steps higher initial ones. experiments evaluate whether context translation model enable imitation-fromobservation well representative prior methods perform type imitation learning task. speciﬁc questions answer context translation model handle image observations changes viewpoint changes appearance positions objects contexts? well prior imitation learning methods perform presence variation comparison approach? well method perform real-world images enable real-world robotic system learn manipulation skills? results including illustrative videos experiment details found https//sites.google.com/site/imitationfromobservation/. order provide detailed comparisons alternative prior methods imitation learning four simulated manipulation tasks using mujoco simulator demonstrations generated using ground truth reward function prior policy optimization algorithm tasks illustrated ﬁrst task requires robotic reach goal position indicated disk presence variation color appearance. second task requires pushing white cylinder onto coaster presence varied distractor objects. third task requires simulated robot sweep grey balls dustpan variation viewpoint. fourth task involves using manipulator strike white ball onto target goal. appendix illustrates variability appearance object positioning four tasks also presents example translations individual demonstration sequences. results comparative evaluation approach presented performance evaluated terms ﬁnal distance target object goal testing. reaching task distance robot’s hand goal pushing task distance cylinder goal sweeping task corresponds mean distance balls inside dustpan striking task ﬁnal distance ball figure example illustration demonstration reaching task performed context translated observation sequences illustrations tasks found appendix figure comparisons several prior methods reaching pushing sweeping striking tasks. results show method successfully learned task prior methods unable perform reaching pushing striking tasks pretrained visual features approach able improve well sweeping task. third person imitation learning generative adversarial imitation learning success rate graph. goal position. distances normalized dividing initial distance start task success measured thresholding normalized distance. comparisons include method trpo policy learning oracle trains policy trpo ground truth reward function represents upper bound performance three prior imitation learning methods. ﬁrst prior method learns reward using pre-trained visual features similar work sermanet al.. method features inception-v network trained imagenet classiﬁcation used encode goal image demonstration reward function corresponds distance features averaged training demonstrations. experimented several different feature layers inception-v network chose performed best. second prior method third person imitation learning algorithm explicitly compensates domain shift using adversarial loss third adversarial irl-like algorithm called generative adversarial imitation learning using convolutional model process images suggested among these tpil explicitly addresses changes domain context. results shown indicate method able successfully learn tasks demonstrations provided random contexts. notably none prior methods actually successful reaching pushing striking tasks struggled perform sweeping task. indicates imitation-from-observation presence context differences exceedingly challenging problem. videos qualitative results viewed project website https//sites.google.com/site/imitationfromobservation/. evaluate different components model also performed detailed ablation study. found losses described section ˆrfeat ˆrimg reward terms important consistent performance. ablation results described appendix evaluate whether method able scale real-world images robots focus manipulation tasks involving tool object positions camera viewpoints differ contexts. demonstrations provided human learned skills performed robot. since method assumes contexts demonstrations learner sampled distribution human robot used tool perform tasks avoiding systematic domain shift might result differences appearance human robot arm. future work plan investigate compensation domain shift domain adaptation methods since present work assumes contexts come distribution simpliﬁcation reasonable. ﬁrst task goal push cylinder marked goal position. evaluate method different settings real-world demonstrations provided human imitation done figure video method successfully pushing object onto target arbitrary viewpoint sawyer robot. left demonstrations provided human. right imitation learned robot robot simulation; real-world demonstrations provided human imitation done robot real world. setting human demonstrator provided demonstrations using pushing tool shown source video videos recorded different viewpoints starting goal positions changing demonstrations. method used learn perform task simulated environment similar visual appearance. seen method able effectively real world demonstrations control simulated robot outperforming prior method successful previous section baselines. next experiment evaluated method used learn pushing behavior real-world robotic manipulator using -dof sawyer robot. since trpo algorithm data intensive learn real-world physical systems policy learning comparison also test reward involves tracking pre-trained visual features inception-v network well baseline reward function attempts reach ﬁxed joint angles speciﬁed kinesthetic demonstration. note approach require kinesthetic demonstrations. order include high-dimensional visual features state guided policy search apply reduce dimensionality method uses dimensions part state. found method could successfully learn skill using demonstrations different viewpoints outperforms pre-trained features kinesthetic baseline shown results pushing task illustrates basic capability method learn skills involving manipulation rigid objects. however major advantage learning visual reward functions demonstration ability acquire representations used manipulate scenes harder represent analytically granular media. next experiment study well method learn variants sweeping task ﬁrst robot must sweep crumpled paper dustpan second must sweep pile almonds. used almonds place dirt ﬂuids avoid damaging robot. human demonstrator provided demonstrations paper sweeping demonstrations almond sweeping variety viewpoints. quantitative results summarized easier crumpled paper task method kinesthetic teaching approach works well reward uses pre-trained visual features insufﬁcient accomplish task. almond sweeping task method achieves higher success rate alternative approaches. success metric deﬁned average percentage almonds inside dustpan. figure left demonstrations human demonstrator sweeping almonds dustpan. bottom left demonstrations human demonstrator pouring almonds cooking pan. right execution robot successfully sweeping almonds dustpan using method. bottom right execution robot successfully pouring almonds cooking using method. figure plot success rate method versus baselines real world tasks sawyer robot. experiments axis success rates different methods axis. success metrics differ task described section last task combines granular media dynamic behavior. task robot must ladle almonds cooking task requires keeping ladle upright dumping turning wrist. average fraction almonds ladled method alternative approaches generally could perform task. quantitative results comparisons summarized investigated imitation-from-observation performed learning translate demonstration observation sequences different contexts differences viewpoint. translating observations target context track observations allowing learner reproduce observed behavior. translation model trained translating different contexts observed training generalizes unseen context learner. experiments show method used perform variety manipulation skills follow real-world demonstrations tool provided human demonstrator used real-world robotic control diverse range tasks patterned common household chores. although method performs well real-world tasks several tasks simulation number limitations. first requires substantial number demonstrations learn translation model. training end-to-end model scratch task inefﬁcient practice combining method higher level representations proposed prior work would likely lead efﬁcient training second require observations demonstrations multiple contexts order learn translate them. practice number available contexts scarce. case would valuable explore multiple tasks combined single model different tasks might come different contexts. finally would exciting explore explicit handling domain shift future work handle large differences embodiment learn robotic skills directly videos human demonstrators obtained example internet. abhishek gupta graduate research fellow. research supported iis- iis- darpa fundamental limits learning program equipment donation nvidia. also thank frederik ebert russell mendonca roberto calandra dylan hadﬁeld-menell michael mcdonald assistance robot experiments. also thank pierre sermanet greg kahn singh chelsea finn bradly stadie advice discussion. references pieter abbeel andrew apprenticeship learning inverse reinforcement learning. proceedings twenty-ﬁrst international conference machine learning icml york acm. ./.. http//doi.acm.org/. brenna argall sonia chernova manuela veloso brett browning. survey robot learning demonstration. robot. auton. syst. issn ./j.robot.... http//dx.doi.org/./j.robot.... mariusz bojarski davide testa daniel dworakowski bernhard firner beat flepp prasoon goyal lawrence jackel mathew monfort muller jiakai zhang zhang jake zhao karol zieba. learning self-driving cars. corr abs/. http//arxiv.org/abs/.. abdeslam boularias jens kober peters. relative entropy inverse reinforcement learning. proceedings fourteenth international conference artiﬁcial intelligence statistics aistats fort lauderdale april http //www.jmlr.org/proceedings/papers/v/boulariasa/boulariasa.pdf. konstantinos bousmalis nathan silberman david dohan dumitru erhan dilip krishnan. unsupervised pixel-level domain adaptation generative adversarial networks. corr abs/. http//arxiv.org/abs/.. duan marcin andrychowicz bradly stadie jonathan jonas schneider ilya sutskever pieter abbeel wojciech zaremba. one-shot imitation learning. corr abs/. http//arxiv.org/abs/.. chelsea finn sergey levine pieter abbeel. guided cost learning deep inverse optimal control policy optimization. proceedings international conference machine learning icml york city june http//jmlr.org/proceedings/papers/v/finn.html. abhishek gupta coline devin yuxuan pieter abbeel sergey levine. learning invariant feature spaces transfer skills reinforcement learning. proceedings international conference learning representations iclr toulon france april http//arxiv.org/abs/.. jonathan stefano ermon. generative adversarial imitation learning. advances neural information processing systems annual conference neural information processing systems december barcelona spain http//papers. nips.cc/paper/-generative-adversarial-imitation-learning. phillip isola jun-yan tinghui zhou alexei efros. image-to-image translation conditional adversarial networks. corr abs/. http//arxiv.org/abs/ justin johnson alexandre alahi fei-fei. perceptual losses real-time style transfer super-resolution. computer vision eccv european conference amsterdam netherlands october proceedings part ----_. https//doi.org/./----_. mrinal kalakrishnan peter pastor ludovic righetti stefan schaal. learning objective functions manipulation. ieee international conference robotics automation karlsruhe germany ./icra... http//dx.doi.org/./icra... jangwon michael ryoo. learning robot activities ﬁrst-person human videos using convolutional future regression. corr abs/. http//arxiv.org/abs/ sergey levine zoran popovic vladlen koltun. nonlinear inverse reinforcement learning gaussian processes. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december granada spain. http//papers.nips.cc/paper/ -nonlinear-inverse-reinforcement-learning-with-gaussian-processes. sergey levine chelsea finn trevor darrell pieter abbeel. end-to-end training deep visuomotor policies. mach. learn. res. january issn http//dl.acm.org/citation.cfm?id=.. ming-yu oncel tuzel. coupled generative adversarial networks. advances neural information processing systems annual conference neural information processing systems december barcelona spain http//papers.nips. cc/paper/-coupled-generative-adversarial-networks. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature http//dx.doi.org/./nature. andrew stuart russell. algorithms inverse reinforcement learning. proceedings seventeenth international conference machine learning icml francisco morgan kaufmann publishers inc. isbn ---. http//dl.acm.org/citation.cfm?id=.. deepak ramachandran eyal amir. bayesian inverse reinforcement learning. ijcai proceedings international joint conference artiﬁcial intelligence hyderabad india january http//ijcai.org/proceedings// papers/.pdf. nathan ratliff andrew bagnell martin zinkevich. maximum margin planning. machine learning proceedings twenty-third international conference pittsburgh pennsylvania june ./.. http//doi.acm.org/./.. stéphane ross narek melik-barkhudarov kumar shaurya shankar andreas wendel debadeepta andrew bagnell martial hebert. learning monocular reactive control cluttered ieee international conference robotics automation natural environments. karlsruhe germany ./icra... https//doi.org/./icra... schaal. imitation learning route humanoid robots? trends cognitive sciences http//www-clmc.usc.edu/publications/s/schaal-tics. pdf;http//www-clmc.usc.edu/publications/s/schaal-tics-rep.pdf. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning icml lille france july http//jmlr.org/ proceedings/papers/v/schulman.html. bradly stadie pieter abbeel ilya sutskever. third-person imitation learning. proceedings international conference learning representations iclr toulon france april http//arxiv.org/abs/.. christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. ieee conference computer vision pattern recognition cvpr vegas june ./cvpr... https//doi.org/./cvpr... emanuel todorov erez yuval tassa. mujoco physics engine model-based control. ieee/rsj international conference intelligent robots systems iros vilamoura algarve portugal october ./iros... https//doi.org/./iros... eric tzeng coline devin judy hoffman chelsea finn pieter abbeel sergey levine kate saenko trevor darrell. adapting deep visuomotor representations weak pairwise constraints. workshop algorithmic robotics jun-yan taesung park phillip isola alexei efros. unpaired image-to-image translation using cycle-consistent adversarial networks. corr abs/. http//arxiv. org/abs/.. brian ziebart andrew maas andrew bagnell anind dey. maximum entropy inverse reinforcement learning. dieter carla gomes aaai aaai press isbn ----. http//dblp.uni-trier.de/db/conf/aaai/ aaai.htmlziebartmbd. appendix network architecture training encoders simulation stride- convolutions kernel. perform convolutions ﬁlter sizes followed fully-connected layers size leakyrelu activations leak layers. translation module consists hidden layer size input concatenation output size decoder simulation fully connected layer input four fractionally-strided convolutions ﬁlter sizes stride skip connections every layer context encoder corresponding layer decoder concatenation along ﬁlter dimension. real world images encoders perform convolutions ﬁlter sizes strides respectively. fully connected layers feature layers size instead decoder uses fractionally-strided convolutions ﬁlter sizes strides respectively. real world model only apply dropout every fully connected layer keep probability weights enc. train using adam optimizer learning rate train using videos simulated reach videos simulated push videos simulated sweep videos simulated strike videos simulated push real videos videos real push real videos videos real sweep paper videos real sweep almonds videos cooking almonds. evaluate different loss functions training translation model different components reward function performing imitation performed ablations removing components model training policy learning. understand importance translation cost remove cost ltrans understand whether features need properly aligned remove model losses lrec lalign. removal losses signiﬁcantly hurts performance subsequent imitation. removing feature tracking loss rfeat image tracking loss ˆrimage overall performance across tasks worse. figure ablations model losses reward functions reaching pushing pushing real world demonstrations tasks. across tasks components model necessary success. higher better method components consistently best across tasks", "year": 2017}