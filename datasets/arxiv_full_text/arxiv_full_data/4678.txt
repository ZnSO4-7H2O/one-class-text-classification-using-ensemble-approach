{"title": "Simple Regret Optimization in Online Planning for Markov Decision  Processes", "tag": ["cs.AI", "cs.LG"], "abstract": "We consider online planning in Markov decision processes (MDPs). In online planning, the agent focuses on its current state only, deliberates about the set of possible policies from that state onwards and, when interrupted, uses the outcome of that exploratory deliberation to choose what action to perform next. The performance of algorithms for online planning is assessed in terms of simple regret, which is the agent's expected performance loss when the chosen action, rather than an optimal one, is followed.  To date, state-of-the-art algorithms for online planning in general MDPs are either best effort, or guarantee only polynomial-rate reduction of simple regret over time. Here we introduce a new Monte-Carlo tree search algorithm, BRUE, that guarantees exponential-rate reduction of simple regret and error probability. This algorithm is based on a simple yet non-standard state-space sampling scheme, MCTS2e, in which different parts of each sample are dedicated to different exploratory objectives. Our empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art. We then extend BRUE with a variant of \"learning by forgetting.\" The resulting set of algorithms, BRUE(alpha), generalizes BRUE, improves the exponential factor in the upper bound on its reduction rate, and exhibits even more attractive empirical performance.", "text": "consider online planning markov decision processes online planning agent focuses current state only deliberates possible policies state onwards interrupted uses outcome exploratory deliberation choose action perform next. performance algorithms online planning assessed terms simple regret agent’s expected performance loss chosen action rather optimal followed. date state-of-the-art algorithms online planning general mdps either best eﬀort guarantee polynomial-rate reduction simple regret time. introduce monte-carlo tree search algorithm brue guarantees exponentialrate reduction simple regret error probability. algorithm based simple non-standard state-space sampling scheme mctse diﬀerent parts sample dedicated diﬀerent exploratory objectives. empirical evaluation shows brue provides superior performance guarantees also eﬀective practice favorably compares state-of-the-art. extend brue variant learning forgetting. resulting algorithms brue generalizes brue improves exponential factor upper bound reduction rate exhibits even attractive empirical performance. erman deﬁned possible agent states agent actions stochastic transition function reward function s×a×s depending problem domain representation language description either declarative generative case description assumed concise. declarative models provide agents greater algorithmic ﬂexibility generative models expressive types models allow simulated execution feasible action sequences state mdp. current state agent fully observable objective agent maximize accumulated reward. ﬁnite horizon setting used paper reward accumulated predeﬁned number steps agent rather computing quality policy entire taking action focuses action perform next. decision process consists deliberation phase planning terminated either according predeﬁned schedule external interrupt followed recommended action current state. action applied real environment decision process repeated obtained state select next action quality action recommended state steps-to-go assessed terms probability sub-optimal terms measure simple regret latter captures performance loss results taking following optimal policy remaining steps instead following beginning recent exceptions developed declarative mdps algorithms online planning constitute variants called monte-carlo tree search earliest best-known mcts algorithms mdps sparse sampling algorithm kearns mansour sparse sampling oﬀers nearoptimal action selection discounted mdps constructing sampled lookahead tree time exponential discount factor suboptimality bound independent state space size. however terminated action proved near-optimal sparse sampling oﬀers quality guarantees action selection. thus really setup online planning. several later works introduced interruptible anytime mcts algorithms mdps probably widely used algorithm days. anytime mcts algorithms designed provide convergence best action enough time given deliberation well gradual reduction performance loss deliberation time successors devised speciﬁcally mdps algorithms also successfully used partially observable adversarial settings general relative empirical attractiveness various mcts planning algorithms depends speciﬁcs problem hand cannot usually predicted ahead time. comes formal guarantees expected performance improvement planning time algorithms provide guarantees general mdps none breaks barrier worst-case polynomial-rate reduction simple regret choice-error probability time. precisely contribution here. introduce monte-carlo tree search algorithm brue guarantees exponential-rate reduction simple regret choice-error probability time general mdps ﬁnite state spaces. algorithm based simple eﬃciently implementable sampling scheme mctse diﬀerent parts sample dedicated diﬀerent competing exploratory objectives. motivation objective decoupling came recently growing understanding current mcts algorithms mdps optimize reduction simple regret directly optimizing called cumulative regret performance measure suitable setting reinforcement learning empirical evaluation standard benchmarks comparison mcts planning algorithms shows brue provides superior performance guarantees also eﬀective practice favorably compares state art. extend brue variant learning forgetting. resulting family algorithms brue generalizes brue improves exponential factor upper bound reduction rate exhibits even attractive empirical performance. mcts performs iterative construction tree rooted iteration mcts issues state-space sample expands tree using outcome sample updates information stored nodes simulation phase over mcts uses information collected nodes recommend action perform compatibility notation prior literature follows refer tree nodes states associated nodes. note that markovian nature mdps unreasonable distinguish nodes associated state depth. hence actual graph constructed instances probably popular algorithm days give concrete sense mcts’s components well ground intuitions discussed later describe speciﬁc setting mcts corresponding core algorithm figure illustrates tree construction denoting number state-space samples. update-statistics procedure. given next-on-the-sample action selected according deterministic policy originally proposed optimal cumulative regret minimization stochastic expand-tree state-space sample induces state trace inside well state trace outside principle expanded preﬁx popular choice prior work appears expanding upper-most node si+. interestingly action recommendation protocol never properly speciﬁed diﬀerent applications adopt diﬀerent decision rules including maximization estimated q-value augmented estimated q-value number times action selected simulation well randomized protocols based information collected root. property exploration search space obtained considering hierarchy forecasters minimizing cumulative regret loss total reward incurred exploring environment pseudo-agent forecaster corresponds state/steps-to-go pair respect according theorem kocsis szepesv´ari asymptotically achieves best possible cumulative regret. however recently pointed numerous works cumulative regret seem right objective online planning rewards collected simulation phase ﬁctitious. furthermore work bubeck munos stoltz multiarmed bandits shows minimizing cumulative regret minimizing simple regret somewhat competing objectives. indeed theorem kocsis szepesv´ari claims polynomial-rate reduction probability choosing non-optimal action results bubeck simple regret minimization mabs stochastic rewards imply achieves polynomial-rate reduction simple regret time. attempts recently made adapt mcts-based planning general optimizing simple regret online planning directly attempts empirically rather successful however best knowledge none breaks uct’s barrier worst-case polynomial-rate reduction simple regret time. show exponential-rate reduction simple regret online planning achievable. ﬁrst motivate introduce family mcts algorithms two-phase scheme generating state space samples describe concrete algorithm family brue guarantees probability recommending nonwork bubeck pure exploration multi-armed bandit problems probably ﬁrst stress minimal simple regret increase bound cumulative regret decreases. high level bubeck show eﬃcient schemes simple regret minimization exploratory possible thus improving expected quality recommendation issued learning process. particular showed simple round-robin sampling actions followed recommending action highest empirical mean yields exponential-rate reduction simple regret strategy balances exploration exploitation yields polynomial-rate reduction measure. respect situation mdps seemingly diﬀerent thus monte-carlo planning focus exploration only. however answer question means exploratory possible mdps less straightforward special case mabs. intuition pure exploration dilemma mdps somewhat complicated consider state/steps-to-go pairs pseudo-agents acting behalf root pseudo-agent aims minimizing simple regret stochastic induced applicable actions clearly oracle would provide optimal action deliberation would needed execution however task characteristics exception rather rule. suppose oracle provides optimal actions pseudoagents despite richness information sense remains clueless before choose actions needs least ordinal information expected value alternatives. hence sampling futures non-root pseudo-agent devoted objectives note objectives exploratory problem somerespect choices made actually make sense competing. sample issued priori devoted increasing conﬁdence samples unavoidable learning acting setup reinforcement learning necessarily case online planning. moreover sample overloading comes high price shown coquelin munos number samples bounds simple cumulative regret become meaningful might high hyper-exponential state possible outcome states action ﬁnite horizon first sense separation exploratory concerns online planning begin perspective mdps corresponding policy acting steps starting current state policy minimal partial mapping state/steps-to-go pairs actions fully speciﬁes acting strategy steps starting sampling straightforward prescribes precisely action applied every state possibly encountered along execution reward stochastic consider simple algorithm naiveuniform systematically samples policy loop updates estimation corresponding obtained reward. stopped iteration algorithm recommends arm/policy best empirical value ˆµπn. iteration algorithm sampled least note naiveuniform uses sample update estimation single policy however recalling arms problem actually compound policies sample principle used update craftyuniform generates samples choosing actions along uniformly random uses outcome sample update policies consistent note sampling arms craftyuniform cannot done systematically naiveuniform policies updated iteration stochastic. note transition period length still much better hyper-exponential moreover unlike rate simple regret reduction exponential number iterations. simple regret convergence rate well length transition period craftyuniform attractive much help craftyuniform requires explicit reasoning arms thus cannot eﬃciently implemented. however show promise separation concerns online planning. introduce mcts family algorithms referred mctse allows utilizing promise large extent. generation done phases iteration actions states selected according exploration policy algorithm actions states selected according estimation policy. sample suﬃxes improving value estimates current candidates particular separation allows introduce speciﬁc mctse instance brue tailored simple regret minimization. brue setting mctse described below figure illustrates dynamics. sample issued iteration state/action pair immediately preceding switching state along updated. information obtained second phase used improving estimate state pushed sample. appear wasteful even counterintuitive locality update required satisfy formal guarantees brue discussed below. rather general parametrizations even guarantee convergence optimal action. this instance case modiﬁcation brue purely uniform estimation policy. short mctse parametrized care. second follows focus brue instances mctse appear empirically eﬀective well respect reduction simple regret time. them similarly brue also guarantee exponential-rate reduction simple regret time. hence clearly cannot claim uniqueness brue respect. finally families mcts algorithms sophisticated mctse give rise even eﬃcient optimizers simple regret. brue algorithms discuss later example. result brue algorithm theorem below. proof theorem well several required auxiliary claims given appendix outline issues addressed proof provide high-level proof terms central auxiliary claims. eters possible dependence partly alleviated bubeck showed distribution-free exponential bounds simple regret reduction rate cannot achieved even mabs even single-step-to-go mdps based lower bound cumulative probably eliminated accurate bounding numerous bounding steps towards proof theorem however improvements tried made already lengthy proof theorem even involved. deﬁnition rewards planned initial state ﬁnite horizon state reachable steps still action applicable policy induced running brue exactly samples ﬁnished exploration phase applying action lemma rewards planned initial state ﬁnite horizon state reachable steps still action applicable considering eth+ δth+ deﬁnition lemma holds horizon consider evolution action value estimates brue time that internal nodes estimates based biased samples stem selection non-optimal actions descendant nodes. bias tends shrink samples accumulated tree. consequently estimates become accurate probability selecting optimal action increases accordingly bias ancestor nodes shrinks turn. interesting question context shouldn’t weigh diﬀerently samples obtained diﬀerent stages sampling process? intuition tells biased samples still provide valuable information especially have value information decreases obtain accurate samples. hence principle putting weight samples smaller bias could increase accuracy estimates. question course possible weighting schemes reasonable employ preserve exponential-rate reduction expected simple regret. estimates fraction recent samples. discuss value addition perspective formal guarantees well perspective empirical prospects. brue diﬀers brue points proof lemma induction following line proof lemma fact deviates latter application modiﬁed hoeﬀding-azuma inequality modiﬁed capture partial sums brue. brue poses tradeoﬀ decreasing reduces sampling bias thus decreases term increases exponential term leading constant. obviously since bias leaf nodes makes sense there. however tree bias tends grow also expect samples thus perspective formal guarantees seems appealing choose smaller values nevertheless optimize value first optimizing bounds doesn’t necessarily lead optimized empirical accuracy. second underlying optimization would speciﬁc horizon sample size thus anyway would consider rough approximations optimization problem. finally biased samples practice might valuable theory suggests long actions state/steps-to-go decision point experience similar bias. evaluated brue empirically sailing domain used previous works evaluating planning algorithms well random game trees used original empirical evaluation sailing domain sailboat navigates destination -connected grid representing marine environment ﬂuctuating wind conditions. goal reach destination quickly possible choosing grid location neighbor location move duration move depends direction move direction wind relative sailing direction tack. direction wind changes time strength assumed ﬁxed. sailing problem formulated goal-driven ﬁnite state space ﬁnite actions state capturing position sailboat wind direction tack. instance unlikely optimal path locations grid larger complete encircling considered area. note however recommendation-oriented samples always terminal state similar rollouts issued \u0001-greedy uct. snapshots results diﬀerent grid sizes shown figure compared brue mcts-based algorithms algorithm recent modiﬁcation \u0001-greedy obtained former replacing policy root node \u0001-greedy policy motivation behind design \u0001-greedy improve empirical simple regret results \u0001-greedy reported impressive. also show results brueper slight modiﬁcation brue permissive update scheme instead updating stateaction node level switching point also update ancestor either applicable actions sampled chosen action identical best empirical one. four algorithms implemented within single software infrastructure. suggested recent works exploration coeﬃcient \u0001-greedy empirical best value action decision point parameter \u0001-greedy experiments tolpin shimony algorithm randomly chosen initial states performance algorithm assessed terms average error diﬀerence true values action chosen algorithm optimal action \u0001-greedy larger tasks less notable. turn brue substantially outperformed \u0001-greedy improvement consistent except relatively short planning deadlines brueper performed even better brue. allows conclude brue attractive terms formal performance guarantees also eﬀective practice online planning. likewise learning forgetting extension brue also practical merits. parameter setting \u0001-greedy also evaluated three algorithms domain random game trees whose goal simple modeling two-person zero-sum games amazons globber. games winner decided global evaluation board evaluation employing another feature counting procedure; rewards thus associated terminal states. rewards calculated ﬁrst assigning values moves summing values along paths terminal states. note move values used tree construction made available players. values chosen players maximize/minimize individual payoﬀ reach terminal high possible objective similar mutatis mutandis. simple game tree model similar spirit many game tree models used previous work except success/failure players measured ternary scale win/lose/draw actual payoﬀs receive. experiments diﬀerent settings branching factor tree depths sailing domain compared convergence rate obtained brue \u0001-greedy uct. figure plots average error rate conﬁgurations average setting obtained trees. results appear encouraging well brue overtaking algorithms quickly deeper trees. introduced brue simple monte-carlo algorithm online planning mdps guarantees exponential-rate reduction performance measures interest namely simple regret probability erroneous action choice. improves previous algorithms guarantee polynomial-rate reduction measures. algorithm formalized ﬁnite horizon mdps analyzed such. however empirical evaluation shows also performs well goal-driven mdps two-person games. setting γ-discounted mdps inﬁnite horizons straightforward employ brue horizon algorithm derive guarantees aforementioned measures interest simh guarantees brue improved employing sophisticated combinations exploration estimation samples. another important point consider speed convergence optimal action opposed speed convergence good actions. brue geared towards identifying optimal action although many large mdps good often best hope for. identify optimal solution brue devotes samples equally depths. however focusing nodes closer root node improve quality recommendation planning time severely limited. finally core tree sampling scheme employed brue diﬀers standard scheme employed previous work. diﬀerence plays critical role establishing work partially supported carried technion-microsoft electronic commerce research center well partially supported force oﬃce scientiﬁc research usaf grant number fa---.", "year": 2012}