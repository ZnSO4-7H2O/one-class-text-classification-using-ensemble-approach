{"title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.", "text": "many modern computer vision machine learning applications rely solving difﬁcult optimization problems involve non-differentiable objective functions constraints. alternating direction method multipliers widely used approach solve problems. relaxed admm generalization admm often achieves better performance efﬁciency depends strongly algorithm parameters must chosen expert user. propose adaptive method automatically tunes algorithm parameters achieve optimal performance without user oversight. inspired recent work adaptivity proposed adaptive relaxed admm derived assuming barzilaiborwein style linear gradient. detailed convergence analysis aradmm provided numerical results several applications demonstrate fast practical convergence. modern methods computer vision machine learning often require solving difﬁcult optimization problems involving non-differentiable objective functions constraints. popular applications include sparse models low-rank models support vector machines alternating direction method multiplier prominent optimization tools solve problems tackles problems following form convergence admm guaranteed fairly general assumptions penalty relaxation parameters held constant. however practical performance admm depends strongly choice parameters well problem solved. good penalty choices known certain admm formulations strictly convex quadratic problems gradient descent parameter linearized admm adaptive penalty methods achieve good performance without user oversight. nonrelaxed admm authors propose methods modulate penalty parameter primal dual residuals approximately equal size. residual balancing approach generalized work preconditioned variants admm distributed admm spectral penalty parameter method proposed uses local curvature objective achieve fast convergence. methods study conditions admm converges adaptive penalty relaxation parameters. approach utilizes variational inequality methods forward results measure convergence using primal dual residuals deﬁned τkat section prove adaptive relaxed admm converges algorithm parameters satisfy either assumption assumption presenting proof show choose relaxation parameters lead efﬁcient performance practice. spectral stepsize selection methods vanilla admm discussed here modify adaptive admm framework important ways. first discuss selection penalty parameters presence relaxation term. second discuss adaptive methods also automatically selecting relaxation parameter. paper study adaptive parameter choices relaxed admm jointly automatically tune penalty parameter relaxation parameter section address theoretical questions convergence admm non-constant penalty relaxation parameters. section discuss practical methods choosing parameters. section apply proposed aradmm several problems machine learning computer vision image processing. finally section compare aradmm admm variants examine beneﬁts proposed approach real-world regression classiﬁcation image processing problems. sparse rank methods widely used computer vision machine learning image processing admm extensively applied solve problems recently found applications neural networks tensor decomposition structure motion vision problems. convergence rate non-relaxed admm established mild conditions convex problems convergence rate discussed least functions assumed either strongly convex smooth. general relaxed admm formulation convergence rate provided mild conditions linear convergence achieved strong convexity assumptions results assume constant parameters—it considerably harder prove convergence algorithm parameters adaptive. fixed optimal parameters discussed literature. speciﬁc case objective quadratic criterion proposed authors suggest grid search semideﬁnite programming based method determine optimal relaxation penalty parameters. methods however make strong assumptions objective require knowledge condition numbers. adaptive penalty methods proposed accelerate practical convergence non-relaxed admm relaxation parameter suggested over-relaxation accelerate convergence achieves faster convergence speciﬁc distributed computing application. proposed aradmm simultaneously adapts penalty relaxation parameter thus fully automated. relaxation parameter denotes subdifferential evaluated referring back admm deﬁning ˆλk+ sequences satisfy conditions thus admm problem equivalent dual detailed proof provided supplementary material. adaptive stepsize rules spectral type originally proposed simple gradient descent smooth problems barzilai borwein found dramatically outperform constant stepsizes many applications spectral stepsize methods work modeling gradient objective linear function selecting optimal stepsize simpliﬁed linear model. spectral methods recently used determine penalty parameter non-relaxed admm inspired work derive spectral stepsize rules assuming linear model/approximation iteration given local curvature estimates respectively obtain curvature estimates exploit following simple proposition whose proof given supplementary material. adaptive method works ﬁtting linear model gradient objective using proposition select optimal stepsize pair obtains zero residual model problem. convergence theory hold need ﬁxed values minimal value still optimal linear model occurs choose propose simple method ﬁtting linear model dual objective terms formulas section used obtain stepsizes. linear models formed optimal penalty parameter relaxation term calculated thanks equivalence relaxed admm drs. estimation dual components k-th iteration primal admm described easy verify model parameters relaxed admm estimated based results iteration older iteration similar way. deﬁne spectral stepsize methods simple gradient descent paired backtracking line search guarantee convergence case linear model assumptions break unstable stepsize produced. admm methods analog backtracking. rather adopt correlation criterion proposed test validity local linear assumption rely adaptive model assumptions deemed valid. deﬁne constant. easily veriﬁed parameter sequence satisﬁes assumption practice update schemes converges reliably without explicitly enforcing conditions. large conditions triggered ﬁrst thousand iterations provide constraints theoretical interests. complete adaptive relaxed admm shown algorithm suggest updating stepsize every iterations. suggest ﬁxed safeguarding threshold \u0001cor used experiments section overhead adaptive scheme modest requiring inner product calculations. \u0001cor quality threshold curvature estimates spectral stepsizes estimated section update uses model parameters accurately estimated. model effective large make update conservative relative update. model effective small make update aggressive relative update. convergence theory requires either assumption assumption satisﬁed suggests convergence guaranteed bounded adaptivity penalty relaxation parameters. conditions guaranteed explicitly adding constraints stepsize choice aradmm. apply order prove following lemmas contraction proof show difference iterates decreases iterates approach true solution. ‘the remaining details proof supplementary material. ready state main convergence results. proof theorem shown full leverages lemma produce contraction argument. proof theorem extremely similar shown supplementary material. consensus -regularized logistic regression admm become important tool solving distributed optimization problems typical problem consensus -regularized logistic regression represents local variable distributed node global variable number samples block sample corresponding label. unwrapped unwrapped formulation used distributed computing environments transpose reduction tricks applies admm primal form solve sample training data corresponding label. admm applied splitting -norm regularizer non-differentiable hinge loss term. total variation image denoising total variation image denoising often performed solving represents given noisy image discrete gradient operator computes differences adjacent image pixels. admm applied splitting -norm term non-differentiable total variation term. rpca robust principal component analysis broad applications computer vision imaging rpca recovers low-rank matrix sparse matrix solving focus following statistical image processing problems involving non-differentiable objectives linear regression elastic regularization low-rank least squares quadratic programming consensus -regularized logistic regression support vector machine total variation image restoration robust principle component analysis study several vision benchmark datasets extended yale face dataset mnist digital images cifar object images also synthetic benchmark datasets obtained repository libsvm page. experimental setups problem brieﬂy described here implementation details provided supplementary material. low-rank least squares nuclear norm -norm matrix singular values) convex surrogate matrix rank. admm applied solve rank least squares problems denotes nuclear norm denotes frobenius norm rn×m data matrix rn×d contains measurements rm×d contains variables. admm applied splitting regression term non-differentiable regularizer composed nuclear frobenius norm. lrls used formulate exemplar classiﬁers discover visual subcategories relaxation parameter non-adaptive relaxed admm ﬁxed suggested parameters aadmm selected initial penalty initial relaxation used problems except canonical problem initial parameters geometric mean maximum minimum eigenvalues matrix proposed quadratic problems table reports convergence speed admm variants applications described section experimental results including table test cases convergence curves visual results image restoration robust face decomposition provided supplementary material. relaxed admm often outperforms vanilla admm compete adaptive methods like aadmm aradmm. proposed aradmm performs best test cases. study sensitivity different admm variants initial penalty initial relaxation parameter fig. presents iteration counts wide range values elastic regression synthetic datasets. left center plots vary other. number iterations needed convergence plotted algorithm parameters vary. right plot grid search optimal different figure sensitivity convergence speed synthetic problem regularized linear regression. sensitivity initial penalty sensitivity relaxation sensitivity relaxation optimal selected grid search. based curvature estimations always accepted \u0001cor parameters never changed. proposed aadmm method insensitive \u0001cor performs well wide range \u0001cor various applications except unwrapping rpca. though tuning hyper-parameters improve performance aradmm applications ﬁxed \u0001cor performs well experiments proposed aradmm fully automated performs well without parameter tuning. proposed adaptive method jointly tuning penalty relaxation parameters relaxed admm without user oversight. analyzed adaptive relaxed admm schemes provided conditions convergence guaranteed. experiments wide range machine learning computer vision image processing benchmarks demonstrated proposed adaptive method outperforms admm variants without user oversight parameter tuning. adaptive method improves applicability relaxed admm facilitating fully automated solvers exhibit fast convergence usable non-expert users. supported ofﬁce naval research grant n--- national science foundation grant ccf-. partially supported fundac¸˜ao para ciˆencia tecnologia grant uid/eea//. supported general research fund hong kong research grants council grant hkbu-. supported part xilinx inc. grants eccs- ccf- career ccf. figure sensitivity convergence speed safeguarding threshold \u0001cor proposed aradmm. synthetic problems various applications studied. best viewed color. values fig. shows adaptive methods relatively stable respect initial penalty aradmm outperforms aadmm choices initial fig. suggests relaxation generally less important value chosen unlikely good choice compensate. proposed aradmm jointly adjusts generally better simply adding relaxation existing adaptive methods aadmm. fig. shows sensitivity using grid search choose optimal optimal signiﬁcantly improves performance vanilla admm relaxed admm even using optimal stepsize non-adaptive methods aradmm superior competitive non-adaptive methods. note experiment meant show best-case scenario non-adaptive methods; practice user generally knowledge optimal value adaptive methods achieve optimal nearoptimal performance without expensive grid search.", "year": 2017}