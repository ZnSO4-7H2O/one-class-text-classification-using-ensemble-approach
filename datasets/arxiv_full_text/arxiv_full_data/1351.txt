{"title": "Towards universal neural nets: Gibbs machines and ACE", "tag": ["cs.CV", "cs.LG", "cs.NE", "I.2.6; I.4; I.5; I.6"], "abstract": "We study from a physics viewpoint a class of generative neural nets, Gibbs machines, designed for gradual learning. While including variational auto-encoders, they offer a broader universal platform for incrementally adding newly learned features, including physical symmetries. Their direct connection to statistical physics and information geometry is established. A variational Pythagorean theorem justifies invoking the exponential/Gibbs class of probabilities for creating brand new objects. Combining these nets with classifiers, gives rise to a brand of universal generative neural nets - stochastic auto-classifier-encoders (ACE). ACE have state-of-the-art performance in their class, both for classification and density estimation for the MNIST data set.", "text": "conditional independence. exponential/gibbs class densities. macroscopic quantities. variational pythagorean theorem. generative error. conditional latent densities. boltzmann-gibbs thermodynamic identity. generative thermodynamic identity. q-gibbs densities. perturbative nets. reconstruction error. fully-generative nets. regimes. variational error. cross-entropy decomposition. cross-entropy upper bound. gibbs machines. estimating variational error. generative conditional independence. study physics viewpoint class generative neural nets gibbs machines designed gradual learning. including variational auto-encoders offer broader universal platform incrementally adding newly learned features including physical symmetries. direct connection statistical physics information geometry established. variational pythagorean theorem justiﬁes invoking exponential/gibbs class probabilities creating brand objects. combining nets classiﬁers gives rise brand universal generative neural nets stochastic auto-classiﬁerencoders state-of-the-art performance class classiﬁcation density estimation mnist data set. universality. probabilistic quantum viewpoint equilibrium setting. gibbs machines. observation entropies equilibrium curse. intricates. symmetries latent manifold. ace. buck recent trend building highly specialized neural nets exploring nets accomplish multiple tasks without compromising performance. universal tentatively described which among things works variety applications i.e. visual recognition/reconstruction speech recognition/reconstruction natural language processing etc; performs various tasks classiﬁcation generation probability density estimation etc; iii) self-contained i.e. specialized external machine learning methods; biologically plausible. input neural typically data matrix span space observations column-vectors {xi}n ...n example enumerate pixels screen span space observables. asked perform classiﬁcation estimation generation tasks generative nets accomplished randomly generating latent observations every observation induced uncertainty µ-th state modeled model conditional density copy-cat imaginary time/space wave function quantum mechanics fully describes µ-th conditional state statistical mechanics parlance latents ﬂuctuating microscopic variables macroscopic observables obtained aggregation. absence physical time observations thus interpreted partial equilibria independent small parts expanded original data set. simply every visible observations surrounded cloud virtual observations. creating original observation amounts nothing sampling cloud. strictly speaking employ unbounded densities hence stochastic analysis formalism centerpiece diffusion equation. formally equivalent quantum-mechanical formalism centerpiece schrodinger equation imaginary time/space coordinates. equilibrium i.e. small ﬂuctuations viewpoint statistical mechanics appears originated einstein then-unpublished lecture einstein used exponential model density pexp space derived brownian diffusion i.e. gaussian model density space/time. special cases broad class densities gibbs exponential densities form foundation classic statistical mechanics. gibbs densities variational maximum-entropy densities hence optimal modeling equilibria. also offer platform adding incrementally macroscopic descriptive variables landau lifshitz section argue sub-sections gibbs densities also optimal modeling fully-generative equilibrium nets call nets gibbs machines. inspired ﬁrst fully-generative nets variational auto-encoders kingma welling rezende employ upper bound cross-entropy target like physics counterparts gibbs machines offer platform mimicking gradual nature learning already learned symmetry statistics like space/time symmetries added incrementally accelerate learning sections unlike equilibrium statistical mechanics human data decidedly non-equilibrium nature exhibits large ﬂuctuations non-gaussian behavior. quantifying non-gaussianity distance equilibrium easy dealing large number observables plot figure shows gaussianization effect non-linearities dropout mnist data lecun bounded non-linearities gaussianize because compressive nature straighten unlikely observations refer intricates. dropout gaussianizes drops latent variables thus decreases kurtosis. figure plots gaussian density negative log-likelihoods pg}µ mnist test observations layer -layer standard feed-forward classiﬁer right branch figure appendix implementation details. layer sizes ----. learning rate decay epochs batch size right plot dropout input layer hidden layers. exception rules appendix tanh activation function used ﬁrst hidden layer. left. dropout non-linearity highly non-gaussian. right. dropout non-linearity severely gaussianized especially intricates towards right precisely intricates entropy extreme non-gaussianity figure ideal candidates feature vectors classiﬁcation tasks hyvarinen section conjugates receptive ﬁelds feature detectors open problem section show plot figure least likely images mnist ascending recall that given row-vector observation conjugate xµc− covariance matrix. constant gaussian negative log-likelihood thus inner product observation conjugate standard euclidean metric observations luckily one-dimensional proxy non-gaussianity multi-dimensional data non-gaussianity negative gaussian loglikelihoods pg}µ. here zµc−zµ +const multivariate gaussian nnlat) model density empirical covariance. bold counter-intuitive re-read boltzmann’s statistical mechanics einstein interpreted loglikelihoods {log pg}µ observation entropies einstein denote einstein entropy observation sub-section viewpoint observation partial equilibrium multiple virtual observations right einstein’s paradigm entropy deﬁned classical boltzmann fashion cloud virtual observations. visible observation stands locally maximum entropy. observation entropies central modern theory ﬂuctuations landau lifshitz chapter also elegant linear-algebraic incarnation singular value decomposition data matrix addition second moment kurtosis measuring fatness probability density {zµ}. unfortunately features modern neural nets like non-linear activation functions dropout srivastava come high price gaussianizing data i.e. lead higher-entropy less informative conﬁgurations. right quantile-quantile density mardia sections typical case nlat proportional chi-squared density turn converges rescaled gaussian nlat order negative logs thought entropies large positive constant added. clear context nevertheless clarity superscript distinguish einstein entropy observations standard boltzmann entropy probability density introduced sub-section figure left. ﬁrst mnist training images projected three least likely i.e. intricate conjugate images ranked ascending order einstein entropy highly nongaussian -dimensional distribution. right. mnist images projected three likely conjugate images ranked einstein entropy. much gaussian-looking. shows dominant dimension classes mnist. so-called manifold learning modern feed-forward nets pioneered contractive auto-encoders rifai symmetry context loosely speaking one-dimensional parametric transformation leaves cross-entropy unchanged. probabilistic terms equivalent existence one-parametric density symmetric observations sampled below. nets currently learn symmetries training data artiﬁcially augmented e.g. adding rotated translated rescaled images case visual recognition. symmetry learned make sense re-learn every data set. figure dominant dimension mnist classes corresponding separate class. rotational symmetry dominates classes size i.e. scaling symmetry clearly dominates class digit creative regime sub-section equally spaced deterministic grid latent layer sizes ---- branch branch figure appendix learning rate decay epochs batch size symmetry explicitly latent layer alongside noether invariant gelfand fomin take example translational symmetries two-dimensional system coordinates imply conservation horizontal vertical momenta −i\u0002∂/∂z) quantum mechanical wave function p−h)+ p−v) offsets landau lifshitz section switching figure top. lowest-entropy mnist training images using einstein entropy class corresponding digit quite intricate indeed. bottom. highest-entropy mnist training images class. much vanillalooking. i.e. two-dimensional laplacian gibbs machine paradigm demonstrate section build-in translational scaling rotational symmetry computing symmetry statistics like explicitly estimating invariants rest parameters. general reﬁned optimization e.g. jadeberg details georgiev order preserve non-gaussianity data improve performance signiﬁcantly along combine classiﬁers auto-encoders hence name auto-classiﬁer-encoder auto-encoders reconstruction error cross-entropy optimization target thus force faithful data. simultaneously classiﬁes reconstructs assuming independence hence additivity respective cross-entropies ﬁrst non-generative installment standard classiﬁer shallow auto-encoder dual space observations. still beats handily peers class figure right. technically laplacian exponential class exponential densities domains deﬁned mean densities exponential class respective domains. laplacian biologically-plausible bi-product squaring gaussians. versal hence tend work better dimension latent layers nlat i.e. so-called overcomplete representation coates nlat given n-dimensional observation small number latents {zµj}nlat deviate signiﬁcantly zero. sparse representations sampling high-entropy gaussian-like densities right plot figure ﬂawed. sampling instead fat-tail densities offers signiﬁcant performance improvement mnist figure right. mathematical ﬁnance stochastic volatility jumps arguably ﬁrst natural source non-gaussianity almost fully-tractable. q-gibbs machines offer anvenue sub-section even greater issue current nets spontaneous clumping clusterization prevalent real-life data sets. statistical mechanics deals introducing higher-hierarchy densities conditional low-hierarchy densities. fermi density discussed sub-section example higher-hierarchy density built boltzmann density subject additional constraints landau lifshitz section clusterization aggregates low-hierarchy partial equilibria observations higher-hierarchy partial equilibria clusters sub-section mimic universal phenomenon second generative installment combines classiﬁer generative auto-encoder space observables brand auto-encoder supervision figure generalizes classic idea using separate decoders separate classes hinton training conditional latent density sub-section generalized class label µ-th observation. since course classiﬁcation labels used testing sampling during testing mixture densities class probabilities supplied classiﬁer mixture densities posterior also used kingma albeit different architecture. universal sense subsection achieves state-of-the-art performance classiﬁer density estimator figure relation information geometry open problem section figure architecture stands auto-encoder stands classiﬁer. training supervised i.e. labels used auto-encoder class separate decoder unimodal sampling latent layer. ωµcp class probabilities statistical physics e.g. naudts formally consider generalized entropy case non-trivial base measure below. discussed sub-section latents generative nets sampled closed-form conditional model density latents course given priori joint empirical density corresponding marginal empirical densities section cover thomas problem arithmetic average across observations bayes identity optimization target this decomposition imply independence observations latent variables general contain information observation example case time series autoregression. boltzmann entropy model distribution conditional given observation sample latent observables observation commonly right-hand side reduces done conditional independence. hidden/latent observables {zj}nlat conditionally independent given observation independence bound boltzmann entropy cover thomas chapter conditional independence minimizes negative entropy term right-hand side everything else equal conditional independence hence optimal nets. broad class probability density families gibbs a.k.a. canonical exponential families dominate choices model densities physics neural nets. class includes sufﬁciently large number density families gaussian bernoulli exponential gamma etc. general closed form physics expectations sufﬁcient statistics epλ] form complete macroscopic thermodynamic quantities state variables like a.k.a. energy momenta number particles fully describing µ-th conditional state sub-section landau lifshitz sections neural nets sufﬁcient statistics typically monomials like whose expectations form vector moments. proposed sub-section list symmetry statistics section details. deﬁnition free energy derives immediately generative function expectations exponential/gibbs class families special variational maximum entropy class base density trivial unique functional form maximizes boltzmann entropy across universe densities given macroscopic quantities cover thomas chapter natural parameters computed satisfy constraints. lagrange constraints multipliers variational calculus derivation maximum entropy property. gibbs class special even stronger sense minimum divergence class. arbitrary base density kullback-leibler divergence d||p)) minimizes divergence d||p) across universe densities given macroscopic quantities follows variational pythagorean theorem figure chentsov kulhav`y section d||p) d||pλ) d||p) sub-section minimizing divergence d||p) across unknown priori family conditional distributions crucial quality generative net. smaller divergence likely sampled newly created objects resemble training set. minimum divergence property implies always better choosing gibbs class hence name gibbs machines. refer minimum divergence d||p) generative error superscript ‘gen’ short generative. shortly free energy negative generative error −dgen previous sub-section concave conjugates hence shared superscript. practice order tractable speciﬁc parametric family within gibbs class tractable family e.g. gaussian exponential etc. except symmetry statistics introduced secmacroscopic quantities tions µ-th quantum state free parameters. together symmetry statistics thought quantum numbers distinguishing observations a.k.a. partial equilibrium states another spirit quantum statistics landau lifshitz section sub-section here. quantum numbers added rest free parameters optimized standard methods like backpropagation/ stochastic gradient descent etc. identities relating various macroscopic quantities referred statistical physics thermodynamic identities. recall classic boltzmann-gibbs distribution special case exponential/gibbs distribution trivial base density sufﬁcient statistics microscopic energy hamiltonian increase free energy physics interpretation work needed done outside environment order increase macroscopic quantity system i.e. energy case. negative sign conﬁrms intuition that increase internal energy less ordered systems need less work outside. mentioned sub-section general context like negative generative error −dgen) plays role generalized entropy base measure standard statistical mechanics shown chapter −dgen) concave function expressable legendre transform conjugate generative free energy scalar product vectors allowed free. note that variational pythagorean theorem establishes minimum property dgen functional dgen space functions dgen maximum viewed explicit function dgen macroscopic quantities natural parameters equivalently similarly derivative left-hand side deﬁnes function λgen λgen function everywhere. generative free energy deﬁned every unless function invertible image subset full space natural parameters assuming invertibility generalization skipped brevity dependence counter classic thermodynamics plus sign? factors comprising divergence dgen clearly work other. hand negative entropy term reduces free energy usual less ordered systems take less work create. hand system resides inﬁnitely-large thermostat non-trivial density cross-entropy term increases free energy back measures amount work takes counter thermostat’s inﬂuence. chain rule give addition indirect dependence dgen natural parameters gibbs densities special case broad class q-gibbs densities. corresponding nonextensive statistical mechanics tsallis describes adequately long-range-interacting many-body systems like typical human-generated data sets. virtue replacing exponential qexponential q-log deﬁning respective qentropy formalism classic thermodynamics generalized. many properties exponential class remain true q-exponential class fully-generative creates original observations sampling unconditional model density unencumbered observations {xµ}. perturbative nets hand contain unconditional model density rely instead initial observation conditional density decomposition parameter estimation. successful family perturbative nets date boltzmann machines smolensky multiple re-incarnations. assuming completeness state variables boltzmann machines adopt joint model density boltzmann density special case boltzmann-gibbs equilibrium density sub-section single suf. ﬁcient statistics bi-linear hamiltonian function trivial base density temperature tractable partition function computed closed form. hand discrete data conditional density restricted boltzmann machines tractable familiar fermi density intractable joint density term handled approximations gradients like contrastive divergence hinton avoid brute-force monte carlo methods averaging impossibly many paths. simple bi-linear shape hamiltonian rbm-s latent variables conditionally independent visible variables despite limitations handling non-binary data deep boltzmann machines salakhutdinov hinton recently dominant universal nets perform well classiﬁers srivastava probability density estimators salakhutdinov murray neural said reconstruction capabilities decoder given latent assign reconstruction density prec observation following standard procedure reconstruction error usual cross-entropy reconstruction density empirical density reconstruction densities typically exponential/gibbs class bernoulli gaussian unity covariance matrix trivial base density reconstruction macroscopic quantity expectation mrec generally intractable function given decoder. cross-entropy lrec depends generative natural parameters expectation density derivatives lrec) non-creative regime common regime training validation testing. latent observables sampled closed-form model conditional density sub-section observations {xµ} attached net. closed-form reconstruction model density prec subsection also chosen. creative regime trained latent observables {zj}nlat sampled closed-form model density unencumbered observations {xµ}. reconstruction density prec non-creative regime used. implied conditional density generally intractable fully generative nets. moreover implied conditional course different chosen priori non-creative regime. given observation divergence called variational error generative error divergence generative densities non-creative creative regimes. minimizing ensures general similarity objects generated regimes. interpreted hypotenuse variational pythagorean theorem computable closed form many gibbs/exponential densities. lrec reconstruction error measures negative likelihood getting back transformations randomness inside net. computed endowed decoder standard monte carlo averaging traditional auto-encoders importantly training order compute gradients respect generative macroscopic quantities mgen change variable needed replacing sampling sampling transformations exists many exponential/gibbs probability families kingma welling variational error measures divergence chosen functional form latent density implied latent density intractability variational error computed numerically monte carlo methods sub-section open problem section explicit form µ-th observation ep]+ ep]. subtracting ﬁrst term adding third using deﬁnitions generative error dgen reconstruction error lrec variational error dvar ﬁnal expression minimization target dgen lrec dvar. last expression formally equivalent general expression density correct conditional density joint density merely approximation implied conditional clear derivation universal fully-generative nets hence used ﬁrst fully-generative nets vae-s kingma welling rezende vae-s name variational error term introduced context general sampling densities. everything else equal variational pythagorean theorem implies latent sampling densities gibbs class minimize generative error. hence call respective nets gibbs machines. variational error approximation variational principle gibbs class derived fundamental statistical mechanics. intercept error term. observation ﬁrst made richard zhang later used salimans knowles estimate variational error context variational bayes. adding sides adding/subtracting right side recalling transforms into estimated either monte carlo methods closed form. assuming example gaussian variance yields dvar interesting dvar estimates train neural full cross-entropy upper bound open problem section without offering rigorous proof believe conditional independence argument sub-section generalized context given reconstruction error generative error dgen hence upper bound minimized latent variables conditionally independent. gaussian multi-variate sampling follows explicit form generative error table hadamard’s inequality cover thomas chapter show brevity build spatial invariances -dim square visual recognition model. real-life data sets symmetry statistics computed another georgiev every observable i.e. pixel assigned horizontal vertical integer coordin}. nates screen e.g. coordinates row-observation {xµi}n becomes matrix-observation {xµhivi} layer size center mass becomes layer size every observation deﬁnes latent symmetry statistics {hµ}p sub-sections without loss generality assumed hence coordinate system centered every coordinates zero observables observation. coordinates layer size becomes layer size summary auto-encoders apply mapping input inverse output net; classiﬁers apply mapping input only; iii) both include addition symmetry statistics latent layer needed georgiev prior model density symmetry statistics assumed equal parametrized posterior options. sampling symmetry statistics independent laplacians e.g. respective density means hand free density scales parameters principle optimized noncreative regime alongside rest parameters sub-section argued sub-section inverted scales scaled momenta. creative regime sampling e.g. alone horizontally shifted identical replicas. open problem section motivation non-generative comes einstein observation entropies pg}µ sub-section relation singular value decomposition recall data matrix observations observables vλwt matrix projection mapping; diagonals constant negative einstein observation entropies sub-section i.e. gaussian log-likelihoods pg}µ; iii) invariant i.e. consider shallow auto-encoder figure left dual space observations figure right. shown tied weights absence non-linearities optimal nlat hidden layer solution left georgiev figure shallow auto-encoder space observables observations minimization targets reconstruction errors respective spaces lrecon recon deﬁned binarized data appendix non-linearities. figure left. plot right figure non-generative sub-section hyper-parameters right figure right. classiﬁcation error mnist test function training epochs i.e. full swipe training observations. line standard classiﬁer right figure bottom line classiﬁcation error non-generative hyper-parameters. architecture figure minimization target cross-entropy replaced upper bound laplacian sampling density used training mixed laplacian testing explicit formulas generative error appendix generative produces similarly outstanding clasnon-generative minimization target composite cross-entropy replaced dual reconstruction error orthogonality inlat implies need additional batch normalization similar ioffe szegedy appendix best known results test classiﬁcation error feed-forward non-convolutional nets without artiﬁcial data augmentation handle srivastava table shown right figure non-generative offers improvement. siﬁcation results non-generative regular mnist data figure left. even without tweaking hyper-parameters also produces outstanding results density estimation binarized mnist data figure right. upper bound negative loglikelihood handle ballpark best non-recurrent nets gregor table left. classiﬁcation error mnist figure test set. line standard classiﬁer right figure bottom lines generative classiﬁcation mode gaussian sampling laplacian layer sizes branch branch figure appendix learning rate decay epochs batch size dual reconstruction error recon sub-section added overall cost. right. upper bound negative log-likelihood binarized mnist test set. line standard gibbs machine gaussian sampling layer sizes ---- hyper-parameters below. middle line laplacian sampling. bottom line generative laplacian mixture sampling. layer sizes branch branch figure appendix learning rate decay epochs batch size freely available intricates sub-section directly feature detectors lieu artiﬁcially computed independent component analysis features hyvarinen estimate variational error richard zhang salimans knowles training minimize full cross-entropy merely upper bound suggested sub-section here. appreciate motivating discussions ivaylo popov nikola toshev. credit goes christine haas coining terms intricates creative/non-creative regimes.", "year": 2015}