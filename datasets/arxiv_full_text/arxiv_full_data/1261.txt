{"title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.", "text": "neural networks grow deeper wider learning networks hard-threshold activations becoming increasingly important network quantization drastically reduce time energy requirements creating large integrated systems deep networks non-differentiable components must avoid vanishing exploding gradients effective learning. however since gradient descent applicable hard-threshold functions clear learn principled way. address problem observing setting targets hard-threshold hidden units order minimize loss discrete optimization problem solved such. discrete optimization goal targets unit including output linearly separable problem solve. given targets network decomposes individual perceptrons learned standard convex approaches. based this develop recursive mini-batch algorithm learning deep hardthreshold networks includes popular poorly justiﬁed straight-through estimator special case. empirically show algorithm improves classiﬁcation accuracy number settings including alexnet resnet- imagenet compared straight-through estimator. original approach neural classiﬁcation learn single-layer models hard-threshold activations like perceptron however proved difﬁcult extend methods multiple layers hard-threshold units zero derivative almost everywhere discontinuous origin cannot trained gradient descent. instead community turned multilayer networks soft activation functions sigmoid recently relu gradients computed efﬁciently backpropagation approach enjoyed remarkable success enabling researchers train networks hundreds layers learn models signiﬁcantly higher accuracy variety tasks previous approach. however networks become deeper wider growing trend towards using hard-threshold activations quantization purposes enable binary low-precision inference rastegari zhou talathi training tang micikevicius greatly reduce energy computation time required modern deep networks. beyond quantization scale output hard-threshold units independent scale input alleviate vanishing exploding gradient issues help avoid pathologies occur low-precision training backpropagation avoiding issues crucial developing large systems deep networks used perform even complex tasks. reasons interested developing well-motivated efﬁcient techniques learning deep neural networks hard-threshold units. work propose framework learning deep hard-threshold networks stems observation hard-threshold units output discrete values indicating combinatorial optimization provide principled method training networks. specifying discrete targets hidden-layer activation network decomposes many individual perceptrons trained easily given inputs targets. difﬁculty learning deep hard-threshold network thus setting targets trained perceptron including output units linearly separable problem solve thus achieve targets. show networks possible learned using mixed convex-combinatorial optimization framework. building framework develop recursive algorithm feasible target propagation learning deep hard-threshold networks. since discrete optimization problem develop heuristics setting targets based per-layer loss functions. mini-batch version ftprop used explain justify oft-used straight-through estimator seen instance ftprop speciﬁc choice per-layer loss function target heuristic. finally develop novel per-layer loss function improves learning deep hard-threshold networks. empirically show improvements algorithm straight-through estimator cifar convolutional networks imagenet alexnet resnet- multiple types hard-threshold activation. common method learning deep hard-threshold networks backpropagation straight-through estimator simply replaces derivative hard-threshold unit identity function. used quantized network literature propagate gradients quantized activations used shalev-shwartz training activations. later work generalized replace hard-threshold derivative functions including saturated versions identity function however tends work quite well practice know rigorous justiﬁcation analysis works choose replacement derivatives. beyond unsatisfying regard well understood lead gradient mismatch errors compound number layers increases show special case framework thus providing principled justiﬁcation basis exploring understanding alternatives. another common approach training hard-threshold units randomness either stochastic neurons hubara probabilistic training methods soudry williams methods softening hard-threshold units. contrast goal learn networks deterministic hard-threshold units. finally target propagation method explicitly associates target output activation network updates layer’s weights make activations similar targets. framework viewed instance target propagation uses combinatorial optimization discrete targets whereas previous approaches employed continuous optimization. madaline rule algorithm also seen special case framework target propagation target time. learning deep networks hard-threshold units given dataset t)}m vector-valued inputs binary targets interested learning -layered deep neural network hard-threshold units weight matrices rnd×nd−} element-wise activation function sign sign sign function sign otherwise. layer units deﬁne input layer denote output hidden layer layer unit similarly denote pre-activation output layer compactness incorporated bias term weight matrices. denote column matrix respectively entry column wdjk. using matrix notation write model matrix dataset instances matrix outputs. denote matrix ﬁnal-layer targets denote simplest case hard-threshold network hidden layers perceptron introduced rosenblatt goal learning perceptron hard-threshold network classify unseen data. useful ﬁrst step able correctly classify training data focus simplicity developing framework; however standard generalization techniques regularization easily incorporated framework. since perceptron linear classiﬁer able separate linearly-separable dataset. deﬁnition dataset t)}m linearly separable exists vector real number dataset linearly separable perceptron algorithm guaranteed separating hyperplane ﬁnite number steps number steps required dependent size margin however linear separability strong condition even simple functions linearly separable thus cannot learned perceptron would thus like able learn multilayer hard-threshold networks. consider simple single-hidden-layer hard-threshold network dataset hidden-layer activations. clearly collections perceptrons. backpropagation cannot used train input layer’s weights since hidden activation output perceptron knew value hidden unit take input could perceptron algorithm produce values. refer target given matrix hidden-layer targets +}n×m layer learned separately longer depend other goal perceptron learning update weights layer activations equal targets given inputs td−. figure shows example decomposition. denote targets -layer network hidden-layer targets dataset targets often notational convenience. auxiliary-variable-based approaches admm target propagation methods similar process decomposing layers network; however focus continuous variables impose constraints ensure activation equals auxiliary variable. take different approach here inspired combinatorial nature problem perceptron algorithm. since ﬁnal layer perceptron training instances separated hidden-layer activations linearly separable respect dataset targets thus hidden-layer targets must linearly separable respect dataset targets since hidden-layer targets intended values activations however order ensure hidden-layer activations equal targets training hidden-layer targets must able produced ﬁrst layer possible hidden-layer targets linearly separable respect inputs thus sufﬁcient condition separate data hidden-layer targets induce linear separability units layers network. refer property feasibility. deﬁnition setting targets -layer deep hard-threshold network feasible dataset unit layer dataset formed inputs targets linearly separable feasibility much weaker condition linear separability since output decision boundary multilayer hard-threshold network feasible targets general highly nonlinear. follows deﬁnition feasibility convergence perceptron algorithm feasible setting network’s targets dataset exists network separate training data. proposition dataset -layer hard-threshold network feasible targets layer trained separately inputs targets correctly classify instance learning deep hard-threshold network thus reduces ﬁnding feasible setting targets optimizing weights given targets i.e. mixed convex-combinatorial optimization. simplest method perform exhaustive search targets. exhaustive search iterates possible settings hidden-layer targets updating weights perceptron whose inputs targets changed returns weights feasible targets result lowest loss. impractical exhaustive search worth brieﬂy examining better understand solution space. particular decomposition afforded setting targets exhaustive search targets sufﬁcient learn globally optimal deep hard-threshold network even though weights learned gradient descent. proposition feasible setting deep hard-threshold network’s targets dataset exists exhaustive search returns global minimum loss time exponential number hidden units. learning improved feasibility relaxed instead perceptron algorithm robust method used perceptron learning. example perceptron learned non-linearlyseparable dataset minimizing hinge loss convex loss perceptron’s pre-activation output target maximizes margin combined regularization. general however method learning linear classiﬁers used. denote loss used train weights layer loss ﬁnal layer output loss. search spectrum hill climbing. iteration hill climbing evaluates neighboring states current state chooses lowest loss. search halts none states improve loss. state evaluated optimizing weights perceptron given state’s targets computing output loss. hill climbing practical exhaustive search since need explore exponential number states also provides local optima guarantee gradient descent soft-threshold networks. proposition hill climbing targets deep hard-threshold network returns local minimum loss iteration takes time linear size proposed targets. exhaustive search hill climbing comprise ends discrete optimization spectrum. beam search maintains beam promising solutions explores each another powerful approach contains hill climbing exhaustive search special cases. general however discrete optimization algorithm used setting targets. example methods satisﬁability solving integer linear programming constraint satisfaction might work well linear separability requirements feasibility viewed constraints search space. believe mixed convex-combinatorial optimization framework opens many avenues developing learning algorithms deep networks including non-differentiable modules. following section ideas develop learning algorithm hews much closer standard methods fact contains straight-through estimator special case. open question preceding section hidden-layer targets. generating good feasible targets entire network difﬁcult problem; instead easier approach propose targets layer time. backpropagation makes sense start output layer since ﬁnal-layer targets given successively targets upstream layer. further since hard know priori setting layer’s targets feasible given network architecture simple alternative targets layer optimize upstream weights check targets feasible. since goals optimizing layer’s weights setting upstream targets namely induce feasibility natural method setting target values choose targets reduce layer’s loss however targets discrete moves target space large non-smooth cannot guaranteed lower loss without actually performing move. thus heuristics necessary. discuss detail below. determining feasibility targets layer done recursively updating weights layer proposing targets layer given targets layer recursion continues input layer reached feasibility easily determined optimizing layer’s weights given targets dataset inputs. targets layer updated based information gained recursion upstream weights altered based outputs layer call recursive algorithm feasible target propagation ftprop. pseudocode shown algorithm name implies ftprop form target propagation uses discrete instead continuous optimization targets. ftprop also highly related rdis powerful nonconvex optimization algorithm based satisﬁability solvers recursively chooses sets subsets variables order decompose underlying problem simpler subproblems. rdis applied continuous problems ideas behind rdis generalized discrete variables sum-product theorem suggests interesting connection ftprop leave future work. algorithm train -layer hard-threshold network dataset feasible target propagation using loss functions {ld} initialize weights randomly initialize targets outputs hidden units ftprop) function ftprop course modern deep networks always feasible setting targets given dataset. example convolutional layer imposes large amount structure weight matrix making less likely layer’s input linearly separable respect targets. further ensuring feasibility general cause learning overﬁt training data worsen generalization performance. thus would like relax feasibility requirements. addition many beneﬁts using mini-batch instead full-batch training including improved generalization keskar reduced memory usage ability exploit data augmentation prevalence tools designed fortunately straightforward convert ftprop mini-batch algorithm relax feasibility requirements. particular since important overcommit mini-batch mini-batch version ftprop updates weights targets layer mini-batch; takes small gradient step layer’s weights instead optimizing fully; sets targets downstream layer parallel updating current layer’s weights since weights change much; removes checks feasibility. call algorithm ftprop-mb present pseudocode algorithm appendix ftprop-mb closely resembles backpropagation-based methods allowing easily implement standard libraries. activations layer differentiable backpropagation provides method telling layer adjust outputs improve loss. conversely hard-threshold networks target propagation provides method telling layer adjust outputs improve next layer’s loss long targets effectively. gradients cannot propagate hard-threshold units derivatives within layer still computed. effective efﬁcient heuristic setting target activation layer sign partial derivative next layer’s loss. speciﬁcally either pre-activation post-activation output depending choice loss. used update single target time heuristic often target value correctly results lowest loss. particular convex negative partial derivative respect deﬁnition points direction global minimum ld+. without loss generality follows convexity loss ﬂipping keeping variables would increase ld+. hand ﬂipping reduce loss since convexity cannot tell results smaller ld+. however discrepancy indicates lack conﬁdence current value hdj. natural choice thus push pre-activation value towards making likely ﬂip. setting accomplishes this. note that heuristic performs well still room improvement example extending better handle case combining information across batch. leave investigations future work. hinge loss shown figure robust version perceptron criterion thus natural per-layer loss function ﬁnding feasible setting targets weights. however preliminary experiments found learning tended stall become erratic time using hinge loss layer. attribute separate issues. first hinge loss sensitive noisy data outliers cause learning focus instances unlikely ever classiﬁed correctly instead instances near separator. second since convolutional layers large noisy datasets unlikely layer’s inputs entirely linearly separable thus important prioritize targets others. ideally highest priority targets would largest effect output loss. ﬁrst issue solved saturating hinge loss thus making less sensitive outliers saturated hinge loss shown figure hinge max) threshold make derivative symmetric. second problem solved variety ways including randomly subsampling targets weighting loss associated target according heuristic. simplest accurate method found weight loss target magnitude partial derivative next layer’s loss respect target’s hidden unit saturated hinge loss works well input ever moves range derivative become zero unit longer trainable. avoid this propose soft hinge loss shown figure soft hinge tanh like saturated hinge soft hinge slope threshold symmetric derivative; however also beneﬁts larger input region non-zero derivative. note bengio report using derivative sigmoid performed worse identity function. based experiments loss functions including variations squared hinge loss loss likely slope sigmoid less unity threshold causes vanishing gradients. loss functions asymmetric derivatives around threshold also seemed perform worse symmetric derivatives show different per-layer loss functions derivatives figure shows quantized relu activation step functions corresponding saturated-hinge-loss derivatives soft-hinge-loss approximation found work best hinge losses). experiments show soft hinge loss outperforms saturated hinge loss sign quantized relu activations discuss below. relationship straight-through estimator loss term hidden layer scaled magnitude partial derivative downstream layer’s loss target based sign partial derivative target propagation transmits information output loss every layer network despite hard-threshold units. interestingly combination loss function target heuristic exactly reproduce weight updates straight-through estimator speciﬁcally weight updates result using scaled saturated hinge loss target heuristic exactly saturated straight-through estimator deﬁned hubara replaces derivative sign |z|≤ indicator function. stes correspond different choices per-layer loss function. connection provides justiﬁcation existing approaches seen instances ftprop speciﬁc choice per-layer loss function target heuristic. believe enable more-principled investigations extensions methods future work. quantized activations straight-through estimation also commonly used backpropagate quantized variants standard activations relu. figure shows quantized relu evenly spaced thresholds. simplest popular straight-through estimator qrelu derivative saturated relu relu relu min). however instead consider qrelu activation viewpoint ftprop qrelu becomes step functions qrelu step otherwise linear transformation sign. resulting derivative saturated hinge losses shown figure clearly quite different described above. initial experiments performed well better ste; however achieved additional performance improvements using softened approximation shown yellow figure simply derivative soft hinge scaled shifted match qrelu domain. natural choice derivative small number soft hinge losses shape similar derivative single soft hinge loss. experiments evaluated ftprop-mb soft hinge per-layer losses training deep networks sign -bit qrelu activations comparing models trained ftp-sh trained saturated straight-through estimators described earlier also trained model full-precision relu saturated relu activations baselines. weight quantization main interest training hard-threshold activations recent work shown weights quantized little effect performance tested training methods cifar- imagenet datasets. cifar- trained simple -layer convolutional network -layer convolutional network zhou imagenet trained alexnet common model quantization table best top- test accuracy network epochs trained sign qrelu full-precision activations cifar- imagenet. hard-threshold activations trained ftprop-mb per-layer soft hinge losses saturated straight-through estimator bold numbers denote best performing activation pair. test accuracies -layer -layer convolutional network cifar- shown table simpler -layer model ftp-sh shows consistent accuracy gain sste entire training trajectory resulting improvement shown table however -bit qrelu activation sste ftp-sh perform nearly identically -layer model. conversely complex -layer model ftp-sh accuracy sste qrelu activation ftp-sh achieves consistent improvement sste. posit decrease performance sign activation moving -layer model methods able effectively train higher-capacity model achieve close best possible performance dataset whereas opposite true qrelu activation; i.e. restricted capacity -layer model limits ability methods train expressive qrelu effectively. true expect ftp-sh outperform sste sign qrelu activations harder dataset. unsurprisingly none low-precision methods perform well high-precision methods; however narrowness performance -bit qrelu ftp-sh full-precision relu encouraging. results imagenet experiments also shown table predicted cifar experiments ftp-sh improves test accuracy alexnet sign -bit qrelu activations challenging imagenet dataset. also shown figure plots top- train test accuracy curves different activation functions alexnet figure top- train test accuracies alexnet different activation functions imagenet. inset ﬁgures show test accuracy ﬁnal epochs detail. ﬁgures ftprop-mb soft hinge outperforms saturated straight-through estimator left ﬁgure shows network sign activations. right ﬁgure shows -bit quantized relu trained method performs nearly well full-precision relu. interestingly saturated relu outperforms standard relu. best viewed color. imagenet. left-hand plot shows training sign activations ftp-sh provides consistently better test accuracy sste throughout training trajectory despite hyperparameters optimized sste. improvement even larger -bit qrelu activation righthand plot ftp-sh qrelu even outperforms full-precision relu part trajectory outperforms sste-trained qrelu almost interestingly saturated relu outperforms standard relu almost full point accuracy. believe regularization effect caused saturating activation. also account surprisingly good performance ftp-sh qrelu relative full-precision relu hard-threshold activations also provide strong regularization effect. finally single experiment resnet- imagenet using hyperparameters previous works used sste check whether soft hinge loss exhibits vanishing gradient behavior diminishing slope away origin evaluate performance ftpsh less-quantized relu ftp-sh slightly worse sste sign function believe hyperparameters tuned sste vanishing gradients would expect much worse accuracy case. results qrelu activation provide evidence vanishing gradients ftp-sh qrelu outperforms sste almost top- accuracy work presented novel mixed convex-combinatorial optimization framework learning deep neural networks hard-threshold units. combinatorial optimization used discrete targets hard-threshold hidden units unit linearly-separable problem solve. network decomposes individual perceptrons learned standard convex approaches given targets. based this developed recursive algorithm learning deep hard-threshold networks call feasible target propagation efﬁcient mini-batch version showed commonly-used poorly-justiﬁed straight-through estimator special case ftprop-mb results using saturated hinge loss layer target heuristic. finally deﬁned soft hinge loss showed ftprop-mb soft hinge loss layer improves classiﬁcation accuracy multiple models cifar- imagenet compared ste. future work plan develop novel target heuristics layer loss functions investigating connections framework constraint satisfaction satisﬁability. also intend explore beneﬁts deep networks hard-threshold units. particular recent research clearly shows ability reduce computation energy requirements also less susceptible vanishing exploding gradients less susceptible covariate shift adversarial examples. yoshua bengio nicholas l´eonard aaron courville. estimating propagating gradients stochastic neurons conditional computation. arxiv preprint http//arxiv.org/abs/ abram friesen pedro domingos. recursive decomposition nonconvex optimization. qiang yang michael woolridge proceedings international joint conference artiﬁcial intelligence aaai press abram friesen pedro domingos. sum-product theorem foundation learning tractable models. proceedings international conference machine learning volume kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. francis bach david blei proceedings international conference machine learning volume lille france nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. proceedings international conference learning representations yann lecun. learning process asymmetric threshold network. bienenstock fogelman souli´e weisbuch disordered systems biological organization springer berlin heidelberg yann lecun l´eon bottou genevieve klaus-robert m¨uller. efﬁcient backprop. gr´egoire montavon genevi`eve klaus-robert m¨uller neural networks tricks trade second edition springer berlin heidelberg berlin heidelberg paulius micikevicius sharan narang jonah alben gregory diamos erich elsen david garcia boris ginsburg michael houston oleksii kuchaev ganesh venkatesh mixed precision training. arxiv preprint mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. proceedings european conference computer vision david rumelhart geoffrey hinton williams. learining internal representations error propagation. parallel distributed processing explorations microstructure cognition volume press olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei imagenet large scale visual recognition challenge. international journal computer vision daniel soudry itay hubara meir. expectation backpropagation parameter-free training multilayer neural networks continuous discrete weights. advances neural information processing systems gavin taylor ryan burmeister zheng bharat singh ankit patel goldstein. training neural networks without gradients scalable admm approach. proceedings international conference machine learning volume rodney winter bernard widrow. madaline rule training algorithm neural networks. proceedings ieee international conference neural networks diego ieee. shuchang zhou yuxin zekun xinyu zhou yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. arxiv preprint http//arxiv.org/abs/.. mini-batch feasible target propagation algorithm train -layer hard-threshold network dataset mini-batch feasible target propagation using loss functions {ld} initialize weights randomly minibatch cifar- training images test images divided classes trained simple -layer convolutional network deeper -layer convolutional network used methods compared top- accuracies test set. pre-processed images mean normalization augmented dataset random horizontal ﬂips random crops images padded pixels. hyperparameters chosen based small amount exploration validation set. ﬁrst network tested cifar- simple -layer convolutional network structured conv conv conv indicate convolutional layer fully-connected layer respectively channels. convolutional layers used kernels. max-pooling stride used convolutional layer non-linearity placed layers except ﬁrst. adam learning rate weight decay used minimize cross-entropy loss epochs. learning rate decayed factor epochs. order evaluate performance ftprop-mb soft hinge loss deeper network adapted -layer convnet zhou cifar-. network convolutional layers fully-connected layer output uses batch normalization non-linearity. optimized cross-entropy loss adam using learning rate weight decay sign activation qrelu baseline activations. trained epochs decaying learning rate epochs. figure top- test accuracies -layer convolutional network different activation functions cifar-. inset ﬁgures show test accuracy ﬁnal epochs detail. left ﬁgure shows network sign activations. right ﬁgure shows network -bit quantized relu activations full-precision baselines. best viewed color. figure top- test accuracies -layer convolutional network different activation functions cifar-. inset ﬁgures show test accuracy ﬁnal epochs detail. left ﬁgure shows network sign activations. right ﬁgure shows network -bit quantized relu activations full-precision baselines. best viewed color. imagenet much challenging dataset roughly training images validation images divided classes trained alexnet commonly used model quantization literature different activations compared top- top- accuracies trained models validation set. standard practice treat validation test data. images resized mean normalized randomly cropped randomly horizontally ﬂipped. models tested centered crops test images. hyperparameters based zhou used sste train alexnet imagenet. trained zhou variant alexnet imagenet sign -bit qrelu relu saturated relu activations. version alexnet removes dropout replaces local contrast normalization layers batch normalization. implementation split convolutions separate blocks. used adam optimizer learning rate cross-entropy loss epochs decaying learning rate epochs. sign activation used weight decay zhou relu saturated relu activations much likely overﬁt used weight decay used krizhevsky qrelu activation used weight decay since expressive sign less relu. alexnet trained resnet- imagenet sign qrelu relu saturated relu activations; however resnet- used qrelu steps used resnet code provided pytorch. optimized cross-entropy loss learning rate momentum epochs decaying learning rate factor epochs. sign activation used weight figure top- train test accuracies alexnet different activation functions imagenet. inset ﬁgures show test accuracy ﬁnal epochs detail. left ﬁgure shows network sign activations. right ﬁgure shows network -bit quantized relu activations full-precision baselines. best viewed color. figure top- train test accuracies resnet- different activation functions imagenet. inset ﬁgures show test accuracy ﬁnal epochs detail. left ﬁgure shows network sign activations. right ﬁgure shows network -bit quantized relu activations full-precision baselines. best viewed color.", "year": 2017}