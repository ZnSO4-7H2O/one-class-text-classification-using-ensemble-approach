{"title": "Explanation Methods in Deep Learning: Users, Values, Concerns and  Challenges", "tag": ["cs.AI", "cs.LG", "stat.ML", "68-02"], "abstract": "Issues regarding explainable AI involve four components: users, laws & regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.", "text": "issues regarding explainable involve four components users laws regulations explanations algorithms. together components provide context explanation methods evaluated regarding adequacy. goal chapter bridge expert users users. diﬀerent kinds users identiﬁed concerns revealed relevant statements general data protection regulation analyzed context deep neural networks taxonomy classiﬁcation existing explanation methods introduced ﬁnally various classes explanation methods analyzed verify user concerns justiﬁed. overall clear explanations given various aspects inﬂuence input output. however noted explanation methods interfaces users missing speculate criteria methods interfaces satisfy. finally noted important concerns diﬃcult address explanation methods concern bias datasets leads biased dnns well suspicion unfair outcomes. increasingly artiﬁcial intelligence used order derive actionable outcomes data overall goal chapter bridge expert users users highlighting explanation needs sides analyzing current state explainability. taking detailed look component mentioned figure finally address concerns context dnns. issues regarding explainable involve four components users laws regulations explanations algorithms. together components provide context explanation methods evaluated regarding adequacy. brieﬂy discuss components figure serious impact society large scale adoption digital automation techniques involve information processing prediction. deep neural networks belong automation technique used increasingly capability learn information. dnns humongous amount digital information easily collected users. currently much debate regarding safety trust data processes general leading investigations regarding explainability supported decision making. level concern topics reﬂected oﬃcial regulations general data protection regulation also mentioned incentives promote ﬁeld explainability institutional initiatives ensure safe development openai. technology becomes widespread dnns particular dependency said technology increases trust technology becomes necessity. current dnns achieving unparalleled performance areas computer vision natural language processing used real world applications bone assessment medical imagery implemented critical vision based applications tesla cars powering legal technology assist lawyers. challenge dnns particular lies providing insight processes leading outcomes thereby helping clarify circumstances trusted perform intended cannot. unlike methods machine learning decision trees bayesian networks explanation certain decision made cannot retrieved simply looking internal process. inner representation information complicated architectures deeper number learnable parameters increases. uncommon networks millions parameters. architectures complex often consisting various types components result interaction components oftentimes unknown. ﬁnally complex architectures lead complex information ﬂow. complications dnns often called black models opposed glassbox models fortunately problems escaped attention ml/deep learning community long artiﬁcial neural networks existed research done interpret explain decision process developing explanation methods objective explanation methods make speciﬁc aspects information representation information interpretable humans. various types users distinguished. users entertain certain values; include ethical values fairness neutrality lawfulness autonomy privacy safety functional values accuracy usability speed predictability. values certain concerns regarding dnns arise e.g. apprehensions discrimination accuracy. concerns translated questions system e.g. factor race inﬂuence outcome system reliable data used? section identify least general types users expert users users total speciﬁc kinds users. note could overlap users described below particular user classiﬁed belonging categories. engineers generally researchers involved extending ﬁeld detailed knowledge mathematical theories principles dnns. engineers interested explanations functional nature e.g. eﬀects figure issues regarding explainable dnns involve four components users algorithms laws explanations. together components provide context explanations evaluated regarding adequacy. developers generally application builders make software solutions used people. developers often make oﬀ-the-shelf dnns often re-training along tuning certain hyperparameters integrating various software components resulting functional application. developer concerned goals overall application assesses whether solution. developers interested explanation methods allow understand behavior various contexts integrated software application. users need knowledge implemented underlying mathematical principles knowledge integrated software components resulting ﬁnal functional application. least four users identiﬁed owner software application embedded. owner usually entity acquires application possible commercial practical personal use. example owner organization purchases application users clients owner also consumer purchases application personal use. latter case categorization owner fully overlaps next category users users. owner concerned explainability questions capabilities application e.g. justiﬁcation prediction prediction given input data aspects accountability e.g. extent application malfunction attributed component? user application intended used user uses application part profession personal use. user concerned explainability capabilities application e.g. justiﬁca data subject entity whose information processed application entity directly aﬀected application outcome. outcome output application context case. sometimes data subject entity user example case application meant personal use. data subject mostly concerned ethical moral aspects result actionable outcomes. actionable outcome outcome consequences outcome important decisions based. stakeholders people organizations without direct connection either development outcome application reasonably claim interest process instance runs counter particular values protect. governmental non-governmental organizations forward legitimate information requests regarding operations consequences dnns. stakeholders often interested ethical legal concerns raised phase process. engineer creates solution problem object segmentation object classiﬁcation experimenting various types networks. given video input solution gives output type object location object video. important initiative within european union general data protection regulation approved april became enforceable gdpr distinguishes personal data data subjects data processors data controllers personal data deﬁned information relating identiﬁed identiﬁable natural person data processor natural legal person public authority agency body processes data behalf data controller determines purposes conditions means processing. hence function tool used data processor whereas owners users role data controllers. gdpr focuses part proﬁling form automated processing personal data consisting personal data evaluate certain personal aspects relating natural person particular analyse predict aspects concerning natural person’s performance work economic situation health personal preferences interests reliability behaviour location movements according articles personal data collected data subject automated decisionmaking data subject right access data controller obliged provide meaningful information logic involved. article stipulates provision information data subjects concise transparent intelligible easily accessible form using clear plain language.. right meaningful information translates demand actionable outcomes dnns need explained i.e. made transparent interpretable comprehensible humans. transparency refers extent explanation makes particular outcome understandable particular users. understanding context amounts person grasping particular outcome reached dnn. note need imply agreeing conclusion i.e. accepting outcome valid justiﬁed. general transparency considered recommendable leading e.g. greater sense control acceptance applications. transparency normally also precondition accountability i.e. extent responsibility actionable outcome attributed legally relevant agents however transparency also negative consequences e.g. regarding privacy creating possibilities manipulation relation need explanation reasons investigation stand particular. first appear dysfunction i.e. fail operate intended e.g. bugs code second misfunction e.g. producing unintended undesired eﬀects deemed societally ethically unacceptable related dysfunction ﬁrst category explanations. category based information necessary order understand system’s basic processes e.g. assess whether functioning properly intended whether dysfunctions type explanation normally required developers expert users. information used interpret predict monitor diagnose improve debug repair functioning system application made available non-expert users normally certain guarantees regarding systems proper functioning place. generally speaking owners users data subjects stakeholders interested second category explanations suspicions dnns misfunctioning leads requests local explanations. users request information particular outcome reached aspects input data learning factors parameters system inﬂuenced decision prediction. information used assess appropriateness outcome relation concerns values users local explanations strengthen conﬁdence trust users system conﬂicting values i.e. violate fairness neutrality. note implies oﬀered explanations match particular users capacity understanding indicated gdpr. users gdpr role explanations discussed. bridge area technical area explanation methods need able evaluate capabilities existing methods context users needs. bridge ways. first identify high level desirable properties explanation methods. properties based desired characteristics explainers interpretable local ﬁdelity model-agnostic global perspective. break interpretable aspects clarity parsimony. local ﬁdelity captured overall term ﬁdelity. model-agnostic captured term generalizability. clear meant global perspective. second introduce taxonomy categorize types explanation methods third assess presence desirable properties categories taxonomy. high fidelity degree interpretation method resembles input-output mapping dnn. term appears fidelity arguably important property explanation model possess. explanation method faithful original model cannot give valid explanations input-output mapping incorrect. general local methods faithful global methods. high clarity degree resulting explanation unambiguous. property extremely important safety-critical applications ambiguity avoided. introduces quantiﬁable measure clarity method. high parsimony refers complexity resulting explanation. explanation parsimonious simple explanation. concept generally related occam’s razor case explaining dnns principle also importance. degree parsimony part dependent user’s capabilities. high generalizability range architectures explanation method applied. increases usefulness explanation method. methods model-agnostic highest generalizability. high explanatory power context means many phenomena method explain. roughly translates many types questions method answer. previously section identiﬁed number questions users have. relatively short period time plethora explanation methods strategies come existence driven need expert users analyze debug dnns. however apart non-exhaustive overview existing methods classiﬁcation schemes purely visual methods little known eﬀorts rigorously categorize whole explanation methods underlying patterns guide explanation methods. section attempt coherent taxonomy explanation methods proposed. three main classes explanation methods identiﬁed features described. taxonomy derived analyzing historical contemporary trends surrounding topic interpretation dnns explainable realize cannot foresee future developments dnns explainability methods. possible future taxonomy needs extended classes. propose following taxonomy measures importance component changing input internal components recording much changes aﬀect model performance. methods known names fall category occlusion perturbation erasure ablation inﬂuence. attribution methods often visualized sometimes referred visualization methods. rule-extraction methods extract human interpretable rules approximate decisionmaking process dnn. older genetic algorithm based rule extraction methods anns found specify three categories rule extraction methods. decompositional refers taking apart network. words means break network smaller individual parts. decompositional approach architecture network and/or outputs used process. uses decompositional algorithm extracts rules layer dnn. rules merged together ﬁnal merging step produce rules describe network behaviour means inputs. succeeded extracting rules lstm applying decompositional approach. introduced named pedagogical approach involves viewing rule extraction learning task target concept function computed network input features simply network’s input features. pedagogical approach advantage inherently model-agnostic. recent examples found according membership category assigned techniques utilize knowledge internal architecture and/or weight vectors trained artiﬁcial neural network complement symbolic learning algorithm. terms ﬁdelity local explanations faithful global explanations. ruleextraction means rules govern result speciﬁc input neighborhood inputs faithful rules govern possible inputs. rule extraction arguably interpretable category methods taxonomy considering resulting rules unambiguously interpreted human kind formal language. therefor high degree clarity. terms parsimony ruleset small enough parsimony higher ruleset large. determines small enough large diﬃcult quantify formally also dependent user; expert decompositional approach used likely method generalizable pedagogical approach used method highly generalizable. terms explanatory power rule-extraction methods validate whether network working expected terms overall logic explain aspects input data eﬀect lead speciﬁc output. attribution term introduced also referred relevance contribution class saliency inﬂuence aims reveal components high importance input eﬀect input propagated network. property categorize following methods attribution category occlusion erasure perturbation adversarial examples prediction diﬀerence analysis methods belong category worth mentioning attribution methods apply image input also forms input majority explanation methods dnns visualize information obtained attribution methods. visualization methods popularized recent years concerned important features visualized. identiﬁes current methods focus three aspects visualization feature visualization relationship visualization process visualization. overall visualization methods intuitive methods gain variety insight decision process many levels including architecture assessment model quality assessment even user feedback integration e.g. creates intuitive visualization interfaces image processing dnns. shown recently attribution methods lack reliability explanation sensitive factors contribute model prediction. furthermore introduce notion input invariance prerequisite accurate attribution. words attribution method satisfy input invariance consider ﬁdelity. terms clarity degree ambiguity inherent methods visual explanations interpreted multiple ways diﬀerent users even users user category. contrast precise results rule-extraction methods information results attribution methods less structure. addition degree clarity dependent degree ﬁdelity method ﬁdelity cause incorrect attribution resulting noisy output distracting attributions increase ambiguity. degree parsimony depends method visualization itself. methods visualize significant attributions exhibit higher degree parsimony. degree generalizability depends components used determine attribution. methods input output inherently model-agnostic resulting highest degree generalizability. following logic methods make internal components generalizable degree models share components. example deconvolutional networks applied models make convolutions extract features input images. terms explanatory power class methods reﬂect intuitively visual explanations factors input dimension signiﬁcant impact output dnn. however methods explain reason importance particular factor attribution. previous categories designed make explainable aspects process separate training dnn. contrast category aims improve interpretability internal representations methods part architecture e.g. part loss function modules additional capabilities part architecture structure terms operations layers provides interpretive loss function increase visual ﬁdelity learned features. importantly shows training dnns adversarial data consistent loss trace back errors made individual neurons identify whether data adversarial. gives ability answer relational reasoning questions speciﬁc environment introducing relational reasoning module learns relational function applied dnn. builds work introduces recurrent relational network take temporal component account. introduces explicit structure dnns visual recognition building and-or grammar directly network structure. leads better interpretation information network hence increased parsimony attribution methods. make generative neural networks perform causal inference generative neural networks learn functional causal models. intrinsic methods explicitly explain anything themselves instead increase ﬁdelity clarity parsimony attribution methods. class methods diﬀerent attribution methods tries make inherently interpretable changing architecture attribution methods already transform aspects representation something meaningful network trained. nature category methods desirable properties cannot attributed them. indicated figure users certain values relation particular technology lead concerns relation particular applications lead speciﬁc questions. distinguish various concerns users have. however types concerns discuss focus large extent inconclusiveness inscrutability misguidedness used evidence. concern signiﬁcant extent reliability accessibility used data although concerns important relation dnns well discuss concerns here. instead focus types concerns reasonably thought especially salient relation dnns. addition apprehensions data concerns involve aspects processing itself e.g. inferential validity algorithm. also questions raised validity training process following concerns question taken adapted context process. concerns questions speciﬁc particular user category. given broad application context process available explanation methods analyze whether concern justiﬁed whether accompanying questions could answered. note list concerns questions incomplete list serves example possible concerns questions exist. inconclusive process statistical inferences produce inconclusive knowledge identify signiﬁcant correlations rarely causal relations. users feel certainty regarding outcome required inconclusive process allows. concern justiﬁed considering often diﬃcult certainty relationship various variables causal least dnns. dnns learn correlations input output discovering meaningful features. note meaningful meaningful humans vice versa. explanation methods resolve concern reveal causal relations either. stated explanation methods make dnns comprehensible. however resolve stated concern. inscrutable data process data used aspects datas scope origin quality addition exact data unknown user. opaque. users worry data exactly outcome. ﬁrst concern justiﬁed since oftentimes data details data used unknown user. second concern resolved certain cases remains justiﬁed others. dnns process image data exist methods help visualize features learns section visualizations learned features reason came certain conclusions. sentiment analysis bodies text explanation methods also highlight part text certain predictions. cases result explanation method intuitive human interpretation. cases primarily regarding data easily visualized diﬃcult understand features learned. lost cause however since attribution methods adept pointing various ﬂows inﬂuence going input outcome. drawback result explanation methods often diﬃcult interpret especially users interpretation requires knowledge users likely possess. misguided process data collection process biased certain ways aﬀecting conclusions based users apprehensive data acquisition process. concern justiﬁed process collecting annotating data inherently imposes bias data another. since dnns trained datasets cannot escape bias. case mean bias particular outcomes supported inherent property process deﬁnition bias statistical sense. current case bias means data represent aspects applied. instead data small unrepresentative sample situations occur real life. lead trained able generalize properly. unfair outcomes users feel outcome somehow unfair relation particular values hold e.g. violating fairness privacy. concern justiﬁed dnns used responsibly sense users misuses dnn. level happen experts fail provide important information e.g. information kind data trained circumstances meant used. higher level happens users utilize inappropriate task. concern arises miscommunication lack knowledge requirement explanation kind. chapter tried analyze question what explained? given users needs laws regulations existing explanation methods. explicitly looked capabilities explanation methods analyzed questions/concerns explainability context methods address. overall clear explanations given various aspects inﬂuence input output making ruleextraction attribution methods. also used combination attribution methods intrinsic methods lead explainable dnns. likely future rise category explanation methods combination methods deﬁned combination rule-extraction attribution intrinsic methods answer speciﬁc questions simple human interpretable language. furthermore obvious current explanation methods tailored expert users since interpretation results require knowledge process. aware explanation methods e.g. intuitive explanation interfaces users exist. ideally explanation methods would exist able answer simple human language questions every operation application performs. easy task since number conceivable questions could working application incredibly large. particular concerns diﬃcult address explanation methods concern bias datasets leads biased dnns well suspicion unfair outcomes indicate biased remove bias? applied responsibly? problems directly solvable explanation methods. however explanation methods alleviate ﬁrst problem extent learned features visualized analyzed bias using methods explanation methods. second problem general measures regulations laws would need developed.", "year": 2018}