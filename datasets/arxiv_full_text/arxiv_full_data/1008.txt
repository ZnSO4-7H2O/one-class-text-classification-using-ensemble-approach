{"title": "Noisy Activation Functions", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.", "text": "functions fact easier optimize backpropagation smooth activation functions sigmoid tanh. recent successes piecewise linear functions particularly evident computer vision relu become default choice convolutional networks. propose technique train neural networks activation functions strongly saturate input large. mainly achieved injecting noise activation function saturated regime learning level noise. using approach found possible train neural networks much wider family activation functions previously. adding noise activation function considered relu units explored feed-forward networks boltzmann machines encourage units explore make optimization easier. recently resurgence interest elaborated gated architectures lstms grus also encompassing neural attention mechanisms used memory networks automatic image captioning video caption generation wide areas applications common thread running works soft-saturating non-linearities sigmoid softmax emulate hard decisions digital logic circuits. spite success problems approach. although gates often operate soft-saturated regime architecture prevents fully open closed. follow novel approach address problems. method addresses second problem hard-saturating nonlinearities allow gates common nonlinear activation functions used neural networks cause training difﬁculties saturation behavior activation function hide dependencies visible vanilla-sgd gating mechanisms softly saturating activation functions emulate discrete switching digital logic circuits good examples this. propose exploit injection appropriate noise gradients easily even noiseless application activation function would yield zero gradient. large noise dominate noise-free gradient allow stochastic gradient descent explore more. adding noise problematic parts activation function allow optimization procedure explore boundary degenerate well-behaved parts activation function. also establish connections simulated annealing amount noise annealed down making easier optimize hard objective functions. experimentally replacing saturating activation functions noisy variants helps training many contexts yielding state-of-the-art competitive results different datasets task especially training seems difﬁcult e.g. curriculum learning necessary obtain good results. introduction piecewise-linear activation functions relu maxout units profound effect deep learning major catalyst allowing training much deeper networks. thanks relu ﬁrst time shown deep purely supervised networks trained whereas using tanh nonlinearity allowed train shallow networks. plausible hypothesis recent surge interest piecewise-linear activation introducing hard-saturating nonlinearities exacerbated problem gradient since gradients saturated regime precisely zero instead negligible. however introducing noise activation function grow based magnitude saturation encourage random exploration. test time noise activation functions removed replaced expectation experiments show resulting deterministic networks outperform soft-saturating counterparts wide variety tasks allow reach state-of-the-art performance simple drop-in replacement nonlinearities existing training code. technique propose addresses difﬁculty optimization hard-activations test time gating units propose performing simulated annealing neural networks. hannun used relu activation functions simple rnns. paper successfully show that possible piecewise-linear activation functions gated recurrent networks lstm gru’s. saturating activation functions deﬁnition activation function function differentiable almost everywhere. deﬁnition activation function derivative said right saturate limit zero. activation function said saturate left right saturates. common activation functions used recurrent networks saturating. particular soft saturating meaning achieve saturation limit. deﬁnition constant implies left hard saturates implies hard saturates left right hard saturates. activation function saturates achieves zero gradient limit said soft saturate. construct hard saturating versions soft saturating activation functions taking ﬁrst-order taylor expansion zero clipping results appropriate range. motivation behind construction introduce linear behavior around zero allow gradients easily unit saturated providing crisp decision saturated regime. ability hard-sigmoid hard-tanh make crisp decisions comes cost exactly gradients saturated regime. cause difﬁculties training small inﬁnitesimal change pre-activation help reduce objective function reﬂected gradient. rest document refer generic activation function denote linearization based ﬁrst-order taylor expansion zero. hard-sigmoid saturates hard-tanh saturates denote threshold absolute values threshold hard-sigmoid hard-tanh. remark. note hard-sigmoid sigmoid tanh contractive mapping. hard-tanh becomes contractive mapping input greater threshold. important difference among activation functions ﬁxed points. hard-sigmoid ﬁxed point however ﬁxed-point sigmoid ﬁxed-point hard-tanh ﬁxed-point tanh tanh sigmoid point attractors ﬁxed-points. mathematical differences among saturating activation functions make behave differently rnns deep networks. highly non-smooth gradient descent trajectory bring parameters state pushes activations unit towards gradient regime particular example become difﬁcult escape unit stuck gradient regime. consider noisy activation function injected noise replace saturating nonlinearity hard-sigmoid hard-tanh introduced previous section. next section describe proposed noisy activation function used experiments want consider larger family noisy activation functions variant stochastic gradient descent training. variance mean want characterize happens gradually anneal noise going large noise levels noise furthermore assume noise level becomes large derivative respect figure example one-dimensional non-convex objective function simple gradient descent behave poorly. large noise escape saddle points local-minima result exploration. anneal noise level eventually converge local-minima noise limit recover deterministic nonlinearity experiments piecewise linear allows capture kind complex function want learn. illustrated figure large noise limit large gradients obtained backpropagating gives rise large derivatives. hence noise drowns signal example-wise gradient parameters much larger would therefore sees noise move around anywhere parameter space without seeing trend. annealing also related signal noise ratio deﬁned ratio variance noise model σsignal σnoise σsignal pure random exploration. anneal increase σnoise converges source exploration training come noise monte carlo estimates stochastic gradients. precisely need methods simulated annealing continuation methods helpful context optimization difﬁcult non-convex objectives. high noise free explore parts space. noise level decreased prefer regions signal strong enough visible given ﬁnite number steps noise averaged variance continues dominate. noise level reduced spends time globally better regions parameter space. approaches zero ﬁne-tuning solution converging near minimum noise-free objective function. related approach adding noise gradients annealing noise investigated well. showed annealed noise globally converge local-minima nonconvex objective functions polynomial number iterations. recently mobahi propose optimization method applies gaussian smoothing loss function annealing weight noise monte carlo estimator that. novel idea behind proposed noisy activation amount noise added nonlinearity proportional magnitude saturation nonlinearity. hard-sigmoid hard-tanh parametrization noise translates fact noise added hard-nonlinearity saturates. different previous proposals noisy rectiﬁer bengio noise added rectiﬁer unit independently whether input linear regime saturating regime nonlinearity. intuitively would like noise saturated regime since large change parameters would required desaturate conversely close saturation threshold small change parameters would sufﬁcient escape. make difference original activation function linearization choosing scale noise. eqs. deﬁnitions hard-sigmoid hard-tanh respectively. quantity zero unsaturated regime saturates grows proportionally distance saturation threshold also refer magnitude saturation. experimented different ways scaling empirically found following formulation performs better equation free scalar parameter learned course training. changing model able adjust magnitude noise also effects sign gradient well. hyper-parameter changes scale standard deviation noise. figure simple depiction adding gaussian noise linearized activation function brings average back hard-saturating nonlinearity bold. linearization noisy activation difference vector indicates discrepancy linearized function actual function noise added note that zero non-saturating parts function matches perfectly. non-zero almost surely. non-saturated regime optimization exploit linear structure near origin order tune output. saturated regime randomness drives exploration gradients still back since scale noise still depends reiterate gradient information every point spite saturation variance gradient information saturated regime depends variance unsatisfying aspect formulation unbiased noise that depending value occasionally gradient point wrong way. cause backwards message would push direction would worsen objective function average intuitively would prefer messages push back saturated unit towards non-saturated state gradient used safely. also absolute value reparametrization noise noise sampled half-normal distribution. ignored sign direction noise pushes activations determined point towards matching sign noise sign would ensure avoid sign cancellation noise gradient message backpropagation. required push activations towards bias introduced. practice hyperparameter inﬂuences mean added term near approximately satisﬁes condition seen fig. rewrite noisy term noise either added linearized function relationship visualized figure expressed experimented different types noise. empirically terms performance found half-normal normal noise better. provide formulation activation function noise sampled half-normal distribution noise sampled normal distribution. seen three paths gradients neural network linear path nonlinear path stochastic path gradients different pathways across different layers makes optimization activation function easier. figure stochastic behavior proposed noisy activation function different values noise sampled normal distribution approximating hard-tanh nonlinearity condition satisﬁed learned. experimentally found small values work better. ﬁxed small gets larger away threshold noise less likely able push activations back linear regime. also investigated effect injecting input noise activations saturate experiments used noise training test time replaced noise variable expected value. performed experiments drop-in replacement activation functions existing experimental setups without changing previously hyper-parameters. hence plausible could obtain better results performing careful hyper-parameter tuning models noisy activation functions. experiments initialized uniform randomly range provide experimental results using noisy activations normal half-normal noise normal noise input function normal noise input function learned normal noise sanity-check performed small-scale control experiments order observe behavior noisy units. showed learning curves different types activations various types noise contrast tanh hard-tanh units. models single-layer mlps trained mnist classiﬁcation show average negative log-likelihood general found models noisy activations converge faster using tanh hard-tanh activation functions lower tanh network. trained −layer dataset generated mixture gaussian distributions different means standard deviations. layer contains -hidden units. model tanh noisy− tanh activations able solve task almost perfectly. using learned values figure showed scatter plot activations unit layer derivative function unit layer respect input. figure derivatives unit layer respect input three-layered trained dataset generated three normal distributions different means standard deviations. words learned training unit layer. character-level language modeling. used language model sequences length used model train activation functions hyperparameters except grid-search nani nanis values. choose best based validation bit-per-character observed important difference among nani terms training performance seen figure problem predicting output short program introduced proved challenging modern deep learning architectures. authors curriculum learning model capture knowledge easier examples ﬁrst increase level difﬁculty examples training. figure show learning curves simple character-level language model sequences length ptb. nani similar learning curves. nanis beginning training better progress nanis training curve stops improving. table performance noisy network learning execute task. changing activation function proposed noisy yielded improvement accuracy. test accuracy default gradient clipping order avoid numerical stability problems. evaluating network length executed programs nesting default settings released code tasks. reference model model noisy activations trained combined curriculum sophisticated best performing one. results show applying proposed activation function leads better performance reference model. moreover shows method easy combine non-trivial learning curriculum. results presented table figure penntreebank experiments trained −layer word-level lstm language model penntreebank. used model proposed zaremba simply replaced sigmoid tanh units noisy hard-sigmoid hard-tanh units. reference model well-ﬁnetuned strong baseline noisy experiments used exactly setting decreased gradient clipping threshold provide results different figure training curves reference model noisy variant learning execute problem. noisy network converges faster reaches higher accuracy showing noisy activations help better optimize hard optimize tasks. models table terms validation test performance observe difference additive noise normal half-normal distributions substantial improvement noise makes result state-of-the-art task know. trained neural machine translation model europarl dataset neural attention model replaced sigmoid tanh units noisy counterparts. scaled weight matrices initialized orthogonal scaled multiplying evaluation done newstest test set. models trained table image caption generation flickrk. time added noisy activations code obtain substantial improvements higher-order bleu scores meteor metric well nll. soft attention hard attention refers using backprop versus reinforce training attention mechanism. ﬁxed nani nanil. table penntreebank word-level comparative perplexities. replaced code zaremba sigmoid tanh corresponding noisy variants observe substantial improvement perplexity makes state-of-the-art task. table neural machine translation europarl. using existing code nonlinearities replaced noisy versions much improved performance also simply using hard versions nonlinearities buys half gain. early-stopping. also compare model hard-tanh hard-sigmoid units model using noisy activations able outperform both shown table again substantial improvement respect reference english french machine translation. evaluated noisy activation functions network trained flickrk dataset. used soft neural attention model proposed reference model. scaled weight matrices initialized orthogonal scaled multiplying shown table able obtain better results model using dropout ratio output lstm layers context. tried without dropout table observed improvements addition dropout noisy activation function. main improvement seems coming introduction noisy activation functions since model without dropout already outperforms reference model. designed task where given random sequence integers objective predict number unique elements sequence. lstm network input sequence performed time average pooling hidden states lstm obtain ﬁxed-size vector. feed pooled lstm representation simple relu order predict unique number elements input sequence. experiments ﬁxed length input sequence input values order anneal noise started training scale hyperparameter standard deviation noise annealed schedule incremented every minibatch updates. noise annealing combined curriculum strategy best models obtained. second test used annealing procedure order train neural turing machine associative recall task trained model minimum items maximum items. show results noisy activations controller annealed noise compare regular terms validation error. seen figure network using noisy activation converges much faster nails task whereas original network failed approach error. table experimental results task ﬁnding unique number elements random integer sequence. illustrates effect annealing noise level turning training procedure continuation method. noise annealing yields better results curriculum. nonlinearities neural networks blessing curse. blessing allow represent complicated functions curse makes optimization difﬁcult. example found experiments using hard version sigmoid tanh nonlinearities often improved results. past various strategies proposed help deal difﬁcult optimization problem involved training deep networks including curriculum learning approximate form continuation method. earlier work also included softened versions nonlinearities gradually made harder training. motivated prior work introduce formalize concept noisy activations general framework injecting noise nonlinear functions large noise allows exploratory. propose inject noise activation functions either input function output unit would otherwise saturate allow gradients even case. show noisy activation functions easier optimize. also achieves better test errors since noise injected activations also regularizes model well. even ﬁxed noise level found proposed noisy activations outperform sigmoid tanh counterpart different tasks datasets yielding state-of-the-art competitive results simple modiﬁcation example penntreebank. addition found annealing noise obtain continuation method could improved performance. bengio yoshua l´eonard nicholas courville aaron. estimating propagating gradients stochastic arxiv preprint neurons conditional computation. arxiv. kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. hannun awni case carl casper jared catanzaro bryan diamos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho coates adam deep speech scaling end-to-end speech recognition. arxiv preprint arxiv. hermann karl moritz kocisky tomas grefenstette edward espeholt lasse will suleyman mustafa blunsom phil. teaching machines read advances neural information comprehend. processing systems developing powerful tool scientiﬁc computing. caglar gulcehre also thanks watson research statistical knowledge discovery group research supporting work internship. nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning neelakantan arvind vilnis luke quoc sutskever ilya kaiser lukasz kurach karol martens james. adding gradient noise improves learning deep networks. arxiv preprint arxiv. kelvin jimmy kiros ryan courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caparxiv preprint tion generation visual attention. arxiv. torabi atousa kyunghyun ballas nicolas christopher larochelle hugo courville aaron. describing videos exploiting temporal structure. computer vision ieee international conference ieee authors would like acknowledge support following agencies research funding computing support nserc calcul qu´ebec compute canada samsung canada research chairs cifar. would also like thank developers theano", "year": 2016}