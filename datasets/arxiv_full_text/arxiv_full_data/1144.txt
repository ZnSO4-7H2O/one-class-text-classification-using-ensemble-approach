{"title": "FeTa: A DCA Pruning Algorithm with Generalization Error Guarantees", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers, often with little or no drop in classification accuracy. However, most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. We start by proposing a cheap pruning algorithm for fully connected DNN layers based on difference of convex functions (DC) optimisation, that requires little or no retraining. We then provide a theoretical analysis for the growth in the Generalization Error (GE) of a DNN for the case of bounded perturbations to the hidden layers, of which weight pruning is a special case. Our pruning method is orders of magnitude faster than competing approaches, while our theoretical analysis sheds light to previously observed problems in DNN pruning. Experiments on commnon feedforward neural networks validate our results.", "text": "recent pruning algorithms succeeded reducing number parameters fully connected layers often little drop classiﬁcation accuracy. however existing pruning schemes either applied training require costly retraining procedure pruning regain classiﬁcation accuracy. start proposing cheap pruning algorithm fully connected layers based difference convex functions optimisation requires little retraining. provide theoretical analysis growth generalization error case bounded perturbations hidden layers weight pruning special case. pruning method orders magnitude faster competing approaches theoretical analysis sheds light previously observed problems pruning. experiments commnon feedforward neural networks validate results. recently deep neural networks achieved state-of-the results number machine learning tasks lecun training networks computationally intensive often requires dedicated expensive hardware. furthermore resulting networks often require considerable amount memory stored. using pascal titan popular alexnet vgg- models require hours days respectively train requiring respectively store. large memory requirements limit dnns embedded systems portable devices smartphones ubiquitous. dation classiﬁcation performance. approaches include introducing bayesian sparsity-inducing priors louizos blundell molchanov binarization courbariaux .other methods include hashing trick used chen tensorisation novikov efﬁcient matrix factorisations yang however trained models used researchers developers dedicated hardware train them often general feature extractors transfer learning. settings important introduce cheap compression method i.e. implemented postprocessing step little retraining. ﬁrst work direction although still require lengthy retraining procedure. closer approach recently aghasi authors propose convexiﬁed layerwise pruning algorithm termed net-trim. building upon net-trim authors dong propose lobs algorithm layerwise pruning loss function approximation. pruning neural network layer introduces perturbation latent signal representations generated layer. pertubated signal passes layers non-linear projections perturbation could become arbitrarily large. robustness hidden layer perturbations investigated random noise raghu case pruning aghasi dong authors conduct theoretical analysis using lipschitz properties dnns showing stability latent representations training pruning. methods employed connections recent work sokolic bartlett neyshabur used lipschitz properties analyze generalization error dnns useful performance measure. laboratory epfl lausanne switzerland idcom laboratory university edinburgh edinburgh correspondence pitas konstantinos <konstantinos.pitasepﬂ.ch>. work introduce cheap pruning algorithm dense layers dnns. also conduct theoretical analysis pruning affects generalization error trained classiﬁer. samples drawn denoted {si}m start net-trim formulation show cast difference convex functions problem. training signal assume also access inputs outputs fully connected layer rectiﬁer non-linearity max. optimisation problem want solve show sparsity-inducing objective proposed aghasi cast difference convex functions problem efﬁcient solution. fully connected layer input dimension output dimension training samples net-trim lobs scale like respectively. iterative algorithm scales like odd) precision solution related lipschitz strong convexity constants outer iteration number. emprirically algorithm orders magnitude faster competing approaches. also extend formulation allow retraining layer convex regulariser. build upon work sokolic bound case bounded perturbations hidden layer weights pruning special case. theoretical analysis provides principled pruning managing sharp contrast analysis aghasi dong analysis correctly predicts previously observed phenomenon accuracy degrades exponentially remaining depth pruned layer. sparsity parameter. term bj|| ensures nonlinear projection remains training signals. term convex regulariser imposes desired structure weight matrix objective equation non-convex. show optimisation objective cast difference convex functions problem. assume training sample simplicity latent representations following notation sequelmatrices column vectors scalars sets denoted boldface uppercase letters boldface lower-case letters italic letters calligraphic upper-case letters respectively. covering number d-metric balls radius denoted -regular kdimensional manifold constant captures intrinsic properties covering number assume samples drawn according probability distribution deﬁned training proximal operator non-smooth regulariser applied result. hyperparameters acc-proxsvrg acceleration parameter gradient step found experiments using gives best results. name algorithm feta fast efﬁcient trimming algorithm. optimized pruned layer training want stable test set. denote original representation pruned representation. assume training second assume ||a− ai|| third linear operators frames upper frame bounds respectively. theorem testing point distance original representation pruned representation bounded well known programs efﬁcient optimisation algorithms. propose algorithm iterative algorithm consists solving iteration convex optimisation problem obtained linearizing around current solution. although guaranteed reach local minima authors state often converges global minimum used succefully optimise fully connected layer fawzi iteration linearized optimisation problem given solution estimate iteration detailed procedure given algorithms assume regulariser convex possibly non-smooth case optimisation performed using proximal methods. order solve linearized problem propose accelerated proximal svrg presented nitanda detail method algorithm iteration minibatch drawn. gradient smooth part calculated algorithm takes step direction step size represents l−th layer parameters output l−th layer denoted i.e. input layer corresponds output last layer denoted need following deﬁnitions classiﬁcation margin score take sokolic useful later measuring generalization error. deﬁnition classiﬁer training sample score classiﬁcation margin training sample radius largest metric ball centered contained decision region associated classiﬁcation label note possible classiﬁer misclassify training point restate useful result sokolic corollary assume regular k-dimensional manifold assume also classiﬁer achieves take loss. probability least theorem assume -regular k-dimensional manifold assume also classiﬁer achieves lower bound classiﬁcation score take loss. furthermore assume prune classiﬁer layer using algorithm obtain classiﬁer probability least bound also depends spectral norm hidden layers i||. small spectral norms lead larger base respect pruning result quite pessimistic pruning error multiplied factor i||. thus analysis grows exponentially respect remaining layer depth pertubated layer. line previous work raghu demonstrates layers closer input much less robust compared layers close output. algorithm applied fully connected layers much closer output compared convolutional layers. extend bound include pruning multiple layers. theorem assume -regular k-dimensional manifold assume also classiﬁer achieves lower bound classiﬁcation score take loss. furthermore assume prune classiﬁer layers using algorithm obtain classiﬁer probability least detailed proof found appendix bound predicts pruning multiple layers much greater individual pruning. note also generality result; even though assumed speciﬁc form pruning bound holds type bounded perturbation hidden layer. first compare execution time feta lobs nettrim-admm. ||u|| sparsity. input dimensions output dimensions number training samples. assuming llipschitz smooth µ-strongly convex optimise optimal solution feta scales like odd). obtain multiplying number outer iterations number gradient evaluations required reach good solution inner algorithm ﬁnally multiplying gradient evaluation cost. conversely lobs scales like nettrim-admm scales like required cholesky factorisation. gives computational advantage algorithm settings input dimension large. validate constructing dataset samples generated i.i.d gaussian entries. plot figure results line theoretical predictions. figure time complexity plot calculation time feta nettrim lobs dataset. computation time line theoretical predictions. feta scales roughly nettrim lobs scale like size input dimensions increases feta becomes orders magnitude faster competing approaches. compare original full-precision network following compressed networks feta ||u|| net-trim lobs hard thresholding. refer respective papers net-trim lobs. hard thresholding deﬁned elementwise indicator function hadamard product positive constant. cifar-this contains color images object classes. images training remaining testing. training data augmented random cropping pixels random ﬂips left right contrast ﬁrst prune ﬁrst fully connected layer clarity. figure shows classiﬁcation accuracy compression ratio feta nettrim lobs hard thresholding. hard thresholding works adequately sparsity. level sparsity performance hard thresholding degrades rapidly feta higher accuracy average marginally worse lobs nettrim. task pruning ﬁrst fully connected layer also show detailed comparison results methods table lenet- model feta achieves accuracy net-trim faster. expected algorithms optimise similar objective feta exploits structure objective achieve lower complexity optimisation. furthermore feta achieves marginally lower classiﬁcation accuracy compared lobs faster signiﬁcantly better thresholding. cifarnet model table net-trim feasible machine used experiments requires ram. compared lobs feta achieves marginally lower accuracy faster. next prune fully connected layers architectures sparsity level show results table lower achieved sparsity methods mnist accuracy results pruning single layer feta achieving marginally worse results faster nettrim faster lobs. cifar experiment feta shows bigger degradation performance compared lobs remaining faster. thresholding achieves notably result accuracy makes method essentially inapplicable multilayer pruning. note degraded performance feta layer pruning cifar poor solution second dense layer. combining feta ﬁrst dense layer thresholding second dense layer achieve accuracy computational cost. furthermore mentioned dong wolfe retraining recover classiﬁcation accuracy lost pruning. starting good pruning doesn’t allow much degradation signiﬁcantly reduces retraining time. proof concept generality approach apply method imposing low-rank regularisation learned matrix rank compare methods feta ||u|| optimised acc-prox-svrg hard thresholding singular values using truncated deﬁned diag. plot results figure given rd×d commpression ratio deﬁned results line regularisation signiﬁcant degredation classiﬁcation accuracy hard thresholding figure accuracy sparsity plot classiﬁcation accuracy pruned lenet- architecture different sparsity levels. sparsity level roughly methods equal. sparsity levels greater feta clearly outperforms hard thresholding remaining competitive lobs. plot classiﬁcation accuracy pruned cifarnet architecture different sparsity levels. results consistent lenet- experiment. next test multilayer pruning bound. prune sparsity levels layers sets plot results figure evident accuracy loss layer groups simply addition accuracy losses individual layers shows exponential drop accordance theoretical result. figure accuracy plot classiﬁcation accuracy low-rank compressed lenet- architecture different levels. level roughly methods equal. levels greater feta clearly outperforms hard thresholding. plot classiﬁcation accuracy pruned cifarnet architecture different levels. results consistent lenet- experiment. according theoretical analysis drops exponentially remaining layer depth. corroborate train lenet- high accuracy pick single layer gradually increase sparsity using hard thresholding. layers closer input exponentially less robust pruning line theoretical analysis. plot results figure layers sudden increase accuracy around sparsity could small size dnn. point empirical results raghu much larger networks degradation entirely smooth. well bound captures exponential behaviour. take networks pruned layer unpruned network make number simplifying assumptions. first assume theorem empirical value single layer pruning multilayer pruning. theoretical predictions tight layers small remaining depth loose layers remaining depth. ﬁrst focus pruning sparsity. layer predicted exponentially less robust compared layers focus pruning layer layers sparsity. even though errors negligible error exponentially greater pruning interestingly empirical estimate exists artifact around sparsity partially captured prediction. parameter. next make assumption dimensionality values common mnist dataset result simple dimensionality analysis using pca. also deviate slightly sparsity level well average scores es∼s)]. plot theoretical predictions single layer pruning figure theoretical predictions multilayer pruning figure that loose theoretical predictions correctly capture qualitatively behaviour speciﬁcally layers predicted exponentially less robust remaining layer depth. also predicted pruning multiple layers resulting exponentially greater individual ges. paper presented efﬁcient pruning algorithm fully connected layers dnns based difference convex functions optimisation. algorithm orders magnitude faster competing approaches allowing controlled increase provided theoretical analysis increase resulting bounded perturbations hidden layer weights pruning special case. analysis correctly predicts previously observed phenomenon network layers closer input exponentially less robust pruning compared layers close output. experiments chen wenlin wilson james tyree stephen weinberger kilian chen yixin. compressing neural networks hashing trick. international conference machine learning courbariaux matthieu hubara itay soudry daniel elyaniv bengio yoshua. binarized neural networks training deep neural networks weights activations constrained or-. arxiv preprint arxiv. fawzi alhussein davies mike frossard pascal. dictionary learning fast classiﬁcation based softthresholding. international journal computer vision song huizi dally william deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. wolfe nikolas sharma aditya drude lukas bhiksha. incredible shrinking neural network perspectives learning representations lens pruning. arxiv preprint arxiv. yang zichao moczulski marcin denil misha freitas nando smola alex song wang ziyu. deep fried convnets. proceedings ieee international conference computer vision yong-deok park eunhyeok sungjoo choi taelim yang shin dongjun. compression deep convolutional neural networks fast power mobile applications. arxiv preprint arxiv. neyshabur behnam bhojanapalli srinadh mcallester david srebro nathan. pac-bayesian approach spectrally-normalized margin bounds neural networks. arxiv preprint arxiv. proof theorem denote original representation pruned representation. assume that training third linear operators frames upper frame bounds respectively. following lemmas useful lemma operator lipschitz continuous upper lipschitz constant proceed follows. ﬁrst introduce prior results hold general class robust classiﬁers. give speciﬁc prior generalization error results case classiﬁers operating datapoints cm-regular manifolds. afterwards provide prior results speciﬁc case clasiﬁers. finally prove novel generalization error bound provide link prior bounds. ﬁrst formalize robustness generic classiﬁers following assume loss function positive bounded deﬁnition algorithm robust partitioned disjoint sets denoted {tt}k direct substitution result deﬁniton cm-regular manifold theorem corollary assume regular k−dimensional manifold assume also classiﬁer achieves classiﬁcation margin take loss. probability least first assume score point original classiﬁer then second classiﬁer take point lies decision boundary assume simplicity that pruning classiﬁcation decisions change make", "year": 2018}