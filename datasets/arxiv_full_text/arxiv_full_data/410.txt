{"title": "Self-Supervised Vision-Based Detection of the Active Speaker as a  Prerequisite for Socially-Aware Language Acquisition", "tag": ["cs.CV", "cs.CL", "cs.HC", "cs.LG", "stat.ML", "I.2; I.4; I.5"], "abstract": "This paper presents a self-supervised method for detecting the active speaker in a multi-person spoken interaction scenario. We argue that this capability is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. Our methods are able to detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. Our methods do not rely on external annotations, thus complying with cognitive development. Instead, they use information from the auditory modality to support learning in the visual domain. The methods have been extensively evaluated on a large multi-person face-to-face interaction dataset. The results reach an accuracy of 80% on a multi-speaker setting. We believe this system represents an essential component of any artificial cognitive system or robotic platform engaging in social interaction.", "text": "social learning perspective main prerequisite language acquisition ability engage social interactions. artiﬁcial cognitive system address challenge must least aware people environment detect status speaking/not speaking infer possible objects active speaker focusing attention study address problem detecting active speaker multi-person language learning scenario. propose evaluate three self-supervised methods tackle task based temporal synchronization sensory modalities visual auditory. demonstrate methods system learns construct visual representations speaking/not speaking faces exploiting information temporally aligned auditory modality. result system detect keep track active speaker real-time using visual information. order impose little constraints possible social interaction requirements methods. ﬁrst particular method must operate realtime practice means method require future information. second requirement methods make assumptions possible artiﬁcial cognitive system interacts. therefore methods assume noise-free environment known number participants interaction known spatial conﬁguration. proposed methods address requirements engagement interactions outlined above detecting people environment detecting status speaking/not speaking. information turn prerequisite hypothesizing possible objects speaking person focusing his/her attention shown play important role infants’ language acquisition rest manuscript organized follows. first examine previous research forms context current study section describe proposed methods section iii. experiments conducted described section results experiments presented section discussion data used evaluation metric together assumptions made found section conclude paper section vii. abstract—this paper presents self-supervised method detecting active speaker multi-person spoken interaction scenario. argue capability fundamental prerequisite artiﬁcial cognitive system attempting acquire language social settings. methods able detect arbitrary number possibly overlapping active speakers based exclusively visual information face. methods rely external annotations thus complying cognitive development. instead information auditory modality support learning visual domain. methods extensively evaluated large multi-person face-toface interaction dataset. results reach accuracy multi-speaker setting. believe system represents essential component artiﬁcial cognitive system robotic platform engaging social interaction. humans provide artiﬁcial cognitive systems unique communication capability means referencing objects events relationships. turn artiﬁcial cognitive system capability able engage natural effective interactions humans. furthermore developing systems help understand underlying processes language acquisition initial stages human life. mentioned bosch modeling language acquisition complex integrate different aspects signal processing statistical learning visual processing pattern discovery memory access organization. according many studies alternatives human language acquisition individualistic learning social learning. case individualistic learning infant exploits statistical regularities multi-modal sensory inputs discover linguistic units phonemes words word-referent mappings. case social learning infant determine intentions others exploiting different social cues. therefore social manuscript submitted november research supported chist-era project iglu next generation. would like acknowledge nvidia corporation donating geforce titan cards used research swedish national infrastructure computing parallel data center computational time allocation. section divided parts. first introduce research language acquisition supports motivation build active speaker detector language learning artiﬁcial cognitive system. second part section turn focus research related problem identifying active speaker visual auditory perceptual inputs. literature language acquisition offers several theories infants learn ﬁrst words. main problems researchers face ﬁeld referential ambiguity discussed example clerkin pereira yurovsky referential ambiguity stems idea infants must acquire language linking heard words perceived visual scenes order form word-referent mappings. everyday life however visual scenes highly cluttered results many possible referents heard word within learning event bloom similarly many computational models language acquisition rooted ﬁnding statistical associations verbal descriptions visual scene interactive robotic manipulation experiments however nearly assume clutter-free visual scene objects observed isolation simpliﬁed background theories offer alternative mechanisms infants reduce uncertainty present learning environment. mechanism statistical aggregation word-referent co-occurrences across learning events. problem referential ambiguity within single learning event addressed smith suggesting infants keep track co-occurring words potential referents across learning events aggregated information statistically determine likely wordreferent mapping. however authors argued type statistical learning beyond abilities infants considering highly cluttered visual scenes. order study visual scene clutter infants’ perspective pereira yurovsky performed experiments infants equipped head-mounted eyetracker. conclusion learning events ambiguous dominant object considering infants’ point view. consequence researchers argued input language learning must understood infants’ perspective regularities make contact infants’ sensory system affect language learning. although related language acquisition attempt modeling saliency multi-modal stimuli learner perspective proposed ruesch bottom approach exclusively based statistical properties sensory inputs. caregivers’ intent mentioned studies. although word heard context many objects infants treat objects equally likely referents. instead infants social cues rule contenders named object. smith used eye-tracking record gaze data caregivers infants found caregiver visually attended object infants’ attention directed infants extended duration visual attention object thus increasing probability successful wordreferent mapping. infants learn interactions directly involved also observe attend interactions caregivers. handl meng performed studies examine body orientation inﬂuence infants’ gaze shifts. studies inspired large body research gaze following suggests infants’ others’ gaze guide attention infants attention conversations joint attention effect early learning. main conclusion static body orientation alone function infants’ observations guides attention. barton tomasello also reasoned multi-person context important language acquisition. triadic experiments joint attention important factor facilitating infants’ participation conversations infants likely take turn shared joint attentional focus speaker. ballard also proposed speakers’ movements head movements among others reveal referential intentions verbal utterances could play signiﬁcant role automatic language acquisition system. studies consider infants might know caregiver actively speaking therefore requires attention. believe important prerequisite modeling automatic language acquisition. focus study described manuscript therefore investigate different methods inferring active speaker. interested methods plausible developmental cognitive system perspective. main implications methods require manual annotations. fig. overview approaches active speaker detection considered study. perceptual inputs automatically modiﬁed passed memoryless transfer learning dynamic methods. identifying active speaker important many applications. area different constraints imposed methods. generally three different approaches audio-only audio-visual approaches forms inputs detection. audio-only active speaker detection process ﬁnding segments input audio signal associated different speakers. type detection known speaker diarization. speaker diarization studied extensively. miro offer comprehensive review recent research ﬁeld. audio-visual speaker detection combines information audio video signals. application audiovisual synchronization speaker detection broadcast videos explored nock unsupervised audio-visual detection speaker meetings proposed friedland zhang presented boosting-based multi-modal speaker detection algorithm applied distributed meetings give three examples. mutual correlations associate audio source regions video signal demonstrated fisher slaney covell showed audio-visual correlation used temporal synchronization audio signal speaking face. recent studies researchers employed deep architectures build active speaker detectors audio-visual input. multi-modal long short-term memory model learns shared weights modalities approaches speaker detection include general pattern recognition framework used besson kunt applied detection speaker audio-visual sequences. visual activity focus visual attention used features hung determine current speaker real meetings. stefanov used action units input features hidden markov models determine active speaker multi-party interactions vajaria demonstrated information body movements improve recognition performance. approaches cited section either evaluated small amounts data proved usable real-time settings. furthermore usually require manual annotations spatial conﬁguration interaction relative position input sensors known. goal usually ofﬂine video/audio analysis task semantic indexing retrieval broadcasts meetings video/audio summarization. believe challenge real-time detection active speaker dynamic cluttered environments remains. automatic language acquisition want infer possible objects active speaker focusing attention context assumptions known sensor arrangement participants’ position number environment unrealistic avoided. therefore study present methods several desirable characteristics types scenarios work real-time assume speciﬁc spatial conﬁguration number possible active participants free change interaction externally produced labels required rather acoustic inputs used reference visually based learning. goal methods described section detect real-time status visible faces multi-person language learning scenario. methods proposed study visual information illustration desired output method seen figure self-supervised learning approach construct active speaker detector machine learning methods supervised labels obtained automatically auditory modality learn models visual modality. overview approaches considered current study given figure ﬁrst ﬁgure illustrates perceptual inputs automatically extracted audio video streams. visual input consists cropped images face extracted viola jones’s face detection system labels extracted acoustic input correspond voice activity information ability extract pieces information given starting point system motivated section methods make feature extractor based convolutional neural networks following classiﬁer. test kinds classiﬁers memory less dynamic also consider ways training models transfer learning employ pre-trained feature extractor train classiﬁer speciﬁcally data task; task speciﬁc learning train feature extractor classiﬁer simultaneously data. model outputs posterior probability distribution possible outcomes since goal detection active speaker happens corresponding probability exceeds evaluation method performed computing accuracy predictions frame-by-frame basis illustration task speciﬁc learning method reported second figure train convolutional neural network feature extractor combination perceptron classiﬁer goal classifying input image either speaking speaking. training phase images labels used gradientbased optimization procedure adjust weights cnn/perceptron model. prediction phase images used trained detectors generate labels. cnn/perceptron model works frame-by-frame basis memory past frames. illustration method seen third figure similarly previous method cnn/perceptron model. time however model pre-trained object recognition task adapt model active speaker detection task remove object classiﬁcation layer truncated model feature extractor. train perceptron layer features speaker activity information. custom above method memory past frames. model originally trained supervised classify objects raises question suitable model context developmental language acquisition. support model comes literature visual perception demonstrates ability infants recognize objects early development. dynamic method illustrated forth figure method based feature extractors described above introduces model time evolution perceptual signals. training phase images custom pre-trained feature extractor constructs n-dimensional feature vector image. features labels used gradient-based optimization procedure adjust weights long short-term memory model prediction phase images converted features model features used trained detectors generate labels. methods presented section implemented evaluated using multi-modal multi-party interaction dataset described stefanov beskow main purpose dataset explore patterns focus visual attention humans following three different conditions humans involved task-based interaction robot humans involved task-based interaction robot replaced third human free three-party human interaction dataset contains parts sessions duration approximately minutes each sessions duration approximately minutes. dataset rich modalities recorded data streams. includes streams generated kinect devices high quality audio streams generated close-talking microphones high resolution video streams generated gopro cameras touch-events stream task-based interactions generated interactive surface system state stream generated robot involved ﬁrst condition. second part dataset also includes data streams generated tobii glasses trackers worn participants. interactions english data streams spatially temporally synchronized aligned. interactions occur around round interactive surface participants seated. figure illustrates spatial conﬁguration setup dataset. unique participants. report results based random sampling data subject data used training used testing used validation. evaluated methods available test data results also reported subset data different characteristics conditions. game participants often focusing interactive table thus showing faces properly cameras. comparing results games ones game wish verify orientation faces respect cameras strong impact performance methods. video input generated kinect device directed subject consideration audio input generated close-talking microphone. dynamic models trained evaluated frame long segments without overlaps. models comprise three convolutional layers width receptive ﬁelds rectiﬁer activations interleaved pooling layers window size output last pooling layer densely connected layer size rectiﬁer activation functions ﬁnally perceptron layer logistic sigmoid activations. lstm models include long short-term memory layer size hyperbolic tangent activations followed densely connected perceptron layer similarly model. training models used adam optimizer default parameters binary crossentropy loss function. memory less model trained epochs dynamic model trained epochs. model corresponding best validation performance selected ﬁnal results. methods implemented keras tensorflow backend. prediction phase color data generated automatic face detection module used input. described above considered methods outputs posterior probability distribution possible outcomes speaking/not speaking. therefore evaluating models’ performance threshold assigning class frame-level prediction. results reported terms frame-by-frame weighted accuracy calculated with consequence regardless actual class distribution test baseline chance performance using metric always although metric allows easily compare results different subjects methods conservative measure performance. example frames belong active class inactive class method classiﬁes frames active come back wacc point discuss results. multi-speaker dependent experiment build model group subjects test group. experiment tests scalability proposed methods subject. divided subjects groups call brevity. division based random assignment subject dataset sets resulting subjects subjects data partitioned train test validation sets subject following general approach outlined above. data distribution experiment summarized table speaker independent experiment used groups models multi-speaker dependent experiment. experiment however model built group subjects tested data group. experiment tests transferability proposed methods unseen subjects. higher experiments data drawn game compared experiments data drawn game indicates orientation faces respect cameras impact performance methods. highest mean result experiment game lstm model game highest lstm model. complete results provided appendix text table illustrate complete results figure ﬁgure weighted accuracy varies signiﬁcantly subjects. inter subject variability conﬁrmed high standard deviations. mention variability subjects higher difference obtain different methods subject. also note that although results improve considering game subjects game results worse game results. summarized mean results standard deviations obtained speaker dependent experiment provided table iii. results show regardless method used experiment mean weighted accuracy always goal multi-speaker dependent experiment test scalability proposed methods speaker. summarized results experiment table observed previous experiment mean performance methods better data drawn game compared game highest mean result speaker independent experiment tests transferability proposed methods unseen subjects. summarized results experiment table highest mean result experiment game lstm model game highest lstm model. play game touch surface. conditions subjects interact mainly touch surface discuss partner solve given task. therefore subjects’ overall gaze direction towards touch surface. raises challenging visual conditions extracting information face. show three examples figure observation motivated separate experiment using data third condition interaction yielded better overall results although amount data signiﬁcantly less compared full dataset discussed previously also note improvement results game condition shared subjects. would suggest random sampling train test validation sets subject also inﬂuence performance methods. therefore future work might include incorporating k-fold cross validation subject dependent evaluation. another source difﬁculty estimation speaker activity face fact subjects wearing recording hardware could interfered visual representation faces proposed methods. firstly mention proposed methods estimate probability speaking independently face. advantage able detect several speakers active time many applications might sufﬁcient select active speaker among detected faces. would allow combine single predictions joint probability thus increasing performance. order interpret results presented section need make number considerations evaluation method data. also consider advantages limitations metric used detail assumptions made methods main contributions study. section iv-a outlines main characteristics dataset used study. interaction dataset divided three conditions ﬁrst second condition related collaborative game scenario subjects discuss several characteristics metric used study. evaluating proposed methods frame-byframe basis gives detailed measure performance. however might argue frame-level accuracy necessary artiﬁcial cognitive systems employing proposed methods context automatic language acquisition. evaluating methods ﬁxed-length sliding time window might sufﬁcient application. furthermore deﬁnition weighted accuracy ampliﬁes short mistakes exempliﬁed section iv-b. consider case continuous talking speaker takes short pauses recollect memory structure argument. perfect module detect silence certain length acoustic signal label corresponding video frames speaking. however interaction point view speaker might still active resulting also visual activity. method misses short pauses would strongly penalized metric achieving accuracy frames classiﬁed correctly. advantage weighted accuracy metric however enables seamlessly compare performance subjects methods. different underlying class distributions particular data accounted metric resulting baseline considered experimental conﬁgurations. order motivate plausibility assumptions context computational model language acquisition consider research developmental psychology. according studies reported labarbera young-browne infants discriminate several facial expressions suggests capable detecting human faces. assumption system detect speech seems supported research recognition mother’s voice infants ﬁnal assumption reasonable considering infants interact small number speakers ﬁrst months many cases parent available caregiver speciﬁc time. methods proposed study similar previous work active speaker detection multi-party humanrobot interactions summarize main differences section. ﬁrst difference better performing pre-trained model visual feature extraction compared previously used alexnet also signiﬁcantly extended experiments performed evaluate compare proposed methods. current study evaluated effect using dynamical models comparing performance lstm models similar ones evaluated memory less perceptron classiﬁers. furthermore compared performance transfer learning methods described above models built speciﬁcally current application trained exclusively data. finally experiments multi-speaker speaker independent settings. ﬁndings that given optimize classiﬁer task necessary optimize feature extractor suggests pre-trained feature extractor works well independently speaker used extend results even beyond subjects present data set. also results multi-speaker dependent experiment proves proposed methods scale beyond single subject without signiﬁcant decrease performance. combining observation observation applicability transfer learning discussed suggests mixture proposed methods indeed useful component real life artiﬁcial cognitive system. finally results speaker independent experiment good results previous experiments. mention however that cognitive system’s perspective might unnecessarily challenging condition. fact expect infants familiar number caregivers thus justifying condition similar multi-speaker dependent test. study proposed evaluated three methods automatic detection active speaker based solely visual input. proposed methods could assist artiﬁcial cognitive system engage social interactions shown beneﬁcial language acquisition. tried reduce assumptions language learning environment minimum. therefore proposed methods allow different speakers speak simultaneously well silent; methods assume speciﬁc number speakers probability speaking estimated independently speakers thus allowing number speakers change social interaction. evaluated proposed methods large dataset including challenging visual conditions. example around half time subjects dataset interact touch surface look talking making feature extraction face detection difﬁcult. methods perform well speaker dependent multispeaker dependent fashion reaching accuracy weighted frame-based evaluation metric. combined results obtained transfer learning multi-speaker learning experiments promising suggest proposed methods generalize unseen perceptual inputs incorporating model adaptation step face. acknowledge general difﬁculty problem addressed study. humans generally produce many facial conﬁgurations not-speaking might highly overlapping conﬁgurations associated speaking. methods proposed study prerequisite socially-aware language acquisition seen mechanisms constraining visual input thus providing higher quality appropriate data statistical learning word-referent mappings. therefore main purpose methods help bringing artiﬁcial cognitive system step closer resolving referential ambiguity cluttered dynamic visual scenes. clerkin hart rehg smith real-world visual statistics infants’ ﬁrst-learned object names philosophical transactions royal society london biological sciences vol. salvi montesano bernardino santos-victor language bootstrapping learning word meanings perception-action association ieee transactions systems cybernetics part vol. ruesch lopes bernardino hornstein santos-victor pfeifer multimodal saliency-based bottom-up attention framework humanoid robot icub ieee international conference robotics automation anguera bozonnet evans fredouille friedland vinyals speaker diarization review recent research ieee transactions audio speech language processing vol. fisher darrell freeman viola learning joint statistical models audio-visual fusion segregation advances neural information processing systems press slaney covell facesync linear operator measuring synchronization video facial images audio tracks advances neural information processing systems press besson kunt hypothesis testing evaluating multimodal pattern recognition framework applied speaker detection journal neuroengineering rehabilitation vol. stefanov sugimoto beskow look who’s talking visual identiﬁcation active speaker multi-party human-robot interaction proceedings workshop advancements social signal processing multimodal interaction. vajaria sarkar kasturi exploring co-occurrence speech body movement audio-guided video localization ieee transactions circuits systems video technology vol. viola jones rapid object detection using boosted cascade simple features proceedings ieee computer society conference computer vision pattern recognition. cvpr vol. i––i–. skantze moubayed iristk statechart-based toolkit multi-party face-to-face interaction proceedings international conference multimodal interaction. stefanov beskow multi-party multi-modal dataset focus visual attention human-human human-robot interaction proceedings international conference language resources evaluation lrec abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp isard jozefowicz kaiser kudlur levenberg man´e monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warden wattenberg wicke zheng tensorflow large-scale machine learning heterogeneous systems available http//tensorﬂow.org/ stefanov beskow salvi vision-based active speaker detection multiparty interaction proceedings international workshop grounding language understanding krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems curran associates inc. kalin stefanov received degree artiﬁcial intelligence university amsterdam currently pursuing degree computer science royal institute technology supervision professor jonas beskow. research interests include machine learning computer vision speech technology. jonas beskow professor speech communication research interests multimodal speech technology modelling generating verbal non-verbal communicative behaviour well embodied conversational agents social robots speech gesture and/or modalities order accomplish human-like interaction. also cofounder furhat robotics startup developing innovative social robotics platform based research. giampiero salvi received degree electrical engineering universit`a sapienza degree computer science royal institute technology post-doctoral fellow institute systems robotics lisbon portugal. currently associate professor machine learning director masters programme machine learning royal institute technology. main interests machine learning speech technology cognitive systems.", "year": 2017}