{"title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for  Task-Oriented Dialogue Systems", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We present a new algorithm that significantly improves the efficiency of exploration for deep Q-learning agents in dialogue systems. Our agents explore via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Our algorithm learns much faster than common exploration strategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based ones. Additionally, we show that spiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.", "text": "present algorithm signiﬁcantly improves efﬁciency exploration deep q-learning agents dialogue systems. agents explore thompson sampling drawing monte carlo samples bayes-by-backprop neural network. algorithm learns much faster common exploration strategies \u0001-greedy boltzmann bootstrapping intrinsic-reward-based ones. additionally show spiking replay buffer experiences successful episodes make q-learning feasible might otherwise fail. increasingly interact computers natural-language dialogue interfaces. simple question answering bots already serve millions users amazon’s alexa apple’s siri google’s microsoft’s cortana. bots typically carry single-exchange conversations aspire develop general dialogue agents approaching breadth capabilities exhibited human interlocutors. work consider task-oriented bots agents charged conducting multi-turn dialogue achieve task-speciﬁc goal. case attempt assist user book movie tickets. complex dialogue systems often impossible specify good policy priori dynamics environment change time. thus learning policies online interactively reinforcement learning emerged popular approach inspired breakthroughs atari board games employ deep reinforcement learning learn policies dialogue systems. deep qnetwork agents typically explore \u0001-greedy heuristic rewards sparse action spaces large strategy tends fail. experiments randomly exploring q-learner never experiences success thousands episodes. offer efﬁcient solution improve exploration q-learners. propose bayesian exploration strategy encourages dialogue agent explore state-action ∗this work done microsoft. copyright association advancement artiﬁcial intelligence rights reserved. regions agent relatively uncertain action selection. algorithm bayes-by-backprop q-network explores thompson sampling drawing monte carlo samples bayesian neural network order produce temporal difference targets q-learning must generate predictions frozen target network show using maximum posteriori assignments generate targets results better performance also demonstrate effectiveness replay buffer spiking simple technique pre-ﬁll experience replay buffer small transitions harvested na¨ıve occasionally successful rule-based agent. technique proves essential bbqns standard dqns. evaluate dialogue agents variants movie-booking task. agent interacts user book movie. success determined dialogue movie booked satisﬁes user. benchmark algorithm baselines using agenda-based user simulator similar schatzmann thomson young make task plausibly challenging simulator introduces random mistakes account effects speech recognition language understanding errors. ﬁrst variant environment remains ﬁxed rounds training. second variant consider non-stationary domainextension environment. setting attributes ﬁlms become available time increasing diversity dialogue actions available user agent. experiments stationary domain-extension environments demonstrate bbqns outperform dqns using either \u0001-greedy exploration boltzmann exploration bootstrap approach introduced osband furthermore real user evaluation results consolidate effectiveness approach bbqns effective dqns exploration. besides also show agents work given replay buffer spiking although number pre-ﬁlled dialogues small. paper consider goal-oriented dialogue agents speciﬁcally aims help users book movie tickets. course several exchanges agent gathers information movie name theater number tickets ultimately completes booking. typical dialogue pipeline shown figure every turn conversation language understanding module converts text structured semantic representations known dialog-acts pass state-tracker maintain record information accumulated previous utterances. dialogue policy selects action transformed natural language form generation module. conversation continues dialogue terminates. numerical reward signal used measure utility conversation. details process given below. dialog-acts following schatzmann thomson young represent utterances dialog-acts consisting single collection pairs informed others requested example utterance like kind traitor tonight seattle maps structured semantic representation request. state tracker information inferred previous utterances state-tracker also interact database providing policy information many movies match current constraints. delexicalizes dialog-act allowing dialogue policy upon generic states. tracked state dialogue consisting representation conversation history several database features passed policy select actions. actions action de-lexicalized dialog-act. movie-booking task consider actions. include basic actions greeting thanks deny conﬁrm question conﬁrm answer closing. additionally actions slot inform value request pipeline ﬂows back user. slots informed policy ﬁlled state tracker. yields structured representation inform mapped natural language generation module textual utterance this movie playing tonight cinemark lincoln square. navigates markov decision process interacting environment sequence discrete steps step agent observes current state chooses action according policy agent receives reward observes state continuing cycle episode terminates. work assume actions denoted ﬁnite. dialogue scenario state-tracker produces states actions de-lexicalized dialog-acts described earlier state transitions governed dynamics conversation properly deﬁned reward function used measure degree success dialogue. experiment example success corresponds reward failure reward apply per-turn penalty encourage pithy exchanges. goal optimal policy maximize long-term reward. q-function measures every state-action pair maximum expected cumulative discounted reward achieved choosing following optimal policy thereafter maxπ discount factor. owing large state spaces practical reinforcement learners approximate q-function parameterized model example used paper neural network represents weights learned. good estimate found good approximation greedy policy maxa near-optimal policy popular learn neural-network-based q-function known appendix details. bayes-by-backprop captures uncertainty information neural networks maintaining probability distribution weights network. simplicity explain idea multilayer perceptrons l-layer model parameterized weights bl}l bl−) activation function sigmoid tanh rectiﬁed linear unit standard neural network training weights optimized minimize loss function squared error. bayes-by-backprop impose prior distribution weights learn full posterior distribution given training data yi}n practice however computing arbitrary posterior distribution intractable. instead approximate posterior variational distribution work choose gaussian diagonal covariance i.e. weight sampled ensure remain strictly positive parameterize softplus function log) giving variational parameters d-dimensional weight vector kl||p] sampled objective function estimated empirical version minimized sgvb using reparametrization trick popularized kingma welling appendix details. ready introduce bbqn algorithm learning dialogue policies deep learning models. bbqn builds upon deep q-network uses bayesian neural network approximate qfunction uncertainty approximation. since work ﬁxed-length representations dialogues extending methodology recurrent convolutional neural networks straightforward. action selection distinct feature bbqn explicitly quantiﬁes uncertainty q-function estimate used guide exploration. qfunction represented network parameter bbqn contrast maintains distribution described previous section multivariante gaussian diagonal covariance parameterized words weight posterior distribution log). given posterior distribution natural effective approach exploration posterior sampling thompson sampling actions sampled according posterior probability optimal current state. formally given state network parameter step action selected probability computing probabilities usually difﬁcult fortunately need sample action corresponding multinomial distribution. ﬁrst draw maxa veriﬁed process samples actions probabilities given equation also considered integrating \u0001-greedy approach exploring thompson sampling probability uniformly random probability empirically uniform random exploration confers supplementary beneﬁt task. bbqn bbqn initialized prior distribution consists isotropic gaussian whose variance single hyper-parameter introduced model. initialize variational parameters match prior. initialized zero vector variational standard deviation matches prior weight. note unlike conventional neural networks need assign weights randomly sampling breaks symmetry. consequence initialization outset agent explores uniformly random. course training experience buffer ﬁlls mean squared error starts dominate objective function variational distribution moves prior. given experiences form consisting transitions collected apply q-learning approach optimize network parameter similar maintain frozen periodically updated copy bbqn whose parameter denoted transition network used compute target value resulting regression data apply bayesby-backprop method described previous section optimize converges replaced ways generate target value ﬁrst uses monte carlo sample frozen network compute target maxa ˜w). speed training minibatch draw sample target generation sample sample-based variational inference implementation training speeds bbqn roughly equivalent. second uses maximum posterior estimate compute maxa ˜µ). computationally efﬁcient choice motivated observation that since require uncertainty estimates exploration necessary sample frozen network synthesizing targets. furthermore early training predictive distribution networks high variance resulting large amount noise target values slow training. bbqn intrinsic reward variational information maximizing exploration introduces exploration strategy based maximizing information gain agent’s belief environment dynamics. adds intrinsic reward bonus reward function quantiﬁes agent’s surprise ηdkl||p] demonstrated strong empirical performance. explore version bbqns incorporates intrinsic reward vime terming approach bbqn-vime-mc/map. bbqn-vime variations encourage agents explore state-action regions relatively unexplored bbqn relatively uncertain action selection. full-domain experiment bbqn bbqn-vime variations achieve similar performance signiﬁcant difference domainextension experiments observe bbqn-vime-mc slightly outperforms bbqn-map. replay buffer spiking reinforcement learning multiple sources uncertainty. include uncertainty parameters model uncertainty unseen parts environment. bbqn addresses parameter uncertainty struggle given extreme reward sparsity. researchers various techniques accelerate learning settings. approach leverage prior knowledge reward shaping imitation learning. approach falls category. fortunately setting it’s easy produce successful dialogues manually. even though manual dialogues follow optimal policy contain successful movie bookings indicate existence large reward signal. pre-ﬁlling replay buffer experiences dramatically improves performance experiments construct simple rule-based agent that sub-optimal achieves success sometimes. experiment harvest dialogues experiences rule-based agent adding replay buffer. that task essential bbqn approaches. interestingly performance strictly improve number pre-ﬁlled dialogues note replay buffer spiking different imitation learning. works well even small number warmstart dialogues suggesting helpful communicate even existence reward. even example successful dialogue replay buffer could successfully jump-start q-learner. evaluate methods variants moviebooking task. experiments adapt publicly available simulator described ﬁrst agent interacts user simulator rounds. round consists simulated dialogues followed epochs training. slots available starting ﬁrst episode. second test model’s ability adapt domain extension periodically introducing slots. time slot augment state space action space. start essential slots train training rounds front. then every rounds introduce slot ﬁxed order. added slot state space action space grow accordingly. experiment terminates rounds. experiments quantifying uncertainty network weights important guide effective exploration. represent state dialogue turn construct dimensional feature vector consisting following one-hot representations slot corresponding current user action separate components requested informed slots; corresponding representations slot corresponding last agent action; slots corresponding previously ﬁlled slots course dialog history; scalar one-hot representation current turn count; counts representing number results knowledge base match presently ﬁlled-in constraint well intersection ﬁlled-in constraints. domain-extension experiments features corresponding unseen slots take value seen. domain extended features corresponding weights input layer initializing weights trick lipton vikram mcauley training details training ﬁrst naive occasionally successful rule-based agent rbs. experiments dialogues spike replay buffer. note experiments showed models insensitive precise number. round simulated dialogues agent freezes target network parameters updates qfunction training epochs re-freezes trains another epochs. reasons proceeding -dialog spurts rather updating minibatch turn. first deployed system real-time updates might realistic. second train batches turn customary literatures owing economic considerations computational costs negligible failed dialogues either consume human labor confer opportunity costs baseline methods demonstrate efﬁcacy bbqn compare \u0001-greedy standard dqn. additionally compare boltzmann exploration approach probability selecting action given state determined softmax function applied predicted q-values. here afﬁnity exploration parameterized boltzmann temperature. also compare bootstrapping method osband bootstrap experiments bootstrap heads assign data point head probability evaluate four methods full domain learning problem domain extension problem. also tried comparing gaussian processes based approaches. however setting highdimensional inputs large number time steps unable good results. experiments computation memory requirement grow quadratically time memory starts explode round. limiting data size helpful. furthermore contrast gaˇsi´c state -dimensional experiments -dimensional states making scalability even bigger challenge. recent paper compares deep gp-sarsa simpler dialogue policy learning problem. order make gaussian processes computationally tractable rely sparsiﬁcation methods gaining computation efﬁciency expense accuracy. despite undertaking make feasible competitive found deep approaches outperform gp-sarsa respect ﬁnal performance regret computational expense consider gaussian processes evolving area worthwhile gaussian processes sparsiﬁcation methods compare deep approaches future work. architecture details models mlps relu activations. network hidden layers hidden nodes each. optimize parameters using adam batch size initial learning rate determined grid search. avoid biasing experiments towards methods determine common hyper-parameters using standard dqn. bbqn confers regularization equip models dropout regularization shown blundell confer comparable predictive performance holdout data. model additional hyper-parameters. example \u0001-greedy exploration requires initial value attenuation schedule. boltzmann exploration requires temperature. bootstrapping-based method osband requires number bootstrap heads probability data point assigned head. bbqn requires determine variance gaussian prior distribution variance gaussian error distribution. simulation results shown figure bbqn variants perform better baselines. particular bbqn-map performs best full domain setting bbqn-vimemc achieves best performance domain extension setting respect cumulative successes training ﬁnal performance trained models note domain extension problem becomes difﬁcult every epochs sustained performance corresponds getting better declining performance imply policy becomes worse. problems method achieves single success absent rbs. evaluating best algorithm using dialogues using dialogues agents learn quickly longterm performance worse. heuristic future discard pre-ﬁlled experiences meeting performance threshold. also considered perhaps promising trajectories might never sampled bbqn. thus constructed experiment exploring hybridization bbqn’s thompson sampling \u0001-greedy approach. probability agent selects action thompson sampling given monte carlo sample bbqn probability agent selects action uniformly random. however uniformly random exploration confers additional beneﬁt. human evaluation evaluate agents trained using simulated users real users recruited authors’ afﬁliation. conducted study using bbqn-map agents. full-domain setting agents trained slots. domain-extension setting ﬁrst picked bbqn agents domain extension training epoch performance agents tied nearly success rate. training epoch started introduce slots selected another agents training epoch total compare three agent pairs {dqn bbqn} full domain {b-dqn b-bbqn} domain extension {a-dqn abbqn} domain extension. real user study dialogue session select agents randomly converse user. present user user goal sampled corpus. dialogue session user asked give rating scale based naturalness coherence task-completion capability agent total collected dialogue sessions. figure presents performance agents real users terms success rate. figure shows comparison user ratings. full-domain setting bbqn agent signiﬁcantly better agent terms success rate user rating. domain-extension setting domain extension performance agents tied; domain extension bbqn agent signiﬁcantly outperforms terms success rate user rating. paper touches several areas research namely bayesian neural networks reinforcement learning deep qnetworks thompson sampling dialogue systems. work employs q-learning popular method model-free broad resource point sutton barto recently mnih achieved super-human performance atari games using deep q-learning incorporating techniques experience replay efﬁcient exploration remains deﬁning challenges provably efﬁcient exploration strategies known problems ﬁnite states/actions problems nice structures less known general case especially general nonlinear function approximation used. ﬁrst papers relied upon \u0001-greedy exploration heuristic recently stadie levine abbeel houthooft introduced approaches encourage exploration perturbing reward function. osband textual bandits consider challenging case mdps. paper also builds prior work task-oriented dialogue systems learning dialogue policies domain-extension experiments take inspiration gaˇsic user simulator modeled schatzmann thomson young learning dialogue policies bbqns explore greater efﬁciency traditional approaches. results similarly strong static domain extension experiments simulation real human evaluation. additionally showed beneﬁt combining bbqlearning other orthogonal approaches exploration work perturbing reward function bonus uncovering surprising transitions i.e. state transitions given probability dynamics model previously rarely seen states bbqn addresses uncertainty q-value given current policy whereas curiosity addresses uncertainty dynamics under-explored parts environment. thus synergistic effect combining approaches. domain extension task bbqn-vime proved especially promising outperforming methods. several promising paths future work. notably given substantial improvements bbqns exploration strategies would like extend work popular deep reinforcement learning benchmark tasks domains like robotics cost exploration high confers comparably dramatic improvement.", "year": 2016}