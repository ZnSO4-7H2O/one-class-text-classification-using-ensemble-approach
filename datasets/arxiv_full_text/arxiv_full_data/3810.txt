{"title": "End-To-End Memory Networks", "tag": ["cs.NE", "cs.CL"], "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.", "text": "introduce neural network recurrent attention model possibly large external memory. architecture form memory network unlike model work trained end-to-end hence requires signiﬁcantly less supervision training making generally applicable realistic settings. also seen extension rnnsearch case multiple computational steps performed output symbol. ﬂexibility model allows apply tasks diverse question answering language modeling. former approach competitive memory networks less supervision. latter penn treebank text datasets approach demonstrates comparable performance rnns lstms. cases show concept multiple computational hops yields improved results. grand challenges artiﬁcial intelligence research build models make multiple computational steps service answering question completing task models describe long term dependencies sequential data. recently resurgence models computation using explicit storage notion attention manipulating storage offers approach challenges. storage endowed continuous representation; reads writes storage well processing steps modeled actions neural networks. work present novel recurrent neural network architecture recurrence reads possibly large external memory multiple times outputting symbol. model considered continuous form memory network implemented model work easy train backpropagation required supervision layer network. continuity model present means trained end-to-end input-output pairs applicable tasks i.e. tasks supervision available language modeling realistically supervised question answering tasks. model also seen version rnnsearch multiple computational steps output symbol. show experimentally multiple hops long-term memory crucial good performance model tasks training memory representation integrated scalable manner end-to-end neural network model. model takes discrete inputs stored memory query outputs answer contains symbols coming dictionary words. model writes memory ﬁxed buffer size ﬁnds continuous representation continuous representation processed multiple hops output allows backpropagation error signal multiple memory accesses back input training. single layer start describing model single layer case implements single memory operation. show stacked give multiple hops memory. input memory representation suppose given input stored memory. entire {xi} converted memory vectors {mi} dimension computed embedding continuous space simplest case using embedding matrix query also embedded obtain internal state embedding space compute match memory taking inner product followed softmax function input output smooth easily compute gradients backpropagate recently proposed forms memory attention take approach notably bahdanau graves also generating ﬁnal prediction single layer case output vector input embedding passed ﬁnal weight matrix softmax produce predicted label overall model shown fig. training three embedding matrices well jointly learned minimizing standard cross-entropy loss true label training performed using stochastic gradient descent figure single layer version model. three layer version model. practice constrain several embedding matrices multiple layers extend model handle operations. memory layers stacked following input layers ﬁrst output input layer adjacent output embedding layer input embedding above i.e. also constrain answer prediction matrix ﬁnal output embedding question embedding match input embedding ﬁrst layer i.e. layer-wise input output embeddings across different layers i.e. found useful linear mapping update hops; mapping learnt along rest parameters used throughout experiments layer-wise weight tying. three-layer version memory model shown fig. overall similar memory network model except hard operations within layer replaced continuous weighting softmax. note layer-wise weight tying scheme model cast traditional divide outputs internal external outputs. emitting internal output corresponds considering memory emitting external output corresponds predicting label. point view fig. eqn. hidden state model generates internal output using model ingests using updates hidden state here unlike standard explicitly condition outputs stored memory hops keep outputs soft rather sampling them. thus model makes several computational steps producing output meant seen outside world. number recent efforts explored ways capture long-term structure within sequences using rnns lstm-based models memory models state network latent inherently unstable long timescales. lstm-based models address local memory cells lock network state past. practice performance gains carefully trained rnns modest model differs uses global memory shared read write functions. however layer-wise weight tying model viewed form produces output ﬁxed number time steps intermediary steps involving memory input/output operations update internal state. early work neural networks steinbuch piske taylor considered memory performed nearest-neighbor operations stored input vectors parametric models retrieved sets. similarities single layer version model. subsequent work explored types memory example mozer introduced explicit stack push operations revisited recently context model. closely related model neural turing machine graves also uses continuous memory representation. memory uses content address-based access unlike explicitly allows former although temporal features introduce section allow kind address-based access. however part always write memory sequentially model somewhat simpler requiring operations like sharpening. furthermore apply memory model textual reasoning tasks qualitatively differ abstract operations sorting recall tackled ntm. note view terminology input output fig. ﬂipped viewed traditional special conditioning outputs becomes part output embedding becomes input embedding. model also related bahdanau work bidirectional based encoder gated based decoder used machine translation. decoder uses attention model ﬁnds hidden states encoding useful outputting next translated word; attention model uses small neural network takes input concatenation current hidden state decoder encoders hidden states. similar attention model also used generating image captions. memory analogous attention mechanism although single sentence rather many case. furthermore model makes several hops memory making output; important good performance. also differences architecture small network used score memories compared scoring approach; simple linear layer whereas sophisticated gated architecture. apply model language modeling extensively studied task. goodman showed simple effective approaches combine n-grams cache. bengio ignited interest using neural network based models task rnns lstms showing clear performance gains traditional methods. indeed current state-of-the-art held variants models example large lstms dropout rnns diagonal constraints weight matrix appropriate weight tying model regarded modiﬁed form recurrence indexed memory lookups word sequence rather indexed sequence itself. perform experiments synthetic tasks deﬁned given task consists statements followed question whose answer typically single word answer available model training time must predicted test time. total different types tasks probe different forms reasoning deduction. samples three tasks note question subset statements contain information needed answer others essentially irrelevant distractors memory networks weston supporting subset explicitly indicated model training difference work information longer provided. hence model must deduce training test time sentences relevant not. formally tasks given example problems sentences {xi} question sentence answer word sentence represented one-hot vector length representation used question answer versions data used training problems task second larger task. model details unless otherwise stated experiments used hops model adjacent weight sharing scheme. tasks output lists take possible combination possible outputs record separate answer vocabulary word. sentence representation sentences. ﬁrst experiments explore different representations takes sentence axij cxij. input vector representing question also embedded words bqj. drawback cannot capture order words sentence column vector structure number words sentence dimension embedding. sentence representation call position encoding means order words affects representation used questions memory inputs memory outputs. temporal encoding many tasks require notion temporal context i.e. ﬁrst example section model needs understand bedroom kitchen. enable model address them modify memory vector axij special matrix encodes temporal information. output embedding augmented matrix cxij tc). learned training. also subject sharing constraints note sentences indexed reverse order reﬂecting relative distance question last sentence story. learning time invariance injecting random noise found helpful dummy memories regularize training time randomly empty memories stories. refer approach random noise training details babi training held-out form validation used select optimal model architecture hyperparameters. models trained using learning rate anneals every epochs epochs reached. momentum weight decay used. weights initialized randomly gaussian distribution zero mean trained tasks simultaneously training samples epochs used learning rate anneals every epochs training uses batch size gradients norm larger divided scalar norm experiments explored commencing training softmax memory layer removed making model entirely linear except ﬁnal softmax answer prediction. validation loss stopped decreasing softmax layers re-inserted training recommenced. refer linear start training. training initial learning rate capacity memory restricted recent sentences. since number sentences number words sentence varied problems null symbol used ﬁxed size. embedding null symbol constrained zero. tasks observed large variance performance model remedy this repeated training times different random initializations picked lowest training error. baselines compare approach range alternate models memnn strongly supervised am+ng+nl memory networks approach proposed best reported approach paper. uses operation layer trained directly supporting facts employs n-gram modeling nonlinear layers adaptive number hops query. memnn-wsh weakly supervised heuristic version memnn supporting sentence labels used training. since unable backpropagate operations layer enforce ﬁrst memory share least word question second memory share least word ﬁrst least word answer. memories conform called valid memories goal training rank higher invalid memories using ranking criteria strongly supervised training. results report variety design choices position encoding sentence representation; training tasks independently jointly training phase training linear start softmaxes removed initially training softmaxes start; varying memory hops results across tasks given table training along mean performance training set. show number interesting points best memnn models reasonably close supervised models representation improves bag-of-words demonstrated clear improvements tasks word ordering particularly important. linear start training seems help avoid local minima. task table alone gets error using reduces jittering time index random empty memories described section gives small consistent boost performance especially smaller training set. joint training tasks helps. importantly computational hops give improved performance. give examples hops performed illustrative examples fig. appendix task supporting fact supporting facts supporting facts argument relations argument relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coreference time reasoning basic deduction basic induction positional reasoning size reasoning path ﬁnding agent’s motivation mean error failed tasks training data mean error failed tasks table test error rates tasks models using training examples bag-of-words representation; position encoding representation; linear start training; random injection time index noise; rnn-style layer-wise weight tying joint joint training tasks figure example predictions tasks show labeled supporting facts dataset memnn training probabilities used model inference. memnn successfully learns focus correct supporting sentences. figure average activation weight memory positions memory hops. white color indicates model attending hop. clarity normalized maximum value model trained penn treebank text dataset. operate word level opposed sentence level. thus previous words sequence embedded memory separately. memory cell holds single word need linear mapping representations used tasks. employ temporal embedding approach section since longer question fig. ﬁxed constant vector output softmax predicts word vocabulary next sequence. cross-entropy loss used train model backpropagating error multiple memory layers manner tasks. training apply relu operations half units layer. layer-wise weight sharing i.e. query weights layer same; output weights layer same. noted section makes architecture closely related traditionally used language modeling tasks; however sequence network recurrent text memory hops. furthermore weight tying restricts number parameters model helping generalization deeper models effective task. different datasets penn tree bank consists k/k/k train/validation/test words distributed vocabulary words. preprocessing used. text pre-processed version ﬁrst million characters dumped wikipedia. split .m/.m/m character train/validation/test sets. word occurring less times replaced <unk> token resulting vocabulary size training details training procedure tasks except following. mini-batch update norm whole gradient parameters measured larger scaled norm crucial good performance. learning rate annealing schedule namely validation cost decreased epoch learning rate scaled factor training terminates learning rate drops i.e. epochs weights initialized using batch size penn tree dataset repeat training times different random initializations pick smallest validation cost. however done single training text dataset limited time constraints. results table compares model lstm structurally constrained recurrent nets baselines benchmark datasets. note baseline architectures tuned give optimal perplexity. memnn approach achieves lower perplexity datasets note memnn parameters rnns number hidden units lstm parameters. also vary number hops memory size memnn showing contribution performance; note particular increasing number hops helps. fig. show memnn operates memory multiple hops. shows average weight activation memory position test set. hops concentrate recent words hops broad attention memory locations consistent idea succesful language models consist smoothed n-gram model cache interestingly seems types hops tend alternate. also note unlike traditional cache decay exponentially roughly average activation across entire memory. source observed improvement language modeling. conclusions future work work showed neural network explicit memory recurrent attention mechanism reading memory successfully trained backpropagation diverse tasks question answering language modeling. compared memory network implementation supervision supporting facts model used wider range settings. model approaches performance model signiﬁcantly better baselines level supervision. language modeling tasks slightly outperforms tuned rnns lstms comparable complexity. tasks increasing number memory hops improves performance. however still much model still unable exactly match performance memory networks trained strong supervision fail several tasks. furthermore smooth lookups scale well case larger memory required. settings plan explore multiscale notions attention hashing proposed acknowledgments authors would like thank armand joulin tomas mikolov antoine bordes sumit chopra useful comments valuable discussions also fair infrastructure team help support.", "year": 2015}