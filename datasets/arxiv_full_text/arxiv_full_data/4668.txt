{"title": "Mastering 2048 with Delayed Temporal Coherence Learning, Multi-Stage  Weight Promotion, Redundant Encoding and Carousel Shaping", "tag": ["cs.AI", "cs.LG", "I.2.6; I.2.8"], "abstract": "2048 is an engaging single-player, nondeterministic video puzzle game, which, thanks to the simple rules and hard-to-master gameplay, has gained massive popularity in recent years. As 2048 can be conveniently embedded into the discrete-state Markov decision processes framework, we treat it as a testbed for evaluating existing and new methods in reinforcement learning. With the aim to develop a strong 2048 playing program, we employ temporal difference learning with systematic n-tuple networks. We show that this basic method can be significantly improved with temporal coherence learning, multi-stage function approximator with weight promotion, carousel shaping, and redundant encoding. In addition, we demonstrate how to take advantage of the characteristics of the n-tuple network, to improve the algorithmic effectiveness of the learning process by i) delaying the (decayed) update and applying lock-free optimistic parallelism to effortlessly make advantage of multiple CPU cores. This way, we were able to develop the best known 2048 playing program to date, which confirms the effectiveness of the introduced methods for discrete-state Markov decision problems.", "text": "abstract— engaging single-player nondeterministic video puzzle game which thanks simple rules hard-to-master gameplay gained massive popularity recent years. conveniently embedded discrete-state markov decision processes framework treat testbed evaluating existing methods reinforcement learning. develop strong playing program employ temporal difference learning systematic n-tuple networks. show basic method signiﬁcantly improved temporal coherence learning multi-stage function approximator weight promotion carousel shaping redundant encoding. addition demonstrate take advantage characteristics n-tuple network improve algorithmic effectiveness learning process delaying update applying lock-free optimistic parallelism effortlessly make advantage multiple cores. able develop best known playing program date conﬁrms effectiveness introduced methods discrete-state markov decision problems. engaging single-player nondeterministic puzzle game gained massive popularity recent years. already ﬁrst week release march man-years spent playing game according author. numerous clones downloaded tens millions times online mobile stores making part global culture. perspective artiﬁcial intelligence simple rules hard-to-master complexity presence nondeterminism makes ideal benchmark learning planning algorithms. conveniently described terms markov decision process paper treat challenging testbed evaluate effectiveness existing novel ideas reinforcement learning discrete-state mdps purpose build upon earlier work demonstrated temporal difference learningbased approach algorithm used standard rule learn weights n-tuple network institute computing science poznan university technology piotrowo pozna´n poland swiss idsia galleria manno-lugano switzerland email wjaskowskics.put.poznan.pl approximating afterstate-value function. paper extend method several directions. first show effectiveness temporal coherence learning technique automatically tuning learning rates. second introduce three novel methods concern learning process value function approximator multi-stage weight promotion carousel shaping iii) redundant encoding primary contributions paper. computational experiments reveal effectiveness ﬁrst techniques depend depth tree controller allowed search. synergy above-mentioned techniques allowed obtain best controller date. result would possible without paying attention computational effectiveness learning process. line also introduce delayed temporal difference online learning algorithm based whose performance used n-tuple network function approximator vastly independent decay parameter finally demonstrate large lookup table-based approximators n-tuple networks make multithreading capabilities modern cpus imposing lock-free optimistic parallelism learning process. work conﬁrms virtues n-tuples network function approximators. results indicate effective efﬁcient even involving billion parameters never shown before. although methods introduced paper developed general thus transferred discrete-state markov decision problems classical board games included. methodology whole directly used solve similar small-board nondeterministic puzzle games threes numerous clones. single-player nondeterministic perfect information video game played board. square board either empty contain single v-tile positive power denotes value tile. game starts randomly generated tiles. time random tile generated -tile -tile placed empty square board. sample initial state shown fig. objective game slide tiles merge adjacent ones ultimately create tile value turn player makes move consisting sliding tiles four directions right expectimax depth able achieve points average. reﬁnement controller lead scoring average recently matsuzaki systematically evaluated different n-tuple shapes construct -ply controller achieving points average rodgers levine investigated various tree search techniques able score average also numerous open source controllers available internet. best knowledge successful xiao based efﬁcient bitboard representation variable-depth expectimax transposition tables hand-designed stateevaluation heuristic optimized cma-es player average score mehta proved version game random moves pspace hard assuming known advance. game also recently appreciated tool teaching computer science artiﬁcial intelligence techniques roots reinforcement learning games tracked samuel’s famous work checkers popularized tesauro’s td-gammon master-level program backgammon obtained temporal difference learning. various reinforcement learning algorithms applied success games othello connect tetris atari video games recently alphago used reinforcement learning among others determine weights deep artiﬁcial neural network beat professional player n-tuple network bledsoe browning optical character recognition. context board games idea popularized lucas application othello however worth noticing similar concept called tabular value function already used buro strong othello-playing logistello program perspective reinforcement learning literature n-tuple network variant tile coding another name albus’s cerebellar model articulator controller section introduce basic concepts temporal difference learning using n-tuple network. learning framework presented last subsection corresponds method used previous work foundation improvements subsequent sections. left. move legal least tile slid. move -tile -tile randomly generated according aforementioned probabilities. instance fig. illustrates sliding initial tiles random -tile placed upper left corner. operation allows obtaining tiles larger values consists merging adjacent tiles. making move pair adjacent tiles value combined single tile along move direction. tile assigned total value joined tiles. additionally player gains reward equal value tile merge. figure shows board resulting combining -tiles upper sliding tiles left direction. another left move leads creating -tile moves generate rewards respectively. game considered -tile appears board. however players continue game even reaching tile. game terminates legal moves possible i.e. squares occupied adjacent tiles sharing value. game score rewards obtained throughout game. ﬁrst approached szubert ja´skowski employed million training episodes learn afterstate-value function represented systematic n-tuple network best function involved nearly million parameters scored average -ply. later extended work employing million training games learn larger systematic n-tuple system million parameters scoring average. employing three ntuple networks enabled different stages game along straight -tuple board. according values lookup table given board state returns since empty square encoded square containing square containing board value location locij constant denoting number possible tile values. empty square encoded square containing value e.g. encoded fig. illustration. n-tuple network principle linear approximator based highly nonlinear features number parameters n-tuple network number n-tuples size largest them. thus easily involve millions parameters since number grows exponentially time however state evaluation quick since algorithm deﬁned make biggest advantages n-tuple networks complex function approximators artiﬁcial neural networks. practice largest factor inﬂuencing performance n-tuple network evaluation memory accesses neglect claim time complexity following baseline function approximator network shown fig. name corresponds shapes tuples used. network consists four -tuples implies parameters learn. following earlier works n-tuple networks also make advantage symmetric sampling weight sharing technique allowing exploit symmetry board. symmetric sampling n-tuple employed eight times possible board rotation reﬂection. symmetric sampling facilitates generalization improves overall performance learning system without expense increasing size model practical problems infeasible represent value function lookup table entry state since state space large. example game states signiﬁcantly exceeds memory capabilities current computers. practice instead lookup tables function approximators used. function approximator function form vector parameters undergo optimization learning. using function approximator dimensionality search space reduced |θ|. result function approximator gets capabilities generalize. thus choice function approximator important choice learning algorithm itself. n-tuple network consists ni-tuples tuple’s size. given board state calculates values returned individual n-tuples. nituple consists predetermined sequence board locations j=...ni look-up table latter contains weights board pattern observed sequence board locations. n-tuple network parametrized weights lookup tables i=...m. baseline afterstate learning framework using rule n-tuple network function approximator presented alg. agent learns episodes one. learning episode starts initial state obtained according game speciﬁcation ends terminal criterion agent follows greedy policy w.r.t. afterstate-value function. determining action yields maximum afterstate value agent makes obtaining reward afterstate next state st+. information used compute prediction error backpropagate error afterstate predecessors notice special case terminal state implementation update rule ntuple network consisting n-tuples shown lines algorithm updates n-tuple network weights involved computing value state since term λt−k decreases decreasing yields negligible update values efﬁciency limited update horizon recently visited states. used logλ order retain updates expression λt−k important observe standard implementation involving eligibility traces cannot applied n-tuple network. eligibility traces vector length weight vector. standard sutton’s implementation consists updating elements vector learning step. computationally infeasible dealing hundreds millions weights. potential solution would maintain traces activated weights method applied connect four. notice however number activated weights episode depends length. connect four lasts maximally moves require several thousand moves. thus implementation requires limited update horizon efﬁcient.for convenience learning rate scaled number n-tuples result independent number n-tuples network easier interpret maximum sensible value since immediately reduces prediction error zero learning framework include exploration i.e. making non-greedy actions. despite experimenting \u0001-greedy softmax custom exploration ideas found non-greedy behavior signiﬁcantly inhibits temporal difference learning basic effective state-value function-based method solve reinforcement learning problems. reinforcement learning problem unknown learning agent know transition function reward function. case game mechanics given advance perfectly applicable successful also known mdps. updates state-value function response current next state agent’s action immediate consequences. assuming agent achieved state reward making actions state rule uses gradient descent update following prediction error learning rate decay parameter determines much prediction error back-propagated previously visited states. special case convenient assumption resulting rule called boils afterstate learning framework n-tuple networks agent make decision evaluate possible states maximally states lead serious performance overhead. previous work shown order reduce computational burden exploit concept afterstates encountered domains. afterstate denoted state reached agent’s action made random element applied. board position obtained immediately sliding tiles. process shown graphically section presenting common experimental setup introduce existing several novel algorithms improving afterstate learning framework introduced section stopping condition length episode highly depends agent’s competence increases number learning episodes. experiments paper limit number actions agent make lifetime instead number episodes play. unless stated otherwise give methods computational budget actions corresponds approximately learning episodes best algorithms. performance measures aimed maximizing expected score obtained agent. monitor learning progress every actions learning agent played episodes using greedy policy w.r.t. value function. average score denote -ply performance. since ultimately interested combining learned evaluation function tree search also regularly checked agent performs coupled expectimax depth played episodes purpose. computational aspects algorithms presented paper implemented java. experiments executed machine equipped opteron™ cpus ram. order take advantage cores applied lock-free optimistic parallelism reducing learning time factor comparing single-threaded program. lock-free optimistic parallelism consists letting threads play learn parallel using shared data without locks. since threads simultaneously read write n-tuple network lookup tables theory race conditions occur. practice however since n-tuple network involve millions parameters probability race condition negligible observed adverse effects technique. critical parameters temporal difference learning learning rate hard setup correctly hand. several algorithms automatically learning rate adapt time proposed. linear function approximation list online adaptive learning algorithms include beal’s temporal coherence dabney’s α-bounds sutton’s idbd mahmood’s autostep thorough comparison methods recently performed bagheri connect shown that learning rate adaptation methods clearly outperform standard difference substantial especially long run. evaluated here simplest temporal coherence advanced tuning-free autostep among best methods connect also boxes them adjust learning rate automatically maintain separate learning rates learning parameter. algorithm shows tcupdate function implements replacing tdupdate alg. except algorithm involves functions afterstate accumulate errors absolute errors respectively initially learning rate learning parameter determined expression |ei]| /ai] stay high error accumulators sign. signs start vary autostep autostep also uses separate learning rates function approximator weight updates signiﬁcantly sophisticated autostep extends sutton’s idbd normalizing step-size update setting upper bound reducing stepsize value detected large. three parameters meta-learning rate discounting factor initial step-size αinit. implementation autostep strictly followed original paper computational experiment evaluated three methods autostep differ number parameters. computational cost learning able systematically examine whole parameter space concentrated learning rate meta learning rate parameter autostep. choices made result informal preliminary experiments. table shows results computational experiment. results indicate that expected sensitive although autostep automatically adapt learning rates results show still signiﬁcantly susceptible meta-learning parameters. figure presents comparison learning dynamics best parameters found three algorithms considered. clearly automatic adaptation outperforms baseline automatic learning rate adaptation methods simple works better signiﬁcantly sophisticated autostep. autostep starts sharply learning dynamics slows considerably. autostep also characterized higher variance poor performance autostep might fact formulated supervised tasks disadvantage autostep since involve n-tuple networks need memory access thus increase learning time. experiments autostep took roughly times complete however performance improvements shown fig. worth overhead. observed computational bottleneck learning process lies access n-tuples lookup tables. lookup tables large cache. implementation presented alg. number lookup table accesses number n-tuples update horizon .this step current prediction error used update weights active last states. thus weight active given state updated times state falls beyond horizon. notice cumulative error used update signal weighted δt+kλk. novel algorithm delayed-td removes dependency h.it operates follows. delays update steps. updates weights cumulative error ∆t−h. weights active given visited state updated once. since vector stored errors constitutes small continuous memory block processor cache. makes access time negligible compared time required access main memory large n-tuple network lookup tables reside. delayed versions shown alg. notice since actual update delayed done also episode ﬁnished algorithm needs take care recently visited states. finally function executed last statement learnfromepisode function rules computational experiment results shown table reveal statistically signiﬁcant differences standard delayed versions temporal difference algorithms. hand expected delayed versions algorithms signiﬁcantly quicker. delayedtc roughly times effective standard difference minor weight update procedure quick anyway dominated action selection following experiments delayed version stages improve learning process split game stages used separate function approximator game stage. method already used buro’s logistello master-level othello playing program variant manually selected stages already successfully applied motivation multiple stages results observation order precisely approximate statevalue function certain features different weights depending stage game. game split stages small positive integer. assuming maximum tile obtain length stage +−g. example game starts stage switches stage -tile achieved stage -tile stage board stage also need multi-stage versions functions algorithm thus lines alg. become stage) estage) astage) weight promotion initial experiments multi-stage approximator revealed increasing model using multi-stage approach actually harms learning performance. because stage n-tuple network must learn scratch. result generalization capabilities multi-stage function approximator signiﬁcantly limited. order facilitate generalization without removing stages initialize weight upon ﬁrst access weight corresponding weight preceding stage implement idea following lines added evaluate function lines alg. stage) stage) results shown table fig. indicate multi-stage approximator involving stages actually slows learning compared single stage baseline. generally true -ply -ply -ply -stages approximator eventually outperforms stage baseline. however fig. shows drop performance -stage approximator rather increase learning performance multi-stage approach. actions -stage approximator’s -ply performance starts decrease overﬁtting -ply settings used learning. -stage baseline weight promotion. although algorithm variants weight promotion still learn slower pace baseline eventually outperform regardless search tree depth. optimal number stages? answer question depends depth agent evaluated. results show stages overkill since works worse stages. choice evident though. -ply approximator involving stages learns faster achieves slightly better performance stages. -ply situation opposite clearly excels since interested best score absolute terms favor stages variant. stages involve parameters learn. already stated stages imply times parameters -stage baseline. better spent parameter budget? order answer question constructed large single stage network containing -tuples shapes shown fig. network similar size stages network involving weights. fig. reveals network indeed excels -ply performs poorly -ply. interestingly -ply improve network also suffers -ply overﬁtting syndrome case -stage baseline. tempting conclude multi-stage approximators immune -ply overﬁtting syndrome. notice however overﬁtting happens process converges neither multi-stage variants converged yet. best setup -ply involving stages weight promotion signiﬁcantly improved -stage baseline. although neither number stages weight selection negatively inﬂuence learning time algorithm found require times actions converge -ply performance always positively correlate -ply performance. algorithms exhibit overﬁtting -ply settings better agent gets -ply worse scores -ply. carousel shaping deal -ply overﬁtting introducing carousel shaping concentrates learning effort later stages motivated fact small fraction episodes gets later stages game. carousel shaping tries compensate this. accurate estimates later stages game particularly important k-ply agents since look ahead -ply ones. carousel shaping notion initial state stage. game’s initial state initial state stage subsequent states t−)+ stage last visited initial states stage maintained learning. carousel shaping algorithm starts initial state stage learning episode proceeds subsequent stage. starts episode randomly selected initial state current stage. restarts stage hits empty initial states learning often visits later stages game. note technique different introduced algorithm learns n-tuple networks stage stage. number learning episodes given stage moves next never updates weights previous again. state-value function learned accurate. information learned given stage backpropagate previous stages. might lead suboptimal behavior. redundant encoding last technique apply redundant encoding. deﬁne redundant encoding additional features function approximator increase expressiveness. although features redundant point view function approximator facilitate learning quicker generalization. n-tuple network redundant encoding consists adding smaller n-tuples comprised larger ones. experiments ﬁrst extended standard network -tuples shapes straight lines length squares denoted finally added results results applying carousel shaping redundant encoding shown table. clearly effect carousel shaping -ply performance consistently slightly negative. less important however since method signiﬁcantly improves -ply performance regardless network architecture. best algorithm involves redundant -tuples -tuples. compared standard network redundant -tuples help make difference -ply. redundant tuples show signiﬁcant improvement -ply. ﬁgure also indicates synergetic effect redundant encoding carousel shaping especially algorithm using techniques scores nearly points average. downside redundant encoding makes learning slower involving n-tuples results table lookups state evaluation. table shows learning time roughly proportional number n-tuples involved. notice however redundant n-tuples required learning. after learning ﬁnished lookup tables corresponding redundant n-tuples integrated lookup tables n-tuples contain redundant ones. table performance algorithms involving carousel shaping redundant encoding actions. variants used network stages weight promotion trained delayed-tc table shows performance network depends deep expectimax allowed search game tree. experiment player allowed either constant depth constant time move latter case agent could deeper -ply time allows happen especially end-games branching factor gets smaller. transposition tables used improve performance expectimax. program executed intel single-threaded. table shows that generally deeper search tree better average score also clear however time spent effectively limit time move rather search tree depth also increasing time move allows make better decisions renders better scores. table shows performance comparison best controller published players. clearly player outperforms rest. important show large controller runner-up n-tuple network outranks regardless expectimax settings move -ply depth moreover network even slightly better runner-up -ply play times faster. conﬁrms effectiveness learning methodology developed paper. order obtain result took advantage several existing introduced reinforcement learning techniques first applied temporal coherence learning turned signiﬁcantly improve strength player compared vanilla temporal difference learning. second used different function approximator stage game shown pays additional generalization methods applied. introduced weight promotion technique. third demonstrated controller improved carousel shaping employed. makes later stages game visited often. worsens controller -ply improves -ply really care about. finally redundant encoding boosted learning effectiveness substantially. redundant encoding facilitates generalization result learning speed ﬁnal controller score. although slows learning redundant n-tuples incorporated non-redundant ones learning ﬁnished making ﬁnal agent quicker. also showed techniques lessen computational burden. introduced delayed versions temporal difference update methods demonstrated reduce learning time temporal coherence several times making state-value function update mechanism vastly independent decay parameter secondly proved practical efﬁcacy lock-free optimistic parallelism made possible utilize cores learning. broader point view although concentrated speciﬁc problem game techniques introduced general applied discrete markov decision problems n-tuple networks broadly tile encodings effectively used function approximation. worth emphasize role n-tuple networks overall result. although scale state spaces large dimensionality undeniable advantages state spaces moderate sizes dimensions. provide nonlinear transformations computational efﬁciency. demonstrated n-tuple networks billions parameters time tiny fraction involved evaluate given state update state-value function. hand enormous number parameters n-tuple network seen disadvantage requires vast amount computer memory. hand also reason lock-free optimistic parallelism work seamlessly. perspective work extended several directions. first although n-tuple networks allowed obtain competent player believe generalize well problem. rather learn heart. humans generalize signiﬁcantly better. example board states tiles doubled values human player would certainly similar same strategy. n-tuple network cannot that. learning takes days. open question design efﬁcient state-value function approximator generalizes better? actually clear whether possible since example state-value approximator supposed return different values. speculate better generalization require turning away state-value functions learning policies directly. directly learned policy need return numbers second direction improve controller performance involves including hand-designed features. although local features would probably help since n-tuple networks involve possible local features function approximator lacks global features like number empty tiles board. introduced techniques could also improved ﬁne-tuned least. example experiment carousel shaping. possible parametrize order control much puts learning attention later stages. question effectively involve exploration also open. exploration mechanism tried worked lack exploration eventually make learning prone stuck local minima efﬁcient exploration another open question. moreover learning rate adaptation mechanism temporal coherence somewhat trivial arbitrary. since sophisticated autostep failed opportunity learning rate adaptation method. last least demonstrated learned evaluation function harnessed tree search algorithm. used standard expectimax iterative deepening transposition tables advanced pruning could potentially improve performance. author thank marcin szubert comments manuscript adam szczepa´nski implementing efﬁcient version agent. work supported polish national science centre grant dec-//d/st/. computations performed poznan supercomputing networking center. beal martin smith. temporal coherence prediction proceedings international decay learning. joint conference artiﬁcial intelligence volume pages morgan kaufmann publishers inc. marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research michael buro. simple features sophisticated evaluation functions. proceedings first international conference computers games pages london springer. lucian bus¸oniu robert babuˇska bart schutter damien ernst. reinforcement learning dynamic programming using function approximators. press boca raton florida wojciech ja´skowski marcin szubert. coevolutionary cma-es knowledge-free learning game position evaluation. ieee transactions computational intelligence games wojciech ja´skowski marcin szubert paweł liskowski krzysztof krawiec. high-dimensional function approximation knowledgefree reinforcement learning case study sz-tetris. gecco’ proceedings annual conference genetic evolutionary computation pages mardid spain july press. simon lucas. learning play othello n-tuple systems. australian journal intelligent information processing systems special issue game technology ashique rupam mahmood richard sutton thomas degris acoustics patrick pilarski. tuning-free step-size adaptation. speech signal processing ieee international conference pages ieee java source code used perform learning experiments shown paper available https//github.com/wjaskowski/mastering-. efﬁcient code running best player along best n-tuple network found https//github.com/aszczepanski/. maarten schadd mark winands uiterwijk. chanceprobcut forward pruning chance nodes. computational intelligence games ieee symposium pages ieee bruno scherrer mohammad ghavamzadeh victor gabillon boris lesner matthieu geist. approximate modiﬁed policy iteration journal machine application game tetris. learning research page david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature marcin szubert wojciech ja´skowski. temporal difference learning ieee conference n-tuple networks game computational intelligence games pages dortmund ieee. marcin szubert wojciech ja´skowski krzysztof krawiec. scalability generalization hybridization coevolutionary learning case study othello. ieee transactions computational intelligence games marcin szubert wojciech ja´skowski krzysztof krawiec. scalability generalization hybridization coevolutionary learning case study othello. ieee transactions computational intelligence games markus thill. temporal difference learning methods automatic step-size adaption strategic board games connect- dots-andboxes. master’s thesis cologne univ. applied sciences markus thill samineh bagheri patrick koch wolfgang konen. temporal difference learning eligibility traces game connect four. ieee conference computational intelligence games pages ieee markus thill patrick koch wolfgang konen. reinforcement learning n-tuples game connect-. carlos coello coello editor parallel problem solving nature ppsn volume lecture notes computer science pages springer i-chen kun-hao chao-chin liang chia-chuan chang chiang. multi-stage temporal difference learning technologies applications artiﬁcial intelligence pages springer", "year": 2016}