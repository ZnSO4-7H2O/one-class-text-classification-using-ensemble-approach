{"title": "Continuous Relaxation of MAP Inference: A Nonconvex Perspective", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In this paper, we study a nonconvex continuous relaxation of MAP inference in discrete Markov random fields (MRFs). We show that for arbitrary MRFs, this relaxation is tight, and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm. In addition, we study the resolution of this relaxation using popular gradient methods, and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers (ADMM). Experiments on many real-world problems demonstrate that the proposed ADMM significantly outperforms other nonconvex relaxation based methods, and compares favorably with state of the art MRF optimization algorithms in different settings.", "text": "optimization constantly attracting signiﬁcant amount research last decades. since problem general np-hard various approximate methods proposed roughly grouped classes methods stay discrete domain move-making belief propagation methods move continuous domain solving convex relaxations quadratic programming relaxations semi-deﬁnite programming relaxations prominently linear programming relaxations convex relaxations allow beneﬁt tremendous convex optimization literature solved exactly polynomial time often produce real-valued solutions need rounding step converted integer ones reduce signiﬁcantly accuracy relaxations tight. contrary discrete methods tackle directly original problem combinatorial nature challenging task. refer recent comparative study methods wide variety problems. paper consider different approach. present nonconvex continuous relaxation inference problem arbitrary mrfs. based block coordinate descent rounding scheme guaranteed increase energy continuous solutions show nonconvex relaxation tight actually equivalent original discrete problem. noted relaxation previously discussed pairwise paper study nonconvex continuous relaxation inference discrete markov random ﬁelds show arbitrary mrfs relaxation tight discrete stationary point easily reached simple block coordinate descent algorithm. addition study resolution relaxation using popular gradient methods propose effective solution using multilinear decomposition framework based alternating direction method multipliers experiments many real-world problems demonstrate proposed admm signiﬁcantly outperforms nonconvex relaxation based methods compares favorably state optimization algorithms different settings. finding maximum posteriori conﬁguration fundamental inference problem undirected probabilistic graphical models also known markov random ﬁelds problem described follows. s×···×sn denote assignment discrete random variables variable takes values ﬁnite states graph nodes cliques consider representing joint distribution factorizes i.e. takes form product tensor multiple vectors deﬁned consecutive product tensor vector order multiplied vectors matter. example product th-order tensor rn×n×n×n vectors modes back problem node )s∈si vector composed possible values clique potential function sid∀ indices thus seen αth-order tensor dimensions |si| |si| |siα|. denote tensor. recall energy term corresponding mrfs importantly directly solved. signiﬁcance nonconvex relaxation remained purely theoretical since then. paper demonstrate great practical signiﬁcance well. addition establishing theoretical properties nonconvex relaxation arbitrary mrfs based study popular generic optimization methods projected gradient descent frank-wolfe algorithm solving methods however empirically shown suffer greatly trivial hardness nonconvex optimization getting stuck local minima. overcome difﬁculty propose multilinear decomposition solultion based alternating direction method multipliers experiments different real-world problems show proposed nonconvex based approach outperform many previously mentioned methods different settings. remainder paper organized follows. section presents necessary notation formulation approach. section nonconvex relaxation introduced properties studied resolution presented section together convergence analysis section section presents experimental validation comparison state methods. last section concludes paper. often convenient rewrite energy using indicator functions labels assigned node. denote nodes graph function deﬁned node takes label otherwise. easily seen minimizing equivalent following problem rewritten function {xi}i∈v later convenience reformulation using tensor notation needed. ﬁrst give brief review tensor. real-valued dth-order tensor multidimensional array belonging rn×n×···×nd dimension tensor called mode. elements denoted fii...id index along mode clear advantage relaxation relaxation compactness. indeed nodes number labels number variables number constraints relaxation respectively relaxation numbers respectively degree mrf. section interesting properties presented. particular prove relaxation tight show obtain discrete stationary point ﬁrst propose simple algorithm solve relaxation tightness properties follow naturally. number nodes. vector seen n-block vector block corresponds node starting initial solution solves iteratively optimizing ﬁxing blocks. note subsequent analysis still valid variants updating random order using subgraphs trees update blocks. keep presentation simple however choose update deterministic order update step consists solving update becomes minimizing solved using following straightforward lemma. lemma argminβ problem minu=u≥ optimal solution follows compute using position smallest element clearly solution returned update step discrete. easily seen update equivalent assigning node following label remark. starting discrete solution equivalent iterated conditional modes note however designed continuous problem whereas relies discrete problem theorem continuous relaxation tight. proof. since continuous closed according weierstrass extreme value theorem must attain minimum denote xmrf xrlx respectively. obviously solution initialization xrlx. hand since descent algorithm hand since solution returned discrete yielding putting together implies i.e. tight. proposed method shares similarities method introduced solving graph matching. however make admm efﬁcient effective inference following important practical contributions formulate problem using individual potential tensors clique allows better exploitation problem structure computational quantities node cached based neighboring nodes yielding signiﬁcant speed-ups; discuss choose decomposed constraint sets result best accuracy inference different). addition present convergence analysis proposed method section instead dealing directly high degree polynomial highly challenging idea decompose different variables handled separately using lagrangian relaxation. consider following multilinear function guaranteed reach discrete stationary point guarantee quality point. practice shown later experiments performance compares poorly state optimization methods. fact challenge nonconvex optimization might many local minima consequence algorithms easily trapped ones even multiple initializations. next section study resolution using sophisticated methods come multilinear decomposition admm reach good local minima different real-world models. since energy differentiable worth investigating whether gradient methods effectively optimize brieﬂy present methods next section. proposed admm based algorithm presented subsequent section. provide convergence analysis methods section projected gradient descent frank-wolfe algorithm among popular methods solving constrained optimization. refer excellent presentation methods. algorithm projected gradient descent solving initialization compute projection step-sizes follow chosen update rule. straightforward diminishing rule example however practice step-sizes often lead slow convergence. better alternative expression looks complicated intuition simple given node degree search cliques satisfying conditions sizes bigger equal node position cliques; clique multiply potential tensor nodes except node products together. therefore update becomes minimizing quadratic function suitable decompositions problem much simpler form efﬁciently solved. example choose cyclic decomposition step reduced ﬁnding projection vector onto note linear constraint general enforce inﬁnite number particular instances. example suitable choices ≤d≤d linear constraint become either following sets constraints belong simreason constraining plex make reach consensus faster without allowed vary freely tend bypass good solutions. idea looser constraint sets e.g. becomes simply found leaving yields best accuracy. therefore implementation parallelization since dependency among nodes constraint sets projection clearly reduced independent projections node. moreover iteration expensive computation also performed parallel nodes. therefore proposed admm highly parallelizable. caching signiﬁcant speed-ups achieved avoiding re-computation unchanged quantities. seen depends decomposed variables neighbors thus variables changed last iteration need recompute current iteration. similarly projection omitted section establish convergence results presented methods. space constraints proofs provided supplementary material. deﬁnition continuously differentiable function closed convex point called stationary point problem minu∈m satisﬁes multiplier concatenation vectors ≤d≤d corresponding constraints similar results obtained speciﬁc decompositions star symmetric well. refer supplement details. observed similar performance among decompositions cyclic included evaluation practice found penalty parameter constraint sets ≤d≤d greatly affect convergence well solution quality admm. address together practical considerations. adaptive penalty observed small leads slower convergence often better energy inversely large obtain good trade-off follow following adaptive scheme initialize small value iterations improvement residual achieved every iterations increase factor addition stop increasing reaches value ρmax convergence properties presented next section still apply. experiments normalize potentials ρmax recall deﬁnition condition equivalent x∗d. therefore point must form vector proposition sequence generated admm assume residual converges limit point sequence point note result partial since need assumption converges practice found assumption always holds large enough. unlike gradient methods convergence admm kind problem less known current active research topic. example global convergence admm nonconvex nonsmooth functions established numerous assumptions applicable case. interesting relation solutions returned methods following. method improve method returned solution initialization output better solution. proposition convergence compare proposed nonconvex relaxation methods following ones α-expansion fast primal-dual convex relaxation sequential tree reweighted message passing tree reweighted belief propagation alternating direction dual decomposition bundle dual decomposition max-product linear programming extension extension α-expansion higher-order using reduction technique generalization trws higher-order code methods obtained either opengm library authors’ websites except implementation code publicly available different initializations pick best one. admm methods evaluated several real-world vision tasks image inpainting feature matching image segmentation stereo reconstruction. methods included whenever applicable. summary models given table except higher-order stereo models previously considered recent benchmark evaluating optimization methods model ﬁles publicly available. higher-order stereo model presented disparity encouraged piecewise smooth using second-order prior labels obtained pre-generated piecewiseplanar proposals. apply model image pairs middlebury dataset refer supplement details models. experiments carried -bit linux machine .ghz processor memory. time limit hour methods. tables report runtime energy value ﬁnal integer solution well lower bound available averaged instances particular model. detailed results given supplement. general admm signiﬁcantly outperforms nonconvex relaxation method compares favorably methods. particular outperforms trbp addd bundle mplp mplp-c models feature matching model typical example showing standard relaxation loose. methods solving dual produce poor results largely outperformed trws nonconvex relaxation methods problem mplp-c reaches global optimum instances. interesting observation performs worse nonconvex methods models means simply solving relaxation straightforward manner already better adding sophisticated convexiﬁcation step. ﬁnding rather surprising. image segmentation srmp performs exceptionally well producing global optimum instances fast. admm slightly outperformed srmp terms energy value clearly outperform methods. large scale models stereo trws/srmp perform best terms energy value followed move making algorithms admm. example estimated disparity maps given figure srmp nonconvex relaxation methods. results methods given supplement. presented tight nonconvex relaxation problem inference studied four different methods solving block coordinate descent projected gradient descent frank-wolfe algorithm admm. high nonconvexity challenging obtain good solutions relaxation shown performance ﬁrst three methods. latter however outperforms many existing methods thus demonstrates directly solving nonconvex relaxation lead accurate results. methods memory efﬁcient thanks small number variables constraints that proposed admm algorithm also highly parallelizable case methods like trws srmp. therefore admm also suitable distributed realtime applications gpus. acknowledgements research partially supported european research council starting grant diocles vision project authors thank jeanchristophe pesquet useful discussion gradient-based methods thank anonymous reviewers insightful comments.", "year": 2018}