{"title": "Spontaneous vs. Posed smiles - can we tell the difference?", "tag": ["cs.CV", "cs.AI"], "abstract": "Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.", "text": "abstract. smile irrefutable expression shows physical state mind true deceptive ways. generally shows happy state mind however ‘smiles’ deceptive example people give smile feel happy sometimes might also give smile feel pity others. work aims distinguish spontaneous smile expressions posed smiles extracting analyzing global motion face subtle changes facial expression features tracking series facial ﬁducial markers well using dense optical ﬂow. speciﬁcally eyes lips features captured used analysis. aims automatically classify smiles either ‘spontaneous’ ‘posed’ categories using support vector machines experimental results large database show promising results compared relevant methods. people believe human face mirror/screen showing internal emotional state human body responds external world. means that individual thinks feels understands deep inside brain imitated outside world face facial smile expression undeniably plays huge pivotal role understanding social interactions within community. people often give smile imitating internal state body. example generally people smile happy sudden humorous things happen/appear front them. however people sometimes forced pose smile outside pressure external factors. example people would pose smile even don’t understand joke humor. sometimes people would also pose smile even reluctantly unwillingly perform something front bosses/peers therefore able identify type smiles individuals would give aﬀective computing deeper understanding human interactions. large amount research psychology neuroscience studying facial behavior demonstrate spontaneous deliberately displayed facial behavior diﬀerences utilized facial muscles dynamics compared posed ones example spontaneous smiles smaller amplitude longer duration slower onset oﬀset times posed smiles humans capturing subtle facial movements diﬃcult often fail distinguish them. surprising computer vision algorithms developed classifying pose spontaneous smiles usually fail generalize subtlety complexity human pose spontaneous aﬀective behavior numerous researchers asserted dynamic features duration speed smile play part diﬀerentiating nature smile spontaneous smile usually take longer time reach onset apex oﬀset compared posed smile non-dynamic features aperture size eyes found useful clue generally higher value extracted spontaneous smile compared posed one. hand symmetry movement spontaneous posed smiles produce signiﬁcant distinction identifying therefore much useful multi-modal system using geometric features shoulder head inner facial movements fused together gentlesvm-sigmod used classify posed spontaneous smiles. proposed technique feature extraction compared performance using geometric facial appearance features. appearance based features computed recording statistics overall pixel values image even using edge detection algorithm gabor wavelet filter. comprehensive study shows geometric features generally eﬀective detecting posed spontaneous expressions spatiotemporal method involving natural infrared face videos distinguish posed spontaneous expressions proposed using temporal space image sequences volume extended complete local binary patterns texture based descriptor spatiotemporal features classify posed spontaneous smiles. dibeklioglu used dynamics eyelid movements deﬁned distance based angular features changes aperture. using several classiﬁers shown superiority eyelid movements eyebrows cheek movements smile classiﬁcation. later used dynamic characteristics eyelid cheek corner movements classifying posed spontaneous smiles. temporal facial information obtained segmenting facial expression onset apex oﬀset cover entire duration smile. reported good classiﬁcation performance small database using combination features extracted diﬀerent phases. block diagram proposed method shown fig. given smile video sequences various subjects apply facial features detection tracking ﬁducial points entire smile video clip. using d-markers important parameters extracted important regions face eyes lips. smile discriminative features extracted using dense optical along temporal domain facial tracking algorithm developed nguyen obtain ﬁducial points face. tracking markers labeled placed following convention shown fig. markers manually annotated ﬁrst frame video user input thereafter automatically tracks remaining frames smile video good accuracy precision compared facial tracking software markers placed important facial feature points eyelids corner lips subject. convention followed approach selecting ﬁducial markers shown fig. reduce inaccuracy subject’s head motion video cause change angle respect roll pitch rotations face normalization procedure described represents feature points used align faces shown fig. three non-collinear points used form plane centers deﬁned angles positive normal vector unit vectors axes give relative head pose follows denote vectors point points respectively. knρk represents magnitudes vectors respectively. using human face conﬁguration estimate exact roll angles face respect camera. start frontal face pitch angles computed subtracting initial value. using estimated head pose tracked ﬁducial points normalized respect rotation scale translation follows aligned point. denote rotation matrices given angles. euclidean distance measure. essentially constructs normal vector perpendicular plane face using three points calculate angle formed axis regards normal vector face plane. thereafter process normalize every point frame accordingly interocular distance pixels middle point acting origin face center. ﬁrst part strategy focus extracting subject’s eyelid lips features. ﬁrst construct amplitude signal variable based facial feature markers eyelid regions. compute amplitude eyelid movements smile using procedure described eyelid amplitude signals computed using eyelid aperture displacement time given denotes relative vertical location function equals located face otherwise. equation uses markers eyelids namely shown fig. construct amplitude signal calculate eyelid aperture size frame amplitude signal deyelid computed obtain series features. addition amplitudes speed acceleration signal also extracted computing second derivatives amplitudes. eyelids markers corner points. onset phase deﬁned longest continuous increase dlip. similarly oﬀset phase detected longest continuous decrease dlip. apex deﬁned phase last frame onset ﬁrst frame oﬀset. displacement signals eyelids corners could calculated using tracked points. onset apex oﬀset phases smile estimated using maximum continuous increase decrease mean displacement eyelids corners. d-marker able extract descriptive features eyelids corner vector features obtained frame features concatenated passed training classiﬁcation. second phase feature extraction proposed dense optical capturing global local motions appearing smile videos. approach divided four distinct stages fully automatic require human intervention. ﬁrst step detect frame face present. previously developed face integration sketch graph patterns eyes mouth detectors face recognition wearable devices human-robot-interaction region interest face accuracy entire uva-nemo smile database second step determine area corresponding right left mouth blue accuracy entire database. fig. left face eyes mouth detections. yellow face detection eyes detection blue mouth detection. middle consecutive frames subject’s smile video right optical ﬂows xy-directions. third step optical computed image time time video sequence components optical illustrated fig. bottom shows optical along x-axis optical along y-axis. using dense optic algorithm time process picture relatively important. speed processing computed optic three regions right left mouth. optical computed approach pyramidal diﬀerential dense algorithm based following constraint attachment term based thresholding method regularization term based method developed meyer weight controlling ratio attachment term control. ouarti proposed regularization usual wavelet non-stationary wavelet packet generalize concept wavelet extracting optical information. extend idea extracting grained information micro macro motion variations smile videos shown fig. fig. shows dense optical ﬂows spontaneous posed smiles variations. fourth step three rois median optical determined give global motion area. histogram also computed based optical bins. three bins term cardinality kept among bins. linear regression applied major axis point group three bins determined. obtain median value value value also calculates intercept slope points bins result features frame used features classify posed spontaneous smiles. major advantage approach obtain useful smile discriminative features using fully automatic analysis videos marker needed annotated operator/user. moreover rather attempting classify optical design processing obtain sparse representation optical signal. representation helps classiﬁcation extracting useful information dimensions speeds calculation svm. finally information completely connected positioning diﬀerent knowing positioning vary frame another test proposed algorithm uva-nemo smile database largest extensive smile database videos total subjects aged years giving total individual videos. video consists short segment either posed spontaneous smiles. videos extracted frames frames second. extracted frames also converted gray scale downsized experiments split database used training samples remaining used testing samples. binary classiﬁer libsvm used form hyperplane based training samples. testing sample passed uses hyperplane determine class sample falls under. process repeated times using -fold cross validation method. measure subtle diﬀerences spontaneous posed smiles compute confusion matrices smiles much accuracy obtain using actual classiﬁed separately. results processes averaged shown confusion tables compared methods table tables bracket show accuracy rates distinguishing spontaneous smiles posed ones using eyes lips features. results show features play crucial role ﬁnding posed smiles lips features important spontaneous smiles. overall could obtain accuracy using eyes lips features respectively. table show classiﬁcation performance using combined features eyes lips. evident table using facial component features pose smile classiﬁed better compared spontaneous ones. features using dense optical described section movement xy-directions recorded every consecutive frames video. confusion matrices shown tables bracket tables performance optical lower compared component based approach. however facial component based feature extraction method requires user initialization track ﬁducial points dense optical features fully automatic. require user intervention useful practical applications like ﬁrst-person-views egocentric views wearable devices like google glass improving real-time social interactions combine features obtained facial component based parameters dense optical single vector apply svm. table shows confusion matrix using spontaneous posed smiles. seen performance spontaneous smiles classiﬁcation improved using features dense optical ﬂow. experimental results table show features facial components dense optical ﬂows important improving accuracy. features facial components useful encoding information arising muscle artifacts within face however regularized dense optical features helps encoding grained information micro macro motion variations face smile videos. diﬀerentiating spontaneous smiles posed ones challenging problem involves extracting subtle minute facial features learning them. work analysed features extracted facial component based parameters using fuducial points markers tracking them. also obtained fully automatic features dense optical eyes mouth patches. shown facial component based parameters give higher accuracy compared dense optical features smile classiﬁcation. however former requires initialization ﬁducial markers ﬁrst frame hence fully automatic. dense optical advantage features obtained without manual intervention. combining facial components parameters dense optical gives highest accuracy classifying spontaneous posed smiles. experimental results largest uva-nemo smile database shows eﬃcacy proposed method.", "year": 2016}