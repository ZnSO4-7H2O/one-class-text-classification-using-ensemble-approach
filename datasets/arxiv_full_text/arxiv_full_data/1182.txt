{"title": "MatConvNet - Convolutional Neural Networks for MATLAB", "tag": ["cs.CV", "cs.LG", "cs.MS", "cs.NE"], "abstract": "MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.", "text": "matconvnet implementation convolutional neural networks matlab. toolbox designed emphasis simplicity ﬂexibility. exposes building blocks cnns easy-to-use matlab functions providing routines computing linear convolutions ﬁlter banks feature pooling many more. manner matconvnet allows fast prototyping architectures; time supports eﬃcient computation allowing train complex models large datasets imagenet ilsvrc. document provides overview cnns implemented matconvnet gives technical details computational block toolbox. overview network structures sequences directed acyclic graphs computing derivatives backpropagation derivatives tensor functions derivatives function compositions backpropagation networks backpropagation dags backpropagation networks convolution convolution transpose spatial pooling activation functions relu sigmoid spatial bilinear resampling normalization local response normalization batch normalization spatial normalization softmax categorical losses classiﬁcation losses attribute losses comparisons p-distance matconvnet matlab toolbox implementing convolutional neural networks computer vision applications. since breakthrough work cnns major impact computer vision image understanding particular essentially replacing traditional image representations ones implemented vlfeat open source library. cnns obtained composing simple linear non-linear ﬁltering operations convolution rectiﬁcation implementation trivial. reason cnns need learned vast amounts data often millions images requiring eﬃcient implementations. libraries matconvnet achieves using variety optimizations chieﬂy supporting computations gpus. numerous machine learning deep learning open source libraries exist. cite popular ones cudaconvnet torch theano caﬀe. many libraries well supported dozens active contributors large user bases. therefore creating another library? motivation developing matconvnet provide environment particularly friendly eﬃcient researchers investigations. matconvnet achieves deep integration matlab environment popular development environments computer vision research well many areas. particular matconvnet exposes simple matlab commands building blocks convolution normalisation pooling combined extended ease create architectures. many blocks optimised implementations written cuda matlab native support computation means often possible write blocks matlab directly maintaining computational eﬃciency. compared writing components using lower level languages important simpliﬁcation signiﬁcantly accelerate testing ideas. using matlab also provides bridge towards https//code.google.com/p/cuda-convnet/ http//cilvr.nyu.edu/doku.php?id=codestart http//deeplearning.net/software/theano/ http//caffe.berkeleyvision.org while user perspective matconvnet currently relies matlab library developed clean separation matlab code cuda core; therefore future library extended allow processing convolutional networks independently matlab. matconvnet learn large models alexnet deep networks millions images. pre-trained versions several powerful models downloaded matconvnet home page. powerful matconvnet remains simple install. implementation fully self-contained requiring matlab compatible compiler demonstrated section possible download compile install matconvnet using three matlab commands. several fully-functional examples demonstrating small large networks learned included. importantly several standard pre-trained network immediately downloaded used applications. manual complete technical description toolbox maintained along toolbox. features make matconvnet useful educational context too. matconvnet simple install use. provides complete example classiﬁes image using latest-generation deep convolutional neural network. example includes downloading matconvnet compiling package downloading pre-trained model evaluating latter matlab’s stock images. command example vl_simplenn wrapper takes input pre-processed image produces output structure results. particular wrapper used model networks simple structure namely chain operations. examining code vl_simplenn note wrapper transforms data sequentially applying number matlab functions speciﬁed network conﬁguration. function discussed detail chapter called building blocks constitute backbone matconvnet. blocks implement simple operations makes trivial eﬃciency well support backpropagation allow learning cnns. next demonstrate building blocks directly. sake example consider convolving image bank linear ﬁlters. start reading http//devblogs.nvidia.com/parallelforall/deep-learning-image-understanding-planetary-science/ http//www.vlfeat.org/matconvnet/ http//www.vlfeat.org/matconvnet/matconvnet-manual.pdf example laboratory experience based matconvnet downloaded http//www. figure complete example including download installing compiling running matconvnet classify matlab stock images using large pre-trained imagenet. users encouraged make blocks directly create architectures matlab provides wrappers vl_simplenn standard architectures alexnet network-in-network furthermore library provides numerous examples including code learn variety models mnist cifar imagenet datasets. examples examples/cnn_train training code implementation stochastic gradient descent training code perfectly serviceable quite ﬂexible remains examples/ subdirectory somewhat problem-speciﬁc. users welcome implement optimisers. matconvnet simple design philosophy. rather wrapping cnns around complex layers software exposes simple functions compute building blocks linear convolution relu operators directly matlab commands. building blocks easy combine complete cnns used implement sophisticated learning algorithms. several real-world examples small large architectures training routines provided always possible back basics build using eﬃciency matlab prototyping. often coding required architectures. such matconvnet ideal playground research computer vision cnns. computational blocks. optimized routines computing fundamental building blocks cnn. example convolution block implemented y=vl_nnconv image ﬁlter bank vector biases derivatives computed vl_nnconv dzdy derivative output w.r.t chapter describes blocks detail. wrappers. matconvnet provides simple wrapper suitably invoked vl_simplenn implements linear topology also provides much ﬂexible wrapper supporting networks arbitrary topologies encapsulated dagnn.dagnn matlab class. pre-trained models. matconvnet provides several state-of-the-art pre-trained models used oﬀ-the-shelf either classify images produce image encodings spirit caﬀe decaf. three main sources information matconvnet. first website contains descriptions functions several examples tutorials. second manual containing great deal technical details toolbox including detailed mathematical descriptions building blocks. third matconvnet ships several examples examples fully self-contained. example order mnist example suﬃces point matlab matconvnet root directory type addpath examples followed cnn_mnist. problem size imagenet ilsvrc example requires preparation including downloading preprocessing images several advanced examples included well. example illustrates top- top- validation errors model similar alexnet trained using either standard dropout regularisation recent batch normalisation technique latter shown converge third epochs required former. matconvnet website contains also numerous pre-trained models i.e. large cnns trained imagenet ilsvrc downloaded used starting point many problems include alexnet vgg-s vgg-m vgg-s vgg-vd vgg-vd- example code shows model used lines matlab code. simply converts arguments gpuarrays matlab vl_nnconv gpuarray manner switching fully transparent. note matconvnet also make nvidia cudnn library signiﬁcant speed space beneﬁts. next evaluate performance matconvnet training large architectures imagenet ilsvrc challenge data test machine dell server intel xeon clocked four nvidia titan black gpus experiments matconvnet beta cudnn matlab data preprocessed avoid rescaling images matlab stored disk faster access. code uses vl_imreadjpeg command read large batches jpeg images disk number separate threads. driver examples/cnn_imagenet.m used experiments. train models discussed section imagenet ilsvrc. table reports training speed number images second processed stochastic gradient descent. alexnet trains images/s cudnn faster vanilla implementation times faster using cpus. furthermore note that despite matlab overhead implementation speed comparable caﬀe note also that model grows size size batch must decreased increasing overhead impact somewhat. table reports speed vgg-vd- large model using multiple gpus. case batch size images. divided sub-batches images memory; latter distributed among four gpus machine. substantial communication overhead training speed increases images/s addressing overhead medium term goals library. matconvnet community project acknowledgements contributors. kindly thank nvidia supporting project providing top-of-the-line gpus mathworks ongoing discussion improve library. chapter provides brief introduction computational aspects neural networks convolutional neural networks particular emphasizing concepts required understand matconvnet. sequence simpler functions called computational blocks layers. outputs layer network denote network input. intermediate output computed previous output applying function parameters convolutional neural network data spatial structure rhl×wl×cl array tensor ﬁrst dimensions interpreted spatial dimensions. third dimension instead interpreted number feature channels. hence tensor represents ﬁeld cldimensional feature vectors spatial location. fourth dimension tensor spans multiple data samples packed single batch eﬃciency parallel processing. number data samples batch called batch cardinality. network called convolutional functions local translation invariant operators like linear convolution. also possible conceive cnns spatial dimensions additional dimensions represent volume time. fact little a-priori restrictions format data neural networks general. many useful contain mixture convolutional layers together layer process data types text strings perform operations strictly conform assumptions. matconvnet includes variety layers contained matlab/ directory vl_nnconv vl_nnconvt vl_nnpool vl_nnrelu vl_nnsigmoid vl_nnsoftmax vl_nnloss vl_nnbnorm vl_nnspnorm vl_nnnormalize eﬃcient method networks contain several million parameters need trained millions images; thus eﬃciency paramount matlab design discussed section also requires compute derivatives explained next section. simplest case layers arranged sequence; however complex interconnections possible well fact useful many cases. section discusses conﬁgurations introduces graphical notation visualize them. limited chaining layers another. fact requirement evaluating that layer evaluated input evaluated prior possible exactly interconnections layers form directed acyclic graph short. order visualize dags useful introduce additional nodes network variables example fig. boxes denote functions circles denote variables example inputs outputs. functions take number inputs number outputs noteworthy properties graph variables incoming arrows parameters computed network must prior evaluation i.e. inputs. variable used output although usually variables outgoing arrows. learning requires computing derivative loss respect network parameters. derivatives computed using algorithm called backpropagation memory-eﬃcient implementation chain rule derivatives. first discuss derivatives single layer whole network. derivatives tensor functions layer function input rh×w×c output rh×w tensors. derivative function contains derivative output component yijk respect input component xijk total elements naturally arranged tensor. instead expressing derivatives tensors often useful switch matrix notation stacking input output tensors vectors. done operator visits element tensor lexicographical order produces vector storing requires space single precision. purpose backpropagation algorithm compute derivatives required learning without incurring huge memory cost. explicitly stored. construction repeated multiplying pairs factors left right obtaining sequence tensors desired derivative obtained. note that large tensor ever stored memory. process known backpropagation. implement backpropagation able compute products without explicitly computing storing memory second factor large jacobian matrix. since computing derivative linear operation product interpreted derivative layer projected along direction best illustrated example. consider layer convolution operator implemented matconvnet vl_nnconv command. forward mode calls function vl_nnconv) apply ﬁlters input obtain output backward mode calls vl_nnconvp). explained above size respectively. computation large jacobian encapsulated function call never carried explicitly. context back-propagation useful think projection linearization rest network variable loss. projected derivative also though layer that computing derivative mini-network operates reverse direction structure. furthermore order simplify notation assume list contains data parameter variables distinction moot discussion section. point sequence ﬁxing arbitrary value dropping layers feed them eﬀectively transforming ﬁrst variables inputs. then rest deﬁnes function maps input variables output beginning backpropagation since intermediate variables function last layer fπl. thus projected derivatives projected derivatives resulting equation here uniformity iterations fact initialized zero anaccumulate values instead storing them. practice update operation needs carried variables actual inputs often tiny fraction variables dag. given information next iteration backpropagation updates variables contain projected derivatives instead. general given derivatives backpropagation computes derivatives using relation more update needs explicitly carried variables actual inputs fπl. particular data input parameter original neural network depend variables parameters nullary function case update anything. iteration completes backpropagation remains with previous step variable input layers would output layers reversed network creates conﬂict. resolve conﬂicts inserting summation layer adds contributions easy enough combine computational blocks chapter manually. however usually much convenient wrapper implement architectures given model speciﬁcation. available wrappers brieﬂy summarised section matconvnet also comes many pre-trained models image classiﬁcation image segmentation text spotting face recognition. simple illustrated section simplenn wrapper suitable networks consisting linear chains computational blocks. largely implemented vl_simplenn function support functions vl_simplenn_move vl_simplenn_display vl_simplenn takes input structure representing well input potentially output derivatives dzdy depending mode operation. please refer inline help vl_simplenn function details input output formats. fact implementation vl_simplenn good example basic neural building blocks used together serve basis complex implementations. dagnn wrapper complex simplenn support arbitrary graph topologies. design object oriented class implementing layer type. adds complexity makes wrapper slightly slower tiny architectures practice much ﬂexible easier extend. note image preprocessed running network. preprocessing speciﬁcs depend model pre-trained model contains net.meta.normalization ﬁeld describes type preprocessing expected. note particular network takes images ﬁxed size input requires removing mean; also image intensities normalized range note several extensions possible. first images cropped rather rescaled. second multiple crops network results averaged usually improved results. third output network used generic features image encoding. matconvnet compute derivatives using backpropagation simple implement learning algorithms basic implementation stochastic gradient descent therefore straightforward. example code provided examples/cnn_train. code ﬂexible enough allow training nminst cifar imagenet probably many datasets. corresponding examples provided examples/ directory. consider preprocessing data convert images height pixels. done supplied utils/preprocess-imagenet.sh script. manner training resize images every time. forget point training code pre-processed data. chapters describes individual computational blocks supported matconvnet. interface computational block <block> designed discussion chapter block implemented matlab function vl_nn<block> takes input matlab arrays representing input data parameters returns array output. general real arrays packing maps images discussed above whereas arbitrary shape. function implementing block capable working backward direction well order compute derivatives. done passing third optional argument dzdy representing derivative output network respect case function returns derivatives vl_nn<block> respect input data parameters. arrays dzdx dzdy dzdw dimensions respectively diﬀerent functions slightly diﬀerent syntax needed many functions take additional optional arguments speciﬁed property-value pairs; parameters others take multiple inputs parameters case dzdx dzdy dzdw. rest chapter matlab inline help details syntax. other parts library wrap functions objects perfectly uniform interface; however low-level functions providing straightforward obvious interface even means diﬀering slightly block block. figure convolution. ﬁgure illustrates process ﬁltering signal ﬁlter obtain signal ﬁlter elements applied stride samples. purple areas represented padding zeroﬁlled. filters applied sliding-window manner across input signal. samples involved calculation sample shown arrow. note rightmost sample never processed ﬁlter application sampling step. case sample padded region happen also without padding. output size. vl_nnconv computes valid part convolution; i.e. requires application ﬁlter fully contained input support. size output computed section given receptive ﬁeld size geometric transformations. often useful geometrically relate indexes various array input data terms coordinate transformations size receptive ﬁeld derived section fully connected layers. libraries fully connected blocks layers linear functions output dimension depends input dimensions. matconvnet distinguish fully connected layers convolutional blocks. instead former special case latter obtained output dimensions internally vl_nnconv handles case eﬃciently possible. filter groups. additional ﬂexibility vl_nnconv allows group channels input array apply diﬀerent subsets ﬁlters group. feature specify input bank ﬁlters rh×w ×d×d divides number input dimensions treated ﬁlter groups; ﬁrst group applied dimensions input second group dimensions note output still array rh×w application grouping implementing krizhevsky hinton network uses streams. another application pooling; latter case specify groups dimensional ﬁlters identical ﬁlters value input tensor ﬁlters output tensors. imagine operating reverse direction using ﬁlter bank convolve output obtain input using deﬁnitions given section convolution operator; since convolution linear expressed matrix convolution transpose computes instead process illustrated slice important applications convolution transpose. ﬁrst called deconvolutional networks networks convolutional decoders transpose convolution. second implementing data interpolation. fact convolution block supports input padding output downsampling convolution transpose block supports input upsampling output cropping. figure convolution transpose. ﬁgure illustrates process ﬁltering signal ﬁlter obtain signal ﬁlter applied sliding-window pattern transpose ﬁlter samples total although ﬁlter application uses circulant manner. purple areas represent crops discarded. samples involved calculation sample shown arrow. note that diﬀerently samples right involved convolution operation. consider output sample index chosen divides according sample obtained weighted summation xi/sh xi/sh− weights ﬁlter elements fshfsh subsampled step consider computing element yi+; rounding quotient operation output sample obtained weighted combination elements input used compute however ﬁlter weights shifted place right fsh+fsh+ true cycle restarts shifting padding stride. similar convolution operator section vl_nnpool supports padding input; however eﬀect diﬀerent padding convolutional block pooling regions straddling image boundaries cropped. pooling vl_nnbilinearsampler uses bilinear interpolation spatially warp image according input transformation grid. operator works input image grid output image follows where feature channel output yijc location weighted input values xijc neighborhood location gij). weights given correspond performing bilinear interpolation. furthermore grid coordinates expressed pixels relative reference frame extends vl_nnnormalize implements local response normalization operator. operator applied independently spatial location groups feature channels follows vl_nnbnorm implements batch normalization batch normalization somewhat diﬀerent neural network blocks performs computation across images/feature maps batch note case input output arrays explicitly treated tensors order work batch feature maps. tensors deﬁne component-wise multiplicative additive constants. output feature given vl_nnspnorm implements spatial normalization. spatial normalization operator acts diﬀerent feature channels independently rescales input feature energy features local neighbourhood first energy features neighbourhood note operator applied across feature channels convolutional manner spatial locations. softmax seen combination activation function normalization operator. section implementation details. operator sense loss evaluated independently spatial location. however contribution diﬀerent samples summed together output loss scalar. section losses useful multi-class classiﬁcation section losses useful binary attribute prediction. technical details section vl_nnloss implements following these. rh×w×c×n c}h×w××n slice xijn represent vector class scores cijn ground truth class label. \u0001instanceweights\u0001 option used specify tensor weights otherwise ones; dimension loss negative posterior log-probability. case interpreted vector posterior probabilities classes. loss negative log-probability ground truth class note also that unless loss undeﬁned. reasons usually output block softmax guarantee conditions. however composition naive loss softmax numerically unstable. thus implemented special case below. structured multi-class hinge loss. structured multi-class logistic loss also know crammer-singer loss combines multi-class hinge loss block computing score margin attribute losses similar classiﬁcation losses case classes mutually exclusive; instead binary attributes. attribute losses decompose additively follows relationship hinge loss structured multi-class hinge loss analogous relationship binary logistic loss multi-class logistic loss. namely hinge loss rewritten note operator applied convolutionally i.e. spatial location extracts compares vectors xij. specifying option 'noroot' true possible compute variant omitting root section interested understanding components depend components layers particular components input. since cnns incorporate blocks perform complex operations example cropping inputs based data-dependent terms information generally available time cannot uniquely determined given structure network. furthermore blocks implement complex operations diﬃcult characterise simple terms. therefore analysis necessarily limited scope. consider blocks convolutions deterministically establish dependency chains network components. also assume inputs outputs usual form spatial maps therefore indexed xijdk spatial coordinates. consider layer interested establishing components inﬂuence components also assume relation expressed terms sliding rectangular window ﬁeld called receptive ﬁeld. means output addition computing receptive ﬁeld geometry often interested determining sizes arrays throughout architecture. case ﬁlters obtained reasoning slice notice largest value receptive ﬁelds falls outside height input array condition matconvnet treats pooling operators like ﬁlters using rules above. library caﬀe done slightly diﬀerently creating incompatibilities. case pooling window allowed shift enough last application always includes last pixel input. stride greater means last application pooling window partially outside input boundaries even padding oﬃcially zero. conclude that choices allowed caﬀe formula violate constraint unit. caﬀe special provision lowers needed. furthermore case assumed caﬀe) equation also satisﬁed caﬀe skips check. next matconvnet equivalents parameters. assume caﬀe applies align part output signal. match caﬀe last sample last ﬁlter application right last caﬀe-padded pixel convolution transpose block similar simple ﬁlter somewhat complex. recall convolution transpose transpose convolution operator turn ﬁlter. reasoning slice input convolution transpose block output. furthermore upsampling suppose determined later receptive ﬁeld transformation suppose given block transpose like convolution transpose layer transpose convolution layer. this mean that depends depends consider combination domains same. given rule above possible compute output sample depends input sample input sample suppose gives receptive ﬁelds respectively. assume domain coincide i.e. goal determine combined receptive ﬁeld. possible case fact possible sliding window receptive ﬁeld tightly encloses receptive ﬁeld points according formulas receptive ﬁelds compatible. range input samples aﬀect output sample given used implement convolutional operator; seem ineﬃcient instead fast approach number ﬁlters large allows leveraging fast blas blas implementations. order understand deﬁnition convolution transpose obtained convolution operator deﬁned section since linear operation rewritten suitable matrix convolution transpose computes instead simple describe term matrices happens term indexes tricky. order derive formula convolution transpose start standard convolution downsampling factor padding length input signal length ﬁlter padding index input data exceed range implicitly assume signal zero padded outside range. expressed matrix form suitable selector matrix d)×. derivatives written null points operator diﬀerentiable max-pooling similar relations exists diﬀerences depend input binary order account normalization factors. summary expressions", "year": 2014}