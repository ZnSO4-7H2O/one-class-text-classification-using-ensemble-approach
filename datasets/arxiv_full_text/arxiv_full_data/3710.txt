{"title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial  Training", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "We propose an adversarial training procedure for learning a causal implicit generative model for a given causal graph. We show that adversarial training can be used to learn a generative model with true observational and interventional distributions if the generator architecture is consistent with the given causal graph. We consider the application of generating faces based on given binary labels where the dependency structure between the labels is preserved with a causal graph. This problem can be seen as learning a causal implicit generative model for the image and labels. We devise a two-stage procedure for this problem. First we train a causal implicit generative model over binary labels using a neural network consistent with a causal graph as the generator. We empirically show that WassersteinGAN can be used to output discrete labels. Later, we propose two new conditional GAN architectures, which we call CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained causal implicit generative model for the labels is then a causal implicit generative model over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.", "text": "propose adversarial training procedure learning causal implicit generative model given causal graph. show adversarial training used learn generative model true observational interventional distributions generator architecture consistent given causal graph. consider application generating faces based given binary labels dependency structure labels preserved causal graph. problem seen learning causal implicit generative model image labels. devise two-stage procedure problem. first train causal implicit generative model binary labels using neural network consistent causal graph generator. empirically show wasserstein used output discrete labels. later propose conditional architectures call causalgan causalbegan. show optimal generator causalgan given labels samples image distributions conditioned labels. conditional combined trained causal implicit generative model labels implicit causal generative network labels generated image. show proposed architectures used sample observational interventional image distributions even interventions naturally occur dataset. generative adversarial networks neural generative models trained using backpropagation mimick sampling high dimensional nonparametric distributions generator network models sampling process feedforward computation. generator output constrained reﬁned feedback competitive \"adversary network\" attempts discriminate generated real samples. application sampling distribution images generator typically neural network outputs image given independent noise variables. objective generator maximize loss discriminator gans shown tremendous success generating samples distributions image video even proposed language translation figure observational interventional image samples causalbegan. architecture used sample joint distribution also interventional distribution e.g. intervention resulting distributions clearly diﬀerent evident samples outside dataset e.g. females mustaches. extension idea gans enable sampling class conditional data distributions feeding labels generator. various neural network architectures proposed solving problem aware works class labels chosen independently another. therefore choosing label aﬀect distribution labels. result architectures provide functionality condition label sample labels image. concreteness consider generator trained output images birds given color species labels. hand feed generator color=blue since species label independent color label likely blue eagles well blue jays. however expect blue eagles conditioned color=blue dataset real bird images. similarly consider generator trained output face images given gender mustache labels. labels chosen independently another images generated mustache contain males females clearly diﬀerent conditioning mustache understanding unifying notions conditioning able sample distributions diﬀerent dataset’s causality. think generating image conditioned labels causal process labels determine image distribution. generator functional labels image distributions. consistent simple causal graph \"labels cause image\" represented graph labels generated image. using ﬁner model also include causal graph labels. using notion causal graphs interested extending previous work conditional image generation example consider causal graph gender mustache labels. causal relation clearly gender causes mustache shown graph conditioning gender=male expect males without mustaches based fraction males mustaches population. condition mustache expect sample males since population contain females mustaches. addition sampling conditional distributions causal models allow sample various diﬀerent distributions called interventional distributions explain next. reality confounder variables i.e. variables aﬀect both observable. work ignore eﬀect assuming graph causal suﬃciency i.e. exist unobserved variables cause observable variable. labels. however practice labels independent even clear causal connections using empty causal graph instead true causal graph setting label particular value equivalent intervening label original causal graph also ignoring aﬀects variables. intervention experiment ﬁxes value variable without aﬀecting rest causal mechanism diﬀerent conditioning. intervention variable aﬀects descendant variables causal graph. unlike conditioning aﬀect distribution ancestors. example instead causal graph gender causes mustache used empty causal graph labels intervening gender female would create females mustaches whereas correct causal graph yield females without mustaches since setting gender variable aﬀect variables downstream e.g. mustache. figure sample results illustrate concept bald mustache variables. similarly generating birds causal graph species causes color intervening color blue allows sample blue eagles whereas conditioning color blue not. implicit generative model mechanism sample probability distribution cannot provide likelihoods data points. work propose causal implicit generative models mechanisms sample probability distributions also conditional interventional distributions. show generator structure inherits neural connections causal graph gans used train causal implicit generative models. wassersteingan train causal implicit generative model image labels part two-step procedure training causal implicit generative model images image labels. second step propose novel conditional architecture loss function called causalgan. show optimal generator sample correct conditional interventional distributions summarized following theorem. theorem output generator given label latent vector global optimal generator loss function rest network trained optimality. generator samples conditional image distribution given label i.e. pdata pdata data probability density function image labels probability density function induced random variable image random variable. concatenated causalgan causal implicit generative model labels image. corollary suppose causal implicit generative model causal graph image labels observational joint distribution labels strictly positive. class conditional generator sample image distribution conditioned given label combination causal implicit generative model causal graph words corollary states following consider causal graph image labels image variable every label causes image. combining implicit causal generative model induced subgraph labels conditional generative model image given labels yields causal implicit generative model consider problem conditional interventional sampling images given causal graph binary labels. propose two-stage procedure train causal implicit generative model binary labels image. part procedure propose novel conditional architecture loss function. show global optimal generator provably samples class conditional distributions. propose natural nontrivial extension began accept labels using motivations margins began arrive \"margin margins\" term cannot neglected. show empirically model call causalbegan produces high quality images capture image labels. evaluate causal implicit generative model training framework labeled celeba data show combined architecture generates images capture observational interventional distributions images labels jointly show surprising result causalgan causalbegan produce high-quality label-consistent images even label combinations realized interventions never occur training e.g. \"woman mustache\". using generative adversarial network conditioned image labels proposed before authors propose extend generative adversarial networks setting extra information labels. label image generator discriminator. architecture called conditional gan. authors propose architecture called infogan attempts maximize variational lower bound mutual information labels given generator image. authors propose conditional architecture performs well higher resolution images. class label given generator. image dataset also chosen conditioned label. addition deciding image real fake discriminator also output estimate class label. using causal principles deep learning using deep learning techniques causal inference recently gaining attention. authors observe connection conditional layers structural equation models. based observation cgan learn causal direction variables dataset. authors propose using neural network order discover causal relation image class labels based static images. authors propose regularization training neural network call causal regularization order assure model predictive causal sense. recent work authors point connection gans causal generative models. however image cause neural weights labels. bigan improve standard framework provide functionality learning mapping image space latent space. cogan authors learn joint distribution given samples marginals enforcing weight sharing generators. example used learn joint distribution image labels. clear however approach work generator structured causal graph. sd-gan architecture splits latent space \"identity\" \"observation\" portions. each generated labels sharply concentrated around global optimal remaining network trained optimality. code available https//github.com/mkocaoglu/causalgan generate faces person identity portion latent code. works well datasets identity multiple observations. authors conditional one-hot encoded vector encodes interval. generator conditioned one-hot vector used changing attribute face image. another application generative models compressed sensing authors give compressed sensing guarantees recovering vector data lies close output trained generative model. section give brief introduction causality. speciﬁcally pearl’s framework i.e. structural causal models uses structural equations directed acyclic graphs random variables represent causal model. explain causal principles apply framework examples. detailed treatment subject technical details consider random variables within structural causal modeling framework causal suﬃciency assumption causes simply means exists function unobserved random variable independent unobserved variables also called exogenous. causal graph represents relation general causal graph directed acyclic graph implied structural equations parents node causal graph represent causes variable. causal graph constructed structural equations follows parents variable appear structural equation determines value variable. formally structural causal model tuple contains functions random variables exogenous random variables probability distribution exogenous variables observable variables joint distribution implied distributions functional relations distribution projection onto variables shown causal graph directed acyclic graph nodes node parent node domain i.e. parents variable shown bayesian network induced joint probability distribution observable variables assume causal suﬃciency every exogenous variable direct parent observable variable. intervention operation changes underlying causal mechanism hence corresponding causal graph. intervention denoted diﬀerent conditioning following intervention removes connections node parents whereas conditioning change causal graph data sampled. interpretation that example value longer determined function intervention nodes deﬁned similarly. joint distribution variables intervention calculated follows since bayesian network joint distribution observational nodes assigned corresponding values {xi}i∈. intervention nodes causally suﬃcient system every unobserved variable aﬀects single observed variable. deﬁnition provided assumes causal suﬃciency i.e. exogenous variables aﬀect observable variable. causal suﬃciency pearl’s model assumes distribution exogenous variables product distribution i.e. exogenous variables mutually independent. general possible identify true causal graph variables without performing experiments making additional assumptions. multiple causal graphs lead joint probability distribution even variables paper address problem learning causal graph assume causal graph given learn causal model i.e. functions distributions exogenous variables comprising structural equations. signiﬁcant prior work learning causal graphs could used method e.g. true causal graph unknown feasible graph i.e. bayesian network respects conditional independencies present data. conditional independencies known richer model used although larger number functional relations learned case. explore eﬀect used bayesian network section used bayesian network edges inconsistent true causal graph conditional distributions correct interventional distributions diﬀerent. implicit generative models used sample probability distribution without explicit parameterization. generative adversarial networks arguably successful examples implicit generative models. thanks adversarial training procedure gans able produce realistic samples distributions high dimensional space images. sample desired distribution samples vector known distribution gaussian uniform feeds feedforward neural network trained given dataset. although implicit generative models sample data distribution provide functionality sample interventional distributions. causal implicit generative models provide sample observational interventional distributions. show generative adversarial networks also used training causal implicit generative models. consider simple causal graph causal suﬃciency assumption model written functions jointly independent variables. following simple observation useful training framework generator neural network connections arranged reﬂect causal graph structure. consider figure feedforward neural networks used represent functions noise terms chosen independent complying condition jointly independent. hence feedforward neural network used represents causal graph within class functions represented given family neural networks. following proposition well known causality literature. shows given true causal graph causal models observational distribution interventional distribution intervention. proposition nfpn)m nfqn) causal models. figure causal graph implied standard generator architecture feedforward neural network. neural network implementation causal graph feed forward neural captures function structural equation model proof. note causal bayesian networks interventional distributions causal bayesian networks directly calculated conditional probabilities causal graph. thus interventional distributions. following deﬁnition ties feedforward neural network causal graph deﬁnition mutually independent random variables. feedforward neural network outputs vector called consistent causal graph layers written fi}j∈p zsi) parents causal implicit generative models trained given causal graph samples joint distribution. however application image generation binary labels found diﬃcult simultaneously learn joint label image distribution applications focus dividing task learning causal implicit generative causal model subtasks first learn causal implicit generative model small variables. then learn remaining variables conditioned ﬁrst variables using conditional generative network. training consistent causal structure every node ﬁrst come node second respect partial order causal graph. assume problem generating images based image labels inherently contains causal graph similar given figure makes suitable two-stage training first train generative model labels train generative model images conditioned labels. show next architecture loss function assures optimum generator outputs label conditioned image distributions. assumption joint probability distribution labels strictly positive combining pretrained causal generative model labels label-conditioned image generator gives causal implicit generative model images. formal statement corollary postponed section describe adversarial training causal implicit generative model binary labels. generative model call causal controller used controlling distribution images sampled intervened conditioned labels. section structure causal controller network sequentially produce labels according causal graph. since theoretical results hold binary labels prefer generator sample essentially discrete label distribution however standard training suited learning discrete distribution properties jensen-shannon divergence. able sample discrete distribution employ wassersteingan used model lipschitz constraint gradient replaced penalty term loss. part two-step process proposed section learning causal implicit generative model labels image variables design conditional architecture generate images based labels causal controller. unlike previous work architecture loss function assures optimum generator outputs label conditioned image distributions. pretrained causal controller updated. labeler anti-labeler separate labeler neural networks. labeler trained estimate labels images dataset. anti-labeler trained estimate labels images sampled generator. label generated image label produced causal controller. generator objective generator -fold producing realistic images competing discriminator capturing labels given produced images minimizing labeler loss avoiding drifting towards unrealistic image distributions easy label maximizing anti-labeler loss. optimum causal controller labeler anti-labeler later show optimum generator samples distribution class conditional images. important distinction causalgan existing conditional architectures uses anti-labeler network addition labeler network. notice theoretical guarantee develop section hold anti-labeler network used. intuitively anti-labeler loss discourages generator network generate typical faces ﬁxed label combination. phenomenon call label-conditioned mode collapse. literature minibatch-features popular techniques used avoid mode-collapse however diversity within batch images diﬀerent label combinations make approach ineﬀective combatting label-conditioned mode collapse. observe intuition carries practice. present results single binary label general case binary labels extension labeler generator losses slightly modiﬁed. explain extension supplementary material section along proof optimal generator samples class conditional distribution given d−dimensional label vector. mappings generator discriminator labeler anti-labeler respectively. generator loss function causalgan contains label loss terms loss added loss term discriminator. addition term generator loss able prove optimal generator outputs class conditional image distribution. result also true multiple binary labels. remark although authors additive term ex∼pg deﬁnition loss function practice term ex∼pg interesting note extra loss terms need global optimum correspond class conditional image distributions label loss. section propose simple non-trivial extension began feed image labels generator. central contributions began control theory-inspired boundary equilibrium approach encourages generator training discriminator near optimum gradients informative. following observation helps carry idea case labels label gradients informative image quality high. here introduce loss margins reﬂect intuition. formally average pixel-wise autoencoder loss image began. squared loss term i.e. sample data distribution image corresponding label. similarly image sample generator label used generate image. denoting space images generator. naive attempt extend original began loss formulation include labels write following loss functions however naive formulation address margins extremely critical began formulation. better trained began discriminator creates useful gradients image generation better trained labeler prerequisite meaningful gradients. motivates additional margin-coeﬃcient tuple shown generator tries jointly minimize loss terms formulation empirically observe occasionally image quality suﬀer images best exploit labeler network often obliged realistic noisy misshapen. based this label loss seems unlikely provide useful gradients unless image quality remains good. therefore encourage generator incorporate label loss image quality margin large compared label margin achieve this introduce margin margins term result margin equations update rules summarized follows learning rates coeﬃcients. advantages began existence monotonically decreasing scalar track convergence gradient descent optimization. extension preserves property deﬁne section show best causalgan generator given loss function outputs class conditional image distribution causal controller outputs real label distribution labelers operate optimum. show result case single binary label proof extended multiple binary variables explain supplementary material section aware ﬁrst conditional generative adversarial network architecture guarantee. first optimal discriminator ﬁxed generator. note terms discriminator optimize loss hence optimal discriminator behaves standard gan. then following lemma directly applies discriminator deﬁne generator loss discriminator labeler anti-labeler optimum. then show generator minimizes outputs class conditional image distributions. theorem global minimum virtual training criterion achieved data i.e. given label generator output class conditional image distribution pdata. proof. please supplementary material. show stage procedure used train causal implicit generative model causal graph image variable sink node captured following corollary corollary suppose causal implicit generative model causal graph image labels observational joint distribution labels strictly positive. class conditional sample image distribution conditioned given label combination causal implicit generative model causal graph proof. please supplementary material. theorem show optimum generator samples class conditional distributions given single binary label. objective extend result case binary labels. first show labeler anti-labeler trained output scalars interpreted posterior probability particular label combination given image minimizer samples class conditional distributions given labels. result shown theorem supplementary material. however large architecture hard implement. resolve this propose alternative architecture implement experiments extend single binary label setup cross entropy loss terms label. requires labeler anti-labeler outputs. however although need generator capture joint label posterior given image assures generator captures label’s posterior distribution i.e. this general guarantee class conditional distributions true data distribution. however many joint distributions practical interest labels completely determined image show guarantee implies joint label posterior true data distribution implying optimum generator samples class conditional distributions. please section formal results details. section explain implementation details wasserstein causal controller generating face labels. used total variation distance distribution generator data distribution metric decide success models. gradient term used penalty estimated evaluating gradient points interpolated real fake batches. interestingly wasserstein approach gives opportunity train causal controller output discrete labels practice though still found beneﬁt rounding passing generator. generator architecture structured accordance section based causal graph figure using uniform noise exogenous variables layer neural networks functions mapping parents children. training used wasserstein discriminator updates generator update learning rate practice stochastic gradient descent train model. dcgan convolutional neural net-based implementation generative adversarial networks extend causal framework. expanded adding labeler networks training causal controller network modifying loss functions appropriately. compared dcgan important distinction make generator updates discriminator update average. discriminator labeler networks concurrently updated single iteration. notice loss terms deﬁned section contain single binary label. practice feed d-dimensional label vector need corresponding loss function. extend labeler anti-labeler loss terms simply averaging loss terms every label. coordinates d-dimensional vectors given labelers determine loss terms label note diﬀerent architecture given section discriminator outputs length-d vector estimates probabilities label combinations given image. therefore approach guarantee sample class conditional distributions data distribution restricted. however type labeled image dataset work labels seem completely determined given image architecture suﬃcient guarantees. details please section supplementary material. compared theory have another diﬀerence implementation swapped order terms cross entropy expressions labeler losses. provided sharper images training. important challenge comes gradient-based training anti-labeler. observe following early stages training anti-labeler quickly minimize loss generator falls label-conditioned mode collapse. recall deﬁne labelconditioned mode-collapse problem generating typical faces label ﬁxed. example generator output face eyeglasses variable helps generator easily satisfy label loss term loss function. notice however label-conditioned mode collapse occurs anti-labeler easily estimate true labels given image since always provided image. hence maximizing anti-labeler loss early stages training helps generator avoid label-conditioned mode collapse loss function. later stages training loss terms generator outputs realistic images drives anti-labeler similar labeler. thus maximizing anti-labeler loss minimizing labeler loss become contradicting tasks. moves training direction labels captured less less generator hence losing conditional image generation property. terms loss term loss labeler loss anti-labeler respectively number iterations training time constant exponential decaying coeﬃcient anti-labeler loss. chosen experiments corresponds roughly epoch training. figure causal graph used simulations causalgan causalbegan called causal graph also edges form complete graph \"cg\". also make graph obtained reversing direction every edge labels input causalbegan taken causal controller. parameter tunings. learning rate generator discriminator update simultaneously simply expect model sensitive parameter values achieve good performance without hyperparameter tweaking. customized margin learning rates reﬂect asymmetry quickly generator respond margin. example much \"spiky\" fast responding behavior compared others even paired smaller learning rate although explored parameter space depth. margin behaviors observe best performing models three margins \"active\" near frequently taking small positive values. figure convergence total variation distance generated distribution true distribution causal implicit generative model generator structured based diﬀerent causal graphs. data generated line graph best convergence behavior observed true causal graph used generator architecture. data generated collider graph fully connected layers perform better true graph depending number layers. collider complete graphs performs better line graph implies wrong bayesian network. data generated complete graph fully connected layers performs best followed complete fully connected layers. line collider graphs implies wrong bayesian network show convergence behavior. consider causal implicit generative model convergence synthetic data whose three features arise three causal graphs \"line\" \"collider\" \"complete\" node cubic polynomial variables computes value node given parents uniform exogenous variable. repeat creating synthetic dataset causal model report averaged results runs model. data generating graphs compare convergence joint distribution true joint terms total variation distance generator structured according line collider complete graph. completeness also include generators knowledge causal structure fully connected neural networks uniform random noise output variables using either layers respectively. results given figure data generated line causal graph collider causal graph complete causal graph curve shows convergence behavior generator distribution generator structured based causal graphs. expect convergence causal graph used structure generator capable generating joint distribution true causal graph long correct bayesian network able true joint. example complete graph encode joint distributions. hence expect complete graph work well data generation models. standard fully connected layers correspond causal graph latent variable causing observable variables. ideally model able causal generative model. however convergence behavior adversarial training across models unclear exploring figure line graph data best convergence behavior line graph used generator architecture. expected complete graph also converges well figure number line unit length binned unequal bins along percent causal controller samples bin. results obtained sampling joint label distribution times forming histogram scalar outputs corresponding label. note causal controller output labels approximately discrete even though input continuum uniform almost zero near progression total variation distance causal controller output respect number iterations causal graph used training wasserstein loss. slight delay. similarly fully connected network layers show good performance although surprisingly fully connected layers perform much worse. seems although fully connected encode joint distribution theory practice adversarial training number layers tuned achieve performance using true causal graph. using wrong bayesian network collider also yields worse performance. collider graph surprisingly using fully connected generator layers shows best performance. however consistent previous observation number layers important using layers gives worst convergence behavior. using complete collider graphs achieves decent performance whereas line graph wrong bayesian network performs worse two. complete graph fully connected performs best followed fully connected complete graph. expect line collider graphs cannot encode distributions complete graph performs worst actually show convergence behavior. causal graph training ﬁrst verify wasserstein training allows generator learn mapping continuous uniform noise discrete distribution. figure shows samples averaged labels causal graph generator appears real line. result emphasizes proposed causal controller outputs almost discrete distribution samples appear .−neighborhood outputs shown unrounded generator outputs. assymptotes around corresponds incorrect conditional independence assumptions makes. suggests given complete causal graph lead nearly perfect implicit causal generator labels bayesian partially incorrect causal graphs still give reasonable convergence. section train whole causalgan together using pretrained causal controller network. results given figures a-a. diﬀerence intervening conditioning clear certain features. implement conditioning rejection sampling. works conditioning implicit generative models. figure intervening/conditioning mustache label causal graph since ustache causal graph expect aﬀect probability i.e. accordingly shows males females mustaches even though generator never sees label combination ustache training. bottom images sampled conditional distribution shows male images dataset figure intervening/conditioning bald label causal graph since bald causal graph expect aﬀect probability i.e. accordingly shows bald males bald females. bottom images sampled conditional distribution shows male images dataset figure intervening/conditioning wearing lipstick label causal graph since earinglipstick causal graph expect aﬀect probability i.e. accordingly shows males females wearing lipstick. however bottom images sampled conditional distribution shows female images dataset figure intervening/conditioning mouth slightly open label causal graph since smiling outhslightlyopen causal graph expect aﬀect probability smiling i.e. however bottom conditioning mouth slightly open increases proportion smiling images although images enough show diﬀerence statistically. figure intervening/conditioning narrow eyes label causal graph since smiling narrow eyes causal graph expect aﬀect probability smiling i.e. however bottom conditioning narrow eyes increases proportion smiling images although images enough show diﬀerence statistically. figure intervening/conditioning mustache label causal graph since ustache causal graph expect aﬀect probability i.e. accordingly shows males females mustaches even though generator never sees label combination ustache training. bottom images sampled conditional distribution shows male images dataset figure intervening/conditioning bald label causal graph since bald causal graph expect aﬀect probability i.e. accordingly shows bald males bald females. bottom images sampled conditional distribution shows male images dataset figure intervening/conditioning mouth slightly open label causal graph since smiling outhslightlyopen causal graph expect aﬀect probability smiling i.e. however bottom conditioning mouth slightly open increases proportion smiling images although images enough show diﬀerence statistically. figure intervening/conditioning narrow eyes label causal graph since smiling narrow eyes causal graph expect aﬀect probability smiling i.e. however bottom conditioning narrow eyes increases proportion smiling images although images enough show diﬀerence statistically. rare artifact dark image third column generator appears rule possibility narrow eyes instead demonstrating narrow eyes proposed novel generative model label inputs. addition able create samples conditional labels generative model also sample interventional distributions. theoretical analysis provides provable guarantees correct sampling interventions conditionings. diﬀerence sampling mechanisms causality. interestingly causality leads generative models creative since produce samples diﬀerent training samples multiple ways. illustrated point models numerous label examples.", "year": 2017}