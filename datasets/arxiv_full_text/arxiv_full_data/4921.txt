{"title": "Sample Efficient Policy Search for Optimal Stopping Domains", "tag": ["cs.AI", "cs.LG"], "abstract": "Optimal stopping problems consider the question of deciding when to stop an observation-generating process in order to maximize a return. We examine the problem of simultaneously learning and planning in such domains, when data is collected directly from the environment. We propose GFSE, a simple and flexible model-free policy search method that reuses data for sample efficiency by leveraging problem structure. We bound the sample complexity of our approach to guarantee uniform convergence of policy value estimates, tightening existing PAC bounds to achieve logarithmic dependence on horizon length for our setting. We also examine the benefit of our method against prevalent model-based and model-free approaches on 3 domains taken from diverse fields.", "text": "optimal stopping problems consider question deciding stop observation-generating process order maximize return. examine problem simultaneously learning planning domains data collected directly environment. propose gfse simple ﬂexible model-free policy search method reuses data sample efﬁciency leveraging problem structure. bound sample complexity approach guarantee uniform convergence policy value estimates tightening existing bounds achieve logarithmic dependence horizon length setting. also examine beneﬁt method prevalent model-based model-free approaches domains taken diverse ﬁelds. introduction sequential decision making learning unknown environments commonly modeled reinforcement learning aspect artiﬁcial intelligence. important subclass optimal stopping processes agent decides step whether continue terminate stochastic process reward upon termination function observations seen far. many common problems computer science operations research modeled within setting including secretary problem house selling american options trading product pricing asset replacement well problems artiﬁcial intelligence like mission monitoring robots metareasoning value additional computation automatically deciding purchase airline ticket often stopping process dynamics unknown advance ﬁnding good stopping policy requires learning experience environment. real experience incur real losses desire algorithms quickly learn good policies achieve high reward problems. interestingly prior work optimal stopping focused planning problem compute near-optimal policies given access dynamics reward stochastic stopping process optimal stopping problems also framed partially observable markov decision process also exists work learning good policy acting pomdps bounds number samples required identify near optimal policy class policies however work either makes strong assumption algorithm access generative model stochastic process makes work suited improving efﬁciency planning using simulations domain trajectories directly collected environment incurs exponential horizon dependence. paper consider quickly learn nearoptimal policy stochastic optimal stopping process unknown dynamics given input class policies. assume ﬁxed maximum length horizon acting make simple powerful observation stopping problems process-dependent rewards outcomes full length trajectory provide estimated return halting step steps till horizon. single full-length trajectory yields sample return stopping policy. based this propose algorithm ﬁrst acts stopping full length horizon number trajectories performs policy search input policy class full length trajectories used provide estimates expected return policy considered policy class. policy highest expected performance selected future use. provide sample complexity bounds number full length trajectories sufﬁcient identify near optimal policy within input policy class. results similar general results pomdps structure optimal stopping achieve beneﬁts bounds’ dependence horizon logarithmic instead linear exponential results apply learning stochastic stopping processes generative model required. simulation results student tutoring problem formulation consider standard stochastic discrete-time optimal stopping process setting. tsitsiklis assume stochastic process generates observations actions halt continue process. reward model known deterministic function sequence observations choice whether continue halt. exist domains reward model nondeterministic function observations actions common optimal stopping problems fall within framework considered here including secretary problem house selling asset replacement etc. focus episodic setting ﬁxed maximum time horizon process. ﬁnite horizon value policy expected return following horizon steps expectation taken stochastic process dynamics note policy choose halt steps. goal maximize return across episodes. focus direct policy search methods precisely assume input parameterized policy class policy parameters. direct policy search require building model domain successful variety reinforcement learning contexts sample efﬁcient policy search particularly interested domains evaluation policy incurs real cost environment stock market options selling. settings wish sample efﬁcient methods policy search minimize number poor outcomes real world. challenge know stochastic dynamics possible advance acting perform policy search identify good policy. instead obtain information domain dynamics executing policies real world. seek efﬁciently leverage experience quickly make good decisions. present simple approach gfse sample efﬁcient policy search. gfse collects full-length trajectories uses evaluate performance policy input policy class identiﬁes good policy executes resulting policy future episodes. insight ﬁrst step gathering data used evaluate performance policy policy class monte carlo estimation used estimate expected return policy running many times. however scales poorly cardinality policy class. algorithm gather full search execute input policy class search method theorem gather full trajectories environment identify policy using \\\\evaluation uses execute building dynamics model data efﬁcient model used simulate performance policy requires make certain assumptions domain lead biased estimates. alternatively importance sampling used off-policy evaluation unfortunately estimates tend high variance. however simple powerful observation fullhorizon trajectory used yield sample return optimal stopping policies given full length trajectory performance particular policy simulated providing target policy halts time step therefore take subsequence observations directly compute return would observed executing trajectory. single full-horizon trajectory provide sample return policy. full-horizon trajectories used provide sample returns given policy thereby providing empirical estimate policy evaluation policy class prior work shown given access generative model domain policy search done efﬁcient using common random numbers evaluate policies differently episode setting full-horizon trajectory essentially equivalent access generative model produce single return policy. however access full length trajectory obtained running environment whereas generic generative models typically require teleporation ability simulate would happen next particular action given arbitrary prior history hard unless planning scenario already knowedge dynamics process. results require weaker assumptions prior results stronger generative models obtain similar sample efﬁciency also achieving better sample efﬁciency approaches access similar generative models. shortly provide sufﬁcient condition number full length trajectories guarantee evaluate policy sufﬁciently accurately enable policy search identify near-optimal policy course empirically often wish select smaller simulation experiments demonstrate often small still enables identify good policy. theoretical analysis provide bounds sample complexity gfse number full length trajectories required obtain near accurate estimates policies policy class. suffirst note optimal stopping problems consider paper viewed particular instance pomdp. brieﬂy hidden state space dynamics model determines current state transitions state stochastically given continue action. observation function hidden state reward also function hidden state action. main result that given policy class sample complexity scales logarithmically horizon. make assumption access generative model. signiﬁcant improvement prior sample complexity results policy search generic pomdps large mdps required access generative model environment sample complexity scaled linearly horizon. results thought bounding computation/simulation time required planning access generative model used sample outcome given prior history action. contrast results apply learning agent generative model domain must instead explore observe different outcomes. without generative model domain sample complexity results policy search generic pomdps learning scale exponentially horizon optimal stopping trajectories related trajectory trees kearns used evaluate returns different pomdp policies. pomdp actions trajectory tree complete binary tree rooted start state. nodes tree labeled state observation path root node tree denotes series actions taken policy. trajectory tree used evaluate policy since every action sequence part tree. however generic pomdps size trajectory tree exponential horizon optimal stopping problems tree size linear horizon allows obtain signiﬁcantly tighter dependence generic pomdps. analysis closely follows prior sample complexity results kearns proceeded ﬁrst considering bound vc-dimension viewed real-valued mappings histories returns function vc-dimension viewed mappings histories actions. result bound sample complexity needed near-accurate estimates returns policies policy class. follow similar procedure bound sample complexity contains potentially inﬁnite number deterministic policies. vc-dimension policy class. well-deﬁned since optimalstopping policy maps trajectories actions vc-dimension viewed real-valued mappings full trajectories returns assume bounded vmax. know computed {i|π otherwise return full trajectory lemma deterministic optimal-stopping policies vc-dimension viewed maps trajectories actions. then viewed maps space full trajectories dimension bounded proof. proof proceeds similarly lemma kearns al.. crucial difference policies operate full-trajectory structure contains nodes rather kearns al.’s trajectory trees nodes. setting point agent gets consider whether halt continue halt action chosen trajectory terminates. implies contrast standard expectimax trees size tree depends action space exponential horizon |a|h setting dependence induced actions linear thus produce much smaller behaviors dependence logarithmic rather polynomial. formally sauer’s lemma trajectories la)d ways first note full trabeled atmost action labeling trajectories corresponds selecting paths path starts ﬁrst observation ends terminal node. number possible selections path viewed mapping thus atmost distinct binary labelings full trajectoatmost generate atmost distinct labelings full trajectories. shatter full trajectories result follows. based ripper’s decision rules wait else corresponds halting. also constructed complex class parameters learns different price thresholds depending departure date consider nonstop ﬂights routes nyc-msp msp-nyc sea-iad training/testing separately. method gfse collects full length trajectories during ﬁrst days uses construct single stopping policy. performs simple policy search sampling evaluating policies randomly policy space. uses best identiﬁed policy simulate ticket purchasing decisions departure dates occurring remaining part years restrict data departure dates contain at-least price observations. results test sets shown table policy search method succeeds ﬁnding policy leads non-trivial improvement difﬁcult earliest purchase baseline. improvements line prior approaches speciﬁcally designed particular domain. results highlight setting capture important purchasing tasks approach even simple policy search policies signiﬁcantly better performance competitive domain-speciﬁc baselines. tutoring asset replacement consider simulated domains compare gfse several approaches learning quickly domains. unless speciﬁed results averaged rounds error bars indicate conﬁdence intervals. baselines. natural idea proceed gfse gathered data build parametric domain models used estimate performance potential policies. found shorter trajectories collected close departure date prices ﬂuctuate illustrative policy classes inadequate. cases method adopted risk-averse earliest purchase policy. probability least ˆvπ| holds simultaneously proof. space full trajectories. every policy bounded real-valued i.i.d. full trajectories generated environment dynamics. using result probability supπ∈π practice impossible evaluate every policy select best estimated mean. cases different search method local optima using bound ensure policy values estimated accurately. lastly discuss estimate values ﬁnite-horizon markov optimal stopping problems using linear combination basis functions threshold policy. outline procedure tune basis function weights asymptotically guarantees policy value’s convergence best basis-function approximation. assumptions construct policy class using basis functions inherit useful convergence results relying search procedure along retaining ﬁnite sample complexity results. experiments demonstrate setting consider sufﬁciently general capture several problems interest approach gfse improve performance optimal stopping problems state-of-the-art baselines. ticket purchase many purchasing problems posed optimal stopping process return stopping simply advertised cost. consider deciding purchase airline ticket later trip date order minimize cost. opaque prices competitive pricing makes domain difﬁcult model. prior work focused identifying features create sophisticated models make good purchase decisions. surprisingly hard improve earliest purchase baseline buys ﬁrst observation. data groves gini collected real pricing data ﬁxed routes period years querying travel sites regularly collect price information. route several departure dates distributed year period. price observation sequence length call model-based approaches. second idea consider initial collected data budget free exploration instead budget monte carlo onpolicy evaluation policies. course exploration gfse always optimal. also consider state-of-the-art approach quickly identifying global optima function function initially unknown function evaluation expensive bayesian optimization multiple papers shown used speed online policy search reinforcement learning tasks given policy class selects policy evaluate step maintains estimates expected value every policy. yelp’s gaussian kernel popular expected improvement heuristic picking policies hyper-parameters picked separate optimization maximum-likelihood estimates. simulated student learning. ﬁrst consider simulated student tutor domain. number tutoring systems mastery teaching student provided practice examples estimated mastered material. optimal stopping problem time step observing whether student activity correct tutor decide whether halt continue providing student additional practice. halting student given next problem sequence; objective maximize score ‘posttest’ giving problems possible overall. popular literature model student learning using bayesian knowledge tracing model -state hidden markov model state capturing whether student mastered skill not. within probabilities describe model. simulate student data parameters generate student trajectories using model problems. gfse consider policy classes halt probability student’s next response correct crosses threshold. thus halt threshold. fact policies kind widely used commercial tutoring systems model implement policy class parameterized contains possible instantiations parameters model-free approach search over. also consider policy class based another popular educational data mining model student learning additive factors model logistic regression model used predict probability student next problem correct given past responses. thus number correct past attempts. taking budget exploration using evaluate policy on-policy manner using monte carlo estimation. precisely sample policies policy class budget trajectories. gfse uses trajectories evaluates polik trajectories selects highest mean performance. averaging results across separate runs found gfse identiﬁes much better policy; chose poor policies mislead potential performance policy limited data. also explored performance building model domain setting model matches true domain model-mismatch case policy class based student model maximum likelihood estimation assumed model’s parameters given collected data separately optimize threshold parameters compare varying budget gfse; model-based; results averaged trials. results experiment shown figure approach well settings quickly ﬁnding near optimal policy. would expect model-based approach well matched model setting making full knowledge underlying process dynamics. however ﬁtting mismatched model model-based approach suffers. noted prior work model-ﬁtting procedures focus maximizing likelihood observed data rather trying directly identify policy expected perform well. good policy takes samples since online approach whereas gfse uses ﬁxed budget exploration also compare averaged cumulative performance variants gfse figure mimics scenario care online performance every individual trajectory rather access ﬁxed budget deploying policy. method choose collect less full trajectories ﬁnding best policy. interestingly trajectories initial budget collect full length trajectories gfse meets exceeds performance setting matched mismatched model cases within trajectories. figure average cumulative performance simulated student domain. matched model; model-mismatch. gfse-k collects initial full-length trajectories identifying executing best policy. policy online also using previously collected trajectories yielding robust estimate policy’s performance. similarly gfse deploy policy using initial budget trajectories on-policy trajectory rerun policy search identify another policy next time step figure shows improved methods approaches still performing best. asset replacement. another natural problem falls optimal stopping problem replace depreciating asset simulation model described variants model widely used ﬁeld model observations dimensional vectors form asset starts ﬁxed valuation xmax depreciates stochastically emitting observations every time step. reward function used incorporates cost replacement utility derived asset penalty asset becomes worthless replacement. experiments. construct logistic threshold policy class; replacing depr total depreasset ciation xmax seen addition approaches seen before also include baseline policies choose replace asset immediately; never replace. lastly include optimal value reference. outperforms competing methods considerable margin. appears chosen policy class tricky optimize over policies space perform poorly. random policies chosen space mean cost around conﬁdence interval however domain noisy robust value estimation requiring less trajectories enables method consistently good policy even budget corresponds replacing asset depreciation around improves slowly; either sampling policies sparse nature space disbelieving estimate good policy policies surrounding manually adjusting hyperparameters account improve performance signiﬁcantly. discussion conclusion gfse performed well outperforming state-of-the-art algorithms common baselines variety simulations important domains. randomly searched policies relatively simple policy classes illustration sophisticated search methods policy classes could employed without effecting theoretical guarantees derived. another extension using shorter trajectories terminate horizon policy evaluation useful scenario trajectories on-policy using best policy found gfse. rerun policy search trajectories collected far. policy value estimates biased case since policy halts earlier shorter trajectory evaluation. values policies halt later overestimated biasing pick them. number evaluations policy exceeds number theorem estimates would remain within true values would minimize effect bias. figure works well empirically. summarize introduced method learning optimal stopping problems reuses full length trajectories perform policy search. theoretical analysis empirical simulations demonstrate simple observation lead beneﬁts sample complexity practice. references graeme best wolfram martens robert fitch. spatiotemporal optimal stopping problem mission monitoring stationary viewpoints. robotics science systems thomas ferguson. solved secretary problem? statistical science pages michel glower donald haurin patric hendershott. selling time selling price inﬂuence seller motivation. real estate economics michael kearns yishay mansour andrew approximate planning large pomdps reusable trajectories. nips pages kenneth koedinger emma brunskill ryan baker elizabeth mclaughlin john stamper. potentials data-driven intelligent tutoring system development optimization. magazine travis mandel yun-en sergey levine emma brunskill zoran popovic. ofﬂine policy evaluation across representations applications educational games. aamas pages international foundation autonomous agents multiagent systems steven ritter thomas harris tristan nixon daniel dickison charles murray brendon towle. reducing knowledge tracing space. educational data mining pages john tsitsiklis benjamin roy. optimal stopping markov processes hilbert space theory approximation algorithms application pricing high-dimensional ﬁnancial derivatives. ieee transactions automatic control vladimir naumovich vapnik samuel kotz. estimation dependences based empirical data volume springer-verlag york aaron wilson alan fern prasad tadepalli. using trajectory data improve bayesian optimization reinforcement learning. journal machine learning research", "year": 2017}