{"title": "Half-CNN: A General Framework for Whole-Image Regression", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures.", "text": "figure example applications regression model. rows demonstrate model simultaneous face detection segmentation bottom rows demonstrate model scene saliency prediction. columns correspond input images ground truth feature maps network output results interpretation. softmax classifier training stage. training softmax classifier often replaced classifier improved representations classification performance. sometimes directly used network training process. advance lies ability learn adaptive features opposed hand-crafted features like sift bovw models. handcrafted features intuitive human perception; also considered shallow network. former convolution blocks perform similar tasks feature detectors; however learned features optimized given task. similar statements hold encoding process. linear classifier output also shallow network. thus classification pipeline treated shallow network consisting hand-crafted function blocks whereas deep convolutional neural network achieved great success image classification. classification model also utilized image patch level many applications object detection segmentation. paper propose whole-image regression model removing full connection layer training network continuous feature maps. generic regression framework fits many applications. demonstrate method tasks simultaneous face detection segmentation scene saliency prediction. result comparable models respective fields using small scale network. since regression model trained corresponding image feature pairs requirements uniform input size opposed classification model. framework avoids classifier design process introduce much manual intervention model development. highly correlated classification network offers in-deep review structures. image classification techniques evolved vastly recent years bag-of-visual-words fisher vector encoding state-of-the-art convolutional neural network bovw models typically consist pipeline cascading processes feature description dictionary building feature encoding pooling image classification. linear classifier used final stage whole process considered nonlinear classification model images. techniques extensively studied classification model consists several convolution blocks followed fully connected layers. convolution block consists convolution layer non-linear activation pooling layer sometimes local normalization layer fully connected layer considered final pooling layer feeds image feature classifier. model typically uses face detection important task computer vision. various models appearance learning based models classical models extensively studied models recently developed field paper uses convolution generate feature maps classifier detect face windows. approach similar part-based model object detection face detection algorithms implemented segmentation idea close framework. however network need include specific knowledge facial features e.g. skin color detection segmentation task. features fully learned without manual crafting network. face detection also combined simultaneous face landmark localization part work based model applied detect face landmarks. three-level cascaded network used predict face landmark coarse-to-fine localization process. network also uses regression model different ours. approach network outputs several locations fixed-size real numbers considered mapping real domain real domain i.e. mapping. classification model maps images fixed finite countable considered mapping. fully connected layer needed whenever dimension reduction input output. comparison regression model mapping images feature maps full connection avoided. work considered bridge classification model model. also test face landmark localization framework. however task heavily depends knowledge spatial relationship face landmark points pure texture approach like give independent predictions landmarks ideal. localization model also utilizes spatial relationship landmarks implicitly global-to-local refinement. shall briefly discuss point section aliency prediction another important study field serving bridge computer vision human perception. saliency prediction models evolved itti gbvs detection segmentation models recent cnn-based models extensive study classical models found comparison models found network consisting optimized blocks. explains great success classification tasks. classification model achieves state-of-the-art performance classification datasets imagenet ilsvrc pascal challenges comprehensive study found pre-trained imagenet model also successful generalizing image classification datasets. also widely applied non-classification tasks object detection segmentation model either applied image level patch information aggregation. imagenet pre-trained model also widely used applications. however seems widely used regression tasks also fundamental problem machine learning. regression classification problems highly correlated transferred many scenarios. example softmax classifier related softmax regression classifier based geometric regression. natural develop deep model regression tasks counterpart classification. paper propose simple generic regression model. removing fully connected layer classification network yield network output locally correlated network input also characteristic local regression models. final output feature generated linear combination convolution channels considered partially connected layer. thus local regression network classification network. also propose up-sampling layer reduce downsampling effects pooling operations. relationship classification regression networks discussed detail section ground truth train regression network feature maps generated input images. size maps related input images either identical pre-defined down-sampling factor. scenarios correspond convolution pooling operations neural network. since fully connected layer model pose requirement identical input size problem classification model. apply framework applications simultaneous face detection segmentation saliency prediction. former task ground truth feature maps generated fitting gaussian density function inside detection window. trained network recover position faces also output segmented face regions feature map. saliency prediction natural application network since ground truth already given real-value figure schematic regression model. up-sampling layer used maintain feature size output produced linear combination convolution channels. layers specific framework. function computes feature response location normalizing responses location neighboring channels. practice normalization layer offer limited performance gain used first convolution blocks experiments. limit amount pooling layers used network size limitation ground truth feature maps. example feature times smaller original image max-pooling layers down-sampling factor several ways around this either pooling stride convolution without pooling following blocks. however methods reduce non-linearity network desired property. paper propose up-sampling layer maintain size intermediate features network. up-sampling layer follows pooling layer restores size pooled features retaining non-linearity network. up-sampling function defined follows down-sampling factor used pooling layer typically value function copies value pooled features block following up-sampling layer. back-propagation rule layer average-pooling layer reverse direction scale since block upsampling layer consists value backpropagation principal layer simplified output layer. output layer combines feature channels last convolution block linear combination. sigmoid activation function applied produce final output. layer forward back propagation rules shown below types saliency models bottom-up up-to-down saliency model. former typically focused human perception process level image information whereas latter related scene knowledge interest objects. latter also highly related object detection segmentation; models explicitly detection segmentation techniques give saliency predictions recent models also offer competitive performance field. uses large number randomly initialized cnns picking good performance aggregates output feature maps give predictions. latter deep gaze uses imagenet pre-trained network without full connection layer learns weight linear combination convolution channels. model offers state-of-the-art performance. model structure similar deep gaze model trained different without using classification information. since saliency information provided feature maps natural application fits framework. regression model train saliency model give comparative studies section also detailed discussion section network consists several convolution blocks output layer. convolution blocks follow structure output layer combines feature channels linear combination. onvolution blocks. convolution block includes convolution layer relu activation pooling layer down-sampling factor max-pooling used pooling function. normalization layer introduced also used pooling layer local normalization function below also possible type combinations max-pooling function among convolution channels. however function seems underperform linear combination experiments. size ground truth feature typically related input image. fully connected layers regression framework bypass uniform input size requirement typical classification cnns. input sizes different classification model images warped cropped size. different image aspect ratio also problem. however regression model fundamentally avoid headaches. convolution operation depend image size; however layer input needs properly padded according filter size ensure correct downsampling factor pooling. input output size convolution blocks differ factor pooling without up-sampling used. up-sampling layer present input output sizes convolution blocks identical. theory network accept input images different sizes. batch gradient calculated averaging individual gradients produced single images batch. however fast computation recommended images within batch identical size utilize advanced data structure typical implementations. input sizes differ vastly another convenient bypass uniform-size requirement. input images batch padded size mask recording content input. ground truth feature maps padded accordance corresponding images. mask applied derivative output feature ensure correct gradient computation. technique cannot applied classification network since fully connected layer requires proper arrangement features. applying masks interrupt pre-defined feature sequences. padding-masking scheme also applied filters convolution layer includes filters different sizes. however seems relatively rare setup applications. framework applications. simultaneous face detection segmentation face dataset saliency prediction dataset networks trained matconvnet platform matlab toolbox based famous caffe implementation. face images dataset different viewing angles. images identical size images training rest testing. detection window provided used ground truth. figure examples regression model simultaneous face detection segmentation experiment. columns correspond input ground truth output features results interpretation. network able detect segment multiple faces. simple technique generate ground truth feature maps gaussian density function center detection window diameter window width height. size input images padded stated section down-sampling applied generated feature maps. convolution blocks experiment. first blocks consist filters size respectively max-pooling normalization. following block consists filters size upsampling layer max-pooling. numerical stability ground truth feature maps re-normalized sigmoid activation output layer. also small penalty filter weights biases layer regularization. penalty terms prevent network over-fitting improves generalization ability. sample outputs shown figure output feature overlayed input images visualization. seen output feature maps correspond segmentation faces input. neck area sometimes included feature map; natural color similarity face neck areas. increase network complexity might relieve problem. also tested regression model face landmark localization experiments based work ground truth feature maps generated fitting small gaussian density functions face landmark positions. however network perform well application textural approach without incorporating spatial relationship landmarks. relationship crucial application detecting landmarks independently framework ideal. sample detection results shown figure landmark localization network able detect structural regions like eyes fails less structural regions noses. expected color similarity nose regions. structural contents eyes most moderate mouths; noses offer least information textures. saliency dataset consists indoor outdoor scene images. longest dimension image pixels edge ranges pixels. aspect ratio typically around ground truth saliency prediction already provided feature maps thus application naturally fits regression framework. images down-sampling factor feature maps. network regularization parameters former face detection segmentation experiments. detection window retrieved analyzing feature maps center standard deviation response window reverse process generating gaussian density functions. retrieval rate detection window experiments. though train network simple gaussian density function detection window network surprisingly provides ability detection segmentation small scale network. moreover network detect segment multiple faces shown figure indicates generalization regression model also applied non-image applications. convolution operation local shares similar structure local regression models famous non-linear kernel regression. thus many regression works reviewed regression model. limitations. limitation framework network requires large training perform well; models. example deep gaze uses imagenet pre-trained models saliency prediction achieves best performance. amounts using large training application. nevertheless getting ground truth feature maps easy getting classification labels aggravates difficulty generating training sets. prevent over-fitting. over-fitting common problem regression. adding regularization terms model parameters prevent over-fitting achieve better generalization ability testing sets. regression model simple penalty network applied regularization terms. types penalties norm offer desired properties like sparsity however network penalties complex optimize. adding regularization terms alleviates over-fitting reducing effective degrees freedom network example large network over-fit simple tasks like face detection segmentation problem; adding penalties reduce effective parameters make network behave like small-scale network. determining proper network scale different applications difficult often based test-and-trial. drop-out technique famous prevent overfitting neural networks; proved adaptive regularization networks alleviates testexperiment results shown figure network output highly correlated ground truth saliency map. quantitative evaluation comparison methods shown figure model outperforms classical saliency prediction methods incorporating specific knowledge saliency studies. compare model latest work difference database evaluation methods. omputation time. since regression network small scale l-bfgs algorithm used fast convergence. network trained hours hours face detection segmentation saliency prediction nvidia graphics card. generality. regression model general framework applied variety applications output feature correlated input images. example simple face detection segmentation generate feature maps complicated segmentation tasks pascal challenge neuron membrane segmentation medical images segmented object ground truth. part future study advanced detection segmentation applications. sivic zisserman. video google text retrieval approach object matching videos. computer vision proceedings. ninth ieee international conference", "year": 2014}