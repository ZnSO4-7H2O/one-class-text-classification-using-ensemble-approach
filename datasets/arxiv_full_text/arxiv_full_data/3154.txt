{"title": "CURL: Co-trained Unsupervised Representation Learning for Image  Classification", "tag": ["cs.LG", "cs.CV", "stat.ML", "I.2.6"], "abstract": "In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from Co-trained Unsupervised Representation Learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features. To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art.", "text": "abstract—in paper propose strategy semisupervised image classiﬁcation leverages unsupervised representation learning co-training. strategy called curl co-trained unsupervised representation learning iteratively builds classiﬁers different views data. views correspond different representations learned labeled unlabeled data differ fusion scheme used combine image features. assess performance proposal conducted several experiments widely used data sets scene object recognition. considered three scenarios differ strategy followed exploit unlabeled data. image features considered combination gist phog well features extracted convolutional neural network. moreover embodiments curl investigated using ensemble projection unsupervised representation learning coupled logistic regression based lapsvm. results show curl clearly outperforms supervised semisupervised learning methods state art. semi-supervised learning consists taking account labeled unlabeled data training machine learning models. particularly effective plenty training data instances labeled. last years many semi-supervised learning approaches proposed including generative methods graph-based methods methods based support vector machines co-training another example semi-supervised technique consists training classiﬁers independently which basis level conﬁdence unlabeled data co-train trough identiﬁcation good additional training examples. difference classiﬁers work different views training data often corresponding feature vectors. pioneering works co-training identiﬁed conditional independence views main reason success. recently observed conditional independence sufﬁcient necessary condition even single view considered provided different classiﬁcation techniques used work propose semi-supervised image classiﬁcation strategy exploits unlabeled data different ways ﬁrst image representations obtained unsupervised representation learning image features computed available training data; co-training used enlarge labeled training corresponding co-trained classiﬁers difference image representations built combination image features combination sub-representations separately built feature call proposed strategy curl co-trained unsupervised representation learning schema curl illustrated fig. standard co-training classiﬁer built single view often corresponding single feature. however combination multiple features often required recognize complex visual concepts classiﬁers built curl exploit available image features concepts accurately recognized. argue different fusion schemes together non-linear transformation produced unsupervised learning procedure makes image representations uncorrelated enough allow effective co-training classiﬁers. proposed strategy built base components changing components different embodiments curl experimented evaluated. assess merits proposal conducted several experiments widely used data sets conﬁdent making prediction unlabeled data predicted labels used augment training classiﬁer. concept generalized three views co-training used several computer vision applications including video annotation action recognition trafﬁc analysis speech gesture recognition image annotation biometric recognition image retrieval image classiﬁcation object detection object tracking according blum mitchell sufﬁcient condition effectiveness co-training that beside individually accurate classiﬁers conditionally independent given class label. however conditional independence necessary condition. fact whang zhou showed co-training effective diversity classiﬁers larger errors; results provided theoretical support success single-view co-training variants details necessary sufﬁcient conditions co-training). last years consequence success deep learning frameworks observed increased interest methods make unlabeled data automatically learn representations. fact demonstrated effective pre-training large neural networks restricted boltzmann machines auto-encoder networks notable examples kind methods. tutorial bengio covers detail family approaches conceptually simpler approach consists using clustering algorithms identify frequently occurring patterns unlabeled data used deﬁne effective representations. k-means algorithm widely used purpose computer vision approach popular lead many variants bag-of-visual-words representations brieﬂy clustering unlabeled data used build vocabulary visual words. given image multiple local features extracted similar visual word searched. ﬁnal representation histogram counting occurrences visual words. sparse coding seen extension approach local feature described sparse combination multiple words vocabulary scene data caltech- object classiﬁcation data ilsvcr data contains different classes. considered variety scenarios including transductive learning inductive learning self-taught learning order verify efﬁcacy curl classiﬁcation strategy also tested embodiments uses ensemble projection unsupervised representation coupled logistic regression classiﬁcation based lapsvm semi-supervised classiﬁcation. moreover different variants embodiments evaluated well. results show curl clearly outperforms semi-supervised learning methods state art. large literature semi-supervised learning. sake brevity discuss paradigms involved proposed strategy. information approaches semi-supervised learning found book chapelle blum mitchell proposed co-training veriﬁed effectiveness classiﬁcation pages. basic idea classiﬁers trained separate views used train other. precisely classiﬁers available data ensemble projection samples prototypes. discriminative learning used learn projection functions tuned prototypes. since single projections could noisy multiple sets prototypes sampled build ensemble projection functions. values computed according functions represent components learned representations. lapsvm seen unsupervised representation learning method well. case learned representation explicit implicitly embedded kernel learned unlabeled data. combining multimodal information important issue pattern recognition. fusion multimodal inputs bring complementary information various sources useful improving quality image retrieval classiﬁcation performance problem arises deﬁning modalities combined fused. general existing fusion approaches categorized early late fusion approaches refers relative position feature comparison learning step whole processing chain. early fusion usually refers combination features single representation comparison/learning. late fusion refers combination last stage responses obtained individual features comparison learning universal conclusion strategy preferred method given task. example snoek found late fusion better early fusion trecvid semantic indexing task ayache stated early fusion gets better results late fusion trecvid semantic indexing task. combination approaches also exploited hybrid fusion approach another form data fusion multiple kernel learning introduced lanckriet extension support vector machines instead using single kernel computed image representation standard svms learns distinct kernels. kernels combined linear linear function function’s parameters determined learning process. used learn different kernels image representation learning different kernels different image representation former corresponds different notion similarity choose suitable problem representation hand. latter corresponds multiple representations possibly different deﬁnition similarity must combined together. kind data fusion termed intermediate fusion. semi-supervised image classiﬁcation setup training data consists labeled examples {xly} unlabeled ones {xi}l+u i=l+ denotes feature vector image label number classes. work image different image features considered. views generated using different fusion strategies early late fusion. case early fusion image features concatenated used learn representation unsupervised projection function. case late fusion unsupervised representation independently learned image feature representations concatenated obtain using learned unsupervised representations views built }l+u {xef i=l+. furthermore label sets initialized equal views generated method iteratively co-trains classiﬁers svms logistic regressions similar technique used obtain them. idea iterative co-training small labeled sample train initial classiﬁers respective views iteratively bootstrap taking unlabeled examples classiﬁers conﬁdent not. conﬁdent classiﬁer determines pseudo-labels used true labels improve classiﬁer given classiﬁer conﬁdence scores respectively obtained unlabeled examples pseudo-labeled must belong unlabeled i.e. already used training i.e. pseudo-label ˆylf furthermore conﬁdent classiﬁcation conﬁdence higher ﬁxed threshold selected corresponding pseudo-label added respectively. satisfying found nothing added similarly classiﬁer chooses examples pseudo-label next co-training round classiﬁers trained respective views contain labeled pseudo-labeled examples. complete procedure curl method outlined algorithms curl parametric respect projection function used unsupervised representation learning supervised classiﬁcation technique used co-train ﬁrst embodiment curl used ensemble projection former logistic regression latter. another embodiment based lapsvm presented section v-c. evaluated method data sets scene caltech- scene data contains images divided scene categories indoor outdoor environments. category images. caltech- contains images divided object categories images. furthermore collected random images sampling images imagenet data evaluate method task self-taught image classiﬁcation. since current version imagenet synsets total millions images small probability random images images considered data sets come distribution. experiments used following three features gist pyramid histogram oriented gradients local binary patterns gist computed rescaled images pixels scales orientations respectively. phog computed -layer pyramid directions. uniform radius equal neighbors used. differently others semi-supervised methods train classiﬁer labeled data regularization term learned unlabeled data ensemble projection learns image representation known data trains plain classiﬁer ensemble projection learns knowledge difi= ferent prototype sets feature vector obtained concatenating different features available case late fusion feature vector made considering single feature time number prototypes used order assure unsupervised representations size. conducted kinds experiments comparison strategy competing methods semi-supervised image classiﬁcation; evaluation method different number co-training rounds. considered three scenarios corresponding three different ways using unlabeled data. inductive learning scenario unlabeled data used together labeled data semi-supervised training classiﬁer; remaining used independent test set. transductive learning scenario unlabeled data used training test. self-taught learning scenario unlabeled data taken additional data featuring different distribution image content unlabeled data original data used independent test set. evaluation measure followed used multi-class average precision computed average precision recall values classes. different numbers training images class tested scene- caltech- reported results represent average performance runs random labeledunlabeled splits. compared supervised semisupervised baseline methods. supervised classiﬁers considered support vector machines semi-supervised classiﬁers used lapsvm lapsvm extend framework including smoothness penalty term deﬁned laplacian adjacency graph built labeled unlabeled data. lapsvm experimented linear kernels computed concatenation three available image features parameters lapsvm determined greedy search three-fold cross validation training set. also compared present embodiment curl ensemble projection coupled logistic regression classiﬁer ﬁrst experiment compared curl ep+lr svms lapsvms different kernels. speciﬁcally tested co-trained classiﬁers operating early-fused late-fused representations employing classiﬁer call curl-ef curl-lf respectively. also included variant proposed method. differs number pseudo-labeled examples added co-training round. variant skips non-maximum suppression step round adds examples satisfying denote co-trained classiﬁers variant curl-efn curl-lfn. fig. shows classiﬁcation performance different numbers labeled training images class three learning scenarios scene- caltech- data sets. curl-based methods considered co-training rounds reported performance correspond last round. lapsvm results using kernel reported since consistently showed best performance across experiments. detailed results tested baseline methods curl variants across co-training rounds available tables iii. behavior methods quite stable three learning scenarios slightly respect lower obtained case self-taught learning. evident strategy outperformed methods state included comparison across data sets scenarios considered. among variants considered curl-lf demonstrated best case small number labeled images curl-lfn obtained best results labeled data available. classiﬁers obtained early-fused representations performed generally worse corresponding ones obtained late-fused representations still uniformly better original ep+lr ensemble projection considered non-cotrained version. svms lapsvms performed poorly scene- data outperformed ep+lr curl variants caltech- data set. method svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr method svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr svmlin svmrbf svmχ lapsvmlin lapsvmrbf lapsvmχ ep+lr co-training allows make good early fusion representations otherwise lead worse results late fusion representations. opinion happens views capture different relationships among data. fact visible fig. shows projections obtained applying t-sne method gist phog features concatenation learnt earlylate-fused representations. unsupervised representation learning allows t-sne identify groups images class. moreover representations based early late fusion induce different relationships among classes. instance second fig. blue light green classes placed close bottom right; instead classes well separated. difference fig. mean average precision varying number labeled images class obtained scene- data caltech- data three scenarios considered inductive learning transductive learning self-taught learning note inductive learning caltech- data limited labeled images class otherwise classes wouldn’t enough unlabeled data left training evaluation. curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn analyze details performance strategy across co-training rounds. results reported fig. lines increasing color saturation corresponding rounds ﬁve. curl-lf reported lines curllfn blue. results reported terms improvements respect ep+lr which recall corresponds curl-ef zero cotraining rounds. curl-lf performances always increase number rounds. curllfn true scene- data small number labeled examples. curllfn round co-training adds promising unlabeled samples high chance including wrong pseudo-label. result ‘concept drift’ classiﬁers pulled away concepts represented labeled examples. risk lower caltech- labeled images. original curl-lf conservative since co-training rounds adds single image plots conﬁrm curl-lf better suited small sets labeled images curllfn preferred labeled examples available. representation learned late fused features explains part effectiveness curl. fact even curl-lf without cotraining outperforms baseline represented ensemble projection. proposed classiﬁcation strategy works powerful features used. recent results indicate generic descriptors extracted pre-trained convolutional neural networks able obtain consistently superior results compared highly tuned state systems visual classiﬁcation tasks various datasets extract mean average precision curl variants embodiment varying number labeled images class different co-training rounds obtained caltech- data three learning scenarios considered inductive transductive self-taught clarity curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn curl-ef curl-lf curl-ef&lf curl-efn curl-lfn curl-ef&lfn dimensional feature vector image using caffe implementation deep described krizhevsky discriminatively trained large dataset imagelevel annotations classify images different classes. brieﬂy mean-subtracted image forward propagated convolutional layers fully connected layers. features obtained extracting activation values last hidden layer. details network architecture found leverage features curl using fourth feature addition three used section discriminative power features alone seen fig. projections obtained applying t-sne method reported. experimental results using four features reported fig. scene- caltech data sets. report results transductive scenario only. seen results using four features signiﬁcantly better using three features mainly discriminative power features. furthermore curl section want evaluate curl performance different embodiment. speciﬁcally substitute components lapsvmbased ones. lapsvm ﬁrst unsupervised geometrical deformation feature kernel performed. deformed kernel used classiﬁcation standard thus by-passing explicit deﬁnition feature representation. curl embodiment exploit unsupervised step surrogate component component. view obtained concatenating gist phog features generating corresponding kernel obtained linear combination four kernels computed feature. similar done multiple kernel learning performance previous experiments kernel used views. experimental fig. performance obtained curl-lf curl-lfn varying number co-training rounds. performance reported terms improvement respect ensemble projection. small cardinality classes inductive learning caltech- limited labeled images class. results scene- caltech- data sets transductive scenario reported fig. named variants curl embodiment adding sufﬁx seen behavior different methods previous plots lapsvm-based curl outperforming standard lapsvm. plots conﬁrm curl-lf fig. qualitative results ‘panda’ class caltech- data reported results relative case single instance available training single example added co-training round curl-lf respectively). left part fig. contains training examples added curl-ef curl-lf co-training round right part fig. contain ﬁrst test images ordered decreasing classiﬁcation conﬁdence. samples belonging current class surrounded green bounding used samples belonging classes. fig. mean average precision varying number labeled images class obtained scene- data caltech- data results obtained using gist phog features. fig. mean average precision varying number labeled images class obtained scene- data caltech- data results obtained using gist phog features. sets test images possible positive images recovered. moreover images belonging correct class tends classiﬁed increasing conﬁdence move left conﬁdences images belonging classes decrease pushed right. experiment want test proposed classiﬁcation strategy large scale data namely ilsvrc contains total different classes. experiment ilsvrc validation since training used learn features. ilsvrc validation contains total images class randomly divided training test containing images class. again different numbers training images class tested second embodiment curl used experiment. given large range values plot improvements respect lapsvm baseline also reported. seen behavior similar previous plots lapsvm-based curl variants outperforming lapsvm. previous data sets plots show curl-ef curl-lf better suited small sets labeled images curl-efnand curllfn preferred labeled examples available. remarkable proposed classiﬁcation strategy able improve results lapsvm since features speciﬁcally work proposed curl semisupervised image classiﬁcation strategy exploits unlabeled data different ways ﬁrst image representations obtained unsupervised learning; co-training used enlarge labeled training corresponding classiﬁers. image representations built using different fusion schemes early fusion late fusion. proposed strategy tested scene caltech- ilsvrc data sets compared supervised semi-supervised methods three different experimental scenarios inductive learning transductive learning self-taught learning. tested embodiments curl several variants differing co-trained classiﬁer used number pseudo-labeled examples added co-training round. experimental results showed curl embodiments outperformed methods state included comparisons. particular variants single pseudo-labeled example class co-training round resulted perform best case small number labeled images variants adding examples round obtained best results labeled data available. moreover results curl using combination low/mid high level features outperform obtained features state methods. means curl able effectively leverage less discriminative features boost performance discriminative ones belkin niyogi sindhwani manifold regularization geometric framework learning labeled unlabeled examples journal machine learning research vol. iyengar nock discriminative model fusion semantic concept detection annotation video proceedings eleventh international conference multimedia natarajan vitaladevuni zhuang tsakalidis park prasad multimodal feature fusion robust event detection videos computer vision pattern recognition ieee conference z.-h. zhou improve computer-aided diagnosis machine learning techniques using undiagnosed samples systems cybernetics part systems humans ieee transactions vol. guillaumin verbeek schmid multimodal semisupervised learning image classiﬁcation ieee conf. computer vision pattern recognition javed shah online detection classiﬁcation moving objects using progressively improving detectors computer vision pattern recognition fig. mean average precision varying number labeled images class obtained ilsvrc data values improvements lapsvm baseline results obtained using gist phog features. wang z.-h. zhou co-training insufﬁcient views asian conference machine learning wang zhou analysis co-training proc. int’l conf machine learning hinton osindero fast learning algorithm deep belief nets neural computation vol. bengio learning deep architectures foundations trends machine learning vol. coates learning feature representations k-means neural networks tricks trade lazebnik schmid ponce beyond bags features spatial pyramid matching recognizing natural scene categories ieee conf. computer vision pattern recognition vol. d.-h. pseudo-label simple efﬁcient semisupervised learning method deep neural networks workshop challenges representation learning icml m.-f. balcan blum yang co-training expansion towards bridging theory practice advances neural information processing systems fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding vol. fig. qualitative results proposed strategy ‘panda’ class caltech- data co-training rounds. train images left ﬁrst test images ordered decreasing classiﬁcation conﬁdence right. test images reported fig. oliva torralba modeling shape scene holistic representation spatial envelope international journal computer vision vol. bosch zisserman muoz image classiﬁcation using random forests ferns computer vision iccv ieee international conference ojala pietikainen maenpaa multiresolution gray-scale rotation invariant texture classiﬁcation local binary patterns pattern analysis machine intelligence ieee transactions vol. razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition computer vision pattern recognition workshops ieee conference ieee shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proceedings international conference multimedia. fig. qualitative results proposed strategy ‘panda’ class caltech- data co-training rounds. images ordered decreasing classiﬁcation conﬁdence. training image test images reported fig.", "year": 2015}