{"title": "On landmark selection and sampling in high-dimensional data analysis", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "In recent years, the spectral analysis of appropriately defined kernel matrices has emerged as a principled way to extract the low-dimensional structure often prevalent in high-dimensional data. Here we provide an introduction to spectral methods for linear and nonlinear dimension reduction, emphasizing ways to overcome the computational limitations currently faced by practitioners with massive datasets. In particular, a data subsampling or landmark selection process is often employed to construct a kernel based on partial information, followed by an approximate spectral analysis termed the Nystrom extension. We provide a quantitative framework to analyse this procedure, and use it to demonstrate algorithmic performance bounds on a range of practical approaches designed to optimize the landmark selection process. We compare the practical implications of these bounds by way of real-world examples drawn from the field of computer vision, whereby low-dimensional manifold structure is shown to emerge from high-dimensional video data streams.", "text": "recent years spectral analysis appropriately deﬁned kernel matrices emerged principled extract low-dimensional structure often prevalent high-dimensional data. provide introduction spectral methods linear nonlinear dimension reduction emphasizing ways overcome computational limitations currently faced practitioners massive datasets. particular data subsampling landmark selection process often employed construct kernel based partial information followed approximate spectral analysis termed nystr¨om extension. provide quantitative framework analyse procedure demonstrate algorithmic performance bounds range practical approaches designed optimize landmark selection process. compare practical implications bounds realworld examples drawn ﬁeld computer vision whereby low-dimensional manifold structure shown emerge high-dimensional video data streams. recent years dramatic increases available computational power data storage capabilities spurred renewed interest dimension reduction methods. trend illustrated development past decade several algorithms designed treat nonlinear structure data isomap spectral clustering laplacian eigenmaps hessian eigenmaps diﬀusion maps despite diﬀerent origins algorithms requires computation principal eigenvectors eigenvalues positive semi-deﬁnite kernel matrix. fact spectral methods brethren long held central place statistical data analysis. spectral decomposition positive semi-deﬁnite kernel matrix underlies variety classical approaches principal components analysis low-dimensional subspace explains variance data sought fisher discriminant analysis aims determine separating hyperplane data classiﬁcation multidimensional scaling used realize metric embeddings data. result reliance exact eigendecomposition appropriate kernel matrix computational complexity methods scales turn cube either dataset dimensionality cardinality accordingly write requisite complexity exact eigendecomposition large and/or high-dimensional datasets pose severe computational problems classical modern methods alike. alternative construct kernel based partial information; analyse directly ‘landmark’ dimensions examples selected dataset kind summary statistic. landmark selection thus reduces overall computational burden enabling practitioners apply aforementioned algorithms directly subset original data—one consisting solely chosen landmarks—and subsequently extrapolate results computational cost practitioners often select landmarks simply sampling data uniformly random show article improve upon approach data-adaptive manner slightly higher computational cost. begin review linear nonlinear dimension-reduction methods formally introduce optimal landmark selection problem provide analysis framework landmark selection turn yields clear trade-oﬀs computational complexity quality approximation. finally conclude case study demonstrating applications ﬁeld computer vision. dimension reduction important part statistical landscape since inception ﬁeld. indeed though principal components analysis introduced century still enjoys wide among practitioners canonical method data analysis. recent years however lessening costs computation data storage begun alter research landscape area dimension reduction massive datasets gone rare cases everyday burdens nonlinear relationships amongst entries becoming ever common. faced landscape computational considerations become necessary part statisticians’ thinking approaches methods required treat unique problems posed modern datasets. start introducing notation explaining principal issues simple illustrative example. assume given collection data samples denoted sample comprising measurements. example samples could contain hourly measurements temperature humidity level wind speed particular location period day; case would contain three-dimensional vectors. objective principal components analysis reduce dimension given dataset exploiting linear correlations amongst entries. intuitively hard imagine that temperature increases wind speed might decrease—and thus retaining humidity levels linear combination temperature wind speed would small error informative knowing three values exactly. example consider gathering centred measurements matrix measurement column; example above dimension method principal components consists analysing positive semi-deﬁnite kernel outer products samples eigendecomposition orthogonal matrix whose columns comprise eigenvectors diagonal matrix containing real nonnegative eigenvalues. eigenvectors associated largest eigenvalues yield variables according turn provide directions greatest variability data example successful relationship wind speed temperature linear. nonlinear dimension reduction refers case relationships variables linear whereupon method principal components fail explain adequately nonlinear covariability present measurements. example dataset type shown ﬁgure consisting points sampled two-dimensional disc stretched three-dimensional shape taking form ﬁshbowl. vein however contemporary methods nonlinear dimension reduction based analysis appropriately deﬁned positive semi-deﬁnite kernel. limit describing closely related methods serve illustrate case point diﬀusion maps laplacian eigenmaps given input data cardinality dimension along parameters positive integer diﬀusion maps algorithm involves ﬁrst forming positive semi-deﬁnite kernel whose entry given figure nonlinear dimension reduction contrasting embeddings data panel shown. two-dimensional linear embedding shown panel yields overlap points diﬀerent colour indicating failure recover nonlinear structure data. panels show respectively embeddings obtained diﬀusion maps laplacian eigenmaps; methods successfully recovers nonlinear structure original dataset correctly ‘unfolding’ dimensions. standard euclidean norm deﬁne diagonal matrix markov transition matrix computed. transition matrix describes evolution discrete-time diﬀusion process points transition probabilities given multiplication serving normalize them. well known corresponding transition matrix time steps simply given m-fold product itself; write uλmu− principal eigenvectors eigenvalues transition matrix used embed data according uλm. however note that since stochastic matrix principal eigenvector corresponding eigenvalue equal unity. eigenvector-eigenvalue pair hence ignored purposes embedding depend rather necessarily computing dense kernel case diﬀusion maps laplacian eigenmaps algorithm commences computation k-neighbourhood data point i.e. nearest data points found. weighted graph whose vertices data points computed edge present vertices among closest points vice-versa. weight kernel entry given e−xi−xj/σ edge present corresponding graph otherwise thus immediately arrive sparsiﬁed version diﬀusion maps kernel. consider so-called combinatorial laplacian graph deﬁned positive semi-deﬁnite kernel simple calculation shows constrained minimization reformulated whose solution turn consist eigenvectors smallest eigenvalues—from exclude case diﬀusion maps solution proportional argument employed above analysis easily related normalized laplacian d−/ld−/. recall earlier assumption collection data samples denoted sample comprising measurements. important point analyses that case size kernel dictated either number data samples dimension indeed classical modern spectral methods rely either following inner characteristics point cloud. multidimensional scaling recent extensions perform nonlinear embeddings data points require spectral analysis kernel dimension cardinality point cloud. sets scenarios analysis large kernels quickly induces computational burden impossible overcome exact spectral methods thereby motivating introduction landmark selection sampling methods. since introduction furthermore datasets continue increase size dimension so-called landmark methods seen wide practitioners across various ﬁelds. methods exploit high level redundancy often present high-dimensional datasets seeking small number important examples coordinates summarize relevant information data; amounts eﬀect adaptive compression scheme. separate subset selection problem actual solution corresponding spectral analysis task—and turn accomplished so-called nystr¨om extension nystr¨om reconstruction admits unique property providing conditioned upon selected landmarks minimal kernel completion respect partial ordering positive semi-deﬁniteness literature currently open question optimal landmark selection. choosing appropriate landmarks speciﬁc dataset fundamental task spectral methods successfully ‘scale order large datasets already seen contemporary applications expected grow future. improvements turn translate directly either eﬃcient compression input accurate approximation given compression size. choosing landmarks data-adaptive clearly oﬀer improvement approaches selecting uniformly random latter approach remains popular practitioners clear data-dependent landmark selection methods oﬀer potential least improvement non-adaptive methods uniform sampling bounds performance function computation rigorously addressed literature date. important reason lack unifying framework understand problems landmark selection sampling provide approximation bounds quantitative performance guarantees. section describe analysis framework landmark selection places previous approaches context show leads quantitative performance bounds nystr¨om kernel approximation. noted earlier spectral methods rely low-rank approximations appropriately deﬁned positive semi-deﬁnite kernels. real symmetric kernel matrix dimension write denote positive semideﬁnite. kernel turn expressed spectral coordinates orthogonal matrix diag contains real nonnegative eigenvalues assumed sorted non-increasing order. measure error approximating kernel require following unitarily invariant norm therefore depends singular values argument norm optimal rank-k approximation given uλku diag. given kernel expressed spectral coordinates evaluating quality low-rank described however cost obtaining spectral coordinates exactly often costly computed practice. methods rely either extrinsic dimension point cloud intrinsic dimension training examples cardinality impose large computational burden. illustrate comprise data interest. ‘outer’ methods former category employ rank-k dimension alternatively ‘inner’ methods introduce additional positive-deﬁnite function expxi xj/σ) obtain k-dimensional embedding data n-dimensional aﬃnity matrix nystr¨om method found many applications modern machine learning data analysis applications means obtaining approximate spectral analysis kernel interest brief method solves matrix completion problem preserves positive semi-deﬁniteness follows. deﬁnition subset cardinality denote corresponding principal submatrix n-dimensional kernel take without loss generality partition follows remains open question whether unitarily invariant norm subset selection problem solved fewer operations threshold exact spectral decomposition becomes best option. fact known exact algorithm brute-force enumeration general case. attempts solve landmark selection problem divided categories deterministic methods typically minimize objective function iterative stepwise greedy fashion resultant quality kernel approximation cannot typically guaranteed randomized algorithms instead proceed sampling show section sampling-based methods relative error bounds currently exist subsumed within generalized stochastic framework term annealed determinant sampling. instructive ﬁrst consider problem detail order better characterize properties nystr¨om approximation error. adopt trace norm unitarily invariant norm interest. deﬁnition arbitrary matrix rm×n denote singular value. trace norm deﬁned adopting norm problem therefore allows provide minimax arguments unitary invariance implies natural property results depend spectrum kernel consideration case optimal rank-k approximant note schur complement positive semi-deﬁnite. recalling deﬁnition error incurred nystr¨om approximation norm corresponding schur complement applying deﬁnition trace norm obtain following characterization problem trace norm. proposition subset cardinality denote complement error trace norm induced nystr¨om approximation n-dimensional kernel according deﬁnition conditioned choice subset expressed follows according notation proposition schur complement positive semi-deﬁnite matrix always positive semi-deﬁnite specialization trace norm positive semideﬁnite norms applies. therefore conclude term expression proposition depends selected subset elements diagonal equal term constant. motivated approaches problem based minimizing exclusively latter term proposition gram decomposition partitioned accordance proposition nystr¨om error trace norm error sumof-squares obtained projecting columns closed linear span columns error characterization hand deﬁne introduce notion annealed determinantal distributions turn provides framework analysis comparison landmark selection sampling methods. deﬁnition positive semi-deﬁnite kernel dimension exponent then ﬁxed admits family probability distributions deﬁned follows distribution well deﬁned principal submatrices positive semi-deﬁnite matrix positive semi-deﬁnite hence nonnegative determinant. term annealing suggestive stochastic computation search probability distribution energy function gradually raised nonnegative power course iterative sampling optimization procedure. indeed determinantal annealing deﬁnition amounts ﬂattening distribution whereas becomes peaked. limiting cases recover course uniform distribution range respectively mass concentrated maximal element. instructive consider limiting cases detail. taking observe method uniform sampling typically favoured practitioners trivially recovered negligible associated computational cost. extending result belabbas wolfe induced error bounded follows. note bound tight equality attained diagonal uniform sampling thus averages eﬀects eigenvalues contrast optimal rank-k approximation obtained retaining principal eigenvalues eigenvectors exact spectral decomposition incurs error trace contrast annealed determinant sampling uniform sampling fails place zero probability selection subsets following proposition belabbas wolfe shows exact reconstruction rank-k kernels k-subsets nystr¨om completion requires avoidance subsets. proposition rank suppose subset sampled according annealed determinantal distribution deﬁnition then proof. whenever rank full-rank principal submatrices nonsingular hence admit nonzero determinant. therefore submatrices selected annealed determinantal sampling scheme. proposition full-rank property implies considering limiting case equivalently recover problem maximizing determinant well known np-hard. since notion subset selection based maximal determinant admits following interesting correspondence since vector-valued gaussian random variable covariance matrix schur complement qj×j conditional covariance matrix components given proposition n-dimensional kernel covariance matrix random vector integer minimizing maximum relative entropy coordinates conditional upon observed coordinates corresponds selecting maximized. conclude recent result bounding expected error case turn improves upon additive error bound drineas mahoney sampling according squared diagonal elements theorem nystr¨om extension subset chosen according annealed determinantal distribution result related theorem depends times largest eigenvalue rather smallest eigenvalues. also interpreted terms volume sampling approach proposed deshpande applied gram matrix ‘arbitrary’ matrix det. argument deshpande show result theorem essentially best possible. associated n-dimensional kernel simple markov chain monte carlo method proposed belabbas wolfe shown eﬀective sampling according determinantal distribution k-subsets induced metropolis algorithm easily extended cases covered deﬁnition also note tridiagonal approximations computed operations hence oﬀer alternative cost exact determinant computation. light range methods described optimizing landmark selection process sampling consider case study drawn ﬁeld computer vision low-dimensional manifold structure extracted high-dimensional video data streams. ﬁeld provides particularly compelling example algorithmic aspects space time complexity historically high impact eﬃcacy computer vision solutions. applications areas diverse image segmentation image matting spectral mesh processing object recognition appearance manifolds rely turn eigendecomposition suitably deﬁned kernel. however complexity full spectral analysis real-world datasets often prohibitively costly—requiring practice approximation exact spectral decomposition. indeed aforementioned tasks typically fall category several share common feature kernel approximations obtained exactly way—via process selecting subset landmarks serve basis computation. video datasets often assumed generated dynamical process evolving low-dimensional manifold example line case translation circle case rotation. extracting low-dimensional space applications object recognition appearance manifolds motion recognition pose estimation others. context nonlinear dimension reduction algorithms ingredient mapping video stream lower-dimensional space. vast majority algorithms require obtain eigenvectors positive deﬁnite kernel size equal number figure average normalized approximation error nystr¨om reconstruction diﬀusion maps kernel obtained video ﬁgure using diﬀerent subset selection methods. sampling according determinant yields overall best performance. frames video stream quickly becomes prohibitive entails approximations exact spectral analysis begin case study ﬁrst tested eﬃcacy nystr¨om extension coupled subset selection procedures given diﬀerent video datasets. ﬁgure show exact embedding three dimensions using diﬀusion maps algorithm video honda/ucsd database well selected frames. video subject rotates head front camera several directions motion starting resting position looking straight camera. observe motions associated circular path originate area graph corresponds resting position. ﬁgure average approximation error diﬀusion maps kernel corresponding video evaluated approximation rank results averaged trials. sampling determinant distribution done monte carlo algorithm similar belabbas wolfe determinant maximization obtained keeping subset largest corresponding determinant random choice subsets. setting sampling according determinant distribution yields best results uniformly across range approximations. observe keeping subset maximal determinant give good approximation ranks. analysis showed case chosen landmarks tend concentrate around lower-front-right region graph yields good approximation locally part space fails recover regions properly. behaviour illustrates appeal randomized methods avoid pitfalls. figure exact determinant sampling uniform sampling diﬀusion maps embedding video showing movement uneven speed implemented pixel domain data normalization. note linear structure manifold recovered almost exactly determinantal sampling scheme whereas lost case uniform sampling curve folds itself. diﬀusion maps algorithm non-uniformly sampled straight line. case thus evaluate visual inspection eﬀect approximation diﬀusion kernel quality embedding. shown ﬁgure typical results diﬀerent subset selection methods displayed. sampling according determinant recovers linear structure dynamical process aﬃne transformation whereas sampling uniformly yields folding curve extremities centre. ﬁgure show approximation error kernel associated video averaged trials similarly previous example. case maximizing determinant yields best overall performance. observe sampling according determinant easily outperforms choosing subset uniformly random lending credence analysis framework practical implications landmark selection data subsampling. material based part upon work supported defense advanced research projects agency grant hr--- national institutes health grant national science foundation grants dms- cbet-. work performed part authors visiting isaac newton institute mathematical sciences auspices programme statistical theory methods complex high-dimensional data support gratefully acknowledged. figure average normalized approximation error nystr¨om reconstruction diﬀusion maps kernel obtained video ﬁgure using diﬀerent subset selection methods. sampling uniformly consistently outperformed methods.", "year": 2009}