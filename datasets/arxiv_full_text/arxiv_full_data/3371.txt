{"title": "Mutual Exclusivity Loss for Semi-Supervised Deep Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.", "text": "paper consider problem semi-supervised learning deep convolutional neural networks semi-supervised learning motivated observation unlabeled data cheap used improve accuracy classiﬁers. paper propose unsupervised regularization term explicitly forces classiﬁer’s prediction multiple classes mutuallyexclusive effectively guides decision boundary density space manifolds corresponding different classes data. proposed approach general used backpropagation-based learning method. show different experiments method improve object recognition performance convnets using unlabeled data. training high accuracy classiﬁers often requires large amount labeled training data. recently convnets shown impressive results many vision tasks including limited classiﬁcation detection localization scene labeling however convnets work best large amount labeled data available supervised training. example state-of-the-art results large category ’imagenet’ dataset signiﬁcantly improved using convnets unfortunately building large labeled datasets costly time consuming process. hand unlabeled data easy obtain. several works tried unlabeled data training convnets. convolutional deep belief networks generative model natural images based deep belief networks trained using unlabeled data. unlabeled data also used pre-training convolutional layers convnet effort reduce amount labeled data required supervised training. example predictive sparse decomposition learning ﬁlter coefﬁcients ﬁlter bank layer. however many recent supervised models trained large datasets usually start random initialization ﬁlter weights shows solutions computationally justiﬁed. ladder netdifferent approaches semi-supervised learning general classical approaches include self-training co-training general multiview learning methods strong predictions single classiﬁer multiple classiﬁers added training classiﬁer classiﬁers. another class methods semi-supervised learning called generative models. different methods category based gaussian mixture models hidden markov models generative models generally include unlabeled data modeling probability distribution training data labels. another approach semi-supervised learning transductive goal methods maximize classiﬁcation margin using unlabeled data. large body semi-supervised approaches graph-based methods. methods generally based similarities labeled unlabeled samples similarities encoded edges graph. paper propose semi-supervised learning method makes unlabeled data pushes decision boundary convolutional neural networks less dense areas decision space provides better generalization test data. motivation many visual classiﬁcation tasks easy human classify training samples perfectly; however decision boundary highly nonlinear space pixel intensities. therefore argue that data corresponding every class lies highly nonlinear manifold highdimensional space pixel intensities manifolds don’t intersect other. optimal decision boundary lies manifolds different classes samples. decision boundaries pushed away training samples maximizing margin. furthermore necessary know class labels samples maximize margin classiﬁer tsvms. however ﬁnding classiﬁer large margin possible feature chosen found appropriately. tsvms burden kernel choice. hand since convnets feature generators without ﬁnal fully connected classiﬁcation layer feature space allows large margin classiﬁer capable ﬁnding theory. argument since object recognition relatively easy task human must feature space convnets generate large margin. motivated argument propose regularization term uses unlabeled data encourage classiﬁcation layer convnet large margin. words propose regularization term makes unlabeled data pushes decision boundary less dense area decision space forces predictions multiclass dataset mutually-exclusive. finally substitute disjunction bii= relaxing output classiﬁers binary continuous valued becomes differentiable function applying mentioned approximations deﬁne following unsupervised loss function calculated using samples unsupervised loss function combined loss function used backpropagationbased learning. intuitively loss function forces classiﬁer’s prediction mutually-exclusive every class. addition observed regularization term forces decision boundary possible data sample result placed less dense area decision space. show example. figure shows synthetic dataset three classes diamonds circles crosses. labeled samples shown black circles. trained simple layer neural network dataset. decision areas neural networks shown different colors. figure shows result network trained without unsupervised regularization figure result network proposed unsupervised regularization. unsupervised regularization places decision boundary less dense area space. num. unsupervised regularization function let’s assume {}i=l labeled training data {}i=n i=l+ unlabeled training data. also assume belongs classes ck}. consider output vector general classiﬁer learning parameters input vector deﬁne loss function deﬁned classiﬁer calculated based labeled data loss function quadratic error cross-entropy form loss function. assume ideal case output vector multidimensional binary indicator function dimension input data. sample belongs class binary function following form seen indicator function can’t take arbitrary vector space fact belongs speciﬁc subset multi-dimensional space non-zero element. call subset deﬁne anbinary indicator function determines binary vector also belongs not. deﬁne boolean function using disjunction conjunctions also known disjunctive normal form output indicator function valid prediction ideal classiﬁer one. typically achieved indirectly supervised learning setting onevs-rest classiﬁcation assigns target value correct class target value classes. however labels samples required directly enforce paper enforce condition form regularization term. approximate binary differentiable function optimized gradient descent. replace conjunction also approximate operation binary variable note estimated shown entropy minimization used form regularization term based unlabeled data. similar proposed method minimize loss function based labeled samples also regularization term based calculated using unlabeled data. however multiclass problems regularization term explicitly forces classiﬁer’s prediction different classes mutually-exclusive. experimentally show proposed regularization term generally performs better entropy minimization based datasets especially labeled examples. section present results applying regularization term object recognition using convnets. show extensive results mnist cifar norb svhn datasets. also show preliminary results ilsvrc using alexnet model general divide training data dataset sets consider labeled data unlabeled data section mainly compare performance convnet trained using labeled data convnet trained using labeled data regularization term calculated using labeled unlabeled data. entropy minimization regularization also used training convnets comparison. datasets exception mnist imagenet also trained convnets entropy minimization regularization compared results proposed regularization scheme. must noted entropy regularization previously used convnets best knowledge. every dataset train convnet using different ratio labeled unlabeled data. separate experiments randomly pick training data labeled rest reserved unlabeled set. setting evaluate improvement obtained using unlabeled data. repeat experiment times setting report average standard deviation error rates different experiments. training update model parameters constitute parts. ﬁrst part based labeled data second part unlabeled data. combined parameter according experiments labeled usually smaller unlabeled set. epoch every labeled sample multiple times order compensate difference size labeled unlabeled datasets. experiments ﬁxed experimentally observed performance regularization method overly sensitive incorporated unsupervised regularization term cuda-convnet implementation convnet publicly available exception mnist experiments performed using implementation. setup datasets except mnist imagenet consists convolutional layers followed locally connected layers. maps convolutional layer maps locally connected layer. filters convolutional layers locally connected layers cifar). added fully-connected layer size output. experiments found number epochs learning rates using cross-validation small portion training data repeat training training data. table performance comparison test data mnist dataset. error rates average std. mnist trained mnist using convnet convolutional layers. ﬁrst layer uses ﬁlters produces maps. second layer also uses ﬁlters produces maps. hidden layer nodes added ﬁnal layer. preprocessing performed dataset. annealing momentum. mnist also trained model labeled samples equal labeled data. results given table small number labeled data available proposed unsupervised regularization term signiﬁcantly improves accuracy. norb training norb contains folds images. common practice ﬁrst folds training. test contains folds totalizing original images however scaled similar data translation used training. image translation obtained randomly cropping training images results given table case labeled data supervised term performs better entropy regularization. reason case labeled data sufﬁcient guarantee mutual exclusiveness predictions entropy regularization method. however method explicitly forces mutual exclusiveness. another experiments ﬁxed labeled total training samples. then increased size unlabeled steps. used training data unlabeled separate experiments. results given table seen adding unlabeled data improve performance classiﬁer. svhn svhn contains around images training easier images validation. validation all. test contains images images size generally svhn difﬁcult task mnist large variations images. kind preprocessing dataset. simply converted color images grayscale removing saturation information. results given table similar norb performed experiments ﬁxing size labeled changing size unlabeled data. increased size unlabeled steps. results shown table again observe increasing size unlabeled data actually improve classiﬁcation performance. cifar augmented training data using image translations done taking cropped versions original images random locations. common preprocessing dataset subtract pixel mean training every image. results given table similar norb svhn ﬁxed labeled training data increased size unlabeled steps. imagenet performed preliminary experiments ilsvrc classes. randomly picked class training data labeled rest used unlabeled set. applied regularization term alexnet model using method achieved error rate don’t regularization term error rate shows model effective even large number classes. experiments observed performance improvement using unsupervised regularization. based experiments cases labeled samples advantage using unsupervised regularization term signiﬁcant classiﬁcation task simpler. example cifar challenging dataset compared mnist norb svhn beneﬁts less using unsupervised term. simpler tasks convnet able create feature space less dense areas provides better discriminative features classiﬁer ﬁnal layer. means even challenging tasks unsupervised training data becomes available large convnets trained might able create feature spaces less dense areas larger margins. introduced unsupervised regularization term forces classiﬁer predictions mutually-exclusive different classes moves decision boundary less dense area decision space. showed method applied successfully convnets improve classiﬁcation accuracy using labeled unlabeled data. showed possible improve classiﬁcation accuracy adding unlabeled data. also showed entropy regularization applied convnets successfully. boser john denker henderson richard handwrithoward hubbard lawrence jackel digit recognition back-propagation network advances neural information processing systems. citeseer pierre sermanet david eigen xiang zhang micha¨el mathieu fergus yann lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich going deeper convolutions arxiv preprint arxiv. honglak roger grosse rajesh ranganath andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations proceedings annual international conference machine learning. yann lecun koray kavukcuoglu cl´ement farabet convolutional networks applications vision circuits systems proceedings ieee international symposium ieee kevin jarrett koray kavukcuoglu marc’aurelio ranzato yann lecun what best multi-stage architecture object recognition? computer vision ieee international conference ieee koray kavukcuoglu marc’aurelio ranzato yann lecun fast inference sparse coding algorithms applications object recognition arxiv preprint arxiv. antti rasmus mathias berglund mikko honkala harri valpola tapani raiko semi-supervised learning ladder networks advances neural information processing systems johnson tong zhang semi-supervised convolutional neural networks text categorization region embedding advances neural information processing systems vittori castelli thomas cover relative value labeled unlabeled samples pattern recognition unknown mixing parameter information theory ieee transactions vol. yann lecun huang leon bottou learning methods generic object recognition invariance pose lighting computer vision pattern recognition cvpr proceedings ieee computer society conference ieee vol. ii–. yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning. granada spain vol. ciresan ueli meier j¨urgen schmidhuber multicolumn deep neural networks image classiﬁcation computer vision pattern recognition ieee conference ieee", "year": 2016}