{"title": "A genetic algorithm for autonomous navigation in partially observable  domain", "tag": ["cs.LG", "cs.AI", "cs.NE", "68T05"], "abstract": "The problem of autonomous navigation is one of the basic problems for robotics. Although, in general, it may be challenging when an autonomous vehicle is placed into partially observable domain. In this paper we consider simplistic environment model and introduce a navigation algorithm based on Learning Classifier System.", "text": "avoid unnecessary quality loss robot receives accumulated domain observations imposed also expressed element robot’s observations consists current vision position direction }|x|+ goal number current step current observation policy states making decision result action. executing policy step robot forms path domain quality policy particular domain measured following cost function problem autonomous navigation basic problems robotics. although general challenging autonomous vehicle placed partially observable domain. paper consider simplistic environment model introduce navigation algorithm based learning classiﬁer system. problem navigation partially observable domain computationally challenging since state space environment general grows exponentially size environment. current studies suggest mostly usage partially observable markov decision process example however pomdp usually implies computational challenges make direct application quite difﬁcult. avoid problems number technique used division domain state space hierarchical pomdp etc. alternative approaches also take place including fuzzy logic ‘bug’ algorithms recently shown reactive navigation models successfully trained problem obstacle avoidance paper continue genetic approach train models navigation partially observable domains. contrast mentioned works avoid unnecessary complications consider simplistic model domain. autonomous robot placed cellular two-dimensional static environment ﬁxed width height cell either occupied free robot allowed occupy exactly free cell four directions execute commands forward change direction turning left right forms possible actions robot’s goal conditions formed predeﬁned ﬁnite predicates predicate like conditions function let’s order predicates vector basic predicates size convolution operator deﬁned policy means that using fusion rule represented however always possible main informal assumption model spite fact general genes required express policy efﬁcient ones contains large clusters rules efﬁciently expressed gene reducing size computationally acceptable number also policies build fusion rules contains efﬁcient ones. unlikely experiment shows assumption true model none obtained policies formed however quite easy build model makes every since last case pareto optimal policy exists general case following pomdp approach introduce probability space corresponds prior probability robot placed domain corresponding cost function note expectation operator taken support since required policies path domains outside support becomes undeﬁned. deﬁne navigation problem considered paper given policy minimizes cost function policy deﬁnition general since contains great class algorithms. however optimization algorithmic space problematic. likely possible narrow space mappings ‘extended’ observations actions i.e. stateless algorithms. firstly algorithms able path possible domains worth considerations. since number possible observations robot’s positions directions ﬁnite ﬁnite number policy states used. algorithm ether effectively stateless pareto dominated mapping note deﬁnition accumulated vision observation essential considerations. convenience deﬁne full observation tuple denote simply observation text below. correspondingly extends observation following approach deﬁne navigation policy genes gene condition-action pair conditions viewed predicates fuzzy logic operators described below corresponds ‘false’ ‘true’ maximal uncertainty. able solution least domain. problem lies initial guess search it’s unlikely randomly generate acceptable policy start search with unable evaluate policy search algorithms degenerates random search. solve problem extend model heuristics makes efﬁcient algorithms encoded smaller sets genes least domain probability spaces model extended custom predicates example euclidean distance current position goal also noticeable ‘predictive’ predicates improve quality result policies example ‘will forward command increase real distance goal’ possible values strictly guarantied unknown opposite major changes structure optimal policies introduced complete navigation heuristics obtained policies might optimal able path goal point every domain. complete stateless policy denoted produces predicates experiment heuristic chose ‘naive’ navigation algorithm follows shortest path current point goal considering unknown points free. hand computationally cheap requires operations fewer depending domain structure implementation algorithm. hand good approximation optimal policies domain probability spaces wide support high entropy since model contains complete policy expressed genes rules ether useless since cover special cases observation already covered basic genes play role exceptions complete policy makes policies evaluable allows perform search without degenerating random guessing. selection performed changing weight gene learning phase. genes negative lowweight removed performed action evaluated reward spreads increase weight genes responsible performed action. depending potential deﬁned types learning supervised potential directly corresponds true cost function current domain uses unknown robot information domain unsupervised heuristics applied approximated cost function. supervised learning state performing action then note ‘naive’ policy follows decrease potential approximates distance considering unknown cells free hence cells known accurate estimation becomes. direct usage approximation makes ‘naive’ policy optimal one. instead make estimation accurate suggest usage observation ‘future’ form current estimation holding gene evaluation number steps however even enough procedure guarantee result supervised learning however still able correct policy situations example penalize entering dead-end. example used experiment strategy virtually divides active zone ‘incubation’ zone restricted size ﬁrst one. step genes achieved predeﬁned minimal life time selected weight. koenig simmons xavier robot navigation architecture based partially observable markov decision process models. artiﬁcial intelligence based mobile robotics case studies successful robot systems experiment ofﬁce-like domains size cells selected. domains form support probability space equal probability domain. starting goal points placed opposite sides vision radius ‘naive’ policy alone shows since pending particular domain increasing step generation starting potential ‘naive’ policy denoted classical result experiment policies cost obtained. seen ﬁgure experiment unsupervised learning lower convergence speed cost limit contrast ‘naive’ policy potential converges around cost ‘naive’ policy predicted. experiment shows obtained policies widely heuristic predicates. possibly forming proper predicate dictionary achievable fully negotiate model least introduce types genes standard formed exclusively heuristic predicates majority last type. dictionary allows considerably reduce number optimization parameters speed optimization even allow usage classical method discrete optimization.", "year": 2015}