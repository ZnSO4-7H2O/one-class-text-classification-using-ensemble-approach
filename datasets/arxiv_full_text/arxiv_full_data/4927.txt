{"title": "Stochastic Separation Theorems", "tag": ["cs.LG", "cs.AI", "68T10", "I.2.6"], "abstract": "The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Surprisingly, separation of a new image from a very large set of known images is almost always possible even in moderately high dimensions by linear functionals, and coefficients of these functionals can be found explicitly. Based on fundamental properties of measure concentration, we show that for $M<a\\exp(b{n})$ random $M$-element sets in $\\mathbb{R}^n$ are linearly separable with probability $p$, $p>1-\\vartheta$, where $1>\\vartheta>0$ is a given small constant. Exact values of $a,b>0$ depend on the probability distribution that determines how the random $M$-element sets are drawn, and on the constant $\\vartheta$. These {\\em stochastic separation theorems} provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.", "text": "problem non-iterative one-shot non-destructive correction unavoidable mistakes arises artiﬁcial intelligence applications real world. solution requires robust separation samples errors samples system works properly. demonstrate high dimension separation could achieved probability close linear discriminants. based fundamental properties measure concentration show random depend probability distribution determines random m-element sets drawn constant stochastic separation theorems provide instrument development analysis assessment machine learning methods algorithms high dimension. theoretical statements illustrated numerical examples. artiﬁcial intelligence systems make errors. corrected without damage existing skills. problem non-destructive correction arises many areas research development mathematical neuroscience reverse engineering brain ability learn on-the-ﬂy remains great challenge. desirable corrector errors non-iterative iterative re-training large system requires much time resource cannot done immediately without impeding activity. non-desrructive correction requires separation situations errors samples corresponding correct behavior simple robust classiﬁer. linear discriminants introduced fisher simple inverse covariance matrix data easily modiﬁed assimilation data. rosenblatt revived common interest linear classiﬁers. works sparked intensive scientiﬁc debate gave rise development numerous crucial concepts e.g. vapnik-chervonenkis theory learnability generalization capabilities neural networks linear functionals basic building blocks signiﬁcantly sophisticated systems e.g. multilayer perceptrons convolutional neural networks derivatives. much known linear functionals stand-alone learning machines including generalization margins numerous methods construction linear discriminants regression perceptron learning support vector machines among others. work demonstrate high dimensions even exponentially large samples linear classiﬁers classical fisher’s form powerful enough separate errors correct responses high probability provide efﬁcient solution non-destructive corrector problem. prove linear functionals learning machines surprising concerned peculiar extremal properties high dimension probability every point random i.i.d. drawn m-element sets linearly separable rest. moreover separating linear functional found explicitly without iterations. property holds broad relevant distributions including products probability measures bounded support equidistribution unit ball providing mathematical foundations one-trial correction legacy systems problem data fusion multiagent systems clear similarity problem non-destructive correction. according forney data collected diﬀerent agents naively combined changes context special procedures assimilation without damage gained skills needed. proven stochastic separation eﬀects used approach problem. also shed light possible origins remarkable selectivity stimuli observed in-vivo real brain basic examples linearly separable sets extreme points convex compacts vertices convex polyhedra points n-dimensional sphere. sets extreme points compact linearly separable demonstrated simple examples points belong hyperplane vertices simplex obviously linearly separable. lengths edges bounded volume simplex decreases slower therefore expect suﬃciently regular distribution points random point belong simplex independently chosen random points also linearly separable fast convergence allows hypothesize even large random ﬁnite linearly separable high dimension high probability distribution regular enough. prove statement i.i.d. random points equidistributions ball cube distributions products measures bounded support. ball provides geometric details concentration volume high-dimensional balls. estimate probability cosine angles exceed gorban analyzed asymptotic behavior estimations small idea almost orthogonal bases introduced kainen k˚urkov´a used eﬃciently k˚urkov´a sanguineti estimation cardinality ε-nets compact convex subsets hilbert spaces including sets functions computable perceptrons. figure point belongs spherical layer thickness data centralized centre spheres origin. hyperplane orthogonal vector tangent internal sphere cuts upper spherical separates data points belong external sphere belong cap. included upper half ball centre intersection radius internal sphere layer cube. coordinates random point independent random variables expectations variances vector coordinates large distribution concentrated relatively small vicinity sphere arbitrary centre coordinates radius concentration near spheres diﬀerent centres implies concentration vicinity intersection vicinity spheres distribution concentrated estimated hoeﬀding inequality figure estimates probabilities random point sample points i.i.d. drawn equidistribution unit cube separable remaining points sample function dimension blue stars black triangles green squares means maxima classical measure concentration theorems state random points concentrated thin layer near surface stochastic separation theorems describe thin structure thin layers random points concentrated thin layer linearly separable rest even exponentially large random sets. estimates produced classes distributions high dimension equidistributions balls ellipsoids product distributions compact support numerous generalisations possible example estimates optimal improved. main message exponential dependence upper boundary grow exponentially. numerical experiments show equidistribution cube worse practical point view uniform distribution ball. illustrate this empirically assessed linear separability samples drawn equidistributions unit n-cubes. selected values generated samples random points sample sub-sample points randomly chosen point sub-sample linear functionals constructed. sings calculated numbers instances recorded. empirical frequencies n−/n derived. outcomes experiment summarized fig. experiments demonstrate probability randomly selected point sample linearly separable rest could signiﬁcantly higher simple exponential estimates provided. this however surprising estimates based values means variances take account quantitative properties sample distribution. aration suﬃcient correct mistake legacy system without re-learning modiﬁcation existing skills measure concentration eﬀects reveal hidden geometric background reported success randomized neural networks models empirical covariance matrix. mahalanobis inner product used ‘whitening’ i.e. transformation data cloud spherical form. course attributes highly correlated empiric covariance matrix invertible. analysed separation random points random sets. problem single correction legacy system. question generalisability correction great practical importance. leads problem separation random sets. simple series generalisations immediately produced theorems separation melement random k-element purpose consider linear space span{yi− study separation point m-element projection onto quotient space rn/e. independent separation would likely limited sets small cardinality contrast pair-wise positively correlated expect single functional would separate reported extreme separation capabilities linear functionals oﬀer insights grandmother cell concept cell phenomena broadly reported neuroscience essence phenomenon neurons human brain respond unexpectedly selective particular persons objects. strikingly brain able respond selectively rare individual stimuli also selectivity learnt rapidly limited number experiences question small ensembles neurons deliver sophisticated functionality reliably? stochastic separation theorems provide possible answer. accept linear functionals followed nonlinear threshold-modulated response phenomenological models cells whose activity measured number inputs converging cells large enough statistically independent extreme selectivity responses models follows immediately theorem", "year": 2017}