{"title": "Visualizing the Loss Landscape of Neural Nets", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comp arisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.", "text": "zheng gavin taylor christoph studer goldstein university maryland college park united states naval academy cornell university {haolixuzhtomg}cs.umd.edu taylorusna.edu studercornell.edu neural network training relies ability good minimizers highly non-convex loss functions. well known certain network architecture designs produce loss functions train easier wellchosen training parameters produce minimizers generalize better. however reasons differences effect underlying loss landscape well understood. paper explore structure neural loss functions effect loss landscapes generalization using range visualization methods. first introduce simple ﬁlter normalization method helps visualize loss function curvature make meaningful side-by-side comparisons loss functions. then using variety visualizations explore network architecture affects loss landscape training parameters affect shape minimizers. training neural networks requires minimizing high-dimensional non-convex loss function task hard theory sometimes easy practice. despite np-hardness training general neural loss functions simple gradient methods often global minimizers even data labels randomized training however good behavior universal; trainability neural nets highly dependent network architecture design choices choice optimizer variable initialization variety considerations. unfortunately effect figure loss surfaces resnet- with/without skip connections. vertical axis logarithmic show dynamic range. proposed ﬁlter normalization scheme used enable comparisons sharpness/ﬂatness ﬁgures. choices structure underlying loss surface unclear. prohibitive cost loss function evaluations studies ﬁeld remained predominantly theoretical. high-resolution visualizations provide empirical characterization neural loss functions explore different network architecture choices affect loss landscape. furthermore explore non-convex structure neural loss functions relates trainability geometry neural minimizers affects generalization properties. meaningful propose simple ﬁlter normalization scheme enables side-by-side comparisons different minima found neural networks training. visualizations explore sharpness/ﬂatness minimizers found different methods well effect network architecture choices loss landscape. goal understand differences loss function geometry affect generalization neural nets. article study methods producing meaningful loss function visualizations. then using visualization methods explore loss landscape geometry effects generalization error trainability. speciﬁcally address following issues reveal faults number visualization methods loss functions show simple visualization strategies fail accurately capture local geometry loss function minimizers. present simple visualization method based ﬁlter normalization enables sideby-side comparisons different minimizers. sharpness minimizers correlates well generalization error visualization used even making comparisons across disparate network architectures training methods. observe that networks become sufﬁciently deep neural loss landscapes quickly transition nearly convex highly chaotic. transition convex chaotic behavior coincides dramatic drop generalization error ultimately lack trainability. show skip connections promote minimizers prevent transition chaotic behavior helps explain skip connections necessary training extremely deep networks. study visualization optimization trajectories. explain difﬁculties arise visualizing trajectories show optimization trajectories extremely dimensional space. dimensionality explained presence large nearly convex regions loss landscape observed -dimensional visualizations. visualizations potential help answer several important questions neural networks work. particular able minimize highly non-convex neural loss functions? resulting minima generalize? difﬁculty visualizing loss functions studies loss landscapes largely theoretical nature. number authors studied ability minimize neural loss functions. using random matrix theory spin glass theory several authors shown local minima objective value also shown local minima global minima provided assumes linear neurons wide layers full rank weight matrices assumptions relaxed kawaguchi kawaguchi although assumptions still required. soudry hoffer freeman bruna also analyzed shallow networks hidden layers mild conditions. another approach show expect good minimizers simply endogenous properties neural networks optimizers. restricted network classes hidden layer extra assumptions sample distribution globally optimal near-optimal solutions found common optimization methods networks speciﬁc structures safran shamir haeffele vidal show likely exists monotonically decreasing path initialization global minimum. swirszcz show counterexamples achieve local minima problems. also interest work assessing sharpness/ﬂatness local minima. hochreiter schmidhuber deﬁned ﬂatness size connected region around minimum training loss remains low. keskar suggested ﬂatness characterized magnitude eigenvalues hessian proposed \u0001-sharpness approximation looks maximum loss bounded neighborhood minimum. however dinh show quantitative measure sharpness problematic invariant symmetries network thus sufﬁcient determine generalization ability. chaudhari used local entropy measure sharpness invariant simple transformation dinh difﬁcult accurately compute. dziugaite connects sharpness pac-bayes bounds generalization. theoretical results make restrictive assumptions independence input samples restrictions non-linearities loss functions. reason visualizations play role verifying validity theoretical assumptions understanding loss function behavior real-world systems. next section brieﬂy review methods used purpose. neural nets contain many parameters loss functions live high-dimensional space. unfortunately visualizations possible using low-dimensional plots. several methods exist closing dimensionality gap. -dimensional linear interpolation simple lightweight plot loss functions choose sets parameters plot values loss function along line connecting points. parameterize line choosing scalar parameter deﬁning weighted average finally plot function strategy taken goodfellow studied loss surface along line initial guess nearby minimizer obtained stochastic gradient descent. method widely used study sharpness ﬂatness different minima dependence sharpness batch-size smith topin interpolation technique show different minima peaks them plot line minima obtained different optimizers. linear interpolation method suffers several weaknesses. first difﬁcult visualize non-convexities using plots. indeed goodfellow found loss functions appear lack local minima along minimization trajectory. later using methods loss functions extreme non-convexities non-convexities correlate difference generalization different network architectures. second method consider batch normalization invariance symmetries network. reason visual sharpness comparisons produced interpolation plots misleading; issue explored depth section case. approach used explore trajectories different minimization methods. also used show different optimization algorithms different local minima within projected space. computational burden plotting methods generally result low-resolution plots small regions captured complex non-convexity loss surfaces. below high-resolution visualizations large slices weight space visualize network design affects non-convex structure. random directions approach plotting simple cannot used compare geometry different minimizers different networks. scale invariance network weights. relu non-linearities used network remains unchanged multiply weights layer network divide next layer invariance even prominent batch normalization used. case size ﬁlter irrelevant output layer re-scaled batch normalization. reason network’s behavior remains unchanged re-scale weights. scale invariance prevents making meaningful comparisons plots unless special precautions taken. neural network large weights appear smooth slowly varying loss function; perturbing weights unit little effect network performance weights live scale much larger one. however weights much smaller unit perturbation catastrophic effect making loss function appear quite sensitive weight perturbations. keep mind neural nets scale invariant; small-parameter large-parameter networks example equivalent apparent differences loss function merely artifact scale invariance. scale invariance exploited dinh build pairs equivalent networks different apparent sharpness. remove scaling effect plot loss functions using ﬁlter-wise normalized directions. obtain directions network parameters begin producing random gaussian direction vector dimensions compatible normalize ﬁlter norm corresponding ﬁlter words make replacement represents ﬁlter layer denotes frobenius norm. note ﬁlter-wise normalization different normalize direction without considering norm individual ﬁlters. proposed scaling important factor making meaningful plots loss function geometry. explore importance proper scaling explore sharpness/ﬂatness different minimizers. context show sharpness ﬁlter-normalized plots correlates generalization error plots without ﬁlter normalization misleading. section introduces concept ﬁlter normalization provides intuitive justiﬁcation use. section address issue whether sharp minimizers generalize better minimizers. sharpness minimizers correlates well generalization error ﬁlter normalization used. enables side-by-side comparisons plots. contrast sharpness non-normalized plots appear distorted unpredictable. widely thought small-batch produces minimizers generalize better large batch sizes produce sharp minima poor generalization claim disputed though dinh kawaguchi arguing generalization directly related curvature loss surfaces authors proposing specialized training methods achieve good performance large batch sizes here explore difference sharp minimizers. begin discussing difﬁculties arise performing visualization proper normalization prevent plots producing distorted results. train cifar- classiﬁer using -layer network batch normalization batch sizes large batch size small batch size indicate solutions obtained running using small large batch sizes respectively. using linear interpolation approach plot loss values training testing data sets cifar- along direction containing solutions i.e. section consider running mean running variance trainable parameters include note original study goodfellow consider batch normalization. parameters included future sections needed interpolating minimizers. figure linear interpolation solutions obtained small-batch large-batch methods vgg. blue lines loss values lines accuracies. solid lines training curves dashed lines testing. small batch abscissa large batch abscissa figures show linear interpolation plots x-axis location location observed keskar clearly small-batch solution quite wide large-batch solution sharp. however sharpness balance ﬂipped simply turning weight decay figures show results experiment except time non-zero weight decay parameter. time large batch minimizer considerably ﬂatter sharp small batch minimizer. however table small batches generalize better experiments; apparent correlation sharpness generalization. side-by-side sharpness comparisons extremely misleading fail capture endogenous properties minima. apparent differences sharpness figure explained examining weights minimizer. histograms networks weights shown experiment figure that large batch used zero weight decay resulting weights tends smaller small batch case. reverse effect adding weight decay; case large batch minimizer much larger weights small batch minimizer. difference scale occurs simple reason smaller batch size results weight updates epoch large batch size shrinking effect weight decay pronounced. evolution ﬁlter norms training depicted figure appendix. figure visualizing endogenous sharpness minimizers rather weight scaling. scaling weights networks irrelevant batch normalization re-scales outputs unit variance. however small weights still appear sensitive perturbations produce sharper looking minimizers. filter normalized plots repeat experiment figure time plot loss function near minimizer separately using random ﬁlter-normalized directions. removes apparent differences geometry caused scaling depicted figure results presented figure still show differences sharpness small batch large batch minima however differences much subtle would appear un-normalized plots. comparison sample un-normalized plots shown section appendix. also visualize results using random directions contour plots. shown figure weights obtained small batch size non-zero weight decay wider contours sharper large batch minimizers. results resnet- appear figure appendix. figure histogram weights. zero weight decay small-batch methods produce large weights. non-zero weight decay small-batch methods produce smaller weights. figure shape minima obtained using different optimization algorithms batch size weight decay. title subﬁgure contains optimizer batch size test error. ﬁrst weight decay second uses weight decay figure visualization solutions obtained small-batch large-batch. similar figure ﬁrst uses zero weight decay second sets weight decay generalization flatness using ﬁlter-normalized plots figures make side-by-side comparisons minimizers sharpness correlates well generalization error. large batches produced visually sharper minima higher test error. interestingly adam optimizer attained larger test error predicted corresponding minima visually sharper. results similar experiment using resnet- presented appendix ability global minimizers neural loss functions universal; seems neural architectures easier minimize others. example using skip connections able train extremely deep architectures comparable architectures without skip connections trainable. furthermore ability train seems depend strongly initial parameters training starts. using visualization methods empirical study neural architectures explore non-convexity loss functions seems problematic situations others. provide insight following questions loss functions signiﬁcant non-convexity all? prominent non-convexities exist problematic situations? architectures easy train results sensitive initialization? understand effects network architecture non-convexity trained number networks plotted landscape around obtained minimizers using ﬁlter-normalized random direction method described section consider three classes neural networks vgg-like networks contain shortcut/skip connections. produced networks simply removing skip connections cifar-optimized resnets. call networks resnet--noshort resnet--noshort resnet--noshort. note networks perform well cifar- task. purely experimental purposes explore effect shortcut connections. wide resnets optimized imagenet rather cifar. networks ﬁlters layer cifar optimized networks also different numbers layers. models include resnet- resnet- resnet-. models trained cifar- dataset using nesterov momentum batch-size weight decay epochs. learning rate initialized decreased factor epochs deeper experimental vgg-like networks required smaller initial learning rate high resolution plots minimizers different neural networks shown figure results shown contour plots rather surface plots makes extremely easy non-convex structures evaluate sharpness. surface plots resnet- figure note center plot corresponds minimizer axes parameterize random directions ﬁlter-wise normalization make several observations architecture effects loss landscape. also provide loss error values networks table convergence curves figure appendix. figure network depth dramatic effect loss surfaces neural networks skip connections used. network resnet--noshort fairly benign landscape dominated region convex contours center dramatic non-convexity. however network depth increases loss surface vgg-like nets spontaneously transitions convex chaotic. resnet--noshort dramatic non-convexities large regions gradient directions point towards minimizer center. also loss function becomes extremely large move directions. resnet--noshort displays even dramatic non-convexities becomes extremely steep move directions shown plot. furthermore note minimizers center deep vgg-like nets seem fairly sharp. case resnet--noshort minimizer also fairly ill-conditioned contours near minimizer signiﬁcant eccentricity. shortcut connections dramatic effect geometry loss functions. figure residual connections prevent transition chaotic behavior depth increases. fact width shape .-level contour almost identical -layer networks. interestingly effect skip connections seems important deep networks. shallow networks effect skip connections fairly unnoticeable. however residual connections prevent explosion non-convexity occurs networks deep. effect seems apply kinds skip connections well; figure appendix shows loss landscape densenet shows noticeable non-convexity. effect number convolutional ﬁlters layer compare narrow cifaroptimized resnets wider resnets ﬁlters optimized imagenet. figure wider models loss landscapes noticeable chaotic behavior. increased network width resulted minima wide regions apparent convexity. effect also validated figure plot landscape resnet- multiple number ﬁlter layer zagoruyko komodakis increased width prevents chaotic behavior skip connections dramatically widen minimizers. finally note sharpness correlates extremely well test error. interesting observations seen figure loss landscapes networks considered seem partitioned well-deﬁned region loss value convex contours surrounded well-deﬁned region high loss value non-convex contours. partitioning chaotic convex regions explain importance good initialization strategies also easy training behavior good architectures. using normalized random initialization strategies proposed glorot bengio typical neural networks attain initial loss value less well behaved loss landscapes figure dominated large nearly convex attractors rise loss value greater. landscapes random initialization likely wellbehaved loss region optimization algorithm might never pathological non-convexities occur high loss chaotic plateaus. chaotic loss landscapes shallower regions convexity rise lower loss values. sufﬁciently deep networks shallow enough attractors initial iterate likely chaotic region gradients uninformative. case gradients shatter balduzzi training impossible. unable train layer network without skip connections adds weight hypothesis. figures show landscape geometry dramatic effect generalization. first note visually ﬂatter minimizers consistently correspond lower test error strengthens assertion ﬁlter normalization natural visualize loss function geometry. second notice chaotic landscapes result worse training test error convex landscapes lower error values. fact convex landscapes wide resnets bottom figure generalize best show noticeable chaotic behavior. finally explore methods visualizing trajectories different optimizers. application random directions ineffective. provide theoretical explanation random directions fail explore methods effectively plotting trajectories loss function contours. several authors observed random direction fail capture variation optimization trajectories including gallagher downs lorch lipton liao poggio several failed visualizations depicted figure figure iterates projected onto plane deﬁned random directions. almost none motion captured problem noticed goodfellow visualized trajectories using direction points initialization solution random direction. approach shown figure seen figure random axis captures almost variation leading appearance straight line path. problematic optimization trajectories extremely dimensional spaces. case randomly chosen vector orthogonal low-rank space containing optimization path projection onto random direction capture almost variation. figure suggests optimization trajectories dimensional random direction captures orders magnitude less variation vector points along optimization path. below directions directly validate dimensionality also produce effective visualizations. capture variation trajectories need non-random directions. here suggest approach based allows measure much variation we’ve captured; also provide plots trajectories along contours loss surface. denote model parameters epoch ﬁnal estimate given training epochs apply matrix select explanatory directions. optimizer trajectories loss surfaces along directions shown figure epochs learning rate decreased shown dots. axis measure amount variation descent path captured direction. interesting behavior plots. early stages training paths tend move perpendicular contours loss surface i.e. along gradient directions would expect non-stochastic gradient descent. stochasticity becomes fairly pronounced several plots later stages training. particularly true plots weight decay small batches weight decay small batches used path turn nearly parallel contours orbit solution stepsize large. stepsize dropped effective noise system decreases kink path trajectory falls nearest local minimizer. finally directly observe descent path dimensional variation descent paths lies space dimensions. optimization trajectories figure appear dominated movement direction nearby attractor. dimensionality compatible observations section observed non-chaotic landscapes dominated wide minimizers. paper presented accurate visualization technique provided insights consequences variety choices facing neural network practitioner including network architecture optimizer selection batch size. neural networks advanced dramatically recent years largely back anecdotal knowledge theoretical results complex assumptions. progress continue made general understanding structure neural networks needed. hope effective visualization coupled continued advances theory result faster training simpler models better generalization. references david balduzzi marcus frean lennox leary lewis kurt wan-duo brian mcwilliams. shattered gradients problem resnets answer question? icml yann dauphin razvan pascanu caglar gulcehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. nips marcus gallagher downs. visualization learning multilayer perceptron networks using principal component analysis. ieee transactions systems cybernetics part priya goyal piotr doll´ar ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. iclr mahdi soltanolkotabi adel javanmard jason lee. theoretical insights optimization landscape over-parameterized shallow neural networks. arxiv preprint arxiv. figure change weights norm training vgg-. weight decay disabled weight norm grows steadily training without constraints. nonzero weight decay adopted weight norm decreases rapidly beginning becomes stable learning rate decayed. since ﬁxed number epochs different batch sizes difference weight norm change large-batch small-batch training mainly caused larger number updates small batch used. shown second changes weight norm pace small large batch training terms iterations. ﬁlter norm corresponding ﬁlter approach advocated article used extensively plotting loss surfaces. note ﬁlter normalization limited convolutional layers also applies fully connected layers. layer equivalent conv layer output feature ﬁlter corresponds weights generate neuron. figure shows randomalized plots without normalization. issue non-normalized plots x-axis range must chosen carefully. figure shows enlarged plot x-axis. without normalization plots fail show consistency ﬂatness generalization error. compare ﬁlter normalization layer normalization. ﬁlter normalization accurate layer normalization. failing case layer normalization shown figure figure ﬂatter figure worse generalization error. figure enlarged loss surface vgg- without normalization. range x-axis ﬁrst weight decay second uses weight decay pairs show sharpness correlate test error. similar observations made section sharp sharp dilemma also applies resnet shown figure generalization error solution shown table visualization ﬁlter normalized directions shown figure figure figure linear interpolation solutions obtained small-batch large-batch methods resnet. blue lines loss values lines error. solid lines training curves dashed lines testing. figure shape minima obtained different optimization algorithms resnet- varying batch size weight decay. similar figure ﬁrst uses zero weight decay second uses weight decay. figure visualization solutions resnet- obtained sgd/adam small-batch large-batch. similar figure ﬁrst uses zero weight decay second sets weight decay different direction produce dramatically different surface? plot loss surface vgg- random ﬁlter-normalized directions. shown figure shape different plots close indicates good generalization ability minima. also repeat loss surface plots multiple times resnet--noshort worse generalization error. shown figure apparent changes loss surface different plots however choatic behaviour consistent across plots. figure repeatness surface plots resnet--noshort. model trained batch size initial learning rate weight decay ﬁnal training loss training error test error computing resources generating ﬁgures pytorch code executed multiple workstation well hundreds gpus using mpipy. computation time depends model’s inference speed training resolution plots number gpus. resolution plots figure default resolutions used contours figure figure higher resolutions resnet--noshort used figure show details. example contour resnet- model resolution take hour workstation gpus batch normalization parameters linear interpolation methods parameters including running mean running variance need considered part parameters considered possible reproduce loss accurately minimizers. ﬁlter-normalized visualization random direction applies weights weights note ﬁlter normalization process removes effect weight scaling batch normalization ignored. vgg- architecture details parameters adam vgg- cropped version vgg- keeps ﬁrst conv layers vgg- layers. batch normalization layer added conv layer ﬁrst layer. vgg- efﬁcient network better performance comparing vgg- cifar-. default values adam learning rate schedule used sgd. loss curves training vgg- used section shown figure figure shows loss curves error curves architectures used section table shows ﬁnal error loss values. default setting training using nesterov momentum batch-size weight decay epochs. default learning rate initialized decreased factor epochs figure training loss/error curves vgg- different optimization methods. ﬁrst shows loss curves second error curves. dashed lines testing solid training.", "year": 2017}