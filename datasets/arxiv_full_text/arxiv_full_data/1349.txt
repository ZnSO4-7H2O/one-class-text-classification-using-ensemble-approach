{"title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances.", "text": "deep neural networks branch machine learning seen meteoric rise popularity powerful abilities represent model high-level abstractions highly complex data. area deep neural networks ripe exploration neural connectivity formation. pivotal study brain tissue rats found synaptic formation speciﬁc functional connectivity neocortical neural microcircuits surprisingly well modeled predicted random formation. motivated intriguing ﬁnding introduce concept stochasticnet deep neural networks formed stochastic connectivity between neurons. result type deep neural networks formed stochasticnet allowing neuron connectivity stochastic. stochastic synaptic formations deep neural network architecture allow efﬁcient utilization neurons performing speciﬁc tasks. evaluate feasibility deep neural network architecture train stochasticnet using four different image datasets experimental results show stochasticnet using less half number neural connections conventional deep neural network achieves comparable accuracy reduces overﬁtting cifar- mnist svhn dataset. interestingly stochasticnet less half number neural connections achieved higher accuracy stl- dataset conventional deep neural network. finally stochasticnets faster operational speeds achieving better similar accuracy performances. deep neural networks branch machine learning seen meteoric rise popularity powerful abilities represent model high-level abstractions highly complex data. deep neural networks shown considerable capabilities handling speciﬁc complex tasks speech recognition object recognition natural language processing recent advances improving performance deep neural networks focused areas network regularization activation functions deeper architectures however neural connectivity formation deep neural networks remained largely past decade thus exploration investigation alternative approaches neural connectivity formation hold considerable promise. explore alternate deep neural network connectivity formation take inspiration nature looking brain develops synaptic connectivity neurons. recently pivotal paper hill data living brain tissue wistar rats collected used construct partial brain. based hill al.came surprising conclusion. synaptic formation speciﬁc functional connectivity neocortical neural microcircuits modelled predicted random formation. comparison construction deep neural networks neural connectivity formation largely deterministic pre-deﬁned. motivated hill al.’s ﬁnding random neural connectivity formation investigate feasibility efﬁcacy devising stochastic neural connectivity formation construct deep neural networks. achieve goal introduce concept stochasticnet idea leverage random graph theory form deep neural networks stochastic connectivity neurons. such treat formed deep neural networks particular realizations random graph. stochastic synaptic formations deep neural network architecture potentially allow efﬁcient utilization neurons performing speciﬁc tasks. furthermore since focus neural connectivity stochasticnet architecture used directly like conventional deep neural network beneﬁt approaches used conventional networks data augmentation stochastic pooling dropout dropconnect number stochastic strategies improving deep neural network performance previously introduced important note proposed stochasticnets fundamentally different existing stochastic strategies stochasticnets’ main signiﬁcant contributions deals primarily formation neural connectivity individual neurons construct efﬁcient deep neural networks inherently sparse prior training previous stochastic strategies deal either grouping existing neural connections explicitly enforce sparsity removal/introduction neural connectivity regularization training. speciﬁcally stochasticnets realization random graph formed prior training connectivity network inherently sparse permanent change training. different dropout dropconnect activations connections temporarily removed training back test regularization purposes only resulting neural connectivity network remains dense. notion ’dropping’ stochasticnets subset possible neural connections formed ﬁrst place prior training resulting network connectivity network sparse. stochasticnets also different hashnets connection weights randomly grouped hash buckets bucket sharing weights explicitly sparsifying network since notion grouping/merging stochasticnets; formed stochasticnets naturally sparse formation process. fact stochastic strategies hashnets dropout dropconnect used conjunction stochasticnets. paper organized follows. first review random graph theory presented section theory design considerations behind forming stochasticnet random graph realizations discussed section experimental results using four image datasets mnist svhn stl- investigate efﬁcacy stochasticnets respect different number neural connections well different training sizes presented section finally conclusions drawn section study goal leverage random graph theory form neural connectivity deep neural networks stochastic manner. such important ﬁrst provide general overview random graph theory context. random graph theory random graph deﬁned probability distribution graphs number different random graph models proposed literature. commonly studied random graph model proposed gilbert random graph expressed possible edge connectivity said occur independently probability random graph model generalized kovalenko random graph expressed vertices edge connectivity vertices graph said occur probability illustrative example random graph based model shown figure seen possible edge connectivity nodes graph occur independently probability pij. therefore based generalized random graph model realizations random graphs obtained starting vertices {vq| randomly adding edges vertices based possible edges {eij| independently probability pij. number realizations random figure realizations random graph figure probability edge connectivity nodes graph nodes diagram demonstrates different realization random graph. figure example random graph representing general deep feed-forward neural network. every neuron layer connected neuron layer probability based random graph theory. enforce properties general deep feed-forward neural network neurons {vik| denoting neuron layer denoting number layers denoting number neurons layer probability neural connection occurs neuron vik. based random graph model representing deep neural networks form deep neural network realization random graph starting neurons randomly adding neural connections neurons independently probability must designed properties enforced appropriately resultant random graph realization. consider general deep feed-forward neural network. first deep feed-forward neural network neural connections non-adjacent layers. second deep feed-forward neural network neural connections neurons layer. therefore enforce properties example random graph based random graph model representing general deep feed-forward neural networks shown figure example realization random graph shown figure observed figure neural connectivity neuron different stochastic nature neural connection formation. furthermore speciﬁc types deep feed-forward neural networks additional considerations must taken account preserve properties resultant random graph realization. example case deep convolutional neural networks neural connectivity convolutional layers arranged small spatially localized neural collections connected output neuron next layer. furthermore weights neural connections shared amongst different small neural collections. signiﬁcant beneﬁt architecture allows neural connectivity convolutional layers efﬁciently represented local receptive ﬁelds thus greatly reducing memory requirements computational complexity. enforce properties forming deep convolutional neural networks random graph realizations enforce probability probability neural connectivity deﬁned local receptive ﬁeld level. such neural connectivity randomly realized local receptive ﬁeld based probability distribution figure example realization random graph shown figure example neurons except observed neural connectivity neuron different stochastic nature neural connection formation. connectivity neuron green neuron highlighted show differences neural connectivity. figure forming deep convolutional neural network random graph. neural connectivity randomly realized local receptive ﬁeld determined based probability distribution conﬁguration shape randomly realized local receptive ﬁeld differ. seen shape neural connectivity local receptive ﬁeld completely different local receptive ﬁeld response randomly realized local receptive ﬁeld leads output channel layer formed deep convolutional neural network random graph shown illustrative purposes. given random graph model representing deep convolutional neural networks resulting random graph realization deep convolutional neural network convolutional layer consists randomly realized local receptive ﬁelds randomly realized local receptive ﬁeld denotes receptive ﬁeld layer consisting neural connection weights random neurons within small neural collection output neuron. example realization deep convolutional neural network random graph shown figure investigate efﬁcacy stochasticnets construct stochasticnets deep convolutional neural network architecture evaluate constructed stochasticnets number different ways. first investigate effect number neural connections formed constructed stochasticnets performance task image object recognition. second investigate performance stochasticnets compared baseline deep convolutional neural networks standard neural connectivity different image object recognition tasks based different image datasets. third investigate relative speed stochasticnets classiﬁcation respect number neural connections formed constructed stochasticnets. important note main goal investigate efﬁcacy forming deep neural networks stochastic connectivity form stochasticnets inﬂuence stochastic connectivity parameters network performance obtain maximum absolute performance; therefore performance figure training test error versus number neural connections convolutional layers fully connected layers cifar- dataset. gaussian distributed uniform distributed neural connectivity evaluated. note neural connectivity percentage equivalent convnet since connections made. stochasticnets optimized additional techniques data augmentation network regularization methods. evaluation purposes four benchmark image datasets used cifar- mnist svhn stl- description dataset stochasticnet conﬁguration used described below. cifar- image dataset consists training images categorized different classes natural scenes. image image size. mnist image dataset consists training images test images handwritten digits. image binary image size handwritten digits normalized respect size centered image. svhn image dataset consists training images test images digits natural scenes. image image size. images mnist dataset resized zero padding since stochasticnet network conﬁguration utilized mentioned image datasets. finally stl- image dataset consists labeled training images labeled test images categorized different classes natural scenes. image image size. note unlabeled images stl- image dataset used study. stochasticnets used study datasets realized based lenet- deep convolutional neural network architecture consists convolutional layers local receptive ﬁelds size ﬁrst second third convolutional layers respectively hidden layer neurons neural connections convolutional hidden layers randomly realized based probability distributions. possible take advantage arbitrary distribution construct stochasticnet realizations purpose study neural connection probability hidden layers follow uniform distribution different spatial distributions explored convolutional layers uniform distribution gaussian distribution mean center receptive ﬁeld standard deviation third receptive ﬁeld size. image datasets class label outputs provided network setup. experiment conducted illustrate impact number neural connections modeling accuracy stochasticnets. figure demonstrates training test error versus number neural connections network cifar- dataset. stochasticnet network conﬁguration described section provided train model. neural connection probability varied convolutional layers hidden layer achieve desired number neural connections testing effect modeling accuracy. figure demonstrates training testing error neural connectivity percentage relative baseline convnet different neural connection distributions uniform distribution gaussian distribution mean center receptive ﬁeld standard deviation third receptive ﬁeld size. observed stochasticnet able achieve test error convnet number neural connections stochasticnet less half convnet. also observed that although increasing number neural connections resulted lower training error exhibit reductions test error brings light issue over-ﬁtting. words observed proposed stochasticnets improve handling over-ﬁtting associated deep neural networks decreasing number neural connections effect greatly reduces number computations thus resulting faster network training usage. finally also observed noticeable difference training test errors using gaussian distributed connectivity compared uniform distributed connectivity indicates choice neural connectivity probability distributions noticeable impact model accuracy. figure comparison standard convnet stochasticnet neural connectivity convnet. stochasticnets results shows error based trials since neural connectivity stochasticnets realized stochastically. dashed line demonstrates standard deviation error based trials stochasticnets. motivated results shown figure comprehensive experiment done demonstrate performance proposed stochasticnets different benchmark image datasets. stochasticnet realizations formed neural connectivity gaussian-distributed connectivity compared conventional convnet. stochasticnets convnets trained four benchmark image datasets training test error performances compared other. since neural connectivity stochasticnets realized stochastically performance stochasticnets evaluated based trials reported results based average trials. figure shows training test error results stochasticnets convnets four different tested datasets. observed that despite fact less half many neural connections stochasticnet realizations test errors convnets stochasticnet realizations considered cifar- mnist svhn datasets. interestingly also observed test errors stochasticnet realizations lower achieved using convnet stl- dataset despite fact less half many neural connections stochasticnet realizations. results stl- dataset truly illustrates particular effectiveness stochasticnets particularly dealing number training samples. furthermore training test errors stochasticnets less convnets would indicate reduced overﬁtting stochasticnets. standard deviation trials error curve shown dashed lines around error curve. observed standard deviation trials small indicates proposed stochasticnet exhibited similar performance trials. figure relative classiﬁcation time versus number neural connections. note neural connectivity percentage equivalent convnet since connections made. given experiments previous sections show stochasticnets achieve good performance relative conventional convnets signiﬁcantly fewer neural connections investigate relative speed stochasticnets classiﬁcation respect number neural connections formed constructed stochasticnets. here section neural connection probability varied convolutional layers hidden layer achieve desired number neural connections testing effect classiﬁcation speed formed stochasticnets. figure demonstrates relative classiﬁcation time neural connectivity percentage relative baseline convnet. relative time deﬁned time required classiﬁcation process relative convnet. observed relative time decreases number neural connections decrease illustrates potential stochasticnets enable efﬁcient classiﬁcation. study introduced approach deep neural network formation inspired stochastic connectivity exhibited synaptic connectivity neurons. proposed stochasticnet deep neural network formed realization random graph synaptic connectivity neurons formed stochastically based probability distribution. using approach neural connectivity within deep neural network formed facilitates efﬁcient neural utilization resulting deep neural networks much fewer neural connections achieving modeling accuracy. effectiveness efﬁciency proposed stochasticnet evaluated using four popular benchmark image datasets compared conventional convolutional neural network experimental results demonstrate proposed stochasticnet provides comparable accuracy conventional convnet much less number neural connections reducing overﬁtting issue associating conventional convnet cifar- mnist svhn datasets. interestingly stochasticnet much less number neural connections found achieve higher accuracy compared conventional deep neural networks stl- dataset. such proposed stochasticnet holds great potential enabling formation much efﬁcient deep neural networks fast operational speeds still achieving strong accuracy. work supported natural sciences engineering research council canada canada research chairs program ontario ministry research innovation. authors also thank nvidia hardware used study nvidia hardware grant program. m.s. a.w. conceived designed architecture. m.s. p.s. a.w. worked formulation derivation architecture. m.s. implemented architecture performed experiments. m.s. p.s. a.w. performed data analysis. authors contributed writing paper editing paper.", "year": 2015}