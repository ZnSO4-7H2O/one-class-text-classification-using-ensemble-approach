{"title": "A Structured Self-attentive Sentence Embedding", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "text": "zhouhan lin‡∗ minwei feng cicero nogueira santos bing xiang bowen zhou yoshua bengio‡† watson ‡montreal institute learning algorithms universit´e montr´eal †cifar senior fellow lin.zhouhangmail.com {mfeng cicerons bingxia zhou}us.ibm.com paper proposes model extracting interpretable sentence embedding introducing self-attention. instead using vector matrix represent embedding matrix attending different part sentence. also propose self-attention mechanism special regularization term model. side effect embedding comes easy visualizing speciﬁc parts sentence encoded embedding. evaluate model different tasks author proﬁling sentiment classiﬁcation textual entailment. results show model yields signiﬁcant performance gain compared sentence embedding methods tasks. much progress made learning semantically meaningful distributed representations individual words also known word embeddings hand much remains done obtain satisfying representations phrases sentences. methods generally fall categories. ﬁrst consists universal sentence embeddings usually trained unsupervised learning includes skipthought vectors paragraphvector recursive auto-encoders sequential denoising autoencoders fastsent etc. category consists models trained speciﬁcally certain task. usually combined downstream applications trained supervised learning. generally ﬁnds speciﬁcally trained sentence embeddings perform better generic ones although generic ones used semi-supervised setting exploiting large unlabeled corpora. several models proposed along line using recurrent networks recursive networks convolutional networks intermediate step creating sentence representations solve wide variety tasks including classiﬁcation ranking common approach previous methods consists creating simple vector representation using ﬁnal hidden state pooling either rnns hidden states convolved n-grams. additional works also done exploiting linguistic structures parse dependence trees improve sentence representations tasks people propose attention mechanism lstm model introduce extra source information guide extraction sentence embedding however tasks like sentiment classiﬁcation directly applicable since extra information model given single sentence input. cases common pooling averaging step across time steps figure sample model structure showing sentence embedding model combined fully connected softmax layer sentiment analysis sentence embedding computed multiple weighted sums hidden states bidirectional lstm summation weights computed illustrated blue colored shapes stand hidden representations colored shapes stand weights annotations input/output. common approach many aforementioned methods consists creating simple vector representation using ﬁnal hidden state pooling either rnns hidden states convolved n-grams. hypothesize carrying semantics along time steps recurrent model relatively hard necessary. propose self-attention mechanism sequential models replace pooling averaging step. different previous approaches proposed self-attention mechanism allows extracting different aspects sentence multiple vector representations. performed lstm sentence embedding model. enables attention used cases extra inputs. addition direct access hidden representations previous time steps relieves long-term memorization burden lstm. side effect coming together proposed self-attentive sentence embedding interpreting extracted embedding becomes easy explicit. section details proposed self-attentive sentence embedding model well regularization term proposed model described section also provide visualization method sentence embedding section evaluate model author proﬁling sentiment classiﬁcation textual entailment tasks section proposed sentence embedding model consists parts. ﬁrst part bidirectional lstm second part self-attention mechanism provides summation weight vectors lstm hidden states. summation weight vectors dotted lstm hidden states resulting weighted lstm hidden states considered embedding sentence. combined with example multilayer perceptron applied downstream application. figure shows example proposed sentence embedding model applied sentiment analysis combined fully connected layer softmax layer. besides using fully connected layer also proposes approach prunes weight connections utilizing structure matrix sentence embedding detailed appendix section figure describe model. vector standing dimentional word embedding i-th word sentence. thus sequence represented matrix concatenates word embeddings together. shape n-by-d. encode variable length sentence ﬁxed size embedding. achieve choosing linear combination lstm hidden vectors computing linear combination requires self-attention mechanism. attention mechanism takes whole lstm hidden states input outputs vector weights weight matrix shape da-by-u. vector parameters size hyperparameter arbitrarily. since sized n-by-u annotation vector size tmax ensures computed weights lstm hidden states according weight provided vector representation input sentence. vector representation usually focuses speciﬁc component sentence like special related words phrases. expected reﬂect aspect component semantics sentence. however multiple components sentence together forms overall semantics whole sentence especially long sentences. thus represent overall semantics sentence need multiple focus different parts sentence. thus need perform multiple hops attention. want different parts extracted sentence regard this extend r-by-da matrix note resulting annotation vector becomes annotation matrix formally tmax performed along second dimension input. deem equation -layer without bias whose hidden unit numbers parameters ws}. embedding vector becomes r-by-u embedding matrix compute weighted sums multiplying annotation matrix lstm hidden states resulting matrix sentence embedding embedding matrix suffer redundancy problems attention mechanism always provides similar summation weights hops. thus need penalization term encourage diversity summation weight vectors across different hops attention. best evaluate diversity deﬁnitely kullback leibler divergence summation weight vectors. however found stable case. conjecture maximizing divergence optimizing annotation matrix sufﬁciently small even zero values different softmax output units vast amount zeros making training unstable. another feature doesn’t provide want want individual focus single aspect semantics want probability mass annotation softmax output focused. penalty cant encourage that. hereby introduce penalization term overcomes aforementioned shortcomings. compared divergence penalization term consumes third computation. product transpose subtracted identity matrix measure redundancy. stands frobenius norm matrix. similar adding regularization term penalization term multiplied coefﬁcient minimize together original loss dependent downstream application. let’s consider different summation vectors softmax entries within summation vector thus deemed probability masses discrete probability distribution. non-diagonal elements aij= matrix corresponds summation elementwise product distributions k-th element vectors respectively. extreme case overlap probability distributions correspond otherwise positive value. extreme distributions identical concentrates single word maximum value subtract identity matrix forces elements diagonal approximate encourages summation vector focus number words possible forcing vector focused single aspect elements punishes redundancy different summation vectors. interpretation sentence embedding quite straight forward existence annotation matrix sentence embedding matrix corresponding annotation vector element vector corresponds much contribution lstm hidden state token position contributes thus draw heat embedding matrix visualization gives hints encoded part embedding adding extra layer interpretation. second visualization achieved summing annotation vectors normalizing resulting weight vector since sums aspects semantics sentence yields general view embedding mostly focuses ﬁgure words embedding takes account ones skipped embedding. figure various supervised unsupervised sentence embedding models mentioned section different models proposed method uses self-attention mechanism allows extract different aspects sentence multiple vector-representations. matrix structure together penalization term gives model greater capacity disentangle latent information input sentence. also linguistic structures guide sentence representation model. additionally using method easily create visualizations help interpretation learned representations. recent work also proposed supervised methods intra/self-sentence attention. ling proposed attention based model word embedding calculates attention weight word possible position context window. however method cannot extended sentence level embeddings since cannot exhaustively enumerate possible sentences. proposes sentence level attention similar motivation done differently. utilize mean pooling lstm states attention source re-weight pooled vector representation sentence. apart previous variants want note proposed self attention mechanism question encoding factoid model concurrent work. difference lies encoding still presented vector attention produces matrix representation instead specially designed penalty term. applied model sentiment anaysis entailment model factoid lstmn model also proposed successful intra-sentence level attention mechanism later used parikh attention different granularities. lstmn produces attention vector hidden states recurrent iteration sort online updating attention. it’s ﬁne-grained targeting discovering lexical correlations certain word previous words. contrary attention mechanism performed once focuses directly semantics makes sense discriminating targets. less focused relations words semantics whole sentence word contributes computationally method also scales sentence length better since doesn’t require lstm compute annotation vector previous words time lstmn computes next step. ﬁrst evaluate sentence embedding model applying different datasets dataset yelp dataset stanford natural language inference corpus. datasets fall different tasks corresponding author proﬁling sentiment analysis textual entailment respectively. also perform exploratory experiments validate properties various aspects sentence embedding model. author proﬁling dataset consists twitter tweets english spanish dutch. tweets also provides gender user writing tweet. range split classes english tweets input tweets predict range user. since predicting users refer dataset rest paper. randomly selected tweets training development test set. performances also chosen classiﬁcation accuracy. compare model baseline models bilstm cnn. baseline models. bilstm model uses bidirectional lstm dimensions direction pooling across lstm hidden states sentence embedding vector -layer relu output hidden states output classiﬁcation result. model uses scheme substituting bilstm layer convolutional network. training dropout regularization. stochastic gradient descent optimizer learning rate batch size bilstm also clip model settings bilstm. also -layer relu output hidden units. addition self-attention hidden layer units choose matrix embedding rows coefﬁcient penalization term. train three models convergence select corresponding test performance according best development performance. results show model outperforms bilstm baselines signiﬁcant margin. choose yelp dataset sentiment analysis task. consists yelp reviews take review input predict number stars user wrote review assigned corresponding business store. randomly select review-star pairs training development test set. tokenize review texts stanford tokenizer. dimensional wordvec initialization word embeddings tune embedding training across experiments. target number stars integer number range inclusive. treating task classiﬁcation task i.e. classify review text classes. classiﬁcation accuracy measurement. baseline models setting used author proﬁling dataset except using batch size instead. model also using setting except choose hidden unit numbers output instead. also observe signiﬁcant performance gain comparining baselines. interpretation learned sentence embedding second visualization described section plot heat maps reviews dataset. randomly select examples negative positive reviews test model high conﬁdence predicting label. shown figure model majorly learns capture factors review indicate strongly sentiment behind sentence. short reviews model manages capture factors contribute extreme score longer reviews model still able capture related factors. example review figure seems focus spent single factor i.e. much model puts little amount attention points like highly recommend amazing food etc. biggest dataset textual entailment snli corpus evaluation task. snli collection human-written english sentence pairs manually labeled balanced classiﬁcation labels entailment contradiction neutral. model given pair sentences called hypothesis premise respectively asked tell semantics sentences contradicting not. also classiﬁcation task measure performance accuracy. process hypothesis premise independently extract relation sentence embeddings using multiplicative interactions proposed memisevic -layer relu output hidden units hidden representation classiﬁcation results. parameters bilstm attention shared across hypothesis premise. bilstm dimension direction attention hidden units instead sentence embeddings hypothesis premise rows penalization term coefﬁcient dimensional glove word embedding initialize word embeddings. adagrad optimizer learning rate don’t extra regularization methods like dropout normalization. training converges epochs relatively fast. task different previous tasks sentences input. bunch ways inter-sentence level attention attentions bring beneﬁts. make comparison focused fair compare methods fall sentence encoding-based models. i.e. information exchanged hypothesis premise encoded distributed encoding. lstm encoders bilstm encoders tree-based encoders spinn-pi encoders nti-slstm-lstm encoders encoders skipthoughts pre-training encoders method compared published approaches method shows signiﬁcant gain them except encoders state-of-the-art category. however different relatively small compared differences methods. since purpose introducing penalization term majorly discourage redundancy embedding ﬁrst directly visualize heat maps model presented sentence. compare identical models size detailed section trained separately dataset penalization term penalty. randomly select tweet test compare models plotting heat attention single tweet. since hops attention model makes plotting quite redundant plot them. hops already reﬂect situation hops. figure heat maps models trained dataset. left column trained without penalization term right column trained penalization. shows detailed attentions taken rows matrix embedding shows overall attention summing attention weight vectors. ﬁgure tell model trained without penalization term lots redundancies different hops attention resulting putting focus word relevant author. however right column model shows variations different hops result overall embedding focuses mail-replies spam instead. yelp dataset also observe similar phenomenon. make experiments explorative choose plot heat maps overall attention heat maps samples instead plotting detailed heat maps single sample again. figure shows overall focus sentence embedding three different reviews. observe penalization term model tends focused important parts review. think encouraging focused diagonals matrix validate differences result performance difference evaluate four models trained yelp datasets without penalization term. results shown table consistent expected models trained penalization term outperforms counterpart trained without. snli dataset although observe introducing penalization term still contributes encouraging diversity different rows matrix sentence embedding forcing network focused sentences quantitative effect penalization term obvious snli dataset. models yield similar test accuracies. multiple rows sentence embedding expected provide abundant information encoded content. makes sence evaluate signiﬁcant improvement brought taking models used snli dataset example vary task train resulting models independently note sentence embedding reduces normal vector form. figure effect number rows matrix sentence embedding. vertical axes indicates test accuracy horizontal axes indicates training epoches. numbers legends stand corresponding values conducted dataset conducted snli dataset. difference vector sentence embedding multiple vectors. models also quite invariant respect since ﬁgures wide range values generating comparable curves. paper introduced ﬁxed size matrix sentence embedding self-attention mechanism. attention mechanism interpret sentence embedding depth model. experimental results different tasks show model outperforms sentence embedding models signiﬁcant margin. introducing attention mechanism allows ﬁnal sentence embedding directly access previous lstm hidden states attention summation. thus lstm doesn’t need carry every piece information towards last hidden state. instead lstm hidden state expected provide shorter term context information around word higher level semantics requires longer term dependency picked directly attention mechanism. setting reliefs burden lstm carry long term dependencies. experiments also support that observed model bigger advantage contents longer. more notion summing elements attention mechanism primitive something complex that allow operations hidden states lstm. model able encode sequence variable length ﬁxed size representation without suffering long-term dependency problems. brings scalability model without modiﬁcation applied directly longer contents like paragraphs articles etc. though beyond focus paper remains interesting direction explore future work. downside proposed model current training method heavily relies downstream applications thus able train unsupervised way. major obstacle towards enabling unsupervised learning model decoding don’t know prior different rows embedding divided reorganized. exploring possible divisions using neural network could easily overﬁtting. although still unsupervised learning proposed model using sequential decoder sentence embedding merits structures decoder. authors would like acknowledge developers theano lasagne. ﬁrst author would also like thank watson providing resources fundings valuable discussions make project possible caglar gulcehre helpful discussions. samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. arxiv preprint arxiv. jianpeng cheng dong mirella lapata. long short-term memory-networks machine reading. conference empirical methods natural language processing association computational linguistics cicero santos maira gatti. deep convolutional neural networks sentiment analysis short texts. proceedings coling international conference computational linguistics technical papers minwei feng bing xiang michael glass lidan wang bowen zhou. applying deep learning answer selection study open task. ieee workshop automatic speech recognition understanding asru scottsdale december felix hill kyunghyun anna korhonen. learning distributed representations sentences proceedings conference north american chapter unlabelled data. association computational linguistics human language technologies diego california june association computational linguistics. http //www.aclweb.org/anthology/n-. peng zhengyan xuguang wang ying zhou dataset neural recurrent sequence labeling model open-domain factoid question answering. arxiv preprint arxiv. wang ling chu-cheng yulia tsvetkov silvio amir. contexts created equal better word representations variable attention. proceedings conference empirical methods natural language processing lisbon portugal september association computational linguistics. mingbo liang huang bing xiang bowen zhou. dependency-based convolutional neural networks sentence embedding. proceedings annual meeting association computational linguistics international joint conference natural language processing volume lili peng zhang jin. discriminative neural sentence modeling tree-based convolution. proceedings conference empirical methods natural language processing lisbon portugal september association computational linguistics. http//aclweb.org/anthology/d-. hamid palangi deng yelong shen jianfeng xiaodong jianshu chen xinying song rabab ward. deep sentence embedding using long short-term memory networks analysis application information retrieval. ieee/acm transactions audio speech language processing richard socher jeffrey pennington eric huang andrew christopher manning. semi-supervised recursive autoencoders predicting sentiment distributions. proceedings conference empirical methods natural language processing edinburgh scotland july association computational linguistics. http //www.aclweb.org/anthology/d-. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing volume citeseer ming cicero santos bing xiang bowen zhou. improved representation learning question answer matching. proceedings berlin germany august association computational linguistics. http//www.aclweb.org/anthology/ wenpeng hinrich sch¨utze. convolutional neural network paraphrase identiﬁcation. proceedings conference north american chapter association computational linguistics human language technologies side effect multiple vectors represent sentence matrix sentence embedding usually several times larger vector sentence embeddings. results needing parameters subsequent fully connected layer connects every hidden units every units matrix sentence embedding. actually example shown figure fully connected layer takes around percent parameters. table appendix going introduce weight pruning method which utilizing structure matrix embedding able drastically reduce number parameters fully connected hidden layer. inheriting notation used main paper matrix embedding shape fully connected hidden layer units. normal fully connected hidden layer require hidden unit connected every unit matrix embedding shown figure ends parameters total. however structures matrix embedding make matrix computed weighted lstm hidden states means share similarities reﬂect similarity fully connected layer split hidden states equally sized groups group units. i-th group fully connected i-th matrix representation. connections connects i-th group hidden units rows matrix pruned away. simillarity different rows matrix embedding reﬂected symmetry connecting type hidden layer. result hidden layer interperated also structute number size groups dimensions total number hidden units process prunes away weight values fairly large portion large. dimension another form similarity exists too. vector representation j-th element weighted lstm hidden unit different time steps. certain j-th element vector representations summed lstm hidden unit. also reﬂect similarity symmetry weight connections using pruning method above. thus another structured hidden states sized u-by-q noted figure table takes model yelp dataset concrete example compared number parameters part model pruning. pruning method drastically reduces model size. note structure adjusted freely hyperparameters. also continue corresponding pruning process again stack structured hidden layers like stacking fully connected layers. subsequent softmax layer fully connected i.e. unit softmax layer connected units problem since speed softmax largely dependent number softmax units changed.in addition applications like sentiment analysis textural entailment softmax layer tiny contains several units. experimental results three datasets shown that pruning mechanism lowers performances still allows three models perform comparable better models compared paper. section tested matrix sentence embedding model textual entailment task snli dataset. different former tasks textual entailment task consists pair sentences input. propose multiplicative interactions combine overall structure model snli dipicted figure hypothesis premise extract embeddings independently lstm attention mechanism. parameters part model shared comparing matrix embeddings corresponds green dashed rectangle part ﬁgure computes single matrix embedding factor semantic relation sentences. represent relation connected three-way multiplicative interaction. three-way multiplicative interaction value anyone function product others. type connection originally introduced extract relation images since computing factor relations corresponds encoder part factored gated autoencoder memisevic call gated encoder figure first multiply matrix embedding different weight matrix. repeating rows corresponds batched product matrix weight tensor. inheriting name call resulting matrix factor. batched hypothesis embedding premise embedding respectively.", "year": 2017}