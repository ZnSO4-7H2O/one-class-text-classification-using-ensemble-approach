{"title": "Parameter Space Noise for Exploration", "tag": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "stat.ML"], "abstract": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.", "text": "matthias plappert†‡ rein houthooft† prafulla dhariwal† szymon sidor† richard chen† chen†† tamim asfour‡ pieter abbeel†† marcin andrychowicz† openai karlsruhe institute technology university california berkeley correspondence matthiasopenai.com deep reinforcement learning methods generally engage exploratory behavior noise injection action space. alternative noise directly agent’s parameters lead consistent exploration richer behaviors. methods evolutionary strategies parameter perturbations discard temporal structure process require significantly samples. combining parameter noise traditional methods allows combine best worlds. demonstrate offon-policy methods beneﬁt approach experimental comparison ddpg trpo high-dimensional discrete action environments well continuous control tasks. exploration remains challenge contemporary deep reinforcement learning main purpose ensure agent’s behavior converge prematurely local optimum. enabling efﬁcient effective exploration however trivial since directed reward function underlying markov decision process although plethora methods proposed tackle challenge high-dimensional and/or continuous-action mdps often rely complex additional structures counting tables density modeling state space learned dynamics models self-supervised curiosity orthogonal increasing exploratory nature algorithms addition temporally-correlated noise example done bootstrapped along lines shown addition parameter noise leads better exploration obtaining policy exhibits larger variety behaviors discuss related approaches greater detail section main limitation however either proposed evaluated on-policy setting relatively small shallow function approximators disregard temporal structure gradient information paper investigates parameter space noise effectively combined off-the-shelf deep algorithms ddpg trpo improve exploratory behavior. experiments show form exploration applicable high-dimensional discrete environments continuous control tasks using onoff-policy methods. results indicate parameter noise outperforms traditional action space noise-based baselines especially tasks reward signal extremely sparse. consider standard framework consisting agent interacting environment. simplify exposition assume environment fully observable. environment modeled markov decision process deﬁned states actions distribution initial states reward function transition probabilities off-policy methods allow learning based data captured arbitrary policies. paper considers popular off-policy algorithms namely deep q-networks deep deterministic policy gradients policy implicitly deﬁned argmaxa∈aq). typically stochastic \u0001greedy boltzmann policy derived q-value function encourage exploration relies sampling noise action space. q-network predicts q-value action updated using off-policy data replay buffer. deep deterministic policy gradients ddpg actor-critic algorithm applicable continuous action spaces. similar critic estimates q-value function using off-policy data recursive bellman equation contrast off-policy algorithms on-policy methods require updating function approximators according currently followed policy. particular consider trust region policy optimization extension traditional policy gradient methods using natural gradient direction trust region policy optimization trpo improves upon reinforce computing ascent direction ensures small change policy distribution. speciﬁcally trpo solves following constrained optimization problem discounted state-visitation frequencies induced denotes advantage function estimated empirical return minus baseline step size parameter controls much policy allowed change iteration. applied arbitrary parametric models. achieve structured exploration sample policies applying additive gaussian noise parameter vector current policy importantly perturbed policy sampled beginning episode kept ﬁxed entire rollout. convenience readability denote perturbed policy state-dependent exploration pointed rückstieß crucial difference action space noise parameter space noise. consider continuous action space case. using gaussian action noise actions sampled according stochastic policy generating therefore even ﬁxed state almost certainly obtain different action whenever state sampled rollout since action space noise completely independent current state contrast parameters policy perturbed beginning perturbing deep neural networks immediately obvious deep neural networks potentially millions parameters complicated nonlinear interactions perturbed meaningful ways applying spherical gaussian noise. however recently shown salimans simple reparameterization network achieves exactly this. concretely layer normalization perturbed layers. normalizing across activations within layer perturbation scale used across layers even though different layers exhibit different sensitivities noise. adaptive noise scaling parameter space noise requires pick suitable scale problematic since scale strongly depend speciﬁc network architecture likely vary time parameters become sensitive noise learning progresses. additionally easy intuitively grasp scale action space noise harder understand scale parameter space. propose simple solution resolves aforementioned limitations easy straightforward way. achieved adapting scale parameter space noise time relating variance action space induces. concretely deﬁne distance measure perturbed non-perturbed policy action space adaptively increase decrease parameter space noise depending whether certain threshold parameter space noise off-policy methods off-policy case parameter space noise applied straightforwardly since deﬁnition data collected off-policy used. concretely perturb policy exploration train non-perturbed network data replaying parameter space noise on-policy methods parameter noise incorporated onpolicy setting using adapted policy gradient forth rückstieß policy gradient methods optimize eτ∼]. given stochastic policy expected return expanded using likelihood ratios re-parametrization trick existing state-of-the-art algorithms beneﬁt incorporating parameter space noise? parameter space noise exploring sparse reward environments effectively? parameter space noise exploration compare evolution strategies deep added value parameter space noise action space noise measured highdimensional discrete-action environments continuous control tasks. discrete environments comparisons made using ddpg trpo used continuous control tasks. discrete-action environments discrete-action environments arcade learning environment benchmark along standard implementation. compare baseline agent \u0001-greedy action noise version parameter noise. linearly anneal ﬁrst million timesteps. parameter noise adapt scale using simple heuristic increases scale divergence perturbed non-perturbed policy less divergence greedy \u0001-greedy policy decreases otherwise using approach achieve fair comparison action space noise parameter space noise since magnitude noise similar also avoid introduction additional hyperparameter. parameter perturbation found useful reparametrize network terms explicit policy represents greedy policy implied q-values rather perturbing qfunction directly. represent policy single fully connected layer convolutional part network followed softmax output layer. thus predicts discrete probability distribution actions given state. perturbing instead results meaningful changes since deﬁne explicit behavioral policy. setting q-network trained according standard practices. policy trained maximizing probability outputting greedy action accordingly current q-network. essentially policy trained exhibit behavior running greedy dqn. rule double-headed version alone exhibits signiﬁcantly different behavior always compare parameter space noise approach baselines regular two-headed \u0001-greedy exploration. furthermore randomly sample actions ﬁrst thousand timesteps cases replay buffer starting training. moreover found parameter space noise performs better combined action space noise full experimental details described section chose games varying complexity according taxonomy presented learning curves shown figure selection games agent trained frames. overall performance estimated running conﬁguration three different random seeds plot median return well interquartile range note performance evaluated exploratory policy since interested behavior especially. overall results show parameter space noise often outperforms action space noise especially games require consistency performs comparably remaining ones. additionally learning progress usually starts much sooner using parameter space noise. finally also compare double-headed version \u0001-greedy exploration ensure change architecture responsible improved exploration results conﬁrm. full results available appendix said parameter space noise unable sufﬁciently explore extremely challenging games like montezuma’s revenge. sophisticated exploration methods like bellemare likely necessary successfully learn games. however methods often rely form inner exploration method usually traditional action space noise. would interesting evaluate effect parameter space noise combined exploration methods. ﬁnal note proposed improvements like double prioritized experience replay dueling networks orthogonal improvements would therefore likely improve results further. leave experimental validation theory future work. continuous control environments compare parameter noise action noise continuous control environments implemented openai ddpg algorithm environments similar hyperparameters outlined original paper except fact layer normalization applied layer nonlinearity found useful either case especially important parameter space noise. compare performance following conﬁgurations noise uncorrelated additive gaussian action space noise correlated additive gaussian action space noise adaptive parameter space noise. case parameter space noise adapt scale resulting change action space comparable baselines uncorrelated gaussian action space noise evaluate performance several continuous control tasks. figure depicts results three exemplary environments. agent trained timesteps epoch consists thousand timesteps. order make results comparable conﬁgurations evaluate performance agent every thousand steps using noise episodes. halfcheetah parameter space noise achieves signiﬁcantly higher returns conﬁgurations. that environment exploration schemes quickly converge local optimum parameter space noise behaves similarly initially still explores options quickly learns break sub-optimal behavior. also notice parameter space noise vastly outperforms correlated action space noise environment clearly indicating signiﬁcant difference two. remaining environments parameter space noise performs exploration strategies. notice however even noise present ddpg capable learning good policies. representative remaining environments indicates environments require exploration begin well-shaped reward function. results trpo depicted figure interestingly walkerd environment adding parameter noise decreases performance variance seeds. indicates parameter noise aids escaping local optima. environments previous section required relatively little exploration. section evaluate whether parameter noise enables existing algorithms learn environments sparse rewards uncorrelated action noise generally fails scalable example ﬁrst evaluate parameter noise well-known problem following setup described osband closely possible. environment consists chain states agent always starts state either move left right. state agent receives small reward larger reward state obviously much easier discover small reward large reward increasing difﬁculty grows. environment described greater detail section compare adaptive parameter space noise bootstrapped \u0001-greedy dqn. chain length varied three different seeds trained evaluated. episode evaluate performance current policy performing rollout noise disabled problem considered solved hundred subsequent rollouts achieve optimal return. plot median number episodes problem considered solved full experimental details available section figure median number episodes considered solved different exploration strategies. green indicates problem solved whereas blue indicates solution found within episodes. note less number episodes solved better. figure shows parameter space noise clearly outperforms action space noise even outperforms computational expensive bootstrapped dqn. however important note environment extremely simple sense optimal strategy always right. case agent needs select different optimal action depending current state parameter space noise would likely work less well since weight randomization policy less likely yield behavior. results thus highlight difference exploration behavior compared action space noise speciﬁc case. general case parameter space noise guarantee optimal exploration. continuous control sparse rewards make continuous control environments challenging exploration. instead providing reward every timestep environments yield non-zero reward signiﬁcant progress towards goal. concretely consider following environments rllab modiﬁed according houthooft sparsecartpoleswingup yields reward paddle raised given threshold sparsedoublependulum yields reward agent reaches upright position sparsehalfcheetah yields reward agent crosses target distance sparsemountaincar yields reward agent drives hill swimmergather yields positive negative reward upon reaching targets. tasks time horizon steps resetting. consider ddpg trpo solve environments figure shows performance ddpg results trpo moved appendix overall performance estimated running conﬁguration different random seeds plot median return well interquartile range ddpg sparsedoublependulum seems easy solve general even noise ﬁnding successful policy relatively quickly. results sparsecartpoleswingup sparsemountaincar interesting here parameter space noise capable learning successful policies since forms noise including correlated action space noise never states nonzero rewards. sparsehalfcheetah ddpg least ﬁnds non-zero reward never learns successful policy signal. challenging swimmergather task conﬁgurations ddpg fail. results clearly show parameter space noise used improve exploration behavior off-the-shelf algorithms. however important note improvements exploration guaranteed general case. therefore necessary evaluate potential beneﬁt parameter space noise case-by-case basis. evolution strategies closely related approach since explore introducing noise parameter space lead improved exploration behavior however disregards temporal information uses black-box optimization train neural network. combining parameter space noise traditional algorithms include temporal information well rely gradients computed back-propagation optimization still beneﬁting improved exploratory behavior. compare traditional parameter space noise directly. compare performance games used section performance estimated running episodes seed using ﬁnal policy exploration disabled computing median returns. results obtained salimans obtained training frames. parameter space noise exploration previously described train frames. even though parameter space noise exposed times less data outperforms atari games combined previously described results demonstrates parameter space noise combines desirable exploration properties sample efﬁciency traditional problem exploration reinforcement studied extensively. range algorithms proposed guarantee near-optimal solutions number steps polynomial number states number actions horizon time. however many real-world reinforcements learning problems state action space continuous high dimensional that even discretization algorithms become impractical. context deep reinforcement learning large variety techniques proposed improve exploration however non-trivial implement often computational expensive. idea perturbing parameters policy proposed rückstieß policy gradient methods. authors show form perturbation generally outperforms random exploration evaluate exploration strategy reinforce natural actor-critic algorithms. however policies relatively lowdimensional compared modern deep architectures environments low-dimensional state spaces contribution strictly limited policy gradient case. contrast work also closely related evolution strategies schwefel especially neural evolution strategies glasmachers schaul wierstra context policy optimization work closely related kober peters sehnke recently salimans showed work high-dimensional environments like atari openai continuous control problems. however generally disregards temporal structure present trajectories typically suffers sample inefﬁciency. bootstrapped proposed directed consistent exploration using network multiple heads speciﬁc head selected beginning episode. contrast approach perturbs parameters network directly thus achieving similar simpler exploration behavior. concurrently work fortunato proposed similar approach utilizes parameter perturbations efﬁcient exploration. work propose parameter space noise conceptually simple effective replacement traditional action space noise like \u0001-greedy additive gaussian noise. work shows parameter perturbations successfully combined contemporary onoff-policy deep algorithms ddpg trpo often results improved performance compared action noise. experimental results demonstrate using parameter noise allows solving environments sparse rewards action noise unlikely succeed. results indicate parameter space noise viable interesting alternative action space noise still facto standard reinforcement learning applications. marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research ./jair.. http//dx.doi.org/./jair.. marc bellemare sriram srinivasan georg ostrovski schaul david saxton remi munos. unifying count-based exploration intrinsic motivation. advances neural information processing systems ronen brafman moshe tennenholtz. r-max general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research http//www. jmlr.org/papers/v/brafmana.html. greg brockman vicki cheung ludwig pettersson jonas schneider john schulman tang wojciech zaremba. openai gym. arxiv preprint arxiv. http//arxiv.org/abs/. duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continous control. proceedings international conference machine learning meire fortunato mohammad gheshlaghi azar bilal piot jacob menick osband alex graves vlad mnih remi munos demis hassabis olivier pietquin noisy networks exploration. arxiv preprint arxiv. tobias glasmachers schaul jürgen schmidhuber. natural evolution strategy multi-objective optimization. parallel problem solving nature ppsn international conference kraków poland september proceedings part ./----_ https//doi.org/./----_. tobias glasmachers schaul daan wierstra jürgen schmidhuber. exponential natural evolution strategies. genetic evolutionary computation conference gecco proceedings portland oregon july ./.. http//doi.acm.org/./.. rein houthooft chen chen duan john schulman filip turck pieter abbeel. advances neural information provime variational information maximizing exploration. cessing systems http//papers.nips.cc/paper/ -vime-variational-information-maximizing-exploration. jens kober peters. policy search motor primitives robotics. advances neural information processing systems http//papers.nips.cc/paper/ -policy-search-for-motor-primitives-in-robotics. timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. corr abs/. http//arxiv.org/abs/.. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature ./nature. http//dx.doi.org/./nature. osband charles blundell alexander pritzel benjamin roy. deep exploration bootstrapped dqn. advances neural information processing systems http//papers.nips.cc/paper/-deep-exploration-via-bootstrapped-dqn. osband benjamin zheng wen. generalization exploration randomized value functions. proceedings international conference machine learning icml http//jmlr.org/proceedings/papers/v/osband.html. georg ostrovski marc bellemare aäron oord rémi munos. count-based exploration neural density models. arxiv preprint arxiv. http//arxiv.org/abs/. thomas rückstieß martin felder jürgen schmidhuber. state-dependent exploration policy gradient methods. proceedings european conference machine learning knowledge discovery databases ecml/pkdd ./----_. http //dx.doi.org/./----_. salimans jonathan chen ilya sutskever. evolution strategies scalable alternative reinforcement learning. arxiv preprint arxiv. http//arxiv.org/abs/. schaul tobias glasmachers jürgen schmidhuber. high dimensions heavy tails natural evolution strategies. annual genetic evolutionary computation conference gecco proceedings dublin ireland july ./.. http//doi.acm.org/./.. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning icml lille france july http//jmlr.org/proceedings/papers/ v/schulman.html. frank sehnke christian osendorfer thomas rückstieß alex graves peters jürgen schmidhuber. parameter-exploring policy gradients. neural networks ./j.neunet... http//dx.doi.org/./j.neunet.... bradly stadie sergey levine pieter abbeel. incentivizing exploration reinforcement learning deep predictive models. arxiv preprint arxiv. http//arxiv.org/abs/. sainbayar sukhbaatar ilya kostrikov arthur szlam fergus. intrinsic motivation automatic curricula asymmetric self-play. arxiv preprint arxiv. http//arxiv.org/ abs/.. daan wierstra schaul jürgen schmidhuber. stochastic search using natural gradient. proceedings annual international conference machine learning icml montreal quebec canada june ./.. http //doi.acm.org/./.. daan wierstra schaul jürgen schmidhuber. efﬁcient natural evolution strategies. genetic evolutionary computation conference gecco proceedings montreal québec canada july ./.. http//doi.acm.org/. haoran tang rein houthooft davis foote adam stooke chen duan john schulman filip turck pieter abbeel. exploration study count-based exploration deep reinforcement learning. arxiv preprint arxiv. daan wierstra schaul tobias glasmachers peters jürgen schmidhuber. natural evolution strategies. journal machine learning research http//dl.acm.org/ citation.cfm?id=. ronald williams. simple statistical gradient-following algorithms connectionist reinforcement learning. machine learning ./bf. http//dx.doi.org/. /bf. network architecture described mnih used. consists convolutional layers followed hidden layer units followed linear output layer unit action. relus used layer layer normalization used fully connected part network. parameter space noise also include second head convolutional stack layers. head determines policy network architecture q-value network except softmax output layer. target networks updated every timesteps. q-value network trained using adam optimizer learning rate batch size replay buffer hold state transitions. \u0001-greedy baseline linearly anneal ﬁrst timesteps. parameter space noise adaptively scale noise similar effect action space effectively ensuring maximum divergence perturbed non-perturbed softly enforced. policy perturbed beginning episode standard deviation adapted described appendix every timesteps. notice perturb policy head convolutional part network avoid getting stuck also \u0001-greedy action selection cases perform random actions collect initial data replay buffer training starts. clip rewards clip gradients output layer within observations frame down-sampled pixels converted grayscale. actual observation network consists concatenation subsequent frames. additionally noop actions beginning episode. setup identical described mnih ddpg similar network architecture described lillicrap actor critic hidden layers relu units each. critic actions included second hidden layer. layer normalization applied layers. target networks soft-updated critic trained learning rate actor uses learning rate actor critic updated using adam optimizer batch sizes critic regularized using penalty replay buffer holds state transitions used. observation dimension normalized online estimate mean variance. parameter space noise ddpg adaptively scale noise comparable respective action space noise dense environments action space noise sparse environments action space noise trpo uses step size policy network hidden layers tanh units nonlocomotion tasks hidden layers tanh units locomotion tasks. hessian calculation subsampled factor batch size epoch timesteps. baseline learned linear transformation observations. chain environment follow state encoding proposed osband observation denotes indicator function. used simple network approximate q-value function consists hidden layers relu units. layer normalization used hidden layers applying nonlinearity. agent trained episodes. chain length varied three different seeds trained evaluated. episode performance current policy evaluated sampling trajectory noise disabled problem considered solved hundred subsequent trajectories achieve optimal episode return. figure depicts environment. compare adaptive parameter space noise bootstrapped \u0001-greedy adaptive parameter space noise single head perturb directly works well setting. parameter space noise adaptively scaled cases replay buffer holds state transitions learning starts initial episodes target network updated every timesteps network trained using adam optimizer learning rate batch size parameter space noise on-policy methods policy gradient methods optimize eτ∼]. given stochastic policy expected return expanded using likelihood ratios reparametrization trick parameter space noise requires pick suitable scale problematic since scale highly depend speciﬁc network architecture likely vary time parameters become sensitive learning progresses. additionally easy intuitively grasp scale action space noise harder understand scale parameter space. propose simple solution resolves aforementioned limitations easy straightforward way. achieved adapting scale parameter space noise time thus using time-varying scale furthermore related action space variance induces updated accordingly. concretely following simple heuristic update every timesteps denotes distance non-perturbed perturbed policy used rescale denotes threshold value. idea based levenberg-marquardt heuristic concrete distance measure appropriate choice depends policy representation. following sections outline choice methods behavioral policies. experiments always naïve distance measure pitfalls. example assume perturbed policy case naïve distance measure like norm would nonzero although policies andπ respectively) exactly equal. equally applies case policies applying softmax function predicted values denotes q-value i-th action.π deﬁned analogously uses perturbed instead using probabilistic relate distance measure \u0001-greedy action space noise allows fairly compare approaches also avoids need pick additional hyperparameter concretely divergence greedy policy argmaxaq) ddpg relate noise induced parameter space perturbations noise induced additive gaussian noise. following distance measure non-perturbed perturbed policy estimated batch states replay buffer denotes dimension action space easy show setting adaptive parameter space threshold thus results effective action space noise standard deviation regular gaussian action space noise. order scale noise trpo adapt sampled noise vectors computing natural step h−\u0001σ. essentially compute trust region around noise direction ensure concretely computed conjugate gradient algorithm combined line search along noise direction ensure constraint conformation described appendix schulman table compares ﬁnal performance frames ﬁnal performance \u0001-greedy exploration parameter space noise exploration frames. cases performance estimated running episodes exploration disabled. numbers reported salimans report median return across three seeds dqn. game alien amidar bankheist beamrider breakout enduro freeway frostbite gravitar montezumarevenge pitfall pong privateeye qbert seaquest solaris spaceinvaders tutankham venture wizardofwor zaxxon results invertedpendulum inverteddoublependulum noisy fact small change policy easily degrade performance signiﬁcantly thus hard read. interestingly adaptive parameter space noise achieves stable performance inverteddoublependulum. overall performance comparable exploration approaches. again noise either action parameter space achieves comparable results indicating environments combined ddpg well-suited test exploration. performance trpo noise scaled according parameter curvature deﬁned section shown figure trpo baseline uses action noise using policy network outputs mean gaussian distribution variance learned. results show adding parameter space noise aids either learning much consistently challenging sparse environments.", "year": 2017}