{"title": "Attending to Characters in Neural Sequence Labeling Models", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.", "text": "sequence labeling architectures word embeddings capturing similarity suffer handling previously unseen rare words. investigate character-level extensions models propose novel architecture combining alternative word representations. using attention mechanism model able dynamically decide much information wordcharacter-level component. evaluated different architectures range sequence labeling datasets character-level extensions found improve performance every benchmark. addition proposed attention-based architecture delivered best results even smaller number trainable parameters. many tasks including named entity recognition part-of-speech tagging shallow parsing framed types sequence labeling. development accurate efﬁcient sequence labeling models thereby useful wide range downstream applications. work area traditionally involved task-speciﬁc feature engineering example integrating gazetteers named entity recognition using features morphological analyser pos-tagging. recent developments neural architectures representation learning opened door models discover useful features automatically data. sequence labeling systems applicable many tasks using surface text input able achieve competitive results current neural models generally make word embeddings allow learn similar representations semantically functionally similar words. important improvement count-based models still weaknesses addressed. obvious problem arises dealing out-of-vocabulary words token never seen before embedding model needs back-off generic representation. words seen infrequently embeddings likely quality lack training data. approach also sub-optimal terms parameter usage example certain sufﬁxes indicate likely tags words information gets encoded individual embedding opposed shared whole vocabulary. paper construct task-independent neural network architecture sequence labeling extend different approaches integrating character-level information. operating individual characters model able infer representations previously unseen words share information morpheme-level regularities. propose novel architecture combining character-level representations word embeddings using gating mechanism also referred attention allows model dynamically decide source information word. addition describe objective model training character-level representations optimised mimic current state word embeddings. evaluate neural models datasets ﬁelds pos-tagging chunking error detection learner texts. experiments show including character-based component sequence labeling model provides substantial performance improvements benchmarks. addition attention-based architecture achieves best results evaluations requiring smaller number parameters. ﬁrst describe basic word-level neural network sequence labeling following models described lample yannakoudakis propose alternative methods incorporating character-level information. figure shows general architecture sequence labeling network. model receives sequence tokens input predicts label corresponding input tokens. tokens ﬁrst mapped distributed vector space resulting sequence word embeddings next embeddings given input lstm components moving opposite directions text creating context-speciﬁc representations. respective forwardbackward-conditioned representations concatenated word position resulting representations conditioned whole sequence include extra narrow hidden layer lstm proved useful modiﬁcation based development experiments. additional hidden layer allows model detect higher-level feature combinations constraining small forces focus generalisable patterns finally produce label predictions either softmax layer conditional random ﬁeld softmax calculates normalised probability distribution possible labels word probability label t-th word possible labels k-th output weight matrix optimise model minimise categorical crossentropy equivalent minimising negative log-probability correct labels following huang also output layer conditions prediction previously predicted label. architecture last hidden layer used predict conﬁdence scores word possible labels. separate weight matrix used learn transition probabilities different labels viterbi algorithm used optimal sequence weights. given sequence labels score sequence calculated figure neural sequence labeling model. word embeddings given input; bidirectional lstm produces context-dependent representations; information passed hidden layer output layer. outputs either probability distributions softmax conﬁdence scores crf. atyt shows conﬁdent network label t-th word bytyt+ shows likelihood transitioning label label values optimised training. output model sequence labels largest score found efﬁciently using viterbi algorithm. order optimise model loss function maximises score correct label sequence minimising scores sequences distributed embeddings words space semantically similar words similar vector representations allowing models generalise better. however still treat words atomic units ignore surfacemorphological similarities different words. constructing models operate individual characters word take advantage regularities. particularly useful handling unseen words example never seen word cabinets before character-level model could still infer representation word previously seen word cabinet words sufﬁx contrast word-level model represent word generic out-of-vocabulary representation shared unseen words. research character-level models still fairly early stages models operate exclusively characters competitive word-level models tasks. however instead fully replacing word embeddings interested combining approaches thereby allowing model take advantage information granularity levels. general outline approach shown figure word broken individual characters mapped sequence character embeddings passed bidirectional lstm figure left concatenation-based character architecture. right attention-based character architecture. dotted lines indicate vector concatenation. approach also illustrated figure assumes word-level character-level components learn somewhat disjoint information beneﬁcial give separately input sequence labeler. alternatively word embedding character-level component learn semantic features word. instead concatenating alternative feature sets speciﬁcally construct network would learn representations allow model decide combine information speciﬁc word. ﬁrst construct word representation characters using architecture bidirectional lstm operates characters last hidden states used create vector input word. instead concatenating word embedding vectors added together using weighted weights predicted two-layer network weight matrices calculating logistic function values range vector dimensions acting weight vectors. allows model dynamically decide much information character-level component word embedding. decision done feature separately adds extra ﬂexiblity example words regular sufﬁxes share character-level features whereas irregular words store exceptions word embeddings. furthermore previously unknown words able character-level regularities whenever possible still able revert using generic token necessary. main beneﬁts character-level modeling expected come improved handling rare unseen words whereas frequent words likely able learn high-quality word-level embeddings directly. would like take advantage this train character component predict word embeddings. attention-based architecture requires learned features word representations align extra constraint encourage this. training term loss function optimises vector similar word embedding equation maximises cosine similarity importantly done words out-of-vocabulary want character-level component learn word embeddings exclude embedding shared many words. cost component tokens. character component learns general regularities shared words individual word embeddings provide model store word-speciﬁc information exceptions. therefore want character-based model shift towards predicting high-quality word embeddings desireable optimise word embeddings towards character-level representations. achieved making sure optimisation performed direction; theano disconnected grad function gives desired effect. conll conll- dataset frequently used benchmark task chunking. wall street journal sections penn treebank used training section test data. ofﬁcial development separated training purpose. ptb-pos penn treebank pos-tag corpus contains texts wall street journal annotated part-of-speech tags. label includes main tags additional tags covering items punctuation. fcepublic publicly released subset first certiﬁcate english dataset contains short essays written language learners manual corrections examiners version corpus converted binary error detection task token labeled correct incorrect given context. bcgm biocreative gene mention corpus consists sentences biomedical publication abstracts annotated mentions names genes proteins related entities using single class. chemdner biocreative chemical drug corpus consists abstracts annotated mentions chemical drug names using single class. make ofﬁcial splits provided shared task organizers. jnlpba jnlpba corpus consists biomedical abstracts annotated mentions entity types cell line cell type protein. corpus derived genia corpus entity annotations shared task organized conjuction bionlp workshop. genia-pos genia corpus widely used resources biomedical rich annotations including parts speech phrase structure syntax entity mentions events. here make genia annotations cover pubmed abstracts -document test tsuruoka additionally split sample remaining documents development set. data prepocessing digits replaced character words occurred training data replaced generic token word embeddings still used character-level components. word embeddings initialised publicly available pretrained vectors created using wordvec ﬁne-tuned model training. general-domain datasets used -dimensional vectors trained google news; biomedical datasets used -dimensional vectors trained pubmed pmc. embeddings characters length initialised randomly. lstm layer size direction wordcharacter-level components. hidden layer size combined representation length word embeddings. used output layer experiments found gave beneﬁts tasks larger numbers possible labels. parameters optimised using adadelta default learning rate sentences grouped batches size performance development measured every epoch training stopped performance improved epochs; best-performing model development used evaluafigure visualisation attention values words trained ptb-pos dataset. darker blue indicates features higher weights character-level representation. restructuring present vocabulary bankrupting oov. tion test set. order avoid outlier results randomness model initialisation trained conﬁguration different random seeds present averaged results. evaluating dataset report measures established previous work. token-level accuracy used ptb-pos genia-pos; score erroneous words fcepublic; ofﬁcial evaluation script bcgm allows alternative correct entity spans; microaveraged mention-level score remaining datasets. optimising hyperparameters dataset separately would likely improve individual performance conduct controlled experiments task-independent model. therefore hyperparameters section datasets development used stopping condition. experiments wish determine sequence labeling tasks character-based models offer advantange character-based architecture performs better. results different model architectures datasets shown table seen including character-based component sequence labeling architecture improves performance every benchmark. datasets largest absolute improvement model able learn character-level patterns names also improve handling previously unseen tokens. compared concatenating wordcharacter-level representations attention-based character model outperforms former evaluations. mechanism dynamically deciding much character-level information allows model better handle individual word representations giving advantage experiments. visualisation attention values figure shows model actively using character-based features attention areas vary different words. results general tagging architecture competitive even compared previous work using hand-crafted features. network achieves ptb-pos compared huang jnlpba compared zhou cases also able beat previous best results bcgm compared campos fcepublic compared yannakoudakis lample report considerably higher result conll indicating chosen hyperparameters baseline system suboptimal speciﬁc task. compared experiments presented here model used iobes tagging scheme instead original embeddings pretrained specialised method accounts word order. important also compare parameter counts alternative neural architectures shows learning capacity indicates time requirements practice. table contains parameter counts three representative datasets. keeping model hyperparameters constant character-level models require additional parameters character composition character embeddings. however attention-based model uses fewer parameters compared concatenation approach. representations concatenated overall word representation size increased turn increases number parameters required word-level bidirectional lstm. therefore attention-based character architecture achieves improved results even smaller parameter footprint. table comparison trainable parameters neural model architectures. total shows total number parameters; noemb shows parameter count excluding word embeddings small fraction embeddings utilised every iteration. wide range previous work constructing optimising neural architectures applicable sequence labeling. collobert described ﬁrst task-independent neural tagging models using convolutional neural networks. able achieve good results tagging chunking semantic role labeling without relying hand-engineered features. irsoy cardie experimented multi-layer bidirectional elman-style recurrent networks found deep models outperformed conditional random ﬁelds task opinion mining. huang described bidirectional lstm model layer included hand-crafted features specialised task named entity recognition. yannakoudakis evaluated range neural architectures including convolutional recurrent networks task error detection learner writing. word-level sequence labeling model described paper follows previous work combining useful design choices them. addition extended model alternative character-level architectures evaluated performance different datasets. character-level models potential capturing morpheme patterns thereby improving generalisation frequent unseen words. recent years increase research models resulting several interesting applications. ling described character-level neural model machine translation performing encoding decoding individual characters. implemented language model encoding performed convolutional network lstm characters whereas predictions given word-level. proposed method learning word embeddings morphological segmentation bidirectional recurrent network characters. also research performing parsing text classiﬁcation character-level neural models. ling proposed neural architecture replaces word embeddings dynamically-constructed characterbased representations. applied similar method operating characters combined word embeddings instead replacing them allows model beneﬁt approaches. lample described model character-level representation combined word embeddings concatenation. work proposed alternative architecture representations combined using attention mechanism evaluated approaches range tasks datasets. recently miyamoto also described related method task language modelling combining characters word embeddings using gating. developments neural network research allow model architectures work well wide range sequence labeling datasets without requiring hand-crafted data. word-level representation learning powerful tool automatically discovering useful features models still come certain weaknesses rare words low-quality representations previously unseen words cannot modeled morpheme-level information shared whole vocabulary. paper investigated character-level model components sequence labeling architecture allow system learn useful patterns sub-word units. addition bidirectional lstm operating words separate bidirectional lstm used construct word representations individual characters. proposed novel architecture combining character-based representation word embedding using attention mechanism allowing model dynamically choose information information source. addition character-level composition function augmented novel training objective optimising predict representations similar word embeddings model. evaluation performed different sequence labeling datasets covering range tasks domains. found incorporating character-level information model improved performance every benchmark indicating capturing features regarding characters morphmes indeed useful general-purpose tagging system. addition attention-based model combining character representations outperformed concatenation method used previous work evaluations. even though proposed method requires fewer parameters added ability controlling much character-level information used word improved performance range different tasks. references miguel ballesteros chris dyer noah smith. improved transition-based parsing modeling characters instead words lstms. proceedings conference empirical methods natural language processing james bergstra olivier breuleux frederic fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math compiler python. proceedings python scientiﬁc computing conference jin-dong tomoko ohta yoshimasa tsuruoka yuka tateisi nigel collier. introduction bio-entity recognition task jnlpba. proceedings international joint workshop natural language processing biomedicine applications. john lafferty andrew mccallum fernando pereira. conditional random ﬁelds probabilistic models segmenting labeling sequence data. proceedings international conference machine learning. wang ling tiago lu´ıs lu´ıs marujo ram´on fernandez astudillo silvio amir chris dyer alan black isabel trancoso. finding function form compositional character models open vocabulary word representation. proceedings conference empirical methods natural language processing. tomoko ohta yuka tateisi jin-dong kim. genia corpus annotated research abstract corpus molecular biology domain. proceedings second international conference human language technology research. marek helen yannakoudakis. compositional sequence labeling models error detection learner writing. proceedings annual meeting association computational linguistics. larry smith lorraine tanabe johnson ando cheng-ju i-fang chung chun-nan yu-shi roman klinger christoph friedrich kuzman ganchev manabu torii hongfang barry haddow craig struble richard povinelli andreas vlachos william baumgartner lawrence hunter carpenter richard tzong-han tsai hong-jie feng yifei chen chengjie sophia katrenko pieter adriaans christian blaschke rafael torres mariana neves preslav nakov anna divoli manuel ma˜na-l´opez jacinto mata john wilbur. overview biocreative gene mention recognition. genome biology suppl erik tjong sang sabine buchholz. introduction conll- shared task chunking. proceedings workshop learning language logic conference computational natural language learning erik tjong sang fien meulder. introduction conll- shared task languageindependent named entity recognition. proceedings seventh conference natural language learning hlt-naacl yoshimasa tsuruoka yuka tateishi dong tomoko ohta john mcnaught sophia ananiadou jun’ichi tsujii. developing robust part-of-speech tagger biomedical text. proceedings panhellenic conference informatics. helen yannakoudakis briscoe medlock. dataset method automatically grading esol texts. proceedings annual meeting association computational linguistics human language technologies.", "year": 2016}