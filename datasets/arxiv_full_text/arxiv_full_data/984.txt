{"title": "OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based  Neural Training", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this paper we consider the training of single hidden layer neural networks by pseudoinversion, which, in spite of its popularity, is sometimes affected by numerical instability issues. Regularization is known to be effective in such cases, so that we introduce, in the framework of Tikhonov regularization, a matricial reformulation of the problem which allows us to use the condition number as a diagnostic tool for identification of instability. By imposing well-conditioning requirements on the relevant matrices, our theoretical analysis allows the identification of an optimal value for the regularization parameter from the standpoint of stability. We compare with the value derived by cross-validation for overfitting control and optimisation of the generalization performance. We test our method for both regression and classification tasks. The proposed method is quite effective in terms of predictivity, often with some improvement on performance with respect to the reference cases considered. This approach, due to analytical determination of the regularization parameter, dramatically reduces the computational load required by many other techniques.", "text": "auniversity turin dep. computer sciences c.so svizzera torino italy bnational institute astrophysics astrophys. observ. torino pino t.se italy paper consider training single hidden layer neural networks pseudoinversion which spite popularity sometimes aﬀected numerical instability issues. regularization known effective cases introduce framework tikhonov regularization matricial reformulation problem allows condition number diagnostic tool identiﬁcation instability. imposing well-conditioning requirements relevant matrices theoretical analysis allows identiﬁcation optimal value regularization parameter standpoint stability. compare value derived cross-validation overﬁtting control optimisation generalization performance. test method regression classiﬁcation tasks. proposed method quite eﬀective terms predictivity often improvement performance respect reference cases considered. approach analytical determination regularization parameter dramatically reduces computational load required many techniques. past decades single layer feedforward neural networks training mainly accomplished iterative algorithms involving repetition learning steps aimed minimising error functional space researchers therefore always motivated explore alternative algorithms recently techniques based matrix inversion developed. literature initially employed train radial basis function neural networks idea using also diﬀerent neural architectures suggested instance work huang gave rise great interest neural network community presented technique extreme learning machine slfns randomly chosen input weights hidden layer biases learn sets observations desired precision provided activation functions hidden layer inﬁnitely diﬀerentiable. besides linear output neurons output weights determination brought back linear systems solution obtained moore-penrose generalised inverse hidden layer output matrix; iterative training required. techniques appear anyway require hidden units respect conventional neural network training algorithms achieve comparable accuracy discussed deng many application-oriented studies last years devoted single-pass techniques easy implement computationally fast; described e.g. yearly conference currently held subject international conference extreme learning machines method currently dealt journal special issue e.g. soft computing international journal uncertainty fuzziness knowledge-based systems possible presence singular almost singular matrices pseudoinversion known powerful numerically unstable method nonetheless neural network community often used without singularity checks evaluated approximated methods. paper improve theoretical framework using singular value analysis detect occurrence instability. building tikhonov regularization known eﬀective context present technique named optimally conditioned regularization pseudoinversion replaces unstable ill-posed problems approach based formal deﬁnition matricial formulation allows condition number diagnostic tool. context optimal value regularization parameter analytically derived imposing well-conditioning requirements relevant matrices. issue regularization parameter choice often identiﬁed crucial literature dealt number historical contributions conservative guess might published estimates several dozens. relevant works mentioned section related theoretical background recalled. determination mainly aimed overﬁtting control often done either experimentally cross-validation requiring heavy computational training procedures analytically speciﬁc conditions matrices involved sometimes hardly applicable real datasets discussed section section diagnosis control tool tested applications selected database validated comparison framework regularized cross-validation unregularized one. datasets used section test technique eﬀectiveness performance compared obtained regularized frameworks originated statistical neural domains. stated introduction pseudoinversion based neural training brings back output weights determination linear systems solution section recall general ideas issue next sections specialized deal slfn training. matrix; random expectation value zero variance role ordinary ridge regression estimator alternative estimator presence multicollinearity deeply analized. statistics multicollinearity phenomenon predictor variables multiple regression model highly correlated meaning linearly predicted others non-trivial degree accuracy. situation coeﬃcient estimates multiple regression change erratically response small changes model data. known literature exist estimates smaller mean square error unbiased gauss-markov estimate particular applies expressions ridge parameter proposed share characteristic functions ratio function used comparison proposed method section next section show problem ﬁnding good solution applies context pseudoinversion based neural training specializing involved relevant matricies deal issue. deal standard slfn input neurons hidden neurons output neurons non-linear activation functions hidden layer linear activation functions output layer. considering dataset distinct training samples learning process slfn aims producing matrix desired outputs rn×q matrix input instances rn×l presented input. stated introduction pseudoinverse approach matrix input weights hidden layer biases randomly chosen longer modiﬁed name ﬁxed hidden layer output matrix rn×n rm×m orthogonal matrices rn×m rectangular diagonal matrix elements called singular values non-negative. common convention list singular values descending order i.e. also often done practice computational reasons elements smaller predeﬁned threshold thus actually computing approximated version pseudoinverse matrix approach example used default pseudoinverse evaluation means matlab pinv function tool widely used many scientists example context time applied blindly i.e. without decided threshold zero small approximation priori uncontrolled introduced evaluation. another important property generalization performance training examples must good indicator performance future examples diﬀerence must small. algorithm guarantees good generalization predicts well empirical error small. introduced statistical form leave-one-out stability named eeeloo building cross-validation leave-one-out stability endowed conditions stability expected empirical errors; demonstrated condition necessary suﬃcient generalization consistency class empirical risk minimization learning algorithms also suﬃcient condition generalisation algorithms turn original instable ill-posed problem well-posed regularization methods form often used among them tikhonov regularization common minimises error functional besides bousquet elisseeﬀ proposed notion uniform stability characterize generalization properties algorithm. results state tikhonov regularization algorithms uniformly stable uniform stability implies good generalization regularization thus introduces penalty function improves stability making problem less sensitive initial conditions also important contain model complexity avoiding overﬁtting. idea penalizing square function weights also well known neural literature weight decay wide amount articles devoted argument generally advantage regularization control overﬁtting. among recall remark diﬀerence minima regularized unregularized error functionals. increasing values regularization parameter induce larger larger departure former latter thus regularization process increases bias approximating solution reduces variance discussed bias-variance dilemma section suitable value tikhonov parameter therefore derive compromise suﬃciently large control approaching zero avoiding excess penalty term eq.. tuning therefore crucial. matrix norm. columns linearly independent e.g. case experimental data matrices left inverse i.e. cauchy-schwarz inequality case provides besides largest smallest singular values respectively. easily understand large condition numbers suggest presence small singular values whose numerical inversion required evaluate unregularized solution cause instability. numeric linear algebra also know condition number large problem ﬁnding least-squares solutions corresponding system linear equations ill-posed i.e. even small perturbation data lead huge perturbations entries solution according stability tikhonov regularization algorithms also characterized using classical notion condition number proposed regularization method within context. speciﬁcally aims analitically determining value parameter minimizes conditioning regularized hidden layer output matrix solution stable sense experimental results presented sections evidence quest stable solutions allows also achieve good generalization predictivity. comparison made purpose performance obtained determined standard cross-validation approach aimed overﬁtting control generalization performance optimization. regularized matricial framework makes easier comparison properties corresponding unregularized matrix fact unregularized pseudoinversion used nothing prevents occurrence small singular values make numerically instable evaluation contrary even presence small values original unregularized problem careful choice parameter allows tune singular values regularized matrix preventing numerical instability. bearing mind well-conditioned problems characterized small condition numbers look parameter values which three cases above make regularized condition number smaller. performance assessed statistics diﬀerent extractions input weigths computing either average rmse average percentage misclassiﬁcation rate test set. either quantity labeled tables summarising results. error standard deviation also computed evidence dispersion experimental results. regression abalone regression machine delta ailerons regression regression housing classiﬁcation iris diabetes classiﬁcation classiﬁcation wine segment classiﬁcation regularization strategy labeled optimally conditioned regularization pseudoinversion veriﬁed simulation common approach cross-validation used determine regularization parameter ﬁxed high number hidden neurons perform also hidden neurons number optimization respectively sec. section compare ocrep regularization approach selected cross-validation scheme typically used control under/overﬁtting optimization model generalization performance. split training test applied; then three-fold cross-validation search training identiﬁes best best performance validation values sake comparison ﬁxed high number hidden units used selected according dimension complexity datasets. three datasets machine iris wine simulation performed hidden neurons; abalone delta ailerons housing diabetes neurons; segment units. figure test error trends regression datasets function values selected cross-validation range cross-validation selected black square; proposed ocrep blue circle. standard deviation shown error bar. proposed optimal evidenced blue circle whereas value selected cross-validation shown black square. results case related highest number neurons experimented. figure test error trends classiﬁcation datasets function values selected cross-validation range cross-validation selected black square; proposed ocrep blue circle. table comparison ocrep cross-validation ﬁxed number hidden neurons medium size datasets. delta ailerons average errors standard deviations multiplied thus example iris best performance achieved using neurons ocrep neurons cross-validation. cases e.g. wine clear winner statistical considerations i.e. best results comparable within errors. results appears cross-validation better test error performance number datasets slightly higher ﬁxed number hidden neurons. however important evidence ocrep allows save hundreds pseudoinversion steps required cross-validation wich crucial issue practical implementation. order pursue double performance hidden units optimization ﬁrst interesting step give look variation function hidden layer dimension error trends unregularized models average test error values ocrep sigm-unreg function number hidden nodes gradually increased unity steps. cases initial decrease sigm-unreg test error increases signiﬁcantly. split training test applied; perform three-fold cross-validation selection number hidden neurons minimum error recorded cases. test errors evaluated average diﬀerent random choices input weights. proposed regularization technique provides regression datasets performance comparable cross-validation option always better performance respect unregularized case. classiﬁcation datatsets three cases four ocrep provides better performance respect cross-validation always better performance respect sigm-unreg case. cases statistical signiﬁcance level. goal optimal analytic determination regularization parameter results dramatic improvement computing requirements respect experimental tuning search pre-deﬁned large grid tentative values. latter case choice selected range least pseudoinversion required every output weight determination thus increasing computational load factor besides method designed explicitly optimal conditioning. simulations verify goal fulﬁlled evaluating average condition numbers hidden layer output matrices. statistics performed diﬀerent conﬁgurations input weights ﬁxed number hidden units namely largest used section dataset. results summarised tables respectively regression classiﬁcation datasets. ﬁrst table list ratio average condition numbers matrices associated respectively ocrep sigm-unreg i.e. regularized unregularized approaches. second surprisingly regularization method provides signiﬁcant improvement conditioning respect unregularized approach evidenced ratio values much smaller unity. besides ocrep also provides better conditioned matrices derived selection cross-validation since corresponding condition numbers systematically smaller former case sometimes order magnitude. since literature provides host diﬀerent recipes either choice regularization parameter actual regularization algorithm herefocus couple speciﬁc frameworks. described eqs. comparison method. main motivation choice independence estimate error variance characteristic shared case. dataset select ﬁxed numbers hidden units section evaluate mean standard deviation corresponding regularized test error reported tables also remind tabulated error either average rmse regression tasks average misclassiﬁcation rate classiﬁcation tasks; corresponding standard deviation. performance comparison based statistical signiﬁcance level. remark cases listed tab. ocrep provides statistically better results gcv. situation medium size datasets evidences somewhat mixed behaviour hidden neurons wins; neurons three four datasets performance statistically comparable. cases tab. ocrep provides better statistical results gcv. experimentation made regression datasets theoretical background works referred section directly applies case quantity column matrix. formulation desired target one-column matrix regression tasks. step pseudoinversion regularized method corresponding value. evaluate mean standard deviation regularized test errors reported respectively tables remark method kibria obtains better performance cases sixteen ocrep cases sixteen. besides method hoerl kennard obtains better performance three cases table kibria estimate ridge parameter results ﬁxed number hidden neurons regression datasets. delta ailerons average errors standard deviations multiplied noted respect processing requirements ocrep clear advantages since requires step determination methods require full spectral decomposition additional matrix inversion. whose technique extreme learning machine uses cost parameter considered related inverse regularization parameter authors state order achieve good generalization performance needs chosen appropriately. trying deng propose regularized extreme learning machine wich regularization parameter selected according similar criterion among values cause performance optimized respect number hidden table estimate ridge parameter results ﬁxed number hidden neurons regression datasets. delta ailerons average errors standard deviations multiplied neurons sake comparison ocrep values table obtain statistically signiﬁcant better performance dataset segment diabetes method relm performs better comparing results common regression datasets alternative method trop-elm proposed miche note ocrep achieves always lower rmse values seen table networks using regularized regression methods crucial step regularization parameter determination solved creating diﬀerent models based diﬀerent value parameter among best selected using bayesian information criterion. authors state typical value thus heavy computational load required method focused regression tasks. context regularization techniques single hidden layer neural networks trained pseudoinversion provide optimal value regularization parameter analytic derivation. achieved deﬁning convenient regularized matricial formulation framework singular value decomposition regularization parameter derived under constraint condition number minimization. ocrep method tested datasets regression classiﬁcation tasks. cases regularization implemented using analytically derived proven eﬀective terms predictivity evidenced comparison implementations approaches literature including cross-validation. ocrep avoids hundreds pseudoinversions usually needed methods i.e. quite computationally attractive. activity partially carried context visiting professor program gruppo nazionale calcolo scientiﬁco italian istituto nazionale alta matematica work partially supported contracts i///- --r..", "year": 2015}