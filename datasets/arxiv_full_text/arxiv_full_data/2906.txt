{"title": "Generating Images Part by Part with Composite Generative Adversarial  Networks", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "Image generation remains a fundamental problem in artificial intelligence in general and deep learning in specific. The generative adversarial network (GAN) was successful in generating high quality samples of natural images. We propose a model called composite generative adversarial network, that reveals the complex structure of images with multiple generators in which each generator generates some part of the image. Those parts are combined by alpha blending process to create a new single image. It can generate, for example, background and face sequentially with two generators, after training on face dataset. Training was done in an unsupervised way without any labels about what each generator should generate. We found possibilities of learning the structure by using this generative model empirically.", "text": "figure examples generated images cgan. images generated three generators ﬁnal output. similar real images shown comparison. black white checkerboard default background transparent images. maximum likelihood estimation pixel-wise loss functions. simultaneously trains models generator tries generate real images discriminator classiﬁes real images come training data fake images come discriminator alleviates lack semantic consideration pixel-wise loss function used auto-encoder models. proven enough capacity data distribution formed converges distribution real data practice however convergence intractable easy overﬁt exponential complexity images multiple objects exist position noisy features. solve issues propose composite generative adversarial network generate images part part instead whole images directly. cgan differs recurrent generative models sequence generated images intermixing image overlapping areas. address problem used alpha channel opacity along channels stack images iteratively alpha blending process. alpha blending process maintains previous image areas overlaps image entirely areas. instance given transparent image model snowy background ﬁrst later trees characters sequentially shown figure image generation remains fundamental problem artiﬁcial intelligence speciﬁcally deep learning. generative adversarial network architecture successful generating high-quality samples natural images. propose model called composite generative adversarial network disentangles complicated factors images multiple generators generator generates part image. parts combined alpha blending process create single image. example generate background face hair sequentially three generators trained face images. supervision generator generate. cgan assigns roles generator factorizing common factors images creates realistic samples good gan. also combined variational autoencoder cgan visualize sub-manifolds latent space learned. several obimages structure various jects styles shapes. deep learning models used patterns build distributed feature representations solve classiﬁcation generation problems using large datasets. lots classiﬁcation tasks focused abstraction ﬁnite number labels generation tasks need reconstruct images inversely latent variables. achieve task latent variables generative models must contain detailed information images case feature vectors discriminative models. gives interesting challenges generation task fundamental problem artiﬁcial intelligence. even though easily imagine scene combining mixing semantic parts current generative models reaching abilities. based deep neural networks successful unsupervised learning models generate samples natural images generalized training data. provides alternative intractable restricted boltzmann machines deep boltzmann machines provided pre-training deep neural networks. deep belief networks variants hybrid models pre-trained dbms sigmoid belief networks layer-wise mixed. dbns reproduce input multiple hidden layers restricted simple dataset computationally costly step markov chain monte carlo methods. proposed variational auto-encoder encoder approximates posterior distribution continuous latent variables decoder reconstructs data latent variables trained stochastic variational inference algorithm. extended deep recurrent attention writer processed recurrently incorporating differentiable attention mechanism. conditional aligndraw model extension draw generates image conditioned sentence. also introduced recurrent variational autoencoder architecture signiﬁcantly improves image modeling. models differ cgan construct images gradually ﬁrst image recurrent feedbacks. similar draw recurrent adversarial network adds generated images multiple generators sequentially puts sigmoid function end. adding images based channels results intermixing pixels. model uses additional alpha channel avoid issue. variants lapgan dcgan recurrent adversarial network improved quality generated images. vae/gan replaced pixel-wise loss function feature-wise loss function features come discriminator gan. used structure-gan generates structures; style-gan puts styles structures. generative adversarial networks networks generator tries generate real data given noise discriminator classiﬁes real data pdata fake data objective true data distribution deceiving playing following minimax game figure structure cgan cgan+vae. discriminator generator main components cgan. accepts noise sequence input recursively generates passed generator independently creating images rgba channels. images combined sequentially alpha blending process form ﬁnal output note generators share weights meaning different networks. additionally combined variational autoencoder create directly images. inputs cgan multiple latent variables prior distribution limitations investigating relation latent space output images. propose cgan+vae combination cgan variational autoencoder visualize relation. show latent variables form sub-manifold conditioned previous ones described figure illustrate cgan generates complicated images part part used oxford flowers dataset ﬂower images celeba dataset consisting face images. also used collection cartoon videos children titled pororo limited number characters. autoencoder encoder regularized prior latent distribution combined cgan adding encoders bottom like figure encoder generates noise remaining process cgan. note prior noise distribution cgan latent space models. maximizes likelihood data maximizing variational lower bound. variational lower bound calculated subtracting prior regularization term reconstruction term alpha blending alpha blending combines translucent images producing blended image. value alpha pixel fully transparent fully opaque denote cijrgb -dimensional vector values position cija scalar alpha value position. cgan uses alpha blending covers previous image next image make image composite generative adversarial networks cgan extension consists multiple generators connected recurrent neural network shown figure generators cgans different gans additional alpha channels output. images combined sequentially alpha blending form ﬁnal image. preserves consistency generators generated images related. denote generators generated images respectively. note rgba image pixels four dimensional vectors. illustrate ﬁnal output image made avoid problem constrained alpha values predeﬁned bound still generators generate similar images small alpha values rather heterogeneous images also constrained alpha value near zero images resized antialias ing. rnn. structures generator discriminator similar dcgan series four fractionally strided convolutions chose multivariate normal distribution prior dataset celeba face images celeba dataset contains face images number identities. images dataset cover large pose variations background clutter. discriminator also used rich feature extractor. shown minimizing reconstruction error expressed hidden layer discriminator improves overall quality reconstructed images. hidden vector last convolutional layer p|z) deﬁned normal distribution whose mean comes sample decoder alpha loss even though multiple generators single generator govern ﬁnal output image fulﬁll objective function. problem frequently occurs practice. disentanglement factors images latent variables cgan represent factors images. since passed sequential model input dependent previous zn−. ﬁrst latent variable determines overoutline images latent variables manipulate rest variations conditioned seen figure visualize learned space precisely utilized create latent variables images. figure shows common factors inﬂuenced ﬁnal images. cgan successfully disentangles factors images labels. disentanglement high dimensional data thoroughly studied sufﬁciently solved using unsupervised learning. compared humans video real life numerous years images might small learning tasks. found pororo cartoon video pororo cartoon video minutes total running time. dataset large diversities poses sizes positions characters backgrounds snowy mountains glaciers forests wooden houses. captured frames second video shufﬂed avoid bias. evaluation problematic evaluation various objectives unsupervised learning. since target generate images part part qualitative analysis takes part assessment cgan. measure quality images structured similarity index used quantitative measure rather pixel-wise error. ssim perception-based model incorporates important perceptual phenomena including luminance masking contrast masking terms. measured taking small windows images compare structural information images locally. evaluate based models compared samples test data ssim. largest ssim value among samples taken account test data generating images part part used three generators result shown figure even though overall process stochastic behavior cgan unpredictable unsupervised setting cgan successfully generates images generators sharing efforts impartially. samples cgan+a celeba generated backgrounds faces hair parts respectively ﬁnal images. case ﬂowers generators shared task generating backgrounds ﬂowers separately. cgan also works complicated cartoon videos pororo. results shown figure samples cgan celeba third generator failed generate meaningful images. applying alpha loss cgan problem diminishes. also case cgan+vae using alpha loss forces intermediate images less blurry separable. abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages danihelka alex graves danilo jimenez rezende daan wierstra. draw recurrent neural network image generation. proceedings international conference machine learning icml lille france july pages figure relation latent variables output images cgan+vae+a. various images encoded encoder ﬁxing image encoder. ﬁrst images reconstructed reﬂecting features second images possible learn interacting factors images without labels constructing hierarchical structures images. larger dataset future models could understand images detail might capable changing arrangement objects dynamically without touching backgrounds. model extended domains video text audio combination using specialized encoders generators. primary goals unsupervised learning modality modality mapping. since data hierarchical structures studies decomposing combined data essential.", "year": 2016}