{"title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.", "text": "goal imitation learning mimic expert behavior without access explicit reward signal. expert demonstrations provided humans however often show signiﬁcant variability latent factors typically explicitly modeled. paper propose algorithm infer latent structure expert demonstrations unsupervised way. method built generative adversarial imitation learning imitate complex behaviors also learn interpretable meaningful representations complex behavioral data including visual demonstrations. driving domain show model learned human demonstrations able accurately reproduce variety behaviors accurately anticipate human actions using visual inputs. compared various baselines method better capture latent structure underlying expert demonstrations often recovering semantically meaningful factors variation data. limitation reinforcement learning involves optimization predeﬁned reward function reinforcement signal explicitly deﬁning reward function straightforward cases e.g. games chess. however designing appropriate reward function difﬁcult complex less well-speciﬁed environments e.g. autonomous driving need balance safety comfort efﬁciency. imitation learning methods potential close learning perform tasks directly expert demonstrations succeeded wide range problems among them generative adversarial imitation learning model-free imitation learning method highly effective scales relatively high dimensional environments. training process gail thought building generative model stochastic policy coupled ﬁxed simulation environment produces similar behaviors expert demonstrations. similarity achieved jointly training discriminator distinguish expert trajectories ones produced learned policy gans imitation learning example demonstrations typically provided human experts. demonstrations show signiﬁcant variability. example might collected multiple experts employing different policy. external latent factors variation explicitly captured simulation environment also signiﬁcantly affect observed behavior. example expert demonstrations might collected users different skills habits. goal paper develop imitation learning framework able automatically discover disentangle latent factors variation underlying expert demonstrations. analogous goal uncovering style shape color generative modeling images automatically learn similar interpretable concepts human demonstrations unsupervised manner. propose method learning latent variable generative model produce trajectories dynamic environment i.e. sequences state-actions pairs markov decision process. model accurately reproduce expert behavior also empirically learns latent space observations semantically meaningful. approach extension gail objective augmented mutual information term latent variables observed state-action pairs. ﬁrst illustrate core concepts synthetic example demonstrate application autonomous driving learn imitate complex driving behaviors recovering semantically meaningful structure without supervision beyond expert trajectories. remarkably method performs directly visual inputs using pixels source perceptual information. code reproducing experiments available https//github.com/ermongroup/infogail. particular contributions paper threefold extend gail component approximately maximizes mutual information latent space trajectories similar infogan resulting policy low-level actions controlled abstract high-level latent variables. demonstrate application autonomous highway driving using torcs driving simulator ﬁrst demonstrate learned policy able correctly navigate track without collisions. then show model learns reproduce different kinds human-like driving behaviors exploring latent variable space. preliminaries tuple deﬁne inﬁnite-horizon discounted markov decision process represents state space represents action space s×a×s denotes transition probability distribution denotes reward function distribution initial state discount factor. denote stochastic policy denote expert policy access demonstrations. expert demonstrations trajectories generated using policy consists sequence state-action pairs. expectation respect policy denote goal imitation learning learn perform task directly expert demonstrations without access reinforcement signal typically approaches imitation learning behavior cloning learns policy supervised learning stateaction pairs expert trajectories apprenticeship learning assumes expert policy optimal unknown reward learns policy recovering reward solving corresponding planning problem. tends poor generalization properties compounding errors covariate shift hand advantage learning reward function used score trajectories typically expensive requires solving reinforcement learning problem inside learning loop. recent work adopted different approach learning policy without directly estimating corresponding reward function. particular generative adversarial imitation learning recent method inspired generative adversarial networks gail framework agent imitates behavior expert policy matching generated state-action distribution expert’s distribution optimum achieved policy wish imitate with discriminative classiﬁer tries distinguish state-action pairs trajectories generated γ-discounted causal entropy policy instead directly learning reward function gail relies discriminator guide imitating expert policy. gail model-free requires interaction environment generate rollouts need construct model environment. unlike gans gail considers environment/simulator black thus objective differentiable end-to-end. hence optimization gail objective requires techniques based monte-carlo estimation policy gradients. optimization gail objective performed alternating gradient step increase respect discriminator parameters trust region policy optimization step decrease respect demonstrations typically collected human experts. resulting trajectories show signiﬁcant variability among different individuals internal latent factors variation levels expertise preferences different strategies. even individual might make different decisions encountering situation potentially resulting demonstrations generated multiple near-optimal distinct policies. section propose approach discover disentangle salient latent factors variation underlying expert demonstrations without supervision learn policies produce trajectories correspond latent factors visual inputs external perceptual information. deﬁne formally assume expert policy mixture experts generative process expert trajectory discrete latent variable selects speciﬁc policy mixture expert policies prior distribution similar gail setting consider apprenticeship learning problem dual occupancy measure matching problem treat trajectory state-action pairs. instead learning policy solely based current state extend include explicit dependence latent variable objective recover policy approximation samples prior trajectories generated conditional policy similar expert trajectories measured discriminative classiﬁer. learning demonstrations generated mixture experts challenging access policies employed individual experts. proceed unsupervised similar clustering. original generative adversarial imitation learning method would fail assumes demonstrations come single expert incentive separating disentangling variations observed data. method automatically disentangle demonstrations meaningful thus needed. address problem introduce latent variable policy function without constraints applying gail directly could simply ignore fail separate different types behaviors present expert trajectories incentivize model much possible utilize information-theoretic regularization enforcing high mutual information state-action pairs generated trajectory. concept introduced infogan latent codes utilized discover salient semantic features data distribution guide generating process. particular regularization seeks maximize mutual information latent codes trajectories hyperparameter information maximization regularization term hyperparameter casual entropy term. introducing latent code infogail able identify salient factors expert trajectories mutual information maximization imitate corresponding expert policy generative adversarial training. allows disentangle trajectories arise mixture experts different individuals performing task. optimize objective simpliﬁed posterior approximation since directly working entire trajectories would expensive especially dimension observations high parameterize policy discriminator posterior approximation weights respectively. optimize stochastic gradient methods using trpo updated using adam optimizer outline optimization procedure shown algorithm input initial parameters policy discriminator posterior approximation expert trajectories containing state-action pairs. output learned policy complex less well-speciﬁed environments imitation learning methods potential perform better reinforcement learning methods require manual speciﬁcation appropriate reward function. however expert performing sub-optimally policy trained recovered rewards also suboptimal; words imitation learning agent’s potential bounded capabilities expert produced training data. many cases difﬁcult fully specify suitable reward function given task relatively straightforward come constraints would like enforce policy. motivates introduction reward augmentation general framework incorporate prior knowledge imitation learning providing additional incentives agent without interfering hyperparameter. approach seen hybrid imitation reinforcement learning part reinforcement signal policy optimization coming surrogate reward part discriminator i.e. mimicking expert. example autonomous driving experiment show providing agent penalty collides cars drives road able signiﬁcantly improve average rollout distance learned policy. gail successful tasks low-dimensional inputs largest observation continuous variables) explored tasks input dimension high order effectively learn policy relies solely high-dimensional input make following improvements original gail framework. well known traditional objective suffers vanishing gradient mode collapse problems propose wasserstein technique alleviate problems augment objective function follows note modiﬁcation especially important setting want model complex distributions trajectories potentially large number modes. also several variance reduction techniques including baselines replay buffers besides baseline three models update infogail framework represented neural networks discriminator network policy network posterior estimation network update using rmsprop update using adam trpo respectively. include detailed training procedure appendix speed training initialize policy behavior cloning note discriminator network posterior approximation network treated distinct networks opposed infogan approach share network parameters ﬁnal output layer. current wgan training framework requires weight clipping momentum-free optimization methods training changes would interfere training expressive share network parameters. demonstrate performance method applying ﬁrst synthetic example challenging driving domain agent imitating driving behaviors visual inputs. conducting experiments environments show learned policy imitate expert behaviors using high-dimensional inputs small number expert demonstrations cluster expert behaviors different semantically meaningful categories reproduce different categories behaviors setting high-level latent variables appropriately. driving experiments conducted torcs environment. demonstrations collected manually driving along race track show typical behaviors like staying within lanes avoiding collisions surpassing cars. policy accepts visual inputs external inputs state produces three-dimensional continuous action consists steering acceleration braking. assume policies gaussian distributions ﬁxed standard deviations thus constant. figure learned trajectories synthetic plane environment. color denotes speciﬁc latent code. behavior cloning deviates expert demonstrations compounding errors. gail produce circular trajectories fails capture latent structure assumes demonstrations generated single expert tries learn average policy. method successfully distinguishes expert behaviors imitates mode accordingly demonstrate effectiveness infogail synthetic example. environment plane agent move around freely constant velocity selecting direction time agent observations time positions expert demonstrations contain three distinct modes generated stochastic expert policy produces circle-like trajectory objective distinguish three distinct modes imitate corresponding expert behavior. consider three methods behavior cloning gail infogail particular experiments assume architecture latent code one-hot encoded vector dimensions uniform prior; infogail regularizes latent code. figure shows introduction latent variables allows infogail distinguish three types behavior imitate behavior successfully; methods however fail distinguish distinct modes. suffers compounding error problem learned policy tends deviate expert trajectories; gail learn generate circular trajectories fails separate different modes lack mechanism explicitly account underlying structure. rest section show infogail infer latent structure human decisionmaking driving domain. particular agent relies visual inputs sense environment. high dimensional nature visual inputs poses signiﬁcant challenges learning policy. intuitively policy simultaneously learn identify meaningful visual features leverage achieve desired behavior using small number expert demonstrations. therefore methods mitigate high sample complexity problem crucial success domain. paper take transfer learning approach. features extracted using pre-trained imagenet contain high-level information input images adapted vision tasks transfer learning however clear whether relatively high-level features directly applied tasks perception action tightly interconnected; demonstrate possible experiments. perform transfer learning exploiting features pre-trained neural network effectively convert images relatively highlevel information particular deep residual network pre-trained imagenet classiﬁcation task obtain visual features used inputs policy network. policy accepts certain auxiliary information internal input serve short-term memory. auxiliary information accessed along visual inputs. experiments auxiliary information policy time consists following velocity time three dimensional vector; actions time three dimensional vectors; damage real value. auxiliary input dimensions total. figure visualizing training process turn. show trajectories infogail different stages training. blue indicate policies different latent codes correspond turning inner lane turning outer lane respectively. rightmost ﬁgure shows trajectories latent codes suggests that extent method able generalize cases previously unseen training data. policy network input visual features passed convolutional layers combined auxiliary information vector latent code parameterize baseline network architecture except ﬁnal layer scalar output indicates expected accumulated future rewards. discriminator accepts three elements input input image auxiliary information current action. output score wgan training objective supposed lower expert state-action pairs higher generated ones. posterior approximation network adopts architecture discriminator except output softmax discrete latent variables factored gaussian continuous latent variables. include details architecture appendix experiment consider subsets human driving behaviors turn expert takes turn using either inside lane outside lane; pass expert passes another vehicle either left right. cases expert policy signiﬁcant modes. goal infogail capture separate modes expert demonstrations unsupervised way. discrete latent code one-hot encoded vector possible states. settings expert trajectories total frames trajectory; prior latent code uniform discrete distribution states. performance learned policy quantiﬁed metrics average distance determined distance traveled agent collision accuracy deﬁned classiﬁcation accuracy expert state-action pairs according latent code inferred constant reward every time step reward augmentation used encourage \"stay alive\" long possible regarded another reducing collision off-lane driving average distance sampled trajectories different stages training shown figures turn pass respectively. initial stages training model distinguish modes high chance colliding driving off-lane limitations behavior cloning training progresses trajectories provided learned policy begin diverge. towards training types trajectories clearly distinguishable exceptions. turn corresponds using inside lane corresponds outside lane. pass kinds latent codes correspond passing right left respectively. meanwhile average distance rollouts steadily increases training. learning modes separately requires accurate inference latent code. examine accuracy posterior inference select state-action pairs expert trajectories obtain corresponding latent code table although explicitly provide label model able correctly distinguish state-action pairs pass figure experimental results pass. left trajectories infogail different stages training blue indicate policies using different latent code values correspond passing right left. middle traveled distance denotes absolute distance start position averaged rollouts infogail policy trained different epochs. right trajectories pass produced agent trained original gail objective. compared infogail gail fails distinguish different modes. comparison also visualize trajectories pass original gail objective figure mutual information regularization. gail learns expert trajectories whole cannot distinguish modes expert policy. interestingly instead learning separate trajectories gail tries left trajectory swinging suddenly left surpassed right. believe reﬂects limitation discriminators. since requires state-action pairs input policy required match state-action pairs; matching rollout whole expert trajectories necessary. infogail discrete latent codes alleviate problem forcing model learn separate trajectories. conduct series ablation experiments demonstrate proposed improved optimization techniques section indeed crucial learning effective policy. policy drives race track along cars whereas human expert provides trajectories frames trying drive fast possible without collision. reward augmentation performed adding reward encourages drive faster. performance policy determined average distance. longer average rollout distance indicates better policy. ablation experiments selectively remove improved optimization methods section infogail includes optimization techniques; gail excludes techniques; infogail\\wgan switches wgan objective objective; infogail\\ra removes reward augmentation; infogail\\rb removes replay buffer samples recent rollouts; behavior cloning behavior cloning method human expert policy. table shows average rollout distances different policies. method able outperform expert help reward augmentation; policies without reward augmentation wgans perform slightly worse expert; removing replay buffer causes performance deteriorate signiﬁcantly increased variance gradient estimation. major paradigms vision-based driving systems mediated perception two-step approach ﬁrst obtains scene information makes driving decision behavior reﬂex hand adopts direct approach mapping visual inputs driving actions many current autonomous driving methods rely two-step approach requires hand-crafting features detection lane markings cars approach hand attempts learn features directly vision actions. mediated perception approaches currently prevalent believe end-to-end learning methods scalable lead better performance long run. introduce end-to-end imitation learning framework learns drive entirely visual information test approach real-world scenarios. however method uses behavior cloning performing supervised learning state-action pairs well-known generalize poorly sophisticated tasks changing lanes passing vehicles. gail method learn perform sophisticated operations easily. performs end-to-end visual imitation learning torcs dagger querying reference policies training many cases difﬁcult. imitation learning methods end-to-end driving rely heavily lidar-like inputs obtain precise distance measurements inputs usually available humans driving. particular applies gail task modeling human driving behavior highways. contrast policy requires visual information external input practice information humans need order drive. also introduced pre-trained deep neural network achieve better performance imitation learning relatively demonstrations. speciﬁcally introduce pre-trained model learn dense incremental reward functions suitable performing downstream reinforcement learning tasks real-world robotic experiments. different approach transfer learning performed critic instead policy. would interesting combine reward approach reward augmentation. paper present method imitate complex behaviors identifying salient latent factors variation demonstrations. discovering latent factors require direct supervision beyond expert demonstrations whole process trained directly standard policy optimization algorithms. also introduce several techniques successfully perform imitation learning using visual inputs including transfer learning reward augmentation. experimental results torcs simulator show methods automatically distinguish certain behaviors human driving learning policy imitate even outperform human experts using visual information sole external input. hope work inspire end-to-end learning approaches autonomous driving realistic scenarios. thank shengjia zhao neal jean assistance advice. toyota research institute provided funds assist authors research article solely reﬂects opinions conclusions authors toyota entity. research also supported intel corporation grants silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search nature vol. englert toussaint inverse kkt–learning cost functions manipulation tasks demonstrations proceedings international symposium robotics research finn levine abbeel guided cost learning deep inverse optimal control policy optimization proceedings international conference machine learning vol. ermon toth dilkina bernstein damoulas clark degloria mude barrett learning large-scale dynamic discrete choice models spatiotemporal preferences application migratory pastoralism east africa. aaai goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative adversarial nets advances neural information processing systems mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski human-level control deep reinforcement learning nature vol. sharif razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition proceedings ieee conference computer vision pattern recognition workshops russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge international journal computer vision vol. chen seff kornhauser xiao deepdriving learning affordance direct perception autonomous driving proceedings ieee international conference computer vision conv denote convolutional layer denote fully connected layer. synthetic example initialize infogail network behavior cloning replay buffers cheap rollout setting. observations dimensional actions dimensional. similar torcs experiments wasserstein gans training. video showing learning process infogail available supplemented video. provide input image lower resolution discriminator mainly improved training speed. since inferring high-level actions policy involve ﬁne-grained details present higher resolution inputs improve training speed without suffering performance loss. also provide additional video demonstrates failure case reward augmentation failure caused assigning excessively high reward avoiding collision. agent \"afraid\" previous tries repeated turning increase friction slow dramatically clearly undesirable behavior.", "year": 2017}