{"title": "An ensemble diversity approach to supervised binary hashing", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.ML"], "abstract": "Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.", "text": "binary hashing well-known approach fast approximate nearest-neighbor search information retrieval. much work focused aﬃnity-based objective functions involving hash functions binary codes. objective functions encode neighborhood information data points often inspired manifold learning algorithms. ensure hash functions diﬀer constraints penalty terms encourage codes orthogonal dissimilar across bits couples binary variables complicates already diﬃcult optimization. propose much simpler approach train hash function independently other introduce diversity among using techniques classiﬁer ensembles. surprisingly faster trivially parallelizable also improves complex coupled objective function achieves state-of-the-art precision recall experiments image retrieval. information retrieval tasks searching query image document database essentially nearest-neighbor search dimensionality query size database large approximate search necessary. focus binary hashing query database mapped onto low-dimensional binary vectors search performed. speedups computing hamming distances much faster computing distances high-dimensional ﬂoating-point vectors; entire database becomes much smaller reside fast memory rather disk constructing hash functions well retrieval measures precision recall usually done optimizing aﬃnity-based objective function relates hamming distances supervised neighborhood information training set. many objective functions form pairwise terms indicate whether training points neighbors here dataset high-dimensional feature vectors binary hash functions b-bit code vector input minh means minimizing parameters hash function loss function compares codes images ground-truth value measures aﬃnity original space images often restricted subset image pairs keep runtime low. output algorithm hash function binary codes training points examples objective functions supervised hashing kernels binary reconstructive embeddings similar dissimilar; laplacian loss similar dissimilar examples objective functions include models developed dimension reduction spectral locally linear embedding anchor graphs nonlinear elastic embedding t-sne well objective functions designed speciﬁcally binary hashing semi-supervised sequential projection learning hashing produce good hash functions. focus laplacian loss paper. designing objective functions needs eliminate types trivial solutions. laplacian loss mapping points code i.e. global optimum positive neighbors term avoided negative neighbors. hash functions identical other i.e. avoided introducing constraints penalty terms mathematical devices couple dimensions. example laplacian loss encourage codes orthogonal constraint penalty term although generates dense matrices losses squaring product hamming distance codes couples bits. important downside approaches diﬃculty optimization. fact objective function nonsmooth binary output hash function. large number binary variables larger number pairwise interactions less using sparse neighborhoods) variables coupled said constraints penalty terms. optimization approximated diﬀerent ways. papers ignore binary nature codes optimize real values binarize truncation ﬁnally classiﬁer bits separately. example laplacian loss constraints involves solving eigenproblem laplacian eigenmaps approximated using landmarks fast relaxing codes optimization generally optimal. recent papers respect binary nature codes optimization using techniques alternating optimization min-cut graphcut others classiﬁers alternating optimization directly hash function parameters even recently optimize jointly binary codes hash functions approaches slow limited small datasets quadratic number pairwise terms objective. propose diﬀerent much simpler approach. rather coupling hash functions single objective function train hash function independently using single-bit objective function form. show avoid trivial solutions injecting diversity hash function’s training using techniques inspired classiﬁer ensemble learning. section discusses relevant ideas ensemble learning literature section describes independent laplacian hashing algorithm section gives evidence image retrieval datasets simple approach indeed works well section discusses connection hashing ensembles. ﬁrst sight optimizing without constraints seem like good idea since separates bits obtain independent identical objectives hash function global optimum. hash functions equal equivalent using them give much lower precision/recall. fact issue arises training ensemble classiﬁers here training input vectors output class labels want train several classiﬁers whose outputs combined classiﬁers equal gain nothing single classiﬁer. hence necessary introduce diversity among classiﬁers disagree predictions. ensemble learning literature identiﬁed several mechanisms inject diversity. important ones apply binary hashing setting follows using diﬀerent data classiﬁer done using diﬀerent feature subsets classiﬁer. works best features somewhat redundant. using diﬀerent training sets classiﬁer. works best unstable algorithms decision trees neural nets unlike linear nearest neighbor classiﬁers. prominent example bagging generates bootstrap datasets trains model each. injecting randomness training algorithm possible local optima exist algorithm randomized done using diﬀerent initializations adding noise updates using diﬀerent choices randomized operations algorithm connection binary hashing ensemble learning oﬀers many possible options terms choice type hash function binary hashing objective function optimization algorithm diversity mechanism. paper focus following choices. linear kernel svms hash functions. without loss generality laplacian objective single takes form optimize two-step approach ﬁrst optimize bits learn hash function ﬁtting binary classiﬁer. ﬁtting classiﬁer.) laplacian objective np-complete negative neighbors approximately optimize using min-cut algorithm applied alternating fashion submodular blocks described ﬁrst partitions points disjoint groups containing nonnegative weights. group deﬁnes submodular function whose global minimum found polynomial time using min-cut. order groups optimized randomized iteration approximate optimizer found depends initial diﬀerent training sets hash function uses training points diﬀerent disjoint hash functions. aﬀord binary hashing training sets potentially large computational cost optimization limits training sets thousand points. later show outperforms using bootstrapped training sets. diﬀerent feature subsets hash function trained random subset features sampled without replacement subsets corresponding diﬀerent hash functions overlap. equivalence objective functions single-bit case several binary hashing objective functions diﬀer general case bits become essentially identical case. example expanding pairwise terms positive negative elements well zeros. submodular nonpositive elements case equivalent min-cut/max-ﬂow problem solved polynomial time depends hamming distances bits written anmznzm bnm. even more arbitrary function binary variables depends hamming distances written quadratic function variables. however variables generally true computational advantages training hash functions independently important advantages. first training functions parallelized perfectly. speedup orders magnitude typical values coupled objective functions exhibit obvious parallelism trained alternating optimization inherently sequential. second even single processor binary optimizations variables generally easier binary optimization variables. search spaces contain states resp. enumeration much faster independent case using approximate polynomial-time algorithm independent case also faster runtime superlinear number variables asymptotic runtimes respectively. case best practical graphcut max-ﬂow/min-cut algorithms third solution exhibits nesting solution bits need take solution bits unlike methods based coupled objective function solution bits cannot obtained adding solve bits scratch. model selection number bits selecting number bits received much attention binary hashing literature. obvious would maximize precision test subject exceeding preset limit nesting property makes computationally easy simply keep adding bits test precision stabilizes decreases reach maximum still beneﬁt parallel processing processors available train hash functions parallel evaluate precision also parallel. still need increase train hash functions etc. following labeled datasets cifar contains images classes. gist features image. images training test. inﬁnite mnist generated using elastic deformations original mnist handwritten digit dataset images training test classes. represent image vector pixels. appendix contains experiments additional datasets. computational cost aﬃnity-based methods previous work used training sets limited thousand points unless otherwise indicated train hash functions subset points training report precision recall searching test query entire dataset hash functions linear svms kernel svms report precision recall test queries using ground truth training points label query. retrieved contains nearest neighbors query point hamming space. report precision diﬀerent values test robustness diﬀerent algorithms. diversity mechanisms understand eﬀect diversity evaluate mechanisms ilhi ilht ilhf combination ilhitf range number bits training size baseline coupled objective using two-step training ﬁrst codes using alternating min-cut method described earlier classiﬁers. faster generally ﬁnds better optima original optimization denote kshcut. fig. shows results. clearly best diversity mechanism ilht works better mechanisms even combined them signiﬁcantly better kshcut. explain follows. although mechanisms introduce diversity ilht distinct advantage eﬀectively uses times much training data hash function disjoint dataset. using training points kshcut would orders magnitude slower. ilht equal even better combined ilhitf since already enough diversity ilht extra diversity ilhi ilhf help; ilhf uses less data hurt precision; also seen precision methods saturates increases; bits ilht achieves nearly maximum precision points. fact continued increase per-bit training size ilht eventually bits would training diversity would disappear precision would drop drastically precision using single practical image retrieval datasets large unlikely occur unless large linear svms stable classiﬁers known beneﬁt less ensembles less stable classiﬁers decision trees neural nets remarkably strongly beneﬁt ensemble case. hash function solving diﬀerent classiﬁcation problem resulting svms fact quite diﬀerent other. conclusions kernel hash functions similar. tried cases hash functions using same common centers figure diversity mechanisms baseline precision cifar dataset function training size number bits ground truth points label query. retrieved nearest neighbors query. errorbars shown ilht avoid clutter. bottom hash functions linear kernel kernel private centers. left right diversity mechanisms combination baseline kshcut. radial basis functions hash function using centers. nonlinear classiﬁers less stable linear ones. case beneﬁt much linear svms diversity. achieve higher precision since powerful models particularly using private centers. fig. shows results ilhf varying number features used hash function. intuitively classiﬁer receives little information make near-random codes. indeed precision comparable high also work badly would eliminate diversity drop precision single actually happen additional source diversity randomization alternating min-cut iterations. eﬀect similar ilhi indeed comparable precision. highest precision achieved proportion ilhf indicating redundancy features. combined diversity mechanisms highest precision occurs diversity already provided mechanisms using data better. fig. shows results constructing training sets ilht random sample base bootstrapped disjoint random expected disjoint consistently signiﬁcantly better bootstrap introduces independence hash functions learns data overall precision function fig. shows precision function number bits ilht solution bits obtained adding solution since hash functions obtained depend order bits show orders remarkably precision increases nearly monotonically continues increasing beyond bits panel precision function number hash functions diﬀerent methods results show precision using training points. errorbars random training sets. ground truth points label query. retrieved nearest neighbors query cifar inﬁnite mnist trees; kuncheva eﬀective training size proportional variance precision decreases increases. contrast kshcut variance larger precision barely increases higher variance kshcut fact value involves training scratch converge relatively diﬀerent local optimum. ilht adding random projections increases precision monotonically reach precision best since lacks supervision. also show curve thresholded whose precision tops around decreases thereafter. likely explanation high-order principal components essentially capture noise rather signal i.e. random variation data produces random codes bits destroy neighborhood information. bagging tpca make tpca improve monotonically result still competitive. reason little diversity among ensemble members principal components accurately estimated even small samples. result uses tpca ensembles member principal components i.e. bits. using single-bit members ilht precision bits barely better bit. precision ilht incomplete optimization objective local optima? veriﬁed random perturbations kshcut optimum lower precision; optimizing kshcut using ilht codes initialization increases precision still remains ilht. conﬁrms optimization algorithm ilht diversity mechanism superior coupling hash functions joint objective. codes orthogonal? result learning binary hashing hash functions represented matrix wb×d real weights linear svms matrix binary codes entire dataset. deﬁne measure code orthogonality follows. deﬁne matrices codes weights matrix entries equal normalized product codes weight vectors diagonal entries equal perfect orthogonality happens encouraged many binary hashing methods. fig. shows ilht cifar inﬁnite mnist plots image well histogram entries histograms also contain control histogram corresponding normalized products random vectors known tend delta function dimension grows. although tendency orthogonality number bits used increases clear that codes weight vectors distribution products wide strict orthogonality. hence enforcing orthogonality seem necessary achieve good hash functions codes. comparison binary hashing methods compare original min-cut optimization kshcut representative subset aﬃnitybased unsupervised hashing methods supervised binary reconstructive embeddings supervised self-taught hashing spectral hashing iterative quantization binary autoencoder thresholded locality-sensitive hashing create aﬃnities aﬃnity-based methods using dataset labels. training point similar neighbors points labels dissimilar neighbors points chosen randomly among points whose labels diﬀerent figure comparison diﬀerent binary hashing methods precision precision/recall using linear svms hash functions using diﬀerent numbers bits cifar inﬁnite mnist. ground truth points label query. retrieved nearest neighbors range datasets methods trained using subset points. given kshcut already performs well ilht consistently outperforms precision runtime expect ilht competitive state-of-the-art. fig. shows generally case particularly number bits increases ilht beats methods able increase precision much ilht does. much faster aﬃnity-based hashing methods kshcut among faster methods. runtime min-cut pass single comparable ours needs sequential passes complete alternating optimization iteration functions trained parallel. summary ilht achieves remarkably high precision compared coupled objective using optimization algorithm introducing diversity feeding diﬀerent data independent hash functions rather jointly optimizing them. also compares well state-of-the-art methods precision/recall competitive bits used clear winner bits used fast embarrassingly parallel. revealed ﬁrst time connection supervised binary hashing ensemble learning could open door many hashing algorithms. although focused speciﬁc objective identiﬁed particularly successful speciﬁc diversity mechanism choices better depending application. core idea propose independent training hash functions introduction diversity means coupling terms objective constraints. come surprise area learning binary hashing work last years focused proposing complex objective functions couple hash functions developing sophisticated optimization algorithms them. another surprise orthogonality codes hash functions seems unnecessary. ilht creates codes hash functions diﬀer orthogonal achieve good precision keeps growing bits. thus introducing diversity diﬀerent training data seems better mechanism make hash functions diﬀer coupling codes orthogonality constraint otherwise. also simpler faster train independent single-bit hash functions. ﬁnal surprise wide variety aﬃnity-based objective functions b-bit case reduces binary quadratic problem -bit case regardless form b-bit objective sense unique objective -bit case. prior attempt bagging truncated experiments show that improves truncated performs poorly supervised binary hashing. unsupervised user-provided similarity information disagree euclidean distances image space; estimating principal components samples diversity. also computationally simple little gain bagging unlike diﬃcult optimization supervised binary hashing. supervised binary hashing work proposed learn hash functions sequentially function orthogonality-like constraint force diﬀer previous functions. hence learn functions independently seen greedy optimization joint objective functions. binary hashing diﬀer ensemble learning important point predictions classiﬁers combined single prediction instead concatenated binary vector labels classiﬁers unknown implicitly explicitly learned together hash functions themselves. means well-known error decompositions error-ambiguity decomposition bias-variance decomposition apply. also real goal binary hashing well information retrieval measures precision recall hash functions directly optimize this. theoretical understanding diversity helps learning binary hashing important topic future work. respect also relation error-correcting output codes approach multiclass classiﬁcation. ecoc represent classes b-bit binary vector ensuring large enough vectors suﬃciently separated hamming distance. corresponds partitioning classes groups. train binary classiﬁers decision trees. given test pattern output class label closest hamming distance b-bit output classiﬁers. redundant error-correcting codes allow small errors individual classiﬁers improve performance. ecoc also seen ensemble classiﬁers manipulate output targets obtain classiﬁer apply majority vote ﬁnal result main beneﬁt ecoc seems variance reduction ensemble methods binary hashing seen ecoc classes training point ecoc prediction test pattern nearest-neighbor class codes hamming distance. however unlike ecoc binary hashing codes learned preserve neighborhood relations training points. also ideally codes diﬀerent guaranteed binary hashing. ﬁnal diﬀerent example shows important role diversity i.e. making hash functions diﬀer learning good hash functions. binary hashing methods optimize objective essentially following form linear projection matrix idea force projections close possible binary values. orthogonality constraint ensures trivial solutions optimal. remarkably objective function contains explicit information neighborhood preservation reconstruction input although orthogonal projections preserve euclidean distances true preserving binarized projections. produce good hash functions initialized learn projections reconstruct inputs optimally local optimum objective that. thus would appear part success approaches relies constraint providing form diversity among hash functions. much work supervised binary hashing focused designing sophisticated objective functions hash functions force compete trying preserve neighborhood information. shown surprisingly training hash functions independently simpler faster parallel also achieve better retrieval quality long diversity introduced hash function’s objective function. establishes connection ensemble learning allows borrow techniques showed hash function optimize laplacian objective disjoint subset data works well facilitates selecting number bits use. although evidence mostly empirical intuition behind sound agreement many results showing power ensemble classiﬁers. ensemble learning perspective suggests many ideas future work pruning large ensemble using diversity techniques. also possible characterize theoretically performance precision binary hashing depending diversity hash functions. main paper state that single case laplacian loss functions vector binary codes data point written form binary quadratic function without linear term generally consider arbitrary objective function binary vector depends hamming distances bits form aﬃnity-based loss function used many binary hashing papers single-bit case. term function written anmznzm bnm. fact already noted function binary variables take diﬀerent values however true general. seen comparing dimensions function spaces spanned arbitrary function quadratic function. consider ﬁrst general quadratic function binary variables +}n. always take symmetric write bnzn free parameters. vector possible values possible binary vectors linear function free parameters hence dimension space quadratic functions consider arbitrary function binary variables depends hamming distances. although hamming distances determined ﬁrst distances because given distance determines entire vector distances. also given distances value produces vector whose bits reversed produced hamming distances. hence free binary variables determine vector possible values possible binary vectors hence dimension space arbitrary functions hamming distances since quadratic functions general cannot represent arbitrary binary functions hamming distances using binary variables. uses hinge loss implement goal similar points diﬀer bits dissimilar points diﬀer bits more hamming distance easy single-bit case loss lmlh becomes constant independent codes—because using hamming distance either only. paragraph codes orthogonal? main paper deﬁne measure orthogonality either binary codes hash function weight vectors wb×d based matrices normalized products respectively. prove several statements make paragraph. distribution products random vectors control hypothesis orthogonality binary codes hash function vectors used distribution products random vectors. give mean variance explicitly function dimension. hence dimension increases variance decreases distribution tends delta means random high-dimensional vectors practically orthogonal. random histograms based sample random vectors normalize vector). follow theoretical distribution well. also include results additional unsupervised dataset flickr million image dataset flickr randomly select images test rest training. mpeg- edge histogram features. since labels available create pseudolabels declaring similar points true nearest neighbors dissimilar points random subset points among remaining points. ground truth nearest neighbors query euclidean space. hash functions trained using points. retrieved nearest neighbors query point hamming space range important diﬀerence locality-sensitive hashing achieves high precision flickr dataset considerably higher kshcut. understandable following reasons flickr unsupervised dataset neighborhood information provided kshcut form aﬃnities limited small subset positive negative neighbors access full feature vector every image. dimensionality flickr feature vectors quite small still ilht beats signiﬁcant margin. addition methods used supervised datasets compare ilht spectral hashing iterative quantization binary autoencoder thresholded locality-sensitive hashing again ilht beats state-of-the-art methods comparable best them particularly number bits increases.", "year": 2016}