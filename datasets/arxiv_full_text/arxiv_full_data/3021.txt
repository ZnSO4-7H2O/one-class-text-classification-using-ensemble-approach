{"title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image  Captioner", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries -- captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.", "text": "impressive image captioning results achieved domains plenty training image sentence pairs however transferring target domain signiﬁcant domain shifts paired training data remains largely unexplored. propose novel adversarial training procedure leverage unpaired data target domain. critic networks introduced guide captioner namely domain critic multi-modal critic. domain critic assesses whether generated sentences indistinguishable sentences target domain. multi-modal critic assesses whether image generated sentence valid pair. training critics captioner adversaries captioner aims generate indistinguishable sentences whereas critics distinguishing them. assessment improves captioner policy gradient updates. inference propose novel critic-based planning method select high-quality sentences without additional supervision evaluate mscoco source domain four datasets target domains. method consistently performs well datasets. particular cub-- achieve cider-d improvement adaptation. utilizing critics inference gives another boost. introduction datasets large corpora paired images sentences enabled latest advance image captioning. many novel networks trained paired data achieved impressive results domain-speciﬁc setting training testing domain. however domain-speciﬁc setting creates huge cost collecting paired images sentences domain. real world applications prefer cross-domain captioner trained source figure propose cross-domain image captioner adapt sentence style source target domain without need paired image-sentence training data target domain. left panel sentences mscoco mainly focus location color size objects. right panel sentences cub- describe parts birds detail. bottom panel shows generated sentences adaptation. domain paired data generalized target domains little cost training high-quality cross-domain captioner challenging large domain shift image sentence spaces. instance mscoco mostly consists images large scene object instances whereas cub-- consists cropped birds images. moreover sentences mscoco typically describe location color size objects whereas sentences cub- describe parts birds detail case exworks propose leverage different types unpaired data domains tackle challenge. propose leverage image dataset category labels sentences however focus ability generate words unseen paired training data anderson propose leverage image taggers test time. however requires robust crossdomain tagger. moreover focus selecting different words changing overall style. propose novel adversarial training procedure leverage unpaired images sentences. critic networks introduced guide procedure namely domain critic multi-modal critic. domain critic assesses whether generated captions indistinguishable sentences target domain. multi-modal critic assesses whether image generated caption valid pair. training critics captioner adversaries captioner aims generate indistinguishable captions whereas critics distinguishing them. since sentence assessed completed monte carlo rollout estimate assess generated word. then apply policy gradient update network captioner. last least propose novel critic-based planning method take advantage learned critics compensate uncertainty sentence generation policy additional supervision testing. evaluate mscoco source domain cub- oxford- flickrk tgif target domains. method consistently performs well datasets. particular cub- achieve cider-d improvement adaptation. utilizing critic inference gives another boost. codes available https//github.com/tsenghungchen/ show-adapt-and-tell. finally contributions paper summarized below ligence connects computer vision natural language processing. thanks recent advances deep neural networks release several large-scale datasets mscoco flickrk many works shown different levels success image captioning. typically employ convolutional neural network image encoding decoding caption recurrent neural network many attempts improve basic encoder-decoder framework. commonly used approach spatial attention mechanism. introduce attention model automatically learn look depending generated words. besides images apply lstms video encoder generate video descriptions. particular zeng propose framework jointly localize highlights videos generate titles. addressing exposure bias. recently issue exposure bias well-addressed sequence prediction tasks. happens model trained maximize likelihood given ground truth words follows predictions test inference. result training process leads error accumulation test time. order minimize discrepancy training inference bengio propose curriculum learning strategy gradually ignore guidance supervision training. lamb introduce adversarial training method regularization sampling mode teacher-forced mode. recently plenty works using policy gradient directly optimize evaluation metrics. methods avoid problem exposure bias improve cross entropy methods. however cannot applied cross-domain captioning since need ground truth sentences compute metric bleu. reward modeling. contrast works learn reward function cross-domain setting reward computed even testing enable novel critic-based planning method. several works incorporate auxiliary models rewards. hendricks minimize discriminative loss ensure generated sentences class speciﬁc. similar method also introduce critic learn reward function. however proposed method random sentence generation designed domain adaptation. domain adaptation. conventional dnn-based domain adaptation learn latent space minimize distance metrics central moment discrepancy data domains. hand existing adversarial domain adaptation methods domain classiﬁer learn mappings source target domains. ajakan introduce domain adaptation regularizer learn representation sentiment analysis. ganin component consists critics provide reward. critic assesses similarity style. critic assesses relevancy given paired data source domain example pairs. critics compute reward generated sentence captioner critics iteratively trained using novel adversarial training procedure. next describe captioner critics detail. time captioner takes action according stochastic policy observed image generated partial sentence parameter policy. utilize existing cnn-rnn model model policy. sequentially generating word policy special end-of-sentence token complete sentence generated. standard image captioning following total expected per-word loss minimized. number images length sentence loss cross-entropy loss ground truth partial sentence word respectively. cross-domain captioning ground truth sentence target domain. hence introduce critics assess quality generated complete sentence particular critics compute reward utilizing example sentences target domain example paired data source domain. given reward modify train agent using policy gradient. policy gradient. main idea policy gradient replace per-word loss loss another computable term related state-action reward state characterized image partial sentence action current generated word state-action reward deﬁned expected future reward figure captioner standard cnn-rnn architecture predicted word previous step serve input current step inference. <bos> <eos> represent begin-of-sentence end-of-sentence respectively. propose gradient reversal layer aligning distribution features across source target domain. hoffman propose unsupervised domain adversarial method semantic segmentations street scenes. chen collect dataset road scene images across countries cross-city adaptation. performance improvement shown sentiment analysis image classiﬁcation person re-identiﬁcation scene segmentation tasks. however aware adversarial domain adaptation approach applied cross-domain captioning. ﬁrst formally deﬁne task cross-domain image captioning; then give overview proposed method. cross-domain setting. common setting data domains available. source domain given paired image ground truth sentence describing sentence consists sequence word length target domain given separate sets information example images {xn}n example sentences {ˆyn}n. note collecting paired data source domain typically costly target domain. image captioning. standard image captioning goal generate sentence similar ground truth sentence cross-domain image captioning since ground truth sentence image available goal becomes following. image generating sentence similar style relevant pair similar pairs overview method. achieve goal crossdomain image captioning propose novel method consisting main components. ﬁrst component standard cnn-rnn-based captioner however captioner treated agent taking sequential actions agent trained using policy gradient given reward generated sentence. second figure system overview. left panel captioner generates sentence condition image representation step expected reward newly generated word computed domain multi-modal critics using monte carlo rollout. policy gradient update captioner toward generating sentences higher reward. right panel critics observe sentences generated captioner discriminating true data target source domains. adversarial training captioner critics iteratively updated achieve competing goals. cross-domain image captioning good caption needs satisfy criteria generated sentence resembles sentence drawn target domain. generated sentence relevant input image. critics follow rules assign reward generated sentence. introduce domain critic multi-modal critic below. domain critic. order address domain shift sentence space train domain critic classify sentences source domain target domain generated ones. model consists encoder classiﬁer. sentence ﬁrst encoded highway connection sentence representation. then pass representation fully connected layer softmax layer generate probability {source target generated}. note scalar probability indicates likely sentence target domain. multi-modal critic. order check relevance between sentence image propose multimodal critic classify paired unpaired generated data. model consists parameters learned denotes element-wise multiplication probabilities three classes paired unpaired generated data. sentence encoded lstm-based sentence encoder. then encoded image sentence representations fused element-wise multiplication similar finally fused representation forwarded fully connected layer softmax layer generate probability {paired unpaired generated}. scalar probability indicates generated caption relevant image please supplementary intuition empirical studies design choices sentence reward. deﬁne reward ensures sentence receives high reward believes sentence target domain believes sentence relevant image. training critics. introduce training objective below. goal classify sentence source target generated data. formulated supervised classiﬁcation training objective follows number sentences model parameter ˆysrc denotes sentences source domain ytgt denotes sentences target domain notes sentences generated captioner policy given target domain images xtgt. empty generated sentences empty paired image-generated-sentence pgen; input sentences ˆysrc image-sentence pairs psrc unpaired data psrc source domain; sentences ˆytgt images xtgt target domain; model parameter psrc paired data source domain ´psrc unpaired data intentionally collected randomly shufﬂing images sentences source domain pgen source-imagegenerated-sentence pairs. adversarial training cross-domain image captioning system summarized fig. captioner critics learn together pursuing competing goals described below. captioner generates sentence would prefer sentence large reward implies large values contrast critics would prefer large values implies small values propose novel adversarial training procedure iteratively updating captioner critics algorithm short ﬁrst pretrain captioner using cross-entropy loss source domain data. then iteratively update captioner critics ratio critics updated often captioner learned critics also used measure quality computing using here expected value models randomness future words call method critic-based planning. critic-based planning takes advantage learned policy network well critics. default maxy generated word. howlect ever difference maximum probability second largest probability threshold take words according evaluate then select word highest value generated word. note sentences generated critic-based planning exactly greedy search. critic-based planning method obtain performance improvement typically dataset large domain shift experiments perform extensive evaluations number popular datasets. experiments mscoco source dataset cub- oxford- tgif flickrk target datasets. show method generalizes datasets large domain shift datasets regular domain shift also show critic-based planning improve performance during inference datasets large domain shift. finally conduct ablation study flickrk show contribution different components. implementation details data preprocessing. source domain dataset select mscoco training split contains images along captions each. prune vocabulary dropping words frequency less resulting words including special begin-ofsentence end-of-sentence tokens. vocabulary experiments. target domain datasets remove training sentences containing out-of-vocabulary words pre-training details. architecture captioner cnn-lstm hidden dimension image features extracted using pre-trained resnet- sentences represented one-hot encoding. ﬁrst pre-train captioner source domain dataset cross entropy objective using adam optimizer learning rate apply learning rate decay factor every three epoches. improve performance schedule sampling mitigate exposure bias. best model selected according validation performance serve initial model adversarial training. adversarial training details. train captioner critics using adam optimizer learning rate apply dropout training phase prevent over-ﬁtting also served input noise similar monte carlo rollout model samples words token current policy times. sentences critics estimating state-action value critics trained scratch using standard classiﬁcation objective. ﬁrst pre-train captioner mscoco training set. next update captioner adversarial training procedure unpaired data training target domains. finally evaluate method four target domain datasets representing different levels domain shift. baseline. re-implement deep compositional captioner baseline method. consists lexical classiﬁer language model. former model trained predict semantic attributes latter lstm model trained unpaired text. overall model combines models linear layer trained paired image-caption data. fair comparison apply following settings lexical classiﬁer resnet- model language model trained target domain sentences. note resnet- ﬁne-tuned visual concepts extracted captions. finally source domain image-caption pairs ﬁne-tune dcc. also ﬁne-tune pre-trained source domain model ditable results adaptation across four target domain datasets. source pre-trained baseline methods. fine-tuning paired data target domain serves upper bound performance cnn-rnn captioner. categorize three kinds domain shift between mscoco target datasets namely general v.s. ﬁne-grained descriptions difference verb usage subtle difference sentence style. general v.s. ﬁne-grained descriptions. large domain shift mscoco cub-/oxford- suggests challenging domain adaptation scenario. cub-/oxford- descriptions give detailed expressions attributes beak bird stamen ﬂower. contrast mscoco descriptions usually main scene character. illustrate differences word-level distribution among mscoco oxford- using venn-style word clouds rows fig. show model describe birds ﬂowers detailed also appearance ﬁne-grained object attributes. blocks table method outperforms source pretrained models considerable margin evaluation metrics. difference verb usage. next move towards verb usage difference source target domains. according motion verbs dance shake facial expressions tgif verbs mscoco mostly static ones stand sit. examples fig. show model accurately describe human activities object interactions. third panel table method also signiﬁcantly improves source pre-trained models. subtle difference sentence style. order test generalizability method conduct experiment using similar dataset target domain. bottom block table method also offers noticeable improvement. addition reverse route adaptation method also improves source pre-trained model method shows great potentials unsupervised domain adaptation across datasets regardless regular large domain shift. critic-based planning. instead directly generating word policy network take advantage adversary critics inference. results shown table threshold cub- oxford-. every time-step choose words according words determined critics cub- oxford- respectively. compared greedy search critic-based planning achieve better performance many evaluation metrics especially datasets large domain shift source domain dataset compared beam search beam size critic-based planning also typically gets higher performance. beam search method generates words depending captioner itself critic-based planning method acquires different point view critics. case regular domain shift critic-based planning achieves comparable performance beam search greedy search. impressive examples shown fig. ablation study figure examples captions domain adaptation four target domain datasets. last demonstrates failure cases generated captions accurately describe images. figure results critic-based planning. stands greedy search beam search critic-based planning. underlined words denote difference maximum probability second largest probability lower critic-based planning choose word maximum probability word colored red. table results proposed critic-based planning compared greedy search beam search. critic order analyze effectiveness critics ablation comparison either both. table shows using insufﬁcient since aware sentence style target domain. hand using contributes signiﬁcantly. finally combining achieves best performance evaluation metrics. argue vital cross-domain image captioning. table ablation study critic models flickrk. multi-modal critic domain critic. propose novel adversarial training procedure cross-domain image captioning. novel critic-based planning method naturally introduced improve caption generation process testing. method consistently outperforms baseline methods four challenging target domain datasets future would like improve ﬂexibility method combining multiple critics plug-and-play fashion. acknowledgement thank microsoft research asia mediatek --e-- support. also thank kuo-hao zeng shao-pin chang useful feedbacks internal review.", "year": 2017}