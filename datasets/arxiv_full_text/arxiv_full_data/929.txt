{"title": "Understanding Boltzmann Machine and Deep Learning via A Confident  Information First Principle", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Typical dimensionality reduction methods focus on directly reducing the number of random variables while retaining maximal variations in the data. In this paper, we consider the dimensionality reduction in parameter spaces of binary multivariate distributions. We propose a general Confident-Information-First (CIF) principle to maximally preserve parameters with confident estimates and rule out unreliable or noisy parameters. Formally, the confidence of a parameter can be assessed by its Fisher information, which establishes a connection with the inverse variance of any unbiased estimate for the parameter via the Cram\\'{e}r-Rao bound. We then revisit Boltzmann machines (BM) and theoretically show that both single-layer BM without hidden units (SBM) and restricted BM (RBM) can be solidly derived using the CIF principle. This can not only help us uncover and formalize the essential parts of the target density that SBM and RBM capture, but also suggest that the deep neural network consisting of several layers of RBM can be seen as the layer-wise application of CIF. Guided by the theoretical analysis, we develop a sample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and a CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are studied in a series of density estimation experiments.", "text": "typical dimensionality reduction methods focus directly reducing number random variables retaining maximal variations data. paper consider dimensionality reduction parameter spaces binary multivariate distributions. propose general conﬁdent-information-first principle maximally preserve parameters conﬁdent estimates rule unreliable noisy parameters. formally conﬁdence parameter assessed fisher information establishes connection inverse variance unbiased estimate parameter cram´er-rao bound. revisit boltzmann machines theoretically show single-layer without hidden units restricted solidly derived using principle. help uncover formalize essential parts target density capture also suggest deep neural network consisting several layers seen layerwise application cif. guided theoretical analysis develop sample-speciﬁc cif-based contrastive divergence algorithm cif-based iterative projection procedure rbm. cd-cif studied series density estimation experiments. recently deep learning models stacked denoising auto-encoder deep boltzmann machine etc.) drawn increasing attention impressive results various application areas computer vision natural language processing information retrieval despite practical successes debates fundamental principle design training deep architectures. important problem unsupervised pre-training would parameters better capture structure input distribution probabilistic modeling perspective process interpreted attempt recover model parameters generative neural network would well describe distribution underlying observed high-dimensional data. general suﬃciently depict original high-dimensional data requires high-dimensional parameter space. however overﬁtting usually occur model excessively complex. hand dauphin bengio empirically shows failure neural networks leveraging added capacity reduce underﬁtting. hence important uncover understand ﬁrst principle would reducing dimensionality parameter space concerning sample size. question also recognized erhan empirically show unsupervised pre-training acts regularization parameters parameters region better basins attraction reached. regularization parameters could seen kind dimensionality reduction procedure parameter spaces restricting parameters desired region. however intrinsic mechanisms behind regularization process still unclear. thus theoretical justiﬁcations needed order formally analyze essential parts target density neural networks capture. indepth investigation question lead signiﬁcant results formal explanation exactly neural networks would perform pre-training process; theoretical insights better. duin pe¸ekalska empirically show sampling density given dataset resulting complexity learning problem closely interrelated. initial sampling density insuﬃcient result preferred model lower complexity satisfactory sampling estimate model parameters. hand number samples originally abundant preferred model become complex could represented dataset details. moreover connection becomes complicated observed samples contain noises. obstacle incorporate relationship dataset preferred model learning process. paper mainly focus analyzing boltzmann machines main building blocks many neural networks novel information geometry perspective. assuming exists ideal parametric model general enough represent system phenomena goal parametric reduction derive lower-dimensional sub-model given dataset reducing number free parameters paper propose conﬁdent-informationfirst principle maximally preserve parameters highly conﬁdent estimates rule unreliable noisy parameters respect density estimation binary multivariate distributions. point view conﬁdence parameter assessed fisher information establishes connection inverse variance unbiased estimate considered parameter cram´er-rao bound worth emphasizing proposed principle parametric reduction fundamentally diﬀerent traditional feature reduction methods latter focus directly reducing dimensionality feature space retaining maximal variations data e.g. principle components analysis oﬀers principled method deal high-dimensional data parameter spaces strategy universally derived ﬁrst principle independent geometric metric feature spaces. takes information-oriented viewpoint statistical machine learning. information rooted variations imperfect observations transmits throughout whole learning process. idea also well recognized modern physics stated wheeler things physical information-theoretic origin participatory universe...observer participancy gives rise information; information gives rise physics.. following viewpoint frieden uniﬁes derivation physical laws major ﬁelds physics dirac equation maxwell-boltzmann velocity dispersion using extreme physical information principle information used uniﬁcation exactly fisher information measures quality measurement. terms statistical machine learning paper folds incorporate fisher information modelling intrinsic variations data give rise desired model using framework show examples existing probabilistic models e.g. comply principle derived main contributions propose general principle parameter reduction maximally preserve parameters high conﬁdence eliminate unreliable parameters binary multivariate distributions framework also give geometric interpretation showing maximally preserve expected information distance. implementation derivation probabilistic models illustrated revisiting widely used boltzmann machines i.e. single layer withhidden units restricted deep neural network consisting several layers seen layer-by-layer application cif. many kinds information deﬁned probability distribution e.g. entropy fisher information. entropy global measure smoothness fisher information local measure sensitive local rearrangement points based theoretical analysis cif-based iterative projection procedure inherent learning uncovered. traditional gradient-based methods e.g. maximum-likelihood contrastive divergence seen approximations experimental results indicate robust sampling biases separation positive sampling process gradient estimation. beyond general propose sample-speciﬁc integrate algorithm conﬁne parameter space conﬁdent parameter region indicated samples. leads signiﬁcant improvement series density estimation experiments sampling insuﬃcient. rest paper organized follows section introduces preliminaries general principle proposed section section analyze implementations using without hidden units i.e. rbm. that sample-speciﬁc cif-based learning method cifbased iterative projection procedure proposed experimentally studied section finally draw conclusions highlight future research directions section section introduce develop theoretical foundations information geometry manifold binary multivariate distributions given number variables i.e. open simplex probability distributions family probability distributions considered diﬀerentiable manifold certain parametric coordinate systems. case binary multivariate distributions four basic coordinate systems often used p-coordinates η-coordinates θ-coordinates mixed-coordinates mixed-coordinates vital importance analysis. probabilistic distribution. index regarded subset stands probability variables indicated equal complemented variables zero. example rob. note null also legal index p-coordinates indicates probability variables zero denoted p.... expectation taken respect probability distribution grouping coordinates orders η-coordinate system denoted superscript indicates order number corresponding parameter. example denotes parameters order number rob{xi n}}. θ-coordinate denoted subscript indicates order number corresponding parameter. note order indices locate diﬀerent positions following convention amari fisher information measures amount information data statistic carries unknown parameter fisher information matrix vital importance analysis inverse fisher information matrix gives asymptotically tight lower bound covariance matrix unbiased estimate considered parameters moreover fisher information matrix distance metric invariant re-parameterization proved unique metric invariant random variables corresponding suﬃcient statistic another important concept related analysis orthogonality deﬁned fisher information. coordinate parameters called orthogonal fisher information vanishes i.e. meaning inﬂuences likelihood function uncorrelated. technical meaning orthogonality maximum likelihood estimates orthogonal parameters independently performed. distribution almost infeasible determine coordinates high-dimensional parameter space acceptable accuracy reasonable time. given target distribution general manifold consider problem realizing lowerdimensionality submanifold. deﬁned problem parametric reduction multivariate binary distributions. section formally illuminate general parametric reduction. intuitively construct coordinate system conﬁdences parameters entail natural hierarchy high conﬁdent parameters signiﬁcantly distinguished conﬁdent ones general conveniently implemented keeping high conﬁdent parameters unchanging setting lowly conﬁdent parameters neutral values. however choice coordinates crucial usage. strategy infeasible terms p-coordinates η-coordinates θ-coordinates since easy hierarchies conﬁdences coordinate systems signiﬁcant shown example later section. following propositions show mixed-coordinates meet requirement realizing general cif. mixed-coordinates deﬁned section proposition gives closed form calculating fisher information matrix entail natural hierarchy ﬁrst part high conﬁdent parameters separated second part conﬁdent parameters neutral value moreover parameters orthogonal ones indicating figure projecting point submanifold l-tailored-mixedcoordinates gives desirable maximally preserve expected fisher information distance projecting ε-neighborhood centered onto could estimate parts independently hence implement general principle parametric reduction replacing conﬁdent parameters neutral value zeros reconstructing resulting distribution. turns submanifold tailored becomes call l-tailored-mixed-coordinates. grasp intuitive picture general strategy signiﬁcance w.r.t mixed-coordinates consider example conﬁdences coordinates given diagonal elements corresponding fisher information matrices. applying -tailored mixed-coordinates loss ratios fisher information ratio fisher information tailored parameter remaining parameter smallest fisher information hand ratios become respectively. entails submanifold l-tailored-mixed-coordinates however exist many diﬀerent submanifolds question exist general criterion distinguish projection best? principle exist right one? following proposition shows general entails geometric interpretation illuminated figure would lead optimal submanifold proposition given statistical manifold l-mixed-coordinates corresponding l-tailored-mixed-coordinates free parameters. then among k-dimensional submanifolds submanifold determined maximally preserve expected information distance induced fisher-rao metric. previous section general uncovered coordinates multivariate binary distributions. consider implementations equals using boltzmann machines speciﬁcally show kinds i.e. single layer without hidden units restricted indeed instances following general principle. case application interpreted perspectives algebraic geometric interpretation. many neural networks ﬁxed architecture high-order deep belief networks proposed approximately realize underlying distributions diﬀerent application scenarios. neural networks designed fulﬁll parametric reduction certain tasks specifying number adjustable parameters namely number connection weights number biases. believe exists general criterion design structure neural submanifolds particular application hand problem parametric reduction equivalent choice submanifolds. next brieﬂy introduce general gradient-based learning algorithm. note related fundamentally diﬀerent m-projection amari amari focuses problem projecting point submanifold shows m-projection point closest actually m-projection special case -projection present paper focus problem developing general criterion could help optimal submanifold project interactions visible-hidden interactions visible self-connections hidden selfconnections diagonals zero. express boltzmann distribution joint space below given sample generated underlying distribution maximum-likelihood commonly used gradient ascent method training order maximize log-likelihood parameters based equation log-likelihood given follows following sections revisit special namely theoretically show derived using principle. helps formalize essential parts target density capture. logarithm represented linear decomposition θ-coordinates shown equation since impractical recognize coordinates target distribution would like approximate part k-dimensional submanifold number free parameters. here dimensionality i.e. candidate submanifolds comparable submanifold endowed next rationale underlying design msbm illuminated using general perspectives algebraically geometrically. -mixed-coordinates applying general parametric reduction rule preserve high conﬁdent part parameters replace conﬁdent parameters ﬁxed neutral value zero. thus derive -tailored-mixed-coordinates optimal approximation k-dimensional submanifolds. hand given -mixed-coordinates projection msbm proved thus deﬁnes probabilistic parameter space exactly derived cif. next corollary following proposition shows geometric derivation sbm. make explicit projection msbm could maximally preserve expected information distance comparing tailored submanifolds dimensionality corollary given general manifold -mixed-coordinates deﬁnes kdimensional submanifold maximally preserve expected information distance induced fisher-rao metric. cif-based derivation conﬁnes statistical manifold parameter subspace spanned directions high conﬁdences proved maximally preserve expected information distance. exactly traditional gradient-based learning algorithms intend train sbm. next proposition shows method training equivalent learn tailored -mixed coordinates previous section general uncovers uses coordinates ndorder i.e. preserves η-coordinates st-order nd-order. section investigate cases hidden units introduced. particularly fundamental problem neural network research unsupervised representation learning attempts characterize underlying distribution discovery latent variables many algorithmic models proposed restricted boltzmann machine auto-encoders learning level feature extraction. then deep learning models representation learnt level used input learning next level etc. however important questions remain clariﬁed algorithms implicitly learn whole density aspects? capture essence target density formalize link essential part omitted part? section answer questions using cif. concept compactness neural network two-folds. model-scale compactness restriction number hidden units order give parsimonious representation w.r.t underlying distribution; structural compactness restriction hidden units connected redundancy hidden representation minimized. paper mainly focus latter case. general manifold probability distributions joint space visible units hidden units general manifold visible units given observation distribution problem marginal distribution best approximates consistent compactness completeness conditions. here divergence deﬁned equation used criterion approximation. input variables equation similarly compactness corresponds extraction statistically independent hidden variables given input i.e. contains hidden variables equation given following coordinate system comparing equation submanifold mrbm deﬁned equivalent since share exactly coordinate system. indicates compactness completeness conditions indeed realized rbm. simpler notation denote mrbm. next show interpret training process rbm. amari proposed similar iterative algorithm framework fully-connected present paper reformulate iterative algorithm learning give explicit expressions projections achieved. show process projection derived i.e. highly conﬁdent coordinates preserved lowly conﬁdent coordinates neutral value zero. fractional coordinates system closed form fisher information matrix good expression formula like proposition possessed mixed-coordinate system next show fractional mix-coordinates derived three steps jointly applying completeness compactness conditions. corresponding -mixed ζ-coordinates first apply general parametric reduction replacing lowly conﬁdent coordinates neutral value zeros preserving remaining coordinates resulting tailored mix-coordinates described section then transmit fractional coordinate system i.e. finally completeness compactness conditions require also neutral value zeros. hence coordinates projection exactly given equation note second phase hidden units visible samples thus sub-learning task similar training without hidden units implemented traditional gradient-based methods. given current parameters samples share sampling process sampling projection phase positive phase terms quality sampling process suﬃcient positive phase achieve accurate estimation hand insuﬃcient sampling biases respect introduced sampling process accurate estimation guaranteed. however updating rule diﬀerent realizes parameter updating using sub-learning task adjusts parameters directly using equation denote distribution denote stationary distributions parameter updating using respectively. proper learning rate parameter updating phase i+]. since therefore would lead decrease divergence projection seen unmature projection guarantee theoretical projection reached. achieve projection point needs multiple updating iterations iteration moves current distribution towards gradient direction oracle step size another diﬀerence separates positive sampling process gradient estimation phases meaning positive sampling sub-learning however needs constantly adjust gradient direction respect certain learning rate immediately sampling process. later experiments section indicate give advantage robustness sampling biases especially gradient small distinguishable biases learning process. deep boltzmann machine several layers compose deep architecture order achieve representation suﬃcient abstraction level hidden units trained capture dependencies units lower layers shown figure section give discussion theoretical insights deep architectures terms principle. figure multi-layer visible units hidden layers greedy layer-wise training deep architecture maximally preserve conﬁdent information layer layer. note prohibition sign indicates fisher information lowly conﬁdent coordinates preserved. seen composition series representation learning stages. then immediate question kind representation data generated output stage? information abstraction point view stage deep architecture could build abstract features using highly conﬁdent information parameters transmitted less abstract features lower layers. abstract features would potentially greater representation power principle describes information ﬂows representation transformations illustrated figure propose layer determines submanifold could maximally preserve highly conﬁdent information parameters shown section whole seen process repeatedly applying layer achieving tradeoﬀ abstractness representation features intrinsic information conﬁdence preserved parameters. good representation found level layer-wise application unsupervised greedy pre-training used initialize train deep neural networks supervised learning recall straightforward application gradient-based methods train layers simultaneously tends fall poor local minima next question layer-wise pre-training give reasonable parameter initialisation? empirically erhan shows unsupervised pretraining acts regularization parameters parameters region better basins attraction reached. theoretically using fractional mixed coordinates shown regularized region actually layer-wise restriction using i.e. highly conﬁdent coordinates preserved respect target density lowly conﬁdent coordinates neutral value zero illustrated figure eﬀectively parameter space regularized fall region parameters conﬁdently estimated based given data. cif-based regularization pre-training seen searching reasonable parameter setting good representation input data generated layer. section empirically investigate principle density estimation tasks types boltzmann machines i.e. rbm. speciﬁcally investigate take eﬀect learning trajectory respect speciﬁc sample hence conﬁne parameter space region corresponding conﬁdent information contained given data. inconvenient sample-speciﬁc strategy since information hidden variables missed. alternatively investigate potential iterative projection procedure proposed previously. baseline learning methods i.e. contractive divergence maximum-likelihood adopted. learning described section seen approximation underlying probability distribution sample generated independently. goal train equation based realizes faithfully possible. comparing learning realizes gradient descend diﬀerent objective function avoid diﬃculty computing log-likelihood gradient shown follows artiﬁcial binary dataset ﬁrst randomly select target distribution chosen uniformly open probability simplex random variables. then dataset samples generated news groups binary dataset news groups collection approximately newsgroup documents partitioned evenly across diﬀerent newsgroups collection preprocessed using porter stemmer stop-word removal. select terms highest frequency collection. document represented -dimensional binary vector element indicates whether certain term occurs current document not. solution apply take eﬀect learning trajectory respect speciﬁc samples hence conﬁne parameter space region indicated conﬁdent information contained samples. experiment shows given speciﬁc samples need preserve conﬁdent parameters certain extend exist golden ratio would produce best performance average. main modiﬁcation cif-based algorithm generate samples based parameters conﬁdent information conﬁdent information carried certain parameter inherited sample could assessed using fisher information computed terms sample. expectations estimated given sample weights whose fisher information less considered unreliable w.r.t. practice could setup ratio specify remaining summary cd-cif realized phases. ﬁrst phase initially guess whether certain parameter could faithfully estimated based ﬁnite sample. second phase approximate gradient using scheme except cif-based ﬁring function used. section empirically investigate justiﬁcations principle especially sample-speciﬁc cif-based learning works context density estimation. experimental setup evaluation metric computation simplicity artiﬁcial dataset -dimensional. three learning algorithms investigated cd-cif. divergence used evaluate goodness-of-ﬁt figure performances cd-cif diﬀerent sample sizes; performances cd-cif various values typical sample sizes i.e. learning trajectories last steps cd-cif focus case variable number relatively small order analytically evaluate divergence give detailed study algorithms. changing number variables oﬀers trivial inﬂuence experimental results since obtained qualitatively similar observations various variable numbers automatically adjusting diﬀerent sample sizes fisher information additive i.i.d. sampling. sample size changes naturally require total amount fisher information contained tailored parameters steady. hence constant learning model underlying distribution family given. turns ﬁrst identify using optimal w.r.t. several distributions generated underlying distribution family determine optimal various sample figure cd-cif method shows signiﬁcant improvements cd-. could expect reliable identiﬁcations model parameters insuﬃcient samples hence cd-cif gains advantages using parameters could conﬁdently estimated. result consistent previous theoretical insight fisher information gives reasonable guidance parametric reduction conﬁdence criterion. sample size increases large samples model parameters reasonably estimated hence eﬀect parameter reduction using gradually becomes marginal. figure figure show sample size aﬀects interval achieves improvements cd-. cd-cif achieves signiﬁcantly better performances wide range while cd-cif marginally outperform baselines narrow range start three methods parameter initialization. intermediate state represented -dimensional vector formed current parameter values. figure that ﬁnal steps three methods seem staying diﬀerent regions parameter space cd-cif conﬁnes parameter relatively thinner region compared cd-; true distribution usually located side cd-cif indicating potential converging optimal solution. note claims based general observations figure shown illustration. hence conclude cd-cif regularizes learning trajectories desired region parameter space using sample-speciﬁc principle. section empirically investigate sample-speciﬁc cif-based learning works real-world datasets context density estimation. particular learn underlying probability density terms news groups binary dataset. learning rate cd-cif manually tuned order converge properly since infeasible compute devergence high dimensionality averaged hamming distance samples dataset generated used evaluate goodness-of-ﬁt documents document -dimensional binary vector. evaluate parameter ξsbm ﬁrst randomly generate samples stationary distribution denoted vn}. averaged hamming distance dham practically interesting since higher representational power. section compare three diﬀerent learning algorithms carreira-perpinan hinton shown biased respect almost data distributions. section compared theoretically. section empirical study three algorithms conducted. experimental setup evaluation metric computational simplicity artiﬁcial dataset dimensionality number hidden units three learning algorithms investigated divergence used evaluate goodness-of-ﬁt rbm’s trained various algorithms. diﬀerent sample sizes tested namely sample size learning rates observe could converge properly. sub-learning phase adopt algorithm training without hidden units whose learning rate also need scan dataset multiple times order iteratively update parameters full scan whole dataset called epoch. maximal number epoches iterations iteration sub-training maximal number epoches setting adopt baseline method. results analysis on-average performances three methods dataset diﬀerent scales shown table context density estimation. order study behavior plot sequences divergence target distribution iteration along whole learning trajectory shown figure comparing divergences decrease similar converging rate taking number iterations converge given tolerance consistent conclusion hinton convergence behavior shown figure general trend divergence decreases steadily small ﬂuctuations. since iterations reasonable select best performance reached whole learning process called best divergence sample distribution rbm’s stationary distribution adopted selection metric. thus addition converging performance also show best performance selected among iterations table note best performances reported since converging performances often approximately best ones. small sample size converging performance comparable respect sample size increases gradually outperforms shows signiﬁcant improvement large sample figure illustrate averaged learning curves randomly chosen data distributions sample size respectively. x-axis scale. compare eﬃciency diﬀerent algorithms along time-line plot three time stamps note iteration contains sub-training task trained using epoch experiment. size best performance signiﬁcantly better sample sizes indicating potential improve performance using suitable selection metric. comparing takes much shorter iterations expected achieve performance threshold. theoretically converge local minimum based theoretical analysis section proper learning rate selected also converge local minimum. however interesting result sometimes diﬀerence convergence points even cases sampling suﬃcient practice needs constantly positive negative sampling updating produce much sample biases. gradient decreasing small value correct gradient direction ﬂuctuated sample biases. thus instead converging local minimum might ﬂuctuate around sub-optimal region. actually sampling process also introduce sample biases. ﬂuctuated sample insuﬃcient shown figure given suﬃcient samples sampling biases decreases ﬂuctuation declines shown figure though sampling biases whole dataset becomes small given suﬃcient samples gradient estimation sample still closely intergraded sample bias certain sample meaning inseparable coupling relationship results biased gradient estimation. main advantage procedure traditional gradient-based methods separation section empirically investigate works real-world datasets context density estimation. learn probability density terms news groups binary dataset. number hidden units learning rate cd-and manually tuned order converge properly learning rate sub-learning task also iterations settings similar experiments section averaged hamming distance used evaluate goodness-of-ﬁt rbm’s trained various algorithms. average hamming distances shown figure achieves better performances settings hamming distance drops dramatically increases. trend explained follows. grows sampling biases increases interference sampling biases respect gradient estimation becomes serious. limits actual performance rbms learnt respect growing modelling power gained increasing shown section procedure separates positive sampling process gradient estimation phases result shows potential achieve optimal solutions reach real-world applications. principle proposed paper tackles problem dimensionality reduction parameter space preserving parameters highly conﬁdent estimates tailoring less conﬁdent parameters. provides strategy derivation probabilistic models. speciﬁc examples regard. theoretically shown achieve reliable representation parameter spaces exactly using general principle. gives principled context-independent address questions parameter reduction based also show deep neural networks consisting several layers seen layer-wise usage leading theoretical interpretations rationale behind deep learning models. interesting result shown experiments that although cd-cif biased algorithm could signiﬁcantly outperform sample insuﬃcient. suggests gives reasonable criterion recognizing utilizing conﬁdent information underlying data fails another interesting observation cif-based lead diﬀerent convergence points training rbm. experimental results indicate advantage robustness sampling biases separation positive sampling process gradient estimation. future develop formal justiﬁcation w.r.t various contexts also conduct extensive experiments real world applications document classiﬁcation handwritten digit recognition justify properties also extend train deep neural networks. work partially supported chinese national program basic research project natural science foundation china european union framework marie-curie international research staﬀ exchange programme according cram´er-rao bound parameter unique asymptotically tight lower bound variance unbiased estimate given corresponding element inverse fisher information matrix involving parameter recall index parameters shared index parameters shared manifold i.e. denotes euclid norm coordinates neighbor uniformly sampled ζq+dq corresponding coordinates. small calculate expected information distance follows since fisher information matrix positive deﬁnite symmetric exists singular value decomposition orthogonal matrix diagonal matrix diagonal entries equal eigenvalues index coordinates choose form tailored submanifold mixedcoordinates based equation expected information distance proportional eigenvalues sub-matrix equals trace next show sub-matrix speciﬁed gives maximum trace. based proposition elements main diagonal sub-matrix lower bounded upper bounded one. therefore gives maximum trace among sub-matrices completes proof. proof msbm probability distributions realized sbm. amari proves mixed-coordinates resulting projection msbm given -mixed-coordinates msbm equivalent submanifold tailored i.e. corollary follows proposition since deﬁnes e-ﬂat submanifold msbm converges unique solution gives best approximation msbm converges hence thus converges stationary distribution preserves coordinates completes proof. divergence gradient vector free parameters {θxi equals zero vector. then also zero-gradient vector hence projection point however since e-ﬂat projection note similar path proof also used theorem amari fully-connected here here reformulate proof derive projection information concept ﬂatness please refer book amari nagaoka treated function rbm’s parameters learning rate. shown albizuri gradient descent method converges minimum divergence proper choices hence achieves projection point last show fractional mixed coordinates equation exactly convergence point learning calculate ﬁrst-order derivative w.r.t", "year": 2013}