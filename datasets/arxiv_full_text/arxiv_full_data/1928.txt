{"title": "Learning Robust Visual-Semantic Embeddings", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.", "text": "zero-shot learning aims performing speciﬁc tasks recognition retrieval novel classes label information available training hand few-shot learning enables labeled examples of-interest categories order compensate missing information zero fewshot setting model learn associate novel concepts image examples textual attributes transfer knowledge training test classes. common strategy deriving visual-semantic embeddings make images textual attributes supervised speciﬁcally learn transformations images textual attributes objective transformed visual semantic vectors class similar joint embeddings space. despite good performance common strategy basically boils supervised learning setting learning labeled paired data only. paper show learn better joint embeddings across different data modalities beneﬁcial combine supervised unsupervised learning labeled unlabeled data. many existing methods learning joint embedding images text supervised information paired images textual attributes. taking advantage recent success unsupervised learning deep neural networks propose end-to-end learning framework able extract robust multi-modal representations across domains. proposed method combines representation learning models together cross-domain learning criteria learn joint embeddings semantic visual features. novel technique unsupervised-data adaptation inference introduced construct comprehensive embeddings labeled unlabeled data. evaluate method animals attributes caltech-ucsd birds dataset wide range applications including zero fewshot image recognition retrieval inductive transductive settings. empirically show framework improves current state many considered tasks. past years availability large amount data advancement training techniques learning effective robust representations directly images text becomes feasible learned representations facilitated number high-level tasks image recognition sentence generation object detection despite useful representations developed speciﬁc domains learning comprehensive representations across different data modalities remains challenging. practice complex tasks image captioning image tagging often involve data different modalities additionally learning process would faster requiring fewer labeled examples hence scalable handling large number categories could transfer cross-domain knowledge effectively motivates learning multi-modal emauto-encoder instead learning representations directly align visual textual inputs choose learn representations auto-encoder using reconstruction objective. second impose crossmodality distribution matching constraint require embeddings learned visual textual auto-decoders similar distributions. minimizing distributional mismatch visual textual domain show improved performance recognition retrieval tasks. finally achieve better adaptation unlabeled data perform novel unsupervised-data adaptation inference technique. show adopting technique accuracy increases signiﬁcantly method also many existing models. fig. illustrates overall end-to-end differentiable model. summarize proposed method successfully combines supervised unsupervised learning objectives learns labeled unlabeled data construct joint embeddings visual textual data. demonstrate improved performance animals attributes caltech-ucsd birds datasets image recognition image retrieval tasks zero few-shot setting. zero-shot few-shot learning related problems somewhat different setting training data. few-shot learning aims learn speciﬁc classes examples zero-shot learning aims learn even examples classes presented. setting zero-shot learning rely side information provided domains. case image recognition often comes form textual descriptions. thus focus zero-shot image recognition derive joint embeddings visual textual data missing information speciﬁc classes could transferred textual domain. since relation pixels text descriptions non-trivial previous work relied learning embeddings large amount data. witnessing success deep learning extracting useful representations much existing work mostly applies deep neural networks ﬁrst transform pixels text informative representations followed using various techniques identify relation between them. example socher used deep architectures learn representations images text used bayesian framework perform classiﬁcation. norouzi introduced simple idea treated classiﬁcation scores output deep network weights convex combination word vectors. proposed method learns projections low-level visual textual features form hypergraph embedding space performed label propagation recognition. number similar methods learn transformations input image representations semantic space recognition retrieval purposes number recent approaches also attempt learn entire task deep models end-to-end fashion. frome constructed deep model took visual embeddings extracted word embeddings input trained model objective visual word embeddings class well aligned linear transformations. predicted output weights convolutional fully connected layers deep convolutional neural network. instead using textual attributes word embeddings model reed proposed train neural language model directly text goal encoding relevant visual concepts various categories. visual semantic knowledge transfer developed multi-task deep visualsemantic embeddings model selecting video thumbnails based side semantic information incorporating knowledge objects similarities visual semantic domains tang improved object detection semisupervised fashion. kottur proposed learn visually grounded word embeddings showed improvements text word embeddings various challenging tasks. reed designed text-conditional convolutional architecture synthesize image text. recently wang introduced structure-preserving constraints learning joint embeddings images text image-to-sentence sentence-to-image retrieval tasks. unsupervised multi-modal representations learning contributions effectively combine supervised unsupervised learning tasks learning multi-modal embeddings. inspired supported several previous works provided evidence unsupervised learning tasks could beneﬁt cross-modal feature learning. proposed various models based restricted boltzmann machine deep belief network deep auto-encoder perform feature learning multiple modalities. derived multi-modal features demonstrated improved performance single-modal features audio-visual speech classiﬁcation tasks. srivastava salakhutdinov developed multimodal deep figure shows basic formulation visualsemantic embeddings model. method built basic architecture adding additional components well modifying existing ones. although basic architecture provides utilize label information training learning process could beneﬁt unlabeled data provided time. speciﬁc propose combine supervised unsupervised learning objectives together incorporating auto-encoders image text data. typical setting auto-encoders consists symmetric encoder-decoder architecture hidden representations middle compact representations could used reconstruct original input data. model auto-encoders added image text data processed pre-trained networks. learning visual embeddings contractive auto-encoder proposed able learn robust visual codes images class. given visual feature vector contractive auto-encoder maps hidden representation seeks reconstruct ˜vh. denote reconstructed vector model parameters thus learned minimizing regularized reconstruction error hand given semantic feature vector textual attribute vanilla auto-encoder ﬁrst encode reconstruct hidden representation hence minimize reconstruction error boltzmann machine fusing together multiple diverse modalities even absent. providing inputs images text generative model manifested noticeable performance improvement classiﬁcation retrieval tasks. first deﬁne problem setting correspond}ntr notation. denote labeled training }nut images classes denote unlabeled training images possibly different classes }nte denote test images novel classes. class following textual attributes either provided human annotated attributes learned unsupervised text corpora denote class-speciﬁc tex}cut tual attributes labeled training unlabeled training test classes respectively. denotes model parameters. also consider few-shot learning labeled training images available test classes. following omit model parameters brevity. basic formulation goal learning multi-modal embeddings formulated learning transformation functions given image textual attribute similar class. much previous work learning multi-modal embeddings generalized formulation. instance cross-modal transfer viewed pre-deﬁned feature extraction model followed two-layer neural network identity matrix. speciﬁc learning nonlinear projection directly visual features semantic word vectors. past years deep architectures shown learn useful representations could embed high-level semantics visual textual data. gives rise attempts applying successful deep architectures learn example devise designed model followed linear transformation matrix. hand adopted well known skip-gram text modeling architecture learning text wikipedia. worth noting indicates encoding positive negative classes denotes dot-product. worth noting adopt different loss functions including binary cross-entropy loss multi-class hinge loss. however empirically using simple binary prediction loss results best performance model. refer unsupervised-data adaptation inference acts self-reinforcing strategy using unsupervised data unknown labels. intuition minimizing adapt unlat based beled data learning empirical predictions. choice inﬂuence effectiveness. however setting works quite well many methods considered work. representing trade-off parameters different components. note also view unsupervised objective regularizer learning robust visual textual representations computing loss useful perform normalization output scores along batch-wise direction. viewed mixture batch normalization layer normalization idea simple encourage competence different instances data batch rather across different categories. practice access large unlabeled test inputs easily incorporate reconstruction loss. experimental results visual textual auto-encoders image text data transformed visual textual embeddings meaningful representations. order transfer knowledge across modalities impose discriminative constraints hidden representations learned auto-encoders discuss next. distributions matching technique proven effective transferring knowledge modality another common nonparametric analyze compare distributions maximum mean discrepancy criterion. view two-sample test thus loss formulated distributions visual textual embeddings feature canonical form reproducing kernel hilbert space endowed characteristic kernel note kernel criterion must universal kernel thus empirically choose gaussian kernel minimize criterion visual textual embeddings minimize regarded shrinking information across data modalities. experiments loss helps improve model performance recognition retrieval tasks zero few-shot setting. experiments denote proposed method revise extensive experiments zero few-shot image recognition retrieval tasks conducted using benchmark datasets animals attributes caltech-ucsd birds ﬁne-grained dataset objects visually semantically similar general concept dataset. training test splits table lists statistics datasets. verify performance method consider state-of-the-art deep-embeddings methods devise devise viewed special case proposed method difference devise learns nonlinear transformation visual images textual attributes alignment purpose learns nonlinear transformation visual semantic embeddings. choose googlenet model devise architecture. textual attributes classes consider three alternatives human annotated attributes wordvec attributes glove attributes continuous attributes judged humans contains attributes contains attributes. unsupervised methods obtaining distributed text representations words. pre-extracted wordvec glove vectors wikipedia provided dim. features. network design training procedure following partitioning strategy split dataset classes dataset classes labeled training/ unlabeled training/ test data. adopt attributes textual description class. zero-shot learning labels images unknown unlabeled training test classes also disjoint across labeled training/ unlabeled training/ test splits. verify unlabeled training data could beneﬁt learning revise provide four variants revisea reviseb revisec revise. revisea consider supervised objective. reviseb take unsupervised objective labeled training data account; lreconstruct considered lunsupervised labeled training data. next revisec consider unlabeled training data lunsupervised withunsupervised-data adaptation technique last revise denotes complete training architecture. completeness also consider technique unsupervised-data adaptation inference devise words also evaluate devise beneﬁt unlabeled training data. adopt procedure report results devise* cmt* respectively. similar results comparisons reported using top- classiﬁcation accuracy mean average precision recognition retrieval tasks respectively unlabeled training test images. speciﬁc deﬁne prediction score given image sults provided ranking test classes. training. example dataset test images retrieval deterioration devise devise* recognition deterioration cmt*. hand proposed method enjoys recognition improvement retrieval improvement reviseb revisec. shows learning method proposed architecture actually beneﬁt unlabeled training data tut. next examine different variants proposed architecture. comparing average results revisea reviseb observe recognition improvement retrieval improvement. indicates taking unsupervised objectives lreconstruct account results learning better feature representations thus yields better recognition/ retrieval performance. moreover unsupervised-data adaptation technique introduced enjoy average recognition improvement average retrieval improvement revisec revise. worth noting signiﬁcant performance improvement unlabeled training images veriﬁes unsupervised-data adaptation technique leads accurate prediction vut. transductive zero-shot learning subsection extend experiments transductive setting test data available training. therefore test data regarded unlabeled training data perform experiments table split dataset disjoint classes dataset disjoint classes labeled training/ test data. order evaluate different components revise provide variants revise† revise††. revise† consider distributional matching codes across modalities revise†† consider contractive loss visual auto-encoder similar previous subsection also consider devise* cmt* revisec evaluate effect unsupervised-data adaptation inference. state-of-the-art methods large margin. average least gain compared methods without using unsupervised objective gain compared devise* cmt*. note methods work better human annotated attributes unsupervised attributes dataset. possible reason visually semantically similar classes ﬁne-grained dataset attributes obtained unsupervised cannot fully differentiate them. nonetheless general concept dataset using either supervised unsupervised textual attributes performance differ much. instance method achieves comparable performance using dataset. recognition performance devise* cmt* compared devise veriﬁes using unsupervised-data adaptation inference technique beneﬁt transductive zero-shot recognition. furthermore variants revise using unsupervised-data adaptation inference noticeable improvement devise* cmt*. shows proposed model succeeds leveraging unsupervised information test data constructing effective cross-modal embeddings. next evaluate effects different components designed architecture. first compare results revise† revise. performance gain indicates minimizing distance visual textual codes enables model learn robust visualsemantic embeddings. words better associate cross-modal information match distributions across visual textual domains second observe that without contractive loss performance slightly drop surprising since contractive auto-encoder aims learning less varying features/codes similar visual input therefore expect learn robust visual codes. finally similar observations found comparing devise/cmt devise*/cmt* unsupervised-data adaptation inference revise substantially improves average top- classiﬁcation accuracy please supplementary material detailed comparisons following non-deep-embeddings methods conse eszsl jlse latem sync leverage unsupervised information yield better performance compared methods using unsupervised objective. however cases performance drops take unsupervised information account. example dataset devise* performs unfavorably compared devise word embeddings used textual attributes. overall method help improve zero-shot retrieval least compared cmt*/devise* compared cmt/devise. clearly demonstrates effectiveness leveraging unsupervised information improving zero-shot retrieval addition quantitative results also provide qualitative results revise. fig. image retrieval experiments classes chestnut sided warbler white eyed vireo. given class embedding nearest image neighbors retrieved based cosine similarity transformed visual textual features. consider conditions images class chestnut sided warbler images test classes. images correctly classiﬁed also observe three nearest image neighbors also chestnut sided warbler. hand images correctly classiﬁed white eyed vireo three nearest image neighbors form wrong class wilson warbler. test images unsupervised objective dataset attributes. alter fraction unlabeled test images used training stage step size portion test images contributes lunsupervised. fig. clearly indicates performance increases increases. unsupervised information available model better associate supervised unsupervised data. another interesting observation test images available revise achieves favorable performance transductive zero-shot recognition retrieval. expand test-time search space note methods consider that test time queries come test classes. dataset attributes expand test-time search space training test classes perform transductive zero-shot recognition devise* cmt* revise. discover severe performance drops similar results also observed non-deep-embeddings methods. although challenging remains interesting consider generalized zero-/few-shot learning setting future work. subsection extend experiments transductive zero-shot transductive few-shot learning. compared zero-shot learning few-shot learning allows labeled images test classes. here images randomly chosen labeled test category. performance comparison metrics sec. report results. transductive few-shot recognition retrieval tables list results transductive few-shot recognition retrieval tasks. generally speaking revise achieves best performance compared variants methods. moreover expected compare results transductive zero-shot recognition retrieval every methods perform better labeled images observed test classes. example dataset attributes recognition improvement cmt* figure original features reconstructed features visual codes dataset revise transductive zero-shot setting. textual attributes classes. different colors denote different classes. best viewed colors. figure output features devise* cmt* revise. attributes used dataset transductive zero-shot setting. different colors denote different classes. best view colors. also observe performance proposed revise methods reduced compared transductive zero-shot learning. instance average retrieval performance revise improvement devise* zero-shot experiments improvement few-shot experiments. t-sne visualization figure shows t-sne visualization original features reconstructed visual features visual codes dataset attributes transductive zero-shot setting. first observe reconstructed features visual codes separate clusters different classes suggest revise learned useful representations. another interesting observation afﬁnities classes might change learning visual codes. example leopard images near humpback whale images original feature space. however visual code space leopard images humpback whale images. possible explanation know leopard semantically next provide t-sne visualization output visual test scores devise* cmt* revise fig. clearly revise better separate instances different classes. conclusion paper showed augment typical supervised formulation unsupervised techniques learning joint embeddings visual textual data. empirically evaluate proposed method general ﬁne-grained image classiﬁcation datasets comparisons state-of-the-art methods zero-shot few-shot recognition retrieval tasks inductive transductive setting. experiments method consistently outperforms methods substantially improving performance cases. believe work sheds light advantages combining supervised unsupervised learning techniques makes step towards learning useful representations multimodal data.", "year": 2017}