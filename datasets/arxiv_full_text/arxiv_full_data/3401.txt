{"title": "Inducing Interpretable Representations with Variational Autoencoders", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We develop a framework for incorporating structured graphical models in the \\emph{encoders} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.", "text": "develop framework incorporating structured graphical models encoders variational autoencoders allows induce interpretable representations approximate variational inference. allows perform reasoning structural constraints given graphical model deep generative models deal messy highdimensional domains often difﬁcult model variation. learning framework carried end-to-end variational objective applying unsupervised semi-supervised schemes. reasoning complex perceptual domains vision often involves facets ability effectively learn ﬂexible representations complex high-dimensional data ability interpret representations structured form. former measure well capture relevant information data latter means employing consistent semantics such effort help diagnosis enable composition improve generality. probabilistic graphical models enable structured representations often perceptual domains vision require extensive speciﬁcation signiﬁcant feature engineering useful. variational autoencoders form generative model manually speciﬁed feature extractors replaced neural networks. here parameters generative model approximation true posterior called recognition model learned simultaneously. however particular feature approximations exhibit entangled non-interpretable latent representations virtue fact approximating distributions assumed take general ﬂexible form; typically multivariate normal. contribution extends combination deep neural networks graphical models allow arbitrarily structured graphical models variational approximations enforces latent representations conform types structure provided graphical model. structure alone insufﬁcient encourage disentangled representations extend framework perform semi-supervised learning using handful labelled data help disentangle latent representation. framework employs single variational objective parameters generative recognition models learned simultaneously. shares features motivation goals variety recent work. kingma explores ability perform semi-supervised learning setting. accomplished partitioning latent space structured unstructured random variables providing labels structured variables. kulkarni employ particular interpretable model latent space component independent others providing weak supervision customized training procedure rather explicit labels. build work semi-supervised learning extending general models structures latent space. sohn perform fully-supervised learning particular case latents labels taken conditioned data. closest spirit motivation recent work johnson also involves combining graphical models vaes unsupervised learning. employed means extend class problems graphical model inference performed effectively involving relaxation conjugacy constraints likelihoods. finally schulman provides general method estimating gradients stochastic computations applied models structured latent spaces discrete latent variables eslami additional contribution work package torch permits simple simultaneous speciﬁcation deep generative models structured latent spaces corresponding inference networks. fundamentally wish learn parameters graphical model chosen model data. typically generative model data xand latents denoted would like estimate posterior latents given data denoted order extract representation. wish extract interpretable representation corresponds constraining model learning whose posterior distribution amenable human inspection. although general case computation exact posterior distribution intractable recent advances deep generative models enable variational autoencoder learn parametrised approximation here variational approximation used surrogate exact posterior constrained match true posterior pθ). however since cannot actually evaluate true posterior optimises alternate objective called evidence lower bound lower bounds marginal likelihood here generative model parameters recognition model parameters characterised neural networks learned simultaneously. elbo objective also reformulated indicate approximating distribution used along prior latents regularise standard autoencoder objective expected likelihood. recent approaches deep generative modelling places constraints structure generative model incorporate encoder model principal reasons. firstly mean-ﬁeld approximation typically assumed poor perceptual domains vision. complex dependencies arise posterior intricacies rendering process even latent variables considered priori independent means mean-ﬁeld assumption often insufﬁcient. secondly unstructured form variational approximation means recognition model produces latents also unstructured interpretable. attempts imbue purposes manuscript refer latent representations disentangled structured latent representations entangled unstructured. notions entangled disentangled representations relate concise well-deﬁned human interpretability axes variation. interpretation representations typically happens fact adding discriminative model learned representations. adding structure encoder model ameliorates concerns allowing richer dependency structure recognition model also inducing latent representations whose interpretability governed given graphical model. framework enables speciﬁcation wide variety graphical models embedded domain-speciﬁc language expressed directly torch framework. model particularly domains interested here models employ factorise structured latents unstructured latents speciﬁc factorisation imposed structured latent variables. typical form generative model given pθpθ typically multivariate normal distribution appropriately structured latent. unstructured latent variables means capture variation data explicitly modelled jointly learning likelihood function partially constrained structured latents crucially enforcing totally explain data. variational approximation true posterior nominally taken family prior distribution often include additional structure alternate factorisations appropriate. particular factorisation introduces dependence structured unstructured latents approximation conditioning latter former qφqφ. removes implicit mean ﬁeld assumption recognition network reﬂects fact latent variables typically exhibit conditional dependence even latent variables priori independent. models top-level factoring useful situations interpretability required useful model along certain axes variation. useful wish interpret data different viewpoints contexts like choice form labels ﬁxed. useful cannot conceivable capture variation data complexity settle particular restriction case real world visual language data. learning although impose structure recognition network graphical models necessarily certain nodes corresponding particular variables actually encode desired semantics node. example graphical model decomposes described above structured latent encodes digit identity unstructured latent captures style certainty decomposition alone sufﬁcient learn disentangled representations without supervision guarantee structured unstructured latents fulﬁl respective roles scheme. build work kingma construct semi-supervised learning scheme small amount supervision sufﬁcient break inherent symmetry problem learn appropriate representation. framework objective term involving labelled data treats data label observed variables term involving unlabelled data simply marginalises label yover support. also explicit term learn classiﬁer supervised data points. employ objective note cases often cost paid computationally. marginalisation scales poorly shortage labels support size. alternately observe discrete random variables used input neural network parametrises generative model often simply plug-in probability vector discrete distribution instead sampling similar straight-through estimator course applicable general posterior labels close dirac-delta function classifying-digits example good approximation. points difference involve richer approximations encoder decoder form convolutional neural networks introduction supervision rate enabling repeated observation labelled data point different contexts order reduce estimator variance. cnns helps avoid employing stacked model allowing single joint objective comparable performance. supervision rates motivated fact observing labelled data point context different unlabelled data points help moderate variance learning steps. figure classiﬁcation-error rates different labelled-set sizes different runs. classiﬁcation-error mnist dataset different labelled sizes supervision rates evaluate framework ability learn interpretable latents effective recognition model effective generative model. efﬁcacy recognition model evaluated label-classiﬁcation task efﬁcacy generative model evaluated visual analogies task. evaluations conducted mnist google street-view house numbers datasets using generative recognition models shown fig. mnist svhn datasets employed training-test split mnist svhn. mnist dataset standard single-hidden-layer modes encoder decoder. svhn dataset architecture convolutional encoder deconvolutional decoder blocks ﬁlters encoder reverse decoder. learning used adam learning rate momentum-correction terms default values. minibatch sizes varied depending dataset used supervised-set size. evaluate recognition model quantitatively compute classiﬁcation accuracy label-prediction task model datasets. allows measure extent latent-space representations disentangled capturing kinds representations would expect priori given graphical model. results comparison kingma reported fig. mnist dataset compare model standard experiments without performing preliminary feature-learning step. svhn dataset compare stacked model since employ effective feature learner visual data cnn. seen results perform comparably mnist dataset comfortably beat error rates svhn dataset. note recognition networks employed plug-in estimator discussed section particular feature approach ability learn disentangled representations labelled data points. combined ability re-observe particular labelled data point supervision rate framework effectively disentangle latent representations semi-supervised learning regime involving handful labelled data. figure shows error rate varies change supervision rate different labelled sizes. note steep drop error rate handful labels seen times supervision rate corresponds sampling minibatches data points total labelled data points label class equally represented labelled set. another means measuring well latent space disentangled manipulation generative model. here vary values particular variables observe figure exploring disentangled latent space generative model. visual analogies style latent variable kept ﬁxed label varied. exploration style space latent gaussian random variable keeping label ﬁxed. generative model produces outputs suitably reﬂect changes effected. datasets models considered here cast visual analogies task. figure demonstrates effect manipulating latent variables learnt generative model different ways. figure tests changes observed generative model outputs style variable held constant digit label varied. mnist svhn datasets clearly demonstrates changing digit label expected effect varying class maintaining style. latent space sufﬁciently disentangled could case. figure tests changes observed generative model outputs opposite case digit label held constant style variable varied digits mnist dataset. note evaluate capability mnist dataset particular exercise needs style variable -dimensional sufﬁcient capture variations mnist sufﬁcient capture variation complex svhn dataset. again note digits maintain identity outputs systematically reﬂecting changes style. also something would possible latents sufﬁciently disentangled. summary demonstrate utility efﬁcacy employing graphical models encoders recognition networks variational autoencoders induce interpretable latent representations semi-supervised learning. results experiments conducted framework demonstrate qualitatively quantitatively practical effectiveness framework learning interpretable disentangled latent representations. references yoshua bengio nicholas léonard aaron courville. estimating propagating gradients stochastic neurons conditional computation. arxiv preprint arxiv. eslami nicolas heess theophane weber yuval tassa koray kavukcuoglu geoffrey. hinton. attend infer repeat fast scene understanding generative models. arxiv preprint arxiv. matthew johnson david duvenaud alex wiltschko sandeep datta ryan adams. composing graphical models neural networks structured representations fast inference. advances neural information processing systems diederik kingma shakir mohamed danilo jimenez rezende welling. semisupervised learning deep generative models. advances neural information processing systems pages alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. tejas kulkarni william whitney pushmeet kohli josh tenenbaum. deep convolutional inverse graphics network. advances neural information processing systems pages danilo jimenez rezende shakir mohamed daan wierstra. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning pages john schulman nicolas heess theophane weber pieter abbeel. gradient estimation using stochastic computation graphs. advances neural information processing systems pages kihyuk sohn honglak xinchen yan. learning structured output representation using deep conditional generative models. advances neural information processing systems pages", "year": 2016}