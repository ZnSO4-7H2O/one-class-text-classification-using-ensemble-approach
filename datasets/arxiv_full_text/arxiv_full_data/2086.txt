{"title": "Interpolating Convex and Non-Convex Tensor Decompositions via the  Subspace Norm", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio $O(n^{\\lceil K/2 \\rceil /2})$ for recovering a $K$th order rank one tensor of size $n\\times \\cdots \\times n$ by recursive unfolding. In this paper, we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal $O(\\sqrt{n}+\\sqrt{H^{K-1}})$ bound, in which the parameter $H$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $H=O(1)$.", "text": "consider problem recovering low-rank tensor noisy observation. previous work shown recovery guarantee signal noise ratio ok//) recovering order rank tensor size recursive unfolding. paper ﬁrst improve bound much simpler approach careful analysis. propose norm called subspace norm based kronecker products factors obtained proposed simple estimator. imposed kronecker structure bound parameter controls allows show nearly ideal tensor natural express higher order interactions variety data tensor decomposition successfully applied wide areas ranging chemometrics signal processing neuroimaging; survey. moreover recently become active area context learning latent variable models many problems related tensors ﬁnding rank best rank-one approaximation tensor known hard nevertheless address statistical problems well recover low-rank tensor randomly corrupted version partial observations since convert tensor matrix operation known unfolding recent work shown nontrivial guarantees using norms singular value decompositions. speciﬁcally richard montanari shown rank-one order tensor size corrupted standard gaussian noise nontrivial bound shown high probability signal noise ratio nk// method called recursive unfolding. note sufﬁcient matrices also tensors best rankone approximation estimator. hand jain analyzed tensor completion problem proposed algorithm requires samples information theoretically need least samples intractable maximum likelihood estimator would require samples. therefore settings wide ideal estimator current polynomial time algorithms. subtle question address paper whether need unfold tensor resulting matrix become square possible reasoning underlying parallel development non-convex estimators based alternating minimization nonlinear optimization widely applied performed well appropriately therefore would fundamental importance connect wisdom nonconvex estimators theoretically motivated estimators recently emerged. paper explore connection deﬁning norm based kronecker products factors obtained simple mode-wise singular value decomposition unfoldings also known higher-order singular value decomposition ﬁrst study non-asymptotic behavior leading singular vector ordinary unfolding show nontrivial bound signal noise ratio nk/. thus result also applies order tensors conﬁrming conjecture furthermore motivates solution mode-wise truncated svds construct norm. propose subspace norm predicts unknown low-rank tensor mixture low-rank tensors term takes form prove required signal-to-noise ratio recovering order rank tensor ordinary unfolding analysis shows curious phase behavior high probability error shows fast decay error decays slowly conﬁrm numerical simulation. proposed subspace norm interpolation intractable estimators directly control rank tractable norm-based estimators. becomes equivalent latent trace norm cost increased signal-to-noise ratio threshold proposed estimator efﬁcient previously proposed norm based estimators size required algorithm reduced also empirically demonstrate proposed subspace norm performs nearly optimally constant order table comparison required signal-to-noise ratio different algorithms recovering order rank tensor size contaminated gaussian noise standard deviation model bound ordinary unfolding shown corollary bound subspace norm shown theorem ideal estimator proven appendix notation rn×n×···×nk order tensor. often simplify notation results paper generalizes general dimensions. inner product pair tensors deﬁned inner products vectors; i.e. vec. denotes rank-one tensor whose entry uivjwk. rank minimum number rank-one tensors required write linear combination them. mode-k ﬁber tensor dimensional vector obtained ﬁxing index mode-k unfolding matrix constructed concatenating mode-k ﬁbers along columns. denote spectral frobenius norms matrices respectively. perturbation bound left singular vector ﬁrst establish bound recovering left singular vector rank-one matrix perturbed random gaussian noise. unit vectors signal strength noise standard deviation noise matrix assumed random entries sampled i.i.d. standard normal distribution. goal lower-bound correlation left singular vector signal-to-noise ratio high probability. direct application classic wedin perturbation theorem rectangular matrix provide desired result. requires signal noise ratio since spectral norm scales would mean require i.e. threshold dominated number columns note leading eigenvector ﬁrst term adding identity matrix change eigenvectors. moreover notice noise terms ﬁrst term centered wishart matrix independent signal second term gaussian distributed depends signal implies two-phase behavior corresponding either wishart gaussian noise term dominant depending value interestingly different speed convergence phases show next theorem theorem exists constant probability least words sufﬁciently many columns rows signal noise ratio increases ﬁrst converges figure illustrates results. randomly generate rank-one matrix perturbed gaussian noise measure distance phase transition happens regimes different convergence rates theorem predicts. factors unit vectors necessarily identical entries rn×···×n i.i.d samples normal distribution note slightly general symmetric setting studied several estimators recovering noisy version proposed overlapped nuclear norm latent nuclear norm discussed achives relative performance guarantee estimator. bound implies want obtain relative error smaller need signal noise ratio scale proposed square norm deﬁned nuclear norm matrix obtained grouping ﬁrst indices along rows last indices along columns. nk//β) translates requiring norm improves right hand side inequality opk//ε obtaining relative error intuition square richard montanari studied model proved recursive unfolding algorithm achieves factor recovery error dist nk//ε high probability dist) minu also showed randomly initialized tensor power method achieve error slightly worse threshold instance information plus noise model nk−. thus ordinary unfolding satisﬁes condition theorem large enough. corollary consider order rank tensor contaminated gaussian noise exists constant probability least ke−n proves conjectured threshold applies even order case also order case. note hopkins shown similar result without sharp rate convergence. corollary easily extends general tensor replacing conditions nk)/ demonstrate result figure models behind experiment slightly general ones signal rank measure plot shows inner products quality estimating mode- factors. horizontal axis normalized noise nk)/. clearly inner product decays symmetrically corollary shows left singular vectors recovered mild conditions; thus span right singular vectors also recovered. inspired this deﬁne norm models tensor mixture tensors require modek unfolding i.e. rn×h variable rnk−×h ﬁxed arbitrary orthonormal basis subspace choose later kronecker structure next lemma show dual norm subspace norm nk−) scaling restricting inﬂuence noise term subspace deﬁned lemma dual norm |||·|||s semi-norm |x|||s∗ restriction makes sure incoherent i.e. spectral norm random matrix unfolded different mode similar assumptions used low-rank plus sparse matrix decomposition denoising bound latent nuclear norm note right-hand side bound consists terms. ﬁrst term approximation error. term zero lies span}k k=). case choose latent nuclear norm condition corollary satisﬁed smallest kronecker product construction proposed. note regularization constant also scale dual subspace norm residual second term estimation error respect take orthogonal projection span}k ignore contribution residual estimation error scales mildly dimensions ranks. note take recover guarantee synthetic data randomly generated true rank tensor size singular values true factors generated random matrices orthonormal columns. observation tensor generated adding gaussian noise standard deviation approach compared decomposition overlapped approach latent approach. decomposition computed tensorlab random initializations. assume knows true rank subspace norm algorithm described computed solutions values regularization parameter logarithmically spaced overlapped latent norm admm described also computed solutions used subspace norm. minimum error obtained choosing optimal regularization parameter optimal initialization. although regularization parameter could selected leaving entries measuring error entries tensor completion sake simplicity. figure show result experiment. left panel shows relative error representative values subspace norm. black dash-dotted line shows minimum error across λ’s. magenta dashed line shows error corresponding theoretically motivated choice conﬁrms rather sharp increase error around theoretically predicted places also optimal figure tensor denoising. subspace approach three representative synthetic data. comparison different methods synthetic data. comparison amino acids data. grow linearly large best relative error since optimal figure compares performance subspace norm approaches. method smallest error corresponding optimal choice regularization parameter shown. addition place numbers context plot line corresponding clearly error subspace norm optimistic grows rate much slower overlap latent. error increases beyond regularization imposed subspace norm behaving near optimally setting although behavior guaranteed subspace norm whereas hard give guarantee decomposition based nonlinear optimization. amino acid dataset semi-realistic dataset commonly used benchmark rank tensor modeling. consists laboratory-made samples contains different amounts tyrosine tryptophan phenylalanine. spectrum excitation wavelength emission measured ﬂuorescence gives tensor. true factors known three acids data perfectly suits model. true rank proposed approach computed solutions different random initializations solutions approaches different values subspace overlapped approach logarithmically spaced figure shows smallest relative error achieved methods compare. similar synthetic data subspace norm behaves near ideally though relative error larger lack regularization. interestingly theoretically suggested scaling regularization parameter almost optimal. settled conjecture posed showed indeed signal-to-noise ratio sufﬁcient also order tensors. moreover analysis shows interesting two-phase behavior error. ﬁnding lead development proposed subspace norm. estimated mode-wise singular value decompositions. analyzed denoising performance proposed norm shown error bounded terms interpreted approximation error term coming ﬁrst step estimation error term coming second step.", "year": 2015}