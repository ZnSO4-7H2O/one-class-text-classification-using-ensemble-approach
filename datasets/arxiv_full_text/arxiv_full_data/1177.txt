{"title": "PCANet: A Simple Deep Learning Baseline for Image Classification?", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.", "text": "abstract—in work propose simple deep learning network image classiﬁcation comprises basic data processing components cascaded principal component analysis binary hashing block-wise histograms. proposed architecture employed learn multistage ﬁlter banks. followed simple binary hashing block histograms indexing pooling. architecture thus named network designed learned extremely easily efﬁciently. comparison better understanding also introduce study simple variations pcanet namely randnet ldanet. share topology pcanet cascaded ﬁlters either selected randomly learned lda. tested basic networks extensively many benchmark visual datasets different tasks face veriﬁcation multipie extended yale feret datasets face recognition well mnist hand-written digits recognition. surprisingly tasks seemingly naive pcanet model state features either preﬁxed highly hand-crafted carefully learned even surprisingly sets records many classiﬁcation tasks extended yale feret datasets mnist variations. additional experiments public datasets also demonstrate potential pcanet serving simple highly competitive baseline texture classiﬁcation object recognition. index terms—convolution neural network deep learning network random network network face recognition handwritten digit recognition object classiﬁcation. introduction image classiﬁcation based visual content challenging task largely usually large amount intra-class variability arising different lightings misalignment non-rigid deformations occlusion corruptions. numerous efforts made counter intra-class variability manually designing low-level features classiﬁcation tasks hand. representative examples gabor features local binary patterns texture face classiﬁcation sift features object recognition. low-level features hand-crafted great success speciﬁc data tasks designing effective features data tasks usually requires domain knowledge since hand-crafted features cannot simply adopted conditions learning features data interest considered plausible remedy limitation hand-crafted features. example methods learning deep neural networks draws signiﬁcant attention recently idea deep learning discover multiple levels representation hope higher-level features represent abstract semantics data. abstract representations learned deep network expected provide invariance intra-class variability. tsung-han chan shenghua jiwen zinan zeng school information science technology shanghaitech university department university illinois urbana-champaign ingredient success deep learning image classiﬁcation convolutional architectures convolutional deep neural network architecture consists multiple trainable stages stacked other followed supervised classiﬁer. stage generally comprises three layers convolutional ﬁlter bank layer nonlinear processing layer feature pooling layer. learn ﬁlter bank stage convnet variety techniques proposed restricted boltzmann machines regularized autoencoders variations; review references therein. general network typically learned stochastic gradient descent method. however learning network useful classiﬁcation critically depends expertise parameter tuning tricks. many variations deep convolutional networks proposed different vision tasks success usually justiﬁed empirically arguably ﬁrst instance clear mathematical justiﬁcation wavelet scattering networks difference convolutional ﬁlters scatnet preﬁxed simply wavelet operators hence learning needed all. somewhat surprisingly pre-ﬁxed ﬁlter bank utilized similar multistage architecture convnet dnns demonstrated superior performance convnet dnns several challenging vision tasks handwritten digit texture recognition however paper preﬁxed architecture generalize well tasks network closely related pcanet could twostage oriented ﬁrst proposed audio processing noticeable differences pcanet opca couple hashing local histogram output layer. given covariance noises opca gains additional robustness noises distortions. baseline pcanet could also incorporate merit opca likely offering invariance intra-class variability. also explored supervised extension pcanet replace ﬁlters ﬁlters learned linear discriminant analysis called ldanet. extensive experiments additional discriminative information seem improve performance network; sections another somewhat extreme variation pcanet replace ﬁlters totally random ﬁlters called randnet. work conducted extensive experiments fair comparisons types networks existing networks convnet scatnet. hope experiments observations help people gain better understanding different networks. contributions although initial intention studying simple pcanet architecture simple baseline comparing justifying advanced deep learning components architectures ﬁndings lead pleasant thought-provoking surprises basic pcanet fair experimental comparison already quite with often better than state-of-the-art features almost image classiﬁcation tasks including face images hand-written digits texture images object images. speciﬁcally face recognition gallery image person achieves accuracy extended yale dataset accuracy across disguise/illumination subsets dataset. feret dataset obtains state-of-the-art average accuracy achieves best accuracy dup- dup- subsets respectively. dataset achieves competitive face veriﬁcation accuracy unsupervised setting. mnist datasets achieves state-of-the-art results subtasks basic background random background image. section details. overwhelming empirical evidences demonstrate effectiveness proposed pcanet learning robust invariant features various image classiﬁcation tasks. motivations initial motivation study trying resolve apparent discrepancies convnet scatnet. want achieve simple goals first want design simple deep learning network easy even trivial train adapt different data tasks. second basic network could serve good baseline people empirically justify advanced processing components sophisticated architectures deep learning networks. solution comes surprise basic easy operations emulate processing layers typical neural network mentioned above data-adapting convolution ﬁlter bank stage chosen basic ﬁlters; nonlinear layer simplest binary quantization feature pooling layer simply block-wise histograms binary codes considered ﬁnal output features network. ease reference name data-processing network network example figure illustrates two-stage pcanet extracts features input image. least characteristic pcanet model seem challenge common wisdoms building deep learning network convnet scatnet nonlinear operations early stages pcanet last output layer binary hashing histograms conducted compute output features. nevertheless eigenvectors capture main variation meanremoved training patches. course similar scatnet stack multiple stages ﬁlters extract higher level features. denotes convolution boundary make size like ﬁrst stage collect overlapping patches subtract patch mean patch form rkk×mn ¯yilj mean-removed patch deﬁne rkk×n matrix coll rkk×ln output stage hashing histogram input images second stage real-valued outputs second stage. binarize outputs heaviside step conﬁrms certain remarkable beneﬁts cascaded feature learning extraction architectures. even importantly since pcanet consists linear followed binary hashing block histograms amenable mathematical analysis justiﬁcation effectiveness. could lead fundamental theoretical insights general deep networks seems urgent need deep learning nowadays. cascaded linear networks structures network suppose given input training images size assume patch size {ii}n stages. proposed pcanet model illustrated figure ﬁlters need learned input images {ii}n follows describe component block diagram precisely. ﬁrst stage around pixel take patch collect patches image; i.e. ximn denotes vectorized patch subtract patch mean patch obtain ¯xij mean-removed patch. constructing matrix input images putting together pcanet contains non-linearity process between/in stages running contrary common wisdom building deep learning networks; e.g. absolute rectiﬁcation layer convnet modulus layer scatnet tested pcanet absolute rectiﬁcation layer added right ﬁrst stage observe improvement ﬁnal classiﬁcation results. reason could quantization plus local histogram already introduces sufﬁcient invariance robustness ﬁnal feature. layer pcanet completely linear. wonder merge stages equivalently number ﬁlters size receptive ﬁeld. speciﬁc interested single-stage pcanet ﬁlters size could perform comparison two-stage pcanet described section experimented settings faces handwritten digits observed two-stage pcanet outperforms single-stage alternative cases; last rows tables comparison ﬁlters learned single-stage alternative resulting two-stage ﬁlters essentially lowrank factorization possibly lower chance overﬁtting dataset. need deep structure computational perspective single-stage alternative requires learning ﬁlters variables whereas two-stage pcanet learns variables. another beneﬁt ﬁlters totally two-stage pcanet larger receptive ﬁeld contains holistic observations objects images learning invariance essentially capture semantic information. comparative experiments validates hierarchical architectures large receptive ﬁelds multiple stacked stages efﬁcient learning semantically related representations coincides observed local blocks either overlapping nonoverlapping depending applications. empirical experience suggests non-overlapping blocks suitable face images whereas overlapping blocks appropriate hand-written digits textures object images. furthermore histogram offers degree translation invariance extracted features hand-crafted features histogram oriented gradients learned features model average maximum pooling process convnet model parameters pcanet include ﬁlter size number ﬁlters stage number stages block size local histograms output layer. ﬁlter banks require experiments section always inspired common setting gabor ﬁlters orientations although ﬁne-tuned could lead marginal performance improvement. moreover noticed empirically two-stage pcanet general sufﬁcient achieve good performance deeper architecture necessarily lead improvement. also larger block size local histograms provides translation invariance extracted feature comparison convnet scatnet clearly pcanet shares similarities convnet patch-mean removal pcanet reminiscent local contrast normalization convnet. operation moves patches centered around origin vector space learned ﬁlters better catch major variations data. addition viewed simplest class auto-encoders minimizes reconstruction error. training images classiﬁed classes {ii}i∈sc indices images class mean-removed patches associated image distinct classes rkk×mn given. ﬁrst compute class mean intra-class variability patches follows trace operator. solution known σc)†φ superscript denotes pseudo-inverse. full rank though might another handling better numeric stability matkk ﬁlters thus expressed rk×k deeper network built repeating process above. evaluate performance proposed pcanet simple variations various tasks including face recognition face veriﬁcation hand-written digits recognition texture discrimination object recognition section. ﬁrst focus problem face recognition gallery image person. part multipie dataset learn ﬁlters pcanet apply trained pcanet extract features subjects multipie dataset extended yale feret datasets face recognition. computational complexity components constructing pcanet extremely basic computationally efﬁcient. light computational complexity pcanet would take two-stage pcanet example. stage pcanet forming patch-mean-removed matrix costs kkmn ﬂops; inner product complexity ﬂops; complexity eigen-decomposition ﬁlter convolution takes likkmn ﬂops stage output layer conversion binary bits decimal number costs naive histogram operation complexity assuming overall complexity pcanet easy veriﬁed computational complexity applies training phase testing phase pcanet extra computational burden training phase testing phase eigen-decomposition whose complexity ignorable max. comparison convnet ﬁlter learning also simple gradient-based optimization solver overall training time still much longer pcanet. example training pcanet around images pixel dimension took half hour cnn- took hours excluding ﬁne-tuning process; section ...d details. variations randnet ldanet pcanet extremely simple network requiring minimum learning ﬁlters training data. could immediately think possible variations pcanet towards opposite directions could eliminate necessity training data replace ﬁlters layer random ﬁlters size. speciﬁc random ﬁlters i.e. elements generated following standard gaussian distribution. call network random network randnet shorthand. natural wonder much degradation randomly chosen network would perform comparison pcanet. task learned network classiﬁcation could enhance supervision learned ﬁlters incorporating information class labels training data learn ﬁlters based idea multi-class linear discriminant analysis called composed network network ldanet ease reference. interested much enhanced supervision would help improve performance network. training testing multipie dataset. generic faces training set. multipie dataset contains subjects across simultaneous variation pose expression illumination. subjects select images subjects enrolled four sessions. images subject illuminations expressions pose step size total poses collected. manually select corners ground truth registration down-sample images pixels. distance outer corners normalized pixels. generic faces training comprises around images images converted gray scale. assembled face images train pcanet together data labels learn ldanet apply trained networks extract features subjects multi-pie dataset. mentioned above subjects enrolling four sessions used pcanet training. remaining subjects session used gallery training testing. frontal view subject neutral expression frontal illumination used gallery rest testing. classify possible variations test sets namely cross illumination cross expression cross pose cross expression-plus-pose cross illumination-plus-expression cross illuminationplus-pose cross illumination-plus-expression-andpose. cross-pose test speciﬁcally collected poses impact number ﬁlters. comparing randnet pcanet ldanet existing methods test sets ﬁrst investigate impact number ﬁlters networks crossillumination test only. ﬁlter size networks non-overlapping blocks size vary number ﬁlters ﬁrst stage one-stage networks. considering two-stage networks vary results shown figure pcanet- achieves best results pcanet- best test. moreover accuracy pcanet ldanet increases larger randnet also similar performance trend. however performance ﬂuctuation observed randnet ﬁlters’ randomness. impact block size. next examine impact block size robustness pcanet image deformations. cross-illumination test introduce artiﬁcial deformation testing image translation inplane rotation scaling; figure parameters pcanet block sizes considered. figure shows recognition accuracy artiﬁcial deformation. observed pcanet- achieves percent accuracy translation pixels directions in-plane rotation scale varying moreover results suggest pcanet- larger block size provides robustness various deformations larger block side sacriﬁce performance pcanet. impact number generic faces training samples. also report recognition accuracy pcanet differen number generic faces training images. again cross-illumination test set. randomly select images generic training train pcanet varies parameters pcanet block size results tabulated table surprisingly accuracy pcanet less-sensitive number generic training images. performance pcanet- gradually improves number generic training samples increases pcanet- keeps perfect recognition even generic training samples. fig. recognition rate pcanet multipie cross-illumination test different pcanet block size deformation test image. block sizes histogram aggregation tested. simultaneous translation directions. translation direction. translation direction. in-plane rotation. scale variation. randnet pcanet ldanet gabor two-stage scatnet parameters pcanet ﬁlter size number ﬁlters block size learned pcanet ﬁlters shown figure number scales number orientations scatnet- respectively. nearest neighbor classiﬁer chi-square distance randnet pcanet ldanet cosine distance gabor scatnet. classiﬁer different distance measure secure best performances respective features. also compare cnn. since could work successfully applies face recognition tasks caffe framework pretrain two-stage generic faces training set. cnn- fully-supervised network ﬁlter size channels ﬁrst stage face convolved family gabor kernels scales orientations. ﬁlter response down-sampled uniform lattice normalized zero mean unit variance. face divided several blocks size pcanet. histogram uniform binary patterns computed patterns generated thresholding neighboring pixels circle radius using central pixel value. channels second stage. convolution output followed rectiﬁed linear function relu max-pooling. output layer softmax classiﬁer. pre-training cnn- generic faces training cnn- also ﬁne-tuned gallery images epochs. performance methods given table except cross-pose test pcanet yields best precision. test sets performance randnet ldanet inferior pcanet ldanet seem take advantage discriminative information. also whenever illumination variation performance drops signiﬁcantly. pcanet overcomes drawback offers comparable performance cross-pose cross-expression variations. ﬁnal note scatnet seem performing well. case face-related experiments below therefore scatnet included comparison experiments. also include randnet ldanet following face-related experiments show performance superior pcanet. last table shows result pcanet ﬁlters size pcanet- parameter setting mimic reported pcanet- single-stage network number ﬁlters size receptive ﬁeld. pcanet- outperforms pcanet- alternative showing advantages deeper networks. another issue worth mentioning efﬁciency pcanet. training pcanet- generic faces apply pcanet ﬁlters learned multipie extended yale dataset extended yale dataset consists frontal-face images individuals. cropped normalized face images captured various laboratorycontrolled lighting conditions. subject select frontal illumination gallery images rest testing. challenge ourselves test images also simulate various levels contiguous occlusion percent percent replacing randomly located square block test image unrelated image; figure example. size nonoverlapping blocks pcanet compare test images processed illumination normalization p-lbp classiﬁer chi-square distance measure. experimental results given table pcanet outperforms p-lbp different levels occlusion. also observed pcanet illumination-insensitive also robust block occlusion. single sample person setting various difﬁcult lighting conditions pcanet surprisingly achieves almost perfect recognition still sustains accuracy pixels every test image occluded reason could ﬁlter seen detector maximum response patches face. words contribution occluded patches would somehow ignored ﬁltering passed onto output layer pcanet thereby yielding striking robustness occlusion. testing dataset. evaluate ability multipie-learned pcanet cope real possibly malicious occlusions using dataset dataset consists frontal images subjects. images include different facial expressions illumination conditions disguises. experiment chose subset data consisting male subjects female subjects. images cropped dimension converted gray scale. subject select face frontal illumination neural expression gallery training rest testing. size non-overlapping blocks pcanet also compare p-lbp classiﬁer chi-square distance measure. results given table test illumination variations recognition pcanet almost perfect cross-disguise related test sets accuracy results consistent multipie extended yale datasets pcanet insensitive illumination robust occlusions. best knowledge single feature simple classiﬁer achieve performances even extended representation-based classiﬁcation testing feret dataset. ﬁnally apply multipie-learned pcanet popular feret dataset standard dataset used facial recognition system evaluation. feret contains images different individuals images individual captured different lighting conditions non-neural expressions period three years. complete dataset partitioned disjoint sets gallery probe. probe subdivided four categories different expression changes; different lighting conditions; dup-i taken within period three four months; dup-ii taken least half year apart. gray-scale images cropped image size pixels. size non-overlapping blocks pcanet compare fairly prior methods dimension pcanet features reduced whitening projection matrix learned features gallery samples. classiﬁer cosine distance used. moreover addition pcanet trained multipie database also train pcanet feret generic training consisting images people listed feret standard training results pcanet state-of-theart methods listed table surprisingly simple multipie-learned pcanet- feret-learned pcanet- achieve state-of-the-art accuracies average respectively. variations multipie database much richer standard feret training nature multipie-learned pcanet slightly outperforms feret-learned pcanet. importantly pcanet- breaks records dup-i dup-ii. conclusive remarks face recognition. prominent message drawn experiments sections training pcanet face dataset effective capture abstract representation subjects datasets. pcanet trained extracting pcanet- feature test face takes second matlab. anticipate performance pcanet could improved moved toward practical pcanet trained upon wide deep dataset collect sufﬁciently many inter-class intra-class variations. face veriﬁcation dataset besides tests laboratory face datasets also evaluate pcanet dataset unconstrained face veriﬁcation. contains face images different individuals collected large variations pose expression illumination clothing hairstyles etc. consider unsupervised setting best choice evaluating learned features depend metric learning discriminative model learning. aligned version faces namely lfw-a provided wolf used face images cropped pixel dimensions. follow standard evaluation protocal splits view dataset subsets subset containing intra-class pairs inter-class pairs. perform -fold cross validation using subsets pairs view pcanet ﬁlter size number ﬁlters block size respectively. performances measured averaging -fold cross validation. project pcanet features onto dimensions using wpca pcanet- pcanet- respectively classiﬁer cosine distance. table tabulates results. note pcanet followed sqrt parentheses represents pcanet feature taking square-root operation. square-root pcanet outperforms pcanet performance boost square-root operation also observed features dataset moreover square-root pcanet- achieves accuracy quite competitive current state-of-the-art methods. shows proposed pcanet also effective learning invariant features face images captured less controlled conditions. differs largely. works require outside database train convnet face images precisely aligned; e.g. uses -dimensional model face alignment extracts multi-scale features based detected landmark positions. contrary trained pcanet based lfw-a aligned version images using commercial alignment system face.com. move forward test proposed pcanet along randnet ldanet mnist mnist variations widely-used benchmark testing hierarchical representations. classiﬁcation tasks total listed table images size following mnist basic dataset investigate inﬂuence number ﬁlters different block overlap ratios randnet pcanet ldanet compare state-of-the-art methods mnist datasets. impact number ﬁlters vary number ﬁlters ﬁrst stage one-stage networks. regarding two-stage networks change ﬁlter size networks block size overlapping region blocks half block size. results shown figure results consistent multipie face database figure pcanet outperforms randnet ldanet almost cases. impact block overlap ratio number ﬁlters ﬁxed ﬁlter size block size vary block overlap ratio table tabulates results randnet- pcanet ldanet-. clearly pcanet- ldanet- achieve minimum error rates equal respectively pcanet- performs best conditions. compare randnet pcanet ldanet convnet -stage scatnet existing methods. scatnet number scales number orientations respectively. regarding parameters pcanet ﬁlter size number ﬁlters block size tuned cross-validation mnist validation sets mnist variations. overlapping region blocks half block size. unless otherwise speciﬁed linear classiﬁer scatnet randnet pcanet ldanet classiﬁcation tasks. testing error rates various methods mnist shown table fair comparison include results methods using augmented training samples distortions information best known result randnet- pcanet- ldanet- comparable state-of-the-art methods standard mnist task. however mnist many training data methods perform well close difference statistically meaningful. accordingly also report results different methods mnist variations table best knowledge pcanet- achieves state-of-the-art results four eight remaining tasks basic bg-img bg-img-rot convex. especially bg-img error rate reduces table also shows result pcanet- ﬁlters size pcanet- parameter setting mimic reported pcanet- single-stage structure. pcanet- still outperforms pcanet- alternative. furthermore also draw learned pcanet ﬁlters figure figure intriguing pattern observed ﬁlters rect rect-img datasets. rect horizontal vertical stripes patterns attempt capture edges rectangles. image background rect-img several ﬁlters become low-pass order secure responses background images. texture classiﬁcation curet dataset curet texture dataset contains classes image textures. texture class images material different pose illumination conditions. variations specularities shadowing surface normal variations also make classiﬁcation challenging. experiment subset dataset azimuthal viewing angle less methods k-nn-scm k-nn-idm cdbn convnet stochastic pooling convnet conv. maxout dropout scatnet- randnet- randnet- pcanet- pcanet- ldanet- ldanet- pcanet- degrees selected thereby yielding images class. central region cropped selected images. dataset randomly split training testing training images class pcanet trained ﬁlter size number ﬁlters block size linear classiﬁer. testing error rates averaged different random splits shown table pcanet outperforms scatnet- improvement pcanet- pcanet- large scatnet. motivation explore limitation simple pcanet relatively complex database comparison databases faces digits textures experimented with could someroughly aligned prepared. begin with extend ﬁlter learning accommodate images object databases. spirit constructing data matrix gather individual matrix channels images denoted rkk×n respectively. following steps section multichannel ﬁlters easily veriﬁed matkk function maps tensor rk×k×. example learned multichannel ﬁlters demonstrated figure addition modiﬁcation above also connect spatial pyramid pooling output layer pcanet extracting information invariant large poses complex backgrounds usually seen object databases. essentially helps object recognition ﬁnds signiﬁcant improvement previous experiments faces digits textures. object recognition cifar ﬁnally evaluate performance pcanet cifar database object recognition. cifar natural images pixels. contains classes training samples test samples. images cifar vary signiﬁcantly pcanet comprises cascaded linear followed nonlinear output stage. simplicity offers alternative refreshing perspective convolutional deep learning networks could facilitate mathematical analysis justiﬁcation effectiveness. couple simple extensions pcanet; randnet ldanet introduced tested together pcanet many image classiﬁcation tasks including face hand-written digit texture object. extensive experimental results consistently shown pcanet outperforms randnet ldanet generally scatnet variations convnet. furthermore performance pcanet closely comparable often better highly engineered hand-crafted features tasks face recognition pcanet also demonstrates remarkable robustness corruption ability transfer datasets. experiments also convey long images databases somehow well prepared; i.e. images roughly aligned exhibit diverse scales poses pcanet able eliminate image variability gives reasonably competitive accuracy. challenging image databases pascal imagenet pcanet might sufﬁcient handle variability given extremely simple structure unsupervised learning method. intriguing research direction construct complicated deeper pcanet could accommodate aforementioned issues. preprocessing pose alignment scale normalization might needed good performance guarantee. current bottleneck keeps pcanet growing deeper dimension resulted feature would increase exponentially number stages. fortunately seems able ﬁxed replacing -dimensional convolution ﬁlters tensor-like ﬁlters future study. furthermore also leave future work augment pcanet simple scalable baseline classiﬁer readily applicable much overlapping region blocks half block size connected output layer pcanet; i.e. maximum response block histograms pooled pyramid subregions. yields pooled histogram feature dimension dimension pooled feature reduced pca. second experiment concatenate pcanet features learned different ﬁlter size processes model parameters ﬁxed identical single descriptor mentioned last paragraph except ﬁlter size equal respectively. ensure combined features dimension single descriptor fairness. results shown table pcanet- achieves accuracy gains improvement combining features learned different ﬁlter sizes pcanet- around accuracy degradation comparison state-of-the-art method performance fully unsupervised extremely simple pcanet- shown still encouraging. conclusion paper proposed arguably simplest unsupervised convolutional deep learning network— pcanet. network processes input images cascaded binary hashing block histograms. like convnet models network parameters number layers ﬁlter size number ﬁlters given pcanet. parameters ﬁxed training pcanet extremely simple efﬁcient ﬁlter learning pcanet involve regularized parameters require numerical optimization solver. moreover building regardless extensive experiments given paper sufﬁciently conclude facts pcanet simple deep learning network effectively extracting useful information classiﬁcation faces digits texture images; pcanet valuable baseline studying advanced deep learning architectures large-scale image classiﬁcation tasks. bengio courville vincent representation learning review perspectives ieee tpami vol. goodfellow warde-farley mirza courville bengio maxout networks icml jarrett kavukcuoglu ranzato lecun what best multi-stage architecture object recognition iccv bruna mallat invariant scattering convolution networks ieee tpami vol. huang ramesh berg learned-miller labeled faces wild database studying face recognition unconstrained environments technical report university massachusetts amherst tsung-han chan received b.s. degree department electrical engineering yuan university taiwan ph.d. degree institute communications engineering national tsing university taiwan currently working project lead engineer sunplus technology hsinchu taiwan. research interests image processing convex optimization recent emphasis computer vision hyperspectral remote sensing. received b.eng. degree marine engineering northwestern polytechnical university china m.eng. degree electrical computer engineering national university singapore ph.d. degree computer science queen mary university london london u.k. currently research scientist advanced digital sciences center. research interests computer vision machine learning image processing. shenghua received b.e. degree university science technology china received ph.d. degree nanyang technological university currently postdoctoral fellow advanced digital sciences center singapore. awarded microsoft research fellowship research interests include computer vision machine learning. jiwen currently research scientist advanced digital sciences center singapore. research interests include computer vision pattern recognition machine learning biometrics. authored/co-authored scientiﬁc papers peer-reviewed journals conferences including venues tpami cvpr iccv. member ieee. zinan zeng received master degree b.e. degree first honour school computer engineering nanyang technological university singapore. senior software engineer advanced digital sciences center singapore. research interests include statistical learning optimization application computer vision. professor school information science technology shanghaitech university. received bachelors’ degree automation applied mathematics tsinghua university china received m.s. degree eecs m.a. degree mathematics degree eecs berkeley. associate professor department university illinois urbana-champaign holds adjunct position. early principal researcher manager visual computing group microsoft research asia. main research areas computer vision high-dimensional data analysis. recipient david marr best paper prize iccv honorable mention longuet-higgins best paper award eccv received career award national science foundation young investigator program award ofﬁce naval research associate editor ijcv siims ieee trans. pami information theory.", "year": 2014}