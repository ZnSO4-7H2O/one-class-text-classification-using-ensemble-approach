{"title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "tag": ["cs.LG", "cs.CV", "cs.NA", "stat.ML"], "abstract": "Clustering analysis by nonnegative low-rank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a two-step bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.", "text": "clustering analysis nonnegative lowrank approximations achieved remarkable progress past decade. however approximation approaches direction still restricted matrix factorization. propose low-rank learning method improve clustering performance beyond matrix factorization. approximation based twostep bipartite random walk virtual cluster nodes approximation formed cluster assigning probabilities. minimizing approximation error measured kullback-leibler divergence equivalent maximizing likelihood discriminative model endows method solid probabilistic interpretation. optimization implemented relaxed majorization-minimization algorithm advantageous ﬁnding good local minima. furthermore point regularized algorithm dirichlet prior serves initialization. experimental results show method strong performance clustering purity various datasets especially large-scale manifold data. cluster analysis assigns objects groups objects cluster similar clusters. optimization clustering objectives np-hard relaxation soft clustering often required. nonpopular nonnegative low-rank approximation method nonnegative matrix factorization ﬁnds matrix approximates similarities factorized several nonnegative low-rank matrices. originally applied vectorial data ding shown equivalent classical k-means method. later applied graph given pairwise similarities. example ding presented nonnegative spectral cuts using multiplicative algorithm; arora proposed left stochastic decomposition approximates similarity matrix based euclidean distance left-stochastic matrix. another stream direction topic modeling. hofmann gave generative model probabilistic latent semantic indexing counting data essentially equivalent using kullback-leibler divergence tri-factorizations. bayesian treatment plsi using dirichlet prior later introduced blei symmetric plsi bayesian treatment called interaction component model despite remarkable progress relaxation approaches still fully satisfactory following requirements aﬀect clustering performance using nonnegative low-rank approximation approximation error measure takes account sparse similarities decomposition form approximating matrix decomposing matrices contain enough parameters clustering more normalization approximating matrix ensures relatively balanced clusters equal contribution data sample. lacking dimensions severely aﬀect clustering performance. work inspired anchorgraph used large approximative graph construction based two-step random walk data nodes anchor nodes. note anchorgraph clustering method. augment input similarity graph cluster nodes cluster assigning probabilities seen single-step random walk probabilities data nodes augmented cluster nodes. without preference particular samples impose uniform prior data nodes. prior reversed random walk probabilities calculated bayes formula anchorgraph provide error measure approximation. conventional choice squared euclidean distance employs underlying assumption noise additive gaussian. real-world clustering tasks multivariate datasets data points often curved manifold. consequently similarities based euclidean distances reliable small neighborhood. locality causes high sparsity input similarity matrix. sparsity also commonly exists real-world network data. sparsity euclidean distance improper approximation additive gaussian noise lead dense paper present nonnegative low-rank approximation method clustering satisﬁes three requirements. first datasets often curved manifolds similarities small neighborhood reliable adopt kl-divergence handle resulting sparsity. second diﬀerent plsi enforce equal contribution every data sample directly construct decomposition probabilities samples clusters. third probabilities form decomposing matrix learned approach directly gives answer probabilistic clustering. furthermore decomposition method leads doubly-stochastic approximating matrix shown desired balanced graph cuts name method based data-cluster-data random walks. order solve learning objective propose novel relaxed majorization-minimization algorithm handle matrix decomposition type. relaxation strategy works robustly ﬁnding sastisfactory local optimizers stochasticity constraint. furthermore argue complexity control bayesian priors provides initialization algorithm. eliminates problem hyperparameter selection prior. empirical comparison graphbased clustering approaches demonstrates method achieve best nearly best clustering purity tasks. datasets method signiﬁcantly improves state-of-the-art. introductory part present method section including learning objective probabilistic model optimization initialization techniques. section point connections diﬀerences method recent related work. experimental settings results given section finally conclude paper discuss future work section suppose similarities data samples precomputed given nonnegative symmetric matrix matrix seen aﬃnity undirected similarity graph node corresponds data sample clustering analysis algorithm takes input divides data nodes disjoint subsets. probabilistic although possible construct multi-level graphical model similar dirichlet process topic model emphasize smallest approximation error ﬁnal goal. dirichlet prior used order ease optimization. therefore employ complex generative models; section discussion. ways handle constraint first develop multiplicative algorithm procedure proposed yang neglecting stochasticity constraint normalize rows update. however optimization easily gets stuck poor local minima practice. observed graph. contrast kullbackleibler divergence suitable approximation. underlying poisson noise characterizes rare occurrences present sparse input. formulate learning objective following optimization problem model simply uses uniform prior rows prevent using informative priors complexity control. natural choice probabilities dirichlet distribution algorithm jointly minimizes approximation error drives rows towards probability simplex. lagrangian multipliers automatically selected algorithm without extra human tuning labor. quantities sums unconstrained multiplicative learning result quantities balance gradient learning force probability simplex attraction. besides convenience relaxation strategy works robustly brute-force normalization iteration. optimization problems many clustering analysis methods including ours non-convex. usually ﬁnding global optimum expensive even np-hard. local optimizers used optimization trajectory easily stuck poor local optima algorithm starts arbitrary random guess. proper initialization thus needed achieve satisfactory performance. cost initialization much cheaper main algorithm. popular choices k-means normalized ﬁrst applied vectorial data could slow large-scale high-dimensional data. employ second initialization method. original ncut np-hard relaxed ncut known performance plsi improved using bayesian non-parametric modeling. bayesian treatment symmetric version plsi leads interaction component model associates dirichlet priors plsi factorizing matrices makes conjugacy dirichlet multinomial derive collapsed gibbs sampling variational optimization methods. open problem bayesian methods determine hyperparameters control priors. asuncion found wrongly chosen parameters lead mediocre even poor performance. automatic hyperparameters updating method proposed minka necessarily lead good solution terms perplexity clustering purity experiments hofmann asuncion suggested select hyperparameters using smallest approximation error heldmatrix entries however costly might weaken even break cluster structure. contrast prior hyperparameter selection problem method. algorithms using various priors play role initialization. among runs diﬀerent starting points simply select smallest approximation error. nonnegative matrix factorization earliest methods relaxing clustering problems nonnegative low-rank approximation research also opened door multiplicative majorization-minimization algorithms optimization nonnegative matrices. original input nonnegative matrix approximated product low-rank matrices later researchers found constraints normalizations imposed factorizing matrices achieve desired performance. orthogonality popular choice highly sparse factorizing matrices especially cluster indicator matrix. however orthogonality constraint seems exclusive constraints priors. practice orthogonality favors euclidean distance approximation error measure simple update rules requirement besides ncut emphasize minimal approximation error sole learning objective regularized versions e.g. diﬀerent dirichlet priors serve initialization. clustering analysis unlike supervised learning problems need provide inference unseen data. complexity control bayesian priors meant better generalization performance better-shaped space facilitate optimization. sense results diverse regularized versions even clustering algorithms starting guesses pick smallest approximation error among multiple runs. topic model type statistical model discovering abstract topics occur collection documents. early topic model plsi maximizes following log-likelihood major diﬀerence decomposition form approximating matrix. ways model hierarchy latent variables observed ones. topic model uses pure generative method employs discriminative way. plsi gives clustering results indirectly. stochasticity seems natural relaxing hard clustering probabilities. recently arora proposed symmetric using leftstochastic factorizing matrices called lsd. method also directly learns cluster assigning probabilities. however also restricted euclidean distance. method major diﬀerences lsd. first kullback-leibler divergence suitable sparse graph input curved manifold data. also enables make dirichlet multinomial conjugacy pair. second decomposition good interpretation terms random walk. furthermore imbalanced clustering implicitly penalized denominator uses matrix decomposition anchorgraph. however several major diﬀerences methods. first anchorgraph made clustering constructing graph input. anchorgraph learning objective captures global structure data clusters. decomposing matrix anchorgraph learned individually encodes local information. learning decomposing matrix whole. furthermore anchors either selected data samples pre-learned e.g. kmeans. contrast cluster nodes formulation virtual. vectors need physical storage. compared method eight clustering algorithms take symmetric nonnegative sparse matrix input. compared algorithms range classical state-of-the-art methods various principles graph cuts including normalized nonnegative spectral -spectral ratio cheeger nonnegative matrix factorization including projective symmetric -factor orthogonal leftstochastic decomposition topic models including probabilistic latent semantic indexing interaction component model detailed settings compared methods follows. implemented pnmf onmf plsi using multiplicative updates. methods update rules iterations ensure algorithms suﬃciently converged. used default setting -spec. uses collapsed gibbs sampling round sampling sweeps graph once. sampling rounds ensure mcmc burn-in converged hyperparameters automatically adjusted using minka’s method despite mediocre results ncut runs fast gives pretty stable outputs. thus used initialization. getting ncut cluster indicator matrix entries feed starting point methods common initialization setting methods. three initialization points method provided ncut followed using three diﬀerent dirichlet priors clustering result method reported smallest approximation error performance clustering methods evaluated using real-world datasets. particular focus data curved manifold. thus selected datasets publicly available variety domains. data sources given supplemental document. method strong performance terms clustering purity. wins selected datasets. even three datasets ﬁrst second runner-up purities tied close winner. method particularly advantageous large datasets. note datasets table ordered sizes. winners joint winners smaller datasets example faces -spec mfeat digits. plsi performs quite similarly small clustering tasks. however demonstrates clear methods largest datasets. remarkable performance largest dataset mnist. case clustering unsupervised learning using method even achieved classiﬁcation accuracy close many modern supervised approaches whereas need labeled samples remove cluster-class permutation ambiguity. presented clustering method based nonnegative low-rank approximation three major original contributions novel decomposition approach approximating matrix derived two-step random walk; relaxed majorizationtable brief amazon book similarities according amazon.com buying records; votes voting records congress diﬀerent parties; yaleb face images collected under diﬀerent conditions; coil small images; isolet legreco handwritten english letter images; webkb sectors text document collections; mfeat usps pendigits mnist handwritten digit images. preprocessed datasets produce similarity graph input except amazon already sparse graph format. extracted scattering features image data except isolet mfeat feature representation. used tf-idf features text documents. feature extraction constructed k-nearestneighbor graph dataset. smallest datasets datasets. symmetrized binarized graph obtain input similarities hein b¨uhler inverse power method nonlinear eigenproblems applications -spectral clustering sparse pca. advances neural information processing systems minimization algorithm ﬁnding better approximating matrices; strategy uses regularization dirichlet prior initialization. experimental results showed method works robustly various selected datasets improve clustering purity large manifold datasets. dimensions aﬀect clustering performance. practice indicates initialization could play important role current algorithms local optimizers. using dirichlet prior smooth objective function space. could priors regularization techniques achieve better initializations. another dimension input graph. focused grouping procedure given similarities precomputed. notice better features better similarity measure significantly improve clustering purity. though anchorgraph sake including topic models comparison could beneﬁcial construct approximated approximating matrices principle. also suggests clustering analysis could performed deeper using hierarchical pre-training. detailed implementation investigated future.", "year": 2012}