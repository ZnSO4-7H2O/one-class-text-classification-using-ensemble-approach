{"title": "Innovation Pursuit: A New Approach to Subspace Clustering", "tag": ["cs.CV", "cs.IR", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "abstract": "In subspace clustering, a group of data points belonging to a union of subspaces are assigned membership to their respective subspaces. This paper presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of subspace clustering using a new geometrical idea whereby subspaces are identified based on their relative novelties. We present two frameworks in which the idea of innovation pursuit is used to distinguish the subspaces. Underlying the first framework is an iterative method that finds the subspaces consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data potentially orthogonal to all subspaces except for the one to be identified in one step of the algorithm. A detailed mathematical analysis is provided establishing sufficient conditions for iPursuit to correctly cluster the data. The proposed approach can provably yield exact clustering even when the subspaces have significant intersections. It is shown that the complexity of the iterative approach scales only linearly in the number of data points and subspaces, and quadratically in the dimension of the subspaces. The second framework integrates iPursuit with spectral clustering to yield a new variant of spectral-clustering-based algorithms. The numerical simulations with both real and synthetic data demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms, more so for subspaces with significant intersections, and that it significantly improves the state-of-the-art result for subspace-segmentation-based face clustering.", "text": "linear subspace models widely used data analysis since many datasets well-approximated lowdimensional subspaces data high-dimensional space lies single subspace conventional techniques principal component analysis efﬁciently used underlying low-dimensional subspace however many applications data points originating multiple independent sources case union subspaces better model data subspace clustering concerned learning lowdimensional subspaces clustering data points respective subspaces problem arises many applications including computer vision face clustering gene expression analysis image processing difﬁculties associated subspace clustering neither number subspaces dimensions known addition unknown membership data points subspaces. numerous approaches subspace clustering studied prior work including statistical-based approaches spectral clustering algebraic-geometric approach iterative methods section brieﬂy discuss popular existing approaches subspace clustering. refer reader comprehensive survey topic. iterative algorithms ﬁrst methods addressing multi-subspace learning problem. algorithms alternate assigning data points identiﬁed subspaces updating subspaces. drawbacks converge local class algorithms minimum typically assume dimension subspaces number known. another reputable idea subspace segmentation based algebraic geometric approach. algorithms generalized principal component analysis data using polynomials whose gradients point orthogonal subspace containing point. gpca impose restrictive conditions subspaces albeit sensitive noise exponential complexity number subspaces dimensions. class clustering algorithms termed statistical clustering methods make assumptions distribution data subspaces. example iterative algorithm assumes distribution data points abstract—in subspace clustering group data points belonging union subspaces assigned membership respective subspaces. paper presents approach dubbed innovation pursuit problem subspace clustering using geometrical idea whereby subspaces identiﬁed based relative novelties. present frameworks idea innovation pursuit used distinguish subspaces. underlying ﬁrst framework iterative method ﬁnds subspaces consecutively solving series simple linear optimization problems searching direction innovation span data potentially orthogonal subspaces except identiﬁed step algorithm. detailed mathematical analysis provided establishing sufﬁcient conditions proposed approach correctly cluster data points. proposed approach provably yield exact clustering even subspaces signiﬁcant intersections mild conditions distribution data points subspaces. shown complexity iterative approach scales linearly number data points subspaces quadratically dimension subspaces. second framework integrates ipursuit spectral clustering yield variant spectral-clustering-based algorithms. numerical simulations real synthetic data demonstrate ipursuit often outperform state-of-the-art subspace clustering algorithms subspaces signiﬁcant intersections signiﬁcantly improves state-of-the-art result subspace-segmentation-based face clustering. grand challenge contemporary data analytics machine learning lies dealing ever-increasing amounts high-dimensional data multiple sources different modalities. high-dimensionality data increases computational complexity memory requirements existing algorithms adversely degrade performance however observation high-dimensional datasets often intrinsic low-dimensional structures enabled noteworthy progress analyzing data. instance high-dimensional digital facial images different illumination shown approximately lowdimensional subspace development efﬁcient algorithms leverage low-dimensional representations images subspaces gaussian. algorithms typically require prior speciﬁcations number dimensions subspaces generally sensitive initialization. random sample consensus another iterative statistical method robust model ﬁtting recovers subspace time repeatedly sampling small subsets data points identifying consensus consisting points entire dataset belong subspace spanned selected points. consensus removed steps repeated subspaces identiﬁed. main drawback approach scalability since number trials required select points subspace grows exponentially number dimension subspaces. much recent research work subspace clustering focused spectral clustering based methods algorithms consist main steps mostly differ ﬁrst step. ﬁrst step similarity matrix constructed ﬁnding neighborhood data point second step spectral clustering applied similarity matrix. recently several spectral clustering based algorithms proposed theoretical guarantees superior empirical performance. sparse subspace clustering uses -minimization neighborhood construction. shown certain conditions yield exact clustering even subspaces intersection. another algorithm called low-rank representation uses nuclear norm minimization neighborhoods robust outliers provable guarantees data drawn independent subspaces. paper show idea innovation pursuit underlying proposed iterative subspace clustering method also used design spectral clustering based method. result spectral clustering based algorithm shown notably outperform existing spectral clustering based methods. proposed method advances state-of-the-art research subspace clustering several fronts. first ipursuit rests novel geometrical idea whereby subspaces identiﬁed searching directions innovation span data. second best knowledge ﬁrst scalable iterative algorithm provable guarantees computational complexity ipursuit scales linearly number subspaces quadratically dimensions contrast gpca ransac popular iterative algorithms exponential complexity number subspaces dimensions. third innovation pursuit data span enables superior performance subspaces considerable intersections brings substantial speedups comparison existing subspace clustering approaches. fourth formulation enables many variants algorithm inherit robustness properties highly noisy settings fifth proposed innovation search approach integrated spectral clustering yield spectral clustering based algorithm shown notably outperform existing spectral clustering based methods yields state-of-the-art results challenging face clustering problem. bold-face upper-case letters used denote matrices bold-face lower-case letters used denote vectors. given matrix denotes spectral norm. vector denotes -norm -norm. given matrices equal number rows matrix matrix formed concatenation given matrices {ai}n equal number rows union symbol deﬁne matrix vector whose elements equal absolute value elements real number equal complement denoted also positive integer index denoted innovation subspace consider subspaces contained other. means subspace carries innovation w.r.t. other. such corresponding subspace deﬁne innovation subspace novelty relative subspaces. formally innovation subspace deﬁned follows. deﬁnition assume orthonormal bases respectively. deﬁne subspace innovation subspace complement subspace similar deﬁne innovation subspace subspace complement fig. illustrates scenario data lies two-dimensional subspace one-dimensional subspace innovation subspace orthogonal since independent equal dimension. easy dimension equal dimension minus dimension section core idea underlying ipursuit described ﬁrst introducing non-convex optimization problem. then propose convex relaxation show solving convex problem yields correct subspaces mild sufﬁcient conditions. first present idea ipursuit analysis within proposed iterative framework. section present alternative framework wherein proposed innovation search approach integrated spectral clustering yield variant spectral-clusteringbased algorithms. follows following data model. data model data matrix rm×m represented arbitrary permutation matrix. columns rm×ni ri-dimensional linear subspace deﬁne orthonormal basis addition deﬁne space spanned data moreover assumed subspace i.e. subspaces {si}n innovation subspaces that subspace addition columns completely ipursuit multi-step algorithm identiﬁes subspace time. step data clustered subspaces. subspace identiﬁed subspace direct subspaces. data points identiﬁed subspace removed algorithm applied remaining data next subspace. accordingly step algorithm interpreted subspace clustering problem subspaces. therefore ease exposition ﬁrst investigate two-subspace scenario extend result multiple subspaces. thus subsection assumed data follows data model gain intuition consider example stating main result. consider case orthogonal assume non-orthogonality requirement merely used herein easily explain idea underlying proposed approach. optimal point following optimization problem -norm. hence equal number non-zero elements ﬁrst constraint forces search span data equality constraint used avoid trivial solution. assume data points distributed uniformly random. thus data aligned direction high probability optimization problem searches non-zero vector span data orthogonal maximum number data points. claim optimal point lies given assumption number data points greater number data points addition since feasible restricted lies cannot orthogonal data points since data uniformly distributed subspaces. addition cannot orthogonal data points given orthogonal. therefore optimal point cannot given optimal vector orthogonal maximum number data points. similarly optimal point cannot lies orthogonal data points however thus lies cost function decreased. iii. lies none subspaces orthogonal therefore cannot orthogonal maximum number data points. therefore algorithm likely choose optimal point thus obtain span columns corresponding non-zero elements following lemma ensures columns span lemma columns corresponding non-zero elements span conditions satisﬁed cannot follow data model data points union lower dimensional subspaces within innovation w.r.t. subspaces. note second requirement lemma satisﬁed columns follow data model case problem viewed subspace clustering subspaces. clustering problem subspaces addressed section ii-d. remark high level innovation search optimization problem ﬁnds sparse vector space interestingly ﬁnding sparse vector linear subspace bearing effectively used machine learning problems including dictionary learning conceptually assumption related assumption used example section ii-a sense makes likely direction innovation sufﬁcient conditions theorem optimal point characterized terms optimal solution oracle optimization problem feasible replaced oracle problem deﬁned cardinality complement theorem suppose data matrix follows data model also assume condition requirement lemma satisﬁed optimal point oracle program deﬁne follows provide detailed discussion signiﬁcance sufﬁcient conditions reveal interesting facts properties ipursuit. distribution data matters. known permeance statistic data points subspace permeance statistic deﬁned fig. data distributions two-dimensional subspace. blue stars circles data points projections unit circle respectively. left plot data points distributed uniformly random. thus aligned along speciﬁc directions permeance statistic cannot small. right plot data points aligned hence permeance statistic small. cost function non-convex combinatorial -norm minimization computationally tractable. since -norm known provide efﬁcient convex approximation -norm relax non-convex cost function rewrite core program ipursuit direction innovation. here unit -norm vector orthogonal vector chosen random unit vector section ii-f develop methodology learn good choice given data matrix. relaxation quadratic equality constraint linear constraint common technique literature segmentation subspaces performance guarantees based lemma show proposed program yields correct clustering optimal point lies given condition lemma satisﬁed lies given condition lemma satisﬁed following theorem provides sufﬁcient conditions optimal point provided distributed subspaces. setting show conditions naturally satisﬁed. lemma assume follows data model data points drawn uniformly random intersection unit sphere subspace. data align along particular directions much smaller since vector simultaneously orthogonal small number columns noting order sufﬁcient conditions naturally satisﬁed data well-distributed within subspaces. performance guarantees provided theorem extended subspaces. ipursuit identiﬁes subspaces consecutively i.e. subspace identiﬁed step. data lying identiﬁed subspace removed optimal direction-search applied remaining data points next subspace. process continued subspaces. order analyze ipursuit scenarios subspaces helpful deﬁne concept minimum innovation. deﬁnition said minimum innovation w.r.t. vector permeance statistic efﬁcient measure well data distributed subspace. fig. illustrates scenarios distribution data two-dimensional subspace. left plot data points distributed uniformly random. case permeance statistic cannot small since data points concentrated along directions. right plot data points concentrated along direction hence data well distributed subspace. case direction along data small projection. underscores relevance distribution data points within since cannot simultaneously orthogonal large number columns data align along particular directions. hence distribution data points within subspace bearing performance ipursuit. emphasize uniform distribution data points requirement algorithm shown numerical experiments. rather used proof theorem establishes sufﬁcient conditions correct subspace identiﬁcation uniform data distribution worst case scenarios. coherency important factor. important performance factor ipursuit coherency vector subspace clarify suppose satisﬁed assume vector lies optimal point lies ipursuit yield exact clustering. however strongly coherent optimal point rationale euclidean norm feasible point lying large satisfy equality constraint incoherent turn would increase cost function. matter fact factor second inequality conﬁrms intuition importance coherency particular suggests ipursuit could difﬁculty yielding correct clustering projection subspace increased decreased). coherence property could serious effect performance algorithm non-independent subspaces especially dimension intersection signiﬁcant. instance consider scenario vector chosen randomly deﬁne dimension intersection follows e{qt dimension thus e{qt therefore randomly chosen vector likely small projection innovation subspace large. such dealing subspaces signiﬁcant intersection favorable choose vector random. section ii-f section develop simple technique learn good choice given data. technique makes ipursuit remarkably powerful dealing subspaces intersection shown numerical results section. wisdom increasing number subspaces improve performance ipursuit data points well distributed subspaces likely dominate rhs. appendix section investigate sufﬁcient conditions simplify permeance statistic. section alternating direction method multipliers develop efﬁcient algorithm solving deﬁne rm×r orthonormal basis rank thus optimization problem equivalent subject deﬁne hence optimization problem equivalent lagrange multipliers. admm approach uses iterative procedure. deﬁne optimization variables lagrange multipliers iteration. deﬁne element-wise function max. iteration consists following steps obtain minimizing lagrangian function respect variables held constant. optimal obtained update lagrange multipliers follows steps repeated algorithm converges number iterations exceeds predeﬁned threshold. complexity initialization step solver plus complexity obtaining obtaining appropriate complexity applying clustering algorithm random subset rows addition complexity iteration solver thus overall complexity less since number data points remaining keeps decreasing iterations. cases hence overall complexity roughly ipursuit brings substantial speedups existing algorithms following unlike existing iterative algorithms exponential complexity number dimension subspaces according assumption used ﬁrst step linear constraint innovation pursuit optimization problem ipursuit expected ﬁrst identify step problem equivalent disjoining subspaces. particular step algorithm expected identify viewed separating step. proof theorem follows directly theorem subspaces namely similar theorem sufﬁcient conditions characterized terms optimal solution oracle optimization problem deﬁned below. also deﬁne cardinality optimal point theorem suppose data follows data model assume cannot follow data model assume minimum innovation respect deﬁne optimal point mdi)di. assume unit -norm complexity ipursuit linear number subspaces quadratic dimension. addition ipursuit linear complexity spectral clustering based algorithms complexity spectral clustering step plus complexity obtaining similarity matrix importantly solver proposed optimization problem complexity iteration operations whose complexity outside iterative solver. feature makes proposed method notably faster existing algorithms solve high-dimensional optimization problems. instance solving optimization problem algorithm roughly motivates methodology described next aims identify good choice vector optimization problem searches vector small projection columns optimal point closed-form solution namely singular vector corresponding least non-zero singular value subspaces close other optimal point close innovation subspace fact orthogonal hence vector innovation subspace small projection such subspaces close other least singular vector coherent innovation subspace good candidate vector numerical results section shown choice leads substantial improvement performance compared using randomly generated however settings singular values decay rapidly data noisy able obtain exact estimate lead undesirable usage singular vector corresponding noise constraint vector. next section investigate stability issues present robust variants algorithm presence noise. remark real data data noisy least singular vector refer least dominant singular vector corresponding smallest singular value surely associated noise component. noisy data matrix clean data follows data model noise component. rank equal thus singular values divided subsets dominant singular values small singular values consider optimization problem using i.e. clearly optimal point close subspace spanned singular vectors corresponding small singular values. thus denotes optimal solution fairly small elements cannot distinguish subspaces. however span dominant singular vectors approximately equal accordingly propose following approximation orthonormal basis span dominant singular vectors. ﬁrst constraint forces optimal point span serves good approximation span. instance consider columns -dimensional subspace columns another -dimensional subspace deﬁne optimal points respectively. fig. shows maximum element scaled one. clearly used correctly cluster data. addition rank subspace constraint ﬁlter remarkable portion noise component. data noisy singular values decay rapidly hard accurately estimate dimension incorrectly estimated contain singular vectors corresponding noise component wherefore optimal point could near noise singular vector. sequel present techniques effectively avoid undesirable scenario. using data point constraint vector singular vector corresponding noise component nearly orthogonal entire data i.e. small projection data points. thus optimal vector forced strong projection data point unlikely close noise singular vector. thus modify innovation subspace corresponding whp. determine good data point constraint vector leverage principle presented section ii-f. speciﬁcally data point closest least dominant singular vector rather least dominant singular vector itself. sparse representation optimal point rank i.e. direction span data including optimal direction sought ipursuit represented sparse combination data points. settings rewrite regularization parameter. forcing sparse representation optimal direction averts solution lies close proximity small singular vectors normally obtained linear combinations large number data points. using data points constraint vectors effectively accords robustness imperfection forming basis matrix however investigations show enforcing sparse representation optimal direction enhance performance cases. table algorithm details proposed method noisy data along used notation deﬁnitions. algorithm innovation pursuit noisy data initialization integers greater equal positive real numbers less number identiﬁed subspaces less number columns greater obtaining basis remaining data construct orthonormal matrix formed dominant singular vectors choosing vector column closest last column solve deﬁne optimal point deﬁne finding basis identiﬁed subspace construct matrix columns corresponding elements greater deﬁne matrix orthonormal basis dominant left singular vectors data deﬁne vector finding basis rest whose entries equal -norm columns normalize max. construct columns corresponding elements greater deﬁne orthonormal basis columns find data point belonging identiﬁed subspace assign identiﬁed subspace remove data points belonging identiﬁed subspace update removing columns corresponding identiﬁed subspace. remark proposed method made parameter-free avoid thresholding steps algorithm indeed step construct using columns corresponding largest elements chosen large enough sampled columns span identiﬁed subspace hence access upper bound dimension subspaces threshold algorithm chosen appropriately algorithm exhibits strong robustness presence noise. nonetheless data noisy error incurred step algorithm propagate unfavorably affect performance subsequent steps. following discuss main sources error present techniques effectively neutralize impact subsequent iterations. data points erroneously included suppose subspace identiﬁed given step algorithm i.e. optimal point lies innovation subspace corresponding strong data points noise component subspaces erroneously included subsection present technique remove erroneous data points analyze proposed technique robust subspace recovery algorithm). consider columns belongs subspaces deﬁne inner products since contains many data points coherent whp. thus removing portion columns small inner products columns belonging subspaces likely removed. addition obtain principal directions mitigates impact noise erroneous data points. technique used remove wrong data points table algorithm presents details using proposed idea fourth step algorithm remove erroneous data points complexity extra step roughly proposed method remarkably faster state-of-the-art subspace clustering algorithms even additional step since complexity solving underlying optimization problem linear number data points. section compare time proposed method state-of-the-art algorithms. algorithm fourth step algorithm technique removing erroneous data points initialization equal integer finding basis identiﬁed subspace construct matrix columns corresponding elements greater using columns corresponding largest elements deﬁne corresponding columns smallest -norms. deﬁne orthonormal basis data points remain unidentiﬁed suppose identiﬁed given iteration data points belonging identiﬁed i.e. points remain unidentiﬁed. case error occur fig. output proposed method different choices constraint vector. left right plots ﬁrst column used constraint vector respectively. ﬁrst column lies cluster data points column lies cluster data points. matrix number data points belonging increases identiﬁed subspace corresponding gets closer such choose constraint vector leads maximum distance identiﬁed subspaces. distance subspaces measured using orthogonal bases. instance used distance measure inversely proportional distance algorithm final error correction deﬁne matrices ˆdi} error correction deﬁne corresponding columns smallest -norms. obtain orthonomal basis column space ˆdi. update data clustering respect obtained bases ˆvi} innovation main requirement algorithm subspace direct subspaces. words dimension innovation subspace corresponding subspace least thus number subspaces cannot larger ambient dimension. speciﬁcally suppose data lies union d-dimensional subspaces dimension innovation subspace corresponding subspace equal case dimension ambient space point used constraint vector however error easily detected point used vector would sparse since optimal direction orthogonal data points expect remaining points example consider setting follows data model {ri} i.e. contains data {ni} points. fig. shows output right plot shows solution sparse. accordingly output sparse solve using constraint vector. error correction robustness spectral clustering based algorithms stems fact spectral clustering step considers representation data points thus errors similarity matrix signiﬁcantly impact overall performance algorithm. proposed multi-step algorithm complexity however data point assigned incorrect cluster given step assignment change subsequent steps algorithm consider remaining data points. thus scenarios data highly noisy propose technique ﬁnal error correction account correct wrong assignments data points incorrect clusters. table algorithm presents proposed technique applied clustered data algorithm terminates minimize clustering error. uses idea presented algorithm obtain bases subspaces respect clustering subsequently updated. fact algorithm applied multiple times time updating clustering. step proposed algorithm could solve multiple choices pick leading favorable solution. instance nearest neighbors least dominant singular vector among data points data points close least dominant singular vectors solve choices constraint vector. question remains identify best choice ideally would favor constraint vector minimizes clustering error. note step proposed algorithm clustering data points subspaces. clustering error increases distance identiﬁed subspaces decreases. clarify consider spanning subspaces respectively. subspace clustering algorithm clusters data greater equal example assume data points union random -dimensional subspaces i.e. data points. fig. shows left right plots respectively. dimension innovation subspace corresponding subspace equal whp. every subspace carries innovation relative subspaces. left plot non-zero elements correspond subspace also span data points corresponding zero elements contain span corresponding non-zero elements. thus algorithm successfully identify isolate ﬁrst subspace. contrast conditions violated scenario right plot. sub-clusters innovation according second condition lemma data points cluster union lower dimensional subspaces innovation w.r.t. ones. instance assume data points span three independent -dimensional subspaces. case algorithm identify three subspaces since column spaces innovation relative subspaces. sensible cluster data three clusters algorithm yield correct clustering sense. however makes sense cluster data exactly clusters algorithm cluster data correctly. example possible output would optimization problem ﬁnds direction span data large projection data point small projections data points. practice data points within subspace mutually coherent wherefore optimal point naturally strong projection data points subspace containing dek. fact obtained direction innovation simultaneously strongly coherent data point turn neighbors highly incoherent subspaces makes particularly powerful tool identify data points belonging subspace containing point show detail instance even unwieldy scenario right plot fig. data points correspond elements largest values subspace. such solving innovation search optimization problem data points {dek}m corresponding optimal directions utilized construct neighborhood point. basis direction search subspace clustering spectral-clustering-based method uses ipursuit core procedure build similarity matrix. obtains optimal directions solving dimensional used) optimization problem. subsequently similarity matrix formed spectral clustering applied cluster data. present results using section refer reader details. section present numerical experiments study performance proposed subspace clustering algorithm compare performance existing approaches. first present numerical simulations conﬁrming intuition gained performance analysis. then compare time performance algorithm existing algorithms investigate speed capability ipursuit dealing non-independent subspaces noisy data. subsequently apply ipursuit real data motion segmentation. finally present experiments synthetic real data highlight integration proposed direction search spectral clustering overcome limitations ipursuit discussed section iv-a. presented experiments consider subspaces intersection. data experiments generated follows. given data points union d-dimensional subspaces {si}n deﬁne random y-dimensional subspace. generate subspace random dimensional subspace. thus dimension intersection subspaces {si}n equal whp. simulations ipursuit refers algorithm error correction techniques presented algorithm algorithm noisy data solve data points closest least singular vector choose ﬁnal vector whereas motion segmentation data neighboring data points. experiments using synthetic data expect section data points distributed uniformly random within subspaces i.e. data point lying ri-dimensional subspace generated elements sampled independently standard normal distribution simulations performed desktop intel core processor ram. performance ipursuit improved coherent innovation subspace. assumed data lies subspaces dimension subspaces equal dimension intersection varies subspace data points left plot fig. shows phase transition plane dimension deﬁned intersection subspaces. used subspace fig. left phase transition various coherency parameters dimension intersection. white designates exact subspace identiﬁcation. right performance ipursuit versus dimension intersection. fig. left phase transition different values number data points ﬁrst second subspaces. white designates exact subspace identiﬁcation. right clustering error ipursuit versus dimension intersection. shown performance improves increases. left plot fig. shows large enough ipursuit yields exact segmentation even simulation conﬁrms analysis regarding importance coherency parameter. next shown choosing vector using technique proposed section ii-f substantially improve performance algorithm. experiment data points assumed -dimensional subspaces right plot fig. shows probability correct subspace identiﬁcation versus dimension intersection. point plot obtained averaging independent runs used subspace segmentation. again trial considered successful satisﬁed. clear equal least dominant singular vector data algorithm performs substantially better i.e. algorithm robust intersection subspaces. least dominant singular vector coherent innovation subspace subspaces close other. lemma shown optimal point likely innovation subspace ratio increases. section conﬁrm analysis numerically. according presented analysis numerical simulations sections algorithm performs well even however observe effect ratio consider particularly hard subspace clustering scenario signiﬁcant intersection subspaces. speciﬁcally assume given data points -dimensional subspaces dimension intersection equal left plot fig. shows phase transition ipursuit plane used subspace identiﬁcation. generate random solving proposed direction search optimization problem computation complexity. feature makes proposed iterative method notably faster state-of-theart clustering algorithms. section study time proposed method ssc-omp k-ﬂats admm solvers provided authors code provided author. ssc-omp number neighborhing data points found function equal value number parameters equal min. simulation since generated subspaces independent methods except k-ﬂats algorithm yield exact subspace clustering scenarios. table table compares time algorithms different number data points. data points union three -dimensional subspaces {ni} proposed approach exhibits notable speedup spectral clustering based methods. complexity k-ﬂats algorithm roughly speed comparable ipursuit albeit performance sensitive random initialization. also k-ﬂats requires prior knowledge dimensions number subspaces performance degrades subspaces close. table table studies time versus number subspaces. data lies union -dimensional subspaces shown time ipursuit increases linearly number subspaces. time exhibit strong dependence number subspaces. simulation time ssc-omp decreasing function consider setting data points union -dimensional subspaces {si} data points subspace distribution data subspaces uniformly random. experiment compare performance ipursuit stateof-the-art algorithms. number replicates used spectral clustering equal deﬁne clustering error ratio misclassiﬁed points total number data points. right plot fig. shows versus dimension intersection. dimension intersection varies thus rank data ranges point plot obtained averaging independent runs. ipursuit shown yield best performance. proposed algorithm ﬁnds subspaces consecutively thus subspaces identiﬁed steps. section study performance ipursuit ssc-omp different noise levels varying dimensions intersection subspaces gives rise rank high rank data matrices. assumed follows data model {ri} dimension intersection subspaces varies thus rank ranges noisy data follows elements sampled independently zero mean gaussian distribution. fig. shows performance different algorithms versus dimension intersection equal observe even ipursuit signiﬁcantly outperforms algorithms. addition data noisy i.e. yields better performance dimension intersection large. ssc-omp yield better performance lower dimension intersection. explained fact rank data high dimension intersection subspace projection operation always ﬁlter additive noise effectively. distribution worst case scenarios. section provide examples coherent data indicating conditions means necessary. similar experiment section data points union -dimensional subspaces. dimension intersection subspaces equal however unlike experiment section data points distributed uniformly random within subspaces. data point subspace generated ωˆg. vector ﬁxed unit vector sampled uniformly random unit -norm sphere. thus data points subspace concentrated around coefﬁcient determining concentrated smaller implies data points mutually coherent. table provides clustering error different values shown decreasing even result improved performance. reason two-fold data points become coherent elements corresponding subspace constraint vector lies increases also performance error correction technique presented algorithm improves. however trend continue data points become highly concentrated around given direction case algorithm cannot obtain accurate basis subspaces rapid decay singular values fact data noisy dimension intersection fairly large. section apply ipursuit problem motion segmentation using hopkins dataset contains video sequences motions. data generated extracting tracking feature points frames videos checkerboard trafﬁc videos. motion segmentation motion corresponds subspace. thus problem cluster data lying three subspaces. table shows ipursuit ssc-omp k-ﬂats. results reported ssc-omp number parameters motion segmentation equal respectively. observe ipursuit yields competitive results comparable outperforms ssc-omp k-ﬂats. section shown integrating ipursuit spectral clustering effectively overcome limitations discussed section iv-a. detailed performance analysis integration innovation pursuit spectral clustering refer reader synthetic data consider data points lying union random -dimensional subspaces data points dimension intersection subspaces equal subspace innovation subspace dimension w.r.t. subspaces whp. innovation requirement satisﬁed. table shows various algorithms different values integration ipursuit spectral clustering yields accurate clustering even outperforms spectral clustering based methods even subspaces lack relative innovation. face clustering face clustering expressive real world example study second limitation discussed section iv-a. extended yale dataset contains images individuals frontal view different illumination conditions faces subject approximated low-dimensional subspace. thus data containing face images multiple subjects modeled union subspaces. fig. data formed face images individuals. left elements vector ﬁrst column used constraint vector. right elements vector column used constraint vector. illustration compose data matrix contain vectorized images individuals solve ﬁrst column constraint vector. form orthonormal basis described step algorithm columns supposed span column space left plot fig. shows -norm columns observe notable part columns ﬁrst cluster remarkable projections complement space span. similar behavior observed choose data point second cluster constraint vector right plot fig. column used. thus cannot obtain proper basis clusters. fact data points within cluster approximately form sub-clusters relative innovation. apply integration ipursuit spectral clustering face clustering present results different number clusters table performance also compared tsc. heretofore yielded best known result problem. number clusters shown algorithms different random combinations subjects clusters. expedite runtime project data span ﬁrst left singular vectors affect performance algorithms report results without projection projection shown yields accurate clustering notably outperforms performance achieved ssc. simplifying requirements theorem according theorem data points well distributed within subspaces sufﬁcient number data points subspaces optimal point lies even coherent intuition basis unproven conjecture herein simplify sufﬁcient conditions proof lemma assume conditions lemma satisﬁed deﬁne vector projection onto contradiction assume columns corresponding non-zero elements span denote columns orthogonal orthogonal respectively. addition deﬁne within span individual dimension less subspace subset dimension less hence follow data model leads contradiction. conditions ensure feasible point optimal point cost function increased move optimal point along feasible perturbation direction. observe feasible point perturbation satisﬁes theorem conjecture optimal point nonconvex optimization problem lies innovation subspaces enough data points subspaces well distributed. first avoid cumbersome notation deﬁned similar deﬁne projection columns dtm− itm−. inﬁmum permeance static itm−. according conditions columns evident three factors important disjoining {si}m− distribution data points subspaces {si}m− since determines value permance statistic. zhang szlam lerman median k-ﬂats hybrid linear modeling many outliers ieee international conference computer vision workshops chen lerman spectral curvature clustering international journal computer vision vol. m.-h. yang k.-c. kriegman clustering appearances objects varying illumination conditions ieee conference computer vision pattern recognition vol. h.-p. kriegel kr¨oger zimek clustering high-dimensional data survey subspace clustering pattern-based clustering correlation clustering transactions knowledge discovery data vol. spielman wang wright exact recovery sparselyused dictionaries proceedings twenty-third international joint conference artiﬁcial intelligence. aaai press boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends® machine learning vol. k.-c. kriegman acquiring linear subspaces face recognition variable lighting ieee transactions pattern analysis machine intelligence vol. foucart rauhut mathematical introduction compressive deﬁned lemma assumed data points subspaces distributed randomly data points normalized -norm data points equal one). therefore vectors {xi} modeled i.i.d. random vectors uniformly distributed unit sphere based observation make following lemmas obtain lemma suppose i.i.d. random vectors uniformly distributed unit sphere wright yang ganesh sastry robust face recognition sparse representation ieee transactions pattern analysis machine intelligence vol. springer rahmani atia randomized robust subspace recovery outlier detection high dimensional data matrices ieee transactions signal processing vol. march", "year": 2015}