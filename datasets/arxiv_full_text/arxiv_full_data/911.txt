{"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments.", "text": "hessian-free optimization successfully used training deep autoencoders recurrent networks. uses conjugate gradient algorithm construct update directions curvature-vector products computed order time gradients. paper exploit property study stochastic gradient curvature mini-batches independent dataset size. modify martens’ settings integrate dropout method preventing co-adaptation feature detectors guard overﬁtting. stochastic hessian-free optimization gives intermediary achieves competitive performance classiﬁcation deep autoencoder experiments. stochastic gradient descent become popular algorithm training neural networks. simple implement noisy updates often leads solutions well-adapt generalization held-out data furthermore operates small mini-batches potentially allowing scalable training large datasets. training deep networks used ﬁne-tuning layerwise pre-training overcomes many difﬁculties training deep networks. additionally augmented dropout means preventing overﬁtting. recent interest second-order methods training deep networks partially successful adaptation hessian-free instance general family truncated newton methods. second-order methods operate batch settings less substantial weight updates. furthermore computing gradients curvature information large batches easily distributed across several machines. martens’ able successfully train deep autoencoders without pre-training later used solving several pathological tasks recurrent networks iteratively proposes update directions using conjugate gradient algorithm requiring curvature-vector products explicit computation curvature matrix. curvature-vector products computed order time takes compute gradients additional forward backward pass function’s computational graph paper exploit property introduce stochastic hessian-free optimization variation operates gradient curvature mini-batches independent dataset size. goal developing combine generalization advantages second-order information adapt behaviour choice batch size number conjugate gradient iterations behaviour either becomes characteristic additionally integrate dropout means preventing co-adaptation feature detectors. perform experimental evaluation classiﬁcation deep autoencoder tasks. classiﬁcation dropout competitive dropout tasks considered autoencoders performs comparably momentum-based methods. moreover tuning learning rates needs done. much research investigated developing adaptive learning rates incorporating secondorder information sgd. proposed augmenting diagonal approximation hessian adagrad uses global learning rate dividing norm previous gradients update. adagrad shown beneﬁcial training deep distributed networks speech object recognition completely avoid tuning learning rates considered computing rates minimize estimates expectation loss time. proposed sgd-qn incorporating quasi-newton approximation hessian used pascal large scale learning challenge tracks. recently provided relationship krylov subspace descent natural gradient gauss-newton curvature matrix. furthermore argue natural gradient robust overﬁtting well order training samples. methods incorporating natural gradient tonga also showed promise speeding neural network training. analyzing difﬁculty training deep networks done proposing weight initialization demonstrates faster convergence. recently argue large neural networks waste capacity sense adding additional units fail reduce underﬁtting large datasets. authors hypothesize culprit suggest exploration stochastic natural gradient stochastic second-order methods. results motivate development shf. show careful attention parameter initialization momentum schedule ﬁrstorder methods competitive training deep autoencoders recurrent networks. compare methods autoencoder evaluation. related work proposes dynamic adjustment gradient curvature mini-batches convex losses based variance estimations. unlike work batch sizes used dynamic ﬁxed ratio initialized function dataset size. work using second-order methods neural networks include proposed using jacobi pre-conditioner using generate text recurrent networks explored training krylov subspace descent unlike could used hessian-vector products requires additional memory store basis krylov subspace. l-bfgs also successfully used ﬁne-tuning pre-trained deep autoencoders convolutional networks training deep distributed networks developments detailed discussion gradient-based methods neural networks described consider unconstrained minimization function respect parameters speciﬁcally assume written composition convex loss function output neural network non-input layers. mostly focus case non-convex. typically chosen matching loss corresponding transfer function single input layer network expressed transfer function weights connecting layers bias vector. common transfer functions include sigmoid hyperbolic tangent tanh rectiﬁed linear units max. case classiﬁcation tasks calculation requires time thus often prohibitive. hessian-free optimization alleviates using conjugate gradient algorithm compute approximate minimizer speciﬁcally minimizes quadratic objective given corresponding minimizer −b−∇f motivation using follows computing expensive compute product vector computed order time takes compute using r-operator thus efﬁciently compute iterative solution linear system corresponding update direction non-convex hessian positive semi-deﬁnite thus equation longer well deﬁned minimum. following martens instead generalized gauss-newton hessian long matrix deﬁned convex given vector product computed successively ﬁrst computing compute utilize r-operator. r-operator respect deﬁned components corresponding parameters layers r{y} order compute simply apply backjv instead usually done compute thus propagation using vector computed forward backward pass sense are. damping quadratic gives measure conservative quadratic approximation large value conservative updates become similar stochastic gradient descent. alternatively small allows substantial parameter updates especially finally backtracking methods used optimizing select backtracking linesearch compute rate methods operate standard backtracking proposals objective longer decreases. martens’ implementation utilizes full dataset computing objective values gradients mini-batches computing curvature-vector products. naively setting batch sizes small causes several problems. section describe problems contributions modifying martens’ original algorithm setting. =max small positive constant. effect stopping criteria dependency strength damping parameter among attributes current parameter settings. sufﬁciently large requires iterations preconditioner used. decreases iterations required account pathological curvature occur optimizing thus leads expensive iterations. behavior would undesirable stochastic setting preference would towards equal length iterations throughout training. account this number iterations across training classiﬁcation training deep autoencoders. denote cut-off. setting limit number iterations used also damping effect since objective function quadratic approximation tend diverge iterations increase note shorter number runs iterates solution used backtracking step. contributor success martens’ information sharing across iterations. iteration initialized previous solution iteration small decay. rest work denote δ-momentum. δ-momentum helps correct proposed update directions quadratic approximation varies across iterations sense momentum used share gradients. momentum interpretation ﬁrst suggested context adapting setting short runs. unfortunately δmomentum becomes challenging short runs used. given non-zero initialization likely remain positive terminating assuming means reduction ratio negative thus increased compensate. necessarily unwanted behavior occur frequently push conservative possibly result backtracking linesearch reject proposed updates. decay epoch batch training ﬁxed suitable stochastic setting unlikely global decay parameter sufﬁcient. schedule annealing effect sense values near feasible late training even iterations property otherwise hard achieve. allows beneﬁt sharing information across iterations late training similar typical momentum method. remaining question consider sizes gradient curvature mini-batches. discuss theoretical advantages utilizing mini-batches computing gradient curvature vector products. setting lead difﬁculties. using same-sized batches allows training unfortunately become incompatible short hard-limit number iterations since requires work optimize approaches zero. account this classiﬁcation tasks iterations used gradient mini-batches times larger curvature mini-batches. deep autoencoder tasks iterations used instead gradient curvature batches size. behavior dependent whether dropout used training. figure demonstrates behavior classiﬁcation training without dropout. dropout longer converges instead plummets rises ﬂattens out. settings decrease substantially negatively effect proposed solution consequently reduction ratio. thus amount work required remains consistent late training. beneﬁt using larger gradient batches account additional computation computing curvature-vector products would make training longer mini-batches small size. gradients objectives computed using full training throughout algorithm including backtracking backtracking linesearch. utilize gradient mini-batch current iteration order compute necessary gradient objectives throughout algorithm. given suitable initial value converge zero training progresses. observed damping criteria harsh stochastic setting sense frequently oscillate sensible given size curvature mini-batches. instead much softer criterion lambda updated choice although somewhat arbitrary consistently effective. thus reduction ratio values computed curvature mini-batches less overall inﬂuence damping strength. dropout recently proposed method improving training neural networks. training hidden unit omitted probability along optionally omitting input features similar denoising autoencoder dropout viewed ways. randomly omitting feature detectors dropout prevents co-adaptation among detectors improve generalization accuracy held-out data. secondly dropout seen type model averaging. test time outgoing weights halved. consider network single hidden layer feature detectors using mean network test time corresponds taking geometric average networks shared weights. dropout integrated stochastic randomly omitting feature detectors gradient curvature mini-batches last hidden layer figure values damping strength training mnist usps without dropout using classiﬁcation. dropout included damping strength initially decreases followed steady increase time. since curvature estimates noisy important consider stability updates different stochastic networks used computation. weight updates dropout augmented momentum stability also speed learning. speciﬁcally iteration parameter update given momentum learning rate respectively. incorporate additional exponential decay term performing parameter updates. speciﬁcally parameter update computed ﬁxed parameter chosen user. incorporating updates along δ-momentum leads stable updates convergence particularly dropout integrated training. pseudo-code iteration implementation stochastic hessian-free presented. given gradient minibatch ﬁrst sample dropout units inputs last hidden layer network. take form binary vector used multiplied component-wise activations pseudo-code allows updating using levenberg-marquardt style damping. finally backtracking linesearch steps performed compute rate serves last defense potentially poor update directions. since curvature mini-batches sampled subset gradient mini-batch sensible utilize different curvature mini-batches different epochs. along cycling gradient mini-batches epoch also cycle curvature subsets every epochs size gradient mini-batches divided size curvature mini-batches. example gradient batch size curvature batch size curvature mini-batch sampling completes full cycle every epochs. finally simple speed training indicated cache activations initially computing objective iteration requires computing curvaturevector product network parameters ﬁxed thus wasteful re-compute network activations iteration. perform experimental evaluation classiﬁcation deep autoencoder tasks. goal classiﬁcation experiments determine effectiveness test error generalization. autoencoder tasks instead focus measuring effectiveness optimizer training data. datasets experiments summarized follows mnist handwritten digits size training samples testing samples. classiﬁcation train networks size rectiﬁer activations. deep autoencoders encoder architecture ---- symmetric decoding architecture used. logistic activations used binary cross entropy error. classiﬁcation experiments data scaled zero mean unit variance. curves artiﬁcial dataset curves size training samples testing samples. train deep autoencoder using encoding architecture ----- symmetric decoding. similar mnist logistic activations binary cross entropy error used. usps handwritten digits size examples. perform classiﬁcation using randomly sampled batches training examples testing examples batch equal number digit. classiﬁcation networks size trained rectiﬁer activations. data scaled zero mean unit variance. reuters collection text documents categories. document represented -dimensional bag-of-words vector. word counts transformed done publically available train/test split used. train networks size classiﬁcation high dimensionality inputs reduces softmaxregression. classiﬁcation experiments perform comparison without dropout dropout classiﬁcation experiments utilize sparse initialization martens initial biases sparse initialization combination relus make networks similar deep sparse rectiﬁer networks algorithms trained epochs mnist epochs usps reuters. weight decay dropout shf. held-out validation used determining amount input dropout training uses exponential decreasing learning rate schedule initialized combination max-norm weight clipping allows larger learning rates greater exploration early training. linearly increasing momentum schedule used initial momentum ﬁnal momentum weight decay used. additional comparison also train dropout dropout used last hidden layer case dropout shf. deep autoencoder experiments experimental setup chapter particular focus solely training error without penalty order determine effectiveness optimizer modeling training data. comparison made momentum nesterov’s accelerated gradient curves uses initial damping gradient curvature batch sizes iterations batch. mnist initial gradient curvature batch sizes iterations batch. autoencoder training sufﬁcient progress made occurs around epochs curves epochs mnist. figure summarizes classiﬁcation results. epoch dropout achieves errors mnist. result similar achieve errors various network sizes training thousand epochs. without dropout input corruption achieves errors mnist existing methods incorporate prior knowledge pre-training image distortions dropout. hypothesize improvements made ﬁne-tuning unsupervised layerwise pre-training. epochs training random splits usps obtain ﬁnal classiﬁcation errors mean test error algorithms input corruption. additional comparison obtains mean classiﬁcation error using pre-trained deep network large-margin nearest neighbor classiﬁcation size splits. without dropout overﬁts training data. reuters dataset without dropout demonstrate accelerated training. hypothesize speedup also obtained starting training much smaller initialization suspect conservative given problem convex. table training errors deep autoencoder tasks. results obtained refers momentum capped similarily sgd-vi refers using variance normalized initialization figure table summarize results. inspired make additional modiﬁcation algorithms. soon training begins diverge turn decay parameter similar fashion momentum parameter decreased longer initialized previous solution instead initialized zero. dramatic effect training error lesser extent momentum nesterov’s accelerated gradient. describes behaviour effect follows large momentum optimizer able make steady progress along slow changing directions curvature. decreasing momentum late training optimizer able quickly reach local minimum ﬁner optimization along high curvature directions would otherwise difﬁcult obtain aggressive momentum schedule. observation motivates relationship momentum information sharing experimental results demonstrate perform signiﬁcantly better worse datasets compared existing approaches. able outperform curves mnist. attractive property shared requiring careful schedule tuning necessary momentum nag. also attempted experiments using setup classiﬁcation smaller batches iterations. results worse curves lowest training error obtained shows setup useful viewpoint noisy updates test generalization hamper effectiveness making progress hard optimize regions. paper proposed stochastic variation martens’ hessian-free optimization incorporating dropout training neural networks classiﬁcation deep autoencoder tasks. adapting batch sizes number iterations constructed perform well classiﬁcation dropout optimizing deep autoencoders comparing momentum methods. initial results promising interest would adapting stochastic hessian-free optimization network architectures convolutional networks. common approach training convolutional networks incorporating diagonal hessian approximation dropout recently used training deep convolutional network imagenet recurrent networks. largely believed rnns difﬁcult train exploding/vanishing gradient problem. recent years recurrent networks become popular several advancements made training recursive networks. recursive networks successfully used tasks sentiment classiﬁcation compositional modeling natural language word embeddings architectures usually trained using l-bfgs. clear whether setup easily generalizable architectures whether improvements need considered. furthermore additional experimental comparison would involve dropout adaptive methods adagrad well importance pre-conditioning none less hope work initiates future research developing stochastic hessian-free algorithms. author would like thank csaba szepesvári helpful discussion well david sussillo guidance ﬁrst learning implementing author would also like thank anonymous iclr reviewers comments suggestions.", "year": 2013}