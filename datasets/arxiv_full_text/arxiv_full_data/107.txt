{"title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech  Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.", "text": "rnns model input sequence either unidirectionally bidirectionally unidirectional rnns esti− using left context current input processing input left right hidden state forward direction. desirable applications requiring latency inputs corresponding outputs. usually output targets delayed respect features giving access small amount right/future context improving classiﬁcation accuracy withincurring much latency. afford latency seeing entire sequence bidirectional rnns estimate label poste← riors using separate layers processing input forward backward directions. deep lstm architectures built stacking multiple lstm layers. shown perform better shallow models speech recognition bidirectional models lstm layers depth operating forward another operating backward direction input sequence. layers connected previous forward backward layers. output layer also connected ﬁnal forward backward layers. experiment different acoustic units output layer including context dependent states phones context independent context dependent train models distributed manner using asynchronous stochastic gradient descent optimization technique allowing parallelization training large number machines cluster enabling large scale training neural networks weights networks randomly initialized uniform distribution. clip activations memory cells gradients making training stable. recently shown deep long short-term memory recurrent neural networks outperform feed forward deep neural networks acoustic models speech recognition. recently shown performance sequence trained context dependent hidden markov model acoustic models using lstm rnns equaled sequence trained phone models initialized connectionist temporal classiﬁcation paper present techniques improve performance lstm acoustic models large vocabulary speech recognition. show frame stacking reduced frame rate lead accurate models faster decoding. phone modeling leads improvements. also present initial results lstm models outputting words directly. index terms speech recognition acoustic modeling connectionist temporal classiﬁcation long short-term memory recurrent neural networks lstm rnn. speech recognition systems using recurrent feedforward neural networks around decades recently displaced gaussian mixture models state-of-the-art acoustic model. recently shown recurrent neural networks outperform feed-forward networks large-scale speech recognition tasks conventional speech systems cross-entropy training state targets followed sequence training. models blank symbol phonetic labels propose alternative loss conventional cross-entropy training. recently showed rnns lvcsr trained improved smbr sequence training criterion approaches state-of-the-art paper investigate smbr-trained models acoustic speech recognition show appropriate features introduction context dependent phone models outperform conventional lstm models relative recognition accuracy. next section describes lstm rnns summarizes method sequence training. describe acoustic frame stacking well context dependent phone whole-word modeling. following section describes experiments presents results summarized conclusions. labels unknown. implemented softmax output layer using additional unit blank label used estimate probability outputting label given time. blank similar non-perceiving state proposed earlier output label probabilities network deﬁne probability distribution possible labelings input sequences including blank labels. network trained optimize total probability correct labelings training data estimated using network outputs forward-backward algorithm correct labelings input sequence deﬁned possible labelings input target labels correct sequence possibly repetitions blank labels permitted separate labels. targets training efﬁciently easily computed using ﬁnite state transducers described additional optional blank states interposed states sequence labels. conventional hybrid speech handwriting recognition systems usually train ﬁxed alignments forward-backward algorithm reestimate network targets given current model equally applied conventional recurrent feed-forward networks alignment available. conventional realignment systems followed practice choosing alignments maximize likelihood data state sequence match transcript posteriors scaled label priors. hence differs conventional modeling ways. first additional blank label relieves network making label predictions frame uncertain. second training criterion optimizes probability state sequences rather likelihood inputs. whether using posteriors blank symbol conventional model scaled posteriors target posteriors computed forward-backward algorithm gradients cross entropy loss softmax outputs targets backpropagated network. described standard beam search algorithm speech decoding models allowing optional blank state labels output labels search graph. decoding scale blank label posterior constant scalar decided cross-validation held-out set. found models phone labels require language model weight normalize acoustic model scores respect language model scores. however models phone labels perform better weighting constant cross-entropy criteria suboptimal objective word error rate minimization asr. number sequence-level discriminative training criteria incorporating lexical language model constraints used speech decoding shown improve performance acoustic models bootstrapped training criteria paper state-level minimum bayes risk sequence discriminative training criterion improving accuracy acoustic models initialized criterion. discussed decoding models requires scaling blank label posterior. found smbr training scaling issue scale blank label posterior decoding utterance numerator denominator lattices smbr training. alternatively blank label scaling baked bias blank label output unit model adding negative scale starting smbr training state priors baked softmax biases conventional models sequence training. summarize sequence discriminative training difference conventional models blank symbol. henceforth refer models contrast conventional models blank symbol which paper train ﬁxed hard alignments. acoustic features -dimensional ﬁlterbank energy features computed every windows. obtained signiﬁcant improvements increasing number ﬁlterbanks present results latter. past observed training unstable training runs fail converge. found stability improved starting training using output layers conventional loss initializing network whose lstm layers pretrained using loss. suggest inherent arbitrariness alignment considers valid alignment target symbols emitted correct order interspersed arbitrary number blanks. reducing huge space alignments reduce number input frames. done simply decimating input frames though present full acoustic information input signal ﬁrst stack frames networks sees multiple frames time decimate frames skip forward multiple frames processing super-frame. process illustrated figure acoustic model able process full signal acoustic model computation need happen every network ﬁxed size results dramatic reduction acoustic model computation decoding time. figure stacking subsampling frames. acoustic features generated every concatenated downsampled input network frames stacked unidirectional bidirectional models previously models used context independent outputs well known context dependent states outperform context independent models conventional speech recognition systems gmm-based neural-network hybrids. argue context dependency important constraint decoding provides useful labeling state outputs believe useful models. hybrid speech recognition models give similar results context dependent state models provided minimum duration enforced. repeat procedure using hierarchical binary divisive clustering algorithm young context-tying. three frames -dimensional logmel ﬁlterbanks represent whole-phone instance. tree phone constructed maximum-likelihoodgain phonetic question used split data node. training data phones. found before enforcing minimum duration phone found improve word error rates cutoff training-set duration histogram minimum duration cd-phone decoding conventional models. duration model imposed. combination lstm rnns’ memory ctc’s ability learn alignment label acoustic frame sequences relieving network label frame introducing blank label enables longer duration modeling units. instance train acoustic models predicting whole words rather phonemes. previous studies using lstm models keyword spotting tasks small vocabularies paper investigate effectiveness word acoustic models trained large training various large vocabularies ranging words. train evaluate lstm acoustic models handtranscribed anonymized utterances taken real google voice search trafﬁc. training consists million utterances average duration achieve robustness background noise reverberant environments synthetically distort utterance room simulator virtual noise source. noise taken audio youtube videos. utterance randomly distorted variations. multi-style training also alleviates overﬁtting models training data. test set’s utterances distorted held-out noise samples. evaluation uses -gram language model pruned million n-grams. rescoring word lattices larger n-gram model gives similar relative gains acoustic models therefore report results ﬁrst pass decoding. experiments wide beam decoding avoid search errors obtain best possible performance. training networks criterion using ﬁxed alignments training utterances force-aligned using million parameter states. explored variations frame stacking skipping described section conventional unidirectional models’ inputs either stack consecutive feature frames skip frame present single frame frames delayed target approaches give similar results. bidirectional models need single frame input. bidirectional models stack consecutive feature frames input feature vector skip frames. unidirectional models stack consecutive feature frames skip frames found longer context helps unidirectional models needed bidirectional models. unidirectional models used memory cells layer bidirectional models memory cells direction layer. conventional models best results lstm layers cells recurrent projection layer units. table shows word error rates voice search task various unidirectional bidirectional lstm acoustic models trained loss state phone phone labels. expected trying learn state labels state models perform well. unidirectional phone model marginally better corresponding state model. phone models perform similarly state models. phone models give signiﬁcant improvements phone models unidirectional bidirectional. bidirectional models improve unidirectional ones state phone models improving phone models table shows results sequence discriminative training initial ce/ctc models smbr loss. smbr training consistently improves models initially trained loss relative. obtain best results phone models outperforming second best model unidirectional bidirectional model. figure shows label posteriors estimated various phone phone models. seen spikes label posteriors correspond alignment differ models. unidirectional models delay output labels milliseconds. expected bidirectional models make better predictions. models good modeling silence labels. sequence discriminative training changes posteriors spike positions. learning alignment conventional gmm-hmm systems dnn-hmm hybrid systems shown work well learning conventional alignment without blank label using lstm rnns work well. figure shows label posteriors estimated using unidirectional lstm phone model trained loss blank label allowed phone labels. model learned arbitrary alignment. memory models contrast memoryless feed forward neural networks means model delay outputs instead making decisions using local predicted word sequence word highest probability taken ignoring repetitions blank label language model decoding. also experimented vocabulary word models note bidirectional model gives lower unidirectional model. figure shows label posteriors estimated biditable lstm word acoustic models. wers vocabulary rates word models helddata decoding language model. wers last column computed ignoring utterances containing oovs. rectional models vocabulary heldout utterance. plot posteriors labels probability time. words ’dietary’ ’nutritionist’ vocabulary. interesting models make spiky predictions even large vocabulary predictions confused words output time. although models similar spike positions words different model. work shown number improvements recurrent network acoustic models. longer-term feature representations processed lower frame rates brought stability convergence training models blank symbol outputs also resulting considerable reduction computation. sequence training models found perform better previous acoustic models. performance blank-symbol acoustic models improved introduction context-dependent phonetic units result models outperform conventional sequence trained lstm-hybrid models. also shown train word level acoustic models achieve reasonable accuracy medium vocabulary speech recognition without using language model. figure label posteriors estimated various lstm models plotted ﬁxed frame level alignments shown labels alignment heldout utterance ‘museums chicago’. refers blank label. temporal information. therefore model learns alignment chooses adjust labeling according certainty label given input. results arbitrary alignment labels repeated others depending input. note using hybrid approach prior cannot issue. experimented word models various vocabulary sizes output layer directly predicts words rather phonemes. used different vocabularies frequent words training data transcripts. table shows wers bidirectional word acoustic models calculated edit distance reference word sequence kingsbury lattice-based optimization sequence classiﬁcation criteria neural-network acoustic modeling ieee international conference acoustics speech signal processing taipei taiwan apr. kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization interspeech robinson hochberg renals recurrent networks continuous speech recognition automatic speech speaker recognition c.-h. soong norwell kluwer academic publishers vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks interspeech senior irsoy graves beaufays schalkwyk learning acoustic frame labeling speech recognition recurrent neural networks ieee international conference acoustics speech signal processing eyben wollmer schuller graves from speech letters using novel neural network architecture grapheme based automatic speech recognition understanding asru ieee workshop ieee graves mohamed hinton speech recognition deep recurrent neural networks ieee international conference acoustics speech signal processing ranzato monga devin chen corrado dean building high-level features using large scale unsupervised learning international conference machine learning dean corrado monga chen devin ranzato senior tucker yang large scale distributed deep networks advances neural information processing systems seide error back propagation sequence training context-dependent deep networks conversational speech transcription ieee international conference acoustics speech signal processing heigold vanhoucke senior nguyen ranzato devin dean multilingual acoustic models using distributed deep neural networks ieee international conference acoustics speech signal processing vol. vancouver canada apr. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning.", "year": 2015}