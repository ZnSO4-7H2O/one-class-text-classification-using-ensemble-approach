{"title": "Recurrent Neural Network Attention Mechanisms for Interpretable System  Log Anomaly Detection", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to \"bridge the gap\" between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source.  By incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance.  We demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.", "text": "reference format andy brown aaron tuor brian hutchinson nicole nichols. recurrent neural network attention mechanisms interpretable system anomaly detection. proceedings hpdc york pages. https//doi.org/./_ introduction system analysis critical wide range tasks maintaining large scale computer systems enterprise computer networks high performance computing clusters. include security tasks intrusion detection insider threat detection malware detection well general maintenance tasks detecting hardware failure modeling data traffic flow patterns. extracting knowledge information rich system logs complicated several factors factors unaided human monitoring assessment impractical considerable research directed automated methods visualization analysis system logs. furthermore administrative decisions considerable consequence organizations associated persons crucial understanding factors involved automated decision processes even highly effective algorithms. addressing factors present unsupervised recurrent neural network language models system anomaly detection. modeling normal distribution events system logs anomaly detection approach discover complex relationships buried logs. since methods unsupervised models depend time consuming otherwise expensive procurement labeled data. language modeling framework requires little feature engineering applicable serializable logging source. further models trained online using bounded resources dictated daily volume sources. main contributions work twofold evaluate effectiveness augmenting language models several attention mechanisms specifically designed system anomaly abstract deep learning recently demonstrated state-of-the performance tasks related maintenance computer systems intrusion detection denial service attack detection hardware software system failures malware detection. contexts model interpretability vital administrator analyst trust automated analysis machine learning models. deep learning methods criticized black oracles allow limited insight decision factors. work seek bridge impressive performance deep learning models need interpretable model introspection. present recurrent neural network language models augmented attention anomaly detection system logs. methods generally applicable computer system logging source. incorporating attention variants language models create opportunities model introspection analysis without sacrificing state-of-the performance. demonstrate model performance illustrate model interpretability intrusion detection task using alamos national laboratory cyber security dataset reporting upward area receiver operator characteristic curve despite trained single day’s worth data. concepts computing methodologies anomaly detection; online learning settings; feature selection; unsupervised learning; neural networks; machine learning algorithms; permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior specific permission and/or fee. request permissions permissionsacm.org. first workshop machine learning computer systems june tempe association computing machinery. isbn -x-xxxx-xxxx-x/yy/mm.... https//doi.org/./_ methods describe unsupervised language modeling framework extension five variations attention. case language models consume sequence log-line tokens output log-line-level anomaly scores. language modeling. assume log-line consists sequence tokens token denotes vocabulary. language model model assigns probabilities sequences language model often evaluates probability sequence using chain rule probability data consist series log-lines affiliated user. denote user log-line omit superscript non-essential. language models output single anomaly score negative log-likelihood log-line. tokenization. figure illustrates methods partition lines sequences tokens word character tokenization. word based language modeling tokens fields format file. user fields split character generate user name domain tokens. frequency threshold applied replace infrequent words vocabulary token; value must occur field least times added vocabulary. token ensures models non-zero probabilities encountering previously unseen words evaluation. character based language modeling primitive vocabulary consisting printable ascii characters. circumvents vocabulary issues word model. delimiters left character inputs give context switching fields models. word character tokenization time field ignored tokenized. related work recently several researchers used long short-term memory networks system analysis. zhang clustering techniques text multiple sources generate feature sequences lstm hardware software failure predictions. employ customized parsing methods text system logs generate sequences lstm denial service attack detection. contrast methods approach works directly text preprocessing beyond tokenization using known field delimiters. others incorporated lstm networks preprocess sequences process calls components malware detection systems trained labeled malware examples. attention-equipped lstm models used improve performance complex sequence modeling tasks. attention provides dynamic weighted average values different points calculation processing sequence provide long term context downstream discriminative generative prediction. recent work researchers augmented lstm language models attention mechanisms order capacity modeling long term syntactic dependencies. yogatama characterize attention differentiable random access memory. compare attention language models differentiable stack based memory demonstrating superiority stack based memory verb agreement task multiple attractors. daniluk explore three additive attention mechanisms successive partitioning output lstm; splitting output separate value prediction vectors performed best likely removing need single vector encode information multiple steps computation. contrast augment language models product attention also separate vectors components attention mechanisms. many decision processes raise ethical dilemmas applied critical domains high consequence. factors necessitate human interpretation model generating predictions ensure acceptable results. vellido observe data modeling knowledge extraction potential machine learning solutions underscoring need interpretable automated decision processes. however interpretability multiple goals always aligned production generalizable model architecture hence currently large research focus making interpretable deep learning algorithms sensitive critical application areas. proposed model introspection techniques include dimensionality reduction analysis intermediate layers saliency based methods contrast deep learning components attention mechanisms allow immediate view factors affecting model decisions. examine attention weights determine convolutional neural networks looking making predictions. similarly rocktäschel analyze matrices word-to-word attention weights insight lstm entailment classifier reasons sentences. apply cyber anomaly language models recently introduced language modeling framework cyber anomaly forms starting point work. first four models presented event model applies standard lstm token sequences individual events order feed categorical tokens model first perform embedding lookup token yield sequence rlemb embedding dimension hyperparameter lemb. unique embedding vectors element vocabulary; embedding vectors parameters model learned jointly model parameters. lstm maps embedding vector sequence sequence hidden vectors intuitively summary input sequence defined same standard lstm equations used given previous hidden state weight matrix bias vector probability distribution token step conditions prediction tokens precede log-line. second model bidirectional event model updates eqn. also incorporate hidden state backward-running lstm hidden vector additional weight matrix follows condition predictions tokens line. however tuor also introduce tiered language model variants employ upper tier lstm model user’s sequence log-lines log-line still modeled input concatenation embedding vectors along context vector produced upper tier lstm. upper tier lstm takes input summary lower-tier hidden states upper lower tiers trained jointly. later reference name models t-em t-bem respectively. language models optimize model parameters minimizing negative log-likelihood produced predictions. negative log-likelihood minimization objective also serves anomaly score line attention work product attention wherein attention vector generated three values matrix value matrix query vector formulation keys function value matrix parameterized importance timestep determined magnitude product vector query vector attention dimension hyperparameter magnitudes determine weights weighted value vectors lstm information relevant given prediction accumulated propagated lstm’s cell state given prediction however certain tokens likely relevant others. attention provides mechanism predictions directly selectively conditioned subset relevant tokens. practice accomplished making function concatenation attention vector weighted hidden states attention mechanism introduces shortcuts flow information time allowing model readily access relevant information given prediction weights weighted also yield insights model’s decision process aiding interpretability. first examine case adding attention standard token-step associated value matrix query vector value matrix matrix hidden states excluding token-step dimension lstm hidden states. values weighted performed. fixed attention. fixed variation attention fixed learned vector shared across tokens/steps. assumes positions sequence important others importance depend token trying predict. syntax attention. syntax attention differs fixed attention shared across assumes tokens important others importance depends position sequence token predict actual values tokens xt−. semantic attention instead making function variant interpret emitted lstm concatenation vectors query portion used before value defined eqn. contains note that lstm equations tiered attention. shown fig. original formulation tiered model lower tier lstm hidden states averaged process passing information lower tier upper tier. implementation attention tiered language models replaces mean weighted average attention. lower tier hidden states user line online training employ syncopated online training algorithm allows model continually adapt changing distributions activities network deployed high throughput streaming data sources. beginning day/cycle parameters current model fixed evaluation thereby avoiding evolving anomaly score scale issues could result continuous online training. anomaly scores calculated day’s events train current day’s events. days events discarded bounding storage demands algorithm day’s worth activity number events user day. reduces inter-user anomaly bias stem uneven distribution user name tokens. normalization unnecessary character tokenization user names composed common character vocabulary. results section discuss performance different attention mechanisms. note variance model performance across random parameter initializations quite models. variance given single pretraining suggests method behaves predictably despite rapid deployment. cost additional space complexity storing copies model parameters training evaluation phases concurrently. evaluation training parameters synced daily evaluation copy updated parameters training copy beginning day. data evaluate models publicly available lanl dataset. lanl consists billion lines collected consecutive days. logs contain anonymized process network flow authentication information. interleaved attacks team. experiments focus modeling authentication logs contain following fields events collected desktop servers active directory servers using windows filter automated system events discarding log-lines machine listed source user. team event log-lines indicated dataset. models fully unsupervised team labels evaluation model performance. experimental setup assess model’s ability spin rapidly detect anomalies minimal burn-in time limit scope days contain team events respectively. days contains seven million user lines. chose particular days evaluation largest number events dataset. entire experimental process therefore train evaluate simulating rapid deployment process performed hyper-parameter tuning. learning rate fixed train using adam optimizer; minibatch size lstms single layer hidden units; token embedding size attention size estimate model variability trained model five times fixed hyper-parameters different random weight initializations. results section report statistics five runs. metrics score normalization evaluate results using area receiver operating characteristic curve plots true positive rate false positive rate detection threshold swept. perfect detection yields random guessing yields recall anomaly scores given word negative probabilities tokens line word level lstm baselines outperforms however adding attention improves performance match bem. variations attention similar scores. hypothesize word model equally benefits syntax semantic attention consistent syntax structure. tiered word models attention demonstrate significant performance gains however forward bidirectional attention models trend slightly upwards mean maximum values non-attention counterparts. character tokenization models. shown table fixed syntax attention models appear ill-suited characterbased models variable length fields; neither fixed syntax attention improve performance here character model augmented fixed attention standard deviation times models. contrast semantic variants attention weights function current input opposed sequence position improve performance bem. tiered models little difference incorporating attention suggesting shortcuts introduced attention unnecessary propagate user context across loglines. interesting outcome tiered model either attention bidirectional lower tier reduced variance across random initializations large factor character models. analysis attention performs comparably bidirectionality offers substantial advantages interpretability. investigating fields model attending offers clues decision-making. section illustrate approaches analysis attention-equipped lstm language models analysis global model behavior summary statistics attention weights analysis particular model decisions case studies attention weights language model predictions. global behavior gain insight global behavior attention-equipped lstm summary statistics mean standard deviation attention weights course day’s predictions. figure shows average attention weights attention model predicting last meaningful token error bars standard deviation shown illustrate variability weights. heatmaps average attention weights four attention models proposed section provided figures time step sequence generates weights previous hidden states. larger weight values relevant associated hidden state current prediction. note first input token excluded figures previous hidden states attend over. fixed. figure shows mean weights fixed attention single fixed query change context current time step. source user destination domain source dominate weight vectors suggesting important fields model. syntax. syntax model time step gets query weights. makes sense word tokenized models position dependent syntax. example model exhibiting intuitive behavior predicting source model attending heavily source user. case studies consider three case studies evaluated using semantic attention models. figures depict randomly sampled events evaluated word character semantic attention models respectively. contrast figure random non-anomalous event evaluated semantic word model. tokens predicted true values diverge significant interest contribute heavily anomaly score. disregard probabilities predicting source user impossible foresee user associated random input sequence. word tokenization. first consider word case studies. cases source prediction incorrect confidence. low-anomaly case model able correctly predict destination given source token high probability. however team event predicted token associated different field destination examining weights team event attending heavily hidden state taking destination user domain input predicting source user. note common domain lanl dataset attention likely considering prediction made embedding relates current input token. misclassification exposes disadvantage shared vocabulary field. individual vocabularies field could improve performance cost minor feature engineering. character tokenization. finally examine model function processing character tokenized team event. predicting destination characters hidden state associated comma character right prediction source largest associated weight. second largest weight comma character right destination field begins. suggest model learning positional information comma characters accumulating summary vectors fields storing them subsequent delimiter hidden state. another point interest attention weight semantic. semantic attention mechanisms assume fixed syntactic structure figures show semantic attention variants learn reasonable attention strategies fixed syntax data. overall produce similar attention maps attending heavily source user source semantic also attends heavily authentication type semantic also deems destination user destination important. tiered attention models. tiered model lower forward-directional lstm attention weights nearly second last hidden state. state making decision success/fail conceptually makes sense goal tier lstm pass relevant information forward next event. conversely tiered model bidirectional lstm cells attended fully first hidden state. figure shows backward lstm ends first hidden state. thus bidirectional tiered model collecting final hidden state forward lstm backward lstm summary. suggests shortcut connections attention provides needed model task. figure team case character study semantic attention. coloring true token predicted token rows based probability given character prediction. green represents near probability near attention weights correspond predictions. example predicting character character model uses attention weights provide shifted copy predicted tokens bottom figure align hidden states attended best viewed color. vector substantially impact anomaly score model almost confidence next character would true token near probability. heavy dependence delimiter hidden states. conclusions paper propose five attention mechanism implementations. fixed syntactic attention variants effective modeling sequences fixed structure semantic variants effective input sequences varying lengths looser structures. maintaining state-of-the-art performance attention mechanisms provide information feature importance relational mapping features. additionally architectural insights gleaned attention applied future lead designing effective models. future work includes evaluating system different tasks domains could explore additional attention variants; e.g. bidirectional models attention lead gains performance. finally equipping lower tier model ability attend upper tier hidden states effectively weight relevance previous events user’s sequence. acknowledgments research described paper part analysis motion initiative pacific northwest national laboratory; conducted laboratory directed research development program pnnl multi-program national laboratory operated battelle u.s. department energy. authors also thank nvidia donations titan gpu’s used research. michał daniluk rocktäschel johannes welbl sebastian riedel. frustratingly short attention spans neural language modeling. arxiv preprint arxiv. edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce unbounded memory. advances neural information processing systems. diederik kingma jimmy adam method stochastic optimization. corr abs/. arxiv. http//arxiv.org/abs/. zachary chase lipton. mythos model interpretability. corr koushik nagasubramanian sarah jones asheesh singh arti singh baskar ganapathysubramanian soumik sarkar. explaining hyperspectral imaging based plant disease identification saliency maps. razvan pascanu jack stokes hermineh sanossian mady marinescu anil thomas. malware classification recurrent networks. acoustics speech signal processing ieee international conference ieee rocktäschel edward grefenstette karl moritz hermann tomáš kočisk`y phil blunsom. reasoning entailment neural attention. arxiv preprint arxiv. giancarlo salton robert ross john kelleher. attentive language models. proceedings eighth international joint conference natural language processing vol. aaron tuor ryan baerwolf nicolas knowles brian hutchinson nicole nichols jasper. recurrent neural network language models open vocabulary event-level cyber anomaly detection. arxiv preprint arxiv. ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser illia polosukhin. attention need. advances neural information processing systems. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. corr abs/. arxiv. http//arxiv.org/abs/. dani yogatama yishu miao gabor melis wang ling adhiguna kuncoro chris dyer phil blunsom. memory architectures recurrent neural network language models. international conference learning representations. https//openreview.net/forum?id=skfqflaz zhang jianwu martin renqiang guofei jiang konstantinos pelechrinis zhang. automated system failure prediction deep learning approach. data ieee international conference ieee", "year": 2018}