{"title": "Deconvolutional Paragraph Representation Learning", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.", "text": "learning latent representations long text sequences important ﬁrst step many natural language processing applications. recurrent neural networks become cornerstone challenging task. however quality sentences rnn-based decoding decreases length text. propose sequence-to-sequence purely convolutional deconvolutional autoencoding framework free issue also computationally efﬁcient. proposed method simple easy implement leveraged building block many applications. show empirically compared rnns framework better reconstructing correcting long paragraphs. quantitative evaluation semi-supervised text classiﬁcation summarization tasks demonstrate potential better utilization long unlabeled text data. central task natural language processing learn representations sentences multi-sentence paragraphs. representations typically required ﬁrst step toward applied tasks sentiment analysis machine translation dialogue systems text summarization approach learning sentence representations data leverage encoder-decoder framework standard autoencoding setup vector representation ﬁrst encoded embedding input sequence decoded original domain reconstruct input sequence. recent advances recurrent neural networks especially long short-term memory variants achieved great success numerous tasks heavily rely sentence-representation learning. rnn-based methods typically model sentences recursively generative markov process hidden units one-step-ahead word input sentence generated conditioning previous words hidden units emission transition operators modeled neural networks. principle neural representations input sequences encapsulate sufﬁcient information structure subsequently recover original sentences decoding. however recursive nature challenges exist rnn-based strategies fully encode sentence vector representation. typically training generates words sequence conditioning previous ground-truth words i.e. teacher forcing training rather decoding whole sentence solely encoded representation vector. teacher forcing strategy proven important forces output sequence stay close ground-truth sequence. however allowing decoder access ground truth information reconstructing sequence weakens encoder’s ability produce self-contained representations carry enough information steer decoder decoding process without additional guidance. aiming solve problem proposed scheduled sampling approach training gradually shifts learning latent representation ground-truth signals solely encoded latent representation. unfortunately showed scheduled sampling fundamentally inconsistent training strategy produces largely unstable results practice. result training fail converge occasion. inference ground-truth sentences available words ahead generated conditioning previously generated words representation vector. consequently decoding error compounds proportional length sequence. means generated sentences quickly deviate ground-truth error made sentence progresses. phenomenon coined exposure bias propose simple powerful purely convolutional framework learning sentence representations. conveniently without rnns framework issues connected teacher forcing training exposure bias relevant. proposed approach uses convolutional neural network encoder deconvolutional neural network decoder. best knowledge proposed framework ﬁrst force encoded latent representation capture information entire sentence multi-layer speciﬁcation achieve high reconstruction quality without leveraging rnn-based decoders. multi-layer allows representation vectors abstract information entire sentence irrespective order length making appealing choice tasks involving long sentences paragraphs. further since framework involve recursive encoding decoding efﬁciently parallelized using convolution-speciﬁc graphical process unit primitives yielding signiﬁcant computational savings compared rnn-based models. denote t-th word given sentence. word embedded k-dimensional word vector rk×v word embedding matrix vocabulary size denotes v-th column columns normalized unit -norm i.e. ||we|| dividing column -norm. embedding sentence length represented rk×t concatenating word embeddings i.e. t-th column sentence encoding architecture similar though originally proposed image data. consists layers ultimately summarize input sentence latent representation vector layer consists ﬁlters learned data. i-th ﬁlter layer convolutional rk×h convolution ﬁlter operation stride length applies ﬁlter r/r+ size. yields latent feature nonlinear activation function r/r+ denotes convolutional operator. experiments represented rectiﬁed linear unit note original embedding dimension changes ﬁrst convolutional layer r/r+ concatenating results ﬁlters results feature rp×/r+]. ﬁrst convolutional layer apply convolution operation feature using ﬁlter size repeated sequence layers. time length along spatial coordinate reduced h)/r stride length spatial length denotes l-th layer ﬂoor function. ﬁnal layer feature fully-connected layer produce latent representation implementation-wise convolutional layer ﬁlter size equals equivalent fully-connected layer; implementation trick also utilized last layer summarizes remaining spatial coordinates scalar features encapsulate sentence sub-structures throughout entire sentence characterized ﬁlters denotes ﬁlter layer also implies extracted feature ﬁxed-dimensionality independent length input sentence. figure convolutional auto-encoding architecture. encoder input sequence ﬁrst expanded embedding matrix fully compressed representation vector multi-layer convolutional encoder stride. last layer spatial dimension collapsed remove spatial dependency. decoder latent vector multi-layer deconvolutional decoder stride reconstruct cosine-similarity cross-entropy loss. ﬁlters last layer results pl-dimensional representation vector input sentence. example figure encoder consists layers sentence length embedding dimension stride lengths ﬁlter sizes number ﬁlters results intermediate feature maps sizes respectively. last feature size corresponds latent representation vector conceptually ﬁlters lower layers capture primitive sentence information higher level ﬁlters capture sophisticated linguistic features semantic syntactic structures bottom-up architecture models sentences hierarchically stacking text segments building blocks representation vector similar spirit modeling linguistic grammar formalisms concrete syntax trees however pre-specify tree structure based syntactic structure rather abstract data multi-layer convolutional network. deconvolutional decoder apply deconvolution stride conjugate operation convolution decode latent representation back source text domain. deconvolution operation proceeds spatial resolution gradually increases mirroring convolutional steps described above illustrated figure spatial dimension ﬁrst expanded match spatial dimension layer convolution progressively expanded l-th deconvolutional layer output l-layer deconvolution operation aims reconstruct word embedding matrix denote line word embedding matrix columns normalized unit -norm. denoting t-th word reconstructed sentence probability word speciﬁed dcos cosine similarity deﬁned ||x||||y|| v-th column t-th column positive number denote temperature parameter parameter akin concentration parameter dirichlet distribution controls spread probability vector thus large encourages uniformly distributed probabilities whereas small encourages sparse concentrated probability values. experiments note setting cosine similarity obtained inner product provided columns unit -norm speciﬁcation. denotes observed sentences. simple maximum-likelihood objective optimized stochastic gradient descent. details implementation provided experiments. note differs prior related work ways pooling un-pooling operators convolution/deconvolution stride; importantly cosine similarity reconstruction rnn-based decoder. discussion related work provided section could pooling un-pooling instead striding however early experiments observe signiﬁcant performance gains convolution/deconvolution operations stride considerably efﬁcient terms memory footprint. compared standard lstm-based sequence autoencoders roughly number parameters computations case considerably faster using single nvidia titan gpu. high parallelization efﬁciency cnns cudnn primitives comparison deconvolutional decoders proposed framework seen complementary building block natural language modeling. contrary standard lstmbased decoder deconvolutional decoder imposes general less strict sequence dependency compared architectures. speciﬁcally generating word requires vector hidden units recursively accumulate information entire sentence order-preserving manner deconvolutional decoder generation depends representation vector encapsulates information throughout sentence without pre-speciﬁed ordering structure. result language generation tasks decoder usually generate coherent text compared deconvolutional decoder. contrary deconvolutional decoder better accounting distant dependencies long sentences beneﬁcial feature extraction classiﬁcation text summarization tasks. identifying related topics sentiments abstracting summaries user generated content blogs product reviews recently received signiﬁcant interest many practical scenarios unlabeled data abundant however many practical cases potential unlabeled data fully realized. motivated opportunity seek complement scarcer valuable labeled data improve generalization ability supervised models. ingesting unlabeled data model learn abstract latent representations capture semantic meaning available sentences irrespective whether labeled. done prior supervised model training two-step process. recently rnn-based methods exploiting idea widely utilized achieved state-of-the-art performance many tasks alternatively learn autoencoder classiﬁer jointly specifying classiﬁcation model whose input latent representation instance case product reviews example review contain hundreds words. poses challenges training rnn-based sequence encoders sense abstract information on-the-ﬂy moves sentence often leads loss information particularly long sentences furthermore decoding process uses ground-truth information training thus learned representation necessarily keep information input text necessary proper reconstruction summarization classiﬁcation. consider applying convolutional autoencoding framework semi-supervised learning long-sentences paragraphs. instead pre-training fully unsupervised model cast semi-supervised task multi-task learning problem similar i.e. simultaneously train sequence autoencoder supervised model. principle using joint training strategy learned paragraph embedding vector preserve reconstruction classiﬁcation ability. annealing parameter balancing relative importance supervised unsupervised loss; denote labeled unlabeled data respectively. ﬁrst term sequence autoencoder loss d-th sequence. lsup supervision loss d-th sequence classiﬁer function attempts reconstruct either multi-layer perceptron classiﬁcation tasks cnn/rnn text summarization tasks. latter interested purely convolutional speciﬁcation however also consider comparison. classiﬁcation standard cross-entropy loss text summarization either standard lstm loss rnn. practice adopt scheduled annealing strategy rather ﬁxing priori training gradually transits focusing solely unsupervised sequence autoencoder supervised task annealing small positive value αmin. αmin experiments. motivation annealing strategy ﬁrst focus abstracting paragraph features selectively reﬁne learned features informative supervised task. previous work considered leveraging cnns encoders various natural language processing tasks typically cnn-based encoder architectures apply single convolution layer followed pooling layer essentially acts detector speciﬁc classes h-grams given convolution ﬁlter window size deep architecture framework will principle enable high-level layers capture sophisticated language features. convolutions stride rather pooling operators e.g. max-pooling spatial downsampling following argued fully convolutional architectures able learn spatial downsampling. further uses -layer text classiﬁcation. encoder considerably simpler structure still achieving good performance. language decoders rnns less well studied. recently proposed hybrid model coupling convolutional-deconvolutional network acts decoder deconvolutional model bridge encoder decoder. additionally considered variants pixelcnn text generation. nevertheless achieve good empirical results methods still require sentences generated sequentially conditioning ground truth historical information akin rnn-based decoders thus still suffering exposure bias. efforts made improve embeddings long paragraphs using unsupervised approaches paragraph vector learns ﬁxed length vector concatenating wordvec embedding history sequence predict future words. hierarchical neural autoencoder builds hierarchical attentive uses paragraph-level hidden units embedding. work differs approaches force sequence fully restored latent representation without history information. previous methods considered leveraging unlabeled data semi-supervised sequence classiﬁcation tasks. typically rnn-based methods consider either training sequence-to-sequence autoencoder classiﬁer robust adversarial perturbation initialization encoder supervised model learning latent representation sequence-to-sequence autoencoder using inputs classiﬁer also takes features extracted inputs summarization tasks considered semi-supervised approach based support vector machines however research semi-supervised text summarization using deep models scarce. every visit hotel beacon place love stay conveniently located central park lincoln center great local restaurants rooms lovely beds comfortable great little kitchen wizz bang coffee maker staff accommodating love walking across street fairway supermarket every imaginable goodies every time york lighthouse hotel favorite place stay convenient central park lincoln center great restaurants room wonderful comfortable kitchenette large explosion coffee maker staff inclusive across street walk supermarket channel love kinds lstm-lstm every visit hotel beacon place relax wanting become conveniently located hotel evenings good budget accommodations views great couples manny doorman great come deﬁnitly want leave stay enjoy wonderfully relaxing wind break hour early rick’s cafe perfect easy easy walking distance everything imaginable groceries want watch every visit hotel beacon place love stay closely located central park lincoln center great local restaurants biggest rooms lovely beds comfortable great little kitchen suggestion coffee maker staff turned accommodating love walking across street former fairway supermarket every food taxes word embedding respectively. dimension latent representation vector varies experiment thus reported separately. notational convenience denote convolutional-deconvolutional autoencoder cnndcnn. comparisons also considered standard autoencoders baselines cnnlstm encoder coupled lstm decoder; lstm-lstm lstm encoder lstm decoder. lstm-dcnn conﬁguration included yields similar performance cnn-dcnn computationally expensive. complete experimental setup baseline details provided supplementary material cnn-dcnn least number parameters. example using dimension results million total trainable parameters cnn-dcnn cnn-lstm lstm-lstm respectively. paragraph reconstruction ﬁrst investigate performance proposed autoencoder terms learning representations preserve paragraph information. adopt evaluation criteria i.e. rouge score bleu score measure closeness reconstructed paragraph input paragraph. brieﬂy rouge bleu scores measures n-gram recall precision model outputs references. bleu- rouge- evaluation alignment addition cnnlstm lstm-lstm autoencoder also compared hierarchical lstm autoencoder comparison performed hotel reviews datasets following experimental setup i.e. keep reviews sentence length ranging words resulting training data samples testing data samples. comparisons dimension latent representation table long paragraphs lstm decoder cnn-lstm lstm-lstm suffers heavy exposure bias issues. evaluate performance model different paragraph lengths. shown figure table task cnn-dcnn demonstrates clear advantage meanwhile length sentence increases comparative advantage becomes substantial. lstm-based methods quality reconstruction deteriorates quickly sequences longer. constrast reconstruction quality cnn-dcnn stable consistent regardless sentence length. furthermore computational cost evaluated wall-clock signiﬁcantly lower cnn-dcnn. roughly cnn-lstm times slower cnn-dcnn lstm-lstm times slower single gpu. details reported character-level word-level correction task seeks evaluate whether deconvolutional decoder overcome exposure bias severely limits lstm-based decoders. consider denoising autoencoder input tweaked slightly certain modiﬁcations model attempts denoise unknown modiﬁcation thus recover original sentence. character-level correction consider yahoo answer dataset dataset description setup word-level correction provided follow experimental setup word-level character-level spelling correction considered substituting word/character different random probability character-level analysis ﬁrst characters dimensional embedding vector network structure wordcharacter-level models kept same. figure comparison. black triangles indicate epoch. employ character error rate word error rate evaluation. wer/cer measure ratio levenshtein distance model predictions ground-truth total length sequence. conceptually lower wer/cer indicates better performance. lstm-lstm cnn-lstm denoising autoencoders comparison. architecture word-level baseline models previous experiment. character-level correction dimension also compare actor-critic training following experimental guidelines shown figure table observed cnn-dcnn achieves lower faster convergence. further cnn-dcnn delivers stable denoising performance irrespective noise location within sentence seen figure cnn-dcnn even error detected exactly corrected denoising future words effected cnn-lstm lstm-lstm error gradually accumulates longer sequences expected. word-level correction consider word substitutions only mixed perturbations three kinds substitution deletion insertion. generally cnn-dcnn outperforms cnn-lstm lstm-lstm faster. provide experimental details comparative results semi-supervised sequence classiﬁcation summarization investigate whether cnndcnn framework improve upon supervised natural language tasks leverage features learned paragraphs. principle good unsupervised feature extractor improve generalization ability semi-supervised learning setting. evaluate approach three popular natural language tasks sentiment analysis paragraph topic prediction text summarization. ﬁrst tasks essentially sequence classiﬁcation summarization involves language comprehension language generation. consider three large-scale document classiﬁcation datasets dbpedia yahoo answers yelp review polarity partition training validation test sets datasets follows settings detailed summary statistics datasets shown demonstrate advantage incorporating reconstruction objective training text classiﬁers evaluate model different amounts labeled data whole training unlabeled data. purely supervised baseline model convolutional encoder architecture described above -dimensional latent representation dimension followed classiﬁer hidden layer hidden units. dropout rate word embeddings initialized random. shown table joint training strategy consistently signiﬁcantly outperforms purely supervised strategy across datasets even labels available. hypothesize early phase training reconstruction emphasized features text fragments readily learned. training proceeds discriminative text fragment features selected. further subset features responsible reconstruction discrimination presumably encapsulate longer dependency structure compared features using purely supervised strategy. figure demonstrates behavior model semi-supervised setting yelp review dataset. results yahoo answer dbpedia provided table test error rates document classiﬁcation results methods obtained summarization used dataset composed abstract-title pairs arxiv. abstracttitle pairs selected length title abstract exceed words respectively. partitioned training validation test sets pairs each. train sequence-to-sequence model generate title given abstract using randomly selected subset paired data proportion every value considered purely supervised summarization using abstract-title pairs semisupervised summarization leveraging additional abstracts without titles. compared lstm deconvolutional network decoder generating titles table summarizes quantitative results using rouge-l general additional abstracts without titles improve generalization ability test set. interestingly even joint training objective still yields better performance using lsup alone. presumably since joint training objective requires latent representation capable reconstructing input paragraph addition generating title learned representation better capture entire structure paragraph. also empirically observed titles generated joint training objective likely words appearing corresponding paragraph titles generated using purely supervised objective lsup tend wording freely thus abstractive. possible explanation that joint training strategy since reconstructed paragraph title generated latent representation text fragments used reconstructing input paragraph likely leveraged building title thus title bears resemblance input paragraph. expected titles produced deconvolutional decoder less coherent lstm decoder. presumably since paragraph summarized multiple plausible titles deconvolutional decoder trouble positioning text segments. provide discussions titles generated different setups designing framework takes best worlds lstm generation decoding interesting future direction. proposed general framework text modeling using purely convolutional deconvolutional operations. proposed method free sequential conditional generation avoiding issues associated exposure bias teacher forcing training. approach enables model fully encapsulate paragraph latent representation vector decompressed reconstruct original input sequence. empirically proposed approach achieved excellent long paragraph reconstruction quality outperforms existing algorithms spelling correction semi-supervised sequence classiﬁcation summarization largely reduced computational cost.", "year": 2017}